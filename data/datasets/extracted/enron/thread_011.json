{
    "mails": [
        {
            "subject": "Re: RFC2109 addition..",
            "content": "On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> But that is UI...  it's not really our problem here as I understand it.\n\nA much debated point on this list, of late.  But yes, it is definitely not\nour concern here.  I raise the UI implications of this proposal simply\nbecause the spec should consider the many clients that may implement it.\n\n> > I like David's point about language negotiation being handled through the\n> > follow-up request, though.  \n> \n> I'm afraid I don't fully understand this issue.  Language negotiation\n> is a part of the cookie spec?  I think I missed that part.  Somebody\n> wanna point me at a URL?\n\nSee the 'Accept-Language' header of HTTP.  Basically, the client could say,\n\"I prefer Japanese to English,\" and the server processing the commentURL\nrequest could return a Japanese policy document. \n\n> What I said (meant to say?) is that they should\n> be interpreted EXACTLY as a src=\"...\" field in the body of an html doc.\n\nOkay, that sounds good.  Since we have been debating domain restrictions, I\nthought you meant to restrict the policy URL to the same server as the\nresource setting the cookie.\n\n> > > CommentURL=commenturl\n> > \n> > How about \n> >   CommentURL = '<' commenturl '>'\n> \n> I'm new to this whole process, so I guess I don't understand the\n> difference in notation.  Does this now imply that the attributeline would\n> look like this:\n> CommentURL=<http://www.privacy.net/disclosure>\n> ?\n\nYes.\n\n> If so, I disagree.  It should be parsed the same as all the other\n> attributes...\n> CommentURL=http://www.privacy.net/disclosure\n> However you notate that.... :)  I'm not sure... are there problems\n> with escaped characters in a URL meaning something in the Cookie?\n\nYes, Set-Cookie2 accepts a comma-separated list of cookies.  Since some\nsites (the c|net sites come to mind) use commas in URLs, we would need to\nprotect the URL commas from interpretation.  This would add a little\ncomplexity to URL-parsing.\n\n> > Add: \"A server SHOULD send a comment if sending a commentURL, for use by\n> > those browsers unable to display the CommentURL contents.\"\n> \n> I question this a little bit.  My goal (in addition to the one you\n> pointed out, in providing a common location for collecting policies)\n> is to reduce the amount of data that has to be sent to the client.\n> If you send a comment AND a URL, that doesn't really achive my goal.\n\nGood point, my suggestion isn't necessary.\n\n> > > A user-agent can offer the user the option of inspecting this page before\n> > > accepting a cookie.  \n> > \n> > Should be: \"A user-agent MAY...\"\n> \n> I'll go half way... how about \"A user-agent should\"? :)\n\nI do think it should be \"may\" so that browsers unable or unwilling to open\na second HTML-rendering window can feel perfectly comfortable ignoring the\ncommentURL.  (Hence the suggestion for both commentURL and comment.....)\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "Quoting Dave Morris:\n> On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> \n> > Here's a first crack at the text as I feel it should be included in the\n> > RFC:\n> > \n> > --\n> > CommentURL=commenturl\n> > Optional.  The CommentURL allows an origin server to specify a document\n> > that explains the usage of this cookie, and could optionally also explain\n> > the policies governing the use of information collected through this cookie.\n> > A user-agent can offer the user the option of inspecting this page before\n> > accepting a cookie.  Any cookies issued while attempting to retrieve the\n> > document at commenturl should be refused.\n> \n> I have been working thru a similar idea before presenting it ... BUT thus\n> far my thought is that there shouldn't be any restrictions on what the\n> URL points at, associated cookies, etc. except that we need to work thru\n> the rules to make sure a privacy hole isn't created... but I think if\n\n\nThere's a catch22 situation here though.  If someone is looking to\nmake a decision on whether or not they want to accept a cookie, they\nshould have some \"safe haven\";  They should be able to get the disclosure\ninformation before having to make a decision.  Accepting a cookie by\ndefault, or even being prompted to accept a cookie (which incidently,\nin many cases is likely to be the EXACT same cookie that they're investigating\nthe policies on), is a little bit crazy.  You should NEVER be prompted\nabout whether or not you want to accept a cookie when investingating\nthe policy behind another cookies use.  Since you shouldn't be prompted,\nthere needs to be a default \"accept\" or a default \"reject\".  In this\nparticilar case, I think a default \"accept\" would be irresponsible.\n \n\n> the rules are that retrieving this URL is like following any other link\n> then I don't think there are any new exposures.  (That is the cookie\n> issued by this link would have to fit the URL being retrieved.)  THis\n> has the additional advantage in that language issues can be handled via\n\nI agree completely that language issues should be handled automatically\nin the normal way, but what does that have to do with restrictions on\nthe URL, or with cookies?  Is there a method besides the proposed variant\nlist method (which I understood to be a httpd thing) that requires\ncookies to be accepted in order to negotiate?  I don't recall seeing anything\nin 2109.\n\nI fail to see a huge value in putting a cookie on what should in most cases\nbe a static document describing policies.  It seems sort of hipocritical\nto issue a cookie on a page that is supposed to help people decide\nwhether or not they want to accept a cookie.  But maybe I'm just looking\nat this wrong.\n\n> normal UA / server negotiation. A suggested UI for an UA able to do so\n> would be to open a new browser window to follow the link.\n\nPerhaps, though I could see a user getting completely lost in windows\nif there isn't something to uniquely identify this window, or a way\nof requiring a decision on the cookie issue before continuing.  I personally\nhate when a web page forces me to open a link in a new targeted window.\n\nOf course, I'm assuming the scenario when the user is prompted as\nto whether or not to accept a cookie, and they then go to look at\nthe policies.  A cookie manager portion of a browser should probably\nallow a user to look at the policy for all cookies that have cookie\ncomment or comment url's without necessarily being asked to accept\na cookie.  (man, I'm getting tired of the word cookie.)  This second\nway of looking at the cookie comments might be treated differently.\n\n> Dave Morris\n\nThanks for the input, Dave.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "RE: RFC2109 addition..",
            "content": "Why not just make the comment field syntax into something like\nCommentAttribute = \"Comment\" \"=\" (Quoted-String | \"<\" URI \">\")\n?\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Monday, March 24, 1997 4:40 PM\n> To:Jonathan Stark\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: RFC2109 addition...\n> \n> \n> \n> On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> \n> > Here's a first crack at the text as I feel it should be included in\n> the\n> > RFC:\n> > \n> > --\n> > CommentURL=commenturl\n> > Optional.  The CommentURL allows an origin server to specify a\n> document\n> > that explains the usage of this cookie, and could optionally also\n> explain\n> > the policies governing the use of information collected through this\n> cookie.\n> > A user-agent can offer the user the option of inspecting this page\n> before\n> > accepting a cookie.  Any cookies issued while attempting to retrieve\n> the\n> > document at commenturl should be refused.\n> \n> I have been working thru a similar idea before presenting it ... BUT\n> thus\n> far my thought is that there shouldn't be any restrictions on what the\n> URL points at, associated cookies, etc. except that we need to work\n> thru\n> the rules to make sure a privacy hole isn't created... but I think if\n> the rules are that retrieving this URL is like following any other\n> link\n> then I don't think there are any new exposures.  (That is the cookie\n> issued by this link would have to fit the URL being retrieved.)  THis\n> has the additional advantage in that language issues can be handled\n> via\n> normal UA / server negotiation. A suggested UI for an UA able to do so\n> would be to open a new browser window to follow the link.\n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "On Mon, 24 Mar 1997, Jonathan Stark wrote:\n\n> \n> Quoting Dave Morris:\n> > On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> > \n> > > Here's a first crack at the text as I feel it should be included in the\n> > > RFC:\n> > > \n> > > --\n> > > CommentURL=commenturl\n> > > Optional.  The CommentURL allows an origin server to specify a document\n> > > that explains the usage of this cookie, and could optionally also explain\n> > > the policies governing the use of information collected through this cookie.\n> > > A user-agent can offer the user the option of inspecting this page before\n> > > accepting a cookie.  Any cookies issued while attempting to retrieve the\n> > > document at commenturl should be refused.\n> > \n> > I have been working thru a similar idea before presenting it ... BUT thus\n> > far my thought is that there shouldn't be any restrictions on what the\n> > URL points at, associated cookies, etc. except that we need to work thru\n> > the rules to make sure a privacy hole isn't created... but I think if\n> \n> \n> There's a catch22 situation here though.  If someone is looking to\n> make a decision on whether or not they want to accept a cookie, they\n> should have some \"safe haven\";  They should be able to get the disclosure\n> information before having to make a decision.  Accepting a cookie by\n> default, or even being prompted to accept a cookie (which incidently,\n> in many cases is likely to be the EXACT same cookie that they're investigating\n> the policies on), is a little bit crazy.  You should NEVER be prompted\n> about whether or not you want to accept a cookie when investingating\n> the policy behind another cookies use.  Since you shouldn't be prompted,\n> there needs to be a default \"accept\" or a default \"reject\".  In this\n> particilar case, I think a default \"accept\" would be irresponsible.\n\nI think this may present a wierdness, but one that the UI folks can deal\nwith. (I've been told that they like challenges.) I think rather than\nforbidding such cookies, we should say something like:\n\n  1.  Servers should consider the UI dilemma when sending cookies with\n      the cookie explanation.\n  2.  Warn UA implementors that a cookie may be included with the \n      explanatory document and therefore recursive handling of the\n      document may be necessary.\n\nI proposed the new window as the recommended approach as it makes the\n'branch' visually obvious and provides a fairly clean separation between\nthe cookie associated with the original response and the cookie associated\nwith the cookie backup.  It also allows us avoid being concerned with the\nwhat if the URL with descriptive material includes images or links to\nadditional pages.\n\nI think its cleaner to not place the restriction on the response since\nwe don't have any mechanism to tell the server of the restriction.\n\nIn this UAHINT draft, I propose a popup window which differs mostly in \nthat it might not have the normal toolbar and has a notion of going away\nafter the user has seen it.  I think such a mechanism would apply here.\n\n> > the rules are that retrieving this URL is like following any other link\n> > then I don't think there are any new exposures.  (That is the cookie\n> > issued by this link would have to fit the URL being retrieved.)  THis\n> > has the additional advantage in that language issues can be handled via\n> \n> I agree completely that language issues should be handled automatically\n> in the normal way, but what does that have to do with restrictions on\n> the URL, or with cookies?  Is there a method besides the proposed variant\n> list method (which I understood to be a httpd thing) that requires\n> cookies to be accepted in order to negotiate?  I don't recall seeing anything\n> in 2109.\n\nI don't recall any specific connection.  I brought up the language\nnegotiation as a plus for your proposal and an argument for not handling\nthe URL as a special case.\n\n> \n> I fail to see a huge value in putting a cookie on what should in most cases\n> be a static document describing policies.  It seems sort of hipocritical\n> to issue a cookie on a page that is supposed to help people decide\n> whether or not they want to accept a cookie.  But maybe I'm just looking\n> at this wrong.\n\nI think you are placing an unnecessary restriction on the URL. And as soon\nas it becomes a special case you must deal with what if a link in the\npage happens to have a cookie ... better to try and stay clean and avoid\nspecial cases.\n\nI do agree that I see not much value in having a cookie associated with\nthe policy document and I suspect with the right warning most servers\nwon't do it. But by allowing the case, we cover the LINK from that\ndocument where it seems morelikely that a set cookie will be encountered.\n\nAlso, if a cookie can't be set on the commentURL, can one be sent with the\nrequest. I make the same argument ... keep the protocol clean and not\ncare.\n\n> \n> > normal UA / server negotiation. A suggested UI for an UA able to do so\n> > would be to open a new browser window to follow the link.\n> \n> Perhaps, though I could see a user getting completely lost in windows\n> if there isn't something to uniquely identify this window, or a way\n> of requiring a decision on the cookie issue before continuing.  I personally\n> hate when a web page forces me to open a link in a new targeted window.\n\nI don't object to differentiation in the window ... and I don't see any\ndifference between a special window and a new targeted window. \n\n> Of course, I'm assuming the scenario when the user is prompted as\n> to whether or not to accept a cookie, and they then go to look at\n> the policies.  A cookie manager portion of a browser should probably\n> allow a user to look at the policy for all cookies that have cookie\n> comment or comment url's without necessarily being asked to accept\n\nYup ... they should be able to check any time.\n\n> a cookie.  (man, I'm getting tired of the word cookie.)  This second\n> way of looking at the cookie comments might be treated differently.\n\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "On Mon, 24 Mar 1997, M. Hedlund wrote:\n\n> > > > CommentURL=commenturl\n> > > \n> > > How about \n> > >   CommentURL = '<' commenturl '>'\n> > \n> > I'm new to this whole process, so I guess I don't understand the\n> > difference in notation.  Does this now imply that the attributeline would\n> > look like this:\n> > CommentURL=<http://www.privacy.net/disclosure>\n> > ?\n\nOffhand, I don't recall another case in the HTTP protocol where <> is\nused to quote anything. So it would be better to use the 'normal' form\nof quoting things with possible special characters and use\n    CommentURL=\"http://www.privacy.net/disclosure\"\nif quoting is required. \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: cookie Port summar",
            "content": "On Mon, 24 Mar 1997, Dave Kristol wrote:\n\n> Here's my summary and elaboration of the proposal for restricting ports\n> in cookies.\n>[...] \n> Comments?\n\nSounds good to me.\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "> \n> Why not just make the comment field syntax into something like\n> CommentAttribute = \"Comment\" \"=\" (Quoted-String | \"<\" URI \">\")\n> ?\n> Yaron\n\nI disagree with this approach unless we make it legal to have\nmultiple Comment attributes within the same cookie (And I don't\nthink we really do want to do that.)  As discussed earlier, I think \nthere is a definite value in being able to include a URI AND a \ncomment \"string\".  Some browsers may support one and not the other, \nsome groups may want to dynamically explain their cookies,\nsending a user-specific comment string with each cookie, or who\nknows... I don't think we should limit ourselves to having\none or the other.\n\n(And Dave, I like the quoted notation for the URL's better than\n<> as well.)\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "Dave Morris said:\n> On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> > Quoting Dave Morris:\n> > > On Mon, 24 Mar 1997, Jonathan Stark wrote:\n> > \n> > default, or even being prompted to accept a cookie (which incidently,\n> > in many cases is likely to be the EXACT same cookie that they're investigating\n> > the policies on), is a little bit crazy.  You should NEVER be prompted\n> > about whether or not you want to accept a cookie when investingating\n> > the policy behind another cookies use.  Since you shouldn't be prompted,\n> > there needs to be a default \"accept\" or a default \"reject\".  In this\n> > particilar case, I think a default \"accept\" would be irresponsible.\n> \n> I think this may present a wierdness, but one that the UI folks can deal\n> with. (I've been told that they like challenges.) I think rather than\n> forbidding such cookies, we should say something like:\n> \n>   1.  Servers should consider the UI dilemma when sending cookies with\n>       the cookie explanation.\n\nHow would the server know?  The exact same document might be served\nas part of the normal site on a privacy page that gets the default set\nof cookies sent out with it.  (or more likely, images referenced in the \ncookie explanation document might be standard ones that get served all the \ntime, like the companies name/logo).  If there's a choice between putting\nthe intelligence on the server or on the client side, I vote client.\n\n>   2.  Warn UA implementors that a cookie may be included with the \n>       explanatory document and therefore recursive handling of the\n>       document may be necessary.\n\nRecursive handling is probably not the correct way to go, though.\nSomewhere somehow a flag should get set such that you can't recursively\nrequest the CommentURL, get a cookie on the CommentURL, intuitively\nsay, No, I don't want to accept the cookie I just asked for information\nabout without seeing the policy, request the exact same CommentURL\npage again, get the same cookie again ... forever...  (Never underestimate\nthe stupidity of a fool when making something foolproof.)  But, once\nagain, this is pretty much up to the browser folks.\n\n\n> I think its cleaner to not place the restriction on the response since\n> we don't have any mechanism to tell the server of the restriction.\n\nWhy would the server NEED to know?  If the server \"goofs\", the client\njust silently ignores it.  (And, incidently, doesn't this slightly\nconflict with what you said above about having the server take special\nsteps when serving the cookie explanations?  How would it know it's\nserving the cookie explanations?  By URL alone?  That seems a little\nbit shaky to me.)\n\n> In this UAHINT draft, I propose a popup window which differs mostly in \n> that it might not have the normal toolbar and has a notion of going away\n> after the user has seen it.  I think such a mechanism would apply here.\n\nThat sounds quite appropriate.\n\n> > I agree completely that language issues should be handled automatically\n> > in the normal way, but what does that have to do with restrictions on\n> > the URL, or with cookies?  Is there a method besides the proposed variant\n> > list method (which I understood to be a httpd thing) that requires\n> > cookies to be accepted in order to negotiate?  I don't recall seeing anything\n> > in 2109.\n> \n> I don't recall any specific connection.  I brought up the language\n> negotiation as a plus for your proposal and an argument for not handling\n> the URL as a special case.\n\nSorry... my fault.  I misinterpreted your point.\n\n> > I fail to see a huge value in putting a cookie on what should in most cases\n> > be a static document describing policies.  It seems sort of hipocritical\n> > to issue a cookie on a page that is supposed to help people decide\n> > whether or not they want to accept a cookie.  But maybe I'm just looking\n> > at this wrong.\n> \n> I think you are placing an unnecessary restriction on the URL. And as soon\n> as it becomes a special case you must deal with what if a link in the\n> page happens to have a cookie ... better to try and stay clean and avoid\n> special cases.\n\nWell, I would like to stay clear of special cases too, and would totally agree\nwith you if I thought there weren't the possibity for a situation where my \nmom would be frustrated for the rest of her life going back and forth between \ncookie dialog boxes and half-loaded CommentURL pages (Stuff like that ALWAYS\nends in a very confused phone call to me.) :)  But maybe we're picking\nnits.  The folks at Netscape, Microsoft, etc etc etc are sharp enough\nto make sure that doesn't happen, right?  (Making products mom-proof\nis certainly one of those challenges they're looking for!)\n\n> I do agree that I see not much value in having a cookie associated with\n> the policy document and I suspect with the right warning most servers\n> won't do it. But by allowing the case, we cover the LINK from that\n> document where it seems morelikely that a set cookie will be encountered.\n\nIt seemed that by just having the client ignore them by default,\nthe server wouldn't have to worry about any special cases.  After all,\nthese documents (and especially images) may be perfectly fair game for\ncookies under normal situations.  Only when evaluating whether or not\nto accept cookies does the recursive problem really get nasty.\n\n> Also, if a cookie can't be set on the commentURL, can one be sent with the\n> request. I make the same argument ... keep the protocol clean and not\n> care.\n\nHmm.. that's an interesting point.  I would say yes, a cookie should\nbe able to be sent with the request.  At that point, you've already accepted\nthe cookie (for whatever reason), and should send it off. My big concern\nis more in the craziness of being prompted to accept a cookie while trying\nto figure out the purpose of that exact cookie from a previous request.\n\n> > Perhaps, though I could see a user getting completely lost in windows\n> > if there isn't something to uniquely identify this window, or a way\n> > of requiring a decision on the cookie issue before continuing.  I personally\n> > hate when a web page forces me to open a link in a new targeted window.\n> \n> I don't object to differentiation in the window ... and I don't see any\n> difference between a special window and a new targeted window. \n\nI guess it's, once again, a UI issue that we probably shouldn't worry about.\n\nYou've brought up some excellent points!  Thanks!\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Should server beable to say NoCookie, No Show",
            "content": "It seems to me that there are many applications which will break (in the\nsense of delivering confusing error messages, garbage, etc to the user) if\na cookie isn't accepted by the UA and returned with the request resulting\nfrom a submit of the page which carried the set-cookie2.\n\nSymetry would suggest that since we encourage/allow a UA to discard a\ncookie under the user's discretion, we should have an optional attribute\nwhich allows the server to stipulate one of the following:\n\n  a.  Dont show the page if the user rejects the cookie\n  b.  Warn the user that if the cookie isn't accepted, the application\n      won't operate correctly (this is almost covered by the\n      comment/commentURL but its a different of message I think. Like\n      Windows allows a message box to be one of several types to reflect\n      the content, the significance of the comment to the user would\n      vary depending on the damage to the user's experience by\n      rejecting the cookie.\n\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: cookie Port summar",
            "content": "http-wg@cuckoo.hpl.hp.com writes:\n\n>On Mon, 24 Mar 1997, Dave Kristol wrote:\n>\n>> Here's my summary and elaboration of the proposal for restricting ports\n>> in cookies.\n>>[...]\n>> Comments?\n>\n>Sounds good to me.\n\nIt satisfies my concerns as well.  Nice job, Dave.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "David W. Morris wrote:\n> \n> I fail to understand the rationale behind what to me is the most complex\n> section in the whole document.  Why are we requiring UAs to combine\n> the two headers?\n> \n> I think there is no siginificant loss of functionality if this whole\n> section is dropped along with the forward reference in the previous\n> section.  Simply require the server to send both with appropriate\n> attributes. If the UA understands both forms, it MUST send the new form\n> and it must replace an existing matching form 1 with the new form.\n> \n> Otherwise the UA doesn't understand both and sends the old form.\n\nThis issue has been discussed (but perhaps privately?)\n\nThe complaint from some parties was that the NAME=VALUE part of cookies, in\nparticular, can be (and already is) quite large.  So sending it twice, once\nin Set-Cookie and once in Set-Cookie2 would incur a lot of network traffic.\n\nI agree that sending a completely separate Set-Cookie2 header with its own\nset of values would be much simpler.  But the network traffic that results\nwas deemed excessive.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "> > > > CommentURL=commenturl\n> > >\n> > > How about\n> > >   CommentURL = '<' commenturl '>'\n> >\n> > I'm new to this whole process, so I guess I don't understand the\n> > difference in notation.  Does this now imply that the attributeline would\n> > look like this:\n> >       CommentURL=<http://www.privacy.net/disclosure>\n> > ?\n> \n> Yes.\n> \n> > If so, I disagree.  It should be parsed the same as all the other\n> > attributes...\n> >       CommentURL=http://www.privacy.net/disclosure\n> > However you notate that.... :)  I'm not sure... are there problems\n> > with escaped characters in a URL meaning something in the Cookie?\n> \n> Yes, Set-Cookie2 accepts a comma-separated list of cookies.  Since some\n> sites (the c|net sites come to mind) use commas in URLs, we would need to\n> protect the URL commas from interpretation.  This would add a little\n> complexity to URL-parsing.\n\nI don't think '<' and '>' improve the situation.  The general description of\nvalues says they can be \"-enclosed.  So a URL with special characters like ','\ncould be quoted.  (See my similar remarks about port-list for Port=.)  There's\nno need to add <>.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-pep02.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : PEP: an Extension Mechanism for HTTP                    \n       Author(s) : D. Connolly\n       Filename  : draft-ietf-http-pep-02.txt\n       Pages     : 17\n       Date      : 03/24/1997\n\nHTTP is an extensible protocol. PEP is an extension mechanism designed to \naddress the tension between private agreement and public specification and \nto accommodate extension of HTTP clients and servers by software \ncomponents.    \n\nThe PEP mechanism is to associate each extension with a URI, and \nuse a few new header fields to carry the extension identifier and \nrelated information from HTTP clients, thru proxies and intermediaries, \nto servers, and back again.                                                   \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-pep-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-pep-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-pep-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-rvsa-v1001.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : HTTP Remote Variant Selection Algorithm -- RVSA/1.0     \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-rvsa-v10-01.txt\n       Pages     : 10\n       Date      : 03/24/1997\n\nHTTP allows web site authors to put multiple versions of the same \ninformation under a single URL.  Transparent content negotiation is a \nmechanism for automatically selecting the best version when the URL is \naccessed.  A remote variant selection algorithm can be used to speed up the\ntransparent negotiation process. This document defines the remote variant \nselection algorithm with the version number 1.0.                           \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-rvsa-v10-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-rvsa-v10-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-rvsa-v10-01.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-ietf-http-hit-metering01.tx",
            "content": "Koen writes:\n    - Some exotic proxy arrangements could lead to there being a\n    `metering sub-graph' instead of a tree.  Have you done the analysis\n    to find out if section 5.6 really covers all subcases?\n\nIt \"covers all subcases\" by passing the buck to the designers/implementors\nof cooperative caches.  It says \"if you agree to meter, then you\nhave to ensure that the external behavior of your set of caches\nis consistent with the intention of this specification.\"  I left\nit vague so that we don't overconstrain these designers/implementors.\nBut they always have the option of not participating in hit-metering.\n\n    - Section 3.3: `then the proxy MUST add \"Cache-control:\n       proxy-maxage=0\" to all responses it sends for the resource.'\n    You probably mean: `then the proxy MUST add \"Cache-control:\n       proxy-maxage=0\" to all responses for which metering or limiting was\n       requested.'\n    because you say earlier on that being metered is a property of a response,\n    not of a resource.\n    \nThis paragraph wasn't very clear.  \"The resource\" in that sentence\nshould have been something like \"the current request\".  I rewrote\nthe paragraph more carefully:\n\n   A proxy that receives the Meter header in a request may ignore it\n   only to the extent that this is consistent with its own duty to the\n   next-hop server.  If the received Meter request header is\n   inconsistent with that duty, or if no Meter request header is received\n   and the response from the next-hop server requests any form of\n   metering or limiting, then the proxy MUST add \"Cache-control:\n   proxy-maxage=0\" to any response it forwards for that request.  (A\n   proxy SHOULD NOT add or change the Expires header or max-age\n   Cache-control directive.)\n\nThat's a much wordier rewrite of the paragraph, but I think it\nleaves much less ambiguity.\n\n    - Section 4.1:\n    \n      `The existing (HTTP/1.0) \"cache-busting\" mechanisms for counting\n       distinct users will certainly overestimate the number of users behind\n       a proxy, since it provides no reliable way to distinguish between a\n       user's initial request and subsequent repeat requests caused by\n       insufficient space in the end-client cache.'\n    \n    Hit metering also `provides no reliable way to distinguish between\n    a user's initial request and subsequent repeat requests caused by\n    insufficient space in the end-client cache' if I'm correct (there\n    is no If-* header if the page dropped out of the cache), so\n    limiting this statement to \"cache-busting\" is a bit misleading.\n    \nActually, this was a bug.  The correct statement is:\n\n   The existing (HTTP/1.0) \"cache-busting\" mechanisms for counting\n   distinct users will certainly overestimate the number of users behind\n   a proxy, since it provides no reliable way to distinguish between a\n   user's initial request and subsequent repeat requests that might have\n   been conditional GETs, had not cache-busting been employed.\n\nI.e., if you cache-bust in HTTP/1.0 (e.g., by setting Expires to\nsomething ancient), then when a client sends a conditional GET\nto a caching proxy, the proxy has to forward this to the origin server.\nAnd when a client sends an unconditional GET to a caching proxy,\npresumably a well-implemented proxy can turn this into a conditional\nGET as well (before forwarding it to the origin server), so I see no\nobvious way for the origin server to distinguish these two classes\nof client events.\n\n      `The \"Cache-control:\n       proxy-maxage=0\" feature of HTTP/1.1 does allow the separation of\n       use-counts and reuse-counts, provided that no HTTP/1.0 proxy caches\n       intervene.'\n    \n    How can they intervene?  Do some 1.0 proxies stip off the\n    If-NoMatch headers when forwarding a request on a stale cache\n    entry, or are you talking about something else?\n    \nNo, this simply means that if the origin server relies on proxy-maxage=0,\nit will be ignored by any HTTP/1.0 proxy cache along the path, and so some\nof the use counts could be lost.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: RFC2109 addition..",
            "content": "Some of the recent discussion on cookie restrictions has\nbeen wildly speculative. We risk writing a spec that puts\nunrealistic constraints on implementations.\n\nI'd like to ask people who are making proposals to qualify\ntheir proposals with actual implementation experience, or\neven some sample pseudo-code for exactly how this is supposed\nto work in a client. The kinds of comments that have\nbeen made recently about how we're just providing challenges\nfor implementors are really worrisome.\n\nAs a working group, we're spending WAY too much energy\non cookies, when there are an enormous number of other\nissues that we *MUST* focus on.\n\nIt may be that for us to make progress we will need to spin\n\"state management\" into a separate working group, so that\nHTTP-WG can focus on the core HTTP protocol.\n\nI've been resisting a push from the area directors that\nwe should shut down HTTP-WG and create new working groups\nto deal with these sub-issues, but I could use some help\nfrom you (HTTP-WG members) to keep focus.\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "On Tue, 25 Mar 1997, Dave Kristol wrote:\n\n> David W. Morris wrote:\n> > \n> > section in the whole document.  Why are we requiring UAs to combine\n> > the two headers?\n> > [...]\n> The complaint from some parties was that the NAME=VALUE part of cookies, in\n> particular, can be (and already is) quite large.  So sending it twice, once\n> in Set-Cookie and once in Set-Cookie2 would incur a lot of network traffic.\n> \n> I agree that sending a completely separate Set-Cookie2 header with its own\n> set of values would be much simpler.  But the network traffic that results\n> was deemed excessive.\n\nI think there are two alternative solutions to mitigate network traffic\nfor that subset of cookie using application which need to update large\npieces of data:\n\n  a.  Use out of band informantion such as the User Agent value to decide\n      which cookie to send\n  b.  Minimize the number of times a cookie is set. Perhaps multiple\n      cookies with only one needing upate.\n  c.  Restructure the application to maintain more state information\n      in the server.\n  d.  Once the first cookie is received by the server, it is only \n      necessary to send one of the two formats.  I would speculate that\n      some percentage of large cookie values are related to shopping\n      basket usage and only get large in the course of multiple\n      interactions. \n\nThe combinatorial rules are difficult and must be implemented to some\ndegree by both the server and the client.  In addition, they are in\nsupport of a transition interval. I think they should be dropped in the\ninterest of simplicity.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Unidentified subject",
            "content": "Here is my recommended revision to sections 4.3.5 and 7.1:\n\n4.3.5  Sending Cookies in Unverifiable Transactions  Users must have\ncontrol over sessions in order to ensure privacy.  (See PRIVACY section\n    \nbelow.)  To simplify implementation and to prevent an additional layer\nof complexity where adequate safeguards exist, however, this document\ndistinguishes between transactions that are verifiable and those that\nare unverifiable.  A transaction is verifiable if the user, or a\nuser-|\ndesignated agent, has the option to review the request-URI prior to\nits|\nuse in the transaction.  A transaction is unverifiable if the user does\nnot have that option.  Unverifiable transactions typically arise when a\nuser agent automatically requests inlined or embedded entities or when\nit resolves redirection (3xx) responses from an origin server.\nTypically the origin transaction, the transaction that the user\ninitiates, is verifiable, and that transaction may directly or\nindirectly induce the user agent to make unverifiable transactions.\n\nWhen it makes an unverifiable transaction, a user agent must enable a\nsession only if a cookie with a domain attribute D was sent or accepted\nin its origin transaction, such that the host name in the Request-URI of\nthe unverifiable transaction domain-matches D.\n\nThis restriction prevents a malicious service author from using\nunverifiable transactions to induce a user agent to start or continue a\nsession with a server in a different domain.  The starting or\ncontinuation of such sessions could be contrary to the privacy\nexpectations of the user, and could also be a security problem.\n\nUser agents may offer configurable options that allow the user agent, or\nany autonomous programs that the user agent executes, to ignore the\nabove rule, so long as these override options default to ``off.''\n\nA user agent may verify that the request-URI comes from a trusted |\ndomain by placing a request to a certificate authority to get the |\ncredentials of the domain.  Those credentials may be persistently |\ncached by the user agent to reduce overhead of repetitive verification\n|\nThe user agent may have the default behavior of sending and accepting|\ncookies from the request-URI domain if the credentials indicate that\nthe|\ndomain is trusted and the credentials have not expired.  The user should\n|\nhave the ability to explicitly override the user agent verification for\na |\nspecific domain.|\n\n...\n\n7.1  User Agent Control\n\n...\n\nA user agent usually begins execution with no remembered state\ninformation.  It should be possible to configure a user agent never to\nsend Cookie headers, in which case it can never sustain state with an\norigin server.  (The user agent would then behave like one that is\nunaware of how to handle Set-Cookie2 response headers.)\n\nThe user agent should allow the user to specify whether state\ninformation\nshould be retained each time the user agent terminates; the default |\nshould be \"yes.\"  If the user chooses to retain state information,|\nit would be restored the next time the user agent runs.|\n\nNOTE: User agents should probably be cautious about using files to store\ncookies long-term.  If a user runs more than one instance of the user\nagent, the cookies could be commingled or otherwise corrupted.\n\n------------------------------------------------------------------------\n----------------------------------\n\nMy objective here is to provide a more explicit definition of how a\ncertificate authority would \"verify\" an otherwise unverifiable\ntransaction.\n\nIn addition, there has been no discussion in this forum on the item in\n7.1 that effectively eliminates persistent cookies.  I believe that the\ncurrent wording could allow a browser to provide an \"invisible default\"\nwhere a user could have a default that cookies are not persistent and\nthe user would never know.  My recommendation is that we make it\nexplicit that the user be prompted and the default should be positive.\nOnce again, this provides consistency with the current implementations\nin terms of default behavior yet provides the user with control.\n>)>)>)>)>)---------------------------------------->>>>>>>\nDaniel Jaye                         djaye@engagetech.com    \nEngage Technologies, Inc.                (508)684-3641 v\n100 Brickstone Square, Andover MA 01810  (508)684-3636 f\n\n\n\n"
        },
        {
            "subject": "Re: Should server beable to say NoCookie, No Show",
            "content": "David W. Morris:\n[...]\n>Symetry would suggest that since we encourage/allow a UA to discard a\n>cookie under the user's discretion, we should have an optional attribute\n>which allows the server to stipulate one of the following:\n>\n>  a.  Dont show the page if the user rejects the cookie\n>  b.  Warn the user that if the cookie isn't accepted, the application\n[...]\n\nServers can already do this under the existing spec.  It is relatively easy\nto build a `cookie-enabledness' detector by using redirection.\n\nSo I see no need to burden the protocol with yet another extension: this can\nbe solved with some clever CGI scripting.\n\nThe same goes for the CommentURL proposal that had been floating around in\nanother thread.  I don't think servers need hard-to-implement protocol\nextensions if they want to tell their users about their privacy policy.  Why\nnot just put a link to your privacy policy on your company home page?\n\n>Dave Morris\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "On CommentURSs ...",
            "content": "On Tue, 25 Mar 1997, Koen Holtman wrote:\n\n> The same goes for the CommentURL proposal that had been floating around in\n> another thread.  I don't think servers need hard-to-implement protocol\n> extensions if they want to tell their users about their privacy policy.  Why\n> not just put a link to your privacy policy on your company home page?\n\nBecause there is no obvious way for a user to obtain the information when\nthey are facing a prompt which asks if they want to accept a cookie.\nThe additional problem is that you force users to figure out what the\ncompany home page is associated with a cookie and then to hunt around thru\nthe company pages to the cookie policy page. I don't see this as placing\nany burden on servers.\n\nIt does place a burden on the UA but it also provides the UA with a crisp\nclean way to provide the user with documentation on the purpose of a\nparticular cookie.\n\nIf we feel that privacy is important enough vis a vis cookies to impose \nrestrictions on cookie sharing, then we need to include a clean mechanism\nby which the user can make an informed decision.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Tue, 25 Mar 1997, Dave Kristol wrote:\n>\n>> David W. Morris wrote:\n>> > \n>> > section in the whole document.  Why are we requiring UAs to combine\n>> > the two headers?\n>> > [...]\n>> The complaint from some parties was that the NAME=VALUE part of cookies,\n>> in particular, can be (and already is) quite large.  So sending it twice,\n>> once in Set-Cookie and once in Set-Cookie2 would incur a lot of network\n>> traffic.\n>\n>> I agree that sending a completely separate Set-Cookie2 header with its\n>> own set of values would be much simpler.  But the network traffic that\n>> results was deemed excessive.\n>\n>I think there are two alternative solutions to mitigate network traffic\n>for that subset of cookie using application which need to update large\n>pieces of data:\n>\n>  a.  Use out of band informantion such as the User Agent value to decide\n>      which cookie to send\n>  b.  Minimize the number of times a cookie is set. Perhaps multiple\n>      cookies with only one needing upate.\n>  c.  Restructure the application to maintain more state information\n>      in the server.\n>  d.  Once the first cookie is received by the server, it is only \n>      necessary to send one of the two formats.  I would speculate that\n>      some percentage of large cookie values are related to shopping\n>      basket usage and only get large in the course of multiple\n>      interactions. \n>\n>The combinatorial rules are difficult and must be implemented to some\n>degree by both the server and the client.  In addition, they are in\n>support of a transition interval. I think they should be dropped in the\n>interest of simplicity.\n\nNote also that though the section on combinatorial rules is\nthe most complex in the draft, it does not apppear sufficient to ensure\nequivalent implementations across UAs and reliable exchanges between\nUAs and servers:\n\nThe examples have alternating Set-Cookie and Set-Cookie2 headers\nwhen the Set-Cookie2 header is adding Version 1 attributes to an otherwise\nVersion 0 cookie name=value and attributes, which would help simplify\ntheir combination, but no such ordering is stated as a requirement in\nthe draft, and such alternation would not be necessary if no Version 1\nattributes other than Version are being use, i.e., if a Set-Cookie2 header\nis simply indicating that the server is Version 1 capable such that\nthe UA should include the $Version, $Path and $Domain attributes in\nits Cookie headers.\n\nIf multiple cookies are included in Set-Cookie headers, and\nadditional Version 1 attributes are provided via Set-Cookie2 headers\nbut for some reason the numbers of cookies associated with the \"old\"\nand \"new\" headers do not appear equal, how much should be discarded\n(everything?)?  If the Set-Cookie2 header is simply indicating\nVerson 1 capability, should it then use a comma-separated list of\nVersion=\"1\" attributes to ensure matching for number of cookies, or\nuse them as comma-serarated \"fillers\" if not all of the cookies in\nsuch a Set-Cookie header have other Version 1 attributes?  Note also\nthat in the Examples, the Set-Cookie headers have commas as separators\nfor name/attribute sets, which is invalid for the \"old\" headers, and\ncould be confusing to readers of the draft. \n\nParticularly since large headers are likely to be the result\nof cookie accumulations, and the UA is likely to have sent a Cookie\nheader so that the server need not send both \"old\" and \"new\" headers\nin such cases, the concern for saving bandwidth during the transition\nperiod via a combinational strategy may indeed be penny wise but pound\nfoolish with respect to reliability and consistency of implementations.\n\nAnother possibility if for Version 1 capable UAs to indicate\nthis in a request header, perhaps only when not sending a Cookie header\nwith Version 1 attributes, which itself indicates this capability.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Should server beable to say NoCookie, No Show",
            "content": "On Tue, 25 Mar 1997, Koen Holtman wrote:\n\n> David W. Morris:\n> [...]\n> >Symetry would suggest that since we encourage/allow a UA to discard a\n> >cookie under the user's discretion, we should have an optional attribute\n> >which allows the server to stipulate one of the following:\n> >\n> >  a.  Dont show the page if the user rejects the cookie\n> >  b.  Warn the user that if the cookie isn't accepted, the application\n> [...]\n> \n> Servers can already do this under the existing spec.  It is relatively easy\n> to build a `cookie-enabledness' detector by using redirection.\n> \n> So I see no need to burden the protocol with yet another extension: this can\n> be solved with some clever CGI scripting.\n\nNo, all such a script can be sure of is prior behavior of the user or UA, \nnot what the user/UA will do with the current cookie. \n\nRobust interoperability should be the design principle and requiring\nclever CGI scripting to converge on robustness makes it very difficult for\napplication developers.\n\nSo what we have is the scenario:\n\n1.  User goes to a page and rejects the cookie\n2.  User fills in the form on the page and submits it\n3.  The server comes back and says ... you must have rejected a cookie,\n    I can't accept your request, but if you accept the cookie on this\n    page, you can continue.\n4.  User didn't get to read the page before rejecting the cookie again\n5.  Now the user sees the page and has to decide how to recover.\n\nThe net result is that users will mostly blindly accept cookies because\nthey fear the difficulty of recovering when they don't. All the effort\nspent on privacy by the WG will be moot.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 Issue:  maxage in responses not define",
            "content": "This is actually an old issue that was raised privately just before\ndraft 07 became an RFC.  As I recall, all of the editors agreed to\nthe change.\n\n[from last August ...]\nAlexei Kosut pointed out to me that the max-age cache-response-directive\nis not really defined in the text of the section on Cache-Control (14.9),\nand that the cross-ref to it in Section 13.2.4 incorrectly points to\nsection 14.10 (and looks to have eaten the close-paren as well).  There\nare probably enough indirect references to the meaning of max-age to make\ninterpretation of it clear, but the following from draft 01 should have\nremained in the draft:\n\n   When the \"max-age\" directive is present in a cached response\n   message, an application must refresh the message if it is older\n   than the age value given (in seconds) at the time of a new request\n   for that resource. The behavior should be equivalent to what would\n   occur if the request had included the max-age directive. If both\n   the new request and the cached message have max-age specified, then\n   the lesser of the two values must be used. A max-age value of zero\n   (0) forces a cache to perform a refresh (If-Modified-Since) on\n   every request. The max-age directive on a response implies that the\n   server believes it to be cachable.\n\nGiven the other additions (must-revalidate) to Cache-control, I would\nrewrite this and append it to the first paragraph of section 14.9.3:\n\n   The expiration time of an entity may be specified by the origin server\n   using the Expires header (see section 14.21). Alternatively, it may be\n   specified using the \"max-age\" directive in a response. When the \"max-age\"\n   directive is present in a cached response, a cache SHOULD consider the\n   response to be stale if it is older than the age value given (in seconds)\n   at the time of a new request for that resource.  The \"max-age\" directive\n   on a response implies that the response is cachable.\n\nand then add as a separate paragraph at the end of section 14.9.3:\n\n   If both the new request and the cached entry include \"max-age\"\n   directives, then the lesser of the two values SHOULD be used for\n   determining the freshness of the cached entry for that request.\n\nand fix the cross-ref in Section 13.2.4.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-1715\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Unidentified subject",
            "content": "On Tue, 25 Mar 1997, Jaye, Dan wrote:\n\n> A user agent may verify that the request-URI comes from a trusted |\n> domain by placing a request to a certificate authority to get the |\n> credentials of the domain.  Those credentials may be persistently |\n> cached by the user agent to reduce overhead of repetitive verification\n\nWithout commenting on the overall concept... this paragraph requires\na certificate authority without providing any protocol or reference as to\nhow this is accomplished.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: draft-holtman-http-safe01.tx",
            "content": "> The Safe response header solves _some_ of the problems we have because we\n> don't have GET-WITH-BODY.  Once we GET-WITH-BODY is deployed though, Safe\n> becomes superfluous.\n> \n> So it is Safe now, GET-WITH-BODY later.\n> \n\nI think this is a dangerous strategy for standards track in the IETF,\nand\nI think we should approach any feature that is designed to be\nsuperfluous later\nwith great suspicion.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Revised HTTPWG agenda for April ",
            "content": "This is a revised agenda for the HTTP working group.\nPlease note that for us to have a focussed discusion,\nit is important that EVERYONE who attends have actually\nreviewed the drafts. There are a LOT of drafts, so\nplease do not put off your reading until later.\n\nNote that some items are intended for \"last call\"; this\nmeans that if you have comments or objections, you should\nstate them NOW (don't wait until the meeting).\n\nKeeping track of all of the items we're processing is\ndifficult. If I've left something out, please let me know.\n\nApril 7, 9:30-11:30\n** 30 minutes Agenda review & WG status overview\n** 90 minutes HTTP/1.1 issue list\nMoving RFC 2068 and 2069 from Proposed to Draft Standard.\nList of issues:\n http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\nand internet drafts linked therein. In particular,\n\nPLEASE READ AND SEND ANY COMMENTS BEFORE THE MEETING ON:\n* ftp://ietf.org/internet-drafts/draft-ietf-http-versions-01.txt\nftp://ietf.org/internet-drafts/draft-mogul-http-revalidate-00.txt\n* ftp://ietf.org/internet-drafts/draft-ietf-http-warning-00.txt\n* ftp://ietf.org/internet-drafts/draft-holtman-http-wildcards-00.txt\nftp://ietf.org/internet-drafts/draft-holtman-safe-01.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-uahint-00.txt\nwhere the *'d items are expected to close.\n\nApril 7, 19:30-22:00\n** 30 minutes Content negotiation\nftp://ietf.org/internet-drafts/draft-ietf-http-negotiation-01.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-rvsa-v10-01.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-uahint-00.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-feature-reg-00.txt\n\n** 30 minutes State Management revision to RFC 2109\nftp://ietf.org/internet-drafts/draft-ietf-http-state-man-mec-00.txt\n** 30 minutes PEP\nftp://ietf.org/internet-drafts/draft-ietf-http-pep-02.txt\n** 10 minutes hit metering\nftp://ietf.org/internet-drafts/draft-mogul-hit-metering-01.txt\n** 20 minutes\nRevisit issues from issue list. \n** 30 minutes WG future: new WGs, charter, close by summer.\n\n\n\n"
        },
        {
            "subject": "Re: On CommentURSs ...",
            "content": "Dave is 100% correct.  \n\nIf you're going to put the complexities of a cookie method in to a browser\nat all, the task of adding support for a comment explaining it should be \ntrivial in comparison.  And the user benefit, I believe, is very high.\nFace it, popular opinion of cookies in the media and in the uninformed\nof the world is that they are purely evil.  Comments will go a long way to\ninforming people of the truth.\n\nJonathan\n\n> On Tue, 25 Mar 1997, Koen Holtman wrote:\n> \n> > The same goes for the CommentURL proposal that had been floating around in\n> > another thread.  I don't think servers need hard-to-implement protocol\n> > extensions if they want to tell their users about their privacy policy.  Why\n> > not just put a link to your privacy policy on your company home page?\n> \n> Because there is no obvious way for a user to obtain the information when\n> they are facing a prompt which asks if they want to accept a cookie.\n> The additional problem is that you force users to figure out what the\n> company home page is associated with a cookie and then to hunt around thru\n> the company pages to the cookie policy page. I don't see this as placing\n> any burden on servers.\n> \n> It does place a burden on the UA but it also provides the UA with a crisp\n> clean way to provide the user with documentation on the purpose of a\n> particular cookie.\n> \n> If we feel that privacy is important enough vis a vis cookies to impose \n> restrictions on cookie sharing, then we need to include a clean mechanism\n> by which the user can make an informed decision.\n> \n> Dave Morris\n> \n\n\n\n"
        },
        {
            "subject": "RE: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "All of these ideas have been considered and rejected because of the load\nthey place on the server. Turning a V1 cookie into a V0/V1 cookie, on\nthe server side, requires nothing more than inserting \"CRLF\nSet-Cookie2:\" in the right place in the set-cookie header. The beauty of\nthe solution is that the server doesn't have to sniff UA headers, which\nis expensive, or worry at all about who they are talking to. By just\ninserting the string they guarantee that everyone will work properly\nwithout the need for any conditional code.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Tuesday, March 25, 1997 12:19 PM\n> To:Dave Kristol\n> Cc:http working group\n> Subject:Re: Section 10.1.1 Combining Set-Cookie and Set-Cookie2\n> \n> \n> \n> On Tue, 25 Mar 1997, Dave Kristol wrote:\n> \n> > David W. Morris wrote:\n> > > \n> > > section in the whole document.  Why are we requiring UAs to\n> combine\n> > > the two headers?\n> > > [...]\n> > The complaint from some parties was that the NAME=VALUE part of\n> cookies, in\n> > particular, can be (and already is) quite large.  So sending it\n> twice, once\n> > in Set-Cookie and once in Set-Cookie2 would incur a lot of network\n> traffic.\n> > \n> > I agree that sending a completely separate Set-Cookie2 header with\n> its own\n> > set of values would be much simpler.  But the network traffic that\n> results\n> > was deemed excessive.\n> \n> I think there are two alternative solutions to mitigate network\n> traffic\n> for that subset of cookie using application which need to update large\n> pieces of data:\n> \n>   a.  Use out of band informantion such as the User Agent value to\n> decide\n>       which cookie to send\n>   b.  Minimize the number of times a cookie is set. Perhaps multiple\n>       cookies with only one needing upate.\n>   c.  Restructure the application to maintain more state information\n>       in the server.\n>   d.  Once the first cookie is received by the server, it is only \n>       necessary to send one of the two formats.  I would speculate\n> that\n>       some percentage of large cookie values are related to shopping\n>       basket usage and only get large in the course of multiple\n>       interactions. \n> \n> The combinatorial rules are difficult and must be implemented to some\n> degree by both the server and the client.  In addition, they are in\n> support of a transition interval. I think they should be dropped in\n> the\n> interest of simplicity.\n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Alan Freier and I (and others) have been worried about the fact that RFC2068\nis silent on much of the rational and implementation detail required for\nsuccessful implementation of connection management policy for HTTP persistent\nconnections in HTTP/1.1.  Alan goaded me into drafting this with him\na few weeks ago, and we now have something presentable for working group\ncomments.  I'm submitting this to the ID drafts editor.\n\nI'll make this available in HTML sometime next week, but you'll have\nto live with plain-text for the moment.\n- Jim Gettys\n=========\n\n\n\n\n\n\nHTTP Working Group                    J. Gettys, Digital Equipment Corporation\nINTERNET-DRAFT                  A. Freier, Netscape Communications Corporation\nExpires September 26, 1997                                      March 26, 1997\n\n\n                       HTTP Connection Management\n\n                   draft-ietf-http-connection-00.txt\n\nStatus of This Memo\n\n   This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time. It is inappropriate to use Internet-Drafts as reference\n   material or to cite them other than as \"work in progress.\"\n\n   To learn the current status of any Internet-Draft, please check the\n   \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n   Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\n   Distribution of this document is unlimited.  Please send comments to\n   the HTTP working group at \"http-wg@cuckoo.hpl.hp.com\".  Discussions\n   of the working group are archived at\n   \"http://www.ics.uci.edu/pub/ietf/http/\".  General discussions about\n   HTTP and the applications which use HTTP should take place on the\n   \"www-talk@w3.org\" mailing list.\n\n1. Abstract\n\n   The HTTP/1.1 specification (RFC 2068) is silent about various details\n   of TCP connection management when using persistent connections.  This\n   document discusses some of the implementation issues discussed during\n   HTTP/1.1's design, and introduces a few new requirements on HTTP/1.1\n   implementations learned from implementation experience, not fully\n   understood when RFC 2068 was issued.  This is an initial draft for\n   working group comment, and we expect further drafts.\n\n\n\n\n\n\n\n\nGettys & Freier                                                 [Page 1]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n\n\n2. Table of Contents\n\n   1. Abstract ....................................................... 1\n   2. Table of Contents .............................................. 2\n   3. Key Words ...................................................... 2\n   4. Connection Management for Large Scale HTTP Systems ............. 2\n   5. Resource Usage (Who is going to pay?) .......................... 2\n   6. Go to the Head of the Line ..................................... 6\n   7. The Race is On ................................................. 7\n   8. Closing Half of the Connection ................................. 8\n   9. Capture Effect ................................................. 9\n   10. Security Considerations ...................................... 10\n   12. References ................................................... 12\n   13. Acknowlegements .............................................. 13\n   14. Authors' Addresses ........................................... 13\n\n3. Key Words\n\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n   document are to be interpreted as described in RFC xxxx. [Bradner]\n\n4. Connection Management for Large Scale HTTP Systems\n\n   Recent development of popular protocols (such as HTTP, LDAP, ...)\n   have demonstrated that the standards and engineering communities have\n   not yet come to grip with the concept of connection management. For\n   instance, HTTP/1.0 [HTTP/1.0] uses a TCP connection for carrying\n   exactly one request/response pair. The simplistic beauty of that\n   model has much less than optimal behavior.\n\n   This document focuses HTTP/1.1 implementations but the conclusions\n   drawn here may be applicable to other protocols as well.\n\n   The HTTP/1.1 Proposed Standard [HTTP/1.1] specification is silent on\n   when, or even if, the connection should be closed (implementation\n   experience was desired before the specification was frozen on this\n   topic). So HTTP has moved from a model that closed the connection\n   after every request/response to one that might never close. Neither\n   of these two extremes deal with \"connection management\" in any\n   workable sense.\n\n\n\n\n\n\n5. Resource Usage (Who is going to pay?)\n\n   The Internet is all about scale: scale of users, scale of servers,\n\n\nGettys & Freier                                                 [Page 2]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   scale over time, scale of traffic. For many of these attributes,\n   clients must be cooperative with servers.\n\n   Clients of a network service are unlikely to communicate with more\n   than a few servers (small number of 10s). Considering the power of\n   desktop machines of today, maintaining that many idle connections\n   does not appear to be overly burdensome, particularly when you\n   consider the client is the active party and doesn't really have to\n   pay attention to the connection unless it is expecting some response.\n\n   Servers will find connections to be critical resources and will be\n   forced to implement some algorithm to shed existing connections to\n   make room for new ones. Since this is an area not treated by the\n   protocol, one might expect a variety of \"interesting\" efforts.\n\n   Maintaining an idle connection is almost entirely a local issue.\n   However, if that local issue is too burdensome, it can easily become\n   a network issue.  A server, being passive, must always have a read\n   pending on any open connection.  Some implementations of the multi-\n   wait mechanisms tend to bog down as the number of connections climbs\n   in to the hundreds, though operating system implementations can scale\n   this into the thousands, tens of thousands, or even beyond. Whether\n   server implementations can also scale to so many simultaneous clients\n   is likely much less clear than if the operating system can\n   theoretically support such use. Implementations might be forced to\n   use fairly bizarre mechanisms, which could lead to server\n   instability, and then perhaps service outages, which are indeed a\n   network issues. And despite any heroic efforts, it will all be to no\n   avail. The number of clients that could hold open a connection will\n   undoubtedly overwhelm even the most robust of servers over time.\n\n   When this happens, the server will of necessity be forced to close\n   connections.  The most often considered algorithm is an LRU. The\n   success of LRU algorithms in other areas of computer engineering is\n   based on locality of reference.  I.e., in this case, if LRU is better\n   than random, then this is because the \"typical\" client's behavior is\n   predictable based on its recent history. Clients that have made\n   requests recently are probably more likely to make them again, than\n   clients which have been idle for a while. While we are not sure we\n   can point to rigorous proof of this principle, we believe it does\n   hold for Web service and client reference patterns are certainly a\n   very powerful \"clue\".\n\n   The client has more information that could be used to drive the\n   process.  For instance, it does not seem to much to expect that a\n   connection be held throughout the loading of a page and all its\n   embedded links. It could further sense user sincerity towards the\n   page by detecting such events as mouse movement, scrolling, etc., as\n   indicators that there is still some interest in pursing the page's\n   content, and therefore the chance of accessing subsequent links.  But\n   if the user has followed a number of links in succession away to a\n   different server, it may be likely that the first connection will not\n\n\nGettys & Freier                                                 [Page 3]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   be used again soon. Whether this is significantly better than LRU is\n   an open question, but it is clear that unlikely to be used\n   connections should be closed, to free the server resouces involved.\n   Server resouces are much more scarce than client resources, and\n   clients should be frugal, if the Web is to have good scaling\n   properties.\n\n   Authoritative knowledge that it is appropriate to close a connection\n   can only come from the user. Unfortunately, that source is not to be\n   trusted.  First, most users don't know what a connection is, and\n   having them indicate it is okay to close it is meaningless. Second, a\n   user that does know what a connection is probably inherently greedy.\n   Such a user would never surrender the attention that a connection to\n   a server implies. Research [Mogul2] does show that most of the\n   benefits of persistent connections are gained if connections can be\n   held open after last use approximately one minute for the HTTP\n   traffic studied; this captures most \"click ahead\" behavior of a\n   user's web browsing.\n\n   For many important services, server resources are critical resources;\n   there are many more clients than services. For example, the AltaVista\n   search service handles (as of this writing) tens of millions of\n   searches per day, for millions of different clients. While it is one\n   of the two or three most popular services on the Internet today, it\n   is clearly small relative to future services built with Internet\n   technology and HTTP. From this perspective, it is clear that clients\n   need to cooperate with servers to enable servers to continue to\n   scale.\n\n   System resources at a server:\n\n       * Server resources (open files, file system buffers, processes,\n         memory for applications, memory for socket buffers for\n         connections currently in use (16-64Kbytes each, data base\n         locks). In BSD derived TCP implementations, socket buffers are\n         only needed on active connections. This usually works because\n         it's seldom the case that there is data queued to/from more\n         than a small fraction of the open connections.\n\n       * PCB (Protocol control blocks, only ~100-140 bytes; even after a\n         connection is closed, you can't free this data structure for a\n         significant amount of time, of order minutes. More severe,\n         however, is that many inferior TCP implementations have had\n         linear or quadratic algorithms relating to the number of PCB's\n         to find PCB's when needed.\n\n   These are organized from most expensive, to least.\n\n   Clients should read data from their TCP implementations aggressively,\n   for several reasons:\n\n       * TCP implementations will delay acknowledgements if socket\n\n\nGettys & Freier                                                 [Page 4]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n         buffers are not emptied. This will lower TCP performance, and\n         cause increased elapsed time for the end user. [Frystyk et.\n         al.] while continuing to consume the server's resources.\n\n       * Servers must be able to free the resources held on behalf of\n         the client as quickly as possible, so that the server can reuse\n         these resources on behalf of others. These are often the\n         largest and scarcest server system resource (processes, open\n         files, file system buffers, data base locks, etc.)\n\n   When HTTP requests complete (and a connection is idle), an open\n   connection still consumes resources some of which are not under the\n   server's control:\n\n       * socket buffers (16-64KB both in the operating system, and often\n         similar amounts in the server process itself)\n\n       * Protocol Control Blocks (.15 KB/PCB's). (??? Any other data\n         structures associated with PCB's?)\n\n   If, for example, an HTTP server had to indefinitely maintain these\n   resources, this memory alone for a million clients (and there are\n   already HTTP services larger than this scale in existence today)\n   using a single connection each would be tens of gigabytes of memory.\n   One of the reasons the Web has succeeded is that servers can, and do\n   delete connections, and require clients to reestablish connections.\n\n   If connections are destroyed too aggressively (HTTP/1.0 is the\n   classic limiting case), other problems ensue.\n\n       * The state of congestion of the network is forgotten [Jacobson].\n         Current TCP implementations maintain congestion information on\n         a per-connection basis, and when the connection is closed, this\n         information is lost. The consequences of this are well known:\n         general Internet congestion, and poor user performance\n\n       * Round trip delays and packets to re-establish the connections.\n         Since most objects in the Web are very small, of order half the\n         packets in the network has been due to just the TCP open and\n         close operation.\n\n       * Slow Start lowers initial throughput of the TCP connection\n\n       * PCB's become a performance bottleneck in some TCP\n         implementations (and cannot be reused for a XXX timeout after\n         the connection has been terminated).  The absolute number of\n         PCBs in the TIME_WAIT state could be much larger than the\n         number in the ESTABLISHED state. Closing connections too\n         quickly can actually consume more memory than closing them\n         slowly, because all PCBs consume memory and idle socket buffers\n         do not.\n\n\n\nGettys & Freier                                                 [Page 5]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   From these two extreme examples, it is obvious that connection\n   management becomes a central issue for both clients and servers.\n\n   Clearly, benefits of persistent connections will be lost if clients\n   open many connections simultaneously. RFC2068 therefore specifies no\n   more than 2 connections from a client to a server should be open at\n   any one time, or 2N connections (where N is the number of clients a\n   proxy is serving) for proxies. Frystyk et. al. have shown that\n   roughly twice the performance of HTTP/1.0 using four to six\n   connections can be reached using HTTP/1.1 over a single TCP\n   connection using HTTP/1.1, even over a LAN, once combined with\n   compression of the HTML documents [Frystyk].\n\n6. Go to the Head of the Line\n\n   The HTTP/1.1 specification requires that proxies use no more than 2N\n   connections, where N is the number of client connections being\n   served.  Mogul has shown that persistent connections are a \"good\n   thing\", and Frystyk et. al. show data that significant (a factor of\n   2-8) savings in number of packets transmitted result by using\n   persistent connections.\n\n   If fewer connections are better, then, why does HTTP/1.1 permit\n   proxies to establish more than the absolute minimum of connections?\n   In the interests of brevity, the HTTP/1.1 specification is silent on\n   some of the motivations for some requirements of the specification.\n   At the time HTTP/1.1 was specified, we realized that if a proxy\n   server attempted to aggregate requests from multiple client\n   connections onto a single TCP connection, a proxy would become\n   vulnerable to the \"head of line\" blocking problem. If Client A, for\n   example, asks for 10 megabytes of data (or asked for a dynamicly\n   generated document of unlimited length), then if a proxy combined\n   that request with requests from another Client B, Client B would\n   never get its request processed. This would be a very \"bad thing\",\n   and so the HTTP/1.1 specification allows proxies to scale up their\n   connection use in proportion to incoming connections. This will also\n   result in proxy servers getting roughly fair allocation of bandwidth\n   from the Internet proportional to the number of clients.\n\n   Since the original HTTP/1.1 design discussions, we realized that\n   there is a second, closely related denial of service security arises\n   if proxies attempt to use the same TCPconnection for multiple\n   clients.  An attacker could note that a particular URL of a server\n   that they wished to attack was either very large, very slow (script\n   based), or never returned data. By making requests for that URL, the\n   attacker could easily block other clients from using that server\n   entirely, due to head of line blocking, so again, simultaneously\n   multiplexing requests from different clients would be very bad, and\n   therefore implementations MUST not attempt such multipexing.\n\n   In other words, head-of-line blocking couples the fates of what\n   should be independent interactions, which allows for both denial-of-\n\n\nGettys & Freier                                                 [Page 6]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   service attacks, and for accidental synchronization.\n\n   Here is another example of head-of-line blocking: imagine clients A\n   and B are connected to proxy P1, which is connected to firewall proxy\n   P2, which is connected to the Internet. If P1 only has one connection\n   to P2, and A attempts to connect (via P1 and P2) to a dead server on\n   the Internet, all of B's operations are blocked until the connection\n   attempt from P2 to the dead server times out. This is not a good\n   situation.\n\n   Note that serial reuse of a TCP connection does not have these\n   considerations: a proxy might first establish a connection to an\n   origin server for Client A, and possibly leave the connection open\n   after Client A finishes and closes\n\n   its connection, and then use the same connection for Client B, and so\n   on.  As in normal clients, such a proxy should close idle\n   connections.\n\n   Future HTTP evolution also dictates that simultaneous multiplexing of\n   clients over a connection should be prohibited. A number of schemes\n   for compactly encoding HTTP rely on associating client state with a\n   connection, which HTTP 1.X does not currently do. If proxies do such\n   multiplexing, then such designs will be much harder to implement.\n\n7. The Race is On\n\n   Deleting a connection without authoritative knowledge that it will\n   not be soon reused is a fundamental race that is part of any timeout\n   mechanism.  Depending on how the decision is made will determine the\n   penalties imposed.\n\n   It is intuitively (and most certainly empirically) less expensive for\n   the active (client) partner to close a connection than the server.\n   This is due in most part to the natural flow of events. For instance,\n   a server closing a connection cannot know that the client might at\n   that very moment be sending a request. The new request and the close\n   message can pass by in the night simply because the server and the\n   client are separated by a network. That type of failure is a network\n   issue. The code of both the client and the server must to be able to\n   deal with such failures, but they should not have to deal with it\n   efficiently. A client closing a connection, on the other hand, will\n   at least be assured that any such race conditions are mostly local\n   issues. The flow will be natural, assuming one treats closing as a\n   natural event. To paraphrase Butler Lampson's 1983 paper on system\n   design, \"The events that happen normally must be efficient.  The\n   exceptional need to make progress.\" [Lampson]\n\n   Having the client closing the connection will decrease the\n   probability of the client having to do automatic connection recovery\n   of a pipeline caused by a premature close on server side. From an\n   client implementation point of view this is advantageous as automatic\n\n\nGettys & Freier                                                 [Page 7]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   connection recovery of a pipeline is significantly more complicated\n   than closing an idle connection.  In HTTP, however, servers are free\n   to close connections any time, and this observation does not help,\n   but may simplify other protocols. It will, however, reduce the number\n   of TCP resets observed, and make the exceptional case exceptional,\n   and avoid a TCP window full of requests being transmitted under some\n   circumstances.\n\n   On the one hand, it is a specific fact about TCP that if the client\n   closes the connection, the server does not have to keep the TIME_WAIT\n   entry lying around. This is goodness.\n\n   On the other hand, if the server has the resources to keep the\n   connection open, then the client shouldn't close it unless there is\n   little chance that the client will use the server again soon, since\n   closing & then reopening adds computational overhead to the server.\n   So allowing the server to take the lead in closing connections does\n   have some benefits.\n\n   A further observation is that congestion state of the network varies\n   with time, so the benefits of the congestion state being maintained\n   by TCP diminishes the longer a connection is idle.\n\n   This discussion also shows that a client should close idle\n   connections before the server does. Currently in the HTTP standard\n   there is no way for a server to provide such a \"hint\" to the client,\n   and there should be a mechanism. This memo solicits other opinions on\n   this topic.\n\n8. Closing Half of the Connection\n\n   In simple request/response protocols (e.g. HTTP/1.0), a server can go\n   ahead and close both recieve and transmit sides of its connection\n   simultaneously whenever it needs to. A pipelined or streaming\n   protocol (e.g. HTTP/1.1) connection, is more complex [Frystyk et.\n   al.], and an implementation which does so can create major problems.\n\n   The scenario is as follows: an HTTP/1.1 client talking to a HTTP/1.1\n   server starts pipelining a batch of requests, for example 15 on an\n   open TCP connection.  The server decides that it will not serve more\n   than 5 requests per connection and closes the TCP connection in both\n   directions after it successfully has served the first five requests.\n   The remaining 10 requests that are already sent from the client will\n   along with client generated TCP ACK packets arrive on a closed port\n   on the server. This \"extra\" data causes the server's TCP to issue a\n   reset which makes the client TCP stack pass the last ACK'ed packet to\n   the client application and discard all other packets. This means that\n   HTTP responses that are either being received or already have been\n   received successfully but haven't been ACK'ed will be dropped by the\n   client TCP. In this situation the client does not have any means of\n   finding out which HTTP messages were successful or even why the\n   server closed the connection. The server may have generated a\n\n\nGettys & Freier                                                 [Page 8]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   \"Connection: Close\" header in the 5th response but the header may\n   have been lost due to the TCP reset. Servers must therefore close\n   each half of the connection independently.\n\n9. Capture Effect\n\n   One of the beauties of the simple single connection for each\n   request/response pair is that it did not favor an existing client\n   over another. In general, this natural rotation made for a fairer\n   offering of the overall service, albeit a bit heavy handed. Our\n   expectation is that when protocols with persistent connections get\n   heavily deployed, that aspect of fairness will not exist. Without\n   some moderately complex history, it might be that only the first 1000\n   clients will ever be able to access a server (providing that your\n   server can handle 1000 connections).\n\n   There needs to be some policy indicating when it is appropriate to\n   close connections. Such a policy should favor having the client be\n   the party to initiate the closure, but must provide some manner in\n   which the server can protect itself from misbehaving clients. Servers\n   can control greedy clients in HTTP/1.1 by use of the 503 (Service\n   Unavailable) response code in concert with the Retry-After response-\n   header field, or by not reading further requests from that client, at\n   the cost of temporarily occupying the connection. As long as the\n   server can afford to keep the connection open, it can delay a \"greedy\n   client\" by simply closing the TCP receive window.  As soon as it\n   drops the connection, it has no way to distinguish this client from\n   any other. Either of these techniques may in fact be preferable to\n   closing the client's connection; the client might just immediately\n   reopen the connection, and you are unlikely to know if it is the same\n   greedy client.\n\n   Implementation complexity will need to be balanced against scheduling\n   overhead.  A number of possible server scheduling algorithms exist,\n   with different costs and benefits. The implementation experience of\n   one of us (jg) with the X Window System [Gettys et. al.] may be of\n   use to those implementing Web server schedulers.\n\n       * Strict round robin scheduling: a operating system select or\n         poll operation is executed for each request processed, and each\n         request is handled in turn (across connections). Since select\n         is executed frequently, new connections get a good chance of\n         service sooner rather than later. Some algorithm must be chosen\n         to avoid capture effect if the server is loaded. This is most\n         fair, and approximates current behavior. The disadvantage is,\n         however, a (relatively expensive) system call / request, which\n         will likely become too expensive as Web servers become\n         carefully optimized after HTTP/1.1 is fully implemented.\n\n       * Modified round robin scheduling: a operating system select or\n         poll operation is executed. Any new connections are\n         established, and for each connection showing data available,\n\n\nGettys & Freier                                                 [Page 9]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n         all available requests are read into buffers for later\n         execution. Then all requests are processed, round robin between\n         buffers. Some algorithm must be chosen to avoid capture effect\n         if the server is loaded. This eliminates the system call per\n         operation.  This is quite efficient, and still reasonably\n         fairly apportions server capabilities.\n\n       * Some servers are likely to be multithreaded, possibly with a\n         thread per connection. These servers will have to have some\n         mechanism to share state so that no client can forever capture\n         a connection on a busy server.\n\n   A final note: indefinite round robin scheduling may not in fact be\n   the most desirable algorithm, due to the timesharing fallacy. If a\n   connection makes progress more slowly than possible, not only will\n   the client (the end user) observe poorer performance, but the\n   connection (and the considerable system overhead each one represents)\n   will be open longer, and more connections and server resources will\n   be required as a result.\n\n   At some point, large, loaded servers will have to choose a connection\n   to close; research [Padmanabhan and Mogul] shows that LRU may be as\n   good as more complex algorithms for choosing which to close.\n\n   Further experimentation with HTTP/1.1 servers will be required to\n   understand the most useful scheduling and connection management\n   algorithms.\n\n10. Security Considerations\n\n   Most HTTP related security considerations are discussed in RFC2068.\n   This document identifies a further security concern: proxy\n   implementations that simultaneously multiplex requests from multiple\n   clients over a TCP connection are vulnerable to a form of denial of\n   service attacks, due to head of line blocking problems, as discussed\n   further above.\n\n   The capture effect discussed above also presents opportunities for\n   denial of service attacks.\n\n11. Requirements on HTTP/1.1 Implementations\n\n   Here are some simple observations and requirements from the above\n   discussion.\n\n       * clients and proxies SHOULD close idle connections.  Most of the\n         benefits of an open connection diminish the longer the\n         connection is idle: the congestion state of the network is a\n         dynamic and changing phenomena [Paxson]. The client, better\n         than a server, knows when it is likely not to revisit a site.\n         By monitoring user activity, a client can make reasonable\n         guesses as to when a connection needs closing.  Research has\n\n\nGettys & Freier                                                [Page 10]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n         shown [Mogul2] shows that most of the benefits of a persistent\n         connection are likely to occur within approximately\n         60 seconds. Further research in this area is needed.  On the\n         client side, define a connection as \"idle\" if it meets at least\n         one of these two criteria:\n\n             * no user-interface input events during the last 60 seconds\n               (parameter value shouldn't be defined too precisely)\n\n             * user has explicitly selected a URL from a different\n               server. Don't switch just because inlined images are from\n               somewhere else! Even in this case, dally for some seconds\n               (e.g., 10) in case the user hits the \"back\" button.\n         On the server side, use a timeout that is adapted based on\n         resource constraints: short timeout during overload, long\n         timeout during underload.  Memory, not CPU cycles, is likely to\n         be the controlling resource in a well-implemented system.\n\n       * servers SHOULD implement some mechanism to avoid the capture\n         effect.\n\n       * proxies MUST use independent TCPconnections to origin or futher\n         proxy servers for different client connections, both to avoid\n         head of line blocking between clients, and to avoid the denial\n         of service attacks that implementations that attempt to\n         multiplex multiple clients over the same connection would be\n         open to.\n\n       * proxies MAY serially reuse connections for multiple clients.\n\n       * servers MUST properly close incoming and outgoing halves of TCP\n         connections independently.\n\n       * clients SHOULD close connections before servers when possible.\n         Currently, HTTP has no \"standard\" way to indicate idle time\n         behavior to clients, though we note that the Apache HTTP/1.1\n         implementation advertizes this information using the Keep-Alive\n         header if Keep-Alive is requested. We note, however, that Keep-\n         Alive is NOT currently part of the HTTP standard, and that the\n         working group may need to consider providing this \"hint\" to\n         clients in the future of the standard by this or other means\n         not currently specified in this initial draft.\n\n\n\n\n\n\n\n\n\n\n\n\nGettys & Freier                                                [Page 11]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n12. References\n\n   [Apache]\n      The Apache Authors, The Apache Web Server is distributed by The\n      Apache Group.\n\n   [Bradner]\n      S. Bradner, \"Keywords for use in RFCs to Indicate Requirement\n      Levels\", RFC XXXX\n\n   [Frystyk]\n      Henrik Frystyk Nielsen, \"The Effect of HTML Compression on a LAN\n      \", W3C. URL:\n      http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/LAN.html\n\n   [Frystyk et. al]\n      Henrik Frystyk Nielsen, Jim Gettys, Anselm Baird-Smith, Eric\n      Prud'hommeaux, W3C, H&aring;kon Wium Lie, Chris Lilley, W3C,\n      \"Network Performance Effects of HTTP/1.1, CSS1, and PNG\". W3C\n      Note, February, 1997. See URL:\n      http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/ for this and\n      other HTTP/1.1 performance information.\n\n   [Gettys et. al.]\n      Gettys, J., P.L. Karlton, and S. McGregor, \" The X Window System,\n      Version 11.'' Software Practice and Experience Volume 20, Issue\n      No. S2, 1990 ISSN 0038-0644.\n\n   [HTTP/1.0]\n      T. Berners-Lee, R. Fielding, H. Frystyk.  \"Informational RFC 1945\n      - Hypertext Transfer Protocol -- HTTP/1.0,\" MIT/LCS, UC Irvine,\n      May 1996\n\n   [HTTP/1.1]\n      R. Fielding, J. Gettys, J.C. Mogul, H. Frystyk, T. Berners-Lee,\n      \"RFC 2068 - Hypertext Transfer Protocol -- HTTP/1.1,\" UC Irvine,\n      Digital Equipment Corporation, MIT\n\n   [Jacobson]\n      Van Jacobson. \"Congestion Avoidance and Control.\" In Proc. SIGCOMM\n      '88 Symposium on Communications Architectures and Protocols, pages\n      314-329. Stanford, CA, August, 1988.\n\n   [Lampson]\n      B. Lampson, \"Hints for Computer System Design\", 9th ACM SOSP, Oct.\n      1983, pp. 33-48.\n\n   [Mogul]\n      Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP.\" In\n      Proc. SIGCOMM '95 Symposium on Communications Architectures and\n      Protocols, pages 299-313. Cambridge, MA, August, 1995.\n\n\n\nGettys & Freier                                                [Page 12]\n\nInternet-Draft         HTTP Connection Management             March 1997\n\n\n   [Mogul2]\n      Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP\".\n      Research Report 95/4, Digital Equipment Corporation Western\n      Research Laboratory, May, 1995. URL:\n      http://www.research.digital.com/wrl/techreports/abstracts/95.4.html\n\n   [Padmanabhan and Mogul]\n      Venkata N. Padmanabhan and Jeffrey C. Mogul. Improving HTTP\n      Latency. In Proc. 2nd International WWW Conf. '94: Mosaic and the\n      Web, pages 995-1005. Chicago, IL, October, 1994. URL:\n      http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html\n\n   [Padmanabhan & Mogul]\n       V.N. Padmanabhan, J. Mogul, \"Improving HTTP Latency\", Computer\n      Networks and ISDN Systems, v.28, pp.  25-35, Dec. 1995. Slightly\n      revised version of paper in Proc. 2nd International WWW Conference\n      '94: Mosaic and the Web, Oct. 1994\n\n   [Paxson]\n\n      Vern Paxson, \"End-to-end Routing Behavior in the Internet\" ACM\n      SIGCOMM '96, August 1996, Stanford, CA.\n\n13. Acknowlegements\n\n   Our thanks to Henrik Frystyk Nielsen for comments on the first draft\n   of this document.\n\n14. Authors' Addresses\n\n   Jim Gettys\n   W3 Consortium\n   MIT Laboratory for Computer Science\n   545 Technology Square\n   Cambridge, MA 02139, USA\n   Fax: +1 (617) 258 8682\n   Email: jg@w3.org\n\n   Alan Freier\n   Netscape Communications Corporation\n   Netscape Communications\n   501 East Middlefield Rd.\n   Mountain View, CA 94043\n   Email: freier@netscape.com\n\n\n\n\n\n\n\n\n\n\nGettys & Freier                                                [Page 13]\n\n\n\n\n"
        },
        {
            "subject": "Re: On CommentURSs ...",
            "content": "Dear Mr. Morris,\n \nAm I allowed to make folders in my favorate places site?\nPlease tell me what I,m doing wrong.\n                             PixieO2@aol.com\n                              kathy Pickett\n\nP.S I,ve read so much on so many applications,every thing I  seem to try \nputs deeper in a hole.\n\n\n\n"
        },
        {
            "subject": "RE: Should server beable to say NoCookie, No Show",
            "content": "This is an absolutely excellent idea. I think it will go a long way\ntowards making content providers feel more comfortable about the cookie\nspec.\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Tuesday, March 25, 1997 1:03 AM\n> To:http working group\n> Subject:Should server beable to say NoCookie, No Show?\n> \n> \n> It seems to me that there are many applications which will break (in\n> the\n> sense of delivering confusing error messages, garbage, etc to the\n> user) if\n> a cookie isn't accepted by the UA and returned with the request\n> resulting\n> from a submit of the page which carried the set-cookie2.\n> \n> Symetry would suggest that since we encourage/allow a UA to discard a\n> cookie under the user's discretion, we should have an optional\n> attribute\n> which allows the server to stipulate one of the following:\n> \n>   a.  Dont show the page if the user rejects the cookie\n>   b.  Warn the user that if the cookie isn't accepted, the application\n>       won't operate correctly (this is almost covered by the\n>       comment/commentURL but its a different of message I think. Like\n>       Windows allows a message box to be one of several types to\n> reflect\n>       the content, the significance of the comment to the user would\n>       vary depending on the damage to the user's experience by\n>       rejecting the cookie.\n> \n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "RE: RFC2109 addition..",
            "content": "This is getting totally out of control. Cookies are supposed to be a\nlight weight state mechanism. If the user doesn't trust the site then\nthey shouldn't accept cookies from the site. If the site wishes to\nexplain their use of cookies they can put a link on their home page.\nCommenting is totally out of control. At the rate we are going, we will\nhave to put in features to negotiation on the syntax and language of the\ncomments.\nYaron\n\n> -----Original Message-----\n> From:Jonathan Stark [SMTP:stark@commerce.net]\n> Sent:Tuesday, March 25, 1997 12:20 AM\n> To:Yaron Goland\n> Cc:dwm@xpasc.com; stark@commerce.net; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: RFC2109 addition...\n> \n> > \n> > Why not just make the comment field syntax into something like\n> > CommentAttribute = \"Comment\" \"=\" (Quoted-String | \"<\" URI \">\")\n> > ?\n> > Yaron\n> \n> I disagree with this approach unless we make it legal to have\n> multiple Comment attributes within the same cookie (And I don't\n> think we really do want to do that.)  As discussed earlier, I think \n> there is a definite value in being able to include a URI AND a \n> comment \"string\".  Some browsers may support one and not the other, \n> some groups may want to dynamically explain their cookies,\n> sending a user-specific comment string with each cookie, or who\n> knows... I don't think we should limit ourselves to having\n> one or the other.\n> \n> (And Dave, I like the quoted notation for the URL's better than\n> <> as well.)\n> \n> Jonathan\n\n\n\n"
        },
        {
            "subject": "Re: draft-holtman-http-safe01.tx",
            "content": "Larry Masinter:\n>\n>> The Safe response header solves _some_ of the problems we have because we\n>> don't have GET-WITH-BODY.  Once we GET-WITH-BODY is deployed though, Safe\n>> becomes superfluous.\n>> \n>> So it is Safe now, GET-WITH-BODY later.\n>> \n>\n>I think this is a dangerous strategy for standards track in the IETF,\n>and\n>I think we should approach any feature that is designed to be\n>superfluous later\n>with great suspicion.\n\nI proposed Safe to quicky solve a deployment problem for RFC2070.  It is a\nbug fix, not a new feature.  If there had been more contacts between the\nhttp-wg and the i18n group before both drafts went to last call, we would\nnot have had this deployment problem in the first place.\n\nPlease see my internet draft for a longer discussion of this issue.\n\n>Larry\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Revised HTTPWG agenda for April ",
            "content": "Larry Masinter:\n>\n>This is a revised agenda for the HTTP working group.\n\nJust correcting a duplicate entry:\n\n>April 7, 19:30-22:00\n>** 30 minutes Content negotiation\n>ftp://ietf.org/internet-drafts/draft-ietf-http-negotiation-01.txt\n>ftp://ietf.org/internet-drafts/draft-ietf-http-rvsa-v10-01.txt\n>ftp://ietf.org/internet-drafts/draft-ietf-http-uahint-00.txt\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>ftp://ietf.org/internet-drafts/draft-ietf-http-feature-reg-00.txt\n\nThe uahint draft is not part of content negotiation.  It is also listed\nunder 1.1 issues.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Age Header Field in HTTP/1.",
            "content": "Network Working Group                                        R. Fielding\nINTERNET-DRAFT                                               U.C. Irvine\n<draft-fielding-http-age-00>\nExpires six months after publication date.                 26 March 1997\n\n\n                    Age Header Field in HTTP/1.1\n\n\nStatus of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its\n   areas, and its working groups.  Note that other groups may also\n   distribute working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time.  It is inappropriate to use Internet-Drafts\n   as reference material or to cite them other than as\n   ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please check\n   the ``1id-abstracts.txt'' listing contained in the Internet-Drafts\n   Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast),\n   or ftp.isi.edu (US West Coast).\n\n   Discussion of this memo should take place within the HTTP working\n   group (http-wg@cuckoo.hpl.hp.com).\n\n\nAbstract\n\n   The \"Age\" response-header field in HTTP/1.1 [RFC 2068] is intended\n   to provide a lower-bound for the estimation of a response message's\n   age (time since generation) by explicitly indicating the amount of\n   time that is known to have passed since the response message was\n   retrieved or revalidated.  However, there has been considerable\n   controversy over when the Age header field should be added to a\n   response.  This document explains the issues and provides a set of\n   proposed changes for the revision of RFC 2068.\n\n\n1. Problem Statement\n\n   HTTP/1.1 [1] defines the Age header field in section 14.6:\n\n      The Age response-header field conveys the sender's estimate of the\n      amount of time since the response (or its revalidation) was generated\n      at the origin server. A cached response is \"fresh\" if its age does\n      not exceed its freshness lifetime. Age values are calculated as\n      specified in section 13.2.3.\n\n           Age = \"Age\" \":\" age-value\n\n           age-value = delta-seconds\n\n      Age values are non-negative decimal integers, representing time in\n      seconds.\n\n      If a cache receives a value larger than the largest positive integer\n      it can represent, or if any of its age calculations overflows, it\n      MUST transmit an Age header with a value of 2147483648 (2^31).\n      HTTP/1.1 caches MUST send an Age header in every response. Caches\n      SHOULD use an arithmetic type of at least 31 bits of range.\n\n   This document focuses on the ambiguous use of the term \"caches\" in\n   the second-to-last line above.  The ambiguity is due to the fact that\n   a cache never sends responses --- only a server application (proxy,\n   gateway, or origin server), which may or may not include a cache, is\n   capable of sending a response.  HTTP/1.1 defines a \"cache\" as\n\n      A program's local store of response messages and the subsystem\n      that controls its message storage, retrieval, and deletion. A\n      cache stores cachable responses in order to reduce the response\n      time and network bandwidth consumption on future, equivalent\n      requests. Any client or server may include a cache, though a cache\n      cannot be used by a server that is acting as a tunnel.\n\n   There are two possible interpretations of\n\n      HTTP/1.1 caches MUST send an Age header in every response.\n\n   Either\n\n      a) An HTTP/1.1 server that includes a cache MUST send an Age\n         header field in every response.\n   or\n      b) An HTTP/1.1 server that includes a cache MUST include an Age\n header field in every response generated from its own cache.\n      \n   The remainder of this document discusses the relative merits of these\n   two options, referred to as \"Option A\" and \"Option B\", concluding in\n   section 5 with a set of proposed changes to remove the ambiguity\n   from future editions of the HTTP/1.1 specification.\n\n2. Review of HTTP/1.1 Response Age Calculation\n\n   HTTP/1.1 defines an algorithm for calculating the age of a response\n   message upon receipt by a cache.  This document does not propose any\n   modification of this algorithm; we describe it here in order to\n   provide the background necessary to understand the later analyses.\n   We only provide a brief summary here -- for a full explanation, see\n   section 13.2.3 (Age Calculations) of RFC 2068 [1].\n\n   Summary of age calculation algorithm, when a cache receives a\n   response:\n\n      /*\n       * age_value\n       *      is the value of Age: header received by the cache with\n       *              this response.\n       * date_value\n       *      is the value of the origin server's Date: header\n       * request_time\n       *      is the (local) time when the cache made the request\n       *              that resulted in this cached response\n       * response_time\n       *      is the (local) time when the cache received the\n       *              response\n       * now\n       *      is the current (local) time\n       */\n      apparent_age           = max(0, response_time - date_value);\n      corrected_received_age = max(apparent_age, age_value);\n      response_delay         = response_time - request_time;\n      corrected_initial_age  = corrected_received_age + response_delay;\n      resident_time          = now - response_time;\n      current_age            = corrected_initial_age + resident_time;\n\n3. Analysis of Option A\n\n   If we were to assume that\n\n      An HTTP/1.1 server that includes a cache MUST send an Age\n      header field in every response.\n\n   is true, then an HTTP/1.1 proxy containing a cache would be required\n   to add an Age header field value to every response that was\n   forwarded, including those that were obtained first-hand from the\n   origin server and never touched by the caching mechanism.  This would\n   directly contradict the paragraph in section 13.2.1 of RFC 2068 that\n   states:\n\n      The expiration mechanism applies only to responses taken from a cache\n      and not to first-hand responses forwarded immediately to the\n      requesting client.\n\n   and also directly contradicts the last paragraph of section 13.2.3 of\n   RFC 2068 that states:\n\n      Note that a client cannot reliably tell that a response is first-\n      hand, but the presence of an Age header indicates that a response\n      is definitely not first-hand.\n\n   If we further assume that the above two paragraphs are in error, then\n   the following example illustrates the effect of the age calculation\n   when a first-hand response passes through a hierarchical system of\n   proxy caches (A, B, C), with each segment taking (a, b, c, d) amount\n   of time to satisfy the request:\n\n     UA  ------->  A  ------->  B  --------->  C  ------->  OS\n            a            b             c             d\n\n   Since the age calculation includes an estimation of clock skew by\n   each recipient (apparent_age), we also have the variables\n\n      skewC  = max(0, response_time(C) - date_value(OS));\n      skewB  = max(0, response_time(B) - date_value(OS));\n      skewA  = max(0, response_time(A) - date_value(OS));\n      skewUA = max(0, response_time(UA) - date_value(OS));\n\n   then the received age will be calculated as follows:\n\n     At  C:  age=max(skewC,0)+d\n         B:  age=max(skewB,max(skewC,0)+d)+(c+d)\n         A:  age=max(skewA,max(skewB,max(skewC,0)+d)+(c+d))+(b+c+d)\n        UA:  age=max(skewUA,max(skewA,max(skewB,max(skewC,0)+d)+(c+d))+\n                                                    (b+c+d))+(a+b+c+d)\n\n   Because the response is first-hand, we know that the real age at UA\n   must be less than (a+b+c+d).  Note that (a+b+c+d) will always be\n   added by UA, so the cumulative overestimation of the age will be\n   at least\n\n      max(skewUA,max(skewA,max(skewB,max(skewC,0)+d)+(c+d))+(b+c+d))\n\n   If we further assume that all clocks are synchronized (the minimum\n   case), then the age at UA will be estimated as\n\n      d+(c+d)+(b+c+d)+(a+b+c+d)\n\n   Note that the above is the minimum overestimation; since the variables\n   skewC, skewB, skewA, and skewUA are all unbounded, the clock skew of\n   each host on the request path adds to the perceived response age of\n   all downstream recipients.  Furthermore, a fast clock on the origin\n   will add to the overestimated age at each hop.\n\n   However, in section 13.2.3 of RFC 2068, we also find\n\n      In essence, the Age value is the sum of the time that the response\n      has been resident in each of the caches along the path from the\n      origin server, plus the amount of time it has been in transit along\n      network paths.\n\n   which in our example would imply an age value of (a+b+c+d).  Thus,\n   Option A would result in an incorrect calculation of the age value,\n   resulting in an overestimation of age in all cases, with the amount\n   of error bounded only by the synchronization of clocks for each and\n   every recipient along the request chain, plus the cumulative\n   overestimation of the network transit time by each recipient.\n\n4. Analysis of Option B\n\n   If we were to assume that\n\n      An HTTP/1.1 server that includes a cache MUST include an Age\n      header field in every response generated from its own cache.\n\n   then an Age header field would not be added to a response that is\n   received first-hand, and thus we would not contradict the sections of\n   RFC 2068 that were quoted above.\n\n   Using the same example as in the analysis of Option A, the\n   calculation of age with Option B would be as follows:\n\n     At  C:  age=max(skewC,0)+d\n         B:  age=max(skewB,0)+(c+d)\n         A:  age=max(skewA,0)+(b+c+d)\n        UA:  age=max(skewUA,0)+(a+b+c+d)\n\n   Note that there is no cumulative overestimation of the age.  The\n   estimated age value at each recipient is only dependent on the skew\n   between the recipient's clock and that of the origin server, plus the\n   total amount of time the request and response has been in transit\n   along the network path.  The minimum estimated age at UA is \n\n      (a+b+c+d)\n\n   which matches the description provided in section 13.2.3 of RFC 2068.\n\n5. Counter-arguments\n\n   The only argument voiced against Option B is that the calculation is\n   \"less conservative\" than Option A, and that being \"conservative\" is\n   better in order to \"reduce as much as possible the probability of\n   inadvertently delivering a stale response to a user.\"\n   \n   If \"conservative\" means \"always overestimates more than the other\n   option\", then the argument is certainly true.  However, if the\n   purpose of Age was to provide an overestimate, then why stop there?\n   Why not add arbitrary amounts of age to forwarded response, just in\n   case?  Why not disable caching entirely?\n\n   The reason is because HTTP caching is good for the Internet as a\n   whole, and in particular for the owners of the network bandwidth that\n   would be used to satisfy a request that has already been cached.\n   Overestimating response age reduces the effectiveness of caching, and\n   thus results in increased network congestion, added bandwidth\n   requirements, and in some cases additional per-packet charges.\n\n   Age was created to compensate for the possibility that clock skew\n   between the origin server (represented by the Date header field) and\n   the user agent (represented by the request time) might result in the\n   age of a response being underestimated.  Age was created so that\n   HTTP/1.1 caches can communicate the actual observed age, thus\n   providing a lower-bound for the age calculation that would be more\n   reliable than simply calculating the difference between the date\n   stamps.\n\n   If Age is to be useful, it must be trusted by cache implementers. \n   In order to be trusted by cache implementers, the value of the Age\n   header field must match its definition: the age of the response as\n   observed by the application that generated the response message.\n\n   Furthermore, Option B is guaranteed to be conservative if all of the\n   applications involved are HTTP/1.1-compliant or if the recipient's\n   clock is equal to or ahead of the origin server clock.  The only case\n   in which Option A *might* result in a better estimation than Option B\n   is where one or more HTTP/1.0 caches are in the request chain AND the\n   response came from one of those HTTP/1.0 caches in which it resided\n   for some time AND the user agent's system clock is running behind the\n   origin server's clock.  In this one case, Option A would compensate\n   for the clock skew if there existed an HTTP/1.1 cache between the\n   user agent and the HTTP/1.0 cache generating the response AND the\n   HTTP/1.1 cache is better-synchronized to the origin server clock.\n\n   The above scenario would require a minimum of two proxies in the\n   chain, with at least one outer proxy being an old HTTP/1.0 cache and\n   at least one inner proxy using HTTP/1.1.  Given that, for many other\n   reasons (described in RFC 2068), an HTTP/1.0 proxy is incapable of\n   reliably caching HTTP messages in a proxy hierarchy, this scenario\n   is not compelling.\n\n   In contrast, Option A would overestimate the age on all HTTP/1.1\n   requests, even when there are no longer any HTTP/1.0 proxies.  It\n   would also make the age calculation dependent on the clock\n   synchronization of every recipient along the request chain, with the\n   possibility for drastic overestimation if any of the recipients has a\n   bad clock.  Option A would therefore make the Age header field value\n   consistently less reliable than simple comparison of date stamps.\n\n5. Conclusion and Proposed Changes\n\n   Option B is the correct interpretation of when the Age header field\n   should be added to an HTTP/1.1 response.  The following changes to\n   RFC 2068 will remove the ambiguity.\n\n   In section 14.6 (Age), replace the sentence\n\n      HTTP/1.1 caches MUST send an Age header in every response.\n\n   with\n\n      An HTTP/1.1 server that includes a cache MUST include an Age\n      header field in every response generated from its own cache.\n\n   In section 13.2.3 (Age Calculations), replace the paragraph\n\n      HTTP/1.1 uses the Age response-header to help convey age information\n      between caches. The Age header value is the sender's estimate of the\n      amount of time since the response was generated at the origin server.\n      In the case of a cached response that has been revalidated with the\n      origin server, the Age value is based on the time of revalidation,\n      not of the original response.\n\n   with\n\n      HTTP/1.1 uses the Age response-header to convey the estimated age\n      of the response message when obtained from a cache.  The Age field\n      value is the cache's estimate of the amount of time since the\n      response was generated or revalidated by the origin server.\n\n   Delete the following paragraph from section 13.2.3:\n\n      Note that this correction is applied at each HTTP/1.1 cache along the\n      path, so that if there is an HTTP/1.0 cache in the path, the correct\n      received age is computed as long as the receiving cache's clock is\n      nearly in sync. We don't need end-to-end clock synchronization\n      (although it is good to have), and there is no explicit clock\n      synchronization step.\n\n   Replace the following two paragraphs from section 13.2.3:\n\n      When a cache sends a response, it must add to the\n      corrected_initial_age the amount of time that the response was\n      resident locally. It must then transmit this total age, using the Age\n      header, to the next recipient cache.\n\n        Note that a client cannot reliably tell that a response is first-\n        hand, but the presence of an Age header indicates that a response\n        is definitely not first-hand. Also, if the Date in a response is\n        earlier than the client's local request time, the response is\n        probably not first-hand (in the absence of serious clock skew).\n\n   with\n\n      The current_age of a cache entry is calculated by adding the amount\n      of time (in seconds) since the cache entry was last validated by\n      the origin server to the corrected_initial_age.  When a response\n      is generated from a cache entry, the server must include a single\n      Age header field in the response with a value equal to the cache\n      entry's current_age.\n\n      The presence of an Age header field in a response implies that a\n      response is not first-hand.  However, the converse is not true,\n      since the lack of an Age header field in a response does not imply\n      that the response is first-hand unless all caches along the\n      request path are compliant with HTTP/1.1 (i.e., older HTTP caches\n      did not implement the Age header field).\n\n6. Security Considerations\n\n   The proposed changes close a potential security problem with HTTP/1.1\n   which would become manifest if a proxy with a slow clock (due to a\n   hardware malfunction, failure to properly set, or caused to be reset\n   by some malevolent agent) adds an Age header field to every response\n   it forwarded, instead of only to those retrieved from its own cache,\n   and thus eliminating the ability of a compliant downstream cache to\n   reduce bandwidth usage on a congested network.  Although this is not\n   a serious concern with today's use of HTTP caching, future use of\n   hierarchical cache networks would be impacted.\n\n7. Acknowledgements\n\n   This document was derived from discussions by the author within the\n   HTTP working group, particularly with Jeffrey C. Mogul.\n\n9. References\n\n   [1] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee.\n       \"Hypertext Transfer Protocol -- HTTP/1.1.\" RFC 2068, U.C. Irvine,\n       DEC, MIT/LCS, January 1997.\n\n9. Author's Address\n\n   Roy T. Fielding\n   Department of Information and Computer Science\n   University of California, Irvine\n   Irvine, CA  92697-3425\n\n   Fax: +1(714)824-1715\n   EMail: fielding@ics.uci.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue:  maxage in responses not define",
            "content": "On Tue, 25 Mar 1997, Roy T. Fielding wrote:\n>    The max-age directive on a response implies that the\n>    server believes it to be cachable.\nDoes it mean that: if there is a max-age in a response, it is cacheable\n(independent of the value of max-age)?\nSo, if a server wants a response to be treated as uncacheable then should\nit return a Expires = Date or max-age set to zero?\n\nYours,\nBertold\n\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n Kolics, Bertold                             E-Mail: bertold@tohotom.vein.hu\n University of Veszprem, Hungary        W3: http://tohotom.vein.hu/~bertold/\n Information Engineering Course\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "David W. Morris wrote:\n\n> The combinatorial rules are difficult and must be implemented to some\n> degree by both the server and the client.  In addition, they are in\n> support of a transition interval. I think they should be dropped in the\n> interest of simplicity.\n\nI didn't think the combination rules (\"combinatorial\" conjures a much worse\nsituation) were that bad, but that's a judgement call.  Either way, I think\nthe burden for interpreting them is wholly in the client.  Yaron Goland (in\na separate message) stated that the burden on the server, to emit\nSet-Cookie2, is minor.  And there's no new burden on the server to\ninterpret Cookie, which remains unchanged.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Jim Gettys writes in draft-ietf-http-connection-00.txt:\n>   This discussion also shows that a client should close idle\n>   connections before the server does. Currently in the HTTP standard\n>   there is no way for a server to provide such a \"hint\" to the client,\n>   and there should be a mechanism. This memo solicits other opinions on\n>   this topic.\n\nBecause resources on the web are typically document-like units comprising\nHTML and several inline entities like images and scripts, it would seem\nuseful for the server to send a close-connection hint to the client when the\nserver has transmitted, and received ACKs for, all of the content in the\ncurrent \"page.\"  Presumably the user will spend enough time perusing the\ndocument that the benefits of maintaining the connection will have\ndiminished to the point of negligibility.  Of course, the client may\n(indeed, should) delay acting on the hint for 10-60 sec in case the user\nimmediately follows an anchor to another document on the server.\n\nA server providing web resources of potentially infinite length (like\nstreaming or pushed content) cannot use the same heuristic to determine when\nto send a close-connection hint, but the same format for the hint could be\nused regardless of the means by which the server decides it's time for the\nclient to take a hint.\n\nWhen multiple entities are sent, each is preceded by a server response code\nlike \"HTTP/1.1 200 OK.\"  Perhaps a final hint entity with a status reponse\nlike \"HTTP/1.1 207 Complete Content\" could be sent.  The entity body could\nbe empty, but it would be useful to include therein related information such\nas how long the server plans to keep the connection idle before closing it.\n\n>       * clients SHOULD close connections before servers when possible.\n>         Currently, HTTP has no \"standard\" way to indicate idle time\n>         behavior to clients, though we note that the Apache HTTP/1.1\n>         implementation advertizes this information using the Keep-Alive\n>         header if Keep-Alive is requested. We note, however, that Keep-\n>         Alive is NOT currently part of the HTTP standard, and that the\n>         working group may need to consider providing this \"hint\" to\n>         clients in the future of the standard by this or other means\n>         not currently specified in this initial draft.\n\nApache's Keep-Alive support defines a maximum number of requests and a\nmaximum time to keep the connection open, but these numbers are arbitrary\nand may not apply to every document on the server.  The KeepAliveTimeout\ncould be sent as part of a hint, but the MaxKeepAliveRequests seems useful\nonly for clients that can't take a hint.\n\n\n--Jeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Jeff de la Beaujardiere wrote:\n> \n> \n> Jim Gettys writes in draft-ietf-http-connection-00.txt:\n> >   This discussion also shows that a client should close idle\n> >   connections before the server does. Currently in the HTTP standard\n> >   there is no way for a server to provide such a \"hint\" to the client,\n> >   and there should be a mechanism. This memo solicits other opinions on\n> >   this topic.\n> \n> Because resources on the web are typically document-like units comprising\n> HTML and several inline entities like images and scripts, it would seem\n> useful for the server to send a close-connection hint to the client when the\n> server has transmitted, and received ACKs for, all of the content in the\n> current \"page.\"  Presumably the user will spend enough time perusing the\n> document that the benefits of maintaining the connection will have\n> diminished to the point of negligibility.  Of course, the client may\n> (indeed, should) delay acting on the hint for 10-60 sec in case the user\n> immediately follows an anchor to another document on the server.\n> \n> A server providing web resources of potentially infinite length (like\n> streaming or pushed content) cannot use the same heuristic to determine when\n> to send a close-connection hint, but the same format for the hint could be\n> used regardless of the means by which the server decides it's time for the\n> client to take a hint.\n> \n> When multiple entities are sent, each is preceded by a server response code\n> like \"HTTP/1.1 200 OK.\"  Perhaps a final hint entity with a status reponse\n> like \"HTTP/1.1 207 Complete Content\" could be sent.  The entity body could\n> be empty, but it would be useful to include therein related information such\n> as how long the server plans to keep the connection idle before closing it.\n\nEh? The server doesn't know when the content is complete - but the client does.\nThis suggestion would seem, therefore, to be both pointless and impractical.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: unverifiable transactions (was: Unidentified subject!",
            "content": "Jaye, Dan wrote:\n> [...]\n> A user agent may verify that the request-URI comes from a trusted       |\n> domain by placing a request to a certificate authority to get the               |\n> credentials of the domain.  Those credentials may be persistently               |\n> cached by the user agent to reduce overhead of repetitive verification\n>         |\n> The user agent may have the default behavior of sending and accepting   |\n> cookies from the request-URI domain if the credentials indicate that\n> the     |\n> domain is trusted and the credentials have not expired.  The user should\n>         |\n> have the ability to explicitly override the user agent verification for\n> a       |\n> specific domain.                                                                |\n> \n\nSorry, I'm not willing to entertain the idea of credentials and trusted parties\nfor this version of the spec.  Maybe it's reasonable for the future, but lacking a\ncomplete description of the credentials infrastructure, your proposal isn't\npractical now.\n\n> ...\n> \n> 7.1  User Agent Control\n> \n> ...\n> \n> A user agent usually begins execution with no remembered state\n> information.  It should be possible to configure a user agent never to\n> send Cookie headers, in which case it can never sustain state with an\n> origin server.  (The user agent would then behave like one that is\n> unaware of how to handle Set-Cookie2 response headers.)\n> \n> The user agent should allow the user to specify whether state\n> information\n> should be retained each time the user agent terminates; the default     |\n> should be \"yes.\"  If the user chooses to retain state information,              |\n> it would be restored the next time the user agent runs.                 |\n> \n> NOTE: User agents should probably be cautious about using files to store\n> cookies long-term.  If a user runs more than one instance of the user\n> agent, the cookies could be commingled or otherwise corrupted.\n> \n> ------------------------------------------------------------------------\n> ----------------------------------\n> \n> My objective here is to provide a more explicit definition of how a\n> certificate authority would \"verify\" an otherwise unverifiable\n> transaction.\n\nPerhaps, but you've only made reference to a now-non-existent facility.\n\n> \n> In addition, there has been no discussion in this forum on the item in\n> 7.1 that effectively eliminates persistent cookies.  I believe that the\n> current wording could allow a browser to provide an \"invisible default\"\n> where a user could have a default that cookies are not persistent and\n> the user would never know.  My recommendation is that we make it\n> explicit that the user be prompted and the default should be positive.\n> Once again, this provides consistency with the current implementations\n> in terms of default behavior yet provides the user with control.\n\nI disagree that 7.1 eliminates persistent cookies.  The current wording is:\n====\nWhen the user agent terminates execution, it should let the user discard\nall state information.  Alternatively, the user agent may ask the user\nwhether state information should be retained; the default should be\n``no.''  If the user chooses to retain state information, it would be\nrestored the next time the user agent runs.\n====\nNote the wording:  \"should *let*\".  And \"may ask\".  The spec says the UA should\ngive the user the choice of what to do.  The default in the *dialog* should be\n``no'' (i.e., did not retain).  I see no invisible control. (:-)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "Foteos Macrides wrote:\n> \n> \"David W. Morris\" <dwm@xpasc.com> wrote:\n> >The combinatorial rules are difficult and must be implemented to some\n> >degree by both the server and the client.  In addition, they are in\n> >support of a transition interval. I think they should be dropped in the\n> >interest of simplicity.\n> \n>         Note also that though the section on combinatorial rules is\n> the most complex in the draft, it does not apppear sufficient to ensure\n> equivalent implementations across UAs and reliable exchanges between\n> UAs and servers:\n> \n>         The examples have alternating Set-Cookie and Set-Cookie2 headers\n> when the Set-Cookie2 header is adding Version 1 attributes to an otherwise\n> Version 0 cookie name=value and attributes, which would help simplify\n> their combination, but no such ordering is stated as a requirement in\n> the draft, and such alternation would not be necessary if no Version 1\n> attributes other than Version are being use, i.e., if a Set-Cookie2 header\n> is simply indicating that the server is Version 1 capable such that\n> the UA should include the $Version, $Path and $Domain attributes in\n> its Cookie headers.\n\nAlternation is not \"necessary\".  I wrote the examples that way to emphasize\ntwo points:\n1) An existing application could easily upgrade to V1 cookies by outputting a\nsecond header after the first.\n2) Like headers might not appear consecutively.\n\nAssuming a client aggregates like headers first, I think the hardest part of the\ncombining rules might be walking the combined Set-Cookie and Set-Cookie2 headers\nin parallel and creating new Set-Cookie2 headers.\n\n> \n>         If multiple cookies are included in Set-Cookie headers, and\n> additional Version 1 attributes are provided via Set-Cookie2 headers\n> but for some reason the numbers of cookies associated with the \"old\"\n> and \"new\" headers do not appear equal, how much should be discarded\n> (everything?)?  If the Set-Cookie2 header is simply indicating\n\nThat's a fair question.  I think the only *safe* thing to do is to discard all,\nsince there's no way to discern where an omission occurred.\n\n> Verson 1 capability, should it then use a comma-separated list of\n> Version=\"1\" attributes to ensure matching for number of cookies, or\n\nIt could, or it could use separate Set-Cookie2 headers, each with a Version=1.\n\n> use them as comma-serarated \"fillers\" if not all of the cookies in\n> such a Set-Cookie header have other Version 1 attributes?  Note also\n\nI guess the answer is yes (which makes the Set-Cookie2 effectively null).  But\nwhy would an application want to send some V0 cookies and some V1?\n\n> that in the Examples, the Set-Cookie headers have commas as separators\n> for name/attribute sets, which is invalid for the \"old\" headers, and\n> could be confusing to readers of the draft.\n\nPlease cite specific section/page/text.  My eyes may deceive me, but I only see\n';' as attribute/value separators.\n\n> \n>         Particularly since large headers are likely to be the result\n> of cookie accumulations, and the UA is likely to have sent a Cookie\n> header so that the server need not send both \"old\" and \"new\" headers\n> in such cases, the concern for saving bandwidth during the transition\n> period via a combinational strategy may indeed be penny wise but pound\n> foolish with respect to reliability and consistency of implementations.\n> \n>         Another possibility if for Version 1 capable UAs to indicate\n> this in a request header, perhaps only when not sending a Cookie header\n> with Version 1 attributes, which itself indicates this capability.\n\nThe down-sides of that approach are:\n1) A UA would always send extra header information to indicate cookie\ncapabilities, even to servers that don't use cookies.\n2) An application would have to look for such a header *or* for V1 cookies to\ndecide what kind of cookies to send.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Hi Jim,\n\nThank you for the new Internet-Draft submission.\n\nJust for your information, I am currently working on your\nInternet-Draft <draft-ietf-http-connection-00.txt>.\n\nI'll probably send an announcement to the entire IETF\nregarding your I-D sometime tomorrow or the next day.\n\nFYI:  The plain-text is perfectly acceptable, thanks...\n      \nKind Regards,\nCynthia \n------------------------------------------------------\nCynthia Clark, IETF Internet-Drafts Administrator\nE-mail Address preference:  <cclark@ietf.org>\n-------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Ben Laurie writes:\n> Eh? The server doesn't know when the content is complete - but the client\n> does.  This suggestion would seem, therefore, to be both pointless and\n> impractical.\n\nThanks for your polite, constructive criticism.  As a new contributor, I\nfind it refreshing.\n\nConsider the case of an application which generates content on request, say\na geographic map encased in an HTML form providing a user interface to\nselect a new map.  In the current scheme of things, the HTML goes out the\ndoor with tags like <img src=\"foo.png\"> and <input type=image\nsrc=\"bar.gif\">.  The client then issues a new GET for each embedded entity.\nA simple persistent connection scheme like Apache's keep-alive will help but\ndoes not address the issues discussed by Gettys and Freier\n[draft-ietf-http-connection-00.txt].\n\nIn a more sophisticated implementation, a close-connection hint would be\nboth useful and practical.  Padmanabhan and Mogul\n[http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html],\nfor example, propose pipelining requests using GETALL or GETLIST methods.\nThe server could easily append such a hint to the response.  That doesn't\nmean the client cannot also test for completeness and decide to close the\nconnection; the request by Gettys was for comments on the server providing a\nhint, not an order, to this effect.\n\nIndeed, even without adding these new methods to HTTP a clever server could\nparse a static HTML document and intelligently manage the client's\npersistent connection to issue a close-connection hint when all the inlined\nentities have been sent.\n\nIn the case of dynamic content like the example above, I envision HTTP and\nHTML enhancements that would permit responding to a single \"GET mumble.cgi\"\nrequest with a single stream of data punctuated by Content-type and\nContent-length headers (like the chunked encoding but containing multiple\nentities).  The enclosing HTML is sent first and contains tokens like <img\nsrc=deferred id=ID>; it is followed by ID-tagged blocks of data to populate\nthe deferred inline items.  I would even like to defer some of the HTML\nusing, say, <div src=deferred id=ID> so that I can respond to the request by\nsending the HTML immediately, inserting into the document the map requested\nwhen it is ready, and then inserting more HTML listing the data values\nreported by the stations shown on the map.  Adding a close-connection hint\nto such an application would be both feasible and desirable.\n\n--Jeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "RE: Should server beable to say NoCookie, No Show",
            "content": "At 02:02 PM 3/25/97 -0800, Yaron Goland wrote:\n>This is an absolutely excellent idea. I think it will go a long way\n>towards making content providers feel more comfortable about the cookie\n>spec.\n\n        I concur. The technique outlined below will make the association\nbetween the user's discarding the cookie and the app's probable failure in\nthe cookie's absence. The message would be similar to an \"Authentication\nFailed\", \"Document Returned No Data\" or \"No Response From Server\",\nindicating a C/S failure. The \"CommentURL\" field also being discussed would\nbe helpful as a supplement.\n\n\n>Yaron\n>\n>> -----Original Message-----\n>> From:David W. Morris [SMTP:dwm@xpasc.com]\n>> Sent:Tuesday, March 25, 1997 1:03 AM\n>> To:http working group\n>> Subject:Should server beable to say NoCookie, No Show?\n>> \n>> \n>> It seems to me that there are many applications which will break (in\n>> the\n>> sense of delivering confusing error messages, garbage, etc to the\n>> user) if\n>> a cookie isn't accepted by the UA and returned with the request\n>> resulting\n>> from a submit of the page which carried the set-cookie2.\n>> \n>> Symetry would suggest that since we encourage/allow a UA to discard a\n>> cookie under the user's discretion, we should have an optional\n>> attribute\n>> which allows the server to stipulate one of the following:\n>> \n>>   a.  Dont show the page if the user rejects the cookie\n>>   b.  Warn the user that if the cookie isn't accepted, the application\n>>       won't operate correctly (this is almost covered by the\n>>       comment/commentURL but its a different of message I think. Like\n>>       Windows allows a message box to be one of several types to\n>> reflect\n>>       the content, the significance of the comment to the user would\n>>       vary depending on the damage to the user's experience by\n>>       rejecting the cookie.\n>> \n>> \n>> Dave Morris\n>\n>\n>\n--\nMatthew Rubenstein                     North American Media Engines\nToronto, Ontario   *finger matt for public key*       (416)943-1010\n\n               They also surf who only stand on waves.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "On Wed, 26 Mar 1997, Jeff de la Beaujardiere wrote:\n\n> \n> Ben Laurie writes:\n> > Eh? The server doesn't know when the content is complete - but the client\n> > does.  This suggestion would seem, therefore, to be both pointless and\n> > impractical.\n> \n> Thanks for your polite, constructive criticism.  As a new contributor, I\n> find it refreshing.\n> \n\nBen may have been brusque, but he is basically correct.  \n\n> Consider the case of an application which generates content on request, say\n> a geographic map encased in an HTML form providing a user interface to\n> select a new map.  In the current scheme of things, the HTML goes out the\n> door with tags like <img src=\"foo.png\"> and <input type=image\n> src=\"bar.gif\">.  The client then issues a new GET for each embedded entity.\n> A simple persistent connection scheme like Apache's keep-alive will help but\n> does not address the issues discussed by Gettys and Freier\n> [draft-ietf-http-connection-00.txt].\n> \n> In a more sophisticated implementation, a close-connection hint would be\n> both useful and practical.  Padmanabhan and Mogul\n> [http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html],\n> for example, propose pipelining requests...\n\nHTTP/1.1 provides for pipelining requests.  When pipelining is used\nthere is no functional difference between <img src=\"foo.png\"> and the \n<img src=deferred id=ID> you suggest.  Gettys and Freier are\ndiscussing something else, for example, letting the client know about\na server's policy concerning maximum allowed idle time or maximum\nnumber of pipelined GETs permitted..\n\n> \n> Indeed, even without adding these new methods to HTTP a clever server could\n> parse a static HTML document and intelligently manage the client's\n> persistent connection to issue a close-connection hint when all the inlined\n> entities have been sent.\n> \n\nI think that Ben Laurie was pointing out that the server can't really\nknow \"when all the inlined entities have been sent\" since some may be\nin the browser's cache, but the client surely knows when they have\nbeen received so it is not clear why it would need a close-connection \nhint.\n\n> In the case of dynamic content like the example above, I envision HTTP and\n> HTML enhancements that would permit responding to a single \"GET mumble.cgi\"\n> request with a single stream of data punctuated by Content-type and\n> Content-length headers (like the chunked encoding but containing multiple\n> entities).  The enclosing HTML is sent first and contains tokens like <img\n> src=deferred id=ID>; it is followed by ID-tagged blocks of data to populate\n> the deferred inline items.  \n\nAs mentioned above this functionality currently exists with HTTP/1.1\nand pipelining.\n\n> I would even like to defer some of the HTML\n> using, say, <div src=deferred id=ID> so that I can respond to the request by\n> sending the HTML immediately, inserting into the document the map requested\n> when it is ready...\n\nThis functionality does not exist, but that is a problem with HTML not\nHTTP..  One of the major failings of HTML is its inability to do \n\"client side includes\" i.e. the equivalent of <img src=\"foo.png\">\nfor text rather than images.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Should server beable to say NoCookie, No Show",
            "content": ">>Someone wrote: \n\n> >> Symetry would suggest that since we encourage/allow a UA to discard a\n> >> cookie under the user's discretion, we should have an optional\n> >> attribute\n> >> which allows the server to stipulate one of the following:\n> >> \n> >>   a.  Dont show the page if the user rejects the cookie\n> >>   b.  Warn the user that if the cookie isn't accepted, the application\n> >>       won't operate correctly (this is almost covered by the\n> >>       comment/commentURL but its a different of message I think. Like\n> >>       Windows allows a message box to be one of several types to\n> >> reflect\n> >>       the content, the significance of the comment to the user would\n> >>       vary depending on the damage to the user's experience by\n> >>       rejecting the cookie.\n\n'b.' seems to be the open to same kind of 'hammer the user until they\nyield' abuse some servers use today against people who refuse cookies.  a. \nis fine. If 'b.' is allowed at all, it should be on a 'show this to me\nONCE per session' basis. It was *deliberate* that the option for silent\ncookie refusals was added to the specs. This appears to be an attempt to\nsubvert the intent of the 'silent refusal' aspect of the spec.\n\n\"Welcome To MegaCorp WebSite\"\n\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\nWant a cookie? No\n\n\"Welcome To MegaCorp WebSite\"\n\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\nWant a cookie (our site may not work right if you don't accept it)? No\n\nArgh.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "RE: Should server beable to say NoCookie, No Show",
            "content": "Not to get back on an old horse, but - what right have we to tell a site\nhow to structure itself? If a site wants to send 10,000 warnings, that\nis the site's business, not ours. Its like T.V., if you don't like the\ncontent, change the channel.\nYaron\n\n> -----Original Message-----\n> From:Benjamin Franz [SMTP:snowhare@netimages.com]\n> Sent:Wednesday, March 26, 1997 11:57 AM\n> To:http working group\n> Subject:RE: Should server beable to say NoCookie, No Show?\n> \n> >>Someone wrote: \n> \n> > >> Symetry would suggest that since we encourage/allow a UA to\n> discard a\n> > >> cookie under the user's discretion, we should have an optional\n> > >> attribute\n> > >> which allows the server to stipulate one of the following:\n> > >> \n> > >>   a.  Dont show the page if the user rejects the cookie\n> > >>   b.  Warn the user that if the cookie isn't accepted, the\n> application\n> > >>       won't operate correctly (this is almost covered by the\n> > >>       comment/commentURL but its a different of message I think.\n> Like\n> > >>       Windows allows a message box to be one of several types to\n> > >> reflect\n> > >>       the content, the significance of the comment to the user\n> would\n> > >>       vary depending on the damage to the user's experience by\n> > >>       rejecting the cookie.\n> \n> 'b.' seems to be the open to same kind of 'hammer the user until they\n> yield' abuse some servers use today against people who refuse cookies.\n> a. \n> is fine. If 'b.' is allowed at all, it should be on a 'show this to me\n> ONCE per session' basis. It was *deliberate* that the option for\n> silent\n> cookie refusals was added to the specs. This appears to be an attempt\n> to\n> subvert the intent of the 'silent refusal' aspect of the spec.\n> \n> \"Welcome To MegaCorp WebSite\"\n> \n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> Want a cookie? No\n> \n> \"Welcome To MegaCorp WebSite\"\n> \n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> Want a cookie (our site may not work right if you don't accept it)? No\n> \n> Argh.\n> \n> -- \n> Benjamin Franz\n> \n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "John Franks writes:\n> Gettys and Freier are discussing something else, for example, letting the\n> client know about a server's policy concerning maximum allowed idle time\n> or maximum number of pipelined GETs permitted..\n\nThanks for setting me straight on this.\n\n> I think that Ben Laurie was pointing out that the server can't really know\n> \"when all the inlined entities have been sent\" since some may be in the\n> browser's cache, but the client surely knows when they have been received\n> so it is not clear why it would need a close-connection hint.\n\nI assumed the hint was meant as a request to the client to close the\nconnection and go away.  I agree that caching could be a problem.\n\n--Jeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "pipelining vs. deferred conten",
            "content": "John Franks writes:\n> When pipelining is used there is no functional difference between <img\n> src=\"foo.png\"> and the <img src=deferred id=ID> you suggest.\n\nI believe there is a difference.  Pipelining means the client issues\n\"multiple requests without waiting for each response\" [RFC 2069] and the\nserver sends them back in order on the same connection.  This is useful in\nretrieving the set of inline images in a static document, for example.\n\nI'm suggesting something else: the client has requested an object that will\ntake a non-zero amount of time to generate, such as a map, a visualization,\nor the result of a database query.  The generator can either wait until the\nimage is done and send HTML referencing it, or immediately send HTML\nreferencing a script which generates image data, as in <img\nsrc=\"image_making_script.cgi\">.  In the former case, the client waits while\nthe server grinds away and eventually spits out HTML and the image it\nreferences; in the latter, the client gets something to display while the\nserver is working.  This lets some of the response be transmitted during\notherwise idle time and is friendlier from the user-interface point of view.\nThe big disadvantage of the method is that it requires a second script to be\ninvoked on the server, which is why I avoid it.\n\nMy proposal is that the server say, in effect, \"Here's your page; reserve\nspace for an image and wait just a sec until I give you something to fill\nthe space.\"  The same CGI generates all the content, providing sub-headers\nas appropriate to demarcate each piece.  The client issues a single GET or\nPOST instead of pipelining multiple requests.\n\nI don't believe pipelining handles this situation.  Hacks like\nimage-emitting CGIs and <frame>s that reference scripts can create the\ndesired effect but at a cost to the network and the server.\n\n--Jeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "On Wed, 26 Mar 1997, Jeff de la Beaujardiere wrote:\n\n> \n> Jim Gettys writes in draft-ietf-http-connection-00.txt:\n> >   This discussion also shows that a client should close idle\n> >   connections before the server does. Currently in the HTTP standard\n> >   there is no way for a server to provide such a \"hint\" to the client,\n> >   and there should be a mechanism. This memo solicits other opinions on\n> >   this topic.\n> \n> Because resources on the web are typically document-like units comprising\n> HTML and several inline entities like images and scripts, it would seem\n> useful for the server to send a close-connection hint to the client when the\n> server has transmitted, and received ACKs for, all of the content in the\n> current \"page.\"  Presumably the user will spend enough time perusing the\n> document that the benefits of maintaining the connection will have\n> diminished to the point of negligibility.  Of course, the client may\n> (indeed, should) delay acting on the hint for 10-60 sec in case the user\n> immediately follows an anchor to another document on the server.\n\nThe server generally won't know when the client is finished. The client\nmay obtain some resources from caches, for example.  Futhermore, for the\nserver to know it would have to parse the content. A to be heavily\ndiscouraged requirement. The client knows when it has the full page worth\nof stuff.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Section 10.1.1 Combining Set-Cookie and SetCookie",
            "content": "On Wed, 26 Mar 1997, Dave Kristol wrote:\n\n> Foteos Macrides wrote:\n> >         Another possibility if for Version 1 capable UAs to indicate\n> > this in a request header, perhaps only when not sending a Cookie header\n> > with Version 1 attributes, which itself indicates this capability.\n> \n> The down-sides of that approach are:\n> 1) A UA would always send extra header information to indicate cookie\n> capabilities, even to servers that don't use cookies.\n> 2) An application would have to look for such a header *or* for V1 cookies to\n> decide what kind of cookies to send.\n\nI don't see why it couldn't be:\n\n    Cookie: $version=1\n\nSo there would only be 1 header to examine. But I don't think this is\nnecessary. As I said before, I think this is needless complexity for a\ntransition plan to save an arguably large/small amount of network traffic.\n\nThe server (read application author) burden in terms of the potential for\nsome client not correctly merging the two headers is quite high.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Should server beable to say NoCookie, No Show",
            "content": "On Wed, 26 Mar 1997, Benjamin Franz wrote:\n\n> >>Someone wrote: \n> \n> > >> Symetry would suggest that since we encourage/allow a UA to discard a\n> > >> cookie under the user's discretion, we should have an optional\n> > >> attribute\n> > >> which allows the server to stipulate one of the following:\n> > >> \n> > >>   a.  Dont show the page if the user rejects the cookie\n> > >>   b.  Warn the user that if the cookie isn't accepted, the application\n> > >>       won't operate correctly (this is almost covered by the\n> > >>       comment/commentURL but its a different of message I think. Like\n> > >>       Windows allows a message box to be one of several types to\n> > >> reflect\n> > >>       the content, the significance of the comment to the user would\n> > >>       vary depending on the damage to the user's experience by\n> > >>       rejecting the cookie.\n> \n> 'b.' seems to be the open to same kind of 'hammer the user until they\n> yield' abuse some servers use today against people who refuse cookies.  a. \n> is fine. If 'b.' is allowed at all, it should be on a 'show this to me\n> ONCE per session' basis. It was *deliberate* that the option for silent\n> cookie refusals was added to the specs. This appears to be an attempt to\n> subvert the intent of the 'silent refusal' aspect of the spec.\n\nNope .... well I wrote the question/proposal and there is no such intent\n.... what the intent is is to build more robust www applications. I have\nno objection when/if this written is spec. language that how the warning\nis delivered would be a UI issue. The current spec. is long on telling the\nclient that they must give the user flexibility but really short on\nhelping the user understand the implications of that flexibility.\n\nAnd frankly, I consider clients and servers peers. If the client user is\nsilly enough to keep going back to a site and refusing the cookie then I\ndon't see why they shouldn't keep getting warned. That is the cost of\nusing the server. \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "> My proposal is that the server say, in effect, \"Here's your page; reserve\n> space for an image and wait just a sec until I give you something to fill\n> the space.\"\n\nThis can already be accomplished by providing the image dimentions in\nthe IMG tag.  The only time the client halts layout is when it doesn't\nknow the image dimentions, that is, they were neither in the IMG tag,\nnor yet received as first bytes of the image itself.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "delabeau@iniki.gsfc.nasa.gov writes:\n    My proposal is that the server say, in effect, \"Here's your page;\n    reserve space for an image and wait just a sec until I give you\n    something to fill the space.\"\n\nI agree, this is an optimization that one might hope to make.\n    \nAri Luotonen <luotonen@netscape.com>\n    This can already be accomplished by providing the image dimentions in\n    the IMG tag.  The only time the client halts layout is when it doesn't\n    know the image dimentions, that is, they were neither in the IMG tag,\n    nor yet received as first bytes of the image itself.\n\nActually, this doesn't entirely solve the problem.  If you have\nan HTML file with, say, 41 embedded images, and image #3 will\ntake 10 seconds to precompute at the server, you would really\nlike to be able to use the network to load images #4 - #41\nwhile that is happening.  The IMG tag allows the browser to render\nthe bounding box, but it doesn't solve the retrieval serialization.\n\nIn the current world, you would probably do that by using another\nTCP connection, and that might be an acceptable approach, if it's\nonly a few \"deferred\" images per page.  But it does force you\nto fire up another TCP connection, and if you have already issued\na pipeline full of requests for images #4 - #41 behind the request\nfor image #3, then there are some messy issues to worry about\n(i.e., how to quash the redundant requests without a lot of waste).\n\nAnother approach that would probably work with HTTP/1.1, but would\nnot be compatible with HTTP/1.0 clients, would be for the server\nto respond to the initial request for the \"slow\" image by sending\nHTTP/1.1 503 Service Unavailable\nRetry-after: 10\n(using the example of a 10-second delay).  This would allow the\nserver to continue processing the pipeline of requests for\nimages images #4 - #41, because it would not have to stall the\nresponse for image #3.  At the same time (assuming some parallelism\nbetween image generation and HTTP/TCP processing at the server),\nit could go ahead and generate image #3, but save the result in\na temporary cache.  So, after 10 seconds, when the client retries\nthe request for image #3, it now is available immediately.\n\nAs I said, this isn't interoperable with HTTP/1.0, or even with\nan HTTP/1.1 client that doesn't support Retry-After, so it's not\nclear that this is actually a solution.\n\nI think this is a problem worthy of some more thought, but I\ndoubt we'll solve it before we need to progress HTTP/1.1 to\nDraft Standard.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "At 01:11 PM 26/03/97 -0800, David W. Morris wrote:\n>\n>\n>On Wed, 26 Mar 1997, Jeff de la Beaujardiere wrote:\n>\n>> Because resources on the web are typically document-like units comprising\n>> HTML and several inline entities like images and scripts, it would seem\n>> useful for the server to send a close-connection hint to the client when\nthe\n>> server has transmitted, and received ACKs for, all of the content in the\n>> current \"page.\"  Presumably the user will spend enough time perusing the\n>> document that the benefits of maintaining the connection will have\n>> diminished to the point of negligibility.  Of course, the client may\n>> (indeed, should) delay acting on the hint for 10-60 sec in case the user\n>> immediately follows an anchor to another document on the server.\n>\n>The server generally won't know when the client is finished. The client\n>may obtain some resources from caches, for example.  Futhermore, for the\n>server to know it would have to parse the content. A to be heavily\n>discouraged requirement. The client knows when it has the full page worth\n>of stuff.\n\n  What the client knows may well depend on what the client is. If the\nclient is itself a large caching server, it does not know how long it will\nbe before the end user on whose behalf it is retrieving information will\nask for another object from the same server. An entirely different\nquestion, to which it does not know the answer either, is how long it will\nbe before any of its clients ask it for material from the same server.\n  However, even in the case where the client does not know these things, it\nis quite conceivable that such a caching server will make use of its own\nalgorithm, based on things like time since last request for an object from\nthe server and round-trip time to the server, to work out how long it\nwishes to retain an existing TCP connection, and that it will choose to\nignore any hints it may receive. And the information on which that decision\nis based is very unlikely to be available to the server.\n  \n- Donald Neal\n  \n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": ">>    The max-age directive on a response implies that the\n>>    server believes it to be cachable.\n>Does it mean that: if there is a max-age in a response, it is cacheable\n>(independent of the value of max-age)?\n\nYes, but not independent of other parts of Cache-Control (like private).\n\n>So, if a server wants a response to be treated as uncacheable then should\n>it return a Expires = Date or max-age set to zero?\n\nIt should return Expires = Date and Cache-Control: no-cache\nThe first is for HTTP/1.0 caches and the second id for HTTP/1.1 caches.\nNote that max-age=0 means \"cachable, but treat as stale on future requests\".\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": "After some good comments from Jeff, I am changing my proposed change.\n\nThe first three paragraphs of section 14.9.3 of RFC 2068:\n\n   The expiration time of an entity may be specified by the origin\n   server using the Expires header (see section 14.21). Alternatively,\n   it may be specified using the max-age directive in a response.\n\n   If a response includes both an Expires header and a max-age\n   directive, the max-age directive overrides the Expires header, even\n   if the Expires header is more restrictive. This rule allows an origin\n   server to provide, for a given response, a longer expiration time to\n   an HTTP/1.1 (or later) cache than to an HTTP/1.0 cache. This may be\n   useful if certain HTTP/1.0 caches improperly calculate ages or\n   expiration times, perhaps due to desynchronized clocks.\n\n     Note: most older caches, not compliant with this specification, do\n     not implement any Cache-Control directives.  An origin server\n     wishing to use a Cache-Control directive that restricts, but does\n     not prevent, caching by an HTTP/1.1-compliant cache may exploit the\n     requirement that the max-age directive overrides the Expires\n     header, and the fact that non-HTTP/1.1-compliant caches do not\n     observe the max-age directive.\n\nshould be replaced with\n\n   The expiration time of an entity may be specified by the origin server\n   using the Expires header (see section 14.21). Alternatively, it may be\n   specified using the \"max-age\" directive in a response. When the \"max-age\"\n   directive is present in a cached response, the response is stale if its\n   current age is greater than the age value given (in seconds)\n   at the time of a new request for that resource.  The \"max-age\" directive\n   on a response implies that the response is cachable (i.e., \"public\")\n   unless some other, more restrictive cache directive is also present.\n\n   If a response includes both an Expires header and a max-age\n   directive, the max-age directive overrides the Expires header, even\n   if the Expires header is more restrictive. This rule allows an origin\n   server to provide, for a given response, a longer expiration time to\n   an HTTP/1.1 (or later) cache than to an HTTP/1.0 cache. This may be\n   useful if certain HTTP/1.0 caches improperly calculate ages or\n   expiration times, perhaps due to desynchronized clocks.\n\n   Many HTTP/1.0 cache implementations will treat an Expires value that\n   is less than or equal to the response Date value as being equivalent\n   to the Cache-Control response directive \"no-cache\".  If an HTTP/1.1\n   cache receives such a response, and the response does not include a\n   Cache-Control header field, it SHOULD consider the response to be\n   non-cachable in order to retain compatibility with HTTP/1.0 servers.\n\n     Note: An origin server wishing to use a relatively new HTTP cache\n     control feature, such as the \"private\" directive, on a network\n     that includes older caches which do not understand that feature,\n     will need to combine the new feature with an old Expires value\n     in order to prevent the older caches from caching the response.\n\nand then add as a separate paragraph at the end of section 14.9.3:\n\n   If both the new request and the cached entry include \"max-age\"\n   directives, then the lesser of the two values is used for\n   determining the freshness of the cached entry for that request.\n\nand fix the cross-ref in Section 13.2.4.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-1715\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "ID: Proxy autoconfi",
            "content": "Please have a look at this..\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n\n\n\n\nNetwork Working Group                                         Josh Cohen\nInternet-Draft                                   Netscape Communications\nExpires in 6 Months                                        24 March 1997\n\n\n                       Discovering proxy servers\n                   <draft-cohen-proxy-srvloc-00.txt>\n\nStatus of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups.  Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time.  It is inappropriate to use Internet- Drafts as reference\n   material or to cite them other than as ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please check the\n   ``1id-abstracts.txt'' listing contained in the Internet- Drafts\n   Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\nAbstract\n\n   This document describes a method for automating 'out of the box'\n   configuration of WWW clients via the Server Location Protocol.\n\n   Presently, the most popular method of automatic browser configuration\n   is via a Proxy Autoconfig file, which is delivered upon request to\n   the browser.  Unfortunately, the URL of this PAC file must still be\n   specified by the user or administrator.\n\nIntroduction\n\n   The Server Location Protocol working group has defined a number of\n   Internet Drafts on how to locate and advertise services on IP\n   networks.  This draft suggests using the method described in [SRVURL]\n   and [SVRADV] to advertise the URL of the autoconfig file.\n\n   If has been suggested that a client should use DHCP in [KWAN] to\n   determine this URL, but presently, there is no cross platform way to\n   reference DHCP configuration options.  Because of this, the author\n   suggests using the Server Location protocol.\n\n\n\n\nJ. Cohen                                                        [Page 1]\n\n\n\n\n\nINTERNET-DRAFT         Discovering proxy servers           24 March 1997\n\n\n   By following the recommendations in this draft, an administrator can\n   expect that a user will procure a conforming web client, install it\n   on their computer and have the product automagically configure itself\n   for the appropriate proxy policies based on the clients domain.\n\nAdvertising the URL\n\n   As specified in [SRVADV] and [SRVRR], service: URLs can be advertised\n   via DNS.  The method for advertising these resources in DNS is based\n   on TXT RRs and SRV RRs.  Presently, SRV records are not widely\n   supported, so in the interim, [TXT] recommends using TXT records\n   instead.\n\n   The service URL for a PAC file is a service URL defined in [SRVURL].\n   The general format of a service: URL is:\n\n   service: service-location\n\n   The explicit format of the URL in DNS TXT records is defined in\n   [FIND].  The general format is:\n\n   <service> IN TXT \"service:<srvtag>-<url>\" [preference] [protocol\n\n   According to [FIND], srvtag would be 'yp' short for yellow pages.\n   This is the generic tag for services, as opposed to 'wp' or white\n   pages for people.\n\n   Service should be 'w3-ns-pac' to specify the type of configuration we\n   are looking for.\n\n   An example for a proxy called proxy1.foo.com on port 8080 whose PAC\n   file is /proxy.pac is:\n\n   w3-ns-pac IN TXT \"service:yp-http://proxy1.foo.com:8080/proxy.pac\"\n\nDiscovering the PAC URL\n\n   A client should attempt to discover the PAC URL at least as often as\n   upon each startup.  To do so it shall query DNS for TXT RRs with the\n   identifier w3-ns-pac.\n\n   It should start with the most specific domain, and the query, with a\n   more general domain, until it finds an response.\n\n   For example, for a client whose name is pc1.test.corp.foo.com, the\n   client should query, in order:\n\n   w3-ns-pac.text.corp.foo.com.\n\n\n\nJ. Cohen                                                        [Page 2]\n\n\n\n\n\nINTERNET-DRAFT         Discovering proxy servers           24 March 1997\n\n\n   w3-ns-pac.corp.foo.com.\n   w3-ns-pac.foo.com.\n\n   Note the final '.' to speed unsucessful queries.\n\nThe 'w3-ns-pac' specifier\n\n   The specifier should is unique and it reflects:\n\n   the functional area:      the world wide web\n\n   the origination of:       ns (Netscape Communications)\n   this resource format\n\n   the type of resource:     PAC (Proxy Auto Config)\n\n   By using unique identifiers, administrators can list other\n   resources for other types of PAC files, should they use\n   a browser which has its own format.\n\nThe PAC file format\n\n   This format has generally become a defacto standard, but is\n   not currently defined in any standards body.  Information can be\n   found at: [PAC]\n\nSecurity Considerations\n\n   Since this discovery method depends on DNS, it is subject to the\n   same concerns and restrictions as the Domain Name System with\n   respect to security.\n\n   It is presumed that this functionality will be of most use in an\n   intranet deployment where the DNS servers, and proxy servers are\n   maintained by the same organization.  Therefore, a certain degree\n   of trust is assumed.\n\nReferences\n\n   [HTTP]     R. Fielding, J. Gettys, J.C. Mogul, H. Frystyk, T. Berners-Lee\n              \"Hypertext Transfer Protocol -- HTTP/1.1\", RFC 2068, Jan 1997\n\n   [KWAN]     S. Kwan, \"DHCP Option for Proxy Client Configuration File\",\n              draft-kwan-proxy-client-conf-00.txt,  March 1997\n\n   [SRVRR]      A. Gulbrandsen, P. Vixie, \"A DNS RR for specifying\n              the location of services (DNS SRV),\" RFC 2052, October 1996.\n\n\n\n\nJ. Cohen                                                        [Page 3]\n\n\n\n\n\nINTERNET-DRAFT         Discovering proxy servers           24 March 1997\n\n\n   [SRVURL]   E. Guttman, \"The service: URL Scheme\",\n              <draft-ietf-svrloc-service-scheme-00.txt>, November 1996.\n\n   [SVRLOC]   C. Perkins, S. Kaplan, J. Veizades, E. Guttman,\n              \"Service Location Protocol\", draft-ietf-svrloc-protocol-15.txt,\n              January 1997\n\n   [SVRADV]   R. Moats, M. Hamilton, \"Advertising Services\",\n              draft-ietf-svrloc-advertise-00.txt  February 1997\n\n   [FIND]     R. Moats   M. Hamilton, \"Finding Stuff (How to discover)\",\n               draft-ietf-svrloc-discovery-00.txt, February 1997\n\n   [PAC]      A. Luotonen, \"Netscape Proxy Autoconfiguration\"\n              http://home.netscape.com/eng/mozilla/2.0/relnotes/demo/proxy-live.html\n              March 1996\n\nAuthor's Address\n\n   Josh Cohen\n   Netscape Communications Corporation\n   501 E. Middlefield Rd\n   Mountain View, CA 94043\n\n   Phone (415) 937-4157\n   EMail: josh@netscape.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen                                                        [Page 4]\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Jeff de la Beaujardiere wrote:\n> \n> \n> Ben Laurie writes:\n> > Eh? The server doesn't know when the content is complete - but the client\n> > does.  This suggestion would seem, therefore, to be both pointless and\n> > impractical.\n> \n> Thanks for your polite, constructive criticism.  As a new contributor, I\n> find it refreshing.\n\nIt was not my intention to be rude, just brief. Please accept my apologies.\n\n> \n> Consider the case of an application which generates content on request, say\n> a geographic map encased in an HTML form providing a user interface to\n> select a new map.  In the current scheme of things, the HTML goes out the\n> door with tags like <img src=\"foo.png\"> and <input type=image\n> src=\"bar.gif\">.  The client then issues a new GET for each embedded entity.\n> A simple persistent connection scheme like Apache's keep-alive will help but\n> does not address the issues discussed by Gettys and Freier\n> [draft-ietf-http-connection-00.txt].\n> \n> In a more sophisticated implementation, a close-connection hint would be\n> both useful and practical.  Padmanabhan and Mogul\n> [http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html],\n> for example, propose pipelining requests using GETALL or GETLIST methods.\n> The server could easily append such a hint to the response.  That doesn't\n> mean the client cannot also test for completeness and decide to close the\n> connection; the request by Gettys was for comments on the server providing a\n> hint, not an order, to this effect.\n> \n> Indeed, even without adding these new methods to HTTP a clever server could\n> parse a static HTML document and intelligently manage the client's\n> persistent connection to issue a close-connection hint when all the inlined\n> entities have been sent.\n> \n> In the case of dynamic content like the example above, I envision HTTP and\n> HTML enhancements that would permit responding to a single \"GET mumble.cgi\"\n> request with a single stream of data punctuated by Content-type and\n> Content-length headers (like the chunked encoding but containing multiple\n> entities).  The enclosing HTML is sent first and contains tokens like <img\n> src=deferred id=ID>; it is followed by ID-tagged blocks of data to populate\n> the deferred inline items.  I would even like to defer some of the HTML\n> using, say, <div src=deferred id=ID> so that I can respond to the request by\n> sending the HTML immediately, inserting into the document the map requested\n> when it is ready, and then inserting more HTML listing the data values\n> reported by the stations shown on the map.  Adding a close-connection hint\n> to such an application would be both feasible and desirable.\n\nOK, so you're asking the server to parse all the content it serves. There are\nseveral problems with this kind of approach:\n\n1. Servers typically have a heavier load than clients, and therefore less\nprocessing power available for parsing.\n\n2. Clients have to parse the content anyway.\n\n3. Servers do not necessarily understand the content (it may be a Java class\nfile, VRML or similar - which may have embedded references to further content,\nbut the server won't know that).\n\n4. If a server is distributed across multiple processes, or worse, multiple\nmachines, it really can't tell what a client has seen and what it hasn't.\n\n5. A server cannot know what is in a client's cache, so sending content\nunsolicited is inefficient.\n\nIf this isn't very coherent, I blame my hangover, and I'll try again later ;-)\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "Jeffrey Mogul wrote:\n> \n> delabeau@iniki.gsfc.nasa.gov writes:\n>     My proposal is that the server say, in effect, \"Here's your page;\n>     reserve space for an image and wait just a sec until I give you\n>     something to fill the space.\"\n> \n> I agree, this is an optimization that one might hope to make.\n>     \n> Ari Luotonen <luotonen@netscape.com>\n>     This can already be accomplished by providing the image dimentions in\n>     the IMG tag.  The only time the client halts layout is when it doesn't\n>     know the image dimentions, that is, they were neither in the IMG tag,\n>     nor yet received as first bytes of the image itself.\n> \n> Actually, this doesn't entirely solve the problem.  If you have\n> an HTML file with, say, 41 embedded images, and image #3 will\n> take 10 seconds to precompute at the server, you would really\n> like to be able to use the network to load images #4 - #41\n> while that is happening.  The IMG tag allows the browser to render\n> the bounding box, but it doesn't solve the retrieval serialization.\n> \n> In the current world, you would probably do that by using another\n> TCP connection, and that might be an acceptable approach, if it's\n> only a few \"deferred\" images per page.  But it does force you\n> to fire up another TCP connection, and if you have already issued\n> a pipeline full of requests for images #4 - #41 behind the request\n> for image #3, then there are some messy issues to worry about\n> (i.e., how to quash the redundant requests without a lot of waste).\n> \n> Another approach that would probably work with HTTP/1.1, but would\n> not be compatible with HTTP/1.0 clients, would be for the server\n> to respond to the initial request for the \"slow\" image by sending\n> HTTP/1.1 503 Service Unavailable\n> Retry-after: 10\n> (using the example of a 10-second delay).  This would allow the\n> server to continue processing the pipeline of requests for\n> images images #4 - #41, because it would not have to stall the\n> response for image #3.  At the same time (assuming some parallelism\n> between image generation and HTTP/TCP processing at the server),\n> it could go ahead and generate image #3, but save the result in\n> a temporary cache.  So, after 10 seconds, when the client retries\n> the request for image #3, it now is available immediately.\n> \n> As I said, this isn't interoperable with HTTP/1.0, or even with\n> an HTTP/1.1 client that doesn't support Retry-After, so it's not\n> clear that this is actually a solution.\n> \n> I think this is a problem worthy of some more thought, but I\n> doubt we'll solve it before we need to progress HTTP/1.1 to\n> Draft Standard.\n\nIsn't this problem effectively solved by multiplexing, which is on the agenda\nfor HTTP-NG?\n\nI agree that we are unlikely to solve it before DS time! I also shudder to\nthink of how we'd have to restructure Apache to handle it.\n\nWe are planning changes to Apache to be able to do this kind of thing in V2.0,\nETA sometime next century at the current rate of progress.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "A question about auto-config:  under what circumstances does it come into play?  For\nexample, suppose I start up a browser and it configures itself.  Then I fuss with\nthe configuration, use the browser for awhile, then quit.  When I start it again, is\nthe browser going to override the settings I put in place, or does it try to\nauto-config only if there are no other settings?\n\nIf the browser only tries to configure itself if there are no pre-settings, there's\nanother problem.  Suppose, given the scenario above, that I decide, for whatever\nreason, that I want no proxies.  When I exit and restart, will auto-config force a\nproxy on me again?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-hit-metering02.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Simple Hit-Metering and Usage-Limiting for HTTP         \n       Author(s) : J. Mogul, P. Leach\n       Filename  : draft-ietf-http-hit-metering-02.txt\n       Pages     : 34\n       Date      : 03/26/1997\n\nThis document proposes a simple extension to HTTP, using a new ``Meter'' \nheader, which permits a limited form of demographic information \n(colloquially called ``hit-counts'') to be reported by caches to origin \nservers, in a more efficient manner than the ``cache-busting'' techniques \ncurrently used.  It also permits an origin server to control the number of \ntimes a cache uses a cached response, and outlines a technique that origin \nservers can use to capture referral information without ``cache-busting.'' \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-hit-metering-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-hit-metering-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-hit-metering-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "Jeffrey Mogul writes:\n> Actually, this doesn't entirely solve the problem.  If you have an HTML\n> file with, say, 41 embedded images, and image #3 will take 10 seconds to\n> precompute at the server, you would really like to be able to use the\n> network to load images #4 - #41 while that is happening.  The IMG tag\n> allows the browser to render the bounding box, but it doesn't solve the\n> retrieval serialization.\n\nThank you for restating what I had not stated clearly enough.  I am indeed\nreferring to a situation wherein the image will not yet exist if the client\nrequests it immediately upon parsing the HTML.\n\n> Another approach that would probably work with HTTP/1.1, but would not be\n> compatible with HTTP/1.0 clients, would be for the server to respond to\n> the initial request for the \"slow\" image by sending\n>   HTTP/1.1 503 Service Unavailable\n>   Retry-after: 10\n\nInteresting idea which should help with pipelined requests.  It does,\nhowever, require invoking a second script on the server.  I want a single\nresponse to send HTML, pause while the image is generated, and then send the\nimage.\n\n> I think this is a problem worthy of some more thought, but I doubt we'll\n> solve it before we need to progress HTTP/1.1 to Draft Standard.\n\nI would not dream of suggesting that this idea be included in 1.1.\n\n--Jeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Connection Management (draft-ietf-http-connection00.txt",
            "content": "Ben Laurie writes:\n> It was not my intention to be rude, just brief. Please accept my apologies.\n\nYou are very gracious--no harm done.  Sorry if I seemed hypersensitive.\n\n> OK, so you're asking the server to parse all the content it serves. There\n> are several problems with this kind of approach [...]\n\nYour arguments and others' have been so throughly convincing that I would\nwithdraw my suggestion if only I could extricate from beneath the\naccumulated rebuttals which have flattened it to jelly.\n\nRegards,\nJeff dLB\n\n\n  = J-F Pitot de La Beaujardiere\n  = delabeau@iniki.gsfc.nasa.gov\n  = http://globe2.gsfc.nasa.gov/\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-connection00.tx",
            "content": " A New Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : HTTP Connection Management                              \n       Author(s) : J. Gettys, A. Freier\n       Filename  : draft-ietf-http-connection-00.txt\n       Pages     : 13\n       Date      : 03/26/1997\n\nThe HTTP/1.1 specification (RFC 2068) is silent about various details of \nTCP connection management when using persistent connections.  This document\ndiscusses some of the implementation issues discussed during HTTP/1.1's \ndesign, and introduces a few new requirements on HTTP/1.1 implementations \nlearned from implementation experience, not fully understood when RFC 2068 \nwas issued.  This is an initial draft for working group comment, and we \nexpect further drafts.                                                     \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-connection-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-connection-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-connection-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "On Thu, 27 Mar 1997, Jeff de la Beaujardiere wrote:\n\n> \n> Interesting idea which should help with pipelined requests.  It does,\n> however, require invoking a second script on the server.  I want a single\n> response to send HTML, pause while the image is generated, and then send the\n> image.\n> \n\nI believe that some server implementations allow an \"nph\" script to\ncompletely take over the connection.  In that case a single script\ncould handle a number of requests in one connection in the way you\nwant.  Personally, I have always considered this a security hole.  It\na allows a script to highjack the connection and pretend to be the\nserver for the duration of the connection without the real server\nbeing aware of the transactions.  My guess is that this \"feature\" will\ndisappear from future versions of servers.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Should server beable to say NoCookie, No Show",
            "content": "quoting Matthew Rubenstein                \n\n> At 02:02 PM 3/25/97 -0800, Yaron Goland wrote:\n> >This is an absolutely excellent idea. I think it will go a long way\n> >towards making content providers feel more comfortable about the cookie\n> >spec.\n> \n>         I concur. The technique outlined below will make the association\n> between the user's discarding the cookie and the app's probable failure in\n> the cookie's absence. The message would be similar to an \"Authentication\n> Failed\", \"Document Returned No Data\" or \"No Response From Server\",\n> indicating a C/S failure. The \"CommentURL\" field also being discussed would\n> be helpful as a supplement.\n\nI hate to keep bringing up the same idea, but what do people\nthink about including an alternate URL, let's call it for now\na \"NoCookieURL\" that, if the cookie were rejected, would replace the\npage that wouldn't work without the cookie.  There are some problems\nthere too, but then the user could customize the response returned\nafter a \"required\" (for the app to run) cookie was rejected?\n\n(So, just to try and be clear, if a page was served with a cookie\nand the cookie were refused, the user would instead see the page\nat NoCookieURL.)\n\nSame effect as NoCookie, but a little more customizable.\nPerhaps if NoCookie had a value, it could be interpreted as a\nURL, if not, it could signal one of the above mentioned errors...\n\nJust a thought...\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "   Date: Thu, 27 Mar 1997 09:38:06 -0500\n   From: Dave Kristol <dmk@bell-labs.com>\n\n   A question about auto-config:  under what circumstances does it come into play?  For\n   example, suppose I start up a browser and it configures itself.  Then I fuss with\n   the configuration, use the browser for awhile, then quit.  When I start it again, is\n   the browser going to override the settings I put in place, or does it try to\n   auto-config only if there are no other settings?\n\n   If the browser only tries to configure itself if there are no pre-settings, there's\n   another problem.  Suppose, given the scenario above, that I decide, for whatever\n   reason, that I want no proxies.  When I exit and restart, will auto-config force a\n   proxy on me again?\n\nI think the answer should be that by default, a browser will try automatic\nconfiguration, and if that fails will use no proxy.\n\nHowever, the preferences of the program should allow you to tell it\nto use something other than the default setting.  So you can either\nchoose a proxy manually or disable proxies.\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "On Thu, 27 Mar 1997, nemo/Joel N. Weber II wrote:\n\n>    Date: Thu, 27 Mar 1997 09:38:06 -0500\n>    From: Dave Kristol <dmk@bell-labs.com>\n> \n>    A question about auto-config:  under what circumstances does it come into play?  For\n>    example, suppose I start up a browser and it configures itself.  Then I fuss with\n>    the configuration, use the browser for awhile, then quit.  When I start it again, is\n>    the browser going to override the settings I put in place, or does it try to\n>    auto-config only if there are no other settings?\n> \n>    If the browser only tries to configure itself if there are no pre-settings, there's\n>    another problem.  Suppose, given the scenario above, that I decide, for whatever\n>    reason, that I want no proxies.  When I exit and restart, will auto-config force a\n>    proxy on me again?\n> \n> I think the answer should be that by default, a browser will try automatic\n> configuration, and if that fails will use no proxy.\n> \n> However, the preferences of the program should allow you to tell it\n> to use something other than the default setting.  So you can either\n> choose a proxy manually or disable proxies.\n\nI think the default should be up to the client publisher. I would\nrecommend that once the user has manually altered setting, either those\nsetting persist or the user should be asked if the settings are temporary.\n\nIn the context of Netscape 3.0, the user has three proxy choices:\n   1.  No\n   2.  Manual ... which allows mix and match\n   3.  PAC URL\n\nThe shipping default is NO proxy. I would expect that the new protocol\nwould either replace #3 or add a #4 and I would guess that the install\nscript would include an autoconfig option and either select #1 or #4 but\nI mention these only to put in context the ID as I read it with how it\nmight be used. I don't see any need for the RFC which might result from\nthis dealing with how individual implementations might be seen by the\nuser except as setting the context for understanding the protocol.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "   Date: Thu, 27 Mar 1997 12:35:00 -0800 (PST)\n   From: \"David W. Morris\" <dwm@xpasc.com>\n\n   I think the default should be up to the client publisher. I would\n   recommend that once the user has manually altered setting, either those\n   setting persist or the user should be asked if the settings are temporary.\n\nI agree.\n\nHowever, note that automagic configuration has the ideal that a network\nadministrator should be able configure one server machine and then\nnot have to touch the clients.\n\nWhen people at my school screwed the routing so that the machines\nbehind the firewall could talk to the mail server, but not outside\nmachines, I changed about three teacher's machines manually.  Those teachers\ndidn't have a clue what a proxy is; and it would be no easier for them\nto tell the browser to use automagic setup than it is for them to\nsay to use 204.130.130.62 port 80.\n\n   In the context of Netscape 3.0, the user has three proxy choices:\n      1.  No\n      2.  Manual ... which allows mix and match\n      3.  PAC URL\n\n   The shipping default is NO proxy. I would expect that the new protocol\n   would either replace #3 or add a #4 and I would guess that the install\n   script would include an autoconfig option and either select #1 or #4 but\n   I mention these only to put in context the ID as I read it with how it\n   might be used. I don't see any need for the RFC which might result from\n   this dealing with how individual implementations might be seen by the\n   user except as setting the context for understanding the protocol.\n\nAgreed.\n\nI personally would rahter see dhcp used instead of dns I think.\nBut I need to read the dhcp spec before I comment furthur...\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "Regarding the issue of how one might defer the retrieval of\nan inlined image that will require a long time to generate\n(at the server), Ben Laurie <ben@gonzo.ben.algroup.co.uk> writes:\n\n    Isn't this problem effectively solved by multiplexing, which is on\n    the agenda for HTTP-NG?\n\n    I agree that we are unlikely to solve it before DS time! I also\n    shudder to think of how we'd have to restructure Apache to handle\n    it.\n\n    We are planning changes to Apache to be able to do this kind of\n    thing in V2.0, ETA sometime next century at the current rate of\n    progress.\n    \nGiven the time already spent on HTTP/1.1, I would also not\nwant to count on deployment of HTTP-NG before the next century :-).\n\nBut this is not really the same as multiplexing, because it's not\nan attempt to reorder requests and responses, or to interleave\nchunks of different messages.  Multiplexing would solve this problem,\nbut you don't need anything like the full complexity of multiplexing.\n\nIf we ignore the issue of compatibility with HTTP/1.0 clients (just\nfor the sake of this particular speculation), I don't think the\nimplementation of my \"503 + Retry-After\" approach would be so bad.\n\nOn the server side, this can be done entirely in CGI.  E.g., suppose\nwe have a CGI script that generates a perspective view of the\ntopology of a certain locality.  Suppose that the user can ask\nfor any variation on perspective (e.g., height, viewing angle,\nwidth of scene, latitude/longitude, and lighting model), so it's\ngoing to take some CPU time to generate the scene.\n\nThe first time the CGI script is invoked with a request for the\nimage, instead of generating the image, it estimates how much time\nit would take to generate it.  This is probably not that difficult,\nand certainly shouldn't require much computation itself.  Suppose\nthat this estimate is \"10 seconds\".  So the CGI script does two\nthings:\n(1) fires up a background process to generate the image,\nand store it in a private server-side cache.\n(2) returns \"503 Service Unavailable\" with \"Retry-after: 10\"\nNote that the HTTP server itself is entirely uninvolved in this\ndecision.\n\n10 seconds later, the client makes the same request, so the server\ninvokes the same CGI script.  This time, the CGI script checks its\nprivate cache, and finds a fully-computed image that matches the\nrequest.  So it simply spits out \"200 OK\" and the file.  Again,\nno involvement from the HTTP server code.\n\nUnfortunately, if the client doesn't understand \"Retry-After\",\nthen it will simply treat the first 503 response as an unrecoverable\nerror, and will never retry the request.  So I don't think this\nexact approach will work in real life, at least not until the\ncurrent population of browsers is replaced.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "HTML &quot;include&quot;: see WDobject [was: HTTP Connection Management",
            "content": "To: John Franks<john@math.nwu.edu>\n\nJohn Franks wrote:\n> One of the major failings of HTML is its inability to do\n> \"client side includes\" i.e. the equivalent of <img src=\"foo.png\">\n> for text rather than images.\n\nWe expect to remedy this with deployment support for\nthe <object> spec[1]. For example:\n\n<object data=\"foo.html\">\n<a href=\"foo.html\">would be in-line\nif your client supported WD-object</a>\n</object>\n\n[1] http://www.w3.org/pub/WWW/TR/WD-object\n\n-- \nDan Connolly, W3C Architecture Domain Lead\n<connolly@w3.org> +1 512 310-2971\nhttp://www.w3.org/People/Connolly/\nPGP:EDF8 A8E4 F3BB 0F3C FD1B 7BE0 716C FF21\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "Apparently, protocol working groups have the responsibility\nto ensure that the agents that implement their protocols are\nmanagable. There's an \"application MIB\" working group also\nscheduled for IETF, and one of the issues is network management\nof web servers. Perhaps things like proxy configurations and\nthe network management of clients are a good topic to raise\nin that forum.\n\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Revised SHTTP draf",
            "content": "A new Internet-Draft describing S-HTTP version 1.3 is available from the\non-line Internet-Drafts directory:     \n    \"The Secure HyperText Transfer Protocol\" draft-ietf-wts-shttp-04.txt\n\nIt differs from the previous draft in that the revised specification takes\nnote of and is aligned with HTTP/1.1 (as per comments received during last\ncall). Specific changes made include:\n    * Replaces previous use of application/http with message/http.\n    * References HTTP/1.1's header syntax and BNF.\n    * Removes HTTP-incompatible Content-Transfer-Encoding.\n    * Is 'Host' and 'Connection' header aware.\n\nCheers,\n\n-Allan\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "Jeffrey Mogul wrote:\n> \n> Regarding the issue of how one might defer the retrieval of\n> an inlined image that will require a long time to generate\n> (at the server), Ben Laurie <ben@gonzo.ben.algroup.co.uk> writes:\n> \n>     Isn't this problem effectively solved by multiplexing, which is on\n>     the agenda for HTTP-NG?\n> \n>     I agree that we are unlikely to solve it before DS time! I also\n>     shudder to think of how we'd have to restructure Apache to handle\n>     it.\n> \n>     We are planning changes to Apache to be able to do this kind of\n>     thing in V2.0, ETA sometime next century at the current rate of\n>     progress.\n>     \n> Given the time already spent on HTTP/1.1, I would also not\n> want to count on deployment of HTTP-NG before the next century :-).\n> \n> But this is not really the same as multiplexing, because it's not\n> an attempt to reorder requests and responses, or to interleave\n> chunks of different messages.  Multiplexing would solve this problem,\n> but you don't need anything like the full complexity of multiplexing.\n\nTrue. And multiplexing was causing my shudders, not your proposed solution.\n\n[snip admirably clear discussion of how to do it without touching the server]\n\n> Unfortunately, if the client doesn't understand \"Retry-After\",\n> then it will simply treat the first 503 response as an unrecoverable\n> error, and will never retry the request.  So I don't think this\n> exact approach will work in real life, at least not until the\n> current population of browsers is replaced.\n\nAlso true. How about using a refresh? Or is that pure Netscapism?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "> \n> If the browser only tries to configure itself if there are no pre-settings,\n> there's another problem.  Suppose, given the scenario above, that I\n> decide, for whatever reason, that I want no proxies.  When I exit and\n> restart, will auto-config force a proxy on me again?\n\nI think the issue your talking about is where the browser control resides.\nDoes the control reside with the network administrator, who\n\"knows better\" about what proxies to use and when?\n\nor does the user know better?\n\nKeep in mind, that the browser iplementation is free to query the\nuser on whether or not to accept the configuration presented.\n\nThe intent of the draft is to address concerns from administrators\nin large networks about difficulty in either configuring a large\nnumber of browsers, or educating a non technical crowd how to\nconfigure proxy settings.\n\nIf the administrators have not configured the TXT record,\nthen the browser should default to the configured settings.\n\nWhile I've said that the browser implemention is flexible,\nI would recommend a model like the netscape navigator.\nOut of the box, the browser should be set to\n'discovery proxy settings',\nbut still have the check boxes for\n'manual settings'\n'no proxies'\n'explicit PAC URL'\n\nand that choice be saved in the users preferences.\nThis arrangement seems to meet the needs you specify (IMHO )\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "> \n> I personally would rahter see dhcp used instead of dns I think.\n> But I need to read the dhcp spec before I comment furthur...\nThere is a draft which recommends using dhcp instead, which I cite\nin my draft.\nI beleive the Server Location Method is a better choice because:\n\n1. DHCP doesnt have a cross platform interface to its configuration\n  option.  By using DNS, or even the raw SVRLOC multicast protocol,\n  its still a consistent and relatively easy implementation.\n\n2. DHCP is a one time configuration retreival, the proxy config can \n   change dynamically, and DNS allows a mechanism to do that.\n( ie it has a lifetime )\n   ( yes, DHCP has a lease, but I beleive that is meant to be more\n     long lived )\n\n3. Service location is an appropriate way to advertise a service,\n   which I beleive the proxy is.  The browser is discovering\n   an available proxy service, not a host configuration option.\n\n4. DNS is much more commonly deployed where the web clients are\n(its usually a necessity ), while DHCP seems to be mainly\n popular on PCs\n\n>   I think the default should be up to the client publisher. I would\n>    recommend that once the user has manually altered setting, either those\n>    setting persist or the user should be asked if the settings are\n>    temporary. \nAgreed ( in previous reply )\n\n> machines, I changed about three teacher's machines manually.  Those\n> teachers didn't have a clue what a proxy is; and it would be no easier for\n> them to tell the browser to use automagic setup than it is for them to\n> say to use 204.130.130.62 port 80.\nYes, if thats all your saying, but these days, a common config would\nbe something like:\nset http:// to go to proxy1 on port 80\nset ftp:// to go to proxy2 on port 80\nset https://*.evil.com to proxy 3 in port 443\nset no-proxy for mydomain.com yourdomain.com thishost.thatdomain.com\n\nand more complex ones are coming ie:\nsend HTTP GETS to proxy2 on port 80\nsend HTTP PUTS to proxy3 port 80.\n\nI suspect that if the browser can detect this by itself, out of the box,\nthe teachers life is greatly simplified.\n\nAs an aside, even if the discovery isnt used, the teacher still\nmust navigate to 'proxy settings -> auto' and specifiy\nthe URL: http://proxy:8080/proxy.pac\nwhich is still much easier than a manual configuration.\n\nI want to make the point that as caches as being deployed\nmore frequently and with more complexity, ie hierarchies and\ndynamic ICP type protocols, the configurations need to be\ndynamic and complex.  Well beyond what a nontechnical user\nshould need to know, and extremely difficult to provide\na UI to let a user specify it manually.\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "> Apparently, protocol working groups have the responsibility\n> to ensure that the agents that implement their protocols are\n> managable. There's an \"application MIB\" working group also\n> scheduled for IETF, and one of the issues is network management\n> of web servers. Perhaps things like proxy configurations and\n> the network management of clients are a good topic to raise\n> in that forum.\n\nI dont agree, this is about configuring a user agent's service\nutlization, not configuring or reporting on the server of a service.\n\nI think this is closer a DHCP issue than any SNMP MIB stuff.\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "   Date: Thu, 27 Mar 1997 18:14:26 -0800 (PST)\n   From: Josh Cohen <josh@netscape.com>\n   Sender: josh@birdcage.mcom.com\n\n   2. DHCP is a one time configuration retreival, the proxy config can \n      change dynamically, and DNS allows a mechanism to do that.\n   ( ie it has a lifetime )\n      ( yes, DHCP has a lease, but I beleive that is meant to be more\nlong lived )\n\nThat was not how I interpreted it.\n\nI think most of the DNS servers I work with have a 1 week lifetime.\n\nMy PC has an IP address that will last for the duration of this ppp\nsession--ie an hour or two.\n\n   3. Service location is an appropriate way to advertise a service,\n      which I beleive the proxy is.  The browser is discovering\n      an available proxy service, not a host configuration option.\n\nWhat's the difference between discovering DNS servers and discovering\nHTTP proxies?\n\n   4. DNS is much more commonly deployed where the web clients are\n   (its usually a necessity ), while DHCP seems to be mainly\n   popular on PCs\n\nI understand that point.\n\nBut I don't think that the implementation you propose is not as clean.\n\n   Yes, if thats all your saying, but these days, a common config would\n   be something like:\n   set http:// to go to proxy1 on port 80\n   set ftp:// to go to proxy2 on port 80\n   set https://*.evil.com to proxy 3 in port 443\n   set no-proxy for mydomain.com yourdomain.com thishost.thatdomain.com\n\n   and more complex ones are coming ie:\n   send HTTP GETS to proxy2 on port 80\n   send HTTP PUTS to proxy3 port 80.\n\nOK, but I don't see any settings where I would want to have such a\ncomplex setup.\n\nLike I might run a proxy server on one of my computers at home if I\nget organized.  But I would only need one proxy.\n\nI'd guess you need thousands of users on your network before you'd\nneed multiple proxies.\n\nAnd with proxy cache hierarcies, I would htink it would always work\nto ask your local proxy for the document, and let the proxy figure\nout how to talk to another proxy.  I don't see where the client would\nhave to know about that.\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "> \n> I think most of the DNS servers I work with have a 1 week lifetime.\nIn DNS, each record has a lifetime, specified individually.\nSo, the while your A records would be ttl 1day\n this TXT record could be ttl 1 hour\n\nIn DHCP, all options are stuck with the same lease time.\n\n>  3. Service location is an appropriate way to advertise a service,\n>       which I beleive the proxy is.  The browser is discovering\n>       an available proxy service, not a host configuration option.\n> \n> What's the difference between discovering DNS servers and discovering\n> HTTP proxies?\nThe DNS server is a system wide setting, all applications depend on\nthe system, which depends on the DNS.\nthe proxy settings are for an application.\n\nThis is picking nits.  I initially was hoping to use DHCP, but it\ndoesnt offer the cross platform and lifetime flexibility that DNS\ndoes and its less widely deployed.\n\nAnother fundamental issue here is if you buy into the service location\nprotocol ideas.  I do. They are trying to solve the problem of\n\"how do I use a standard method to look up arbitrary services on a network\".\nGranted, by following their DNS recommendations ( which is only one\nof many ways to advertise services in their world ), its not perfect.\nIn time, I hope that we could support their multicast discovery\nprotocol as well. \n\n I suppose I could have specified that in the\ndraft, but since the rest of the serverloc stuff is so new, \nand virtually undeployed as of yet, I figured this gives us a solution\nto a big problem, today, with protocols and APIs software implementors\nin the realm of the 'world wide web' are commonly using.\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: Add CacheControl extensibility to Vary heade",
            "content": "These are actually PEP-related criticisms, but they fit within the\ncontext of Henrik's post.\n\n>By adding the Cache-Control header extensibility mechanism (14.9.6) to the\n>Vary header, I believe that we make caches much more robust to extended\n>content (or feature) negotiation than by having the current definition of\n>Vary.\n\nVary works well primarily because it is very simple.  PEP's use of a\nsingle header field name \"Protocol\" to represent multiple, unrelated\ncontrol options doesn't work with this simple model.  However, I think\nthe solution is also simple: fix PEP so that it works better within\nthe design of HTTP, rather than the other way around.\n\n>[...] if we instead have two extensions where one is optional\n>and the other required like this:\n>\n>GET /a-document HTTP/1.1\n>Host: a.host\n>Protocol: {http://some.org/a-required-extension {str \"req\"}},\n>   {http://some.org/an-optional-extension {str \"opt\"}}\n>\n>HTTP/1.1 200 OK\n>Protocol: {http://some.org/a-required-extension {str \"req\"}},\n>   {http://some.org/an-optional-extension {str \"opt\"}}\n>Vary: Protocol\n>Content-Type: text/plain\n>\n>then the cache would only be able to serve the request if both extensions\n>are available in the request even though the one is optional.\n>\n>I believe this in many cases is too strict as it will effectively\n>disable caching of all optional extensions.\n\nYep, no disagreement there.  However, I will note that adding all those\nbytes to a typical request is an incredibly bad idea, for the same reason\nthat adding huge amounts of Accept* stuff to typical requests is a bad idea.\nBut that is a separate issue.\n\n>I therefore suggest that we define the Vary header to have the same\n>extension modifiers to existing header values just like the cache-control\n>header. This means that a PEP-aware proxy cache will be able to understand\n>something like this\n>\n>HTTP/1.1 200 OK\n>Protocol: {http://some.org/a-required-extension {str \"req\"}},\n>   {http://some.org/an-optional-extension {str \"opt\"}}\n>Vary: Protocol, \"http://some.org/a-required-extension\"\n>Content-Type: text/plain\n>\n>and serve the cached entity in response to a request like this:\n>\n>GET /a-document HTTP/1.1\n>Host: a.host\n>Protocol: {http://some.org/a-required-extension {str \"req\"}\n>\n>Note that this doesn't _force_ the proxy to serve the request. This is a\n>property of the extension itself: The proxy may not be allowed to reply as\n>an intermediary depending on the contractual agreement between the proxy\n>and the origin server.\n\nThat is asking a great deal from a proxy -- it would not only have to store\nall of the client's Protocol requests, but on every request it would need\nto find the Protocol field, parse it into entries, and then find the\nentry associated with {http://some.org/a-required-extension {str \"req\"}}.\nI suppose it would also need to check for any other end-to-end\n{str \"req\"} options and not serve the cached entity if found.\n\nIf you want to negotiate based on the contents of individual field values,\nthen you will need to exchange a negotiation algorithm in the response\nalong the lines of\n\n   Use-Only-If: Protocol contains\n                {http://some.org/a-required-extension {str \"req\"}}\n\nthough I would recommend using an established notation for describing\nthe algorithm rather than an off-the-cuff example like this.\n\nAnother alternative is to ditch the current definitions of Protocol and\nC-Protocol and do the following:\n\n   1) Reintroduce the Mandatory field from two years ago, consisting\n      of a list of header fields with semantics {str \"req\"}.\n\n   2) Define a standard value for field self-definition, such that\n      any new field whose field-value begins with\n\n          {defined \"http://some.org/whatever\"}\n\n      means that this field is defined according to the semantics\n      of \"http://some.org/whatever\" (which we'll assume to be a\n      URI with the equivalent naming power of a URN).  Any field without\n      that value would have the definition given by the HTTP-version\n      of the message.\n\n   3) Add a field-name to the beginning of each Protocol-Info entry.\n\nThis would also get rid of the unclean use of a URI as the first\npart of a bag.  Originally, the bag notation was intended to follow\nthe BRIO notation, which required that the first element in a list\nbe a symbol (token) for ease of representation.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "System management of HTTP client",
            "content": "I wasn't clear in my message. What I meant to say is that\neven though \"system management of HTTP clients\" doesn't\nappear in our charter, and isn't directly related to\n\"development of the HTTP protocol\", I believe that discussion\nof this topic is \"in scope\" for HTTP-WG, at least to the\nextent that we decide how to deal with the issue or\ndelegate it to some other subgroup.\n\nIt would be useful to enumerate the entire set of requirements\nfor system management for HTTP clients (media type helpers,\nsecurity profiles, site policy about viruses, privacy, etc.)\nas well as HTTP proxies and HTTP servers.\n\nI expect that these issues could easily absorb at least one\nnew working group, and we need to arrange to establish\nthis activity before HTTP-WG itself shuts down.\n\nRegards,\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Revised HTTPWG agenda for April ",
            "content": "Larry Masinter:\n>\n\nI found a mistake in the agenda when getting all the drafts:\n\n>** 10 minutes hit metering\n>ftp://ietf.org/internet-drafts/draft-mogul-hit-metering-01.txt\n                                      ^^^^^              ^^\nThe correct draft name is       draft-ietf-http-hit-metering-02.txt .\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: ID: Proxy autoconfi",
            "content": "1. DHCP doesnt have a cross platform interface to its\nconfiguration\n option.  By using DNS, or even the raw SVRLOC multicast\nprotocol,\n  its still a consistent and relatively easy implementation.\n\n\nLack of API's on different platforms shouldn't be a reason to\npick one wire protocol vs. another.\n\nOn windows, for ex., you can look at DHCP options quite easily\nin the registyr but there aren't API's (yet) for querying RR's from the\nDNS  (unless you want to count on NSLOOKUP output on the commandline)\n\n\n4. DNS is much more commonly deployed where the web clients are\n(its usually a necessity ), while DHCP seems to be\nmainly\n popular on PCs\n\n\nThere are thousands of small intranets that use DHCP for IP\naddressing and NetBios broadcasts for name resolution.  PC's make up the\nbulk of clients and a DHCP-solution solves more than half the case\ninstantly.\n\n\nI'm not quite religious about this DHCP vs. DNS issue and it\nmight not be a bad thing to allow this info to be specified twice.\n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "A few observations.\n\n1)  It is true that DHCP lease times can be \"long-lived\", i.e. on the\norder of weeks in some practical cases.  However this is only important\nif the URL that is delivered in the DHCP option changes, not the proxy\ninformation.  The proxy information is buried in the file referenced by\nthe URL, and the client can reload the file at an arbitrary interval.\nAs long as the server that is distributing the URL doesn't change often,\nyou are ok.\n\n2)  Using DNS will not work for mobile clients.  For example, consider a\nlaptop named SKWAN01.INTRA.MICROSOFT.COM.  While plugged into the\nMicrosoft corporate net, it queries for and receives the TXT RR:\n\nw3-ns-pac.intra.microsoft.com. IN TXT\n\"service:yp-http://proxy1.intra.microsoft.com:8080/proxy.ins\"\n\nI unplug my laptop and take it on a visit to Netscape.  When I plug into\nthe Netscape corporate network, I query for the TXT RR per above and the\nquery fails.  At this point, I have no way of finding the proxy servers\nfor that network, and automatic configuration fails.\n\n3)  It is true that DHCP is not necessarily widely available today, but\nif anything SRVLOC is less available.  The DHCP method at least gives\nyou something you can use now.\n\n4)  The fact that there is no cross-platform API to retrieve DHCP\noptions is interesting, but does not block implementation.  While the\nDHCP WG investigates this problem, use the platform-specific method for\nretrieving options.  Please note that there is no cross-platform\nstandard API for retrieving TXT RRs from DNS.\n\nI have attached my I-D info below for those interested.\n\nCheers,\n- Stuart Kwan\nMicrosoft Corp.\n______\n\n A New Internet-Draft is available from the on-line Internet-Drafts \n directories.\n\n\n       Title     : DHCP Option for Proxy Client Configuration File\n\n       Author(s) : S. Kwan\n       Filename  : draft-kwan-proxy-client-conf-00.txt\n       Pages     : 3\n       Date      : 02/26/1997\n\nApplication-level gateways are used on networks to provide controlled \naccess to the Internet.  Clients in those networks must be configured \nwith the name or address of available proxy servers, the list of local \ndomain names, and other proxy client configuration information before \nthey can access the Internet.  The defacto method of proxy client \nconfiguration is the download of a script or configuration file \nnamed by a URL.  This document describes a DHCP option in which to \ntransmit this URL to a proxy client.\n\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-kwan-proxy-client-conf-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-kwan-proxy-client-conf-00.tx\nt\n \n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "   From: Stuart Kwan <skwan@microsoft.com>\n   Date: Fri, 28 Mar 1997 14:05:38 -0800\n\n   1)  It is true that DHCP lease times can be \"long-lived\", i.e. on the\n   order of weeks in some practical cases.  However this is only important\n   if the URL that is delivered in the DHCP option changes, not the proxy\n   information.  The proxy information is buried in the file referenced by\n   the URL, and the client can reload the file at an arbitrary interval.\n   As long as the server that is distributing the URL doesn't change often,\n   you are ok.\n\nYes, and as I understand it, if you need to change that URL often,\nyou can use 1 hour lease times.\n\n   2)  Using DNS will not work for mobile clients.  For example, consider a\n   laptop named SKWAN01.INTRA.MICROSOFT.COM.  While plugged into the\n   Microsoft corporate net, it queries for and receives the TXT RR:\n\n   w3-ns-pac.intra.microsoft.com. IN TXT\n   \"service:yp-http://proxy1.intra.microsoft.com:8080/proxy.ins\"\n\n   I unplug my laptop and take it on a visit to Netscape.  When I plug into\n   the Netscape corporate network, I query for the TXT RR per above and the\n   query fails.  At this point, I have no way of finding the proxy servers\n   for that network, and automatic configuration fails.\n\nI don't really follow this logic.\n\nWhen you visit Netscape, I assume you'll get a different IP address.\nI also assume that means you'll have a different hostname--maybe\nvisitor01.intra.netscape.com\n\n   3)  It is true that DHCP is not necessarily widely available today, but\n   if anything SRVLOC is less available.  The DHCP method at least gives\n   you something you can use now.\n\nIt should be noted that albert.gnu.ai.mit.edu (the mail server that\nprocesses all my incoming mail) had a version of named which couldn't\nhandle TXT records up until about a month ago.  So even that solution\nis not really completely compatible with existing sites.\n\nHowever, if you want to go the name server route, I wonder why you\ncouldn't make the URL something hardcoded like http://www-ns-pac/proxy.ins\nThat would mean that you have to use port 80 with a standardized path,\nbut that's less strange than the TXT or SRVRLOC records.\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "   2)  Using DNS will not work for mobile clients.  For example,\nconsider a\n   laptop named SKWAN01.INTRA.MICROSOFT.COM.  While plugged into\nthe\n   Microsoft corporate net, it queries for and receives the TXT\nRR:\n\n   w3-ns-pac.intra.microsoft.com. IN TXT\n   \"service:yp-http://proxy1.intra.microsoft.com:8080/proxy.ins\"\n\n   I unplug my laptop and take it on a visit to Netscape.  When\nI plug into\n   the Netscape corporate network, I query for the TXT RR per\nabove and the\n   query fails.  At this point, I have no way of finding the\nproxy servers\n   for that network, and automatic configuration fails.\n\nI don't really follow this logic.\n\nWhen you visit Netscape, I assume you'll get a different IP\naddress.\nI also assume that means you'll have a different hostname--maybe\nvisitor01.intra.netscape.com\n\nPardon that.  I was assuming dynamic DNS.  Future Microsoft clients (and\nDNS servers) will be dynamic-DNS enabled.  When they receive a new IP\naddress, they will register that address under their name.\n\nHowever, you make a good point.  When I plug my laptop in at Netscape, I\nneed a new IP addr.  Either 1) they are running DHCP and this discussion\nis moot, the client can receive the URL via DHCP, or 2) I have dig into\nmy laptop and enter a new IP address by hand - in which case a) the\nclient could do a reverse lookup to determine it's name, or b) I could\nhand configure the new name, or c) I could hand enter the proxy\ninformation.  After all, some sysadmin is going to have to give me an IP\n(and DNS IPs, and gateways, etc etc), I might as well be given the proxy\nserver info too.\n\nI am also not opposed to storing this information in two places.  I am\nonly concerned that we solve the automatic configuration problem.\n\nCheers,\n- Stuart Kwan\nMicrosoft Corp.\n\n\n\n"
        },
        {
            "subject": "Re: ID: Proxy autoconfi",
            "content": "> A question about auto-config: under what circumstances does it come\n> into play?  For example, suppose I start up a browser and it\n> configures itself.  Then I fuss with the configuration, use the\n> browser for awhile, then quit.  When I start it again, is the\n> browser going to override the settings I put in place, or does it\n> try to auto-config only if there are no other settings?\n> \n> If the browser only tries to configure itself if there are no\n> pre-settings, there's another problem.  Suppose, given the scenario\n> above, that I decide, for whatever reason, that I want no proxies.\n> When I exit and restart, will auto-config force a proxy on me again?\n\nThe idea of \"auto-configuration\" is \"to pull the configuration\nautomatically from a central place\", not \"to automatically configure\ndefault values\".  That is, either you use the automatic configuration,\nor manual.\n\nCheers,\n--\nAri Luotonen* * * Opinions my own, not Netscape's * * *\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "Hi,\n\n1. I have not found the mentioned multiplexing stuff on the HTTP-WG issues\npage. Could someone point me to the right place?\n\n>delabeau@iniki.gsfc.nasa.gov writes:\n>    My proposal is that the server say, in effect, \"Here's your page;\n>    reserve space for an image and wait just a sec until I give you\n>    something to fill the space.\"\n>I agree, this is an optimization that one might hope to make.\n\nI think (and that's what I don't like about pipelining) that this is\nbecause the ordering of the requests and the responses in pipelining.\n\nIf the ordering was not as strict as it is now, this was not an issue.\nIf any of the response headers would contain the URI of the requested\nresource (e.g. in the Location header) than ordering would not be\nimportant any more. In this case server could decide which of the\nrequested URIs in a pipelined request should be processed first - e.g.\nfirst the smaller objects should be sent, or the static objects have\nhigher priority, etc. It seems to require more processing on the server\nside, but I think the processing time of the whole pipelined request can\nbe even less than before. \nBut probably this approach would be harmful for HTTP/1.0 clients.\n\nBertold\n\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n Kolics, Bertold                             E-Mail: bertold@tohotom.vein.hu\n University of Veszprem, Hungary        W3: http://tohotom.vein.hu/~bertold/\n Information Engineering Course\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n\n\n\n"
        },
        {
            "subject": "1.1 server available for testin",
            "content": "  Agranat Systems has just announced beta availability of HTTP 1.1\n  support in our EmWeb server for embedded systems.  I'm working on a\n  note for the working group about our experiences implementing 1.1\n  and especially some points that probably don't come up outside\n  embedded systems.  In the mean time, a test version of our 1.1\n  server is running at: <URL:http://www.agranat.com:8080/>.  We would\n  appreciate hearing from anyone who accesses it, especially with 1.1\n  clients.  Thanks.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": "Roy T. Fielding:\n>\n>After some good comments from Jeff, I am changing my proposed change.\n[...]\n>should be replaced with\n[...]\n>   Many HTTP/1.0 cache implementations will treat an Expires value that\n>   is less than or equal to the response Date value as being equivalent\n>   to the Cache-Control response directive \"no-cache\".  If an HTTP/1.1\n>   cache receives such a response, and the response does not include a\n>   Cache-Control header field, it SHOULD consider the response to be\n                                   ^^^^^^\n>   non-cachable in order to retain compatibility with HTTP/1.0 servers.\n    ^^^^^^^^^^^^\n\nEek!  This is a completely new SHOULD as far as I can see.\n\nI oppose adding this SHOULD because it leads to sub-optimal caching.  I\ndon't see any need to be compatible with the `Many HTTP/1.0 cache\nimplementations' the paragraph talks about.  I consider these `many\nimplementations' to be sub-optimal, because they should be using I-M-S to\nrevalidate the stale entry instead of just throwing it away.\n\nAlso, this new SHOULD contradicts the Expires section:\n\n|14.21 Expires\n|\n|   The Expires entity-header field gives the date/time after which the\n|   response should be considered stale. A stale cache entry may not\n|   normally be returned by a cache (either a proxy cache or an user\n|   agent cache) unless it is first validated with the origin server [...]\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Random httpwg statistic",
            "content": "I made these statistics for fun when I was tired of reading internet\ndrafts.  I'm posting them because they provide some background info\nfor discussing the `WG status' in Memphis.\n\n* http-wg mailing list traffic from Jan 4 to Mar 28:\n\n  683 messages\n  ~110 different message authors\n  35% of all messages are about cookies\n\n Top 10 subjects:\n\n  count\n   68Subject: Unverifiable Transactions / Cookie draft\n   37Subject: How to add new \"protocols\" ?\n   23Subject: Issues with the cookie draft\n   21Subject: new cookie draft\n   19Subject: Comments on the new cookie draft\n\n   17Subject: errata for cookie spec\n   16Subject: Comments on draft-ietf-http-negotiation-00.txt\n   14Subject: RFC2109 addition...\n   14Subject: Content encoding problem... \n   14Subject: A broken browser\n\n Mailing list traffic size:\n  1.6 Mb text if headers are removed\n  1.0 Mb text if headers and quoted lines are removed\n\n* compare: www-talk mailing list traffic from Jan 4 to Mar 28:\n\n  169 messages\n  0.3 Mb text if headers are removed\n  0.2 Mb text if headers and quoted lines are removed\n\n* http-wg RFCs: \n\n  4 documents\n  0.57 Mb text\n  244 (60-line) pages\n\n* http-wg Memphis internet drafts\n\n  12 documents \n  0.42 Mb text\n  188 (60-line) pages\n\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": "Roy T. Fielding:\n>   Many HTTP/1.0 cache implementations will treat an Expires value that\n>   is less than or equal to the response Date value as being equivalent\n>   to the Cache-Control response directive \"no-cache\".  If an HTTP/1.1\n>   cache receives such a response, and the response does not include a\n>   Cache-Control header field, it SHOULD consider the response to be\n>   non-cachable in order to retain compatibility with HTTP/1.0 servers.\n\nKoen Holtman:\n    Eek!  This is a completely new SHOULD as far as I can see.\n\n    I oppose adding this SHOULD because it leads to sub-optimal\n    caching.  I don't see any need to be compatible with the `Many\n    HTTP/1.0 cache implementations' the paragraph talks about.  I\n    consider these `many implementations' to be sub-optimal, because\n    they should be using I-M-S to revalidate the stale entry instead of\n    just throwing it away.\n    \nOne goal for HTTP/1.1 is that it not break any HTTP/1.0 implementations.\nThe HTTP/1.0 not-quite-specification (RFC1945) says, for \"Expires\"\n\nApplications must not cache this entity beyond the date given.\n\nI.e., the HTTP/1.0 interpretation here is a MUST, and we are actually\nrelaxing it by saying \"SHOULD\".\n\nLest one think that this is some newfangled thing in RFC1945, I\nwent back to earlier texts for \"Expires\".  Back in March, 1995,\ndraft-ietf-http-v10-spec-00.txt said \n\n   Caching clients (including proxies) \n   must not cache this copy of the resource beyond the date given, \n   unless its status has been updated by a later check of the origin \n   server.\n\n(which is probably clearer than the RFC1945 wording).  Going back\neven further, to November 1993, draft-ietf-iiir-http-00.txt said\nthat \"Expires\"\n\n   Gives the date after which the information given ceases to be valid\n   and should be retrieved again.\n\n(whether this \"should\" is a SHOULD or a MUST isn't clear, since\nthe term \"should\" is often used in this document in places where\nwe would almost certainly use MUST today.)\n\nAside from that, this probably does not pose a serious threat\nto cache performance.  On the other hand, it could probably\nbe worded better.  How about something like this:\n    Many HTTP/1.0 servers expect caches to treat an Expires value that\n    is less than or equal to the response Date value as being equivalent\n    to the Cache-Control response directive \"no-cache\".  If an HTTP/1.1\n    cache receives such a response, and the response does not include a\n    Cache-Control header field, it SHOULD NOT use the response in\n    reply to a subsequent query without first revalidating it.\n\nI.e., while \"max-age=0\" does not imply \"must-revalidate\", \"expires now\"\ndoes.\n\n    Also, this new SHOULD contradicts the Expires section (14.21):\n\n    The Expires entity-header field gives the date/time after which the\n    response should be considered stale. A stale cache entry may not\n    normally be returned by a cache (either a proxy cache or an user\n    agent cache) unless it is first validated with the origin server [...]\n    \nNot as I reworded it.  Note that the word \"cachable\" is used a little\ntoo informally in HTTP/1.1 (RFC2068).  The definition is\n\n   cachable\n      A response is cachable if a cache is allowed to store a copy of\n      the response message for use in answering subsequent requests. The\n      rules for determining the cachability of HTTP responses are\n      defined in section 13. Even if a resource is cachable, there may\n      be additional constraints on whether a cache can use the cached\n      copy for a particular request.\n\nI believe what Roy meant by \"non-cachable\" is \"not usable without\nrevalidation\", not \"not storable\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": ">>   Many HTTP/1.0 cache implementations will treat an Expires value that\n>>   is less than or equal to the response Date value as being equivalent\n>>   to the Cache-Control response directive \"no-cache\".  If an HTTP/1.1\n>>   cache receives such a response, and the response does not include a\n>>   Cache-Control header field, it SHOULD consider the response to be\n>                                   ^^^^^^\n>>   non-cachable in order to retain compatibility with HTTP/1.0 servers.\n>    ^^^^^^^^^^^^\n>\n>Eek!  This is a completely new SHOULD as far as I can see.\n\nActually, it is an old SHOULD that was discarded when the big caching\nchanges were made to Expires.  It reflects what was in HTTP/1.0.\n\n>I oppose adding this SHOULD because it leads to sub-optimal caching.  I\n>don't see any need to be compatible with the `Many HTTP/1.0 cache\n>implementations' the paragraph talks about.  I consider these `many\n>implementations' to be sub-optimal, because they should be using I-M-S to\n>revalidate the stale entry instead of just throwing it away.\n\nThe only way to obtain optimal caching is to use Cache-Control, or no\nExpires at all.  This paragraph would only apply when it is clear that\nan older, RFC 1945-compliant origin server is attempting to force proxies\nnot to cache a message.  The changes to Expires from RFC 1945 to 2068\nremoved those semantics, but in so doing created an incompatibility between\nHTTP/1.0 and HTTP/1.1, which by definition is an error in the new protocol.\n\nThe only significant effect on HTTP/1.1 caching will be the prevention of\ncaching messages from HTTP/1.0 servers that are clearly intended to not be\ncachable.\n\n>Also, this new SHOULD contradicts the Expires section:\n\nThat would be changed as well.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Issue: maxage in responses not define",
            "content": ">I believe what Roy meant by \"non-cachable\" is \"not usable without\n>revalidation\", not \"not storable\".\n\nNo, I meant not \"cachable\" as defined by RFC 2068.  The paragraph\nwould not be needed if I meant \"not usable without revalidation\",\nsince the existing definitition of Expires adequately covers that.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Age Header Field in HTTP/1.",
            "content": "(Reference: draft-fielding-http-age-00.txt)\n\nRoy writes, regarding the specification for the Age header in\nsection 14.6 of RFC2068:\n\n      If a cache receives a value larger than the largest positive integer\n      it can represent, or if any of its age calculations overflows, it\n      MUST transmit an Age header with a value of 2147483648 (2^31).\n      HTTP/1.1 caches MUST send an Age header in every response. Caches\n      SHOULD use an arithmetic type of at least 31 bits of range.\n\n   This document focuses on the ambiguous use of the term \"caches\" in\n   the second-to-last line above. \n\n    [...]\n\n   There are two possible interpretations of\n\n      HTTP/1.1 caches MUST send an Age header in every response.\n\n   Either\n\n      a) An HTTP/1.1 server that includes a cache MUST send an Age\n         header field in every response.\n   or\n      b) An HTTP/1.1 server that includes a cache MUST include an Age\n header field in every response generated from its own cache.\n      \nFor the record, when I wrote this paragraph (and I did write it),\nI should have written \"HTTP/1.1 proxy caches\", not \"HTTP/1.1 caches\".\nMy error; I was being sloppy.\n\nSo the issue here is not the ambiguity of the term \"caches\"; it\nis whether a proxy cache that immediately forwards a response must\nadd an Age header to it, or not.\n\n   If we were to assume that\n\n      An HTTP/1.1 server that includes a cache MUST send an Age\n      header field in every response.\n\n   is true, then an HTTP/1.1 proxy containing a cache would be required\n   to add an Age header field value to every response that was\n   forwarded, including those that were obtained first-hand from the\n   origin server and never touched by the caching mechanism.  This would\n   directly contradict the paragraph in section 13.2.1 of RFC 2068 that\n   states:\n\n      The expiration mechanism applies only to responses taken from a cache\n      and not to first-hand responses forwarded immediately to the\n      requesting client.\n\nNo, this is not a contradiction.  That paragraph from section 13.2.1\npertains to the decision about whether a response is stale or fresh.\nThis is quite distinct from any rules about how the inputs to that\ndecision are provided.\n\n   and also directly contradicts the last paragraph of section 13.2.3 of\n   RFC 2068 that states:\n\n      Note that a client cannot reliably tell that a response is first-\n      hand, but the presence of an Age header indicates that a response\n      is definitely not first-hand.\n\nThis is indeed a contradiction; the Note in section 13.2.3 is erroneous.\nMy fault.  The Note should simply say\n\n      Note that a client cannot reliably tell that a response is first-\n      hand.\n\n(plus the part that Roy didn't quote).\n\n   However, in section 13.2.3 of RFC 2068, we also find\n\n      In essence, the Age value is the sum of the time that the response\n      has been resident in each of the caches along the path from the\n      origin server, plus the amount of time it has been in transit along\n      network paths.\n\n   which in our example would imply an age value of (a+b+c+d).\n\nYou are reading this without paying attention to the clear statement\nin the previous paragraph that \"the Age header value is the sender's\nestimate of the  amount of time since the response was generated at\nthe origin server.\"  The word \"estimate\" there on purpose.  The\nphrase \"In essence\" was meant to reinforce that, but I guess it\ndidn't help enough.  If you prefer, we can rewrite that statement\nas \n      In essence, the Age value is the sum of the time that the response\n      has been resident in each of the caches along the path from the\n      origin server, plus the amount of time it has been in transit along\n      network paths, plus a bound on the estimation error.\n\nAll of the points I make above are minor details; the real question\ncomes up later in Roy's draft.\n\nIn section 4, \"Analysis of Option B\", Roy carefully analyzes the\npossibility that Option B might overestimate the Age value.  He\nfinds that there is no chance for an overestimate.  But he doesn't\ninclude here any analysis of possible *underestimates*.  In the\nfollowing section, he does concede that these are possible.  He\nthen dismisses the scenario as \"uncompelling\" because of the number\nof caches involved.  (I noticed that he doesn't point out that\nin order to get a significant overestimate from the Option A\nanalysis, one also needs a long chain of caches.)\n\nIn section 5, Roy gets confused about what \"conservative\" means.\nHe states:\n\n   The only argument voiced against Option B is that the calculation is\n   \"less conservative\" than Option A, and that being \"conservative\" is\n   better in order to \"reduce as much as possible the probability of\n   inadvertently delivering a stale response to a user.\"\n   \n   If \"conservative\" means \"always overestimates more than the other\n   option\", then the argument is certainly true.  However, if the\n   purpose of Age was to provide an overestimate, then why stop there?\n   Why not add arbitrary amounts of age to forwarded response, just in\n   case?  Why not disable caching entirely?\n\nBut apparently fails to notice that the current language does\nnot do any of these foolish things, nor has anyone advocated\nthat the HTTP/1.1 spec do any of these (as far as I know.)\n\nIt shouldn't be necessary to state, but perhaps it has to be\nmade clear, that the rules in section 14.6 are not there to\nmake the Age estimate arbitrarily high.   They are there to\nmake the Age estimate as accurate as possible WITHOUT UNDERESTIMATING\nthe value.\n\nWe can still disagree whether an underestimate or an overestimate\nis preferrable.  I'll simply quote from what I wrote in August,\nin http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0439.html :\n\n    Underestimation is SERIOUSLY BAD, because it will lead to a cache\n    believing that a response is fresh when it is, in fact, stale.\n\n    Overestimation of the Age can lead to a cache treating a fresh\n    response as stale, which can cause extra revalidation messages.\n    This is somewhat inefficient, but will never lead to a client\n    inadvertently seeing an expired cache entry.  Underestimation is\n    thus a much worse error than overestimation, and so the spec is\n    designed to avoid underestimation as assiduously as possible.\n\nAll arguments about whether caching are good or bad for the Internet\nare moot, if origin servers disable caching because their clients\nare unwittingly seeing stale responses.\n\nPhilosophical disagreements aside, Roy apparently ignores (or\nhas forgotten about) an alternative that Koen proposed in August,\nand which I modified slightly; see\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0456.html\n\nThe proposal is that an HTTP/1.1 proxy cache:\n     (1) MUST add Age when serving from cache memory\n     (2) MUST add Age when relaying a response from a pre-1.1 source\n     (3) SHOULD NOT add Age when relaying a response from a 1.1 or\n higher source\n\nRule #1 is apparently OK with Roy, since this corresponds exactly\nto his preferred option B.  Rule #3 is presumably also acceptable\nto Roy, since this corresponds to his Option B in an all-HTTP/1.1\nenvironment.  (We could quibble about whether this should be a SHOULD\nNOT or a MUST NOT, but my understanding is that our principle is\n\"do not overconstrain the implementor\", which leads to SHOULD NOT.)\n\nKoen suggested that \"source\" might be replaced with \"proxy cache\";\nI can't remember whether we ever resolved this minor point.\n\nRule #2 addresses the \"only case in which Option A *might* result in a\nbetter estimation than Option B\", a case that Roy admits might exist\nbut which he considers rare.  It completely solves the problem that\nI am worried about, and without causing any trouble in an all-HTTP/1.1\nchain.  As Roy points out:\n\n   [Option] A would overestimate the age on all HTTP/1.1\n   requests, even when there are no longer any HTTP/1.0 proxies.\n\nbut with the revised proposal, this problem no longer exists.\n\nAs far as I can tell from the mailing list archives, Roy didn't\naddress this proposal when I made it.  I brought a set of slides\nto the December IETF meeting to discuss this issue, but (if my\nmemory serves) that discussion didn't end up on the agenda.  Anyway,\nI suggest that this is the proposal that Roy should be analyzing,\nnot the language in RFC2068.\n\nTo provide a specific replacement wording for section 14.6, Age\n\n   If a cache receives a value larger than the largest positive integer\n   it can represent, or if any of its age calculations overflows, it\n   MUST transmit an Age header with a value of 2147483648 (2^31).\n   An HTTP/1.1 proxy cache MUST send an Age header in every response,\n   except that HTTP/1.1 proxy caches SHOULD NOT add an Age header\n   to an HTTP/1.1 (or higher) response that is being forwarded\n   immediately.  Caches SHOULD use an arithmetic type of at least\n   31 bits of range.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "At 01:57 PM 3/27/97 PST, Jeffrey Mogul wrote:\n\n>If we ignore the issue of compatibility with HTTP/1.0 clients (just\n>for the sake of this particular speculation), I don't think the\n>implementation of my \"503 + Retry-After\" approach would be so bad.\n\nAssuming that in HTTP/1.0 with multiple connections, the client has no or\nvery little control over the serialization of the requests as this is a\nfunction of how fast the TCP connections gets established and what\nscheduling mechanism the server is using.\n\nUsing pipelining over a single TCP connection has changed this\nsignificantly as all requests now are explicitly serialized. As far as I\ncan see the trick we are looking for is a mechanism for the server to break\nthe serialization. This is also important to proxies, for example, where\nsome of the responses may be server out of the cache and some have to be\nretrieved over the network. Using a \"503 Retry After\" allows this but has a\ncouple of drawbacks:\n\n1) It does not provide the client with a mechanism to forbid the server to\nbreak the serialization\n\n2) Using a time out period will in most cases be empirical and may force\nthe client to try multiple times and in effect to do a polling on the\nresource before it can be returned\n\nAnother mechanism would be to let the client indicate whether the server\ncan play games with the serialization or not. In order not to confuse old\nproxies, I guess that it is safest to make it a Connection header token:\n\nGET /image_1 HTTP/1.1\nHost: microscape\nConnection: order=mixed\n\nGET /image_2 HTTP/1.1\nHost: microscape\nConnection: order=mixed\n\nThe server can then in the response indicate the order relative to the\nimplicit order indicated in the requests\n\nHTTP/1.1 200 OK\nConnection: order=2\nContent-Type: image/png\n...\n\n<image_2>\n\nHTTP/1.1 200 OK\nConnection: order=1\nContent-Type: image/png\n...\n\n<image_1>\n\nHowever, there is a problem with this - the Connection header can't have\nthis type of tokens. Section 14.10 says:\n\nThe Connection header has the following grammar:\n       Connection-header = \"Connection\" \":\" 1#(connection-token)\n       connection-token  = token\n\nI believe it should be\n\nconnection-token = token [ \"=\" ( token | quoted-string ) ]\n\nMaybe a nwe point for the issues list.\n\nThere may be an upper limit to this approach where if the server does not\nexpect to be able to produce a reply then a \"503 Retry After\" would be more\nappropriate.\n\nIn practice I don't think this will have a large impact for now as all Web\nservers as far as I know only accept requests serially on an incoming TCP\nconnection. However, changing this may in some cases lead to significant\nperformance improvements.\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-348\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: Age Header Field in HTTP/1.",
            "content": ">For the record, when I wrote this paragraph (and I did write it),\n>I should have written \"HTTP/1.1 proxy caches\", not \"HTTP/1.1 caches\".\n>My error; I was being sloppy.\n\nThe issue applies equally to server-side (gateway) caches; the wording\nI supplied is correct.\n\n>      The expiration mechanism applies only to responses taken from a cache\n>      and not to first-hand responses forwarded immediately to the\n>      requesting client.\n>\n>No, this is not a contradiction.  That paragraph from section 13.2.1\n>pertains to the decision about whether a response is stale or fresh.\n>This is quite distinct from any rules about how the inputs to that\n>decision are provided.\n\nThe expiration mechanism includes the Age header field, and that is how\nI read the section.\n\n>   and also directly contradicts the last paragraph of section 13.2.3 of\n>   RFC 2068 that states:\n>\n>      Note that a client cannot reliably tell that a response is first-\n>      hand, but the presence of an Age header indicates that a response\n>      is definitely not first-hand.\n>\n>This is indeed a contradiction; the Note in section 13.2.3 is erroneous.\n\nOn what basis do you make that claim?  I know that doesn't follow my\nintentions, and I know it doesn't follow Koen's intention when he asked\nfor the note to be added.  Your interpretation of Age is just wrong.\n\n>   However, in section 13.2.3 of RFC 2068, we also find\n>\n>      In essence, the Age value is the sum of the time that the response\n>      has been resident in each of the caches along the path from the\n>      origin server, plus the amount of time it has been in transit along\n>      network paths.\n>\n>   which in our example would imply an age value of (a+b+c+d).\n>\n>You are reading this without paying attention to the clear statement\n>in the previous paragraph that \"the Age header value is the sender's\n>estimate of the  amount of time since the response was generated at\n>the origin server.\"  The word \"estimate\" there on purpose.  The\n>phrase \"In essence\" was meant to reinforce that, but I guess it\n>didn't help enough.  If you prefer, we can rewrite that statement\n>as \n>      In essence, the Age value is the sum of the time that the response\n>      has been resident in each of the caches along the path from the\n>      origin server, plus the amount of time it has been in transit along\n>      network paths, plus a bound on the estimation error.\n\nOh, come on Jeff.  \"estimate\" is not a synonym for \"inaccurate\", and\n\"in essence\" is not an invitation to ignore the rest of the sentence.\nFurthermore, the equations I give for Option A conclusively demonstrate\nthat there is no bound on its estimation error.\n\n>All of the points I make above are minor details; the real question\n>comes up later in Roy's draft.\n>\n>In section 4, \"Analysis of Option B\", Roy carefully analyzes the\n>possibility that Option B might overestimate the Age value.  He\n>finds that there is no chance for an overestimate.  But he doesn't\n>include here any analysis of possible *underestimates*. \n\nI put that in Section 5, as you know.\n\n>In the\n>following section, he does concede that these are possible.\n\nOf course, since underestimation is also possible with Option A.\nThe only case I included is the only one that is relevant to the\ndiscussion: the only condition under which Option A will not\nunderestimate and Option B will underestimate.\nIn all other cases, Option B will be more accurate than\nOption A and either not underestimate the age, or underestimate it\nno worse than does Option A.\n\n>He\n>then dismisses the scenario as \"uncompelling\" because of the number\n>of caches involved.\n\nAND the fact that all other cases, which are the only ones likely to\noccur in practice, are not helped whatsoever by Option A.\n\n>(I noticed that he doesn't point out that\n>in order to get a significant overestimate from the Option A\n>analysis, one also needs a long chain of caches.)\n\nLong chain?  Try one proxy cache and one user agent cache.  A longer\nchain increases the scope of the error, but the error would affect\nexisting proxy configurations.\n\n>In section 5, Roy gets confused about what \"conservative\" means.\n\nI used the exact words you gave in a response to http-wg.\n\n>He states:\n>\n>   The only argument voiced against Option B is that the calculation is\n>   \"less conservative\" than Option A, and that being \"conservative\" is\n>   better in order to \"reduce as much as possible the probability of\n>   inadvertently delivering a stale response to a user.\"\n>   \n>   If \"conservative\" means \"always overestimates more than the other\n>   option\", then the argument is certainly true.  However, if the\n>   purpose of Age was to provide an overestimate, then why stop there?\n>   Why not add arbitrary amounts of age to forwarded response, just in\n>   case?  Why not disable caching entirely?\n>\n>But apparently fails to notice that the current language does\n>not do any of these foolish things, nor has anyone advocated\n>that the HTTP/1.1 spec do any of these (as far as I know.)\n\nOf course not -- the point is that the purpose of Age is not to\nprovide an overestimate.  It is to provide an estimate.\n\n>It shouldn't be necessary to state, but perhaps it has to be\n>made clear, that the rules in section 14.6 are not there to\n>make the Age estimate arbitrarily high.   They are there to\n>make the Age estimate as accurate as possible WITHOUT UNDERESTIMATING\n>the value.\n\nThat is what Option B does as well, and as effectively as Option A.\n\n>We can still disagree whether an underestimate or an overestimate\n>is preferrable.  I'll simply quote from what I wrote in August,\n>in http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0439.html :\n>\n>    Underestimation is SERIOUSLY BAD, because it will lead to a cache\n>    believing that a response is fresh when it is, in fact, stale.\n>\n>    Overestimation of the Age can lead to a cache treating a fresh\n>    response as stale, which can cause extra revalidation messages.\n>    This is somewhat inefficient, but will never lead to a client\n>    inadvertently seeing an expired cache entry.  Underestimation is\n>    thus a much worse error than overestimation, and so the spec is\n>    designed to avoid underestimation as assiduously as possible.\n>\n>All arguments about whether caching are good or bad for the Internet\n>are moot, if origin servers disable caching because their clients\n>are unwittingly seeing stale responses.\n\nI personally would ignore Age if it does not represent a lower bound.\nThe clock skew dependencies introduced by Option A are impossible to\nwork around, whereas the clock skew dependencies introduced by Option B\ncan be fixed by the recipient.  Option A makes the reliability of the\nage calculation dependent on the clock of every recipient that touches\na message, which is an unacceptable level of error in a system that\ndepends on accurate caching to reduce network costs.\n\n>Philosophical disagreements aside, Roy apparently ignores (or\n>has forgotten about) an alternative that Koen proposed in August,\n>and which I modified slightly; see\n>http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0456.html\n>\n>The proposal is that an HTTP/1.1 proxy cache:\n>     (1) MUST add Age when serving from cache memory\n>     (2) MUST add Age when relaying a response from a pre-1.1 source\n>     (3) SHOULD NOT add Age when relaying a response from a 1.1 or\n> higher source\n>\n>Rule #1 is apparently OK with Roy, since this corresponds exactly\n>to his preferred option B.  Rule #3 is presumably also acceptable\n>to Roy, since this corresponds to his Option B in an all-HTTP/1.1\n>environment.  (We could quibble about whether this should be a SHOULD\n>NOT or a MUST NOT, but my understanding is that our principle is\n>\"do not overconstrain the implementor\", which leads to SHOULD NOT.)\n\nI ignored it because there is no need for such a compromise.  There is\nonly one option that retains the definition of Age such that the age\ncalculation results in a reliable estimate, and that is Option B.\nWhether or not the message has an HTTP-version of HTTP/1.1 has no\nrelevance to the clock skew between the recipient and the origin server,\nand the only difference between Option A and Option B in terms of \nunderestimating received age is that the former relies on everyone\nelse's clock being more accurate than the recipient's clock.\nThe compromise Age would still be unreliable in the presence of any\nHTTP/1.0 sender, and therfore cannot be relied upon by cache implementers,\nand therefore is not preferred to the original definition of Age that\nis described by Option B.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation, features, and related item",
            "content": "On Oct 21,  8:38pm, Koen Holtman wrote:\n\n> ><draft-ietf-http-feature-reg-02.txt>\n> >  Feature Tag Registration Procedures\n> >   Action: This document includes both the 'registration procedure'\n> >     and also the definition of some initial feature tags. These\n> >     should be separated. Ted Hardie is working on this.\n>\n> ???  The above document does not define any initial feature tags, so\n> there is no need to work on separating them.  Or am I missing\n> something obvious here?\n>\n>\n> Koen.\n>-- End of excerpt from Koen Holtman\n\nKoen,\nIn some talks with Andy Mutz and Larry Masinter I mentioned\nthat I thought that it would be easier to get IANA support if we kept\nthe registration procedures draft to just those things which IANA\nwould need to check off when they got a feature to register.  This was\nbased on feedback I got from Jon Postel and Joyce Reynolds when I was\nwriting up registration procedures for SOIF template types (part of\nthe CIP working going on in the FIND working group).  My take on it is\nthat they prefer the registration procedures documents to be revisable\nwithout forcing a revision of the technical specification.  This means\nparing down the draft to just the procedures and putting the other\nsections somewhere else.  That led me to volunteer to Andy and Larry\nthat I would try to split the draft up, so that we could see which\nbits fell where.  I have not yet had the chance to sit down and really\nwrite up the result, but my current sense is that Section 1, Section\n2.4, Section 3.1.*, and Sections 3.[5,6,8] are the ones containing the\nprocedural elements.  I meant to send off a note with the results\nas soon as I had gotten the drafts written, but that has taken longer\nthan I anticipated.  My apologies if the delay has caused any confusion.\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation, features, and related item",
            "content": "At 20:38 21/10/97 +0200, Koen Holtman wrote:\n>Larry Masinter:\n>>\n>[...]\n>>Here's a status update and proposed direction for the various drafts:\n>\n>[...]\n>\n>><draft-ietf-http-feature-reg-02.txt>\n>>  Feature Tag Registration Procedures\n>>   Koen Holtman, TUE\n>>   Andrew Mutz, Hewlett-Packard\n>>   July 28, 1997\n>>   Target: BCP (covering just registration)\n>>   Status: Drafted, comments received on HTTP list, revised.\n>>   Action: This document includes both the 'registration procedure'\n>>     and also the definition of some initial feature tags. These\n>>     should be separated. Ted Hardie is working on this.\n>\n>???  The above document does not define any initial feature tags, so\n>there is no need to work on separating them.  Or am I missing\n>something obvious here?\n\nI think that was a misinterpretation of something I didn't state very\nclearly when I pulled together the document summary information.\n\nWhat my summary meant was not specific initial feature tags, but the\ndescription of the feature tag structure (name space and data types, etc.).\n This would include some of the material in sections 2, 3.1, 3.2, 3.8 of\nyour original draft.\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Adminstrivia: spam reductio",
            "content": "Folks,\n\nDue to the large amount of spam hitting the http-wg mailing list, I propose to \nmake it closed to postings from non-subscribers unless I hear objections to\nthe contrary.\n\nWhat this will mean will be that if a posting arrives from an e-mail address\nthat isn't on a list of acceptable addresses, it will be sent to the list\nadministrator (me) instead of being sent to the list.  I will then either drop\nit in the bit-bucket (for spam) or send a message to the poster telling them\nhow to post to a closed list (but see below).\n\nThe initial list of acceptable addresses are the addresses that you folks\noriginally subscribed to the http-wg list with.  If you want to know your\nsubscription address, just ask me.\n\nWhat I can do to make this process a little less painful is to keep an\nadditional list of alternative posting addresses for people whose posting\naddresses are always different from their subscription address.\n\nFor a _small_ while (like about a week or so), I'll attempt to augment this\nadditional list based on the messages that I see bouncing, and then re-submit\nthe bounced message to the list.\n\nAfter that time, you can either unsubscribe and re-subscribe to the list from\nyour new posting address, or you can send me personal e-mail and ask for an\nalternative posting address to be added for you.\n\nQuestions, comments, objections... feel free to e-mail me.\n\n(The above functionality assumes that I can get Procmail/SmartList to work as\nadvertised.  If it fails, all bets are off and we can all suffer spam.)\n--\n(http-wg list maintainer)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Re: Adminstrivia: spam reductio",
            "content": "I prefer STRONGLY that you keep the function of being added to the\nadditional list as a permanent service.\n\nYou may even consider automating it :-)\n\nThere are all sorts of reasons why people post from addresses other\nthan their listed address, including:\n\n- Subaddressing (the + convention)\n- Travelling\n- Multiple accounts\n- Broken mailers that include hostname of workstation\n\nAnd so on.\nApart from that, I have no objections, just a sad feeling :-(\n\n                   Harald T. Alvestrand\n                     Apps AD\n\n\n\n"
        },
        {
            "subject": "Re: Adminstrivia: spam reductio",
            "content": "Harald writes:\n\n> I prefer STRONGLY that you keep the function of being added to the\n> additional list as a permanent service.\n\nMaybe i wasn't too clear.\n\nYou can send me an e-mail at any time to have additional posting addresses\nadded.  The service I was offering for a week or so was noticing failed\npostings and auto-adding the posters e-mail address to the additional posting\naddress list.\n\nI don't have the time nor the energy to do this for a longer period.  ;-)\n\n> You may even consider automating it :-)\n\n;-)\n\n[...]\n> Apart from that, I have no objections, just a sad feeling :-(\n\nThanks for your input.\n-- \n-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation, features, and related item",
            "content": "Ted Hardie:\n>\n[....]\n>Koen,\n>In some talks with Andy Mutz and Larry Masinter I mentioned\n>that I thought that it would be easier to get IANA support if we kept\n>the registration procedures draft to just those things which IANA\n>would need to check off when they got a feature to register.  This was\n>based on feedback I got from Jon Postel and Joyce Reynolds when I was\n>writing up registration procedures for SOIF template types (part of\n>the CIP working going on in the FIND working group).  My take on it is\n>that they prefer the registration procedures documents to be revisable\n>without forcing a revision of the technical specification.  This means\n>paring down the draft to just the procedures and putting the other\n>sections somewhere else.  That led me to volunteer to Andy and Larry\n>that I would try to split the draft up, so that we could see which\n>bits fell where.  I have not yet had the chance to sit down and really\n>write up the result, but my current sense is that Section 1, Section\n>2.4, Section 3.1.*, and Sections 3.[5,6,8] are the ones containing the\n>procedural elements.  I meant to send off a note with the results\n>as soon as I had gotten the drafts written, but that has taken longer\n>than I anticipated.  My apologies if the delay has caused any confusion.\n>regards,\n>Ted Hardie\n>NASA NIC\n\nHi Ted,\n\nBefore you start editing, here is the reasoning behind the current\nstructure of the draft.  \n\nFor section 3, the entire structure, and much of the wording, was\nlifted from the MIME registration document (RFC 2048), and you should\ndefinately look at that document before you start taking section 3\napart.  \n\nSections 2.[123] explain feature tags to end users, and give some\nguidelines on the appropriate use of feature tags by negotiation\nprotocols.  Note that there is no reference to a specific negotiation\nprotocol in the draft.  If there were, that reference could replace\nthe explanations in sections 2.[123], but there is no such reference.\nThe draft creates a shared namespace for potentially many\nprotocols/mechanisms without mentioning a single specific protocol or\nmechanism.  This is a strange thing to do, but it certainly guarantees\nthe desired decoupling between revising registration procedures and\nrevising specific protocols. \n\nKoen.\n\n\n\n"
        },
        {
            "subject": "HTTP Interoperability Test 103",
            "content": "  The third HTTP/1.1 Multi-Vendor Internet Test Day is Oct 30.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Oct 28.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  If you participated in either of the first two days and would like\n  me to just resend that information, just let me know.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-state-man-mec04.txt,.p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP State Management Mechanism (Rev1)\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-04.txt,.ps\nPages: 19\nDate: 27-Oct-97\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nNOTE: \n\n     Discussion of State Management in the HTTP Working Group has\n     stalled because the issues of the basic functioning of the\n     protocol were intertwined with concerns about the nature of\n     the restrictions placed on it to ensure user privacy.  At the\n     request of the Working Group chair, the discussion of protocol\n     restrictions for user privacy has been temporarily removed\n     from this draft, in order to focus the discussion on the basic\n     protocol elements.  Any Internet Standard for State Management\n     must address security and privacy issues.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-04.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-man-mec-04.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-04.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-jaye-trust-state01.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP Trust Mechanism for State Management\nAuthor(s): D. Jaye\nFilename: draft-ietf-http-jaye-trust-state-01.txt\nPages: 10\nDate: 27-Oct-97\n\nThis document specifies an addition to the state management protocol\nspecified in draft-ietf-http-state-man-mec-03[Kristol].  The intent is\nto provide a mechanism that allows user agents to determine the privacy\npractices of a server and to accept or reject cookies based on those\npractices.  Allowing the user to establish preferences for how to handle\ncookies based on the server's practices provides a practical mechanism \nto provide users control over the privacy implications of cookies.\n \nTo provide verification of server privacy practices, we assume the\nexistence of one or more independent Trust Authorities.  The authority\nestablishes PICS ratings representing server privacy practices. It then\nissues trust-labels, in the form of digitally signed PICS labels, to\norganizations for specific domains and paths based on the server privacy\npractices.  The Trust Authority must be able to audit domains to\nverify their adherence to a given level.  Passing these trust-labels\nalong with cookies allows the user agent to support cookie handling \npreferences based on trusted privacy practices.\n \nThis document describes how PICS-headers are used in conjunction with\nSet-Cookie or Set-Cookie2 headers in [Kristol] to provide trust-labels\nto communicate the privacy practices of servers regarding cookies.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-jaye-trust-state-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-jaye-trust-state-01.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-jaye-trust-state-01.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "New ID for Http Jaye Trust State Mg",
            "content": "Here is the revision to the HTTP Jaye Trust State Mgt draft.\n\n \n\nSummary of Changes:\n\nTrust-label headers no longer apply to the preceding Set-Cookie \nheader.  Instead, they either apply to all cookies in the server \nresponse or to specific cookies listed in an extension to the PICS \nlabel.  This PICS Label extension is a mandatory extension and is also \nused to identify the label as a trust label and to make legacy or \ndocument label parsers ignore the label.  The extension definition is \ncurrently going through the Note submission process at the W3C.\n\nMatching rules for trust-labels and cookies were clarified as well as \nthe process for verifying the\ndigital signature against the plaintext trust-label.\n\nIncorporated various corrections and suggestions from Dave Kristol.\n\nOf course this draft (submitted last week) refers to the (now \nsuperceded) version of Dave Kristol's State Mgt Mech draft v03 instead \nof v04 which was released today.\n\n\nDaniel Jaye                                     djaye@engagetech.com\nChief Technology Officer                             v(508) 684-3641\nEngage Technologies                                  f(508) 684-3636\n100 Brickstone Square, 1st Floor, Andover, MA  01810\n\n\n\n\n\n\n\n\n\n\n\nHTTP Working Group                                           Daniel Jaye\nINTERNET DRAFT                                       Engage Technologies\n\n\n<draft-ietf-http-jaye-trust-state-02.txt>\nOctober 23, 1997                                Expires April 23, 1998\n\n\n       HTTP Trust Mechanism for State Management\n\n\n             Status of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are\n   working documents of the Internet Engineering Task Force\n   (IETF), its areas, and its working groups.  Note that other\n   groups may also distribute working documents as Internet-\n   Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time.  It is inappropriate to use Internet-\n   Drafts as reference material or to cite them other than as\n   ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please\n   check the ``1id-abstracts.txt'' listing contained in the\n   Internet- Drafts Shadow Directories on ftp.is.co.za (Africa),\n   nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n   ds.internic.net (US East Coast), or ftp.isi.edu (US West\n   Coast).\n\n   This is author's draft 2.06.\n\n\nABSTRACT\n\nHTTP TRUST MECHANISM PROPOSAL FOR STATE MANAGEMENT\nOctober 23, 1997\n\n1. ABSTRACT\n\nThis document specifies an addition to the state management protocol\nspecified in draft-ietf-http-state-man-mec-03[Kristol].  The intent is\nto provide a mechanism that allows user agents to determine the privacy\npractices of a server and to accept or reject cookies based on those \npractices.  Allowing the user to establish preferences for how to handle\ncookies based on the server's practices provides a practical mechanism \n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 1]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\nto provide users control over the privacy implications of cookies.\n\nTo provide verification of server privacy practices, we assume the\nexistence of one or more independent Trust Authorities.  The authority \nestablishes PICS ratings representing server privacy practices. It then\nissues trust-labels, in the form of digitally signed PICS labels, to \norganizations for specific domains and paths based on the server privacy\npractices.  The Trust Authority must be able to audit domains to \nverify their adherence to a given level.  Passing these trust-labels \nalong with cookies allows the user agent to support cookie handling \npreferences based on trusted privacy practices.\n\nThis document describes how PICS-headers are used in conjunction with\nSet-Cookie or Set-Cookie2 headers in [Kristol] to provide trust-labels\nto communicate the privacy practices of servers regarding cookies.\n\n\n2. TERMINOLOGY\n\nThe terms user agent, client, server, proxy, and origin server have the\nsame meaning as in the HTTP/1.1 specification [RFC 2068].  The terms \ndomain-match, verifiable transaction, and unverifiable transaction are \ndefined in [Kristol], and those definitions are also used here.\n\nThe term trust-label is used to mean a PICS label [PICS] used to \ncommunicate the cookie-related privacy practices of a server.\nThe term Trust Authority refers to the PICS label rating service for\ntrust-labels who may issue digitally signed trust-labels to domains.\n\n\n3. OUTLINE\n\nThe server sends a Set-Cookie and/or a Set-Cookie2 header to the user \nAgent along with a PICS-Label header containing the trust-label.  The \nuser agent may then use that information to guide the acceptance or \nrejection of the cookie.  If the trust-label has a digital signature,\nthe user agent may use the well-known public key of the Trust \nAuthority to decrypt the signature of the trust-label to verify the \nidentity and practices of the server and scope of the trust-label.\n\n3.1  Syntax: General\n\nThis specification describes how the PICS-Label header, described in \n[PICS], is used to convey the privacy practices of the server to the \nuser-agent The new PICS-Label header syntax is specified below:\n\ntrust-label     = \"PICS-Label:\" labellist\n\nThe header is recognized as a trust-label by the existence of the \ncookieinfo extension.  This trust-label applies to cookies in the \nresponse that are compatible (as described in section 3.3.1) with \nthe domain and path of the \"for\" labelattr of the PICS-Label header.  \n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 2]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\nThe specific cookies are listed in the cookieinfo extension to the \nPICS label or to all compatible cookies if no cookies listed in the \ncookieinfo extension.  \"labellist\" is as specified in the PICS 1.1 label\nsyntax in [PICS], except for the following changes:\n    an extension to include a list of the specific cookies to \n        to which the trust-label applies;\n    an optional extension according to the digital signatures \n        working draft [DSIG];\n    the optional label attributes \"by\" \"gen\" \"for\" \"on\" and \"exp\" \n        are required. \n\nThe modified PICS label syntax is listed here.  \n\nlabellist       = \"(\" version 1*service-info \")\"\nversion         = \"PICS-1.1\"\nservice-info    = serviceID \"label\" 1*label\nserviceID       = quotedURL\nlabel           = labelattr \"ratings\" \"(\" privacy-practice \")\" \n                  cookieinfo\n                  [sigblock]\nlabelattr       = \"by\" quotedname\n                  \"gen\" boolean\n                  \"for\" quotedURL\n                  \"on\" quoted-ISO-date\n                  \"exp\" quoted-ISO-date\nprivacy-practice  \n                = \"noexchange 1\"\n                | \"anonymousexchange 1\"\n                | \"noshare 1\"\n                | \"thirdpartyexchange 1\"\n                | rating\ncookieinfo      = \"extension\" \"(\" \"mandatory\" <\"> \n            \"http://www.w3.org/PICS/extensions/cookieinfo-1_0.html\"\n                  <\"> *cookiename \")\"\ncookiename      = NAME\n\n\"quotedname\", \"quotedURL\", \"rating\",  and \"quoted-ISO-date\" are as \n    defined in the PICS specification [PICS].  \nServiceID references a quoted URL that defines and describes the rating \n    service and references the rating system.  \n\"for\" is the URL or root URL for which this label applies.  \n\"by\" is the email address of the issuing trust authority.\nThe \"gen\" boolean indicates whether the label is generic to the web site\n    or for a specific page.  A value of \"True\"  indicates that the label\n    is generic for all cookies with a Path attribute for which the path \n    component of the URL in the \"for\" attribute is a prefix.\n\"on\" is the date the label was issued.  \n\"exp\" is the date the label expires.  \n\"mandatory\" in cookieinfo causes legacy browsers to ignore the label.\ncookiename is the \"NAME\" of each cookie to which this label applies.\nsigblock is the digital signature extension as described in the digital \n    signature working draft[DSIG].  \n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 3]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\nThe sigblock must contain the SigCrypto token within the SigData block.\nThe SigCrypto token must contain the encrypted trust-label-data \ndescribed below.\n\ntrust-label-data = labelattr-data privacy-practice [cookielist]\nlabelattr-data   = gen-boolean for-URL exp-date\ngen-boolean      = boolean\nfor-URL          = quotedURL\nexp-date         = quoted-ISO-date\n\n\"gen-boolean\", \"for-URL\", and \"exp-date\" refer to the values of the \n\"gen\", \"for\", and \"exp\" attributes in the \"labelattr\" section.  \n\"cookielist\" refers to the list of cookie names in the cookieblock \nextension.\n\nFour well-known privacy-practice values are described here to provide \nrecognized values that should be handled by user agents. \n\nThe \"noexchange 1\" rating indicates that the Trust Authority has \nverified that the server will not use the cookie to collect persistent\nuser information.\n\nThe \"anonymousexchange 1\" rating indicates that the Trust Authority has\nverified that the server will not use the cookie to collect or transmit\npersonally identifying information (e.g., name, address, telephone \nnumber, email address, etc.) but may collect anonymous or aggregated \npersonal information (e.g., gender, geographic region, approximate age,\nderived data such as clickstream, etc.) or implicit information (such as\nweb usage patterns) as long as it will never be associated with\npersonally identifying information.  The server may collect IP Addresses\nbut they must not be associated with personally identifying information \nto be elegible for this rating.\n\nThe \"noshare 1\" rating indicates that the Trust Authority has verified\nthat the server may use the cookie to collect or transmit personally \nidentifying information (e.g., name, address, telephone number, email \naddress, etc.) but will never share that information with companies \nother than the company to which the user provided the information.  \n\nThe \"thirdpartyexchange 1\" rating indicates that the Trust Authority has\nverified that the server may use the cookie to collect or transmit \npersonally identifying information (e.g., name, address, telephone \nnumber, email address, etc.) and may share that information with\nthird parties. \n\nAll other items above are as described in the PICS label syntax [PICS] \nor in the Digital Signatures working draft [DSIG].\n\n3.2Server Role\n\nA server communicates its privacy practices by sending an unsigned or\nsigned trust-label in the same response as the cookie header(s).\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 4]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\nAny server wishing to provide a digitally signed trust-label must \nrequest the label from a Trust Authority.  The Trust Authority must have \nthe ability to evaluate the server and determine the trust rating\nfor which a label will be issued. That evaluation takes place outside\nthe protocol described here, as does the actual granting of the label\nto the origin server.\n\nThe labels should expire no more than thirteen months and no less than \none month after they are issued.  The server should store the trust \nlabels and only request a new trust-label from the Trust Authority when \nthe current trust-label is about to expire.\n\n3.3 User Agent Role\n\nThe user agent receives a cookie headers and trust-labels from\nan origin server.\n\n3.3.1 Interpreting the trust-label\nUser agents interpret cookies as described in RFC 2109.  In addition \nto the cookie attributes, the user agent must now interpret the \ntrust-labels as well.  If the user agent receives a PICS label with a \nserviceID from a recognized label service for trust-labels, it is \nassumed to be a trust-label for all \"compatible\" cookies, as defined \nbelow.\n\nA trust-label and a cookie are defined as \"compatible\" if the following \nconditions are met:\n1) The domain portion of the URL specified in the \"for\" attribute of\n   labelattr domain-matches the Domain attribute of the cookie\n   response header, according to the matching rules in [Kristol].  \n2) The path portion of the URL specified in the \"for\" attribute of\n   labelattr is either a), a prefix of the Path attribute of the cookie \n   if the trust-label is generic or, b), an exact match with the Path \n   attribute of the cookie if the trust-label is not generic.\n \nIf the cookieinfo extension does not contain any cookie names, then \nthe trust-label applies to all cookies in the response that are \ncompatible.\n\nA trust-label is ignored if the \"exp-date\" attribute of labelattr\nis less than or equal to the current date.\n\nTo help verify the trustworthiness of the server, the user agent may \nlook for a digital signature and use the Trust Authority's well known \npublic key to decrypt the trust-label-data from the SigCrypto term.\n\nThe user agent obtains that public key outside this protocol.  Given \nthat we expect only a few well-known Trust Authorities, the user agent \nimplementer should cache public keys from standard trust authorities \nto avoid extra network traffic.\n\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 5]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\nThe labelattr-data, privacy-practice, and cookielist in the decrypted \ntrust-label-data from the sigblock must match the plaintext labelattr,\nprivacy-practice, and cookielist for the signature to be valid.\n\nIf the digital signature is invalid, then the trust-label should be \nignored and the cookie should not be set.\n\nIf the user agent is set to accept all cookies then all trust-label \nprocessing can be skipped.\n\n3.3.2Accepting or rejecting Cookies\n\nIn addition to the rules for rejecting cookies specified in [Kristol], a\nuser or a user-designated agent should be able to designate preferences\nfor accepting or rejecting cookies based on the privacy-practice of the\nserver, whether the transaction is verifiable or unverifiable, and\nwhether the privacy-practice is signed by a recognized Trust Authority.\n\nFor example, a user may have a preference to accept all cookies from \nverifiable transactions or rated \"anonymousexchange 1\" and signed by a \nrecognized Trust Authority.\n\nUser agents should have the following default preferences:\n  \"noexchange 1\", \"anonymousexchange 1\", and \"noshare 1\" rated cookies \n    from verifiable transactions are accepted;\n  \"noexchange 1\" and \"anonymousexchange 1\" rated cookies from\n    unverifiable transactions are accepted;\n  \"thirdpartyexchange 1\" cookies from unverifiable transactions are \n    rejected.\n\n3.3.3  User intervention\nThe user agent may prompt the user to verify that it wishes to reject a\ncookie in conditions where the cookie is being rejected based on\na default preference or no preference applies.\n\nUser agents that solicit user input for cookie handling may wish to \ndisplay the URL of the rating service to better inform the user of the \nmeaning of the privacy ratings for the server.\n\n3.3.4  Cookie request header syntax\nThe syntax for the Cookie request header has not been modified.\n\n3.4  Trust Authority Role\n\nThe Trust Authority referred to in this document must be a neutral third\nparty that can be trusted to accurately characterize the privacy\nbehavior of web sites.  The issuing of trust-labels occurs outside the\nscope of this protocol.  However, the protocol depends on user trust in\nthe Trust Authority.  The Trust Authority must understand the scope to\nwhich a trust-label applies to ensure that for all situations in which \nthe trust-label would be deemed to be applicable, the server(s) are in \nfact operating in accordance with the specified privacy rating.\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 6]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\n3.4.1 Issuing trust-labels\nOn receiving a request for a signed trust-label, the authority should \nverify the privacy practices of the site requesting the trust-label and \nissue the appropriate trust-label.  To issue the trust-label, the Trust \nAuthority assembles the trust-label-data, it canonicalizes whitespace \nfor the trust-label-data, and it encrypts the trust-label-data for the \nsite request using its private key and the algorithm specified in the \nattribution of the digital signature.  The encryption method must be a\npublic-private key pair with a well-known public key to eliminate \nround-trips to the Trust Authority. \n\n3.4.2 Revocation of trust-labels\nTrust-labels must have expiration dates.  When a trust-label is issued,\nthe Trust Authority must receive agreement from the requesting\norganization that the privacy practices for which the trust-label was \nassigned will be maintained until the trust-label expires, the domain \nbecomes inactive, or those cookies are no longer set or examined by the\norganization's servers.\n\n3.4.3 Discovery of privacy-practice ratings\nPrivacy-practice ratings are defined in the PICS label rating system\nreferenced by the Trust Authority's label rating service.  One\nwell-known rating system is proposed in this document.\n\n\n4. EXAMPLES\n\n4.1 Example 1\n\n1.  User Agent preferences:\n\n    In this example, the user agent has a preference for automatically\n    accepting cookies from domains that have valid ratings of \n    \"anonymousexchange 1\" or \"noshare 1\".\n\n2.  User Agent -> Server\n\n      POST /acme/login HTTP/1.1\n      Host: www.acme.com\n      [form data]\n\n    User identifies self via a form.\n\n3.  Server -> User Agent\n      HTTP/1.1 200 OK\n      Set-Cookie2: Customer=\"WILE_E_COYOTE\"; Max-Age = 94608000; \n        Version=\"1\"; Path=\"/acme\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"auditor@aaa.org\" gen true\n        for \"http://www.acme.com/\"\n        exp \"1998.12.31T23:59-0000\" \n\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 7]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\n        extension\n          (mandatory \n            \"http://www.w3.org/PICS/extensions/cookieinfo-1_0.html\")\n        ratings (noshare 1))\n\n    A cookie that includes the user's identity and an unsigned trust \n    label header are sent back to the user agent with the request.  The\n    cookie is accepted because rating \"noshare 1\" is acceptable \n    according to the privacy preferences of the user agent.\n\n4.2 Example 2\n\n1.  User Agent preferences:\n\n    In this example, the user agent has a preference for automatically \n    accepting cookies that are rated \"noexchange 1\", \n    \"anonymousexchange 1\", or \"noshare 1\" or from cookies in \n    unverifiable transactions that are rated \"noexchange 1\" or \n    \"anonymousexchange 1\" by www.aaa.org.\n\n2.  User Agent -> Server\n\n      POST /acme/login HTTP/1.1\n      Host: www.acme.com\n      [form data]\n\n    User requests page with embedded IMG SRC reference to\n    \"http://www.roadrunnermaps.com/cgi-bin/maps?TER=deserts&FE=cliffs\"\n\n3.  Server -> User Agent\n      HTTP/1.1 200 OK\n      Set-Cookie2: Customer=\"0000000123\"; Max-Age = 94608000; \n        Version=\"1\"; Path=\"/birds\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"auditor@aaa.org\" gen true\n        for \"http://www.acme.com/\"\n        exp \"1997.12.31T23:59-0000\" \n        extension\n          (mandatory \n            \"http://www.w3.org/PICS/extensions/cookieinfo-1_0.html\")\n        ratings (noshare 1))\n\n    A Cookie reflecting the users identity is transmitted with an \n    unsigned trust-label back to the user agent.  The Cookie is accepted\n    by the user agent because the rating \"noshare 1\" is compatible with\n    the user agent privacy preference.\n\n4.  User Agent -> Server\n\n      GET cgi-bin/maps?TER=deserts&FE=cliffs HTTP/1.1\n      Host: www.roadrunnermaps.com\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 8]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\n    User requests an image via CGI script from a third party map \n    provider.  This is an unverifiable transaction.\n\n5.  Server -> User Agent (unverifiable transaction)\n      HTTP/1.1 200 OK\n      Set-Cookie2: Customer=\"0000000123\"; Max-Age = 94608000; \n        Version=\"1\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"auditor@aaa.org\" gen true\n        for \"http://www.roadrunnermaps.com/\"\n        exp \"1997.12.31T23:59-0000\" \n        extension\n          (optional \n            \"http://www.w3.org/PICS/extensions/cookieinfo-1_0.html\"\n            Customer)\n        extension \n          (mandatory \"http://www.w3.org/PICS/DSig/sigblock-1_0.html\"\n            (\"AttribInfo\" \n              (\"http://www.w3.org/PICS/DSig/X509.html\" \n                \"base64-x.509-cert\"))\n            (\"Signature\" \"http://www.aaa.org/trust.html\" \n              (\"byName\" \"aaapublickey\") \n              (\"SigCrypto\" \n                \"8E53B19D35A3F198930E5D815B235A38930E53FDA815B2158\")))\n        ratings (anonymousexchange 1))\n\n    A cookie containing the user's system generated id number is \n    transmitted with a signed label back to user agent.  The cookie is \n    accepted by user agent because a cookie rated \"anonymous 1\" in  \n    an unverifiable transaction signed by \"http://www.aaa.org\" is\n    acceptable to the user agent and the Customer Cookie\n\n\n5. SECURITY CONSIDERATIONS\n\n5.1 Revocation of trust-labels\n\nA site could receive a trust-label for a particular trust level rating \nand later change its policies before the trust-label has expired.  To \naddress this Trust Authorities should execute agreements with trust\nlabel recipients to provide legal remedies to discourage this behavior.\n\n5.2 False representation\n\nA site could state a privacy practice that it either intentionally or\nunintentionally does not follow.  If the trust-label is not signed by a\nrecognized trust authority, there is no independent verification of the\nsite's adherence to its stated privacy practice.\n\n\n\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt              [Page 9]\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt       October 23, 1997\n\n\n6. SUMMARY\n\nThis document presents an extension to the state management protocol \ndefined in RFC2109.  It describes only changes to that protocol. Any \nparts of the state management mechanism not explicitly described here \nare assumed to remain as defined in RFC 2109.\n\nThe protocol described here allows a user agent to verify that the \norigin server is using cookies in a manner consistent with the privacy \nexpectations of the user, by providing a trust-label which may be signed\nby a Trust Authority.\n\n\n7. ACKNOWLEDGEMENTS\n\nThis document represents contributions by Toby Bloom, as well as input \nfrom Dave Kristol, Yaron Goland, Jonathan Stark, and Dan Connolly.\n\n\n8. REFERENCES\n\n[PICS] Jim Miller et al, PICS Label Distribution Label Syntax and \nCommunication Protocols, Version 1.1, REC-PICS-labels-961031\nhttp://www.w3.org/PICS/labels.html\n\n[Kristol] Kristol, David M., Montulli, Lou, HTTP State Management \nMechanism (Rev 1).  \nInternet Draft <draft-ietf-http-state-man-mec-03.txt>\nftp://ietf.org/internet-drafts/draft-ietf-http-state-man-mec-03.txt\n\n[DSIG] Philip DesAutels et al, DSIG 1.0 Signature Labels, Version 1.0, \nWD-DSIG-label-970605\nhttp:/www.w3.org/TR/WD-DSIG-label.html/\n\n\n9. AUTHOR'S ADDRESS\n\nDaniel Jaye\nEngage Technologies\n100 Brickstone Square, 1st Floor\nAndover, MA 01810\ndjaye@engagetech.com\n978 684-3641 voice\n978 684-3636 fax\n\n\n\n\n\n\n\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-02.txt             [Page 10]\n\n\n\n"
        },
        {
            "subject": "proxycooki",
            "content": "There was a brief discussion a while ago about a potential proxy-cookie\nproposal.  I never saw anything come of it; did anything actually occur\n(off-line perhaps) and not make it into the WG?\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n"
        },
        {
            "subject": "spec-08 selfcontradiction in 13.5.",
            "content": "Section 13.5.2 of draft-ietf-http-v11-spec-08 contains a minor\nself-contradiction.  It first says that `A cache or non-caching proxy\nMUST NOT modify [...] Content-Length', and later says that `A cache or\nnon-caching proxy MAY modify or add [Content-Length] in a response\nthat does not include no-transform, [...]'.\n\nThe problem is that the MUST NOT above is too strong.  Here is a\nrewrite of the second half of section 13.5.2 which solves the problem.\n\n         [...]\n            A cache or non-caching proxy MUST NOT modify the\n            following field in a response:\n\n               . Expires\n\n            but it may add this field if not already present.  If an\n            Expires header is added, it MUST be given a field-value\n            identical to that of the Date header in that response.\n\n            A cache or non-caching proxy MUST NOT modify the folling\n            field in a response that contains the no-transform\n            Cache-Control directive, or in any request:\n\n               . Content-Length\n\n            A cache or non-caching proxy MAY modify or this field in a\n            response that does not include no-transform, but if it\n            does so, it MUST add a Warning 14 (Transformation applied)\n            if one does not already appear in the response.  A cache\n            MAY add this header if not already present.  If a\n            Content-Length header is added or modified, it MUST\n            correctly reflect the length of the entity-body.\n\n              Note: a typical reason for adding the Content-Length\n              header is that the origin server sent the content\n              chunked encoded.\n\n            A cache or non-caching proxy MUST NOT modify or add any of\n            the following fields in a response that contains the no-\n            transform Cache-Control directive, or in any request:\n\n               . Content-Encoding\n               . Content-Range\n               . Content-Type\n\n            A cache or non-caching proxy MAY modify or add these fields\n         [...]\n\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: New ID for Http Jaye Trust State Mg",
            "content": "I'm confused.  The draft that was announced via the IETF today was\ndraft-ietf-http-jaye-trust-state-01.txt, but your email announced (and\ncontains) draft-ietf-http-jaye-trust-state-02.txt.  How are they\nrelated?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: pipelining vs. deferred conten",
            "content": "It is a hell of a lot easier to just define a multiplexing wrapper\nfor HTTP/1.x messages and use the Upgrade header field to initiate\nthe wrapper on the first (normal) request.  Performing contortions\nwithin the scope HTTP/1.x is a waste of energy, since the result is\nno more compatible with HTTP/1.x than just doing it the right way.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: proxycooki",
            "content": "rlgray@raleigh.ibm.com wrote:\n> \n> There was a brief discussion a while ago about a potential proxy-cookie\n> proposal.  I never saw anything come of it; did anything actually occur\n> (off-line perhaps) and not make it into the WG?\n\nI'm unaware of anything happening, although I'd like to see such a\ncritter.\n\nI think the browser vendors' involvement and interest would be crucial\nhere to make any such proposal real.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: New ID for Http Jaye Trust State Mg",
            "content": "Understandable confusion.\n\nDave,\n\nThe draft that I emailed out from the IETF meeting in Munich (v01) was \nbounced when submitted (because of the rule prohibiting submissions \nuntil after the meeting was concluded).\n\nI submitted the new draft (v02) last week.  The ID administrator \nrenumbered v02 to v01 so it would sequentially follow the previous \n(official) version.  For simplicity, let's assume that the latest \nversion is v01 and ignore the (unofficial) version I emailed to the \nlist from Munich.\n\nApologies for the confusion.\n\n-Dan\n\n\n\n-----Original Message-----\nFrom:Dave Kristol [SMTP:dmk@bell-labs.com]\nSent:Tuesday, October 28, 1997 3:59 PM\nTo:Jaye, Dan\nCc:'http-state@lists.research.bell-labs.com'; \n'http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com'\nSubject:Re: New I-D for Http Jaye Trust State Mgt\n\nI'm confused.  The draft that was announced via the IETF today was\ndraft-ietf-http-jaye-trust-state-01.txt, but your email announced \n(and\ncontains) draft-ietf-http-jaye-trust-state-02.txt.  How are they\nrelated?\n\nDave Kristol\n\n\nDaniel Jaye                                     djaye@engagetech.com\nChief Technology Officer                             v(508) 684-3641\nEngage Technologies                                  f(508) 684-3636\n100 Brickstone Square, 1st Floor, Andover, MA  01810\n\n\n\n"
        },
        {
            "subject": "Administrivia: spam reductio",
            "content": "Folks,\n\nI've had no real objections, so as from next Monday (3rd Nov.) I'm going to\nalter the http-wg mailing list so that it only accepts postings from the\nsubscription addresses of people on the list.\n\nFor one week after that, I'll attempt to notice non-spam messages that got\nbounced, add the address of the poster and also re-post the message to the\nlist.\n\nAt any time, if you know that your posting address may be different from your\nsubscription address, or you want alternative posting addresses, just send me\ne-mail direct.\n\nIf this closed list strategy doesn't work too well, just let me know and I'll\nundo it.\n--\n(http-wg list maintainer)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Need HTTP proxy sourc",
            "content": "Hello,\n\nI need a *simple* HTTP proxy. I want to write a proxy which\ntranslates all HTML files from English into French. Do you\nhave some sourcecode I can use?\n\nThanks for reading this,\nRoland\n---\nRoland Polzer     Langenscheidts T1 - Machine Translation\nGesellschaft fuer multilinguale Systeme - Munich, Germany\n\n\n\n"
        },
        {
            "subject": "Question on byte range",
            "content": "Here is a question arising from the interoperability test.\n\nIf a server receives a byte range requests for a 100 byte file like\n\na)   Range: bytes = 0-10, 130-140\nor\nb)   Range: bytes = 30-20\nor\nc)   Range: bytes = 0-10, 30-20, 40-50\n\n\nwhat is the correct response?  \n\nIn case a) one of the two ranges is invalid, but the spec says send\na 416 only if *all* are invalid.  If the valid ones are returned with\na 206 then there is no way to signal an error.  \n\nIn case b) the spec says\n\n            If the last-byte-pos value is present, it must be greater\n            than or equal to the first-byte-pos in that byte-range-spec,\n            or the byte-range-spec is invalid. The recipient of an\n            invalid byte-range-spec must ignore it.\n\nIf the server ignores it then it should send what?\n\nCase c) is like b) except it is possible to send the valid ranges.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Another Question on byte range",
            "content": "On Fri, 31 Oct 1997, John Franks wrote:\n\n> Here is a question arising from the interoperability test.\n> \n\nAnd another one I forgot:\n\nrom section 14.17\n            A server sending a response with status code 416 (Requested\n            range  not valid) SHOULD include a Content-range field with\n            a content-range-spec of \"*\".\n\nWhat is the purpos of this?\n\nBUT 10.4.17 says about status 416:\n\n            When this status code is returned for a byte-range request,\n            the response MUST include a Content-Range entity-header\n            field specifying the current length of the selected resource\n            (see section 14.17).\n\nI don't think '*' specifies the length. On the other hand I see no\nway to make a Content-Range header which would be legal and would \nspecify the length.\n\nMaybe what is needed is to allow something like \n\n   Content-Range: bytes *-*/200\n\nwhen an illegal range (like 300-400) has been requested from a 200\nbyte file.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Administrvia: list statu",
            "content": "Folks,\n\nThe http-wg mailing list is NOW closed to postings from non-subscribers.\n\nAny problems, e-mail me directly.\n--\n(http-wg list administrator)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Re: byte range question",
            "content": "John Franks <john@math.nwu.edu> wrote on Fri, 31 Oct 1997 19:09:10 -0600 (CST):\n\n  > Here is a question arising from the interoperability test.\n  > \n  > If a server receives a byte range requests for a 100 byte file like\n  > \n  > a)   Range: bytes = 0-10, 130-140\n  > or\n  > b)   Range: bytes = 30-20\n  > or\n  > c)   Range: bytes = 0-10, 30-20, 40-50\n  > \n  > \n  > what is the correct response?  \n  > \n  > In case a) one of the two ranges is invalid, but the spec says send\n  > a 416 only if *all* are invalid.  If the valid ones are returned with\n  > a 206 then there is no way to signal an error.  \n  > \n  > In case b) the spec says\n  > \n  >             If the last-byte-pos value is present, it must be greater\n  >             than or equal to the first-byte-pos in that byte-range-spec,\n  >             or the byte-range-spec is invalid. The recipient of an\n  >             invalid byte-range-spec must ignore it.\n  > \n  > If the server ignores it then it should send what?\n  > \n  > Case c) is like b) except it is possible to send the valid ranges.\n  > [...]\n\nMy reading is that 416 is meant for use when the server parses Range\nand finds no valid byte-range-spec or suffix-byte-range-spec.  For case\n(a), the client should recognize that it didn't get all the ranges it\nasked for.  (That's the error indication.)  Similarly in case (c).  So\nonly case (b) would get a 416 error response.\n\nHowever, I have a problem with section 10.4.17, 416 Requested range not\nfound.  The helpful hint, \"(For byte-ranges, this means that the\nfirst-byte-pos of all of the byte-range-spec values were greater than\nthe current length of the selected resource.)\" is incorrect.  The\nbyte-range-spec \"30-20\" (as in (b) above) is invalid, but the\nfirst-byte-pos is valid.\n\nI also have to join in John Franks's follow-up question about\nContent-Range and a 417 response.  Section 14.17 says \"The\nContent-Range entity-header is sent with a partial entity-body ....\"\nBut when a server sends a 417 response, there is *no* entity.\nSo it appears that sections 10.4.17 and 14.17 are inconsistent.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Netscape Server security question..",
            "content": "Sorry to bother the list with this non-HTTP related question, but I was \nhoping to find major HTTP server implementers on the list that are using\nSSL3 to secure documents. This is an SSL3-related question. The \nInternet Printing Protocol (IPP) working group is considering requiring\nSSL3/TLS framing for negotiating security for internet printing. The\nminimal level of security that is required is basically NULL security.\nThat is, to be compliant, a client or server must use TLS framing with\nsupport for SSL3 compatibility, and the minimum level of security that\nmust be supported is NONE.\n\nMy question is if someone implements IPP as an ISAPI, NSAPI, or CGI\nbackend service, will the generic HTTP 1.1 server accept or allow \nnegotiation down to NULL security (the way the TLS document\nsuggests...).\n\n\nThanks in advance!\n\nRandy\n(again, sorry for the slight misuse of the list. If there is a better\nforum\nor email list to which this question should be directed, please let me\nknow).\n\n\nRandy Turner\nSharp Laboratories\nrturner@sharplabs.com\n\n\n\n"
        },
        {
            "subject": "webmaster&#64;websit",
            "content": "The webmaster@website convention is falling out of use.  Has anyone\nconsidered making it a requirement for HTTP/1.1 compliance?  Is it too\nlate to even be talking about this?\n\nThere have been numerous occasions where a web site has been\nsufficiently broken that I could not see any of the pages.  I try to\nnotify the administrators by sending mail to webmaster@website, but\nthese days, more often than not, the address is unknown.  Then what am I\nsupposed to do?\n\nRFC 1123 requires all SMTP servers to support postmaster, so there is a\nprecedent for a requirement like this.\n\nOr should people be encouraged to report web problems to postmaster when\nthe web site is too broken to show the preferred address?\n\nAMC\n\n\n\n"
        },
        {
            "subject": "Re: Netscape Server security question..",
            "content": "Turner, Randy wrote:\n> \n> Sorry to bother the list with this non-HTTP related question, but I was\n> hoping to find major HTTP server implementers on the list that are using\n> SSL3 to secure documents. This is an SSL3-related question. The\n> Internet Printing Protocol (IPP) working group is considering requiring\n> SSL3/TLS framing for negotiating security for internet printing. The\n> minimal level of security that is required is basically NULL security.\n> That is, to be compliant, a client or server must use TLS framing with\n> support for SSL3 compatibility, and the minimum level of security that\n> must be supported is NONE.\n> \n> My question is if someone implements IPP as an ISAPI, NSAPI, or CGI\n> backend service, will the generic HTTP 1.1 server accept or allow\n> negotiation down to NULL security (the way the TLS document\n> suggests...).\n\nApache-SSL will.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "HTTP Interoperability Test 103",
            "content": "  The fourth HTTP/1.1 Multi-Vendor Internet Test Day is Nov 6.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Nov 4.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  If you participated in either of the first two days and would like\n  me to just resend that information, just let me know.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Another Question on byte range",
            "content": "John Franks writes:\n\n    From section 14.17\n            A server sending a response with status code 416 (Requested\n            range  not valid) SHOULD include a Content-range field with\n            a content-range-spec of \"*\".\n\n    What is the purpos of this?\n\n    BUT 10.4.17 says about status 416:\n\n            When this status code is returned for a byte-range request,\n            the response MUST include a Content-Range entity-header\n            field specifying the current length of the selected resource\n            (see section 14.17).\n\n    I don't think '*' specifies the length. On the other hand I see no\n    way to make a Content-Range header which would be legal and would \n    specify the length.\n    \nWhat happened here is that I drafted the revised language and BNF too\nquickly, and screwed up.\n\n    Maybe what is needed is to allow something like \n    \n       Content-Range: bytes *-*/200\n    \n    when an illegal range (like 300-400) has been requested from a 200\n    byte file.\n    \nIn fact, what we had intended to specify was something very much\nlike that, except that we meant to have:\n\n       Content-Range: bytes */200\n\nSo, here's another stab at this - hopefully, correct this time.\n(But, unlike the last time, maybe someone should else review what\nI wrote?)\n\n(1) The BNF in 14.17 should be:\n\n   Content-Range = \"Content-Range\" \":\" content-range-spec\n\n    content-range-spec      = byte-content-range-spec\n\n    byte-content-range-spec = bytes-unit SP\n          byte-range-resp-spec \"/\"\n      ( entity-length | \"*\" )\n\n    byte-range-resp-spec = (first-byte-pos \"-\" last-byte-pos)\n      | \"*\"\n\n    entity-length           = 1*DIGIT\n\n(2) The following four paragraphs should read:\n\nThe asterisk \"*\" character in an entity-length means that the\nentity-length is unknown at the time when the response was\ngenerated.\n\nUnlike byte-ranges-specifier values, a byte--range-resp-spec\nmay only specify one range, and must contain absolute byte\npositions for both the first and last byte of the range.\n\nA byte-content-range-spec with a byte-range-resp-spec whose\nlast-byte-pos value is less than its first-byte-pos value, or\nwhose entity-length value is less than or equal to its\nlast-byte-pos value, is invalid. The recipient of an invalid\nbyte-content-range-spec MUST ignore it and any content\ntransferred along with it.\n\nA server sending a response with status code 416 (Requested\nrange not valid) SHOULD include a Content-range field with\na byte-range-resp-spec of \"*\".  The entity-length specifies\nthe current length of the selected resource.  A response\nwith status code 206 (Partial Content) MUST NOT include a\nContent-range field with a byte-range-resp-spec of \"*\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "John Franks writes:\n\n    If a server receives a byte range requests for a 100 byte file like\n    \n    a)   Range: bytes = 0-10, 130-140\n    or\n    b)   Range: bytes = 30-20\n    or\n    c)   Range: bytes = 0-10, 30-20, 40-50\n    \n    \n    what is the correct response?  \n    \n    In case a) one of the two ranges is invalid, but the spec says send\n    a 416 only if *all* are invalid.  If the valid ones are returned with\n    a 206 then there is no way to signal an error.  \n    \n    In case b) the spec says\n    \nIf the last-byte-pos value is present, it must be greater\nthan or equal to the first-byte-pos in that byte-range-spec,\nor the byte-range-spec is invalid. The recipient of an\ninvalid byte-range-spec must ignore it.\n    \n    If the server ignores it then it should send what?\n    \n    Case c) is like b) except it is possible to send the valid ranges.\n\nI think the confusion here arises because, when I drafted this part\nof the spec, I failed to make a clear distinction between \"syntactically\ninvalid\" byte-range-specs, and what I probably should have called\n\"unsatisfiable\" byte-range-specs.\n\nA \"syntactically invalid\" byte-range-spec is one for which there are no\ncircumstances in which it would ever be valid.  E.g., \"bytes = 30-20\"\nand \"bytes = 10-20-30\" are both syntactically invalid\nbyte-range-specs.  Basically, this is a coding bug in the client.\n\nAn \"unsatisfiable\" byte-range-spec is one that is syntactically\nvalid, but which cannot be satisfied (even partially) given the\ncurrent state of the resource.  The fact that the client made\nsuch an unsatisfiable request might not be the result of a\ncoding bug; it might just be because the client's information\nabout the resource's current length is either out-of-date or\n\"optimistic\".\n\nNote that some of this terminology already appears in section\n14.17, but it isn't used consistently in the entire document.\n\nThe philosophy behind the entire byte-range design is that\nrequests that have \"syntactically invalid\" byte-range-specs\nshould be treated as buggy, and the server should simply\nignore the Range header and treat the request just as it\nwould have treated a regular GET without the buggy Range.\n\nHowever, when a request contains a Range header which is\nsyntactically valid, the server should provide as much\nas it can to the client, consistent with the request.\nThat means that if any of the byte-range-specs are even\npartially satisfiable, then the server should send a 206\n(Partial content) response; if none of the byte-range-specs\nare satisfiable, then the server should return a 416 response.\n\nThis is basically an application of the Robustness Principle\n(\"be conservative in what you send, be liberal in what you accept\").\n\nI think this also clears up Dave Kristol's concern.  Dave wrote:\n    However, I have a problem with section 10.4.17, 416 Requested range\n    not found.  The helpful hint, \"(For byte-ranges, this means that\n    the first-byte-pos of all of the byte-range-spec values were\n    greater than the current length of the selected resource.)\" is\n    incorrect.  The byte-range-spec \"30-20\" (as in (b) above) is\n    invalid, but the first-byte-pos is valid.\n\nI think the \"helpful hint\" is accurate if you treat 416 as\nmeaning \"Requested range not satisfiable\" rather than \"Requested\nrange not valid\".    \n\nHere are my proposed rewrites:\n\n(1) Section 10.4.17 416 \"Requested range not valid\" should be\nretitled \"Requested range not satisfiable\", and in section\n6.1.1, the BNF for Status-Code should be changed from\n\n                                  | \"416\"   ; Requested range not valid\n\nto\n\n                                  | \"416\"   ; Requested range not satisfiable\n\nThis name change should also be applied at three places in section 14.17\nwhere this status code is mentioned.\n\n(2) In section 14.36.1 Byte Ranges, change\n\n            If the last-byte-pos value is present, it must be greater\n            than or equal to the first-byte-pos in that byte-range-spec,\n            or the byte-range-spec is invalid. The recipient of an\n            invalid byte-range-spec must ignore it.\n\nto\n\n    If the last-byte-pos value is present, it must be greater\n    than or equal to the first-byte-pos in that\n    byte-range-spec, or the byte-range-spec is syntactically\n    invalid.  The recipient of a byte-range-set that includes\n    one or more syntactically invalid byte-range-spec values\n    MUST ignore the header field that includes that\n    byte-range-set.\n\n(3) Also in section 14.36.1 Byte Ranges, before the paragraph that\nstarts \"Examples of byte-ranges-specifier values ...\", insert this\nparagraph:\n\nIf a syntactically valid byte-range-set includes at least one\nbyte-range-spec whose first-byte-pos is less than the current\nlength of the entity-body, or at least one\nsuffix-byte-range-spec with a non-zero suffix-length, then the\nbyte-range-set is satisfiable.  Otherwise, the byte-range-set\nis unsatisfiable.  If the byte-range-set is unsatisfiable, the\nserver SHOULD return a response with a status of 416 (Requested\nrange not satisfiable).  Otherwise, the server SHOULD return a\nresponse with a status of 206 (Partial Content) containing the\nsatisfiable ranges of the entity-body.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "Jeffrey Mogul wrote:\n> [...]\n> (3) Also in section 14.36.1 Byte Ranges, before the paragraph that\n> starts \"Examples of byte-ranges-specifier values ...\", insert this\n> paragraph:\n> \n>         If a syntactically valid byte-range-set includes at least one\n>         byte-range-spec whose first-byte-pos is less than the current\n>         length of the entity-body, or at least one\n>         suffix-byte-range-spec with a non-zero suffix-length, then the\n>         byte-range-set is satisfiable.  Otherwise, the byte-range-set\n>         is unsatisfiable.  If the byte-range-set is unsatisfiable, the\n>         server SHOULD return a response with a status of 416 (Requested\n>         range not satisfiable).  Otherwise, the server SHOULD return a\n>         response with a status of 206 (Partial Content) containing the\n>         satisfiable ranges of the entity-body.\n\nThat's much clearer.  But I think it's backward.  I think 416 should\nmean the request (and therefore the client) is buggy.  And if none of\nthe ranges are satisfiable, return everything (pretend there's no Range\nheader).  I think it's a little strange to return an error if the Range\nheader was well-formed and ignore Range if it's ill-formed.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "On Tue, 4 Nov 1997, Dave Kristol wrote:\n\n> Jeffrey Mogul wrote:\n> > [...]\n> > (3) Also in section 14.36.1 Byte Ranges, before the paragraph that\n> > starts \"Examples of byte-ranges-specifier values ...\", insert this\n> > paragraph:\n> > \n> >         If a syntactically valid byte-range-set includes at least one\n> >         byte-range-spec whose first-byte-pos is less than the current\n> >         length of the entity-body, or at least one\n> >         suffix-byte-range-spec with a non-zero suffix-length, then the\n> >         byte-range-set is satisfiable.  Otherwise, the byte-range-set\n> >         is unsatisfiable.  If the byte-range-set is unsatisfiable, the\n> >         server SHOULD return a response with a status of 416 (Requested\n> >         range not satisfiable).  Otherwise, the server SHOULD return a\n> >         response with a status of 206 (Partial Content) containing the\n> >         satisfiable ranges of the entity-body.\n> \n> That's much clearer.  But I think it's backward.  I think 416 should\n> mean the request (and therefore the client) is buggy.  And if none of\n> the ranges are satisfiable, return everything (pretend there's no Range\n> header).  I think it's a little strange to return an error if the Range\n> header was well-formed and ignore Range if it's ill-formed.\n> \n> Dave Kristol\n> \n\nI also think this clears up the meaning.  Thanks.\n\nBut I would like to see 416 returned if the request is either\nsyntactically incorrect or unsatisfiable.  I agree with Dave that it\nis strange to ignore a syntactically incorrect header.  It is unlikely\nto be helpful to the (buggy) client.  Indeed, to the user it might\nwell look like the server is buggy.  I much prefer for the server to\nannounce definitively, \"Hey, your client screwed up!\"\n\nBut it also makes sense to communicate the correct length of the \nresource when the client has somehow misjudged it.  So an error \nmessage for an unsatisfiable request is a good idea.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "John Franks writes:\n\n    But I would like to see 416 returned if the request is either\n    syntactically incorrect or unsatisfiable.  I agree with Dave that it\n    is strange to ignore a syntactically incorrect header.  It is unlikely\n    to be helpful to the (buggy) client.  Indeed, to the user it might\n    well look like the server is buggy.  I much prefer for the server to\n    announce definitively, \"Hey, your client screwed up!\"\n    \nIt might seem strange to ignore a syntactically incorrect header,\nbut there are plenty of other HTTP headers that could be sent with\nincorrect syntax, and the HTTP/1.1 spec says nothing at all about\nwhat to do.  For example, what does the server return for\n\nGET /home.html HTTP/1.1\nHost: foo.com\nIf-Modified-Since: cheeseburger\n\nor\n\nGET /home.html HTTP/1.1\nHost: foo.com\nAccept-Encoding: Tue, 04 Nov 97 17:04:50 GMT\n\nThe specification for the 400 (Bad Request) status code says\n\n            10.4.1 400 Bad Request\n\n            The request could not be understood by the server due to\n            malformed syntax. The client SHOULD NOT repeat the request\n            without modifications.\n\nbut it's not clear if the examples above are \"could not be understood\"\nby a server.\n\nI tried sending \"If-Modified-Since: cheeseburger\" in an HTTP/1.0\nrequest to www.apache.org; it ignored the malformed header, and\nsent me a status-200 response.\n\nSo, based as before on the Robustness Principle, and especially given\nthe general lack of definitive statements about this kind of problem\nin the HTTP/1.1 spec, that we're better off with servers ignoring\nmalformed non-mandatory headers, rather than sending status-400\nresponses.  \n\nAnd I certainly think it's a mistake to use the same response code\nto mean both \"your code is buggy\" and \"your code is fine, but your\ncache is out of date\".  The latter implies that the client might\ntake some automated action to recover; the former implies that the\nclient should probably not try to do an automatic recovery.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "Jeffrey Mogul wrote:\n> [...]\n> So, based as before on the Robustness Principle, and especially given\n> the general lack of definitive statements about this kind of problem\n> in the HTTP/1.1 spec, that we're better off with servers ignoring\n> malformed non-mandatory headers, rather than sending status-400\n> responses.\n> [...]\n\nThat gets to my point, which John Franks didn't quite get right.\n\nMy remark was, Why send a 416 in response to a well-formed header (whose\nbyte-range-spec is unsatisfiable), but respond with 200 to a malformed\nheader?  Why the distinction, in other words?  The Robustness Principle\nwould argue against the 416 response, too, wouldn't it?  Surely the\nContent-Length or equivalent would be enough to clue the recipient that\nthe byte-range-spec was unsatisfiable.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "On Tue, 1 Apr 1997, Adam M. Costello wrote:\n\n> The webmaster@website convention is falling out of use.  Has anyone\n> considered making it a requirement for HTTP/1.1 compliance?  Is it too\n> late to even be talking about this?\n>\n\nI don't think it's too late. It is essential that there be some such\naddress, and \"webmaster\" certainly seems to bve the logical choice.\nUnfortunately, the term  \"webmaster\" has been coopted (too strong a word?)\nto refer to HTML authors and web page designers. Even so, I don't think it\nwould be overly confusing to use \"webmaster\" in way you suggest. A\nsecondary problem is that many people maintain multiple webs on the same\nsystem (e.g., ISPs that provide web hosting) without having a separate\ndomain. Typically, this results in URLs like\n\nhttp://www.whatever.com/~whoever/\n\nI'm not sure how to handle this situation.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Fwd: Announcing Connectathon '9",
            "content": "  I got this forwarded thorugh a couple of hops.\n\n  There has been some discussion on the IETF http-wg list of setting\n  up a face to face HTTP/1.1 testing session; perhaps this is a good\n  venue for that.\n\n  Please direct any questions to \"cthon@sun.com\".\n\n-----\n\nPlans for Connectathon 98 are already under way!  The 12th annual\nConnectathon event, an interoperability testing event for engineers\nonly, will be held March 4-12, 1998, in San Jose, California.\nConnectathon, sponsored by Sun Microsystems, Inc., hosts over 30\ncompanies annually in an effort to test and debug source code which\nutilize the following technologies and protocols:\n\nNFS and WebNFS\nPPP\nDHCP\nService Location Protocol\nNIS/NIS+\nTI-RPC\nAutomounter\n\nBased on demand, in addition we are considering to offer:\n\nIPv6\nLDAP\nHTTP\n\nIf you are interested in testing IPv6, LDAP or HTTP, please send a note\nto Cthon@Sun.COM and we'll gauge interest.  Or if you have a suggestion\nfor another technology, feel free to contact us as well.\n\nTesting continues 24 hours per day.  Technology testing coordinators\nwill organize testing procedures and test suite material.  In addition,\nthere will be seminars with speakers addressing various topics.\n\nThe registration deadline is February 1, 1998.  For a registration\npacket, please send complete address and fax number to Cthon@Sun.COM, or\nif you have any questions, please feel free to contact Audrey Van\nBelleghem at (408) 358-9598.  For more information about Connectathon,\nwe will be updating our web site continually at\nhttp://www.sun.com/sunsoft/connectathon.\n\nWe look forward to seeing you at the 12th annual Connectathon event!\n\nAudrey Van Belleghem\nConnectathon Manager\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Question on byte range",
            "content": "Dave Kristol writes:\n\n    My remark was, Why send a 416 in response to a well-formed header\n    (whose byte-range-spec is unsatisfiable), but respond with 200 to a\n    malformed header?  Why the distinction, in other words?  The\n    Robustness Principle would argue against the 416 response, too,\n    wouldn't it?  Surely the Content-Length or equivalent would be\n    enough to clue the recipient that the byte-range-spec was\n    unsatisfiable.\n\nNot quite.  Content-Length is defined (section 14.14) this way:\n\n            The Content-Length entity-header field indicates the size of\n            the message-body, in decimal number of octets, sent to the\n            recipient or, in the case of the HEAD method, the size of\n            the entity-body that would have been sent had the request\n            been a GET.\n\nFor a GET request with an unsatisfiable Range request-header, the\nmessage-body is clearly NOT going to have the same length as the\n\"full entity-body\", and so we needed a different mechanism.\n\nI suppose we could have used a 206 response with this kind of\nthing:\n\nContent-Range: */200\n\nbut it was feared that this would break already-deployed implementations\nthat \"knew\" how to parse a 206 response.  Maybe this fear was groundless.\n\n-Jeff\n\nP.S.: In case anyone wishes to point out that the term \"full\nentity-body\" in 14.17 is confusing: I agree.  18 months ago, the HTTP\neditors group had a long argument over how to define the meaning\nof the word \"entity\"; I was on the losing side.  We ended up with\nno term defined to mean \"the content that would be returned right\nnow with a status-200 response for the request\".  Don't complain\nto me.\n\n\n\n"
        },
        {
            "subject": "good words for HTTP interoperability testin",
            "content": "I want to put in a few good words for the ongoing HTTP/1.1\ninteroperabiity testing and to encourage others to participate.  (But\nfirst, thanks to Scott Lawrence for organizing it.)  If you haven't\nsigned up to participate, you may not even realize it's going on.\n\nThe testing of my server (mostly by Ronald Tschalaer, my chief bug\nfinder :-) has turned up a couple of bugs in my server, which I have\nfixed.  It also led to the recent discussions about Range and\nContent-Range, because we determined that the words there needed to be\nclarified.\n\nSo let me others to sign up next week, when Scott sends out his next\nsolicitation, in the interests of a clearer HTTP/1.1 specification and\na higher degree of interoperation.\n\nDave Kristol\n\nP.S.  You are welcome to poke at my server at\n<http://portal.research.bell-labs.com:8000>.  -- DMK\n\n\n\n"
        },
        {
            "subject": "Washington Agenda for HTTP",
            "content": "I notice on the IETF agenda that one slot is scheduled for HTTP.\n\nIs there any anticipation of a second slot this time around or is it\nsafe to return Home Tuesday?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Washington Agenda for HTTP",
            "content": "The request for the second slot is awaiting the approval of the\narea directors. Probably also the arrival of 4 hours of agenda.\n\nI believe we will have ample agenda items to close out HTTP/1.1,\nespecially if more issues arise during interoperability testing.\n\nIn particular, the 'documentation of interoperable implementations'\nwill probably require more time than is available if we go feature-by-feature,\nalthough some of that can happen outside official meeting time.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "FindMail Archiv",
            "content": "I just discovered that this mailing list is also being archived\nat <http://www.findmail.com/listsaver/http-wg/>, which may be more\nconvenient for some readers.  It has a search interface as well.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "Dave Kristol <dmk@bell-labs.com> wrote:\n>Benjamin Franz wrote:\n>> [...]\n>> Could you give an example of something in the current proposal that mixes\n>> the two levels? Or are you suggesting that the split will block the\n>> exploration of options such as the one I mentioned?\n>\n>It wasn't so much that the text in the document were intertwined, but\n>rather that the arguments on the mailing list(s) about the technical and\n>privacy issues got intertwined.  The idea was to disentangle them and\n>deal with them separately.  The hope was that doing so would lead to\n>some measure of progress.\n\nThere has been no substantive discussion about Stage Management\nsince release of the -04 draft, because all of the bugs in the current\nRFC were fixed in the -03 draft and consensus had been reached on the\ntechnical issues, *and* because the few paragraphs that commented on\nprivacy issues were nothing more than advice to implementors, and also\nwere fine as they stood.  I realized you were asked by the Chair to\ndo this, but it appears, in fact, simply to have stalled progress.\nCan it now be put back together again, and move on to Draft Standard\nstatus?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "HTTP Interoperability Test 103",
            "content": "  The fifth HTTP/1.1 Multi-Vendor Internet Test Day is Nov 13.\n\n  There are a couple of efforts under way to define some 'standard'\n  content for origin servers so that some at least semi-automated\n  testing can be run against it.  If you have suggestions for this\n  please let me know.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Nov 11.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "HTTP Interoperability Test 1113 (correct date",
            "content": "  The fifth HTTP/1.1 Multi-Vendor Internet Test Day is Nov 13.\n\n  There are a couple of efforts under way to define some 'standard'\n  content for origin servers so that some at least semi-automated\n  testing can be run against it.  If you have suggestions for this\n  please let me know.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Nov 11.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Two Internet Drafts =&gt; Experimental RFC",
            "content": "The HTTP working group requests that the two drafts\n\n draft-holtman-http-safe-03.txt\n    The Safe response header field\n draft-ietf-http-uahint-01.txt\n    The User Agent Hint Response Header\n\nbe considered for moving forward as Experimental RFCs.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "HTTP working group status; VOLUNTEERS neede",
            "content": "Alas, everyone has had some difficulties getting promised work done\n(myself included, of course), so some items have been delayed. I'm\nhoping we can get back on track in time to sucessfully conclude our\nformal work at, or soon after, the December IETF meeting.\n\nWhat we need:\n*** REVIEW THE DOCUMENTS AND ISSUES\n We're down to the last stretch, folks. If you've been putting off\n paying attention, please do so now.\n*** MAKE SPECIFIC SUGGESTIONS\n If you have some change you'd like, or a resolution for any of\n the open issues, specific wording suggestions are really important.\n*** HELP DOCUMENT IMPLEMENTATIONS\n We need to document EVERY FEATURE of HTTP in AT LEAST TWO independent\n implementations. If any HTTP/1.1 implementors have a section-by-section\n review of which mandatory, suggested or optional features are implemented,\n please send them in. We especially need to flag those optional features\n that are NOT IMPLEMENTED.\n\nHere are some recent changes in status, as far as I can recollect\nbased on recent changes to the issue list. The 'editing group' has\nstarted to have weekly phone conferences again, in order to move\nthe drafting forward.\n\nState management:\n\nA LAST CALL was issued on the revised state management draft (without\nprivacy considerations). We've had some promises of review of the\n(eviscerated) drafts.\n\nSafe & UAHint:\n\nI'm sorry for the unnecessary delay; these have (finally) been\nsent on for Experimental RFC.\n\nContent negotiation:\n\nA BOF at the DC IETF has been scheduled. Ted Hardie has created\na new draft of the 'alternates' header; some of the other expected\nrevisions have not appeared yet.\n\nOptions:\n\nI believe that the options draft is not going to move forward.\nWe'll attempt to clarify OPTIONS in the HTTP/1.1 spec primarily\nas 'a method guaranteed not to do anything'.\n\nAge calculation:\n\nMy reading is that the 'rough consensus' is to follow\ndraft-fielding-http-age-00.txt.\n\nHit metering:\n\nhas issued as RFC 2227 (Proposed Standard)\n\nRevised draft:\n\nPlease note that what we were calling 'draft 08' is now\ncalled 'Rev-00' in order to follow Internet Drafts conventions.\n\nRemaining issues:\n\nBYTERANGE_SYNTAX was added as an issue, raised from interoperability testing.\nJeff Mogul volunteered to propose wording.\n\nHITMETER_FOR_HTTP10 (draft-harada-http-xconn-from-01) may proceed independently,\nas it is not part of HTTP/1.1.\n\nTRAILER_FIELDS (what header fields can go into trailers) is still open. The\ncurrent spec basically leaves it at 'none'. We'd like some wording that says 'a\nsender can put any fields in the trailer that it's willing to have the recipient\nignore'.\n\nRE-VERSION (version number isn't hop-by-hop) is still open.\n\nREDIRECTS (how many redirects should be allowed, required) is still awaiting new\nwording.\n\nCONTENT-ENCODING has revised wording, but it's unclear if there's still an\nissue.\n\nRANGE-ERROR: based on feedback from interoperability testing, the issue has been\nreopened.\n\nMost other issues are awaiting wording and editorial work.\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "Gregory J. Woodhouse wrote:\n> \n> On Tue, 1 Apr 1997, Adam M. Costello wrote:\n> \n> > The webmaster@website convention is falling out of use.  Has anyone\n> > considered making it a requirement for HTTP/1.1 compliance?  Is it too\n> > late to even be talking about this?\n> >\n> \n> I don't think it's too late. It is essential that there be some such\n> address, and \"webmaster\" certainly seems to bve the logical choice.\n> Unfortunately, the term  \"webmaster\" has been coopted (too strong a word?)\n> to refer to HTML authors and web page designers. Even so, I don't think it\n> would be overly confusing to use \"webmaster\" in way you suggest. A\n> secondary problem is that many people maintain multiple webs on the same\n> system (e.g., ISPs that provide web hosting) without having a separate\n> domain. Typically, this results in URLs like\n> \n> http://www.whatever.com/~whoever/\n> \n> I'm not sure how to handle this situation.\n\nI was pondering this the other day. It occurred to me that it would be sensible\nto be able to ask a web server who the webmaster was for a particular URL.\n\nThen all such problems could be solved by careful configuration.\n\nI guess that would need a new method - which could return other stuff that is\nnot normally of interest to the client. \"INFO <url>\", perhaps?\n\nThis may be related to the WEBDAV work that's going on at the moment.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email: ben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: Calculation of age headers (AGECALCULATION last call",
            "content": "Consider this a LAST CALL on:\nhttp://www.w3.org/Protocols/HTTP/Issues/#AGE-CALCULATION\n\nThis hot potato has ended up in my plate despite repeated attempts\nto avoid it :-(.\n\nI read through both Roy's and Jeff's Internet drafts on this topic,\nand all the mailing list mail on the topic yesterday, to come to some\ndecision on the topic, since it hasn't been obviously settled despite\nmany attempts over a long time.\n\nThere is a (very) rough concensus supporting Roy's position in the mailing \nlist, in particular that Jeff's option (c) would cause non-caching proxies \nto have to do onerous implementation work; in fact, one could argue that \nadding another clock to the mess would just make things worse.  Such proxies \nare often used as part of a firewall complex (one tranparent proxy at the \nfirewall itself, with another caching proxy just inside; this avoids having\na big, complicated, caching proxy on the front lines of a firewall; the\nmore code in them, the less they are to be trusted).\n\nWhile I think that Jeff is technically correct, I think a pragmatic attitude \nneeds to be taken on the implementation cost/benefit side; if you have an \nHTTP/1.0 proxy up stream of you, you are in very serious trouble in the \nfirst place, so I've decided to add to the very rough concensus already \non the list (making it rough concensus, I guess; not all that many people \nhave actually commented).\n\nI'm currently leaning against Koen's suggestion clarifying that\nsuch proxies MAY add age headers, as much on complication grounds\nas any other (things are complicated enough as it is).  Koen's mail message is\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0002.html\nThere were no replies to Koen's mail on this topic, so I have no other\nopinions to guide me.  Other opinions on this suggestion are solicited\n(particularly from implementers...).\n\nIf others have opinions on either the AGE-CALCULATION issue, or on Koen's \nsuggestion, NOW is the time to speak up.  My current plan is to adopt the \nlanguage in Roy's draft \n(ftp://ds.internic.net/internet-drafts/draft-fielding-http-age-00.txt).  I \nconsider the wording to encourage synchronized clocks an editorial issue, \nin an implementation note. \n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Warning&quot; in draft 08 (WARNING",
            "content": "Here is the slight wording change by Jeff to close out the comments\nraised on the text in Rev-00 by Klaus Weide in:\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0493.html\n\nI'll be incorporating them in Rev01, which I'm working on now.\n- Jim\n\n\n\nattached mail follows:\nThe warning text may include arbitrary information to be\n               presented to a human user, or logged. A system receiving\n               this warning MUST NOT take any automated action.\n\nshould be\nThe warning text may include arbitrary information to be\n               presented to a human user, or logged. A system receiving\n               this warning MUST NOT take any automated action, besides\n       presenting the warning to the user.\n\n\nThis\n            If an implementation receives a response with a warning-\n            value that includes a warn-date, and that warn-date is\n            different from the Date value in the response, then that\n            warning-value MUST be deleted from the message before\n            storing, forwarding, or using it.  If all of the warning-\n            values are deleted for this reason, the Warning header MUST\n            be deleted as well.\n\nshould be\n\n            If an implementation receives a response with a warning-\n            value that includes a warn-date, and that warn-date is\n            different from the Date value in the response, then that\n            warning-value MUST be deleted from the message before\n            storing, forwarding, or using it.\n    (This prevents bad consequences of naive caching\n    of Warning header fields.)\n    If all of the warning-\n            values are deleted for this reason, the Warning header MUST\n            be deleted as well.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "LAST CALL: draft-ietf-http-state-man-mec04.tx",
            "content": "I thought I had sent out a LAST CALL, but apparently it was stealth,\nand doesn't appear on the mailing list.\n\nI know many of you have expressed your opinion before, but we need\nto ask the question just once more:\n\nLeaving out any consideration of the privacy issues (which will\nbe addressed once we've resolved the protocol issues), are you,\nin favor of proposing the mechanism in\ndraft-ietf-http-state-man-mech-04.txt as a Proposed Standard.\n\nThis is a working group LAST CALL on this issue.\n\nYou may send your (preferably brief) opinion to me privately, or\nto the list publicly. I'll summarize private opinions. (I would\nrather not discuss the process or the privacy issues at this point,\nhowever.)\n\n\nLarry\n\n\n\n"
        },
        {
            "subject": "REVERSION discussion at Munich...",
            "content": "There were a set of people who were going to discuss this\nin the halls, and get back to me (and the working group)\nwith a resolution.\n\nI've never seen any resolution. No one caught me in the hall.\n\nPeople involved included Josh and Henry Sanders.\n\nPlease identify yourselves, and the proposed solution.\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "This documents discussions on the implementation list that shows\na new protocol issue (IMS_INM_MISMATCH).\n\nNow added to the issues list.\n- Jim Gettys\n\n\n\nattached mail follows:\nIs there a consensus on the correct behavior for a server when it\nreceives a request with conflicting If-None-Match and If-Modified-Since\nheaders, for instance where the I-N-M ETag is outdated but the I-M-S\ndate is good?\n\nFor example, take the following sequence\n\nGET /foo.txt HTTP/1.1\nHost: server.company.com\n\nHTTP/1.1 200 OK\nLast-Modified: Wed, 05 Nov 1997 22:10:48 GMT\nETag: \"12345\"\nContent-Length: ...\n\n<data>\n\nGET /foo.txt HTTP/1.1\nHost: server.company.com\nIf-None-Match: \"12344\"\nIf-Modified-Since: Wed, 05 Nov 1997 22:10:48 GMT\n\nAssuming the Last-Modified date hasn't changed, what should the server\nsend back as a response to the 2nd request, 304 or 200? I would have\nthought 200 is correct, since the ETag is invalid or out of date.\nHowever the spec indicates that the I-N-M header is to be treated as if\nit isn't present if the ETag doesn't match, and then the I-M-S would\nlead to a 304 response. Also the paragraph about I-N-M being ignored if\nthe request would otherwise generate a non-2xx response would indicate\nthat a 304 is correct. But there would seem to be a window here, where\n/foo.txt is modified twice in less than a second, the ETag is updated\nbut the L-M date isn't, and a client using an out of date ETag gets a\n304 anyway. Or am I missing something here?\n\nHenry\n\n\n\n\nattached mail follows:\nHenry Sanders writes:\n>Is there a consensus on the correct behavior for a server when it\n>receives a request with conflicting If-None-Match and If-Modified-Since\n>headers, for instance where the I-N-M ETag is outdated but the I-M-S\n>date is good?\n\nHmmm, I don't know about consensus, but I implemented it using the\nprinciple that an ETag test always overrides IMS.  I mentioned it as\nan issue on http-wg a long time ago, since the RFC fails to specify\nthe ordering.  The Apache code (and comments) are below.\n\n>For example, take the following sequence\n>\n>GET /foo.txt HTTP/1.1\n>Host: server.company.com\n>\n>HTTP/1.1 200 OK\n>Last-Modified: Wed, 05 Nov 1997 22:10:48 GMT\n>ETag: \"12345\"\n>Content-Length: ...\n>\n><data>\n>\n>GET /foo.txt HTTP/1.1\n>Host: server.company.com\n>If-None-Match: \"12344\"\n>If-Modified-Since: Wed, 05 Nov 1997 22:10:48 GMT\n>\n>Assuming the Last-Modified date hasn't changed, what should the server\n>send back as a response to the 2nd request, 304 or 200?\n\n200.\n\n>I would have\n>thought 200 is correct, since the ETag is invalid or out of date.\n>However the spec indicates that the I-N-M header is to be treated as if\n>it isn't present if the ETag doesn't match, and then the I-M-S would\n>lead to a 304 response. Also the paragraph about I-N-M being ignored if\n>the request would otherwise generate a non-2xx response would indicate\n>that a 304 is correct. But there would seem to be a window here, where\n>/foo.txt is modified twice in less than a second, the ETag is updated\n>but the L-M date isn't, and a client using an out of date ETag gets a\n>304 anyway. Or am I missing something here?\n\nNope, that about sums it up.  200 is what the response should be, but\nthe RFC is less than clear.\n\n....Roy\n=====================================================================\n\nAPI_EXPORT(int) meets_conditions(request_rec *r)\n{\n    char *etag = table_get(r->headers_out, \"ETag\");\n    char *if_match, *if_modified_since, *if_unmodified, *if_nonematch;\n    time_t mtime;\n\n    /* Check for conditional requests --- note that we only want to do\n     * this if we are successful so far and we are not processing a\n     * subrequest or an ErrorDocument.\n     *\n     * The order of the checks is important, since ETag checks are supposed\n     * to be more accurate than checks relative to the modification time.\n     * However, not all documents are guaranteed to *have* ETags, and some\n     * might have Last-Modified values w/o ETags, so this gets a little\n     * complicated.\n     */\n\n    if (!is_HTTP_SUCCESS(r->status) || r->no_local_copy) {\n        return OK;\n    }\n\n    mtime = (r->mtime != 0) ? r->mtime : time(NULL);\n\n    /* If an If-Match request-header field was given\n     * AND if our ETag does not match any of the entity tags in that field\n     * AND the field value is not \"*\" (meaning match anything), then\n     *     respond with a status of 412 (Precondition Failed).\n     */\n    if ((if_match = table_get(r->headers_in, \"If-Match\")) != NULL) {\n        if ((etag == NULL) ||\n            ((if_match[0] != '*') && !find_token(r->pool, if_match, etag))) {\n            return HTTP_PRECONDITION_FAILED;\n        }\n    }\n    else {\n        /* Else if a valid If-Unmodified-Since request-header field was given\n         * AND the requested resource has been modified since the time\n         * specified in this field, then the server MUST\n         *     respond with a status of 412 (Precondition Failed).\n         */\n        if_unmodified = table_get(r->headers_in, \"If-Unmodified-Since\");\n        if (if_unmodified != NULL) {\n            time_t ius = parseHTTPdate(if_unmodified);\n\n            if ((ius != BAD_DATE) && (mtime > ius)) {\n                return HTTP_PRECONDITION_FAILED;\n            }\n        }\n    }\n\n    /* If an If-None-Match request-header field was given\n     * AND if our ETag matches any of the entity tags in that field\n     * OR if the field value is \"*\" (meaning match anything), then\n     *    if the request method was GET or HEAD, the server SHOULD\n     *       respond with a 304 (Not Modified) response.\n     *    For all other request methods, the server MUST\n     *       respond with a status of 412 (Precondition Failed).\n     */\n    if_nonematch = table_get(r->headers_in, \"If-None-Match\");\n    if (if_nonematch != NULL) {\n        int rstatus;\n\n        if ((if_nonematch[0] == '*')\n            || ((etag != NULL) && find_token(r->pool, if_nonematch, etag))) {\n            rstatus = (r->method_number == M_GET) ? HTTP_NOT_MODIFIED\n                                                  : HTTP_PRECONDITION_FAILED;\n            return rstatus;\n        }\n    }\n    /* Else if a valid If-Modified-Since request-header field was given\n     * AND it is a GET or HEAD request\n     * AND the requested resource has not been modified since the time\n     * specified in this field, then the server MUST\n     *    respond with a status of 304 (Not Modified).\n     * A date later than the server's current request time is invalid.\n     */\n    else if ((r->method_number == M_GET)\n             && ((if_modified_since =\n                  table_get(r->headers_in, \"If-Modified-Since\")) != NULL)) {\n        time_t ims = parseHTTPdate(if_modified_since);\n\n        if ((ims >= mtime) && (ims <= r->request_time)) {\n            return HTTP_NOT_MODIFIED;\n        }\n    }\n    return OK;\n}\n\n\n\n\nattached mail follows:\nJust what you wanted to see, another HTTP/1.1 issue....\n\nThis is something I sent out to the HTTP implementors list about a week\nago. Looks like the spec says to send a 304 when a 200 response would be\nbetter. Any comments?\n\nHenry\n\n> -----Original Message-----\n> From:Henry Sanders (Exchange) \n> Sent:Wednesday, November 05, 1997 4:26 PM\n> To:w3c-http@w3.org\n> Subject:If-None-Match and IMS\n> \n> Is there a consensus on the correct behavior for a server when it\n> receives a request with conflicting If-None-Match and\n> If-Modified-Since headers, for instance where the I-N-M ETag is\n> outdated but the I-M-S date is good?\n> \n> For example, take the following sequence\n> \n> GET /foo.txt HTTP/1.1\n> Host: server.company.com\n> \n> HTTP/1.1 200 OK\n> Last-Modified: Wed, 05 Nov 1997 22:10:48 GMT\n> ETag: \"12345\"\n> Content-Length: ...\n> \n> <data>\n> \n> GET /foo.txt HTTP/1.1\n> Host: server.company.com\n> If-None-Match: \"12344\"\n> If-Modified-Since: Wed, 05 Nov 1997 22:10:48 GMT\n> \n> Assuming the Last-Modified date hasn't changed, what should the server\n> send back as a response to the 2nd request, 304 or 200? I would have\n> thought 200 is correct, since the ETag is invalid or out of date.\n> However the spec indicates that the I-N-M header is to be treated as\n> if it isn't present if the ETag doesn't match, and then the I-M-S\n> would lead to a 304 response. Also the paragraph about I-N-M being\n> ignored if the request would otherwise generate a non-2xx response\n> would indicate that a 304 is correct. But there would seem to be a\n> window here, where /foo.txt is modified twice in less than a second,\n> the ETag is updated but the L-M date isn't, and a client using an out\n> of date ETag gets a 304 anyway. Or am I missing something here?\n> \n> Henry\n> \n\n\n\nattached mail follows:\n> >that a 304 is correct. But there would seem to be a window here,\n> where\n> >/foo.txt is modified twice in less than a second, the ETag is updated\n> >but the L-M date isn't, and a client using an out of date ETag gets a\n> >304 anyway. Or am I missing something here?\n> \n> Nope, that about sums it up.  200 is what the response should be, but\n> the RFC is less than clear.\n> \nI agree that 200 is what the response should be, but saying the\nRFC is unclear is an understatement. I think the RFC actually\ncontradicts this view, twice. I'll pass this on to the HTTP group, maybe\nwe can get it fixed in the update.\n\nHenry\n\n\n\n"
        },
        {
            "subject": "comments on draft-ietf-http-state-man-mec04.tx",
            "content": "Larry asked me to do a detailed review of the draft, which is probably\na dangerous thing.  I do not consider myself to be a Cookie expert;\nin fact, I avoid them like the plague.  My comments are on general\ndraftyness issues.\n\n>                 HTTP State Management Mechanism (Rev1)\n\nI'd prefer not to have \" (Rev1)\" in the title.\n\n>2.  TERMINOLOGY\n>\n>The terms user agent, client, server, proxy, and origin server have the\n>same meaning as in the HTTP/1.1 specification [RFC 2068].\n>\n>Host name (HN) means either the host domain name (HDN) or the numeric\n>Internet Protocol (IP) address of a host.  The fully qualified domain\n>name is preferred; use of numeric IP addresses is strongly discouraged.\n>...\n\nA comment should be included regarding support (or non-support) of IPv6\naddresses [we got pinged on this for the URI draft].\n\nIn general, I find the terminology section more confusing than useful,\neven though I do know what is intended.  I find it unlikely that anyone\nnot involved in the mailing list discussions would have a clue as to\nwhat is being described.  It would be better to postpone the description\nof hostname dot comparison until it can be described as part of the\nmatching algorithm.\n\n>4.  OUTLINE\n\nWhy is the entire protocol specification under the section \"OUTLINE\"?\n\n>We outline here a way for an origin server to send state information to\n>the user agent, and for the user agent to return the state information\n>to the origin server.  The goal is to have a minimal impact on HTTP and\n>user agents.  Only origin servers that need to maintain sessions would\n>suffer any significant impact, and that impact can largely be confined\n>to Common Gateway Interface (CGI) programs, unless the server provides\n>more sophisticated state management support.  (See Implementation\n>Considerations, below.)\n\nCGI has nothing to do with it.  \"particular resource namespaces\" would\nbe more accurate.  \", unless the server provides more sophisticated\nstate management support\" says nothing.\n\n[...]\n>A user agent returns a Cookie request header (see below) to the origin\n>server if it chooses to continue a session.  The origin server may\n>ignore it or use it to determine the current state of the session.  It\n>may send back to the client a Set-Cookie2 response header with the same\n>or different information, or it may send no Set-Cookie2 header at all.\n>The origin server effectively ends a session by sending the client a\n>Set-Cookie2 header with Max-Age=0.\n\nA user agent never returns a Cookie request header field --\nit *sends* a Cookie request header field containing a field value\nmatching that of a previously received Set-Cookie response header field.\nLikewise, a server never \"sends back\" a Set-Cookie2 response header field;\nit either sends a Set-Cookie2 or it doesn't.\n\nMuch of this discussion is unnecessarily confusing.  I think it would be\nbetter to define the cookie exchange protocol as a state machine,\nrather than enumerate in prose all of the optional behavior of each party\nat any given time in the exchange.  For example, I find the TCP specification\nto be less confusing even though it describes a much more complex state\nmachine than is implicit in the cookies protocol.\n\n>4.2.2  Set-Cookie2 Syntax  The syntax for the Set-Cookie2 response\n>header is\n>\n>set-cookie      =       \"Set-Cookie2:\" cookies\n>cookies         =       1#cookie\n>cookie          =       NAME \"=\" VALUE *(\";\" set-cookie-av)\n>NAME            =       attr\n>VALUE           =       value\n>set-cookie-av   =       \"Comment\" \"=\" value\n>                |       \"CommentURL\" \"=\" <\"> http_URL <\">\n>                |       \"Discard\"\n>                |       \"Domain\" \"=\" value\n>                |       \"Max-Age\" \"=\" value\n>                |       \"Path\" \"=\" value\n>                |       \"Port\" [ \"=\" <\"> 1#portnum <\"> ]\n>                |       \"Secure\"\n>                |       \"Version\" \"=\" 1*DIGIT\n>portnum =       1*DIGIT\n\nUnless you wish to define the precedence of \"|\" in your BNF, each\nof the set-cookie-av need to be grouped in parentheses.\n\n>The origin server should send the following additional HTTP/1.1 response\n>headers, depending on circumstances:\n>\n>   * To suppress caching of the Set-Cookie2 header: Cache-control: no-\n>     cache=\"set-cookie2\".\n\nThis type of example should not be allowed to break across a line.\nThere are many other areas where the text translation needs to be\nfixed for a final draft.\n\n>HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n>long in the past) on responses containing Set-Cookie2 response headers\n>unless they know for certain (by out of band means) that there are no\n>upstream HTTP/1.0 proxies.  HTTP/1.1 servers may send other Cache-\n ^^^^^^^^\nActually, that is downstream.  It is better to call it \"proxies in\nthe response chain\".\n\n>6.  IMPLEMENTATION CONSIDERATIONS\n>\n>Here we speculate on likely or desirable details for an origin server\n>that implements state management.\n\nKinda weak statement.  My inclination is to delete things that don't\nsay anything useful.\n\n>6.1  Set-Cookie2 Content\n>\n>An origin server's content should probably be divided into disjoint\n>application areas, some of which require the use of state information.\n>The application areas can be distinguished by their request URLs.  The\n>Set-Cookie2 header can incorporate information about the application\n>areas by setting the Path attribute for each one.\n>\n>The session information can obviously be clear or encoded text that\n>describes state.  However, if it grows too large, it can become\n>unwieldy.  Therefore, an implementor might choose for the session\n>information to be a key to a server-side resource.  Of course, using a\n>database creates some problems that this state management specification\n>was meant to avoid, namely:\n>\n>  1.  keeping real state on the server side;\n>\n>  2.  how and when to garbage-collect the database entry, in case the\n>      user agent terminates the session by, for example, exiting.\n\nUgh! Can we just delete section 6.1?  If not, then you need to be succinct\nin describing the scope of design decisions and their relative trade-offs.\nI.e., \"Cookie Length\", \"Namespace Allocation\", and \"Direct vs Indirect State\nIdentification\" are all design considerations with separable trade-offs,\nand thus should be discussed individually (or not at all).  Likewise,\na discussion of when cookies should be avoided in favor of one of the\nother alternatives would belong in the introduction.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSION discussion at Munich...",
            "content": "The consensus from our lunch meeting was that\nas long as proxies upgrade all requests to the proxy's\nhighest version number, then the re-version issue\ncould stay as it is.\nResponse version is the servers highest version,\n not the message version.\n\nWhile the message version would have merit,\n it seems that people felt that at this point\n we couldnt justify changing it.\n\nThe only remaining fringe case which I can\nthink of where there may be problems are\nwith the example of the chunked post. \nI dont think we can solve that easily.\n\nI propose an edit like this in the version clauses.\n\n\"Proxy servers MUST upgrade all requests to the highest\nversion supported by the proxy\"\n\n\nAccording to Jim Gettys,\n> \n> \n> There were a set of people who were going to discuss this\n> in the halls, and get back to me (and the working group)\n> with a resolution.\n> \n> I've never seen any resolution. No one caught me in the hall.\n> \n> People involved included Josh and Henry Sanders.\n> \n> Please identify yourselves, and the proposed solution.\n> - Jim Gettys\n> \n\n\n-- \n---\nJosh Cohen\njosh@early.com\n\n\n\n"
        },
        {
            "subject": "draft-lundblade-1pass-mult-alt00.txt vs. alternate",
            "content": "Is this a different model for 'alternates'?\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "On Tue, 11 Nov 1997, Jim Gettys wrote:\n\n> This documents discussions on the implementation list that shows\n> a new protocol issue (IMS_INM_MISMATCH).\n\nMy recollection of intent matches the implementation Roy described.\nIf-None-Match supercedes IMS when both are present.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: draft-lundblade-1pass-mult-alt00.txt vs. alternate",
            "content": ">Is this a different model for 'alternates'?\n\nNo.  It is limited to media types and the syntax isn't even capable\nof handling media types with quoted parameters (which is why I introduced\nthe braces notation).  Even if it didn't have those limitations, the\noverhead of multipart/alternative is an order of magnitude greater\nthan an Alternates with the same expressive power.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "> \n> I don't think it's too late. It is essential that there be some such\n> address, and \"webmaster\" certainly seems to bve the logical choice.\n> Unfortunately, the term  \"webmaster\" has been coopted (too strong a word?)\n> to refer to HTML authors and web page designers. Even so, I don't think it\n> would be overly confusing to use \"webmaster\" in way you suggest. A\n> secondary problem is that many people maintain multiple webs on the same\n> system (e.g., ISPs that provide web hosting) without having a separate\n> domain. Typically, this results in URLs like\n> \n> http://www.whatever.com/~whoever/\n> \n> I'm not sure how to handle this situation.\nI guess the question is, does postmaster refer to the\ndomain or the server machine.\nI think reality wise, postmaster refers to the server most often.\n(ie sendmail can only direct stuff to postmaster for mail it receives,\n not that the domain receives... )\nSo, I think its appropriate that a host running a WWW server must\nhave a webmaster address, which goes to the system maintainer,\nwho can then refer it to the appropriate content author.\nKeep in mind that the message to webmaster may not be a content issue,\nit may be about a server misconfiguration.  In the example you mention\nit's usually the same server for all users, and often for all domains\nserved by that system..\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: draft-lundblade-1pass-mult-alt00.txt vs. alternate",
            "content": "I really meant it the other way around: could the proposed\n'Alternates' header be useful for email delivery, or even\nfor a HTTP delivery of multipart/alternatives?\n\n\n\n"
        },
        {
            "subject": "Re: If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "The question was:\n    Is there a consensus on the correct behavior for a server when it\n    receives a request with conflicting If-None-Match and\n    If-Modified-Since headers, for instance where the I-N-M ETag is\n    outdated but the I-M-S date is good?\n    \nThis is specifically answered for proxy servers in section 13.3.4\n(Rules for When to Use Entity Tags and Last-modified Dates).  The text\nthere says:\n\n   An HTTP/1.1 cache, upon receiving a request, MUST use the most\n   restrictive validator when deciding whether the client's cache entry\n   matches the cache's own cache entry. This is only an issue when the\n   request contains both an entity tag and a last-modified-date\n   validator (If-Modified-Since or If-Unmodified-Since).\n\n(Several uses of the word \"cache\" in that paragraph should probably\nbe \"caching proxy\", since end-client caches presumably do not\nreceive cache-conditional request messages.)\n\nThis is followed by:\n\n     A note on rationale: The general principle behind these rules is\n     that HTTP/1.1 servers and clients should transmit as much non-\n     redundant information as is available in their responses and\n     requests. HTTP/1.1 systems receiving this information will make the\n     most conservative assumptions about the validators they receive.\n\nI think the most straightforward extension of \"these rules\" to\nthe correct behavior for an origin server is to do the same:\nuse the most restrictive validator.  But, as Roy implied when\nhe wrote \"the RFC is less than clear\", the term \"most restrictive\"\nisn't really defined.  So I would replace the first paragraph quoted\nabove with these two paragraphs:\n\n   An HTTP/1.1 origin server, upon receiving a conditional request that\n   includes both a Last-modified date (e.g., in an If-Modified-Since or\n   If-Unmodified-Since header field) and one or more entity tags (e.g.,\n   in an If-Match, If-None-Match, or If-Range header field) as cache\n   validators, MUST NOT return a response status of 304 (Not Modified)\n   unless doing so is consistent with all of the conditional header fields\n   in the request.\n\n   An HTTP/1.1 caching proxy, upon receiving a conditional request that\n   includes both a Last-modified date and one or more entity tags as\n   cache validators, MUST NOT return a locally cached response to the\n   client unless that cached response is consistent with all of the\n   conditional header fields in the request.\n\nDave Morris wrote:\nMy recollection of intent matches the implementation Roy\ndescribed.  If-None-Match supercedes IMS when both are\npresent.\n\nThis is not quite the same thing; allowing INM to supersede IMS\nunconditionally might be wrong if the INM contained a \"weak\"\nentity tag.  However, with a strong entity tag, there should be\nno practical difference between Dave's interpretation and the\nmore conservative one ... except in those rare cases where a\nresource's modification date changes, but its value does not.\n\nHenry's question continues:\n\n    For example, take the following sequence\n    \n    GET /foo.txt HTTP/1.1\n    Host: server.company.com\n    \n    HTTP/1.1 200 OK\n    Last-Modified: Wed, 05 Nov 1997 22:10:48 GMT\n    ETag: \"12345\"\n    Content-Length: ...\n    \n    <data>\n    \n    GET /foo.txt HTTP/1.1\n    Host: server.company.com\n    If-None-Match: \"12344\"\n    If-Modified-Since: Wed, 05 Nov 1997 22:10:48 GMT\n    \n    Assuming the Last-Modified date hasn't changed, what should the server\n    send back as a response to the 2nd request, 304 or 200? I would have\n    thought 200 is correct, since the ETag is invalid or out of date.\n    However the spec indicates that the I-N-M header is to be treated as if\n    it isn't present if the ETag doesn't match, and then the I-M-S would\n    lead to a 304 response.\n\nSection 14.26 (If-None-Match) doesn't exactly say \"ignored\"; it contains\nan apparent contradiction, because it says both:\n\n   If any of the entity tags match the entity tag of the entity that\n   would have been returned in the response to a similar GET request\n   (without the If-None-Match header) on that resource, or if \"*\" is\n   given and any current entity exists for that resource, then the\n   server MUST NOT perform the requested method. [...]\n\nand\n\n   If the request would, without the If-None-Match header field, result\n   in anything other than a 2xx status, then the If-None-Match header\n   MUST be ignored.\n\nThese excerpts both contradict the existing language in 13.3.4 (as it\napplies to caching proxies) and the revised language I proposed above.\nI.e., if the entity tags don't match, but the dates do match, 13.3.4\nsays \"return 200\", but the latter of these two excerpts implies \"return\n304\".\n\nI think that's a real bug.\n\nIf the entity tags do match, but the dates don't match, 13.3.4 also\nsays \"return 200\", but the former of these two excerpts implies\n\"return 304\".  I think that's also a bug, but this one is probably\nnot of much practical consequence unless the server is using\nweak entity tags.\n\nI would change the first sentence quoted above to:\n\n   If any of the entity tags match the entity tag of the entity that\n   would have been returned in the response to a similar GET request\n   (without the If-None-Match header) on that resource, or if \"*\" is\n   given and any current entity exists for that resource, then the\n   server MUST NOT perform the requested method, unless required\n   to do so because the resource's modification date fails to\n   match that supplied in an If-Modified-Since header field in the\n   request. [...]\n\nand the second sentence to:\n\n   If the request would, without the If-None-Match header field, result\n   in anything other than a 2xx or 304 status, then the If-None-Match\n   header MUST be ignored.  (See section 13.3.4 for a discussion of\n   server behavior when both If-Modified-Since and If-None-Match\n   appear in the same request.)\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "On Nov 12, 12:22pm, Jeffrey Mogul wrote:\n\n> This is not quite the same thing; allowing INM to supersede IMS\n> unconditionally might be wrong if the INM contained a \"weak\"\n> entity tag.  However, with a strong entity tag, there should be\n> no practical difference between Dave's interpretation and the\n> more conservative one ... except in those rare cases where a\n> resource's modification date changes, but its value does not.\n>\n>\n>-- End of excerpt from Jeffrey Mogul\n\nActually the case where the resource's modification date changes\nwithout its value changing is more common than you'd think, because\nof how people implement dns-load balancing to improve performance.\nFrom what I've seen, it's common for changes to be made on one machine\nand those changes propogated to the other machines in the response\ngroup.  Unless you're very careful with that propogation, it's easy\nfor the modification time to change as a result of the movement.  Since\nthere is no guarantee that a client checking back on a resource\nwill get the same member of the response group as it originally got,\nyou can pretty easily run into this case.  Good implementation advice\non the issue would, however, solve the problem in most cases.\n\nI don't think that changes the logic you've applied to the situation;\nI do believe, however, you'll get a non-trivial number of cases where that\nconservative approach will result in the transmission of a resource\nthat is the same as the one the client has.\n\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: draft-lundblade-1pass-mult-alt00.txt vs. alternate",
            "content": "At 19:35 11/11/97 PST, Larry Masinter wrote:\n>Is this a different model for 'alternates'?\n\nI think so.\n\nSince my first response to this draft, I have thought a little more...\n\nThe main point I would make is that the \"types:\"  proposal (per Lundblade)\nlists *only* the MIME content-types which are present.\n\nOTOH, the \"alternates:\" proposal also allows other negotiable features to\nbe incorporated into the list of options through the extension mechanism.\nSpecifically, it reserves \"features\" for the kind of negotiable feature\ndescribed in Koen's TCN draft and its progeny.\n\nWhich leads me to the question:\n\nCan the 'alternates' proposal be used as a MIME 'multipart/alternate'\nheader in the fashion suggested by Lundblade?\n\nI think the answer hinges mainly on the use of \"absolute URIs\" in the\n'alternates' draft, which seems (at best) rather awkward when trying to\nrefer to body parts within the same MIME entity.  But I am not well\nqualified in this matter.\n\n\n(As an aside, for the 'alternates' header it might be worth making an\nexplicit reference to the Freed/Moore RFC 2231 (?) describing multiline\nMIME headers.  It seems to be a good match.)\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Re: draft-lundblade-1pass-mult-alt00.txt vs. alternate",
            "content": "> I think the answer hinges mainly on the use of \"absolute URIs\" in the\n> 'alternates' draft, which seems (at best) rather awkward when trying to\n> refer to body parts within the same MIME entity.  But I am not well\n> qualified in this matter.\n\nThe 'cid:' URI is used just for this purpose: to refer to body parts within the\nsame MIME entity. (RFC 2111: Content-ID and Message-ID Uniform Resource\nLocators.)\n\nLarry\n\n\n\n"
        },
        {
            "subject": "RE: If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "Jeff Mogul writes:\n\n> Section 14.26 (If-None-Match) doesn't exactly say \"ignored\"; it\n> contains\n> an apparent contradiction, because it says both:\n> \n>    If any of the entity tags match the entity tag of the entity that\n>    would have been returned in the response to a similar GET request\n>    (without the If-None-Match header) on that resource, or if \"*\" is\n>    given and any current entity exists for that resource, then the\n>    server MUST NOT perform the requested method. [...]\n> \n> and\n> \n>    If the request would, without the If-None-Match header field,\n> result\n>    in anything other than a 2xx status, then the If-None-Match header\n>    MUST be ignored.\n> \nThere's also the sentence\n\nIf none of the entity tags match, or if \"*\" is given and no\ncurrent\n   entity exists, then the server MAY perform the requested method as if\n   the If-None-Match header field did not exist.\n\nWhich is what I was referring to when I mentioned treating the\nheader as if it didn't exist. I like your proposed alterations, we just\nneed to strike this sentence too.\n\nHenry\n\n\n\n"
        },
        {
            "subject": "Regarding Authenticatio",
            "content": "Few issues related to Authentication  are as following :\n\n1> In the authentication credentials field defined as following\n\ncredentials  = basic-credentials\n|auth-scheme #auth-param\nin RFC2068.\n\na> Does this mean the server must produce parse error if the client \nsends two or more scheme credentials ?( this problem doesn't exist in \nHTTP1.0 as it support only one scheme)\nb> If any server provides this additional functionality of \nparsing(handling) one or more scheme credentials then can that server be \nsaid as RFC2068 compliant.\n2>If the only one scheme is allowed and  if the agent wants to send a request \nwith the authentication scheme credentials before the challenge \n(unauthorised response)then it really doesn't have much flexibility.A \nsort of agent side negotiation for the authentication schemes.\n\n  If agent is allowed to sent multiple schemes the following  issues \naraise :-\n   1> If an user agent sends 3 schemes and server supports 2 of these \nschemes for the requested realm then\na>  what must be the action of server ? Should it validate realm with \n all the 2 possible schemes or it should take the safest  scheme ?\n   2> Say if the server validates the realm using two or more schemes \nthen if it finds that through one scheme the client is valid and through \nother it is invalid.Then \na>what must be the action of the server? \n   It can always be the server specific but \na>what is the safest option ?\n\nRegards,\nSam.\n\n \n\n\n\n\n"
        },
        {
            "subject": "Re: Regarding Authenticatio",
            "content": ">>>>> \"SR\" == Sambasiva Rao <sams@wipinfo.soft.net> writes:\n\nSR> Few issues related to Authentication  are as following :\n\n1> In the authentication credentials field defined as following\n\nSR> credentials  = basic-credentials\nSR> |auth-scheme #auth-param\nSR> in RFC2068.\n\nSR> a> Does this mean the server must produce parse error if the client\nSR> sends two or more scheme credentials ?( this problem doesn't exist in\nSR> HTTP1.0 as it support only one scheme)\n\n  I think that there is no reason to allow for multiple sets of\n  credentials; it doesn't really add any usefull feature I can think\n  of, and introduces a number of other possible errors (what if one\n  set is ok and another is not?).\n\nSR> 2> If the only one scheme is allowed and if the agent wants to\nSR> send a request with the authentication scheme credentials before\nSR> the challenge (unauthorised response)then it really doesn't have\nSR> much flexibility.  A sort of agent side negotiation for the\nSR> authentication schemes.\n\n  I don't think that there is any interoperability reason why you\n  should not send unsolicited credentials (that is, I don't think that\n  it breaks the protocol itself to do so), but it makes no sense from\n  a security point of view:\n\n  - With Basic all you're doing is publishing your password to someone\n    who may not need it or have any reason to get it (which is what\n    you're doing every time you use Basic anyway...)\n\n  - With Digest you can't generate valid credentials without the nonce\n    from the challenge anyway.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Regarding Authenticatio",
            "content": "According to Scott Lawrence,\n> \n>   I don't think that there is any interoperability reason why you\n>   should not send unsolicited credentials (that is, I don't think that\n>   it breaks the protocol itself to do so), but it makes no sense from\n>   a security point of view:\n> \n>   - With Basic all you're doing is publishing your password to someone\n>     who may not need it or have any reason to get it (which is what\n>     you're doing every time you use Basic anyway...)\n> \n>   - With Digest you can't generate valid credentials without the nonce\n>     from the challenge anyway.\n> \nI agree that you dont generally send unsolicited credentials, but\nthe context isnt necessarily clear.  If you are challenged for credentials\ninitially, but a long while later (potentially hours) in the same\nbrowser session, you might send those same credentials again\nin a later transaction.  One could argue that this\nwould be unsolicited, since its possible for those\ncredentials to be invalid at the later time.\n\n-- \n---\nJosh Cohen\njosh@early.com\n\n\n\n"
        },
        {
            "subject": "new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "attached mail follows:\nThe specification for Range now says:\n\n            14.36.2 Range Retrieval Requests\n\n            HTTP retrieval requests using conditional or unconditional\n            GET methods may request one or more sub-ranges of the\n            entity, instead of the entire entity, using the Range\n            request header, which applies to the entity returned as the\n            result of the request:\n\nHowever, I believe that the spec is silent on what the server\nshould return for a Range if it also uses a content-coding, such\nas \"gzip\".\n\nMore concretely:\n\nSuppose the client sends:\n\nGET /foo.html HTTP/1.1\nHost: bar.com\nRange: bytes=100-199\nAccept-Encoding: gzip\n\nand the server returns\n\nHTTP/1.1 206 OK\nContent-Range: bytes 100-199/400\nContent-Type: text/html\nContent-coding: gzip\n\nThen should then 100 bytes covered by the Range/Content-Range\nrefer to the second hundred bytes of the HTML file before\ncompression, or to the second hundred bytes of the compressed\nform?\n\nIt seems to me that the only reasonable interpretation is that\nRange/Content-Range should apply to the unencoded form of the\nresponse, since the client's ultimate goal is to obtain a specific\npiece of the unencoded form; the use of compression is only\na temporary phase that the response goes through while it is\nbeing transmitted.\n\nHowever, by a strict reading of the phrase \"which applies to the entity\nreturned as the result of the request\", combined with the HTTP/1.1\ndefinition of \"entity\":\n               The information transferred as the payload of a request\n               or response. An entity consists of metainformation in the\n               form of entity-header fields and content in the form of\n               an entity-body, as described in section 7.\none would have to use the other interpretation: that the Range\napplies to the compressed form, since this is the \"payload of the\n[response]\".\n\nOne way to resolve this issue would be to go back about 18 months\nand undo the decision made about adopting the MIME definition\nof \"entity.\"  I argued then that this was a mistake; I continue\nto believe that this is not only a mistake, but one that was made\nwithout any lack of warning.  But I don't expect to win this battle.\n\nAnother approach would be to define a new term, such as \"instance\",\nto mean what \"entity\" should have meant (by the standard English\ndefinition of the word \"entity\").  But I doubt you would accept\nthis change at this stage in the process.\n\nSo, with some trepidation that this is not the only potential\nerror lurking in the the spec as the result of the \"entity\" term,\nI suggest changing the first paragraph of 14.36.2 to read:\n\n            HTTP retrieval requests using conditional or unconditional\n            GET methods may request one or more sub-ranges of the\n            entity, instead of the entire entity, using the Range\n            request header, which applies to the entity returned as the\n            result of the request, prior to the application of\n    any content-coding:\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "Right. There i a distinction between content issues (broken links, etc.) \nand server issues and sometimes people will be sending mail rtegarding one\nor the other. My solution is to set up webmaster@wnetc.com, and almost all\nmail I receive pertains to issues I can handle. If necessary, I can\nforward it on. I suspect tht most people visiting the site won't know the\ndifference between a broken CGI or an errant redirect (things I can\naddress directly) on the one hand, and DNS or connectivity routing issues\non the other. \n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Without advocating a particular position, I would like to note that\none of the 'justifications' for byte ranges was the ability to\ncontinue retrieving a previously interrupted response. In that mode,\nif I were the developer of the client, I would want the byte range\nto apply to the compressed form.\n\nIn the usage associated with partial retrieval of structured data such as\na PDF file, I'd want the byte ranges to apply to the uncompressed \nresource.\n\nIt would be helpful to know what actual use is being made of byte ranges.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "I have said before that the reason there is both a Content-Encoding\nand a Transfer-Encoding is that the former is a property of the\nresource and the latter is a transmission issue.  The reason I say\nthis is because the other metadata describing the entity always\ndescribes the entity-body as whole, e.g.,\n\n   Content-Encoding( Content-Type( data ) )\n\nso that things like Content-MD5 and Range requests apply to the whole.\n\n>Then should then 100 bytes covered by the Range/Content-Range\n>refer to the second hundred bytes of the HTML file before\n>compression, or to the second hundred bytes of the compressed\n>form?\n\nThe compressed form.\n\n>It seems to me that the only reasonable interpretation is that\n>Range/Content-Range should apply to the unencoded form of the\n>response, since the client's ultimate goal is to obtain a specific\n>piece of the unencoded form; the use of compression is only\n>a temporary phase that the response goes through while it is\n>being transmitted.\n\nThis would assume the part of the server performing the range\nhas access to the non-compressed data, which is false.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Dave Morris writes\n\n    one of the 'justifications' for byte ranges was the ability to\n    continue retrieving a previously interrupted response. In that\n    mode, if I were the developer of the client, I would want the byte\n    range to apply to the compressed form.\n\nIf one starts with three assumptions (which might even be \"facts\"):\n\n    (1) The interruption affects the tail of a retrieval.\n    \n    (2) Most HTTP retrievals are attempting to transfer the whole resource\n    value\n    \n    (3) The compression algorithms that we actually use are one-pass\n    algorithms with finite windows, and so it is possible to extract\n    a large portion of the uncompressed form from a partial copy\n    of the compressed form.\n\nThen even in the case where one is recovering from an interrupted\nretrieval of a compressed form, you will be in a situation where\nyou have been able to decompress some prefix of the full file.\n\nThis means that the ability to ask the server for a compressed\ncopy of the part of the file that you don't have is sufficient to\nrecover from the loss.  Further, because of the relatively small\nwindow used by compression algorithms, the result would not be\nmuch larger than if you were able to retransmit exactly the bytes\nof the compressed form that were previously unavailable.\n\nFurther, a slice out of the middle of a compressed form is totally\nuseless by itself; e.g., you can't decompress the output of gzip\nwithout knowing the first few bytes of the output, because the\nrest of the compression depends on that prefix.  On the other hand,\na compressed form of a slice of the uncompressed form has some\nvalue, since it is possible to extract the unencoded slice.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Code 419 should have been 417 (editorial issue CODES",
            "content": "When I prepared draft 08, (now rev-00), I goofed and did not properly assign\nthe response code to be contiguous with previous response codes (416 was\nthe previous code).\n\nUnless people complain, rev-01 will have the code Expectation Failed\nassigned as 417.\n\nYour editor who couldn't count that high right,\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: comments on draft-ietf-http-state-man-mec04.tx",
            "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote on Tue, 11 Nov 1997 16:19:03 -0800:\n\n  > Larry asked me to do a detailed review of the draft, which is probably\n  > a dangerous thing.  I do not consider myself to be a Cookie expert;\n  > in fact, I avoid them like the plague.  My comments are on general\n(Good judgement!)\n  > draftyness issues.\n\nThanks for the extensive comments.  (Turn about is fair play? :-)\n\n  > \n  > >                 HTTP State Management Mechanism (Rev1)\n  > \n  > I'd prefer not to have \" (Rev1)\" in the title.\n\nI thought it was desirable to distinguish this from RFC 2109.\n\n  > \n  > >2.  TERMINOLOGY\n  > >\n  > >The terms user agent, client, server, proxy, and origin server have the\n  > >same meaning as in the HTTP/1.1 specification [RFC 2068].\n  > >\n  > >Host name (HN) means either the host domain name (HDN) or the numeric\n  > >Internet Protocol (IP) address of a host.  The fully qualified domain\n  > >name is preferred; use of numeric IP addresses is strongly discouraged.\n  > >...\n  > \n  > A comment should be included regarding support (or non-support) of IPv6\n  > addresses [we got pinged on this for the URI draft].\n\nCan you suggest wording?  I wanted to avoid specifying syntax for either,\nand I chose the neutral term \"numeric IP addresses\" to (try to) imply both.\n(I also seem to remember that the proposed text representation of IPv6\naddresses conflicts with URI syntax....)\n\n  > \n  > In general, I find the terminology section more confusing than useful,\n  > even though I do know what is intended.  I find it unlikely that anyone\n  > not involved in the mailing list discussions would have a clue as to\n  > what is being described.  It would be better to postpone the description\n  > of hostname dot comparison until it can be described as part of the\n  > matching algorithm.\n\nYMMV.  I prefer to put the terms in one place so someone can find them\neasily if they need to.  It's a bit like the HTTP specification -- you\ncan't really understand this section until you understand the whole thing.\n\n  > \n  > >4.  OUTLINE\n  > \n  > Why is the entire protocol specification under the section \"OUTLINE\"?\n\nYeah, I guess that's pretty silly.  I'll think of a better heading.\n\n  > \n  > >We outline here a way for an origin server to send state information to\n  > >the user agent, and for the user agent to return the state information\n  > >to the origin server.  The goal is to have a minimal impact on HTTP and\n  > >user agents.  Only origin servers that need to maintain sessions would\n  > >suffer any significant impact, and that impact can largely be confined\n  > >to Common Gateway Interface (CGI) programs, unless the server provides\n  > >more sophisticated state management support.  (See Implementation\n  > >Considerations, below.)\n  > \n  > CGI has nothing to do with it.  \"particular resource namespaces\" would\n  > be more accurate.  \", unless the server provides more sophisticated\n  > state management support\" says nothing.\n\nWhen I first wrote this section, about 20 months ago, I was trying to\nanticipate concerns that implementers might have about how much of a\nburden cookies would be to running a server.  I was trying to make a\npoint that the support was relatively localized.  In \"the old days\",\ncookies got handled by CGIs, and only those CGIs that needed cookies\nneeded to support them.  Newer servers may provide built-in support\n(\"more sophisticated...\") for cookies, but that support isn't essential\nto their support.\n\nYour description is correct from the HTTP point of view, of course.\n\n  > \n  > [...]\n  > >A user agent returns a Cookie request header (see below) to the origin\n  > >server if it chooses to continue a session.  The origin server may\n  > >ignore it or use it to determine the current state of the session.  It\n  > >may send back to the client a Set-Cookie2 response header with the same\n  > >or different information, or it may send no Set-Cookie2 header at all.\n  > >The origin server effectively ends a session by sending the client a\n  > >Set-Cookie2 header with Max-Age=0.\n  > \n  > A user agent never returns a Cookie request header field --\n  > it *sends* a Cookie request header field containing a field value\n  > matching that of a previously received Set-Cookie response header field.\n  > Likewise, a server never \"sends back\" a Set-Cookie2 response header field;\n  > it either sends a Set-Cookie2 or it doesn't.\n\nYou're thinking of it from the HTTP perspective, and I'm describing it\nfrom the application perspective.  That's why I was (or tried to be)\ncareful about using the terms \"request header\" and \"response header\" to\nidentify the HTTP role, as distinct from the application role.\n\n  > \n  > Much of this discussion is unnecessarily confusing.  I think it would be\n  > better to define the cookie exchange protocol as a state machine,\n\nPerhaps.  (The thought of doing ASCII art for both .txt and .ps\nintimidates me a bit.)\n\n  > rather than enumerate in prose all of the optional behavior of each party\n  > at any given time in the exchange.  For example, I find the TCP specification\n  > to be less confusing even though it describes a much more complex state\n  > machine than is implicit in the cookies protocol.\n\nPerhaps you noticed I tried to present the behavior in a\ncomponent-centric way.  That means the server-specific behaviors are in\n4.2.1, and the user-agent-specific behaviors are in 4.3.  I don't give\na consolidated overview of the cookie protocol, which I think is what\nyou would prefer.\n\n  > \n  > >4.2.2  Set-Cookie2 Syntax  The syntax for the Set-Cookie2 response\n  > >header is\n  > >\n  > >set-cookie      =       \"Set-Cookie2:\" cookies\n  > >cookies         =       1#cookie\n  > >cookie          =       NAME \"=\" VALUE *(\";\" set-cookie-av)\n  > >NAME            =       attr\n  > >VALUE           =       value\n  > >set-cookie-av   =       \"Comment\" \"=\" value\n  > >                |       \"CommentURL\" \"=\" <\"> http_URL <\">\n  > >                |       \"Discard\"\n  > >                |       \"Domain\" \"=\" value\n  > >                |       \"Max-Age\" \"=\" value\n  > >                |       \"Path\" \"=\" value\n  > >                |       \"Port\" [ \"=\" <\"> 1#portnum <\"> ]\n  > >                |       \"Secure\"\n  > >                |       \"Version\" \"=\" 1*DIGIT\n  > >portnum =       1*DIGIT\n  > \n  > Unless you wish to define the precedence of \"|\" in your BNF, each\n  > of the set-cookie-av need to be grouped in parentheses.\n\nObviously the typography dictates the precedence. :-)  Seriously, do\nyou think there's anything confusing in the current format?  I fear that\nadding ()'s will make it harder to understand, not easier.  Also,\ntraditionally, alternation is lowest precedence after '=', no?\n  > \n  > >The origin server should send the following additional HTTP/1.1 response\n  > >headers, depending on circumstances:\n  > >\n  > >   * To suppress caching of the Set-Cookie2 header: Cache-control: no-\n  > >     cache=\"set-cookie2\".\n  > \n  > This type of example should not be allowed to break across a line.\n  > There are many other areas where the text translation needs to be\n  > fixed for a final draft.\n\nI agree.  It's strange -- I've got hyphenation disabled in the (nroff/troff)\nsource.  I guess that only controls intra-word hyphenation.  (I wonder if I\ncan even persuade nroff/troff not to break around '-'.)\n\n  > \n  > >HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n  > >long in the past) on responses containing Set-Cookie2 response headers\n  > >unless they know for certain (by out of band means) that there are no\n  > >upstream HTTP/1.0 proxies.  HTTP/1.1 servers may send other Cache-\n  >  ^^^^^^^^\n  > Actually, that is downstream.  It is better to call it \"proxies in\n  > the response chain\".\n\n:-)  I think I had \"downstream\" originally, then changed it to \"upstream\"\nin response to a comment from Yaron Goland.  I like your wording better:\nI don't have to know up from down. :-)\n\n  > \n  > >6.  IMPLEMENTATION CONSIDERATIONS\n  > >\n  > >Here we speculate on likely or desirable details for an origin server\n  > >that implements state management.\n  > \n  > Kinda weak statement.  My inclination is to delete things that don't\n  > say anything useful.\n\nOkay.\n\n  > \n  > >6.1  Set-Cookie2 Content\n  > >\n  > >An origin server's content should probably be divided into disjoint\n  > >application areas, some of which require the use of state information.\n  > >The application areas can be distinguished by their request URLs.  The\n  > >Set-Cookie2 header can incorporate information about the application\n  > >areas by setting the Path attribute for each one.\n  > >\n  > >The session information can obviously be clear or encoded text that\n  > >describes state.  However, if it grows too large, it can become\n  > >unwieldy.  Therefore, an implementor might choose for the session\n  > >information to be a key to a server-side resource.  Of course, using a\n  > >database creates some problems that this state management specification\n  > >was meant to avoid, namely:\n  > >\n  > >  1.  keeping real state on the server side;\n  > >\n  > >  2.  how and when to garbage-collect the database entry, in case the\n  > >      user agent terminates the session by, for example, exiting.\n  > \n  > Ugh! Can we just delete section 6.1?  If not, then you need to be succinct\n  > in describing the scope of design decisions and their relative trade-offs.\n  > I.e., \"Cookie Length\", \"Namespace Allocation\", and \"Direct vs Indirect State\n  > Identification\" are all design considerations with separable trade-offs,\n  > and thus should be discussed individually (or not at all).  Likewise,\n  > a discussion of when cookies should be avoided in favor of one of the\n  > other alternatives would belong in the introduction.\n\nI was just trying to point out some of the design considerations.\nBecause the various tradeoffs are obviously implementation-dependent, I\ndidn't want to (nor did I think I should) go into them in detail.  The\nquestion is whether the draft would be better off without the section,\nor with the admittedly meager comments that are there.  I thought it was\nat least worthwhile to point out the issues.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "This is off-topic on this issue, but there was some question\nas to whether a delta coding was best though of as a 'range'\nor an 'encoding'.\n\nSo, can you delta-encode a range? I mean, does the same argument hold?\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Larry Masinter:\n    This is off-topic on this issue, but there was some question as to\n    whether a delta coding was best though of as a 'range' or an\n    'encoding'.\n\n    So, can you delta-encode a range? I mean, does the same argument\n    hold?\n    \nThis isn't exactly off-topic, since the reason that I realized that\nthere is an ambiguity with respect to Range + compression is that\na group of us are trying to figure out how to write a Delta-encoding\nspec, and the same kinds of issue comes up.\n\nFor those of you who don't know what \"Delta-encoding\" means, see\n    Jeffrey C. Mogul, Fred Douglis, Anja Feldmann, and Balachander\n    Krishnamurthy. Potential benefits of delta encoding and data\n    compression for HTTP. In Proc. SIGCOMM '97 Conference, pages\n    181-194.  ACM SIGCOMM, Cannes, France, September, 1997\nor check out the expanded (& somewhat corrected) version at\n    http://www.research.digital.com/wrl/techreports/abstracts/97.4.html\n\nAnyway, delta-encoding is most certainly NOT best thought of as a\n\"range\"; the algorithms that people actually use are really not\ndescribable in this way.\n\nI think it could be described as \"content-coding\" (this is in fact\nwhat we proposed in the paper).  However, it is definitely of interest\nto be able to apply Range retrievals together with delta-encoding,\nand we are beginning to wrestle with how to actually specify that.\n\nBut for the purposes of HTTP/1.1, please don't think too hard about\ndelta-encoding; it will only confuse the issue.  I.e., I don't think\nit is necessary to solve the delta-encoding problems in order to\nfigure out how Range interacts with compression.  Delta-encoding is\nnot exactly analogous to compression, and it would tremendously confuse\nthings to pretend that there is an exactly analogy.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Jeffrey Mogul wrote:\n> \n> Dave Morris writes\n> \n>     one of the 'justifications' for byte ranges was the ability to\n>     continue retrieving a previously interrupted response. In that\n>     mode, if I were the developer of the client, I would want the byte\n>     range to apply to the compressed form.\n> \n> If one starts with three assumptions (which might even be \"facts\"):\n> \n>     (1) The interruption affects the tail of a retrieval.\n> \n>     (2) Most HTTP retrievals are attempting to transfer the whole resource\n>     value\n> \n>     (3) The compression algorithms that we actually use are one-pass\n>     algorithms with finite windows, and so it is possible to extract\n>     a large portion of the uncompressed form from a partial copy\n>     of the compressed form.\n> [...]\n\nJeff,\nI think what you're missing is that most servers store files (er,\nentities?) already compressed as, for example, .gzip files.\n\nCan anyone offer an example of a server that compresses content on the\nfly and returns it in that form?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Dave Kristol writes:\n    I think what you're missing is that most servers store files (er,\n    entities?) already compressed as, for example, .gzip files.\n    \nI understand that.  What I think you're missing is that this not\nthe most desirable state of affairs.  On-the-fly compression would\nimmensely useful in reducing bandwidth requirements for non-image\ndata (see our SIGCOMM '97 paper, and also the SIGCOMM '97 paper\nby Gettys et al.)  HTTP/1.0 doesn't really support this, but we've\ntried to make it possible in HTTP/1.1\n\nWe want to encourage more efficient use of the Internet, not freeze\nthe current (and inefficient) practice.\n\nHaving said that: I realize that there may be a conflict between the\nright thing to do for Ranges with on-the-fly compression, and for\nRanges with .gzip files.  And maybe the spec needs to be able to make\nthe distinction explicit, rather than us arguing about which single\nmode should be supported?\n\n    Can anyone offer an example of a server that compresses content on the\n    fly and returns it in that form?\n\nI believe that Henrik et al. have prototyped this, and their experiences\nwere largely the inspiration for fixing the bugs in Accept-Encoding.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "> \n> The webmaster@website convention is falling out of use.  Has anyone\n> considered making it a requirement for HTTP/1.1 compliance?  Is it too\n> late to even be talking about this?\n\nThere is another RFC in process that was addressing commonly used\ne-mail addresses like abuse@domain and webmaster@domain.\n\nI don't recall where it stands with respect to standards-track\nstatus and completion, but I think this question is\nnot so central to HTTP that this group should spend effort\non something being addresses elsewhere.\n\n--\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "Well, for what it's worth, I'm seeing quite a few 206's in my access \nlogs, mostly due to Netscape clients resuming interrupted retrievals \nof GIF files. The files aren't compressed on disk, but byte range \nrequests certainly are being used.\n\n===\nGregory Woodhouse\nSan Francisco CIO Field Office - Infrastructure\ngregory.woodhouse@med.va.gov\n+1 415 744 6362\nMay the dromedary be with you\n\n\n----------\nFrom:  David W. Morris [SMTP:dwm@xpasc.com]\nSent:  Friday, November 14, 1997 1:16 PM\nTo:  Jim Gettys\nCc:  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; \nhttp-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject:  Re: new editorial issue RANGE_WITH_CONTENTCODING\n\n\nWithout advocating a particular position, I would like to note that\none of the 'justifications' for byte ranges was the ability to\ncontinue retrieving a previously interrupted response. In that mode,\nif I were the developer of the client, I would want the byte range\nto apply to the compressed form.\n\nIn the usage associated with partial retrieval of structured data such \nas\na PDF file, I'd want the byte ranges to apply to the uncompressed\nresource.\n\nIt would be helpful to know what actual use is being made of byte \nranges.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: comments on draft-ietf-http-state-man-mec04.tx",
            "content": ">  > I'd prefer not to have \" (Rev1)\" in the title.\n>\n>I thought it was desirable to distinguish this from RFC 2109.\n\nThat should be done in the abstract (and it is, as I recall).\nI think the draft should contain exactly what the final RFC will\ncontain, aside from the Status section and Header/Footer.\n\n>  > A comment should be included regarding support (or non-support) of IPv6\n>  > addresses [we got pinged on this for the URI draft].\n>\n>Can you suggest wording?  I wanted to avoid specifying syntax for either,\n>and I chose the neutral term \"numeric IP addresses\" to (try to) imply both.\n>(I also seem to remember that the proposed text representation of IPv6\n>addresses conflicts with URI syntax....)\n\nI don't know what to suggest.  What happens when a cookie includes an IPv6\naddress (or is that even possible)?\n\n>  > In general, I find the terminology section more confusing than useful,\n>  > even though I do know what is intended.  I find it unlikely that anyone\n>  > not involved in the mailing list discussions would have a clue as to\n>  > what is being described.  It would be better to postpone the description\n>  > of hostname dot comparison until it can be described as part of the\n>  > matching algorithm.\n>\n>YMMV.  I prefer to put the terms in one place so someone can find them\n>easily if they need to.  It's a bit like the HTTP specification -- you\n>can't really understand this section until you understand the whole thing.\n\nTerms, yes -- algorithms, no.  The hostname dot comparison doesn't make\nany sense without the context for why the hostname dot comparison exists.\n\n>When I first wrote this section, about 20 months ago, I was trying to\n>anticipate concerns that implementers might have about how much of a\n>burden cookies would be to running a server.  I was trying to make a\n>point that the support was relatively localized.  In \"the old days\",\n>cookies got handled by CGIs, and only those CGIs that needed cookies\n>needed to support them.  Newer servers may provide built-in support\n>(\"more sophisticated...\") for cookies, but that support isn't essential\n>to their support.\n>\n>Your description is correct from the HTTP point of view, of course.\n\nI should have also mentioned that I was reading it from that view.\nI like implementation advice, but it needs to be clear what the\nprotocol requirements are as opposed to how one might use them.\nA proposed standard needs to define the interface and avoid defining\nthe application.\n\n>  > Unless you wish to define the precedence of \"|\" in your BNF, each\n>  > of the set-cookie-av need to be grouped in parentheses.\n>\n>Obviously the typography dictates the precedence. :-)  Seriously, do\n>you think there's anything confusing in the current format?  I fear that\n>adding ()'s will make it harder to understand, not easier.  Also,\n>traditionally, alternation is lowest precedence after '=', no?\n\nIt is in the EBNF defined by the drums WG.  I tried to avoid such implied\nthings in the HTTP specs, though I note that a few are in RFC 2068.\nSo, I suppose you are correct.  I'm just one of those programmers who\nlikes to put parentheses around conditions, I guess.\n\n>  > >The origin server should send the following additional HTTP/1.1 response\n>  > >headers, depending on circumstances:\n>  > >\n>  > >   * To suppress caching of the Set-Cookie2 header: Cache-control: no-\n>  > >     cache=\"set-cookie2\".\n>  > \n>  > This type of example should not be allowed to break across a line.\n>  > There are many other areas where the text translation needs to be\n>  > fixed for a final draft.\n>\n>I agree.  It's strange -- I've got hyphenation disabled in the (nroff/troff)\n>source.  I guess that only controls intra-word hyphenation.  (I wonder if I\n>can even persuade nroff/troff not to break around '-'.)\n\nNope, from very painful man-page-writing experience.  I suggest line breaks.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "The fact that compression is a good thing is not an issue -- all of\nthe studies I've seen have been just as valid for pre-compressed data\nas for on-the-fly compression (in fact, most of the timing comparisons\nwere done only with pre-compressed data).\n\n>Having said that: I realize that there may be a conflict between the\n>right thing to do for Ranges with on-the-fly compression, and for\n>Ranges with .gzip files.  And maybe the spec needs to be able to make\n>the distinction explicit, rather than us arguing about which single\n>mode should be supported?\n\nThe spec does make it explicit, at least to the extent that general\ndiscussion of encodings can be explicit.  On-the-fly compression is\na transfer-coding.  Source-based compression is a content-coding.\n\nThe problem is that people keep trying to wedge both into content-coding\ninstead of just defining on-the-fly compression with Transfer-Encoding.\n\nWhether or not Range applies to content-coded entities is not an issue.\nThe implementations demonstrate that it does.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Cache-control: private and offlin",
            "content": "I'd expect 'Cache-control: private' to mean that no one else has\neasy access to the cache. I think that you might wind up doing\nthis differently for 95 and NT, where the NT cache entries could\nbe read-protected per user.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "I'll go along with Roy. Range should apply *after* the content\nencoding. If we need pre-compressed ranges, then let's define\ncompressed transfer encodings.\n\n\n\n"
        },
        {
            "subject": "Re: Cache-control: private and offlin",
            "content": ">Scenario: A server has generated a personalized response for a user that is\n>not suitable for caching by proxies. However the server does want the\n>client's cache to cache it so that it will be available for off-line. The\n>catch is, the user is running on windows 95 which uses a single cache for\n>everyone who access the computer.\n>\n>Solution 1: The server places a cache-control: private header on the\n>response. Thus the proxy will not cache the response. The windows 95 machine\n>is smart enough to see the cache-control: private header and to store it in\n>the shared cache tagged with the user's name. Thus if another user logs into\n>the machine, that user WILL NOT be served up that particular cached page\n>because the name associated with the entry doesn't match. However, the new\n>user could go to the cache directory and sniff through it to get the actual\n>file.\n\nYep.  Be sure to include the Expires trick as well to prevent a good\nHTTP/1.0 cache from caching it.\n\n>Solution 2: The server could place a set-cookie on the response, thus\n>causing the proxy to not cache it, but the client side cache is smart enough\n>to cache such a response but tag it with the user's name.\n\nI think you will find that an HTTP/1.0 proxy will cache it.\n\n>So the question becomes, does solution 1 violate the intent of\n>cache-control: private? There is protection from inadvertently coming across\n>another user's page and the level of security is the same as the level of\n>security for all materials on the machine but there isn't ACL control.\n\nSolution 1 is the intent of private.  The private directive means\nokay-if-it-is-not-used-by-some-other-user.  It doesn't mean privacy or\nsecure, since that is already lost in a non-secure transmission.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "At 15:52 14/11/97 -0800, Roy T. Fielding wrote:\n>The spec does make it explicit, at least to the extent that general\n>discussion of encodings can be explicit.  On-the-fly compression is\n>a transfer-coding.  Source-based compression is a content-coding.\n>\n>The problem is that people keep trying to wedge both into content-coding\n>instead of just defining on-the-fly compression with Transfer-Encoding.\n\nI am *assuming* that your reference to 'transfer-encoding' maps to MIME\n'content-transfer-encoding'.  If this assumption is false, please ignore\nthis comment...\n\nThe MIME specification STRONGLY discourages (its capitals) the creation of\nnew content-transfer-encoding values.  I would think that this would\nencourage designers to try and shoehorn transfer-encoding semantics into\ncontent-coding headers.\n\nGK.\n---\n\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Acknowledgements for Draft standard..",
            "content": "I'm in the home stretch of editing the next draft, and if you are\nnot currently acknowledged in the specification, and believe you should\nbe, please send me mail so I won't forget.\n\nI'll get in any I recieve by Wednesday of this week.  It is unlikely that \nthis will be the draft that goes to draft standard, though we may come very \nclose, so don't panic if you don't see this message in time to respond.\n\nDraft submission deadline is Friday, and I need to have a chance to actually \nproduce the bits (which has been non-trivial in recent memory).\n\nYour Editor...\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: webmaster&#64;websit",
            "content": "I believe that \n\nftp://ftp.ietf.org/internet-drafts/draft-crocker-stdaddr-02.txt\n\nis in (or has passed) \"last call\" as a \"Best Current Practice\":\n\n> This specification enumerates and describes Internet mail\n> addresses (mailbox name @ host reference) to be used when\n> contacting personnel at an organization.  Mailbox names are\n> provided for both operations and business functions.  Additional\n> mailbox names and aliases are not prohibited, but organizations\n> which support email exchanges with the Internet are encouraged to\n> support AT LEAST each mailbox name for which the associated\n> function exists within the organization.\n\nand it includes a recommendation for \"webmaster@site\".\n\nIn any case, this level of site management is not in scope\nfor HTTP-WG.\n\nThanks,\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": ">>The problem is that people keep trying to wedge both into content-coding\n>>instead of just defining on-the-fly compression with Transfer-Encoding.\n>\n>I am *assuming* that your reference to 'transfer-encoding' maps to MIME\n>'content-transfer-encoding'.  If this assumption is false, please ignore\n>this comment...\n\nIt is false.\n\n>The MIME specification STRONGLY discourages (its capitals) the creation of\n>new content-transfer-encoding values.  I would think that this would\n>encourage designers to try and shoehorn transfer-encoding semantics into\n>content-coding headers.\n\nThat is why we now have Transfer-Encoding and do not allow\nContent-Transfer-Encoding at all.  Likewise, the reason we have\nContent-Encoding is because MIME did not provide for declaring the\ntypes of layered encodings.  We would have been better off with a\nhierarchical Content-Type.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Editorial Nit in section 4.1 (new editorial issue GENERIC_MESSAGE",
            "content": "For completeness.\n- Jim\n\n\n\nattached mail follows:\nI'm not sure what the status of this is r.e. the current draft,\nbut the BNF is correct.  The generic message parser needs to look\nfor zero or more header fields.  It may later complain about the\nlack of Host or Date, but that is a different issue (different parser).\n\n....Roy\n\n\n------- Forwarded Message\n\nMessage-ID: <2FBF98FC7852CF11912A000000000001076C6A6D@DINO>\nFrom: \"Dean Justus (Exchange)\" <deanj@exchange.microsoft.com>\nTo: \"'fielding@ics.uci.edu'\" <fielding@ics.uci.edu>\nSubject: rfc2068\nDate: Fri, 14 Nov 1997 11:15:25 -0800\n\nMr. Roy Fielding;\n\nI do not know if this is an appropriate manner in handling this issue,\nbut in reading section 4.1 entitled \"Message Types\" in rfc2068 (Jan '97)\nshouldn't:\n\n generic-message = start-line\n*message-header\n                     CRLF\n                     [ message-body ]\n\nbe written as:\n\n generic-message = start-line\n1*message-header\nCRLF\n                     [ message-body ]\n\n\nThe paragraph above this describes the \"message-header\" as one or more\nheader fields.  Thus, in keeping with the *rule of the BNF, shouldn't it\nbe \"1*message-header\" instead of \"*message-header?\"\n\nI apologize if I am aproaching this issue in an unappropriate manner and\nwould appreciate it if you could communicate to me the right way in\nhandling issues of this same nature.\n\nDean Justus\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: Basic Authentication behavior (last call on PROTECTION_SPACES",
            "content": "This seems to have been dropped on the floor of the Issues list,\nso I've added it.\n\nIn any case, there has been no comment since Sept. 09 on this issue;\nconsider this the last call on the proposed wording.\n- Jim Gettys\n\n\n\nattached mail follows:\nI can't see this on the issues list, but it was discussed on the\nmailing list (with no objections).\n\n....Roy\n\n\n------- Forwarded Message\n\nDate: Tue, 09 Sep 1997 12:12:52 -0500 (EST)\nFrom: Foteos Macrides <MACRIDES@sci.wfbr.edu>\nSubject: Re: Basic Authentication behavior\nTo: fielding@kiwi.ics.uci.edu\nCc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nMessage-Id: <01INFSEQ12BM000NRI@SCI.WFBR.EDU>\nResent-From: http-wg@cuckoo.hpl.hp.com\nX-Mailing-List: <http-wg@cuckoo.hpl.hp.com> archive/latest/4378\n\n\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote:\n>IETF WGs do not generate specifications; authors do.  If someone can\n>author up a chunk of text for the Authentication section that describes\n>how the \"http\" URL space is treated hierarchically for predetermining\n>the reusability of credentials within a given realm, and can do so in\n>a manner less complicated than this sentence, then I am sure Jim can\n>add it to the Issues list and the matter can be resolved.\n\n        Appended is the current text of Section 11.1 in the -08 draft,\nwith a proposed insertion to make explicit the behavior expected for\npaths in Request-URIs as originally implemented for Basic Access\nAuthentication and recently reviewed in the HTTP-WG by Ari Luotonen,\n<URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0766.html>.\n\nBecause neither RFC 1945 nor RFC 2068 addressed the issue of\npaths and implied protection spaces for Basic Access Authentication,\nthe insertion uses SHOULD and MAY rather than MUST for this behavior.\n\n                                Fote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n[CURRENT TEXT:]\n            11.1 Basic Authentication Scheme\n\n            The \"basic\" authentication scheme is based on the model that\n            the client must authenticate itself with a user-ID and a\n            password for each realm. The realm value should be\n            considered an opaque string which can only be compared for\n            equality with other realms on that server. The server will\n            service the request only if it can validate the user-ID and\n            password for the protection space of the Request-URI. There\n            are no optional authentication parameters.\n\n            Upon receipt of an unauthorized request for a URI within the\n            protection space, the origin server MAY respond with a\n            challenge like the following:\n\n                   WWW-Authenticate: Basic realm=\"WallyWorld\"\n\n            where \"WallyWorld\" is the string assigned by the server to\n            identify the protection space of the Request-URI. A proxy\n            may respond with the same challenge using the Proxy-\n            Authenticate header field.\n\n            To receive authorization, the client sends the userid and\n            password, separated by a single colon (\":\") character,\n            within a base64 [7] encoded string in the credentials.\n\n                   basic-credentials = \"Basic\" SP base64-user-pass\n\n                   base64-user-pass  = <base64 [7] encoding of user-pass,\n                                    except not limited to 76 char/line>\n\n                   user-pass   = userid \":\" password\n\n                   userid      = *<TEXT excluding \":\">\n\n                   password    = *TEXT\n\n            Userids might be case sensitive.\n\n            If the user agent wishes to send the userid \"Aladdin\" and\n            password \"open sesame\", it would use the following header\n            field:\n\n                   Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==\n\n[START INSERT]---------------------------------------------------------\n            A user agent SHOULD assume that all paths at or deeper than\n            the depth of the last symbolic element in the path field\n            of the Request-URI also are within the protection space\n            specified by the Basic realm value of the current challenge,\n            and MAY send the corresponding Authorization header with\n            requests for resources in that space without receipt of\n            another challenge from the server.\n[END INSERT]-----------------------------------------------------------\n\n            If a client wishes to send the same userid and password to a\n            proxy, it would use the Proxy-Authorization header field.\n            See section 15 for security considerations associated with\n            Basic authentication.\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Syntax (and other) problems in Digest spe",
            "content": "While implementing the client side of the Digest authentication scheme\n(RFC-2069 and draft-ietf-http-digest-aa-rev-00.txt) I've hit a few syntax\nand other problems.\n\n1) RFC-2069 specifies the domain param in a challenge as\n\n     domain              = \"domain\" \"=\" <\"> 1#URI <\">\n\n     domain\n     A comma-separated list of URIs, as specified for HTTP/1.0.  The\n     intent is that the client could use this information to know the\n     set of URIs for which the same authentication information should be\n     sent.  The URIs in this list may exist on different servers.  If\n     this keyword is omitted or empty, the client should assume that the\n     domain consists of all URIs on the responding server.\n\n   However, the list of URIs is not (unambiguously) parseable because the\n   \",\" may be part of the URI (it's one of \"uchar\"). Consider the following\n   two examples:\n\n     domain=\"http://www.other.com/a/weird,/path\"\n     domain=\"http://www.other.com/,http://www.this.com/\"\n\n   Either can be parsed into a single or into two valid URIs.\n\n   Studying the syntax given in section 3.2.1 of rfc-1945 I conclude that\n   the only separators usable would be one of the \"unsafe\" class minus the\n   \"%\". Of those, SP seems the most reasonable choice to me. Therefore I\n   would suggest changing the domain param definition to:\n\n     domain              = \"domain\" \"=\" <\"> URI *( 1*SP URI ) <\">\n\n   Another issue is that \"as specified for HTTP/1.0\" is a little unclear.\n   Does this really mean any URI? Or just http URLs (and possibly related\n   ones, such as https://...)? And what about relative URIs - I assume that\n   they are to be interpreted relative to the absolute URI of the request\n   which elicited this response? Could this be clarified in the spec.\n\n\n2) Is there a particular reason the syntax for the digest challenge and\n   credentials does not follow the general syntax given in rfc-1945 and\n   rfc-2068?\n\n     challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\n     credentials    = basic-credentials\n                      | auth-scheme #auth-param\n     auth-param     = token \"=\" quoted-string\n\n   This syntax is violated in three respects in the digest spec: \n\n   1: The value parts of the params \"stale\", \"algorithm\" and \"uri\" are not\n      quoted strings:\n\n        stale               = \"stale\" \"=\" ( \"true\" | \"false\" )\n        algorithm           = \"algorithm\" \"=\" ( \"MD5\" | token )\n        digest-uri          = \"uri\" \"=\" digest-uri-value\n        digest-uri-value    = request-uri         ; As specified by HTTP/1.1\n\n      Suggested changes:\n\n        stale               = \"stale\" \"=\" <\"> ( \"true\" | \"false\" ) <\">\n        algorithm           = \"algorithm\" \"=\" <\"> ( \"MD5\" | token ) <\">\n        digest-uri-value    = <\"> request-uri <\">   ; As specified by HTTP/1.1\n\n\n   2: The \"digest-required\" param does not even have a value part:\n\n        digest-required     = \"digest-required\"\n\n      Suggested change:\n\n        digest-required     = \"digest-required\" = <\"> ( \"true\" | \"false\" ) <\">\n\n      (with a corresponding change in textual description of this param)\n\n\n   3: The realm need not be the first param in the challenge:\n\n        digest-challenge    = 1#( realm | [ domain ] | nonce |\n                              [ opaque ] |[ stale ] | [ algorithm ] |\n                              [ digest-required ] )\n\n      Suggested change:\n\n        digest-challenge    = realm 1#( [ domain ] | nonce |\n                              [ opaque ] |[ stale ] | [ algorithm ] |\n                              [ digest-required ] )\n\n      Note that all servers which implement the Digest scheme that I've\n      run into (to wit: Apache, Agranat-EmWeb, WN and DMKHTD) all do\n      actually send the realm first.\n\n\n3) The digest-uri-value is defined as:\n\n        digest-uri-value    = <\"> request-uri <\">   ; As specified by HTTP/1.1\n\n   I believe this is wrong, or at least misleading. When a proxy is involved\n   the digest-uri-value must be the request-uri that the server will see,\n   not the one sent by the client (i.e. the absolute path, not the full\n   absolute uri). I'm not sure how to best describe this in the spec\n   though.\n\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "minimal OPTION",
            "content": "I was asked to write up the changes needed to specify OPTIONS\nminimally as a method that may still be useful sometime in the\nfuture.  Below is a context diff from Rev-00.  The one possible item\nof controversy is whether or not proxy implemementers are willing\nto do the Max-Forwards calculation, but I feel this version is\nmuch clearer and more reliable than the original.\n\nNote that in the process I also deleted the Public header field,\nsince it hasn't been shown to be useful and most servers do not\nimplement it anyway, and its presence was confusing the issue.\nIt may be more appropriate to just move it to an appendix\n(referencing the definition in RFC 2068).\n\n....Roy\n\n\n*** draft-ietf-http-v11-spec-rev-00.txtFri Oct 17 16:24:07 1997\n--- rev-options.txtSun Nov 16 18:09:43 1997\n***************\n*** 85,91 ****\n              a separate internet draft on the topic that you should\n              review NOT incorporated into this draft (though editorial\n              notes identify where changes may occur).  This draft is\n!             draft-ietf-http-options-00.txt.\n  \n              Also an issue: AGE-CALCULATION; Roy Fielding has issued an\n              ID on the topic; Jeff Mogul intends to issue a draft as\n--- 85,91 ----\n              a separate internet draft on the topic that you should\n              review NOT incorporated into this draft (though editorial\n              notes identify where changes may occur).  This draft is\n!             draft-ietf-http-options-02.txt.\n  \n              Also an issue: AGE-CALCULATION; Roy Fielding has issued an\n              ID on the topic; Jeff Mogul intends to issue a draft as\n***************\n*** 400,406 ****\n  \n               14.33  Proxy-Authenticate ..............................141\n               14.34  Proxy-Authorization .............................142\n-              14.35  Public ..........................................142\n               14.36  Range ...........................................143\n                 14.36.1 ...............................Byte Ranges    143\n                 14.36.2 ..................Range Retrieval Requests    144\n--- 400,405 ----\n***************\n*** 2251,2259 ****\n              status code 405 (Method Not Allowed) if the method is known\n              by the server but not allowed for the requested resource,\n              and 501 (Not Implemented) if the method is unrecognized or\n!             not implemented by the server. The list of methods known by\n!             a server can be listed in a Public response-header field\n!             (section 14.35).\n  \n              The methods GET and HEAD MUST be supported by all general-\n              purpose servers. All other methods are optional; however, if\n--- 2250,2256 ----\n              status code 405 (Method Not Allowed) if the method is known\n              by the server but not allowed for the requested resource,\n              and 501 (Not Implemented) if the method is unrecognized or\n!             not implemented by the server.\n  \n              The methods GET and HEAD MUST be supported by all general-\n              purpose servers. All other methods are optional; however, if\n***************\n*** 2320,2346 ****\n              absolute path cannot be empty; if none is present in the\n              original URI, it MUST be given as \"/\" (the server root).\n  \n-             Editorial note: The proposed changes to OPTIONS  will remove\n-             the following down to ***END***. See draft-ietf-http-options-00.txt\n- \n-             If a proxy receives a request without any path in the\n-             Request-URI and the method specified is capable of\n-             supporting the asterisk form of request, then the last proxy\n-             on the request chain MUST forward the request with \"*\" as\n-             the final Request-URI. For example, the request\n- \n- \n-                    OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n- \n-             would be forwarded by the proxy as\n- \n-                    OPTIONS * HTTP/1.1\n-                    Host: www.ics.uci.edu:8001\n- \n-             after connecting to port 8001 of host \"www.ics.uci.edu\".\n- \n-             ***END***\n- \n              The Request-URI is transmitted in the format specified in\n              section 3.2.1. The origin server MUST decode the Request-URI\n              in order to properly interpret the request. Servers SHOULD\n--- 2317,2322 ----\n***************\n*** 2627,2633 ****\n                                     | Age                 ; Section 14.6\n                                     | Location            ; Section 14.30\n                                     | Proxy-Authenticate  ; Section 14.33\n-                                    | Public              ; Section 14.35\n                                     | Retry-After         ; Section 14.38\n                                     | Server              ; Section 14.39\n                                     | Set-Proxy           ; Section 14.48\n--- 2603,2608 ----\n***************\n*** 3308,3348 ****\n              requirements associated with a resource, or the capabilities\n              of a server, without implying a resource action or\n              initiating a resource retrieval.\n  \n!             Editorial note: The proposed changes to OPTIONS  will change\n!             the following down to ***END***. See draft-ietf-http-\n!             options-00.txt.\n! \n!             Unless the server's response is an error, the response MUST\n!             NOT include entity information other than what can be\n!             considered as communication options (e.g., Allow is\n!             appropriate, but Content-Type is not). Responses to this\n!             method are not cachable.\n  \n              If the Request-URI is an asterisk (\"*\"), the OPTIONS request\n!             is intended to apply to the server as a whole. A 200\n!             response SHOULD include any header fields which indicate\n!             optional features implemented by the server (e.g., Public),\n!             including any extensions not defined by this specification,\n!             in addition to any applicable general or response-header\n!             fields. As described in section 5.1.2, an \"OPTIONS *\"\n!             request can be applied through a proxy by specifying the\n!             destination server in the Request-URI without any path\n!             information.\n  \n              If the Request-URI is not an asterisk, the OPTIONS request\n              applies only to the options that are available when\n!             communicating with that resource. A 200 response SHOULD\n!             include any header fields which indicate optional features\n!             implemented by the server and applicable to that resource\n!             (e.g., Allow), including any extensions not defined by this\n!             specification, in addition to any applicable general or\n!             response-header fields. If the OPTIONS request passes\n!             through a proxy, the proxy MUST edit the response to exclude\n!             those options which apply to a proxy's capabilities and\n!             which are known to be unavailable through that proxy.\n  \n-             ***END***\n  \n  \n              9.3 GET\n--- 3283,3334 ----\n              requirements associated with a resource, or the capabilities\n              of a server, without implying a resource action or\n              initiating a resource retrieval.\n+             Responses to this method are not cachable.\n  \n!             If the OPTIONS request includes an entity-body (as indicated\n!             by the presence of Content-Length or Transfer-Encoding), then\n!             the media type MUST be indicated by a Content-Type field.\n!             Although this specification does not define any use for such\n!             a body, future extensions to HTTP may use the OPTIONS body to\n!             make more detailed queries on the server. A server that does not\n!             support such an extension MAY discard the request body.\n  \n              If the Request-URI is an asterisk (\"*\"), the OPTIONS request\n!             is intended to apply to the server in general rather than to\n!             a specific resource.  Since a server's communication options\n!             typically depend on the resource, the \"*\" request is only useful\n!             as a \"ping\" or \"no-op\" type of method; it does nothing beyond\n!             allowing the client to test the capabilities of the server.\n!             For example, this can be used to test a proxy for HTTP/1.1\n!             compliance (or lack thereof).\n  \n              If the Request-URI is not an asterisk, the OPTIONS request\n              applies only to the options that are available when\n!             communicating with that resource.\n! \n!             A 200 response SHOULD include any header fields that indicate\n!             optional features implemented by the server and applicable to\n!             that resource (e.g., Allow), possibly including extensions not\n!             defined by this specification.  The response body, if any,\n!             SHOULD also include information about the communication options.\n!             The format for such a body is not defined by this specification,\n!             but may be defined by future extensions to HTTP.  Content\n!             negotiation MAY be used to select the appropriate response format.\n!             If no response body is included, the response MUST include a\n!             Content-Length field with a field-value of \"0\".\n! \n!             The Max-Forwards request-header field MAY be used to target a\n!             specific proxy in the request chain.  When a proxy receives an\n!             OPTIONS request on an absoluteURI for which request forwarding\n!             is permitted, the proxy MUST check for a Max-Forwards field.\n!             If the Max-Forwards field-value is zero (\"0\"), the proxy\n!             MUST NOT forward the message; instead, the proxy SHOULD respond\n!             with its own commmunication options.  If the Max-Forwards\n!             field-value is an integer greater than zero, the proxy MUST\n!             decrement the field-value when it forwards the request.\n!             If no Max-Forwards field is present in the request, then the\n!             forwarded request MUST NOT include a Max-Forwards field.\n  \n  \n  \n              9.3 GET\n***************\n*** 5913,5919 ****\n  \n                 . Connection\n                 . Keep-Alive\n-                . Public\n                 . Proxy-Authenticate\n                 . Transfer-Encoding\n                 . Upgrade\n--- 5899,5904 ----\n***************\n*** 6750,6759 ****\n  \n              14.7 Allow\n  \n-             Editors Note: The OPTIONS changes would cause possible\n-             changes to Allow and/or Public for consistency with each\n-             other and with section 9.2 (OPTIONS).\n- \n              The Allow entity-header field lists the set of methods\n              supported by the resource identified by the Request-URI. The\n              purpose of this field is strictly to inform the recipient of\n--- 6735,6740 ----\n***************\n*** 6768,6774 ****\n              INTERNET-DRAFT            HTTP/1.1  Wednesday, July 30, 1997\n  \n  \n!                    Allow          = \"Allow\" \":\" 1#method\n  \n              Example of use:\n  \n--- 6749,6755 ----\n              INTERNET-DRAFT            HTTP/1.1  Wednesday, July 30, 1997\n  \n  \n!                    Allow          = \"Allow\" \":\" #method\n  \n              Example of use:\n  \n***************\n*** 6791,6801 ****\n              user agent MAY have other means of communicating with the\n              origin server.\n  \n-             The Allow header field does not indicate what methods are\n-             implemented at the server level. Servers MAY use the Public\n-             response-header field (section 14.35) to describe what\n-             methods are implemented on the server as a whole.\n- \n  \n              14.8 Authorization\n  \n--- 6772,6777 ----\n***************\n*** 8513,8523 ****\n  \n              14.31 Max-Forwards\n  \n-             Editor's note: The OPTIONS changes would allow Max-Forward\n-             with OPTIONS, not just with TRACE.\n- \n              The Max-Forwards request-header field may be used with the\n!             TRACE method (section 14.31) to limit the number of proxies\n              or gateways that can forward the request to the next inbound\n              server. This can be useful when the client is attempting to\n              trace a request chain which appears to be failing or looping\n--- 8489,8497 ----\n  \n              14.31 Max-Forwards\n  \n              The Max-Forwards request-header field may be used with the\n!             TRACE (section 9.8) and OPTIONS (section 9.2) methods to limit\n!             the number of proxies\n              or gateways that can forward the request to the next inbound\n              server. This can be useful when the client is attempting to\n              trace a request chain which appears to be failing or looping\n***************\n*** 8529,8547 ****\n              remaining number of times this request message may be\n              forwarded.\n  \n!             Each proxy or gateway recipient of a TRACE request\n!             containing a Max-Forwards header field SHOULD check and\n              update its value prior to forwarding the request. If the\n!             received value is zero (0), the recipient SHOULD NOT forward\n!             the request; instead, it SHOULD respond as the final\n!             recipient with a 200 (OK) response containing the received\n!             request message as the response entity-body (as described in\n!             section 9.8). If the received Max-Forwards value is greater\n!             than zero, then the forwarded message SHOULD contain an\n!             updated Max-Forwards field with a value decremented by one\n!             (1).\n  \n!             The Max-Forwards header field SHOULD be ignored for all\n              other methods defined by this specification and for any\n              extension methods for which it is not explicitly referred to\n              as part of that method definition.\n--- 8503,8518 ----\n              remaining number of times this request message may be\n              forwarded.\n  \n!             Each proxy or gateway recipient of a TRACE or OPTIONS request\n!             containing a Max-Forwards header field MUST check and\n              update its value prior to forwarding the request. If the\n!             received value is zero (0), the recipient MUST NOT forward\n!             the request; instead, it MUST respond as the final recipient.\n!             If the received Max-Forwards value is greater than zero, then\n!             the forwarded message MUST contain an updated Max-Forwards\n!             field with a value decremented by one (1).\n  \n!             The Max-Forwards header field MAY be ignored for all\n              other methods defined by this specification and for any\n              extension methods for which it is not explicitly referred to\n              as part of that method definition.\n***************\n*** 8650,8693 ****\n              cooperatively authenticate a given request.\n  \n  \n-             14.35 Public\n- \n-             Editors Note: The OPTIONS changes would cause possible\n-             changes to Allow and/or Public for consistency with each\n-             other and with section 9.2 (OPTIONS )\n- \n-             The Public response-header field lists the set of methods\n-             supported by the server. The purpose of this field is\n-             strictly to inform the recipient of the capabilities of the\n-             server regarding unusual methods. The methods listed may or\n-             may not be applicable to the Request-URI; the Allow header\n-             field (section 14.7) MAY be used to indicate methods allowed\n-             for a particular URI.\n- \n-                    Public         = \"Public\" \":\" 1#method\n- \n-             Example of use:\n- \n-                    Public: OPTIONS, MGET, MHEAD, GET, HEAD\n- \n-             This header field applies only to the server directly\n-             connected to the client (i.e., the nearest neighbor in a\n-             chain of connections). If the response passes through a\n-             proxy, the proxy MUST either remove the Public header field\n-             or replace it with one applicable to its own capabilities.\n- \n- \n- \n- \n- \n- \n-             Fielding, et al                                   [Page 142]\n- \n- \n- \n-             INTERNET-DRAFT            HTTP/1.1  Wednesday, July 30, 1997\n- \n- \n              14.36 Range\n  \n  \n--- 8621,8626 ----\n***************\n*** 9576,9594 ****\n              setting remains in effect for `integer' transactions.\n  \n  \n-             14.49 Compliance\n- \n-             Editor's note: The OPTIONS changes would introduce a new\n-             \"Compliance\" header.\n- \n- \n-             14.50 Non-Compliance\n- \n-             Editor's note: The OPTIONS changes would introduce a new\n-             \"Compliance\" header.\n- \n- \n- \n  \n              15 Security Considerations\n  \n--- 9509,9514 ----\n***************\n*** 10053,10062 ****\n              with which it is directly communicating is HTTP/1.1 and if\n              it supports the Set-Proxy header.  To determine this, the\n              client or proxy should use the OPTIONS method to make a\n!             request check for this feature.  The extension string should\n!             be HDR='set-proxy', or, should this be defined in the\n!             Standard RFC for HTTP/1.1, then the string should be\n!             RFC='rfcXXXX'  in the OPTIONS request.\n  \n              Great care should be taken when implementing client side\n              actions based on the 305 or 306.  Since older proxies may\n--- 9973,9979 ----\n              with which it is directly communicating is HTTP/1.1 and if\n              it supports the Set-Proxy header.  To determine this, the\n              client or proxy should use the OPTIONS method to make a\n!             request check for this feature.\n  \n              Great care should be taken when implementing client side\n              actions based on the 305 or 306.  Since older proxies may\n\n\n\n"
        },
        {
            "subject": "WebDAVspecific status code",
            "content": "While reading through the latest HTTP draft, I was dismayed to find several \nnew status codes, since the WebDAV protocol specification is also \nintroducing new status codes.  While I respect the right of the HTTP WG to \ndefine whatever HTTP status codes they need (the WebDAV specification will \nwork around them), I thought it would be worthwhile to state which status \ncodes WebDAV is planning on introducing, so that other HTTP extension \nefforts will be sure to steer clear of them.\n\n102 Processing\n\n207 Multi-Status\n\n418 Unprocessable Entity\n419 Insufficient Space on Resource\n420 Method Failure\n421 Destination Locked\n\nComplete details on the sematics of all these status codes can be found in \nthe next version of the WebDAV protocol specification, which will be \navailable RSN, depending on the I-D submission crunch.\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Ignore prox",
            "content": "Please ignore this message if it is a FAQ.\n\nI wanted to know if there is a way for a proxy to tell a client\nany/all of the following things:\n\n1. Ignore me and fetch this document yourself. (but if you call me\nagain I might change my mind.)\n\n1a. Ignore me for some time.\n\n2. I will not fetch this for you.  (the \"Not Authorized\"  response?)\n\n2a. I will not cache this for you.\n\nI see this as a way (analagous to \"Come Back Later\") for a very busy\nproxy to tell clients that they are on their own.  Also, a caching\nproxy might want to tell a user agent that it will not cache its\nrequests.  Of course, there are problems with proxies which are\ngateways -- they should never refuse proxying, but still might refuse\ncaching.\n\nThank you in advance.  \n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-alternates01.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: The Alternates Header Field\nAuthor(s): T. Hardie, K. Holtman, A. Mutz\nFilename: draft-ietf-http-alternates-01.txt\nPages: 11\nDate: 13-Nov-97\n\n   This document proposes a header, Alternates, which is intended\n   to provide a common method for Internet-based application\n   protocols to indicate that a particular resource exists in\n   multiple forms. The Alternates header provides a\n   machine-readable indication of the bases on which the\n   different forms vary and how the the recipient of the header\n   can select among them.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-alternates-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-alternates-01.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-alternates-01.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "RANGE_WITH_CONTENTCODIN",
            "content": "If I am retrieving a file as text/html with content encoding gzip,\nI can suppose that compression is made on the fly, so it makes sense \nto say that range applies to the original file and not to the compressed\nform.\n\nOn the other hand, if I am retrieving foobar.gz because it is\nstored this way, it has type application/gzip, and it makes sense to apply \nrange to the gzipped form.\n\nMy opinion is therefore that range should not apply to the content\nencoding.\n\n.mau.\n\n\n\n"
        },
        {
            "subject": "Re: comments on draft-ietf-http-state-man-mec04.tx",
            "content": "\"numeric IP addresses\": I'd write \"numeric IPv4 or IPv6 addresses\" and \nadded a reference which shows the canonical form of the latters, or \nexplicitly disallow numeric IPv6 addresses (they are not yet used in\ncookies, are they?)\n\n.mau.\n\n\n\n"
        },
        {
            "subject": "Editorial suggestion to change Request-URI to RequestTarge",
            "content": "Roy suggests this editorial change for future drafts (i.e. we don't plan\nto make it for Rev-01.\n- Jim\n\n>  Actually, when I was writing the OPTIONS stuff I realized that a lot\n>  of gunky definitions could be cleaned up by a global replace of\n>  \n>      Request-URI ===> Request-Target\n>  \n>  and then use the term Request-URI to refer to \"the resource indicated\n>  by a full URI in Request-Target or by the combination of Request-Target\n>  and Host\".  Something to keep in mind for draft XX : XX > 01.\n>  \n>  ....Roy\n\n\n\n"
        },
        {
            "subject": "Re: WebDAVspecific status code",
            "content": "  There was a mechanism proposed a while ago for this sort of thing:\n\n  Assignment of Status Codes for HTTP and HTTP-Derived Protocols\",\n    H. Schulzrinne, 01 Aug 1997.\n\nftp://ds.internic.net/internet-drafts/draft-schulzrinne-http-status-00.txt\n\n  Should we try to advance this draft so that problems like the one\n  Jim points out can be dealt with smoothly in the future?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Claification requested in Host: (HOST issue proposed wording.",
            "content": "The proposed resolution has been to Proxy can add the domain name\nbut not change it if it is already present. \n\nHere is the proposed change to close out the HOST issue. \n\nAdditional clarification paragraph to section 3.2.2:\n\n\"If a proxy recieves a host name which is not a fully qualified domain name,\nit MAY add its domain to the host name it recieved.  If a proxy recieves\na fully qualified domain name, the proxy MUST NOT change the host name.\"\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Wording to close out REDIRECTS issue",
            "content": "This ended up becoming an editorial issue, but for completeness,\nhere's what I've put into the document.\n- Jim\n\nOld text:\n10.3 Redirection 3xx\n\nThis class of status code indicates that further action needs to be taken \nby the user agent in order to fulfill the request.  The action required \nMAY be carried out by the user agent without interaction with the user if \nand only if the method used in the second request is GET or HEAD. A user\nagent SHOULD NOT automatically redirect a request more than 5 times, since such\nredirections usually indicate an infinite loop.\n\nNew text:\n\n10.3 Redirection 3xx\n\nThis class of status code indicates that further action needs to be taken \nby the user agent in order to fulfill the request.  The action required \nMAY be carried out by the user agent without interaction with the user if \nand only if the method used in the second request is GET or HEAD. A client \nSHOULD detect infinite redirection loops, since such loops generate \nnetwork traffic for each redirection. \n\nNote: previous versions of this specification recommended a maximum of \nfive redirections. Application developers should be aware that there may \nbe clients that implement such a fixed limitation.\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "At 15:52 11/14/97 -0800, Roy T. Fielding wrote:\n\n>The spec does make it explicit, at least to the extent that general\n>discussion of encodings can be explicit.  On-the-fly compression is\n>a transfer-coding.  Source-based compression is a content-coding.\n\nThere are two reasons why you can't substitute one for the other:\n\n1) They don't have the same scope - one is end-to-end and the other is\nhop-by-hop. As the message length changes, it can only be used through\nproxies that know about that particular encoding. In other words: the\nstupidest link in the chain decides the encoding.\n\n2) A client can't say that it \"accepts\" transfer codings, so there is no\nway to introduce the funkyflate compression. Currently, the only way is to\nuse Accept-Encoding. I would actually vote for having a Accept-Transfer\nheader field - this would also make the handling of trailers much easier.\n\n>The problem is that people keep trying to wedge both into content-coding\n>instead of just defining on-the-fly compression with Transfer-Encoding.\n\nI think you need both - the real problem is the separation of content\nencoding and content type.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec04.tx",
            "content": "> 4. OUTLINE\n\n  I second Roys comment on the beginning of section 4:\n\n> ...and that impact can largely be confined\n> to Common Gateway Interface (CGI) programs, unless the server provides\n> more sophisticated state management support.\n\n  That whole statement could just be cut without detracting from the\n  spec.  CGI is irrelevant - almost all server implementations provide\n  other mechanisms for programatic access and the state management\n  mechanisms apply to them all equally (whether they are \"more\n  sophisticated\" or not).\n\n================\n\n> 4.1  Syntax:  General\n\n> av-pairs        =       av-pair *(\";\" av-pair)\n> av-pair         =       attr [\"=\" value]                ; optional value\n> attr            =       token\n> value           =       word\n> word            =       token | quoted-string\n\n  Why not just make the syntax for 'value':\n\n value           =       token | quoted-string\n\n  I don't see any use for the 'word' token (and the name 'word' is\n  misleading because it isn't a word in the normal english usage).\n\n> NOTE: The syntax above allows whitespace between the attribute and the =\n> sign.\n\n  ... and between the \"=\" and the value?\n\n  There should just be a reference to the general rule for implied\n  whitespace in HTTP header parsing (section 2.1 in RFC2068).\n\n================\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec04.tx",
            "content": "Scott Lawrence wrote:\n> \n> > 4. OUTLINE\n> \n>   I second Roys comment on the beginning of section 4:\n> \n> > ...and that impact can largely be confined\n> > to Common Gateway Interface (CGI) programs, unless the server provides\n> > more sophisticated state management support.\n> \n>   That whole statement could just be cut without detracting from the\n>   spec.  CGI is irrelevant - almost all server implementations provide\n>   other mechanisms for programatic access and the state management\n>   mechanisms apply to them all equally (whether they are \"more\n>   sophisticated\" or not).\n\nOkay, okay.  It did make sense once, I think, but I'll remove it.\n\n> \n> ================\n> \n> > 4.1  Syntax:  General\n> \n> > av-pairs        =       av-pair *(\";\" av-pair)\n> > av-pair         =       attr [\"=\" value]                ; optional value\n> > attr            =       token\n> > value           =       word\n> > word            =       token | quoted-string\n> \n>   Why not just make the syntax for 'value':\n> \n>  value           =       token | quoted-string\n\nGood point.\n\n> \n>   I don't see any use for the 'word' token (and the name 'word' is\n>   misleading because it isn't a word in the normal english usage).\n> \n> > NOTE: The syntax above allows whitespace between the attribute and the =\n> > sign.\n> \n>   ... and between the \"=\" and the value?\n> \n>   There should just be a reference to the general rule for implied\n>   whitespace in HTTP header parsing (section 2.1 in RFC2068).\n\nI do say that \"White space is permitted between tokens,\" which means\nmuch the same thing.  The point of the NOTE was to alert users of\nNetscape's cookies spec. to this change from that spec.  \n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Ignore prox",
            "content": "> \n> Please ignore this message if it is a FAQ.\n> \n> I wanted to know if there is a way for a proxy to tell a client\n> any/all of the following things:\n> \n> 1. Ignore me and fetch this document yourself. (but if you call me\n> again I might change my mind.)\n> \nNot presently, there is a 305 and maybe 306 redirect which may\nallow this functionality to be discussed at memphis.\nCurrent implementations do not allow this.\nA workaround is using a Proxy Auto Config file, where\nyou specify a failover, that way if the proxy is unavailable,\nthe client will go direct. ( assuming the network allows it )\n\n> 1a. Ignore me for some time.\nsame as above\n> \n> 2. I will not fetch this for you.  (the \"Not Authorized\"  response?)\n> \nYes, the 407  Proxy Authentication Required response is used.\n\n> 2a. I will not cache this for you.\n> \n> I see this as a way (analagous to \"Come Back Later\") for a very busy\n> proxy to tell clients that they are on their own.  Also, a caching\n> proxy might want to tell a user agent that it will not cache its\n> requests.  Of course, there are problems with proxies which are\n> gateways -- they should never refuse proxying, but still might refuse\n> caching.\n\nFor a busy server, it generally makes sense for it to cache\nif it has to retreive it anyway.  Of course, we're assuming\nthe object isnt already in the cache-- if it is, then the\nproxy can potentially return it from the cache.\n\nThe biggest cost for a proxy is the origin retreive (contacting \nthe web server ), writing a cache file is much cheaper.\nIn addition, by caching it, if there is a request for the\nsame object, the proxy can then return it from the cache \nnext time, which helps the proxy a great deal.\n\nI think in a worst case scenario, it would be O.K. for the\nproxy to act such that:\n\"I am too busy to go to the far away web server for you, but\nIll give you what I have in my cache, even though its out of date\".\nThe client can accept that or attempt to go direct.\n\n> \n> Thank you in advance.  \n> \n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy\nRanger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: If-NoneMatch and IMS (new Issue IMS_INM_MISMATCH",
            "content": "Henry Sanders writes:\n\n    There's also the sentence\n\n       If none of the entity tags match, or if \"*\" is given and no\n       current entity exists, then the server MAY perform the requested\n       method as if the If-None-Match header field did not exist.\n    \n    Which is what I was referring to when I mentioned treating the\n    header as if it didn't exist. I like your proposed alterations, we\n    just need to strike this sentence too.\n\nYes, the quoted sentence is also wrong, but striking it would be\na mistake.  The problem is that this sentence combines two different\ncases, and the appropriate action is different in each one.\n\nI suggest replacing it with\n\n       If none of the entity tags match, then the server MAY perform\n       the requested method as if the If-None-Match header field did\n       not exist, but MUST also ignore any If-Modified-Since header\n       field(s) in the request.  That is, if no entity tags match, then\n       the server MUST not return a 304 (Not Modified) response.\n\n       If \"*\" is given and no current entity exists, then the server\n       MAY perform the requested method as if the If-None-Match header\n       field did not exist.\n    \nThe latter paragraph doesn't depend on whether or not an I-M-S field\nis included, since an entity that does not exist cannot have a\nmodification date.\n\nAlso, this next paragraph:\n\n            If the request would, without the If-None-Match header\n            field, result in anything other than a 2xx status, then the\n            If-None-Match header MUST be ignored.\n\nis also wrong; it should be\n\n            If the request would, without the If-None-Match header\n            field, result in anything other than a 2xx or 304 status,\n    then the If-None-Match header MUST be ignored.\n\nI.e., don't ignore If-None-Match just because the I-M-S value\nmatches the current modification date.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: Editorial suggestion to change Request-URI to RequestTarge",
            "content": "Oh ick. Not to be too excessive of a fuddy duddy but the term Request-URI\nhas been used for a very long time, changing the meaning is guaranteed to\nconfuse a whole bunch o'folks. I would suggest that this cat is out of the\nbag and we are just going to have to live with it.\nYaron\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Monday, November 17, 1997 7:56 AM\n> To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Editorial suggestion to change Request-URI to Request-Target\n> \n> Roy suggests this editorial change for future drafts (i.e. we don't plan\n> to make it for Rev-01.\n> - Jim\n> \n> >  Actually, when I was writing the OPTIONS stuff I realized that a lot\n> >  of gunky definitions could be cleaned up by a global replace of\n> >  \n> >      Request-URI ===> Request-Target\n> >  \n> >  and then use the term Request-URI to refer to \"the resource indicated\n> >  by a full URI in Request-Target or by the combination of Request-Target\n> >  and Host\".  Something to keep in mind for draft XX : XX > 01.\n> >  \n> >  ....Roy\n\n\n\n"
        },
        {
            "subject": "Re: comments on draft-ietf-http-state-man-mec04.tx",
            "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote on Fri, 14 Nov 1997 15:33:58 -0800:\n  > >  > I'd prefer not to have \" (Rev1)\" in the title.\n  > >\n  > >I thought it was desirable to distinguish this from RFC 2109.\n  > \n  > That should be done in the abstract (and it is, as I recall).\n  > I think the draft should contain exactly what the final RFC will\n  > contain, aside from the Status section and Header/Footer.\n\nOkay.\n\n  > \n  > >  > A comment should be included regarding support (or non-support) of IPv6\n  > >  > addresses [we got pinged on this for the URI draft].\n  > >\n  > >Can you suggest wording?  I wanted to avoid specifying syntax for either,\n  > >and I chose the neutral term \"numeric IP addresses\" to (try to) imply both.\n  > >(I also seem to remember that the proposed text representation of IPv6\n  > >addresses conflicts with URI syntax....)\n  > \n  > I don't know what to suggest.  What happens when a cookie includes an IPv6\n  > address (or is that even possible)?\n\nI suspect no one sends IPv6 addresses in cookies now, and probably no\nuser agents would know what to do with them.  But I thought the spec.\nshould allow them.\n\nLast I looked (some time ago), the proposed text representation of IPv6\naddresses involved ':' separators, and I think they will pose problems\nfor the URL syntax.  But that syntax shouldn't pose a problem in the\ncontext of the cookie spec.\n\n  > \n  > >  > In general, I find the terminology section more confusing than useful,\n  > >  > even though I do know what is intended.  I find it unlikely that anyone\n  > >  > not involved in the mailing list discussions would have a clue as to\n  > >  > what is being described.  It would be better to postpone the description\n  > >  > of hostname dot comparison until it can be described as part of the\n  > >  > matching algorithm.\n  > >\n  > >YMMV.  I prefer to put the terms in one place so someone can find them\n  > >easily if they need to.  It's a bit like the HTTP specification -- you\n  > >can't really understand this section until you understand the whole thing.\n  > \n  > Terms, yes -- algorithms, no.  The hostname dot comparison doesn't make\n  > any sense without the context for why the hostname dot comparison exists.\n\nI think you and I will have to disagree here.  I'm trying to define\na term, \"domain match\", and it can't be done without describing the\nalgorithm.  And I do give a sense of context:  \"[s]ometimes we\ncompare one host name with another.\"\n  > \n  > > [...]\n  > >\n  > >Your description is correct from the HTTP point of view, of course.\n  > \n  > I should have also mentioned that I was reading it from that view.\n  > I like implementation advice, but it needs to be clear what the\n  > protocol requirements are as opposed to how one might use them.\n  > A proposed standard needs to define the interface and avoid defining\n  > the application.\n\nI've looked over section 4.2.1 particularly, but I'm not sure how you\nwould want me to recast it.  I don't believe I'm defining the\napplication, although I'm certainly describing the things an\napplication can/should/must do.  I prefer the application-centric\napproach; too often I see specs. that describe all the things a\nprotocol can do, but I haven't a clue which ones I need to use.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: LAST CALL: draft-ietf-http-state-man-mec04.txt",
            "content": "Yaron Goland sent the following comments to Larry Masinter.  With\npermission of both parties, I'm responding to the comments on http-wg.\n\n  > Just to be clear, there is no intent to actually issue the draft as an RFC,\n  > rather the purpose of the last call is to get everyone to agree at least on\n  > the syntax so that any further arguments will strictly address the privacy\n  > issues and not digress into a syntax battle?\n\nThat's the idea.\n\n  > \n  > Also, I really think we would be doing the world a major favor if the first\n  > half of the draft just specified what the hell the current cookie\n  > implementation really is, aka document current implementation. This would\n  > mean pointing out various syntax errors in the original proposal,\n  > differences in implementation between different browsers, etc. That way we\n  > at least know where we stand. From there the draft can then specify\n  > backwards compatible extensions and fixes.\n\nI do not know all the possible errors, etc., nor do I know the\nimplementation behavior of various browsers.  One of the reasons for\nwriting the cookie spec. was that Netscape's original was so loose, it\nwas hard to know what it specified.\n\nI would certainly welcome a companion document for the record from\nvendors, who are most likely to know what their products actually\ndo/did.\n\n  > \n  > The definitions given in section 3 don't seem to be either useful or even\n  > accurate. For example, most cookie \"sessions\" are now indefinite. Your\n  > cookie gets set with your id or favorite settings and stay there,\n  > essentially, forever. I think all of section 3 can be removed without\n  > materially effecting the draft.\n\nIf you mean \"attributes\" rather than \"definitions\", I agree with you.\nThe world of cookies has evolved a lot in the nearly two years (!) this\nspec. has been gestating.\n\n  > \n  > 4.2.2 - A note should be included that the name of the name=value may\n  > collide with the name of an attribute in the rest of the cookie but that no\n  > confusion is possible as the first attribute is ALWAYS guaranteed to be the\n  > name=value pair. This isn't a technical flaw, rather I just think putting in\n  > this sentence will clarify matters.\n\nOkay.\n\n  > \n  > In 4.2.2 he defines a production VALUE = value but in so far as I can see,\n  > he never defined the production for value.\n\n\"value\" is defined in 4.1.\n\n  > \n  > In 4.2.2 it says \"Comment=comment\" as one of the paragraph headers. It\n  > should be Comment=value. Additionally he doesn't address the\n  > internationalization issues of having a comment. \n\nRegarding \"=comment\", I do the same with Domain, Max-Age, Path, ....\nThe syntax further up defines the actual syntax for each a-v.\n\"comment\" (\"domain\", \"delta-seconds\", \"path\") in the descriptions imply\nthe semantics.  (In the PostScript version, they're in italics.)\n\nI'm open to alternatives the convey what the semantics are for the value\nof each attribute.\n\nThere had been some discussion on the mailing list about\ninternationalization in Comment.  I think the problem of how to support\ndifferent languages in request headers is larger than just Comment.\nHow do you support negotiation of acceptable languages for attributes?\nI will confess to being unsure of how to address the issues.  Moreover,\npeople noted that CommentURL could solve the problem, by using HTTP\ncontent negotiation.\n\n  > \n  > 4.2.2 - Comment_URL - Something should be said about cookies being on the\n  > URL that the user is redirected to. I have absolutely no problem with a\n  > cookie being sent on that URL, but some mention should probably be made\n  > about the issue.\n\nThat issue was among the sections I removed between state-man-mec-03 and\n-04 because it treads on user interface issues.  So it will return.\n\n  > \n  > 4.2.2 - Discard - What does \"agent termination\" mean? For example, IE 4.0\n  > starts up when you turn the OS on and stops running when you turn the OS\n  > off. I suspect that wasn't the duration that was intended. The same sorts of\n  > issues apply to browsers running in Kiosks.\n\nYour interpretation is fine.\n\nThe intention, which may no longer be realizable in some contexts, was\nfor an application to be able to say the cookie has a defined lifetime,\nbut that lifetime should not persist beyond an \"invocation\" of the\nbrowser.  What we had in mind, for example, is that, if I'm a user of a\nshared PC in a college computer room and I exit the browser, cookies\nthat would otherwise have been saved (because they have a Max-Age\nvalue) will be discarded.  So that's what should happen if you restart\na Win95 system with MSIE 4.0.\n\n  > \n  > 4.2.2 - Domain - It says that if a leading dot isn't specified on a domain\n  > then one should be added by the client. Why? After all I may ONLY want a\n  > cookie to apply to foo.bar.blah NOT *.foo.bar.blah. Shouldn't we just take\n  > the server at its word? If there is a dot, keep it. If there isn't, loose\n  > it. Of course I find the domain matching rules really confusing so maybe I\n  > am just missing something.\n\nThis was actually an attempt at simplification.  If your server is\nfoo.bar.blah and you want cookies to return only to that server, then\nomit the Domain attribute; the default will do what you want.  If you\ngive an explicit Domain, it's assumed to be for a group of domain names.\n\n  > \n  > 4.2.2 - Path - A more specific reference should be included to specify that\n  > there is an implied wildcard at the end of the path specification.\n\nI'm not sure what you're asking for.  Path Selection under 4.3.4\nalready conveys what I think you want.\n\n  > \n  > 4.2.2 - Secure - \n  > \n  > \"When it sends a \"secure\" cookie back to a server, the user agent\n  > should use no less than the same level of security as was used when it\n  > received the cookie from the server.\" \n  > \n  > Y'all don't really expect browsers to record the level of security used when\n  > a cookie was retrieved? In some cases this information may not even be\n  > available because security may be implemented in such a way that the\n  > software handling cookies can't find out about it. I think this requirement\n  > is unrealistic.\n\nYou and I have discussed this item before.  My answer is \"yes\", but note\nthat the spec. says \"should use\".\n\nThe goal of the words is to ensure that the information in the cookie is\ntreated with the same level of security when it is sent to the server as\nwhen it was received from the server.  I'm open to alternative ways to\nsay that.\n\n  > \n  > 4.3.2 - Rejecting Cookies - In this section a domain attribute with no\n  > internal dots results in a cookie that can not be accepted. This effectively\n  > means that cookies can not be used on intranets where single name sites are\n  > common. I realize this prohibition was put in for things like \"com\" but we\n  > have to realize that one word domain names are widely used on intranets.\n  > This discrepancy must be resolved.\n\nThere are two possible resolutions:\n1) Omit Domain, in which case the default is to send the cookie only to\nthe server from which it came; or\n2) Use a fully qualified domain name for Domain, even on an intranet.\n\n  > \n  > Additionally there is the requirement in this section:\n  > \n  > \"* The request-host is a HDN (not IP address) and has the form HD,\n  > where D is the value of the Domain attribute, and H is a string that\n  > contains one or more dots.\"\n  > \n  > This requirement is inappropriate because a site may have good reasons for\n  > wanting matching more than one level deep. We should allow for such\n  > matching, especially when there is a PICs label to go along with it. In\n  > general this restriction always seemed arbitrary.\n\nBecause this falls under the \"privacy\" category, will you accept deferring\nits discussion?\n\n  > \n  > 4.3.3 - This section contains \"[Wording about cookie inspection user\n  > interface to be supplied.]\". I take this to mean we are bundling together UI\n  > issues with privacy issues?\n\nThey tend to get bundled together.  The rule of thumb has been that the\nuser should be able to control privacy by examining what's going on.\nThat control often implies some kind of user interface features.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "As I mentioned in a previous mail [1], content-codings and transfer-codings\nshare a lot of the same sementics but have different scopes. Currently a\nclient can control the content-encodings sent by a server, but can not\ncontrol the transfer-codings. This is the purpose of the Accept-Transfer\nheader field.\n\nBoth Accept-Encoding and Accept-Transfer should have been general header\nfields instead of request header fields but it's too late to change now.\n\nI don't think it makes sense to have a \"*\" as acceptable transfer codings\nas the client MUST know the transfer coding in order to find the End of\nMessage.\n\nHenrik\n\n[1] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0157.html\n\n***\n\nAccept-Transfer\n\nThe Accept-Transfer request-header field is similar to Accept-Encoding, but\nrestricts the transfer-codings (section 3.6) that are acceptable in the\nresponse.\n\n       Accept-Transfer  = \"Accept-Transfer\" \":\"\n                           1#( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n       t-codings        = transfer-codings\n\nExamples of its use are:\n\n       Accept-Transfer: deflate\n       Accept-Transfer:\n       Accept-Transfer: chunk=1.0; deflate=0.5\n\nThe Accept-Transfer header field only applies to the immediate connection.\nTherefore, the accept-transfer keyword MUST be supplied within a Connection\nheader field (section 14.10) whenever Accept-Transfer is present in an\nHTTP/1.1 message.\n\nA server tests whether a transfer-coding is acceptable, according to an\nAccept-Transfer field, using these rules:\n\n1. If the transfer-coding is one of the transfer-codings listed in the\nAccept-Transfer field, then it is acceptable, unless it is accompanied by a\nqvalue of 0.  (As defined in section 3.9, a qvalue of 0 means \"not\nacceptable.\")\n\n2. If multiple transfer-codings are acceptable, then the acceptable\ntransfer-coding with the highest non-zero qvalue is preferred.\n\n3. The \"identity\" transfer-coding is always acceptable, unless specifically\nrefused because the Accept-Transfer field includes \"identity;q=0\", or\nbecause the field includes \"*;q=0\" and does not explictly include the\n\"identity\" transfer-coding.  If the Accept-Transfer field-value is empty,\nthen only the \"identity\" encoding is acceptable.\n\nIf an Accept-Transfer field is present in a request, and if a server cannot\nsend a response which is acceptable according to the Accept-Transfer\nheader, then the server SHOULD send an error response with the 406 (Not\nAcceptable) status code.\n\nIf no Accept-Transfer field is present, the sender MAY assume that the\nrecipient will accept the \"identity\" and \"chunked\" transfer-codings. \n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the Accept-Transfer field.\n\n***\n\n\n\n"
        },
        {
            "subject": "Re: REVERSION discussion at Munich...",
            "content": "Given the heat that this generated at Munich, I'd appreciate it\nif others read Josh's mail, and proposed change to close out the issue.\nHenry, do you agree?\n\n> \"Proxy servers MUST upgrade all requests to the highest\n> version supported by the proxy\"\n\n\nI also wonder if \"Proxy servers\" is correct...  Should it be\n\"Caching proxy servers\", to deal with the case of a transparent firewall\nproxy?\n\nIt was also not specified as to where to put this statement; it looks to\nme like it belongs in section 3.1 HTTP Version.\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Syntax (and other) problems in Digest spe",
            "content": "  Ronald Tschalaer points out a number of issues; in the interest of\n  getting resolutions for as many as possible before the end of this\n  week (the cutoff for December IETF submissions), I've been asked to\n  propose specific wording changes for the new draft.  I've used the\n  suggestions that Ronald included in some cases and changed them\n  slightly in others.  Many thanks to Ronald for all the testing he's\n  been doing in this area.\n\n  Note that for the next round of drafts all the authentication\n  specifications (digest and basic) for HTTP are being moved out of\n  the successor to 2068 and into a separate RFC to be advanced in\n  parallel, so references to section numbers here are to those in the\n  current documents (...rev-00.txt and rfc2069); sorry for any\n  confusion that causes.\n\n  Taking the issues somewhat out of order... Ronald points out that in\n  one major way (from my draft :-) and a couple of minor ones, the\n  digest scheme did not follow the general syntax for an\n  authentication challenge as given in RFCs 1945 and 2068:\n\nRT>      challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\nRT>      credentials    = basic-credentials\nRT>                       | auth-scheme #auth-param\nRT>      auth-param     = token \"=\" quoted-string\n\n  It seems unnecessarily complex to require that the value part of the\n  auth-param always be a quoted string (given that it will often be a\n  simple true/false value, for example).  I propose that the value\n  part of an auth-param be allowed to be a single token.  Ronald also\n  pointed out that RFC2069 did not require the 'realm' parameter to be\n  the first authentication parameter.  It looks to me as though this\n  was actually an attempt to write the fact that this parameter was\n  required into the syntax, and since there is no ambiguity created by\n  removing that requirement, I would prefer to see the more general\n  syntax used in 2069.\n\n  The point may be made that some existing browsers may be broken by\n  this in that they may have coded to the 1945/2068 general rule.\n  I've done considerable testing in this area and have found that\n  browsers fall into two categories:\n    - They recognize that a challenge is not basic and give up,\n      displaying an error to the user saying that they can't deal with\n      this server.\n    - They just send basic credentials no matter what the challenge is\n      and it doesn't work.\n  Browser vendors are invited to figure out which thier product does...\n\n  In either case, changing the spec won't have any effect.\n\n  These changes make the general syntax (now presented in 2068 section\n  11 - Access Authentication):\n\n  ================\n         challenge      = auth-scheme 1*SP 1#auth-param\n         credentials    = basic-credentials\n                          | auth-scheme #auth-param\n         auth-param     = token \"=\" ( token | quoted-string )\n\n    The authentication parameter 'realm' is defined for all\n    authentication schemes:\n\n         realm          = \"realm\" \"=\" quoted-string\n  ================\n\n  This rectifies the minor issues that the value parts of the 'stale',\n  'algorithm', and 'uri' auth-params for the Digest scheme were not\n  specified as quoted-strings.  The syntax for the 'digest-required'\n  flag in the WWW-Authenticate and Authorization header fields was\n  worse in that it had no value part specified; it should become (in\n  the digest spec sections 2.1.1 and 2.1.2):\n\n         digest-required     = \"digest-required\" \"=\" ( \"true\" | \"false\" )\n\n  The description of that parameter in 2.1.1 (WWW-Authenticate\n  Response Header) should be:\n\n  ================\n     digest-required\n\n     If the value of the digest-required parameter is 'true', then any\n     request with an entity-body (such as a PUT or a POST) for the\n     resource(s) to which this response applies MUST include the\n     'digest' attribute in its Authorization header.  If the request\n     has no entity-body (such as a GET) then the digest-required value\n     can be ignored.  If the digest-required parameter is not\n     specified, then its value is 'false'.  If the value of the\n     digest-required parameter is 'false', then the 'digest' attribute\n     is OPTIONAL on requests for the resource(s) to which the response\n     applies.\n  ================\n\n  The description for 'digest-required' in 2.1.2 (Authorization\n  Request Header) should be:\n\n  ================\n     If the value of the digest-required parameter is 'true', the\n     response to this request MUST either include the 'digest' field\n     in its Authentication-Info header or the response should be an\n     error message indicating the server is unable or unwilling to\n     supply this field.  In the latter case the requested entity MUST\n     not be returned as part of the response.  If the digest-required\n     parameter is not specified in the request, then its value is\n     'false'.  If the value of the digest-required parameter is\n     'false', then the 'digest' attribute is OPTIONAL for the response\n     to this request.\n  ================\n\n================================================================\n\n  Ronald also points out that commas are valid unescaped in URIs, so\n  that the 'domain' authentication parameter as specified in the\n  Digest draft:\n\n     domain              = \"domain\" \"=\" <\"> 1#URI <\">\n\n  is ambiguous if one or more URI in the list has a comma in it.  I\n  propose that we accept his suggestion to make the value part of the\n  'domain' authentication parameter a quoted space-separated list of\n  URIs:\n\n  ================\n     domain              = \"domain\" \"=\" <\"> URI *( 1*SP URI ) <\">\n\n  domain\n\n  A space-separated list of URIs.  The intent is that the client could\n  use this information to know the set of URIs for which the same\n  authentication information should be sent.  The URIs in this list\n  may exist on different servers.  If this parameter is omitted or\n  empty, the client should assume that the domain consists of all URIs\n  on the responding server.\n  ================\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: REVERSION discussion at Munich...",
            "content": "From: \"Henry Sanders (Exchange)\" <henrysa@EXCHANGE.MICROSOFT.com>\nDate: Mon, 17 Nov 1997 14:37:33 -0800\nTo: \"'jg@pa.dec.com'\" <jg@pa.dec.com>\nSubject: RE: RE-VERSION discussion at Munich....\n\nStrictly speaking it only has to be caching proxies servers. If you're\ngoing to cache you need to upgrade the request to the highest version\nyou support so that you can satisfy potentially higher level requests\n(than the one that triggered the initial server fetch) from the cache\nlater. If you're not caching, you probably should upgrade, but it won't\nreally hurt if you don't. So caching proxies MUST upgrade, non-caching\nproxies SHOULD (or MAY) upgrade, tunnels MUST NOT upgrade. \n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Monday, November 17, 1997 2:27 PM\n> To:Henry Sanders (Exchange)\n> Subject:RE: RE-VERSION discussion at Munich....\n> \n> Proxy servers, or caching proxy servers?\n> \n> You didn't read my mail carefully :-), so now you get another one...\n> \n> Sorry about the duplicate, I blew the address...\n> - Jim\n\n\n\n"
        },
        {
            "subject": "Re: REVERSION discussion at Munich...",
            "content": "Jim Gettys wrote:\n> \n> Given the heat that this generated at Munich, I'd appreciate it\n> if others read Josh's mail, and proposed change to close out the issue.\n> Henry, do you agree?\n> \n> > \"Proxy servers MUST upgrade all requests to the highest\n> > version supported by the proxy\"\n> \n> I also wonder if \"Proxy servers\" is correct...  Should it be\n> \"Caching proxy servers\", to deal with the case of a transparent firewall\n> proxy?\n\nCaching (was that the agreed spelling <duck>?) doesn't quite capture it.\nThe only thing I can think of, but it is horribly unweildy, is:\n\"non-transparent proxies that operate at the HTTP layer\".\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "RE: REVERSION discussion at Munich...",
            "content": "Ok, here's a closer stab on wording, in section 3.1 HTTP version\n\nOld text:\n\nRequests with a version lower than that of the proxy/gateway's version MAY \nbe upgraded before being forwarded; the proxy/gateway's response to that \nrequest MUST be in the same major version as the request.\n\nNew text:\n\nDue to interoperability problems with HTTP/1.0 proxies discovered since \nthe publication of RFC 2068, caching proxies MUST, gateways MAY, and tunnels \nMUST NOT upgrade the request to the highest version they support. The proxy \nor gateway's response to a request MUST be in the same major version as \nthe request. \n\n\n\n"
        },
        {
            "subject": "Re: Ignore prox",
            "content": "Thanks very much for your help.\n\n> For a busy server, it generally makes sense for it to cache if it\n> has to retreive it anyway.  Of course, we're assuming the object\n> isnt already in the cache-- if it is, then the proxy can potentially\n> return it from the cache.\n\n> The biggest cost for a proxy is the origin retreive (contacting the\n> web server ), writing a cache file is much cheaper.  In addition, by\n> caching it, if there is a request for the same object, the proxy can\n> then return it from the cache next time, which helps the proxy a\n> great deal.\n\n> I think in a worst case scenario, it would be O.K. for the proxy to\n> act such that: \"I am too busy to go to the far away web server for\n> you, but Ill give you what I have in my cache, even though its out\n> of date\".  The client can accept that or attempt to go direct.\n\nA very busy proxy is in a state resembling deadlock.  It should not\naccept any new TCP connections, because it will not be able to serve\nthem anyway.  It might want to remember the request and fetch it later\nfrom the origin server.  It should be able to communicate to the\nclient to leave it alone.\n\nThe response \"I will not cache this\" might be useful if a user agent\nis trying to place some information in a cache.  Also, if a user agent\ndoes not follow exclusion standards, the proxy will take the blame.\nIt might be useful to answer \"I will not fetch this for you\" to\nbothersome user agents.\n\n\n\n"
        },
        {
            "subject": "RE: REVERSION discussion at Munich...",
            "content": "> Given the heat that this generated at Munich, I'd appreciate it\n> if others read Josh's mail, and proposed change to close out the\n> issue.\n> Henry, do you agree?\n> \n> > \"Proxy servers MUST upgrade all requests to the highest\n> > version supported by the proxy\"\n> \nYep, I agree. \n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "In response to the TRAILER_FIELDS issue [1], here is a proposal for a new\ngeneral Trailer header field that indicates which header fields the\nrecipient can expect in the trailer of a message encoded using chunked\ntransfer-encoding. The Trailer header field is modelled after the Vary\nheader field.\n\nThis change requires the Accept-Transfer header field defined in an earlier\nmail (not in the archives at this point)\n\nHenrik\n\n[1] http://www.w3.org/Protocols/HTTP/Issues/#TRAILER_FIELDS\n\n****************************************************************************\n\nChanges:\n\nReplace section 3.6 with the following:\n\n3.6 Transfer Codings\n\nTransfer coding values are used to indicate an encoding transformation that\nhas been, can be, or may need to be applied to an entity-body in order to\nensure \"safe transport\" through the network. This differs from a content\ncoding in that the transfer coding is a property of the message, not of the\noriginal entity. Therefore, transfer codings only apply to the immediate\nconnection.\n\n       transfer-coding         = token\n\nAll transfer-coding values are case-insensitive. HTTP/1.1 uses transfer\ncoding values in the Accept-Transfer header field (section Y) and the\nTransfer-Encoding header field (section 14.40).\n\nTransfer codings are analogous to the Content-Transfer-Encoding values of\nMIME [7], which were designed to enable safe transport of binary data over\na 7-bit transport service. However, safe transport has a different focus\nfor an 8bit-clean transfer protocol. In HTTP, the only unsafe\ncharacteristic of message-bodies is the difficulty in determining the exact\nbody length (section 7.2.2), or the desire to encrypt data over a shared\ntransport.\n\nThe Internet Assigned Numbers Authority (IANA) acts as a registry for\ntransfer-coding value tokens. Initially, the registry contains the\nfollowing tokens: \"chunked\" (section 3.6.1) and \"identity\" (section 3.6.2).\n\nThe registration of new transfer-codings are done as for content-codings.\n\nA server which receives an entity-body with a transfer-coding it does not\nunderstand SHOULD return 501 (Unimplemented), and close the connection. A\nserver MUST NOT send transfer-codings to an HTTP/1.0 client.\n\n3.6.1 Chunked Transfer Coding\n\nThe chunked encoding modifies the body of a message in order to transfer it\nas a series of chunks, each with its own size indicator, followed by an\noptional trailer containing entity-header fields. This allows\ndynamically-produced content to be transferred along with the information\nnecessary for the recipient to verify that it has received the full message.\n\n       Chunked-Body   = *chunk\n                        last-chunk\n                        trailer\n                        CRLF\n       chunk          = chunk-size [ chunk-extension ] CRLF\n                        chunk-data CRLF\n       chunk-size     = 1*HEX \n       last-chunk     = 1*(\"0\") [ chunk-extension ] CRLF\n\n       chunk-extension= *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] ) \n       chunk-ext-name = token\n       chunk-ext-val  = token | quoted-string\n       chunk-data     = chunk-size(OCTET)\n       trailer        = *entity-header\n\nThe chunk-size field is a string of hex digits indicating the size of the\nchunk. The chunked encoding is ended by any chunk whose size is zero,\nfollowed by the trailer, which is terminated by an empty line.\n\nThe trailer allows the sender to include additional HTTP header fields at\nthe end of the message. The Trailer header field can be used to indicate\nwhich header fields are included in a trailer (see section X).\n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the Accept-Transfer field.\n\nAn example process for decoding a Chunked-Body is presented in appendix\n19.4.6.\n\nAll HTTP/1.1 applications MUST be able to receive and decode the \"chunked\"\ntransfer coding, and MUST ignore chunk-extension extensions they do not\nunderstand.\n\n3.6.2 Identity Transfer Coding\n\nThe identity transfer-encoding is the default (identity) encoding; the use\nof no transformation whatsoever.  This transfer-coding is used only in the\nAccept-Transfer header, and SHOULD NOT be used in any Transfer-Encoding\nheader.\n\n****************************************************************************\n\nAlso add the Description of Trailer Header Field as a new header:\n\nTrailer\n\nThe Trailer general field value indicates that the given set of header\nfields are present in the trailer of a message encoded with chunked\ntransfer-coding.\n\n       Trailer  = \"Trailer\" \":\" 1#field-name\n\nAn HTTP/1.1 sender MAY include a Trailer header field in a message using\nchunked transfer-coding with a non-empty trailer. Doing so allows the\nrecipient to know which header fields to expect in the trailer.\n\nIf no Trailer header field is present, the trailer SHOULD NOT include any\nother header fields than Content-MD5 and Authentication-Info.\n\nA server MUST NOT include any other header fields unless the \"chunked\"\ntransfer-coding is present in the request as an accepted transfer-coding in\nthe Accept-Transfer field.\n\nMessage headers listed in the Trailer header field MUST NOT include the\nTransfer-Encoding and the Trailer header field.\n\n****************************************************************************\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "RE: REVERSION discussion at Munich...",
            "content": "At 15:03 11/17/97 -0800, Jim Gettys wrote:\n\n>New text:\n>\n>Due to interoperability problems with HTTP/1.0 proxies discovered since \n>the publication of RFC 2068, caching proxies MUST, gateways MAY, and tunnels \n>MUST NOT upgrade the request to the highest version they support. The proxy \n>or gateway's response to a request MUST be in the same major version as \n>the request. \n\nWe can't say that if the server that the proxy speaks to is buggy and\ndoesn't understand HTTP/1.1. It may have to speak HTTP/1.0 in some\nsituations. It also doesn't make sense to say that gateways MAY upgrade as\nthey are gateways into other protocols.\n\nI can't see what we get out of this that is not already captured in RFC\n2145, section 2.3:\n\n   An HTTP client SHOULD send a request version equal to the highest\n   version for which the client is at least conditionally compliant, and\n   whose major version is no higher than the highest version supported\n   by the server, if this is known.  An HTTP client MUST NOT send a\n   version for which it is not at least conditionally compliant.\n\n   An HTTP client MAY send a lower request version, if it is known that\n   the server incorrectly implements the HTTP specification, but only\n   after the client has determined that the server is actually buggy.\n\n   An HTTP server SHOULD send a response version equal to the highest\n   version for which the server is at least conditionally compliant, and\n   whose major version is less than or equal to the one received in the\n   request.  An HTTP server MUST NOT send a version for which it is not\n   at least conditionally compliant.  A server MAY send a 505 (HTTP\n   Version Not Supported) response if cannot send a response using the\n   major version used in the client's request.\n\n   An HTTP server MAY send a lower response version, if it is known or\n   suspected that the client incorrectly implements the HTTP\n   specification, but this should not be the default, and this SHOULD\n   NOT be done if the request version is HTTP/1.1 or greater.\n\nBut then again, I wasn't in Munich, so I may be mising something...\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "I have promissed Jim to reach closure on this.\n\nI do not believe that the discussion has brought any need for changed\nwording regading content-codings vs transfer-codings.\n\nHowever, it has shown a need for better support for transfer-codings, which\nis the reason for the Accept-Transfer header proposal [1] and the changed\nwordings for Transfer codings [2]. So please review these proposals\nAccept-Transfer proposal ASAP.\n\nThanks,\n\nHenrik\n\n[1] http://www.findmail.com/listsaver/http-wg/7485.html\n[2] http://www.findmail.com/listsaver/http-wg/7492.html\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Basic Authentication behavior (PROTECTION_SPACE issue.",
            "content": "This is in last call... The proposed text is at: \nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0758.html \n\nWhile editing it into the draft, I noticed a nit or two...\n\nFoteos Macrides wording says:\n\n[START INSERT]---------------------------------------------------------\n            A user agent SHOULD assume that all paths at or deeper than\n            the depth of the last symbolic element in the path field\n            of the Request-URI also are within the protection space\n            specified by the Basic realm value of the current challenge,\n            and MAY send the corresponding Authorization header with\n            requests for resources in that space without receipt of\n            another challenge from the server.\n[END INSERT]-----------------------------------------------------------\n\nI believe user agent should be client, rather than the more restrictive\nuser agent in this context.  And the sentence was running on...\n\nMy revised text is:\n\n\"A client SHOULD assume that all paths at or deeper than the depth of the \nlast symbolic element in the path field of the Request-URI also are within \nthe protection space specified by the Basic realm value of the current \nchallenge. A client MAY send the corresponding Authorization header with \nrequests for resources in that space without receipt of another challenge \nfrom the server.\"\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Basic Authentication behavior (PROTECTION_SPACE issue.",
            "content": "jg@pa.dec.com (Jim Gettys) wrote:\n>This is in last call...\n>[...]\n>My revised text is:\n>\n>\"A client SHOULD assume that all paths at or deeper than the depth of the \n>last symbolic element in the path field of the Request-URI also are within \n>the protection space specified by the Basic realm value of the current \n>challenge. A client MAY send the corresponding Authorization header with \n>requests for resources in that space without receipt of another challenge \n>from the server.\"\n\nThat's good.\n\nYou might also want to add an explicit statement in the\nsection on Proxy-Authentication that the implied protection space\nfor Basic is all URLs (which means that the realm value is\nirrelevant, but still must be included in the proxy's challenge\nand in the client's Proxy-Authorization header).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSION discussion at Munich...",
            "content": "Henrik Frystyk Nielsen <frystyk@w3.org> wrote:\n>At 15:03 11/17/97 -0800, Jim Gettys wrote:\n>\n>>New text:\n>>\n>>Due to interoperability problems with HTTP/1.0 proxies discovered since \n>>the publication of RFC 2068, caching proxies MUST, gateways MAY, and tunnels \n>>MUST NOT upgrade the request to the highest version they support. The proxy \n>>or gateway's response to a request MUST be in the same major version as \n>>the request. \n>\n>We can't say that if the server that the proxy speaks to is buggy and\n>doesn't understand HTTP/1.1. It may have to speak HTTP/1.0 in some\n>situations. It also doesn't make sense to say that gateways MAY upgrade as\n>they are gateways into other protocols.\n>\n>I can't see what we get out of this that is not already captured in RFC\n>2145, section 2.3:\n>[...]\n\nI also don't know if something new was raised in Munich,\nbut as far as discussions on this list are concerned, I similarly\nfeel that Section 2.3 covers the issues quite well and would be\ndifficult to improve.  One thing that's missing is an explicit\nreference to the Via header for the case in which proxies are\ninterposed between the browser and origin server, so that scripts\ncan't rely on the SERVER_PROTOCOL variable, but should also\nexamine the HTTP_VIA variable (or their server-side scripting\nequivalents) to assess the browser's version. I'm not the\nappropriate one to attempt a chunk of text about that, but if\nsomeone who is, did, I suspect many implementors and providers\nwould be appreciative.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": ">Accept-Transfer\n>\n>The Accept-Transfer request-header field is similar to Accept-Encoding, but\n>restricts the transfer-codings (section 3.6) that are acceptable in the\n>response.\n>\n>       Accept-Transfer  = \"Accept-Transfer\" \":\"\n>                           1#( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n>       t-codings        = transfer-codings\n>\n>Examples of its use are:\n>\n>       Accept-Transfer: deflate\n>       Accept-Transfer:\n>       Accept-Transfer: chunk=1.0; deflate=0.5\n>\n>The Accept-Transfer header field only applies to the immediate connection.\n>Therefore, the accept-transfer keyword MUST be supplied within a Connection\n>header field (section 14.10) whenever Accept-Transfer is present in an\n>HTTP/1.1 message.\n\nI think this is too verbose (on the wire) and tries to pack in a bunch\nof unneeded features.\n\nQvalues are not useful for transfer encodings -- the coding must not\nbe lossy, and the vast portion of work is being performed on the server\nside, so the server should be capable of choosing which one is best.\nLikewise, chunked and identity are always required -- there is no\nreasonable use for refusal based on lack-of-encoding.  Thus, the only\nfeature we actually need is the ability to request a given\ntransfer-encoding be used.\n\nFor that purpose, I suggest we just introduce a Connection keyword\nfor each new transfer coding.  In other words, an HTTP/1.1 request\ncontaining\n\n    Connection: deflate\n\nindicates the client is willing and able to handle chunked and deflate\n(and none) transfer codings on the response.  This is much shorter and\neasier to parse than\n\n    Connection: Accept-Transfer\n    Accept-Transfer: deflate\n\nNote that we must also include a requirement that chunked be the\nlast encoding applied if there is more than one.  Note also that this\ncan be sent in responses as well to indicate the acceptance of deflated\nrequest bodies on future requests, though this would only be useful for\nthings like printers and WebDAV.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": ">Trailer\n>\n>The Trailer general field value indicates that the given set of header\n>fields are present in the trailer of a message encoded with chunked\n>transfer-coding.\n>\n>       Trailer  = \"Trailer\" \":\" 1#field-name\n>\n>An HTTP/1.1 sender MAY include a Trailer header field in a message using\n>chunked transfer-coding with a non-empty trailer. Doing so allows the\n>recipient to know which header fields to expect in the trailer.\n>\n>If no Trailer header field is present, the trailer SHOULD NOT include any\n>other header fields than Content-MD5 and Authentication-Info.\n\nI don't think this exception is necessary.  As far as I know, none of the\nexisting HTTP/1.1 servers put anything in the trailer yet.\n\n>A server MUST NOT include any other header fields unless the \"chunked\"\n>transfer-coding is present in the request as an accepted transfer-coding in\n>the Accept-Transfer field.\n\nLikewise, that is unnecessary.  We can live with the discrepancy between\nRFC 2068 and now, since nobody uses these features yet.  Somebody should\ncorrect me if I'm wrong [and they do care].\n\n>Message headers listed in the Trailer header field MUST NOT include the\n>Transfer-Encoding and the Trailer header field.\n\nand Content-Length.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": ">>>>> \"RTF\" == Roy T Fielding <fielding@kiwi.ics.uci.edu> writes:\n\n>> If no Trailer header field is present, the trailer SHOULD NOT include any\n>> other header fields than Content-MD5 and Authentication-Info.\n\nRTF> I don't think this exception is necessary.  As far as I know, none of the\nRTF> existing HTTP/1.1 servers put anything in the trailer yet.\n\n>> A server MUST NOT include any other header fields unless the \"chunked\"\n>> transfer-coding is present in the request as an accepted transfer-coding in\n>> the Accept-Transfer field.\n\nRTF> Likewise, that is unnecessary.  We can live with the discrepancy between\nRTF> RFC 2068 and now, since nobody uses these features yet.  Somebody should\nRTF> correct me if I'm wrong [and they do care].\n\n  Released versions of EmWeb have been putting these in chunked\n  trailers for several months.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Ignore prox",
            "content": "> \n> A very busy proxy is in a state resembling deadlock.  It should not\n> accept any new TCP connections, because it will not be able to serve\n> them anyway.  It might want to remember the request and fetch it later\n> from the origin server.  It should be able to communicate to the\n> client to leave it alone.\nIf the proxy did as you say, ie dont accept new TCP connections,\nit will have no way of knowing what the request was going to be.\nTo determine that, and cache it for later, it would have to accept\nthe connection and at least read the headers.  If the proxy is\nalready overloaded, this doesnt help it any.\n\nAs far as the 'deadlock' wording, i dont agree.  The proxy will be\nrunning fine, ie not in a deadlock.  Under intense load, the proxy\nwill not get around to handling or even accepting the clients' request\nbefore the client times out. (Our experience )\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy\nRanger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Test Day Schedul",
            "content": "  Some of you may have noticed that I did not send a reminder for test\n  day registrations for this week.  With the I-D deadline fast\n  approaching, I thought it best to take this week off.  Next week is\n  Thanksgiving in the US, so I will not schedule one then either.\n\n  I do plan to restart them the following week, and will be publishing\n  a web page shortly with pointers to servers that are generally\n  available (feel free to send pointers).\n\n  Sorry I didn't get this note out sooner...\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": "I am generally pursuaded that Roy is right, that we should\nbe using both Content-Encoding and Transfer-Encoding, and\nthat Range selections should be applied after Content-Encoding\nand before Transfer-Encoding.  I also think that Henrik's\nproposal for Accept-Transfer is generally a good one.\n\nBut there are still a few loose ends to tie up.\n\nFirst of all, Henrik wrote (regarding Transfer-Encoding):\n\n    1) They don't have the same scope - one is end-to-end and the other is\n    hop-by-hop. As the message length changes, it can only be used through\n    proxies that know about that particular encoding. In other words: the\n    stupidest link in the chain decides the encoding.\n\nMore specifically, because Transfer-Encoding and Accept-Transfer are\nhop-by-hop fields, a response that flows through an HTTP/1.0 proxy\ncan't have a Transfer-encoding (such as on-the-fly compression)\nfor the two hops that involve that proxy.\n\nThis is sad, but it does avoid a big problem with the use of new\ncompression algorithms (or other codings): the possibility that such\nresponses might be improperly cached by an HTTP/1.0 shared cache, and\nthen delivered to a client that doesn't understand them.  It might\nalso provide some incentive for people to upgrade their 1.0 proxies\nto 1.1, since it's usually the proxy operator who is paying for\nthe bandwidth on at least one side of the proxy.\n\nHowever, there is (at least) one more slight problem with the\nhop-by-hop nature of Transfer-Encoding.  Section 15.1.1 (End-to-end\nand Hop-by-hop Headers) currently says:\n\n            For the purpose of defining the behavior of caches and non-\n            caching proxies, we divide HTTP headers into two categories:\n               . End-to-end headers, which must be transmitted to the\n                 ultimate recipient of a request or response. End-to-end\n                 headers in responses must be stored as part of a cache\n                 entry and transmitted in any response formed from a\n                 cache entry.\n               . Hop-by-hop headers, which are meaningful only for a\n                 single transport-level connection, and are not stored\n                 by caches or forwarded by proxies.\n\n            The following HTTP/1.1 headers are hop-by-hop headers:\n               . Connection\n               . Keep-Alive\n               . Public\n               . Proxy-Authenticate\n               . Transfer-Encoding\n               . Upgrade\n\n[NOTE TO JIM GETTYS: the \"must\"s in that passage should be MUSTs,\nright?]\n\nThe most simplistic interpretation of the phrase \"Hop-by-hop headers\n... and are not stored by caches or forwarded by proxies\" implies\nthat a transfer-encoding has to be removed by a proxy cache before\nthe response is stored.\n\nClearly, if a proxy supports a transfer-coding, and both the\ninbound server and the outbout client also support the same coding,\nthen it would be a real waste of CPU time to do this for compressed\ntransfer-codings.  So I think that after\n\n               . Hop-by-hop headers, which are meaningful only for a\n                 single transport-level connection, and are not stored\n                 by caches or forwarded by proxies.\n\nwe should add:\n\n  Note: a proxy or cache need not actually delete a\n  hop-by-hop header in a case where its external\n  behavior would be equivalent to deleting the header\n  and then adding it again.  For example, a proxy\n  receiving a response with a non-identity\n  transfer-coding need not remove that coding, and the\n  corresponding Transfer-Encoding header field, unless\n  the response is being sent to a client that has not\n  sent an appropriate Accept-Transfer header.\n\n--------\n\nThe other problem that we may need to clarify is that if\na transfer-coding (e.g., \"compress\") is used with a Range response\ncontaining multiple ranges, then should the transfer-coding apply\nto the entire \"multipart/byteranges\" body, or just to the\nparts individually?\n\nIt seems possible to leave this up to the server, and let the\nserver choose the most appropriate option for a given response,\nrather than to try to embed the choice in the protocol specification.\nBut this means that a client sending both a multi-part Range\nrequest and a non-identity \"Accept-Transfer:\" would have to be\nable to grok two different forms of response.\n\nE.g., combining the example in 19.2 with \"compress\", the client might get\n\n       HTTP/1.1 206 Partial content\n       Date: Wed, 15 Nov 1995 06:25:24 GMT\n       Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n       Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n\n       --THIS_STRING_SEPARATES\n       Content-type: application/pdf\n       Content-range: bytes 500-999/8000\n       Transfer-Encoding: compress\n\n       ...the first range..., compressed\n       --THIS_STRING_SEPARATES\n       Content-type: application/pdf\n       Content-range: bytes 7000-7999/8000\n       Transfer-Encoding: compress\n\n       ...the second range, compressed\n       --THIS_STRING_SEPARATES--\n\nor it might get\n\n       HTTP/1.1 206 Partial content\n       Date: Wed, 15 Nov 1995 06:25:24 GMT\n       Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n       Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n       Transfer-Encoding: compress\n       \n       .... compressed form of the multipart/byteranges type .....\n\nNote that section 3.7.2 (Multipart Types) seems to allow the first\nform:\n           In HTTP, multipart body-parts MAY contain header fields\n            which are significant to the meaning of that part.\n\nIf this makes sense to people, then I would add this paragraph to the\nend of section 3.7.2 (Multipart Types).\n\nIf a client has indicated its acceptance of one or more\nnon-identity transfer-codings (using the Accept-Tranfser\nheader, section XXX) and it is otherwise appropriate to use a\nmultipart type, the server MAY either apply a transfer-coding\nto the entire message-body, and MAY apply a transfer-coding to\none or more of the individual body-parts.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "Roy Fielding writes:\n    Qvalues are not useful for transfer encodings -- the coding must not\n    be lossy, and the vast portion of work is being performed on the server\n    side, so the server should be capable of choosing which one is best.\n    \nWe can quibble about whether these should be called \"qvalues\" or\nsomething else, but some sort of preference weighting is most\ndefinitely useful.  Generally, there are both costs and benefits\nassociated with transfer-codings (such as compression) and there\nisn't always a fixed tradeoff to be made.  For example, I tried\nrunning a large file through both compress and gzip.  The output\nof gzip was about 25% smaller than the output of compress, but\nit took almost 4 times as much CPU time to do the compression.\n\nOn the other hand, it took about 33% longer, and slightly more memory,\nto decompress the output of compress.  So, depending on parameters such\nas network bandwidth and client CPU performance (and perhaps client RAM\navailability), the server is not necessarily capable of choosing the\nmost appropriate transfer-coding without some help from the client.\n\nSection 3.9 does say\n\n            \"Quality values\" is a misnomer, since these values merely\n            represent relative degradation in desired quality.\n\nbut it also says\n\n    HTTP content negotiation (section 12) uses short \"floating\n            point\" numbers to indicate the relative importance\n            (\"weight\") of various negotiable parameters.\n\nso I think we should continue to use \"qvalues\" here, rather than\ndefining a \"pvalue\" (preference value) just so we can pretend that\nqvalues are only related to lossy encodings.\n\nOne more thing: I couldn't find a place in the spec where it\nsays that transfer-codings \"must not be lossy\".  In fact, the\nTransSend project at UC Berkeley has demonstrated the utility\nof lossy codings in some applications, and I'm not sure we should\nbe banning these.\n\n    Likewise, chunked and identity are always required -- there is no\n    reasonable use for refusal based on lack-of-encoding.  Thus, the only\n    feature we actually need is the ability to request a given\n    transfer-encoding be used.\n    \nI disagree.  With respect to \"chunked\", we could presumably change\nAll HTTP/1.1 applications MUST be able to receive and decode\n        the \"chunked\" transfer coding,\nto\nAll HTTP/1.1 applications MUST be able to receive and decode\n        the \"chunked\" transfer coding, except for clients that\nexplicitly reject this transfer coding using a qvalue of 0\nin an Accept-Transfer request header field.\n\nI'm not necessarily saying that we should do this, but it seems\nsafe to do so, and it might pay off for implementors of clients\nwith limited code space.\n\nWith respect to \"identity\", I believe that the argument has already\nbeen made (the last time we debated the coding issue) that a client\nmight want to suppress the transfer of a large response if it cannot\nbe compressed first.  (The user might want to choose a different\nlink instead, for example.)\n\n    Note that we must also include a requirement that chunked be the\n    last encoding applied if there is more than one.\n\nIs this really true?  I'm not sure that it would be a major win,\nbut why not allow a server to apply compression after chunking?\nIt would probably improve the overall compression ratio.  (I.e.,\nyou generally get a better ratio when compressing a large file\nthan when compressing a small prefix of the same file.)  What\ngoes wrong if we allow this?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: new editorial issue RANGE_WITH_CONTENTCODIN",
            "content": ">we should add:\n>\n>  Note: a proxy or cache need not actually delete a\n>  hop-by-hop header in a case where its external\n>  behavior would be equivalent to deleting the header\n>  and then adding it again.  For example, a proxy\n>  receiving a response with a non-identity\n>  transfer-coding need not remove that coding, and the\n>  corresponding Transfer-Encoding header field, unless\n>  the response is being sent to a client that has not\n>  sent an appropriate Accept-Transfer header.\n\nYep.\n\n>The other problem that we may need to clarify is that if\n>a transfer-coding (e.g., \"compress\") is used with a Range response\n>containing multiple ranges, then should the transfer-coding apply\n>to the entire \"multipart/byteranges\" body, or just to the\n>parts individually?\n\nThe entire body, since Transfer-Encoding is not a MIME header field.\nThe HTTP/1.1 message data model was designed to be easy to translate\ninto a future version of HTTP that had better layering (i.e., actual\ninstead of theoretical layers) between transfer and payload.  That\ngets harder if we start pushing HTTP elements inside body parts.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "Henrik's proposal includes:\n\n    The Internet Assigned Numbers Authority (IANA) acts as a registry for\n    transfer-coding value tokens. Initially, the registry contains the\n    following tokens: \"chunked\" (section 3.6.1) and \"identity\" (section 3.6.2).\n    \nSince we got into this partly because of the Range+compression question,\nI suggest that this paragraph should read:\n\n    The Internet Assigned Numbers Authority (IANA) acts as a registry\n    for transfer-coding value tokens. Initially, the registry contains\n    the following tokens: \"chunked\" (section 3.6.1), \"identity\"\n    (section 3.6.2), \"gzip\" (section 3.5), \"compress\" (section 3.5),\n    and \"deflate\" (section 3.5).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "Henrik Frystyk Nielsen writes:\n\n       Accept-Transfer  = \"Accept-Transfer\" \":\"\n                           1#( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n\nShouldn't that be:\n\n       Accept-Transfer  = \"Accept-Transfer\" \":\"\n                           #( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n\nsince one of your examples is:\n\n       Accept-Transfer:\n\n(I know, I made the same mistake for Accept-Encoding in rev-00.)\n\nAlso, since (as Roy has pointed out) the requirement for protecting\nAccept-Transfer with Connection makes requests somewhat verbose,\nperhaps we should be using a shorter name ... \"Accept-Trans\"\nwould save 6 bytes per request.  \n\nI'm not even sure this header should be called \"Accept-anything\",\nsince it's a hop-by-hop mechanism and thus pretty much orthogonal\nto content negotiation.  Maybe \"OK-Trans\" (saving another 8 bytes\nper request)?  It's not as if any human being is supposed to be\nreading these headers.\n\n-Jeff\n\nP.S.: OK, I *do* include HTTP implementors in the set of human\nbeings :-)\n\n\n\n"
        },
        {
            "subject": "Parameters for transfercodings",
            "content": "I expect to get flamed for this message, but I sort of promised someone\nthat I would make the attempt.\n\nSome of the potentially-useful transfer-coding algorithms are\ninherently parameterizable.  For example, \"gzip\" has 9 different\ncompression levels (trading off compression ratio for speed).\n\"compress\" has a range of possible settings (I'm not quite sure how\nmany).  In both cases, once the sender has chosen a setting, the\nrecipient should be able to decompress the result, but HTTP provides no\nmechanism for the client to suggest the preferred setting.\n\nSomewhat more exotic, but hardly without practical merit, is the\npossibility of using pre-agreed compression dictionaries to further\nreduce the size of the transferred compressed form.  The idea is that\nit should take fewer bytes to send the name of the appropriate\ndictionary than to transmit its contents.  However, in this case, the\nsender needs a way to name the dictionary used in the message.\n\nOne of my friends in the data compression community has been beating me\nup about this for a while, and I've fought him off by saying \"well,\nit's too late to change Accept-Encoding because that would break too\nmany existing implementations.\"  But I don't think the same excuse\napplies for Accept-Transfer/Transfer-Encoding, so I am stuck with\nmaking the following proposal :-).\n\nSuppose, therefore, that we change this syntax in section 3.6\n(Transfer Codings) from\n\n                   transfer-coding   = \"chunked\" | transfer-extension\n                   transfer-extension      = token\n\nto\n\n                   transfer-coding   = \"chunked\" | transfer-extension\n                   transfer-extension  = token | token token-params\n   \n   token-params = *( \";\" token-param-name [ \"=\" \n                                    token-param-value ] )\n\n   token-param-name = token\n   token-param-value = token | quoted-string\n\nIf I got the BNF right (don't count on it!) then this would lead\nto examples like\n\nAccept-Transfer: gzip_p;level=\"3\", compress_p;bits=\"9-11\"\n\nand\n\nTransfer-Encoding: fooflate;dict=\"http://dicts.net/37\"\n\nThe specific definition of the parameter names and values would be\npart of the specification for a given tranfer-coding (which is\nwhy my examples do not use the already-defined transfer-codings).\n\nBy construction, the BNF does not allow parameters to be used\nwith \"chunked\", and I do not believe that any other transfer-coding\nvalues are used in current implementations ... so this should\nbe compatible with existing implementations.\n\nAlso, this BNF leads to the possibility of\n\nAccept-Transfer: gzip_p;level=\"3\";q=0.5, compress_p;bits=\"9-11\"\n\nbecause of the qvalues allowed with Accept-Transfer, but I don't\nthink the parsing problem is too difficult ... although we might\nwant to say\n\nThe token-param-name MUST NOT be \"q\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "Jeffrey Mogul writes:\n>Roy Fielding writes:\n>    Qvalues are not useful for transfer encodings -- the coding must not\n>    be lossy, and the vast portion of work is being performed on the server\n>    side, so the server should be capable of choosing which one is best.\n>    \n>We can quibble about whether these should be called \"qvalues\" or\n>something else, but some sort of preference weighting is most\n>definitely useful.  Generally, there are both costs and benefits\n>associated with transfer-codings (such as compression) and there\n>isn't always a fixed tradeoff to be made.  For example, I tried\n>running a large file through both compress and gzip.  The output\n>of gzip was about 25% smaller than the output of compress, but\n>it took almost 4 times as much CPU time to do the compression.\n>\n>On the other hand, it took about 33% longer, and slightly more memory,\n>to decompress the output of compress.  So, depending on parameters such\n>as network bandwidth and client CPU performance (and perhaps client RAM\n>availability), the server is not necessarily capable of choosing the\n>most appropriate transfer-coding without some help from the client.\n\nLet me put it this way.  There are a bunch of parameters one might\ngive to the deflate process such that it controls various performance\naspects of the compression algorithm.  These parameters are known\nby the server, and will be set according to the server's needs.  The\nonly thing the server will care about is whether the client is capable\nof decoding the final result.  It doesn't matter what preference the\nclient assigns beyond that yes/no criteria -- the server's performance\nneeds are paramount.\n\nIn other words, I agree that there are tradeoffs, but the client has\nno input in deciding those tradeoffs (nor should it).  That doesn't\nmean the server will necessarily ignore the client's needs -- it just\nmeans the server will decide what is needed based on its own observation\nof the connection throughput, typical client capabilities, and the\ncurrent resource constraint/load on the server itself.  A simple\nordering or relative quality value given by the client says nothing\nof importance to the server, and thus provides no useful input to\nits decision.\n\nAs such, sending additional information in every request, which adds\nits own latency problems, is wrong.  We are at the stage in the\nprotocol developent where new features need demonstrated benefit before\nthey get added to the specification, and the only demonstrated benefit\nhere is the desire to indicate that the client understands a new\ntransfer-coding.\n\n>One more thing: I couldn't find a place in the spec where it\n>says that transfer-codings \"must not be lossy\".  In fact, the\n>TransSend project at UC Berkeley has demonstrated the utility\n>of lossy codings in some applications, and I'm not sure we should\n>be banning these.\n\nThose are not transfer-codings.  If the transmission is capable of\nlosing data, then it is losing the data associated with the other\nmetadata on that response.  In other words, a lossy encoding performs\na transform on the content, and therefore must be a content coding.\n\n>    Likewise, chunked and identity are always required -- there is no\n>    reasonable use for refusal based on lack-of-encoding.  Thus, the only\n>    feature we actually need is the ability to request a given\n>    transfer-encoding be used.\n>    \n>I disagree.  With respect to \"chunked\", we could presumably change\n>All HTTP/1.1 applications MUST be able to receive and decode\n>        the \"chunked\" transfer coding,\n>to\n>All HTTP/1.1 applications MUST be able to receive and decode\n>        the \"chunked\" transfer coding, except for clients that\n>explicitly reject this transfer coding using a qvalue of 0\n>in an Accept-Transfer request header field.\n>\n>I'm not necessarily saying that we should do this, but it seems\n>safe to do so, and it might pay off for implementors of clients\n>with limited code space.\n\nNo, we can't do that.  That has been written in stone (i.e., deployed code).\nIt is also not desirable from a robustness perspective -- HTTP needs an\nindicator of message length in order to differentiate between complete\nand incomplete responses.  HTTP is unsafe for data transfer without that\ninformation, so we don't give the compliant application any choice.\n\n>With respect to \"identity\", I believe that the argument has already\n>been made (the last time we debated the coding issue) that a client\n>might want to suppress the transfer of a large response if it cannot\n>be compressed first.  (The user might want to choose a different\n>link instead, for example.)\n\nI have yet to see or hear of a client that wishes to duplicate connection\nsetup, round-trip request/response, and shutdown costs just to insist on\na given transfer-coding.  Even if such a client were to exist, it would be\nmore efficient to just close the connection if the response was not\nencoded to its satisfaction (after all, that is what it would have to\ndo in your scenario if it gets back an error response indicating that\nthe encoding is unavailable on that connection).  In practice, I doubt\nthat anyone would ever implement such a client.\n\n>    Note that we must also include a requirement that chunked be the\n>    last encoding applied if there is more than one.\n>\n>Is this really true?  I'm not sure that it would be a major win,\n>but why not allow a server to apply compression after chunking?\n>It would probably improve the overall compression ratio.  (I.e.,\n>you generally get a better ratio when compressing a large file\n>than when compressing a small prefix of the same file.)  What\n>goes wrong if we allow this?\n\nI suppose it would not be a problem if the encoding was size-delimited.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Call for Papers - 11th Unicode Conference  September 1997, Californi",
            "content": "                     C A L L   F O R   P A P E R S\n                    *******************************\n\n              Eleventh International Unicode Conference\n                                  and\n                       Global Computing Showcase\n\n   Software Development + the Internet: Going Global with Unicode(R)\n\n                          September 3-5, 1997\n\n                         San Jose, California\n\n**********************************************************************\n\nWith the Internet comes change: businesses world-wide expect accurate \naccess to text, independent of location or language.  Users demand \nsoftware that runs everywhere.  To meet these needs requires global \ncomputing.  The Eleventh International Unicode Conference will address \nall issues and topics relating to global computing with Unicode.\n\nConference attendees will include managers, software engineers, \nsystems analysts, and product marketing personnel responsible for the \ndevelopment of software supporting Unicode/ISO 10646 as well as those \ninvolved in all aspects of the globalization of software and the \nInternet.\n\nSPONSORS\n   Microsoft Corporation\n   Netscape Communications Corporation\n   Reuters Limited\n   W3C (World Wide Web Consortium)\n\nWEB SITES\n   <http://www.reuters.com/unicode/iuc11>\n   <http://www.unicode.org>\n\nDEADLINES\n   Submissions due:          18 April 1997\n   Notification date:        9 May 1997\n   Camera ready papers due:  6 June 1997\n\nTHEME & TOPICS\n\nGlobal computing with Unicode is the overall theme of the Conference.  \nPresentations should be geared towards a technical audience.  \nSuggested topics of interest include, but are not limited to:\n- The Internet and Unicode \n- The World Wide Web (WWW) and Unicode \n- Java and Unicode \n- Character set issues on the Internet and WWW \n- Metadata and Unicode \n- Web search engines and Unicode \n- Library and archival concerns \n- Unicode in UNIX \n- Unicode in databases \n- Unicode in large scale networks \n- The results of using Unicode applications (case studies, solutions) \n- Business topics and market forecasts \n- Language processing issues with Unicode data \n- Migrating legacy applications to Unicode \n- Cross platform issues \n- Compression of Unicode data \n- Testing Unicode applications \n- Usability evaluations of Unicode applications \n\nSESSIONS\n\nThe Conference Program will provide a wide range of sessions \nincluding:\n- Keynote presentations\n- Technical presentations\n- Panel sessions\n- Open discussion sessions \n- Workshops/Tutorials\n\nAll sessions except the Workshops/Tutorials will be of 40 minute \nduration.  In some cases, two consecutive 40 minute program slots may \nbe devoted to a single session.\n\nThe Workshops/Tutorials will each last three hours.  They should be \ndesigned to stimulate discussion and participation, using slides and \ndemonstrations.\n\nPUBLICITY\n\nIf your paper is accepted, your details will be included in the \nConference brochure and Web pages and the paper itself will appear in \nthe book of Conference Proceedings.\n\nSUBMISSIONS\n\nSubmissions must contain:\n\n1  An abstract of 150-250 words, consisting of statement of purpose, \n   paper description, and your conclusions or final summary\n\n2  A brief biography\n\n3  The details listed below:\n\n\n   TITLE OF SESSION:         _________________________________________\n\n   TYPE OF SESSION:          [ ] Keynote presentation\n\n                             [ ] Technical presentation\n\n                             [ ] Panel session\n\n                             [ ] Open discussion session\n\n                             [ ] Workshop/Tutorial\n\n   TARGET AUDIENCE (you may select more than one category):\n\n                             [ ] Manager\n\n                             [ ] Software Engineer\n\n                             [ ] Systems Analyst\n\n                             [ ] Marketer\n\n                             [ ] Other: ______________________________\n\n   NAME:   Dr/Mr/Mrs/Ms:     _________________________________________\n\n   TITLE/POSITION:           _________________________________________\n\n   ORGANIZATION/AFFILIATION: _________________________________________\n\n   ORGANIZATION'S WWW URL:   _________________________________________\n\n   OWN WWW URL:              _________________________________________\n\n   E-MAIL ADDRESS:           _________________________________________\n\n   ADDRESS FOR PAPER MAIL:   _________________________________________\n\n                             _________________________________________\n\n                             _________________________________________\n\n   TELEPHONE:                _________________________________________\n\n   FAX:                      _________________________________________\n\n\nFor e-mail submissions, please submit in ASCII, uncoded, non-\ncompressed text and use the following subject line:\n   Proposal for IUC 11\n\nSubmissions should be sent to the following address by e-mail, post, \nor fax:\n\n   Eleventh International Unicode Conference\n   c/o Global Meeting Services\n   3627 Princess Avenue\n   N.Vancouver, B.C.\n   Canada  V7N 2E4\n\n   Tel:    +1 604 983 9157\n   Fax:    +1 604 983 9158\n\n   E-mail: papers@unicode.org\n       or: info@global-conference.com\n\nEXHIBIT OPPORTUNITIES\n\nThe Conference will have an Exhibition area for corporations or \nindividuals who wish to display and promote their products, technology \nand/or services.\n\nEvery effort will be made to provide maximum exposure and advertising.\n\nExhibit space is limited.  For further information or to reserve a \nplace, please contact Global Meeting Services at the above location.\n\nCONFERENCE VENUE\n\n  Red Lion / Doubletree Hotel San Jose\n  2050 Gateway Place\n  San Jose, California 95110\n  USA \n\nTHE UNICODE STANDARD\n\nThe Unicode Standard is a 16-bit character encoding encompassing the \nworld's principal scripts which provides the foundation for the \ninternationalization and localization of software.  With its simple \nand efficient approach to special and modified characters, the Unicode \nStandard defines a new degree of data portability between platforms \nand across borders.  Software (especially modern, distributed systems) \ncan be adapted for particular countries far more easily than when \nexisting character sets are used.  The Unicode Standard is code-for-\ncode identical to the International Standard ISO/IEC 10646.\n\nFor further information on the Unicode Standard, visit the Unicode Web \nsite at <http://www.unicode.org> or e-mail <unicode-inc@unicode.org>.\n\n\n(R) 'Unicode' is a registered trademark of Unicode, Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Parameters for transfercodings",
            "content": "As our messages fly past each other on the net ...\n\nIn message <9711190116.AA01262@acetes.pa.dec.com>, Jeffrey Mogul writes:\n>I expect to get flamed for this message, but I sort of promised someone\n>that I would make the attempt.\n\nNo flames, but here is some gasoline for ya.\n\n>Some of the potentially-useful transfer-coding algorithms are\n>inherently parameterizable.  For example, \"gzip\" has 9 different\n>compression levels (trading off compression ratio for speed).\n>\"compress\" has a range of possible settings (I'm not quite sure how\n>many).  In both cases, once the sender has chosen a setting, the\n>recipient should be able to decompress the result, but HTTP provides no\n>mechanism for the client to suggest the preferred setting.\n\nYes.  The same thing can be (IMHO) more easily accomplished as\n\n    Connection: deflate\n    Deflate: --best\n\n(or whatever options might be desired).  The sender would still be in\ncontrol, of course, but this is far more informative than a qvalue.\n\nBTW, \"compress\" is a dead encoding -- it has no reliable definition\nother than \"that thing you get when you run compress on Unix.\"\nLikewise, \"deflate\" obsoletes \"gzip\".  In fact, it would probably\nbe sensible to go back and remove \"gzip\" and \"compress\" and simply\nstandardize on \"deflate\", \"x-gzip\", and \"x-compress\" (plus whatever\nnew encodings come out down the road).\n   \nBTW BTW, the reason for this rapid-fire set of responses today is\nthat I am about to go off-line for a while so that I can get some\nreal work done.   Two conference deadlines on Dec 1 and a UCI\nresearch dealine on Dec 12 can't coexist with standards discussions.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "Roy Fielding writes:\n\n    >    Likewise, chunked and identity are always required -- there is no\n    >    reasonable use for refusal based on lack-of-encoding.  Thus, the only\n    >    feature we actually need is the ability to request a given\n    >    transfer-encoding be used.\n    >    \n    >I disagree.  With respect to \"chunked\", we could presumably change\n    >All HTTP/1.1 applications MUST be able to receive and decode\n    >        the \"chunked\" transfer coding,\n    >to\n    >All HTTP/1.1 applications MUST be able to receive and decode\n    >        the \"chunked\" transfer coding, except for clients that\n    >explicitly reject this transfer coding using a qvalue of 0\n    >in an Accept-Transfer request header field.\n    >\n    >I'm not necessarily saying that we should do this, but it seems\n    >safe to do so, and it might pay off for implementors of clients\n    >with limited code space.\n    \n    No, we can't do that.  That has been written in stone (i.e.,\n    deployed code).  It is also not desirable from a robustness\n    perspective -- HTTP needs an indicator of message length in order\n    to differentiate between complete and incomplete responses.  HTTP\n    is unsafe for data transfer without that information, so we don't\n    give the compliant application any choice.\n\nOops, Roy is right about this one.  I was rushing, and forgot\nabout the deployed base of chunk-sending applications.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Parameters for transfercodings",
            "content": "draft-ietf-http-feature-reg-02.txt\n\nis one way to express parameters or configurations of compression\nparameters.\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "How about\n    trans-features:\nwhich are a way of doing hop-to-hop feature negotiation?\n\nLarry\n\n\n\n"
        },
        {
            "subject": "LAST CALL on issues; statu",
            "content": "See http://www.w3.org/Protocols/HTTP/Issues/\n\nWe're trying to pipeline getting the resolution of issues into\nthe draft; Jim Gettys is editing the proposed resolution into\nthe draft even though we're only now doing \"LAST CALL\".\n\nLAST CALL:\n\n        PROTECTION_SPACE\n        CONTRADICTION\n        BYTERANGE_SYNTAX\n        RE-VERSION\n        HOST\n        OPTIONS\n        DIGEST_SYNTAX\n\nThe OPTIONS resolution in the draft will clarify the base\nspecification; however, the issue will remain 'open'.\n\nReclassified from editorial to technical issue:\n        RANGE_WITH_CONTENTCODING\n\nClosed issues:\n        AUTH-CHUNKED\n        REQUIRE-DIGEST\n\n\n\n"
        },
        {
            "subject": "DRAFT agenda for IETF meeting; documenting implementation",
            "content": "Please note that in addition to the HTTP working group meeting,\nthere is a two hour BOF on content negotiation.\n\nCurrently HTTP has only one two-hour session allocated. I have\nasked for a second session. This is intended to be the LAST\nMEETING of the HTTP working group.\n\nAgenda items:\n1. Jim Gettys:   review draft & discuss open issues\n2. Dave Kristol: state management\n3. Scott Lawrence: Options & PEP\n4. Review of other drafts\n\nAvailable time will be split up among the issues.\n\nI would like to arrange a smaller meeting with individuals\nwho are willing to go through the draft and identify whether\ntheir implementation has been tested for interoperability with\nan independent implementation, for each and every feature of\nHTTP/1.1. \n\nThis effort will probably take a couple of hours if we move\nquickly; please let me know if you're available during the\nIETF week, and, if so, when. (A thorough review of the draft\nwhich identifies which features you DO NOT implement or HAVE\nNOT tested against other implementations would be useful preparation.)\n\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Qvalues  a ni",
            "content": "At 16:08 18/11/97 PST, Jeffrey Mogul wrote:\n>Section 3.9 does say\n>\n>            \"Quality values\" is a misnomer, since these values merely\n>            represent relative degradation in desired quality.\n>\n>but it also says\n>\n>    HTTP content negotiation (section 12) uses short \"floating\n>            point\" numbers to indicate the relative importance\n>            (\"weight\") of various negotiable parameters.\n\nTechnically, Qvalues are *fixed* point real numbers.\n\n(Or even just real numbers with limited range and precision.)\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "At 13:34 11/18/97 -0800, Roy T. Fielding wrote:\n\n>>Message headers listed in the Trailer header field MUST NOT include the\n>>Transfer-Encoding and the Trailer header field.\n>\n>and Content-Length.\n\nYes, in the HTTP sense of CL, you are right.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiate-scenario02.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: Scenarios for the Delivery of Negotiated Content\nAuthor(s): E. Hardie\nFilename: draft-ietf-http-negotiate-scenario-02.txt\nPages: 5\nDate: 18-Nov-97\n\nA number of Internet application protocols have a need to provide\ncontent negotiation for the resources with which they interact.  While\nMIME media types [1] provide a standard method for handling one\nmajor axis of variation, resources also vary in ways which cannot be\nexpressed using currently available MIME headers.  This paper asserts\nthat a cross-protocol negotiation framework is needed, both to\nleverage work already done for specific protocols and to allow for\ncontent negotiation when resources will pass through several\nprotocols on their way from provider to recipient.  To justify the\nneed, this paper puts forward several content negotiation scenarios\nand discusses how they affect the requirements for such a framework.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-negotiate-scenario-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiate-scenario-02.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-negotiate-scenario-02.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Inconsistent wording of ContentLengt",
            "content": "Jim,\n\nThe descriptions of Content-Length in rev-00 are slightly out of sync. I\npropose two small changes to 4.4 and 14.14:\n\nSection 4.4\n\n- If a Content-Length header field (section 14.14) is present,\n  its value in bytes represents the length of the message-body.\n\nshould say\n\n- If a Content-Length header field (section 14.14) is present,\n  its decimal value in OCTETs represents the length of the\n  message-body.\n\nLater in 4.4\n\nWhen a Content-Length is given in a message where a message-body\nis allowed, its field value MUST exactly match the number of\nOCTETs in the message-body. HTTP/1.1 user agents MUST notify\nthe user when an invalid length is received and detected.\n\n10.2.7\n\n- Either a Content-Range header field (section 14.17)\nindicating the range included with this response, or a\nmultipart/byteranges Content-Type including Content-Range\nfields for each part. If multipart/byteranges is not used,\nthe Content-Length header field in the response MUST match\nthe actual number of OCTETs transmitted in the message-body.\n\n14.14\n\nThe Content-Length entity-header field indicates the size of\nthe message-body, in decimal number of octets, sent to the\nrecipient or, in the case of the HEAD method, the size of\nthe entity-body that would have been sent had the request\nbeen a GET.\n\nshould say\n\nThe Content-Length entity-header field indicates the size of\nthe message-body, in decimal number of OCTETs, sent to the\nrecipient or, in the case of the HEAD method, the size of\nthe entity-body that would have been sent had the request\nbeen a GET.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "At 16:48 11/18/97 PST, Jeffrey Mogul wrote:\n>Henrik Frystyk Nielsen writes:\n>\n>       Accept-Transfer  = \"Accept-Transfer\" \":\"\n>                           1#( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n>\n>Shouldn't that be:\n>\n>       Accept-Transfer  = \"Accept-Transfer\" \":\"\n>                           #( t-codings [ \";\" \"q\" \"=\" qvalue ] )\n>\n>since one of your examples is:\n>\n>       Accept-Transfer:\n>\n>(I know, I made the same mistake for Accept-Encoding in rev-00.)\n\nduh - yes, you're right\n\n>Also, since (as Roy has pointed out) the requirement for protecting\n>Accept-Transfer with Connection makes requests somewhat verbose,\n>perhaps we should be using a shorter name ... \"Accept-Trans\"\n>would save 6 bytes per request.  \n>\n>I'm not even sure this header should be called \"Accept-anything\",\n>since it's a hop-by-hop mechanism and thus pretty much orthogonal\n>to content negotiation.  Maybe \"OK-Trans\" (saving another 8 bytes\n>per request)?  It's not as if any human being is supposed to be\n>reading these headers.\n\nThe only reason for using an accept-* name is that its BNF falls into that\ncategory of header fields. The same could be said about Accept-Encoding\nalong with a long list of other problems, but that is too late to fix now\n(see [1], for example).\n\nWhat about calling it Accept-TE - that saves 12 bytes.\n\n>P.S.: OK, I *do* include HTTP implementors in the set of human\n>beings :-)\n\nI think that's a different discussion altogether ;-)\n\nHenrik\n\n[1] http://www.w3.org/Protocols/HTTP/Issues/#ENCODING_NOT_CONNEG\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Parameters for transfercodings",
            "content": "At 17:16 11/18/97 PST, Jeffrey Mogul wrote:\n>I expect to get flamed for this message, but I sort of promised someone\n>that I would make the attempt.\n\n...\n\n>Somewhat more exotic, but hardly without practical merit, is the\n>possibility of using pre-agreed compression dictionaries to further\n>reduce the size of the transferred compressed form.  The idea is that\n>it should take fewer bytes to send the name of the appropriate\n>dictionary than to transmit its contents.  However, in this case, the\n>sender needs a way to name the dictionary used in the message.\n\nI wouldn't say that - for small files of often used media types (HTML, XML,\nSGML, for example), it may make a lot of sense to have pregenerated\ndictionaries. Jim and I talked to Mark Adler and Jean-loup Gailly during\nour performance testing but didn't have time to try it out.\n\nIf the dictionary was a resource in itself, it would work fine. Deflate\nuses a Adler32 hash to name the dictionary. Depending on how good the hash\nis (I don't know), it may be used as a location indendent identifier in\nitself or by hashing it again using some stronger hash algorithm.  \n\n>Suppose, therefore, that we change this syntax in section 3.6\n>(Transfer Codings) from\n\nYou have a good point - and we actually already have the BNF for\ngeneralized parameters from the Accept header and the Content-Type header\nfields.\n\nHowever, it introduces the problem of separating extension parameters in\nAccept-TE from extension parameters in transfer-codings just like is\ncurrently the case with Accept (see 14.1):\n\nNote: Use of the \"q\" parameter name to separate media\ntype parameters from Accept extension parameters is\ndue to historical practice.  Although this prevents\nany media type parameter named \"q\" from being used\nwith a media range, such an event is believed to be\nunlikely given the lack of any \"q\" parameters in the\nIANA media type registry and the rare usage of any\nmedia type parameters in Accept. Future media types\nshould be discouraged from registering any parameter\nnamed \"q\".\n\nAre people willing to live with that?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "\"accept-\" request headers are handled specially by the 'vary'\nheader response. Since this doesn't apply end-to-end and might\nbe introduced by an intermediate, I don't think that it should\nbe called \"Accept-\" anything. Roy's suggestion of making this\nan option to 'Connection' might be reasonable, but I'm suspicious\nof adding this last-minute feature into a Draft Standard.\n\nIs there some way that having compression transfer encodings\nas opposed to compression content-encodings could be\nhandled outside of the main HTTP draft, and moved forward\nas a separate Proposed (or even Experimental) standard?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "I don't see the point of the trailer field at all.\nWhy not just say, as suggested:\n\nOnly put fileds in the trailer that can be safely ignored by the\nclient (e.g., Last-Modified).  Content-Length is expressly forbidden in\nthe trailer and if found MUST be ignored.\n\n** Reply to note from Henrik Frystyk Nielsen <frystyk@w3.org> Mon, 17 Nov 1997 18:37:48 -0500\n>   \n>   \n> In response to the TRAILER_FIELDS issue [1], here is a proposal for a new\n> general Trailer header field that indicates which header fields the\n> recipient can expect in the trailer of a message encoded using chunked\n> transfer-encoding. The Trailer header field is modelled after the Vary\n> header field.\n>   \n> This change requires the Accept-Transfer header field defined in an earlier\n> mail (not in the archives at this point)\n>   \n> Henrik\n>   \n> [1] http://www.w3.org/Protocols/HTTP/Issues/#TRAILER_FIELDS\n>   \n> ****************************************************************************\n>   \n> Changes:\n>   \n> Replace section 3.6 with the following:\n>   \n> 3.6 Transfer Codings\n>   \n> Transfer coding values are used to indicate an encoding transformation that\n> has been, can be, or may need to be applied to an entity-body in order to\n> ensure \"safe transport\" through the network. This differs from a content\n> coding in that the transfer coding is a property of the message, not of the\n> original entity. Therefore, transfer codings only apply to the immediate\n> connection.\n>   \n>        transfer-coding         = token\n>   \n> All transfer-coding values are case-insensitive. HTTP/1.1 uses transfer\n> coding values in the Accept-Transfer header field (section Y) and the\n> Transfer-Encoding header field (section 14.40).\n>   \n> Transfer codings are analogous to the Content-Transfer-Encoding values of\n> MIME [7], which were designed to enable safe transport of binary data over\n> a 7-bit transport service. However, safe transport has a different focus\n> for an 8bit-clean transfer protocol. In HTTP, the only unsafe\n> characteristic of message-bodies is the difficulty in determining the exact\n> body length (section 7.2.2), or the desire to encrypt data over a shared\n> transport.\n>   \n> The Internet Assigned Numbers Authority (IANA) acts as a registry for\n> transfer-coding value tokens. Initially, the registry contains the\n> following tokens: \"chunked\" (section 3.6.1) and \"identity\" (section 3.6.2).\n>   \n> The registration of new transfer-codings are done as for content-codings.\n>   \n> A server which receives an entity-body with a transfer-coding it does not\n> understand SHOULD return 501 (Unimplemented), and close the connection. A\n> server MUST NOT send transfer-codings to an HTTP/1.0 client.\n>   \n> 3.6.1 Chunked Transfer Coding\n>   \n> The chunked encoding modifies the body of a message in order to transfer it\n> as a series of chunks, each with its own size indicator, followed by an\n> optional trailer containing entity-header fields. This allows\n> dynamically-produced content to be transferred along with the information\n> necessary for the recipient to verify that it has received the full message.\n>   \n>        Chunked-Body   = *chunk\n>                         last-chunk\n>                         trailer\n>                         CRLF\n>        chunk          = chunk-size [ chunk-extension ] CRLF\n>                         chunk-data CRLF\n>        chunk-size     = 1*HEX \n>        last-chunk     = 1*(\"0\") [ chunk-extension ] CRLF\n>   \n>        chunk-extension= *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] ) \n>        chunk-ext-name = token\n>        chunk-ext-val  = token | quoted-string\n>        chunk-data     = chunk-size(OCTET)\n>        trailer        = *entity-header\n>   \n> The chunk-size field is a string of hex digits indicating the size of the\n> chunk. The chunked encoding is ended by any chunk whose size is zero,\n> followed by the trailer, which is terminated by an empty line.\n>   \n> The trailer allows the sender to include additional HTTP header fields at\n> the end of the message. The Trailer header field can be used to indicate\n> which header fields are included in a trailer (see section X).\n>   \n> A server using chunked transfer-coding in a response MUST NOT use the\n> trailer for other header fields than Content-MD5 and Authentication-Info\n> unless the \"chunked\" transfer-coding is present in the request as an\n> accepted transfer-coding in the Accept-Transfer field.\n>   \n> An example process for decoding a Chunked-Body is presented in appendix\n> 19.4.6.\n>   \n> All HTTP/1.1 applications MUST be able to receive and decode the \"chunked\"\n> transfer coding, and MUST ignore chunk-extension extensions they do not\n> understand.\n>   \n> 3.6.2 Identity Transfer Coding\n>   \n> The identity transfer-encoding is the default (identity) encoding; the use\n> of no transformation whatsoever.  This transfer-coding is used only in the\n> Accept-Transfer header, and SHOULD NOT be used in any Transfer-Encoding\n> header.\n>   \n> ****************************************************************************\n>   \n> Also add the Description of Trailer Header Field as a new header:\n>   \n> Trailer\n>   \n> The Trailer general field value indicates that the given set of header\n> fields are present in the trailer of a message encoded with chunked\n> transfer-coding.\n>   \n>        Trailer  = \"Trailer\" \":\" 1#field-name\n>   \n> An HTTP/1.1 sender MAY include a Trailer header field in a message using\n> chunked transfer-coding with a non-empty trailer. Doing so allows the\n> recipient to know which header fields to expect in the trailer.\n>   \n> If no Trailer header field is present, the trailer SHOULD NOT include any\n> other header fields than Content-MD5 and Authentication-Info.\n>   \n> A server MUST NOT include any other header fields unless the \"chunked\"\n> transfer-coding is present in the request as an accepted transfer-coding in\n> the Accept-Transfer field.\n>   \n> Message headers listed in the Trailer header field MUST NOT include the\n> Transfer-Encoding and the Trailer header field.\n>   \n> ****************************************************************************\n>   \n> Henrik\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\ncc: Henrik Frystyk Nielsen <frystyk@w3.org>\n\n\n\n"
        },
        {
            "subject": "Issue: TRAILER_FIELDS and Transfer Encodings once agai",
            "content": "This is a write-up of the proposed changes to rev-00 regarding\nAccept-transfer (now Accept-TE) and Transfer-Encoding. It takes into\naccount the following\n\n- introduces transfer-encoding parameters\n- prevents chunked from having any parameters in Transfer-Encoding\n- prevents chunked from having q=0 in Accept-TE\n- prevents CL from being in a trailer\n\nLarry, I know this is kind of late, but the trailer field issue and\ntransfer encodings are relatively new issues. However, we are using a lot\nof existing mechanisms already defined in HTTP/1.1 and hence the new stuff\nboils down to the header names.\n\nComments?\n\nHenrik\n\n****\n\n3.6 Transfer Codings\n\nTransfer coding values are used to indicate an encoding transformation that\nhas been, can be, or may need to be applied to an entity-body in order to\nensure \"safe transport\" through the network. This differs from a content\ncoding in that the transfer coding is a property of the message, not of the\noriginal entity. Therefore, transfer codings only apply to the immediate\nconnection.\n\n       transfer-coding         = \"chunked\" | transfer-extension\n       transfer-extension      = token *( \";\" parameter )\n\nParameters may be in the form of attribute/value pairs.\n\n       parameter               = attribute \"=\" value\n       attribute               = token\n       value                   = token | quoted-string\n\n[[Jim, parameter is already defined in 3.7. As it should only be in one\nplace, it can either be here or in section 3.7. In the latter case, there\nshould be a forward reference here]]\n\nAll transfer-coding values are case-insensitive. HTTP/1.1 uses transfer\ncoding values in the Accept-TE header field (section 14.Y) and the\nTransfer-Encoding header field (section 14.40).\n\nTransfer codings are analogous to the Content-Transfer-Encoding values of\nMIME [7], which were designed to enable safe transport of binary data over\na 7-bit transport service. However, safe transport has a different focus\nfor an 8bit-clean transfer protocol. In HTTP, the only unsafe\ncharacteristic of message-bodies is the difficulty in determining the exact\nbody length (section 7.2.2), or the desire to encrypt data over a shared\ntransport.\n\nThe Internet Assigned Numbers Authority (IANA) acts as a registry for\ntransfer-coding value tokens. Initially, the registry contains the\nfollowing tokens: \"chunked\" (section 3.6.1), \"identity\" (section 3.6.2),\n\"gzip\" (section 3.5), \"compress\" (section 3.5), and \"deflate\" (section 3.5).\n\nNew transfer-coding value tokens should be registered in the same way as\nnew content-coding value tokens (section 3.5).\n\nA server which receives an entity-body with a transfer-coding it does not\nunderstand SHOULD return 501 (Unimplemented), and close the connection. A\nserver MUST NOT send transfer-codings to an HTTP/1.0 client.\n\n3.6.1 Chunked Transfer Coding\n\nThe chunked encoding modifies the body of a message in order to transfer it\nas a series of chunks, each with its own size indicator, followed by an\noptional trailer containing entity-header fields. This allows\ndynamically-produced content to be transferred along with the information\nnecessary for the recipient to verify that it has received the full message.\n\n       Chunked-Body   = *chunk\n                        last-chunk\n                        trailer\n                        CRLF\n       chunk          = chunk-size [ chunk-extension ] CRLF\n                        chunk-data CRLF\n       chunk-size     = 1*HEX \n       last-chunk     = 1*(\"0\") [ chunk-extension ] CRLF\n\n       chunk-extension= *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] ) \n       chunk-ext-name = token\n       chunk-ext-value= token | quoted-string\n       chunk-data     = chunk-size(OCTET)\n       trailer        = *entity-header\n\nThe chunk-size field is a string of hex digits indicating the size of the\nchunk. The chunked encoding is ended by any chunk whose size is zero,\nfollowed by the trailer, which is terminated by an empty line.\n\nThe trailer allows the sender to include additional HTTP header fields at\nthe end of the message. The Trailer header field can be used to indicate\nwhich header fields are included in a trailer (see section 14.Y).\n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the Accept-TE field (section 14.X).\n\nAn example process for decoding a Chunked-Body is presented in appendix\n19.4.6.\n\nAll HTTP/1.1 applications MUST be able to receive and decode the \"chunked\"\ntransfer coding, and MUST ignore chunk-extension extensions they do not\nunderstand.\n\n3.6.2 Identity Transfer Coding\n\nThe identity transfer-encoding is the default (identity) encoding; the use\nof no transformation whatsoever. This transfer-coding is used only in the\nAccept-TE header, and SHOULD NOT be used in any Transfer-Encoding header.\n\n14.X Accept-TE\n\nThe Accept-TE request-header field is similar to Accept-Encoding, but\nrestricts the transfer-codings (section 3.6) that are acceptable in the\nresponse.\n\n       Accept-TE        = \"Accept-TE\" \":\"\n                          #( t-codings )\nt-codings        = \"chunked\" | ( transfer-extension [ accept-params ] )\n\nExamples of its use are:\n\n       Accept-TE: deflate\n       Accept-TE:\n       Accept-TE: chunk=1.0; deflate=0.5\n\nThe Accept-TE header field only applies to the immediate connection.\nTherefore, the accept-te keyword MUST be supplied within a Connection\nheader field (section 14.10) whenever Accept-TE is present in an HTTP/1.1\nmessage.\n\nA server tests whether a transfer-coding is acceptable, according to an\nAccept-TE field, using these rules:\n\n1. If the transfer-coding is one of the transfer-codings listed in the\nAccept-TE field, then it is acceptable, unless it is accompanied by a\nqvalue of 0.  (As defined in section 3.9, a qvalue of 0 means \"not\nacceptable.\")\n\n2. If multiple transfer-codings are acceptable, then the acceptable\ntransfer-coding with the highest non-zero qvalue is preferred.\n\n3. The \"identity\" transfer-coding is always acceptable, unless specifically\nrefused because the Accept-TE field includes \"identity;q=0\", or because the\nfield includes \"*;q=0\" and does not explictly include the \"identity\"\ntransfer-coding.  If the Accept-TE field-value is empty, then only the\n\"identity\" encoding is acceptable.\n\n4. The \"chunked\" transfer-coding is always acceptable. The Trailer header\nfield (section 14.Y) can be used to indicate the set of header fields\nincluded in the trailer.\n\nIf an Accept-TE field is present in a request, and if a server cannot send\na response which is acceptable according to the Accept-TE header, then the\nserver SHOULD send an error response with the 406 (Not Acceptable) status\ncode.\n\nIf no Accept-TE field is present, the sender MAY assume that the recipient\nwill accept the \"identity\" and \"chunked\" transfer-codings.\n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the Accept-TE field.\n\n14.Y Trailer\n\nThe Trailer general field value indicates that the given set of header\nfields are present in the trailer of a message encoded with chunked\ntransfer-coding.\n\n       Trailer  = \"Trailer\" \":\" 1#field-name\n\nAn HTTP/1.1 sender MAY include a Trailer header field in a message using\nchunked transfer-coding with a non-empty trailer. Doing so allows the\nrecipient to know which header fields to expect in the trailer.\n\nIf no Trailer header field is present, the trailer SHOULD NOT include any\nother header fields than Content-MD5 and Authentication-Info.\n\nA server MUST NOT include any other header fields unless the \"chunked\"\ntransfer-coding is present in the request as an accepted transfer-coding in\nthe Accept-TE field.\n\nMessage headers listed in the Trailer header field MUST NOT include the\nfollowing header fields:\n\nTransfer-Encoding\nContent-Length\nTrailer\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues: TRAILER_FIELDS",
            "content": "At 17:03 11/18/97 -0800, Roy T. Fielding wrote:\n\n>In other words, I agree that there are tradeoffs, but the client has\n>no input in deciding those tradeoffs (nor should it).  That doesn't\n>mean the server will necessarily ignore the client's needs -- it just\n>means the server will decide what is needed based on its own observation\n>of the connection throughput, typical client capabilities, and the\n>current resource constraint/load on the server itself.  A simple\n>ordering or relative quality value given by the client says nothing\n>of importance to the server, and thus provides no useful input to\n>its decision.\n\nThe parameters may in fact cause the client not to be able to interpret the\nresponse, for example in the case of using an external dictionary, or a\ncertain key length etc.\n\nI think that if (not the \"if\") we allow parametes in the Transfer-Encoding\nheader field then we also need it in the accept-TE header field to allow\nthe client to say no thanks.\n\nWe could also say that this is for the encoding to figure out, for example\nby describing the parameters in the head of the stream. Deflate already\ndoes this to a certain extend but other filters may not.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Process for closing out issues lis",
            "content": "I will try to get an updated issues list done before the end of the\nweek, but an unanticipated April Fools joke has slowed me down.\n\nNew England just had a blizzard, which has cost me a couple days of time\nthis week (biggest storm since the blizzard of 1978).  It was quite\nspectacular (I measured 26\" of snow on the deck behind my house; the\nwhole area got between 24\" and 33\" of snow.\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "At 14:02 11/19/97 EST, rlgray@raleigh.ibm.com wrote:\n>I don't see the point of the trailer field at all.\n>Why not just say, as suggested:\n\nThe purpose of the Trailer header field is to provide a general mechanism\ndescribing where headers are in a message, so that it doesn't have to be on\na case by case basis (see also description of problem [1]).\n\nThe notion \"Only put fileds in the trailer that can be safely ignored by the\nclient (e.g., Last-Modified)\" doesn't really say much as it depends on who\nyou are (as a client) and what you are doing.\n\nThe combination of Accept-TE and Trailer allows the server to put headers\nin the trailer and the client to say no thanks.\n\nHenrik\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0659.html\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "Larry Masinter:\n>\n>\"accept-\" request headers are handled specially by the 'vary'\n>header response. \n\n??  In ther last version I saw, Vary did not treat any field as\nspecial.\n\n>Since this doesn't apply end-to-end and might\n>be introduced by an intermediate, I don't think that it should\n>be called \"Accept-\" anything. Roy's suggestion of making this\n>an option to 'Connection' might be reasonable, but I'm suspicious\n>of adding this last-minute feature into a Draft Standard.\n\nI feel that making a clean separation between content and transfer\ncompression is a cleanup we should have made a long time ago, and on\nthat grounds I think it is reasonable to add it as a last-minute\nfeature.  \n\nIt is not really critical whether it is called Accept- or connection:\nsomething.  I like the connection alternative slightly better.  And no\ngunky parameters about compression quality or dictionaries please --\nadding these would take the whole thing way beyond a 1.1 cleanup, and\nhow are we ever going to claim two independent implementations for\nsuch things if we add them?\n\n>Larry\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Issue: TRAILER_FIELDS and Transfer Encodings once agai",
            "content": "Henrik Frystyk Nielsen wrote:\n> [...]\n> 3. The \"identity\" transfer-coding is always acceptable, unless specifically\n> refused because the Accept-TE field includes \"identity;q=0\", or because the\n> field includes \"*;q=0\" and does not explictly include the \"identity\"\n> transfer-coding.  If the Accept-TE field-value is empty, then only the\n> \"identity\" encoding is acceptable.\n> \n> 4. The \"chunked\" transfer-coding is always acceptable. The Trailer header\n> field (section 14.Y) can be used to indicate the set of header fields\n> included in the trailer.\n> \n> If an Accept-TE field is present in a request, and if a server cannot send\n> a response which is acceptable according to the Accept-TE header, then the\n> server SHOULD send an error response with the 406 (Not Acceptable) status\n> code.\n> \n> If no Accept-TE field is present, the sender MAY assume that the recipient\n> will accept the \"identity\" and \"chunked\" transfer-codings.\n> [...]\n\nSorry, but I find these paragraphs very confusing and, possibly,\ncontradictory.  According to (3), an empty Accept-TE allows only\n\"identity\".  According to (4), \"chunked\" is always acceptable.  Which is\nright?\n\nAlso, in these (earlier) examples of Accept-TE:\n       Accept-TE: deflate\n       Accept-TE:\n       Accept-TE: chunk=1.0; deflate=0.5\n\nIs \"chunk\" a hypothetical new TE, or is it a misspelling of \"chunked\",\nin which case it's invalid (because \"chunked\" can't take a parameter)? \nAnd, syntactically don't those have to be\n       Accept-TE: chunk;q=1.0; deflate;q=0.5\n?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "At 22:10 11/19/97 +0100, Koen Holtman wrote:\n\n>It is not really critical whether it is called Accept- or connection:\n>something.  I like the connection alternative slightly better.  And no\n>gunky parameters about compression quality or dictionaries please --\n>adding these would take the whole thing way beyond a 1.1 cleanup, and\n>how are we ever going to claim two independent implementations for\n>such things if we add them?\n\nI believe consistent header formats with support for generalized parameters\nis another cleanup that is worth while judging on the amount of mails on\nthis list regarding old parsers that would break if new header values were\nintroduced.\n\nThis is definitely in the \"one-hour hacking\" range :)\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Issue: TRAILER_FIELDS and Transfer Encodings once agai",
            "content": "At 16:15 11/19/97 -0500, Dave Kristol wrote:\n\nHi Dave,\n\n>Sorry, but I find these paragraphs very confusing and, possibly,\n>contradictory.  According to (3), an empty Accept-TE allows only\n>\"identity\".  According to (4), \"chunked\" is always acceptable.  Which is\n>right?\n\nThey are indeed - must have been eating something bad. I think this is better:\n\n3. The \"identity\" transfer-coding is always acceptable, unless specifically\nrefused because the Accept-TE field includes \"identity;q=0\". The \"chunked\"\ntransfer-coding is always acceptable. The Trailer header field (section\n14.Y) can be used to indicate the set of header fields included in the\ntrailer.\n\n4. If the Accept-TE field-value is empty, only the \"identity\" and the\n\"chunked\" transfer-codings are acceptable.\n\n>Also, in these (earlier) examples of Accept-TE:\n>       Accept-TE: deflate\n>       Accept-TE:\n>       Accept-TE: chunk=1.0; deflate=0.5\n>\n>Is \"chunk\" a hypothetical new TE, or is it a misspelling of \"chunked\",\n>in which case it's invalid (because \"chunked\" can't take a parameter)? \n>And, syntactically don't those have to be\n>       Accept-TE: chunk;q=1.0; deflate;q=0.5\n\nSorry, it should have been\n\n        Accept-TE: chunked; deflate;q=0.5\n\nmeaning that the client accepts any allowed header to occur in the trailer\nof a chunked encoded message and is somewhat happy to accept deflate.\n\nThanks!\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Issue: TRAILER_FIELDS and Transfer Encodings once agai",
            "content": "To make Draft Standard, we have to document at least two interoperable\nimplementations of each feature. So where are the interoperable\nimplementations of trailer fields and transfer encoding?\n\nIf it's unlikely that we will see interoperable implementations\nof trailer fields in the next month (which seems pretty unlikely\nto me), maybe we can undock 'Trailer Fields'.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "Roy (humbly) proposed:\n\n    The same thing can be (IMHO) more easily accomplished as\n\n    Connection: deflate\n    Deflate: --best\n\nThis has some things to recommend it, but unfortunately it\nwon't quite work.  The most significant problem is that it\ndoesn't work with multiple transfer-codings on the same message.\n\nI.e., the rules for Transfer-Encoding (as for Content-Encoding)\nwith multiple codings are quite clear:\n\n    If multiple encodings have been applied to an entity, the\n            transfer codings MUST be listed in the order in which they\n            were applied.\n\nE.g.,\n\n   Transfer-Encoding: gzip, twiddle\n\nunambigously means that \nmessage-body = twiddle(gzip(entity-body))\n\nand section 4.2 (Message Headers) guarantees that the ordering\nof transfer-codings in this header field is reliable.\n\nHowever, you can't make the same guarantee if each coding\nis described by a separate header, since section 4.2 (Message\nHeaders) says:\n\n    The order in which header fields with differing field names\n            are received is not significant.\n\nWe could fiddle with 4.2, or with the definition of Connection,\nor add some new optional mechanism for imposing an ordering on\nheader fields with differing field names.  But these all seem a\nlot more disruptive (especially for the installed base) than\nsimply doing something analogous to Accept-Encoding/Content-Coding.\n\nKoen writes:\n    And no gunky parameters about compression quality or dictionaries\n    please -- adding these would take the whole thing way beyond a 1.1\n    cleanup, and how are we ever going to claim two independent\n    implementations for such things if we add them?\n\nWell, Henrik has already promised one (right, Henrik?) and I would\nguess that he is right; adding a parameter parser for Accept-TE\nis relatively easy if you've already had to right the same parser\nfor \"chunked\".\n\nBut beyond that, if we don't add parameters NOW, we will never\nbe able to do so, because then there will be an incompatibility\nwith the installed base.  History has shown us that HTTP has\nsuffered in many ways from a lack of extensibility mechanisms,\nand since this one is exactly the same as we already have used\nin other headers, it seems rational to use it for transfer-codings.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Issue: TRAILER_FIELDS and Transfer Encodings once agai",
            "content": "Transfer encoding is one of the \"big wins\" possible in HTTP/1.1; our\ntests showed substantial improvements both in elapsed time for end users,\nand in bandwidth on the net.\n\nSo lets get closure on the technical solution right now,\nindependently of the implementation question for draft standard.\n(Though I would hope we could keep the idea of undocking it in mind).\n\nGetting two interoperable implementations isn't that hard a test to meet,\neven in a month (where did you get that date from, anyway?, though I'm\neven more anxious to be done than you are).  We could even do that here\nat W3C if we had to do so to meet the implementation requirements\n(used to be a cheat when Anselm was sharing an office with Henrik, but\nit wouldn't be now).\n\nThis isn't the only place where we have implementation issues to\ndeal with (e.g. 305/306, which needs help, and for which I know of no\nimplementations).  Ari promised to pick it up from Josh and try to\nget something done this week on that one (which we may also want/need to\nundock).\n\nSo lets keep focussed on solving the technical problems; there are\nVERY few of them left (my last count was 7 outstanding technical issues,\nmost of which are well on their way to closure, I believe).\n- Jim\n\n\n\n"
        },
        {
            "subject": "Proposed amendment to RFC210",
            "content": "As promised, here is a description of our proposal to amend RFC2109\nto allow certification of unverifiable transactions.\n\nI have two questions in particular regarding this proposal:\n\n1)  Should \"certified\" cookies be a third! type of cookie header\n[this is what we have written in the proposal] or can we\nincorporate the certificate as an option in the SetCookie2 header?\n\n2) Can we include this in the next version of RFC2109 or must it be a\nseparate RFC?\n>)>)>)>)>)---------------------------------------->>>>>>>\nDaniel Jaye                         djaye@engagetech.com    \nEngage Technologies, Inc.                (508)684-3641 v\n100 Brickstone Square, Andover MA 01810  (508)684-3636 f\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "Being a server implementer, not a client implementer, I fail to see how\nthe Trailer proposal solves the case-by-case problem.\nIt does, in fact, list (currently 3) fields that MUST NOT occur in the\ntrailer.  This seems to me to be unnecessary.\nPerhaps you could provide an example of what could break by a client\nexpecting a header field in the header, not finding it there and then\nfinding it later in the trailer without a forward declaration?\n\nAnd while I am at it, if you are going to abbreviate\n\"Accept-Transfer-Encoding\" as \"Accept-TE\", why not shorten it to\n\"A-TE\"?  I seem to remember this comes up from time-to time, but the\nfact is, HTTP _is_ a human-readable protocol.  I see only two headers\nin 2068 that contain abbreviations, Content-MD5 and WWW-Authenticate.\n\nI tend to think that since there is some symmetry between the \"accept-\"\nheader fields and the \"content-\" header fields, and that all of the\nAccept- header fields refer to the entity desired, not the format of\nthe message, another header name should be used; perhaps using the\nExpect header field with \"transfer-encoding-deflate\" or just \"deflate\"\nwould be acceptable (err, pun intended ;-)\n\nOr, am I just particularly dense today?\n\n** Reply to note from Henrik Frystyk Nielsen <frystyk@w3.org> Wed, 19 Nov 1997 14:48:27 -0500\n>   \n> At 14:02 11/19/97 EST, rlgray@raleigh.ibm.com wrote:\n> >I don't see the point of the trailer field at all.\n> >Why not just say, as suggested:\n>   \n> The purpose of the Trailer header field is to provide a general mechanism\n> describing where headers are in a message, so that it doesn't have to be on\n> a case by case basis (see also description of problem [1]).\n>   \n> The notion \"Only put fileds in the trailer that can be safely ignored by the\n> client (e.g., Last-Modified)\" doesn't really say much as it depends on who\n> you are (as a client) and what you are doing.\n>   \n> The combination of Accept-TE and Trailer allows the server to put headers\n> in the trailer and the client to say no thanks.\n>   \n> Henrik\n>   \n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0659.html\n> --\n> Henrik Frystyk Nielsen,\n> World Wide Web Consortium\n> http://www.w3.org/People/Frystyk\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\ncc: Henrik Frystyk Nielsen <frystyk@w3.org>\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "Koen writes:\n\n    Let me clarify my position: I think it would be OK to add an\n    extensibility mechanism of the usual ;name=value type, but we should\n    not define lots of complicated name=value pairs right now.\n    \nThen we are in perfect agreement.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "Jeffrey Mogul:\n>\n[...]\n>Koen writes:\n>    And no gunky parameters about compression quality or dictionaries\n>    please -- adding these would take the whole thing way beyond a 1.1\n>    cleanup, and how are we ever going to claim two independent\n>    implementations for such things if we add them?\n>\n>Well, Henrik has already promised one (right, Henrik?) and I would\n>guess that he is right; adding a parameter parser for Accept-TE\n>is relatively easy if you've already had to right the same parser\n>for \"chunked\".\n>\n>But beyond that, if we don't add parameters NOW, we will never\n>be able to do so, because then there will be an incompatibility\n>with the installed base.\n\nLet me clarify my position: I think it would be OK to add an\nextensibility mechanism of the usual ;name=value type, but we should\nnot define lots of complicated name=value pairs right now.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": ">>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> Koen writes:\nJM>     And no gunky parameters about compression quality or dictionaries\nJM>     please -- adding these would take the whole thing way beyond a 1.1\nJM>     cleanup, and how are we ever going to claim two independent\nJM>     implementations for such things if we add them?\n\nJM> [...]\n\nJM> But beyond that, if we don't add parameters NOW, we will never\nJM> be able to do so, because then there will be an incompatibility\nJM> with the installed base.  History has shown us that HTTP has\nJM> suffered in many ways from a lack of extensibility mechanisms,\nJM> and since this one is exactly the same as we already have used\nJM> in other headers, it seems rational to use it for transfer-codings.\n\n  I buy the argument that allowing for extentions in the syntax is a\n  good thing even (perhaps especially) if we don't use them now, so\n  put it in.\n\n  I also feel strongly that the transfer coding is the servers\n  business and 'q' values will, in practice, be ignored.  If this\n  argues for renaming the header to not be Accept-*, then rename it.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: spec-08 selfcontradiction in 13.5.",
            "content": "I propose to just remove Content-Length from the list of headers affected by\n\"no-transform\".\n\nThis is OK -- the section already says that it may not be modified, and it\nit gets added, it has to reflect the true length. This meets the\nrequirements of \"no-transform\", because the fidelity of the entity-body will\nnot be affected.\n\n\n> ----------\n> From: koen@win.tue.nl[SMTP:koen@win.tue.nl]\n> Sent: Tuesday, October 28, 1997 12:09 PM\n> To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Cc: koen@win.tue.nl\n> Subject: spec-08 self-contradiction in 13.5.2\n> \n> \n> Section 13.5.2 of draft-ietf-http-v11-spec-08 contains a minor\n> self-contradiction.  It first says that `A cache or non-caching proxy\n> MUST NOT modify [...] Content-Length', and later says that `A cache or\n> non-caching proxy MAY modify or add [Content-Length] in a response\n> that does not include no-transform, [...]'.\n> \n> The problem is that the MUST NOT above is too strong.  Here is a\n> rewrite of the second half of section 13.5.2 which solves the problem.\n> \n>          [...]\n>             A cache or non-caching proxy MUST NOT modify the\n>             following field in a response:\n> \n>                . Expires\n> \n>             but it may add this field if not already present.  If an\n>             Expires header is added, it MUST be given a field-value\n>             identical to that of the Date header in that response.\n> \n>             A cache or non-caching proxy MUST NOT modify the folling\n>             field in a response that contains the no-transform\n>             Cache-Control directive, or in any request:\n> \n>                . Content-Length\n> \n>             A cache or non-caching proxy MAY modify or this field in a\n>             response that does not include no-transform, but if it\n>             does so, it MUST add a Warning 14 (Transformation applied)\n>             if one does not already appear in the response.  A cache\n>             MAY add this header if not already present.  If a\n>             Content-Length header is added or modified, it MUST\n>             correctly reflect the length of the entity-body.\n> \n>               Note: a typical reason for adding the Content-Length\n>               header is that the origin server sent the content\n>               chunked encoded.\n> \n>             A cache or non-caching proxy MUST NOT modify or add any of\n>             the following fields in a response that contains the no-\n>             transform Cache-Control directive, or in any request:\n> \n>                . Content-Encoding\n>                . Content-Range\n>                . Content-Type\n> \n>             A cache or non-caching proxy MAY modify or add these fields\n>          [...]\n> \n> \n> Koen.\n> \n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": ">    Connection: deflate\n>    Deflate: --best\n>\n>This has some things to recommend it, but unfortunately it\n>won't quite work.  The most significant problem is that it\n>doesn't work with multiple transfer-codings on the same message.\n\nI was talking about the other side of the information exchange, i.e.,\nwhat the sender would like to see in the response, not what has\nbeen applied to the response.  The former is not subject to hierarchy.\n\nMy impression was that all transfer encodings would be self-descriptive\n(i.e., that any applied parameters that might be needed by the decoder\nwould be contained at the beginning of the coded body).  Granted, you\ncould easily define an encoding for which that is not the case, but\nI see no reason why we should.  Even so, you would only need ordered\nparameters in the Transfer-Encoding syntax, whereas my proposal is a\nreplacement for only the Accept-TE stuff that Henrik proposed.\n\nBTW, I *really* *really* dislike Accept-TE, both in terms of syntax\nand in the mistake of associating it with end-to-end content negotiation.\nIt is possible to make this feature so complex that nobody will implement\nit at all, regardless of the potential benefits of compressed transfers.\nIt is also possible to introduce so many optional parameterizations of\nthe \"requesting encoding\" side of the equation that the actual\nimplementation becomes less optimal (i.e., reusable) than no encoding.\nIn this case, keep it simple really means keep it simple or not at all.\n\nI also agree with Larry in that none of these proposals belong in rev-01.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "This memo was discussed at previous working group meetings. The\nissue that WebDAV and RSVP have assigned status codes for HTTP\nand others want to has arisen again. Unless there are any\nobjections, we will propose that this draft be moved forward\nto Proposed Standard.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "Larry Masinter:\n>\n>This memo was discussed at previous working group meetings. The\n>issue that WebDAV and RSVP have assigned status codes for HTTP\n>and others want to has arisen again. Unless there are any\n>objections, we will propose that this draft be moved forward\n>to Proposed Standard.\n\nI object.  The draft defines a 3-digit status code space.  There is a\nsignificant risk that a 3-digit status code space will be too small to\ncater for all future activities by IETF groups and individuals, which\nwould leave IANA in the difficult and unpopular position of having to\nmanage the allocation of a scarce resource.\n\nThe draft should allow for status codes of more than 3 digits, and\nshould allow IANA to allocate large blocks of status codes (say blocks\nlike 456000 - 456999) to individuals.  Failing that, it should at\nleast give sound reasons why a restriction to 3 digit codes is\nabsolutely necessary.\n\n>Larry\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "> I don't see the point of the trailer field at all.\n> Why not just say, as suggested:\n> \n> Only put fileds in the trailer that can be safely ignored by the\n> client (e.g., Last-Modified).  Content-Length is expressly forbidden in\n> the trailer and if found MUST be ignored.\n\nAs a client implementor I strongly support the proposed Trailer field.\nThe reason is that I'm unwilling to calculate an MD5 hash on every\nresponse stream just so that if the server happens to send a Content-MD5\ntrailer I can check it (i.e. I'm in a situation where I don't want to\nbuffer the complete response). With the forward declaration of the\nContent-MD5 field I can do the calculation only if there will be a\nContent-MD5 field in the trailer. In that respect, I'd even prefer\nTrailer\nfield to be *required* (or at least a SHOULD) if a Content-MD5 field\nwill\nbe sent in the trailer.\n\nIs there any reason we can't change\n\n  An HTTP/1.1 sender MAY include a Trailer header field in a message\nusing\n  chunked transfer-coding with a non-empty trailer. Doing so allows the\n  recipient to know which header fields to expect in the trailer.\n\nto\n\n  An HTTP/1.1 sender SHOULD include a Trailer header field in a message\nusing\n  chunked transfer-coding with a non-empty trailer. Doing so allows the\n  recipient to know which header fields to expect in the trailer.\n\n? Is this because of already installed implementations?\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Access control and knowledg",
            "content": "Web caches and indexing robots are examples of user agents who do not act\non behalf of one end user.  The problem of access control when sharing\nindexes or caches is not trivial for documents who have access control\nbased on IP number or domain name, since there is no indication of\naccess control being used for the particular document.  \n\nSeveral popular web servers permit users to create their own access control,\nlike Apache does with local .htaccess files, and the local webmaster\nmay not know about access restrictions.  This excludes the use of robots.txt\nfile for sharing this information with indexing robots, and the caches\nwould not be helped.\n\nCache-control: private has been considered, but it does not\npermit sharing information with those in the same realm.\n\nIf an extra header indicating that access control was used, is sent \nwith the document this problem would be solved.  \nWith information of the access list, caches and indexes may still \nbe shared and give access to the appropriate information\nwithout compromising security.  Two access restrictions that easily\nlend themselves to this are IP numbers and domain names.\n\nProposed new header\n Restricted: ACL\n\nIf the definition of WWW-Authenticate is reused from HTTP/1.1, \nthe two special cases would be\n  Restricted: IPnr realm=\"129.215.0.0/255.255.0.0\"\n  Restricted: Domain realm=\".dcs.ed.ac.uk\"\n\nThis header does not ensure the security of a document, but gives multi-user agents an opportunity to restrict access.  If an unknown realm is encountered, the indexing robot or cache should treat the document as restricted and not share information.\n\n\nIngrid\n (who would rather have sent this message to the HTTP-extentions group or the web cache protocols group or the HTTPSEC group or the shared indexing group)\n-- \n  Ingrid.Melve@uninett.no         UNINETT, N-7034 Trondheim, Norway\n   Phone +47 73 55 79 07   Fax +47 73 55 79 01   \n                 http://domen.uninett.no/~im/eng.html\n \"Sometimes it is better to light a flamethrower than curse the darkness\"\n\n\n\n"
        },
        {
            "subject": "State of draft Rev 01 (and digest rev 00",
            "content": "I will start production of these drafts this evening for submission tomorrow.\n\nAnything I receive after 7:00 PM EST is very unlikely to make the new \ndraft(s).  I don't know if I will have time to deal with what is on the \nlist today, either (we'll see when the time comes), as I'll be at a meeting \nmuch of the day.  We don't have many open issues left, as you'll see below.\n\nThe current state of what I have edited can be divined from\nthe issues list, as always: http://www.w3.org/Protocols/HTTP/Issues/\n\nThe following is a summary of the current state of the drafts and\nof the open issues:\n\nSummary Of Issues (Rev-01 and Digest-00 under preparation)\n\n7 Open Technical issues: RANGE_WITH_CONTENTCODING, TRAILER_FIELDS,\nCONTENT-ENCODING, PUT-RANGE, RE-AUTHENTICATION-REQUESTED, PROXY-REDIRECT \nMinimal OPTIONS is in Rev-01, PROXY-REDIRECT interacts with OPTIONS. \n\n5 Open Editorial issues: XREFS,  REQUEST_TARGET, UPDATE_ACKS, DISPOSITION,\nREQUIREMENTS \n\n12 Last call issues (all edited into Rev-01): DIGEST_SYNTAX, PROTECTION_SPACE, \nCONTRADICTION, IMS_INM_MISMATCH, CONNECTION_METHOD, BYTERANGE_SYNTAX,  \nRE-VERSION,  HOST, AUTH-CHUNKED,  REQUIRE-DIGEST,    DOCKDIGEST,  \nGENERIC_MESSAGE \n\n9 Technical Issues closed since Rev-00 (all edited into Rev-01): 301-302, \nREDIRECTS, CACHING-CGI, WARNINGS, DATE-IF-MODIFIED, 403VS404, AGE-CALCULATION \n(Yeah!), VARY, RANGE-ERROR \n\n13 Editorial issues closed since Rev-00 (all edited into Rev-01) REMOVE_19.6, \nGENERIC_MESSAGE, RANGE_OPTIONAL, EBNF, CODES, UTF-8, DOCKDIGEST, URL-SYNTAX, \n1310_CACHE, MESSAGE-BODY, LENGTH_WORDING, CODE_REGISTRY, CLEANUP, \n\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "FW: Proposed amendment to RFC210",
            "content": "Here is the proposal:\n\n \n\n>----------\n>From: Jaye, Dan\n>Sent: Thursday, April 03, 1997 9:18 PM\n>To: 'http-wg@cuckoo.hpl.hp.com'\n>Cc: 'ahyde@focalink.com'; 'rodger@worldnet.atg.net'\n>Subject: Proposed amendment to RFC2109\n>\n>As promised, here is a description of our proposal to amend RFC2109\n>to allow certification of unverifiable transactions.\n>\n>I have two questions in particular regarding this proposal:\n>\n>1)  Should \"certified\" cookies be a third! type of cookie header\n>[this is what we have written in the proposal] or can we\n>incorporate the certificate as an option in the SetCookie2 header?\n>\n>2) Can we include this in the next version of RFC2109 or must it be a\n>separate RFC?\n>\n>\n>\n\nHTTP STATE MANAGEMENT PROPOSAL FOR CERTIFIED COOKIES\nMarch 30, 1997\n\n\n1. ABSTRACT\nThis document specifies an addition to the state management protocol specified in RFC 2109.  The intent is to provide a mechanism that allows user agents to determine the privacy  policies of a server and to accept or reject cookies based on that policy. Allowing the user to decide whether to accept cookies based on how the server uses them provides far better control over privacy than just distinguishing between servers the users directly accesses (verified transactions) and those  to which the user agent was redirected (unverified transaction.) It furthermore avoids unnecessarily precluding valid distributed applications. \n\nTo provide such information about server privacy behavior, we assume the existence of an  independent certifying authority (or authorities), such as eTrust. The authority establishes levels of \"trust\" and can audit domains to determine their adherence to a given level. It then issues certificates to domains based on the trust level. Passing those certificates along with cookies allows the user-agent to support cookie-acceptance rules based on trust level. \n\nThis document describes a new header, Set-Cookie-Certifiers for sending certificates along with cookies. Set-Cookie2 would still be used to send the cookies. The new header would send certificates intended to verify the trust level of the domain issuing the cookies. This proposal does not alter any existing headers.\n\n\n2. SCOPE OF THIS DOCUMENT\nThis document addresses only the requirement that it be possible to support state management policies at the user-agent that are based on level of privacy adhered to by the server domain. It has therefore not addressed some more general questions raised about the state management protocol, such as the domain-matching rules, or the definition of session itself. Those definitions are assumed in this document.\n\n\n3. TERMINOLOGY\nThe terms  user agent, client, server, origin server, FQHN, FQDN, request host, request-URI and proxy are used as in RFC 2109 on State Management. The terms domain-match, verified transaction and unverified transaction are defined in RFC2109, and those definitions are also used here.\n\nThe term certificate is used to mean an x.509 certificate as referred to in the SSL protocol document, and defined in the x.509 proposed specification[]\n\n\n4. STATE MANAGEMENT PROTOCOL\n4.1 SYNTAX\nThe syntax for the Set-Cookie-Certifier response header is:\n\n    Set-Cookie-Certifier    = \"Set-Cookie-Certifier: \" certificates \n    certificates = x.509-Certificate (\",\") \n\nX.509-Certificate is taken from the SSL Protocol[]. The full specification of x.509 can be found in []. This specification should follow the definitions in those documents. \n\nThe following is a summary version of the x.509 definition. It includes all the fields we use here. \n\n    x.509-Certificate = SEQUENCE {\n      certificateInfo CertificateInfo,\n      signatureAlgorithm AlgorithmIdentifier,\n      signature BIT STRING\n      }\n\n    CertificateInfo  = SEQUENCE {\n      version [0] Version DEFAULT v1988,\n      serialNumber CertificateSerialNumber,\n      signature AlgorithmIdentifier,\n      issuer Name,\n      validity Validity,\n      subject Name,\n      subjectPublicKeyInfo SubjectPublicKeyInfo\n      }\n\nThe token \"signature\" is the hashed value of CertificateInfo, encrypted with the Certificate Authority's private key, by the algorithm specified in signatureAlgorithm. Validity is a date range. Issuer is the name of the certificate authority. We will use different issuer names to convey different levels of certification. The subject name should contain the domain name to which the certification applies. Other fields are not used in this specification and their definitions can be found in the x.509 specification [] or the SSL Protocol specification [].\n\nInformally, the syntax consists of the token \"Set-Cookie-Certifier:\" followed by a comma-separated list of certificates. Note that list of certificates applies to all cookies in the Set-Cookie2 header of the response .  There is no need for separate certificates per cookie, since all cookies in the response come from the same server, and certificates belong to the server not the cookie. More than one certificate may be needed, however, since the server may own certificates from multiple certificate authorities, and any given user-agent may be able to accept certificates from only some authorities.\n\n[NOTE: there is a comment in RFC2109 that states that a gateway may fold multiple Set-Cookie2 headers into a single header. It is assumed that such folding applies only to headers from the same origin server. If that is not the case, we will in fact have to include the certificate in the cookie itself.]\n\n4.2  SERVER ROLE\nAny server wishing to provide certified cookies must request a certificate from the certifying authority. Unlike the certification assumed in general for x.509 certificates, the certifying authority here must have the ability to evaluate the server domain and determine the trust level for which a certificate will be issued. That evaluation takes place outside the protocol described here. The actual certificate request can use the PKCS(10) protocol for certificate-requests [see..]. That protocol will provide the certifying authority with the CertificateInfo to be signed in the issued certificate. The certifying authority then provides the certificate to be included in the set-certified-cookie header.\n\nThe origin server initiates a session, if it so desires. To ensure that user-agents have the most complete information with which to determine whether to allow cookies to be set during this session, it initiates the session by including the header Set-Cookie-Certifier in addiiton to the Set-Cookie2 header. It includes in this header the set of certificates the server has been issued by various trust authorities. (In actuality, this protocol may in fact be useful for authentication in the a broader context than just privacy requirements.) \n\nThe origin server receives cookies from a user-agent in a cookie request header. This proposal does not modify the cookie request header definition. The server may choose to use or ignore cookies supplied in such request headers. Upon receipt of such a cookie, it can return the same or a different cookie in the response, or no cookie at all. \n\n\n4.3 USER AGENT ROLE\nThe user  agent receives cookies from an origin server in the Set-Cookie2 header and cookie certificates in the Set-Cookie-Certifier header. The agent can choose to ignore the Set-Cookie-Certifier header. If it chooses to use the certificate information, then it must validate the certificates and can then decide whether to accept the cookies based on the privacy level of the certificates. \n\nUser agents determine the policy for accepting or rejecting cookies. One suggested policy would be to accept cookies only from sites that are certified by an authority such as eTrust, regardless of whether the transaction is verified. User agents may also provide policy options that distinguish between verified and unverified transactions, for example by allowing all cookies in verified transactions and only certified cookies in unverified transactions.\n\nThe user agent must be able to accept certificates from the authority named as the issuer. It need accept only one certificate in the header to validate the cookies. Thus, a server may send all its certificates, not knowing which a given user agent might accept. Rejection of some is irrelevant, though it does consume processing time. If the cookie acceptance policy is based on certificates and the user agent cannot interpret any certificate, then it must reject the cookies in the response. \n\nTo ensure that the certificate is valid, the user agent decrypts the certificate's signature using the certifying authority's public key. That key can be obtained from the certifying authority by requesting it's own certificate, which includes its public key. Given that we expect few such certifying authorities, the user agent may choose to store keys from standard authorities to avoid extra round trips..\n\nThe issuer of the certificate indicates the certification authority, as well as the level of certification. Thus, if eTrust has 3 levels of certification, they will issue certificates as three different authorities. The user agent may have options that allow cookies to be stored only if they come from domains with an adequate level of privacy certification. To do so, the user agent maintains a list of the certificate authorities that guarantee appropriate privacy levels. \n\nThe subject field in the certificate names the domain for which the certificate is valid. The path of the cookie must domain-match the domain of a certificate if the cookie must be certified. It is the user agent's responsibility to ensure that domains are properly matched. If the user agent is set to accept all cookies then all certificate processing can be skipped. \n\nThis proposal does not alter the domain matching rules described in RFC 2109. Those rules preclude sharing of cookies across some combinations of subdomains. The user agent must reject cookies if  those domain-matching rules are violated.\n\n4.4.CERTIFYING AUTHORITY ROLE\nThe certifying authority referred to in this document must be a neutral third party that can be trusted to accurately characterize the privacy behavior of  web sites.  ETrust is one such organization that might play this role. The issuing of certificates occurs outside the scope of this protocol, but the protocol depends on user trust in that authority.  The certifying authority must understand the scope in which a certificate applies  and the related domain-matching rules, to ensure that for all situations in which the certificate would be deemed to be applicable, the server(s) are in fact operating at the specified trust level.  On receiving a certificate request, the authority must audit the site requesting the certificate and issue the appropriate level certificate.  To issue the certificate, the authority  hashed the CertificateInfo provided by the requester, encrypts that hash value with its private key and includes the resulting value as the signature in the certificate it issues. The authority must also be prepared to respond to requests for its certificate, which contains its public key.  Certificate revocation is handled primarily via revocation lists, as with any x.509 certifier authority.\n\n\n5. SUMMARY\nThis document presents an extension to the state management protocol defined in RFC2109.  It describes only extensions to that protocol. Any parts of the state management not explicitly described here are assumed to remain as defined in RFC 2109. \n\nThe protocol described here allows a user agent to verify that the origin server is using cookies in a manner consistent with the privacy expectations of the user, by providing a certificate issued by a trusted authority.\n- 1 -\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "At 07:16 11/20/97 +0100, Life is hard... and then you die. wrote:\n>Is there any reason we can't change\n>\n>  An HTTP/1.1 sender MAY include a Trailer header field in a message\n>using\n>  chunked transfer-coding with a non-empty trailer. Doing so allows the\n>  recipient to know which header fields to expect in the trailer.\n>\n>to\n>\n>  An HTTP/1.1 sender SHOULD include a Trailer header field in a message\n>using\n>  chunked transfer-coding with a non-empty trailer. Doing so allows the\n>  recipient to know which header fields to expect in the trailer.\n>\n>? Is this because of already installed implementations?\n\nYes, we can't upgrade that requirement.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: AcceptTransfer header field (was HTTP/1.1 Issues",
            "content": "At 16:21 11/19/97 -0800, Roy T. Fielding wrote:\n\n>BTW, I *really* *really* dislike Accept-TE, both in terms of syntax\n>and in the mistake of associating it with end-to-end content negotiation.\n>It is possible to make this feature so complex that nobody will implement\n>it at all, regardless of the potential benefits of compressed transfers.\n>It is also possible to introduce so many optional parameterizations of\n>the \"requesting encoding\" side of the equation that the actual\n>implementation becomes less optimal (i.e., reusable) than no encoding.\n>In this case, keep it simple really means keep it simple or not at all.\n\nThere has been no claim that Accept-TE is part of content negotiation any\nmore than Accept-Encoding and Accept-Range. It is the same as\nContent-Length and Content-MD5 has nothing to do with metadata and to say\nthat User-Agent is not used for content-negotiation.\n\nI said that Accept-TE really should be a general header but that HTTP\ndoesn't have proper support for handling this, it has to be a request header.\n\nI don't buy the argument of complexity. The simple case is indeed simple\nbut that should not prevent future versions from adding more complexity. If\nwe don't allow this, then they will kludge around it and the abstract\nlayering model will break down even more.\n\nAs I said, it is in the one hour hacking range - I did the client side\nsupport in that amount of time in libwww.\n\nThat put aside, I would like to reach closure on this - otherwise, it will\ndrop on the floor and we will not be able to introduce new transport\nfilters. If people think that the name of the header is a problem, then\nthat's easy to fix.  Suggestions of the top of my head are:\n\nTransfer\nTE\n47\n\n\"TE\" seems as a nice compromise to me.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "I think that three digits is too small for arbitrary extenxibility.\nAt least four (as suggested) would be the minimum.\n(\"No one will ever want more than 640K\")\n\n** Reply to note from Larry Masinter <masinter@parc.xerox.com> Wed, 19 Nov 1997 17:01:38 PST\n>   \n> This memo was discussed at previous working group meetings. The\n> issue that WebDAV and RSVP have assigned status codes for HTTP\n> and others want to has arisen again. Unless there are any\n> objections, we will propose that this draft be moved forward\n> to Proposed Standard.\n>   \n> Larry\n> -- \n> http://www.parc.xerox.com/masinter\n>   \n> \n \n\nRichard L. Gray\nwill code for chocolate\n\n\n\n"
        },
        {
            "subject": "Re: Syntax (and other) problems in Digest spe",
            "content": "The changes Scott proposes look good. I do have one comment though:\n\n>   Ronald also\n>   pointed out that RFC2069 did not require the 'realm' parameter to be\n>   the first authentication parameter.  It looks to me as though this\n>   was actually an attempt to write the fact that this parameter was\n>   required into the syntax, and since there is no ambiguity created by\n>   removing that requirement, I would prefer to see the more general\n>   syntax used in 2069.\n> \n>   The point may be made that some existing browsers may be broken by\n>   this in that they may have coded to the 1945/2068 general rule.\n>   I've done considerable testing in this area and have found that\n>   browsers fall into two categories:\n>     - They recognize that a challenge is not basic and give up,\n>       displaying an error to the user saying that they can't deal with\n>       this server.\n>     - They just send basic credentials no matter what the challenge is\n>       and it doesn't work.\n>   Browser vendors are invited to figure out which thier product does...\n> \n>   In either case, changing the spec won't have any effect.\n\nChanging this will affect at least old versions of my code, as it will\nthrow an exception during the parsing when the realm is not the first\nparam, before it even realizes that it may be an authentication scheme\nit can't handle anyway (the parsing into params is a lower level process\nthan handling the specific scheme). I don't know which clients Scott\ntested, but there are quite a few http client libraries and embeded\nclients around, and I wouldn't be surprised if a number of them also use\na two level scheme. Now my only (slight) worry is that the proposed\nchange might confuse end users unnecessarily because of a misleading\nerror (i.e. a parse error instead of an unknown auth scheme error).\n\nTo help alleviate this I propose adding the following words:\n\n>   These changes make the general syntax (now presented in 2068 section\n>   11 - Access Authentication):\n> \n>   ================\n>          challenge      = auth-scheme 1*SP 1#auth-param\n>          credentials    = basic-credentials\n>                           | auth-scheme #auth-param\n>          auth-param     = token \"=\" ( token | quoted-string )\n> \n>     The authentication parameter 'realm' is defined for all\n>     authentication schemes:\n> \n>          realm          = \"realm\" \"=\" quoted-string\n\n      Some older clients include parsers expecting the rfc-2068\n      authentication syntax which requires the realm to be the first\n      parameter in a challenge. While the above syntax does not require\n      this anymore, server implementers are nevertheless encouraged\n      to ensure that the realm is indeed the first parameter in a\n      challenge whenever possible.\n\n>   ================\n\nAs I noted before, most servers seem to do this anyway.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: spec-08 selfcontradiction in 13.5.",
            "content": "Paul Leach:\n>\n>I propose to just remove Content-Length from the list of headers affected by\n>\"no-transform\".\n>\n>This is OK -- the section already says that it may not be modified, and it\n>it gets added, it has to reflect the true length. This meets the\n>requirements of \"no-transform\", because the fidelity of the entity-body will\n>not be affected.\n\nYes, I think just removing Content-Length from the list of headers\nwould work, though I can't thorougly check it now.  As far as I am\nconcerned, the editor may choose either solution.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "schulzrinne-http-status00.txt: three digits not enoug",
            "content": "The members of the HTTP working group like the concept, but\ndon't like the idea that status codes are limited to three\ndigits.\n\nAre you willing to revise your draft, with the assistance of\nthose members of the HTTP working group who are willing to\nprovide input?\n\nThanks,\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "REAUTHENTICATION REQUIRE",
            "content": "Suppose you use your browser, supplying some authentication to server. Then\nyou leave. A while later, a \"bad guy\" walks up to your desk, and starts\nsending requests to a server. Your browser will just keep sending your\ncredentials in the Authorization and/or Proxy-Authorization header, so the\n\"bad guy\" will be granted access as you.\n\nBrowsers should time out, and after a while, stop sending Authorization\nand/or Proxy-Authorization header. (This isn't really simply optional UI\nbehavior -- it's required for security).  However, existing browsers don't.\nAnd, different applications have different requirements in this regard.\nHence, it is necessary for servers to be able to say that their maximum idle\ntime has passed, and that the browser should require the user to enter their\ncredentials again.\n\nAdd sections 10.4.19 and 10.4.20\n\n==============================\n\n10.4.19 420 Reauthentication Required\n\nThis header is similar to \"401 Unauthorized\", except that the user agent\nMUST request credentials from the user before resubmitting the request, even\nif the challenge is the same as on a prior response or if the user agent has\nalready obtained credentials from the user. The user agent should not assume\nthat the current credentials are invalid if the request contained an\nAuthorization header. The server can use this status code to cause the\nbrowser to verify that the current user is the same as the one who supplied\nthe original credentials (say, after a period of inactivity).\n\n10.4.20 421 Proxy Reauthentication Required\n\nThis header is similar to \"407 Proxy Aauthentication Required\", except that\nthe user agent MUST request credentials from the user before resubmitting\nthe request, even if the challenge is the same as on a prior response or if\nthe user agent has already obtained credentials from the user.  The user\nagent should not assume that the current credentials are invalid if the\nrequest contained an Proxy-Authorization header. The server can use this\nstatus code to cause the browser to verify that the current user is the same\nas the one who supplied the original credentials (say, after a period of\ninactivity).\n\n==================================\n\nAfter the following paragraph in section 11:\nIf a prior request has been authorized, the same credentials MAY be reused\nfor all other requests within that protection space for a period of time\ndetermined by the authentication scheme, parameters, and/or user preference.\nUnless otherwise defined by the authentication scheme, a single protection\nspace cannot extend outside the scope of its server.\n\nAdd:\nA user agent MUST NOT reuse the same credentials if a substantial amount of\ntime has passed without any user activity -- for example, the current user\nmay have left their browser, and an unauthorized one started using it. It is\nRECOMMENDED that this time not exceed one hour, and that it be configurable.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "Regarding\n>Is there any reason we can't change\n>\n>  An HTTP/1.1 sender MAY include a Trailer header field in a message using\n>  chunked transfer-coding with a non-empty trailer. Doing so allows the\n>  recipient to know which header fields to expect in the trailer.\n>\n>to\n>\n>  An HTTP/1.1 sender SHOULD include a Trailer header field in a message using\n>  chunked transfer-coding with a non-empty trailer. Doing so allows the\n>  recipient to know which header fields to expect in the trailer.\n>\n>? Is this because of already installed implementations?\n\nHenrik wrote:\n\n    Yes, we can't upgrade that requirement.\n\nWhy not?  Increasing the requirement level could have the effect\nof converting some deployed implementations from \"unconditionally\ncompliant\" to \"conditionally compliant\" ... but ONLY if they\nalready send trailers (which means, in this case, only if they\nsend Content-MD5 in the chunk trailer, since until now nothing\nelse was allowed) and ONLY if they are otherwise unconditionally\ncompliant with the rest of the spec.\n\nSince we haven't actually finished writing the HTTP/1.1 spec,\nit's hard to believe that there are any deployed implementations\nthat could be unconditionally compliant with the final result.\n\nConverting this to SHOULD will make things much simpler, I think.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "Paul Leach wrote:\n> [...]\n> A user agent MUST NOT reuse the same credentials if a substantial amount of\n> time has passed without any user activity -- for example, the current user\n> may have left their browser, and an unauthorized one started using it. It is\n> RECOMMENDED that this time not exceed one hour, and that it be configurable.\n\nI have argued for several years that a browser ought to have a way to\nlet a user say \"forget all my authentication stuff\".  However, as a\nuser, I dislike this suggestion of a timeout.  I keep the browser on my\nworkstation up as long as the browser and OS don't crash.  I don't\nparticularly want to have to reauthenticate myself every hour or so.\n\nThe problem you're trying to solve is one of machines shared by multiple\nusers.  Better to address that problem in all its glory (including\nshared cookies, for example), than to nibble around the edges with a\ntimeout for a specific authentication problem.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 ISSUE: TRAILER_FIELDS  Proposed Resolutio",
            "content": "In all the discussion of trailers and which \"headers\" (hmm, I guess we\nneed a new term) are acceptable in the trailer, I haven't seen anything\nabout conflicts between and normalization of header headers and trailer\nheaders.  The HTTP 1.1 spec is silent, leading me to believe that a\nthere is no material difference between a header appearing twice in the\nheader section and a header appearing in both the header and trailer\nsections.  Put differently, any comma-separated list of elements in a\nheader can be split into several headers, with some preceeding the\nencoded entity body and some following it.\n\nIs this what others understand as well?  It seems a little odd to me, but\nit is consistent with the rest of the spec.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Regarding Authenticatio",
            "content": "Sambasiva Rao <sams@wipinfo.soft.net> writes:\n\n>Few issues related to Authentication  are as following :\n>\n>1> In the authentication credentials field defined as following\n>\n>       credentials  = basic-credentials\n>                       |auth-scheme #auth-param\n>       in RFC2068.\n>\n>       a> Does this mean the server must produce parse error if the client\n>sends two or more scheme credentials ?\n\nYes, that's what it means.  There is no provision in HTTP 1.1 for\nmultiple sets of credentials on an Authorization header.  By extension,\nthat also means that there is no provision multiple Authorization headers\nin a request.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "agenda, HTTP working group, 40th IET",
            "content": "Agenda for 40th IETF HTTP working group.\n\nThe first meeting is scheduled for Monday, December 8 at 1300-1500.\nThe second meeting has not yet been scheduled.\n\nThis is intended to be the last meeting of the HTTP working group\nprior to the submission of HTTP/1.1 as Draft Standard, and other\nproposals as Proposed Standard or Experimental. It is expected\nthat subsequent work on HTTP within IETF will happen in new\nworking groups focused on explicit narrow issues.\n\nAgenda:\n\n2 Hours: Jim Gettys\n   Review the HTTP/1.1 draft, and discuss remaining open issues\n    see http://www.w3.org/Protocols/HTTP/Issues for list\n\n20 minutes: Dave Kristol\n   Review state management draft\n20 minutes: Scott Lawrence\n   Review 'OPTIONS' and 'PEP' drafts\n20 minutes: Larry Masinter\n   Review of other drafts\n20 minutes: Larry Masinter\n   Report on survey of interoperable implementations\n\nIn addition, there will be a meeting Tuesday evening of a small\ngroup who are willing to help document interoperable implementations\nof each feature of HTTP/1.1, in order to progress to draft standard.\nThe time/place will be announced at the first meeting and on the bulletin\nboard.\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "PUT with ContentRang",
            "content": "The basic idea is to allow PUT with Content-Range, and allow caches to apply\nthe update when possible to avoid having to refetch the whole entity-body,\nand to create a model for (e.g.) WebDAV that wants to do partial updates on\nresources.\n\nAdd a section:\n=====================\n9.6.1 Partial PUT (PUT with Content-Range)\n\nA PUT method MAY supply a partial entity body specified by a Content-Range\nheader. The PUT requests that the resource be updated or created with the\npartial entity-body, in a manner entirely up to the origin server. However,\nthe origin server may indicate to caches that they may perform the same\nupdate to a cached copy.\n\nNote: Even if partial PUT is not exceedingly interesting, it should\nestablish a paradigm for possible future methods that do incremental updates\nto resources and want to allow incremental updates to cached copies.\n\n> 1. An HTTP/1.1 server that doesn't support PUT with Content-Range MUST\n> reply with \"507 Partial Update Not Implemented\" and MUST NOT perform any\n> action on the requested resource.\n> \n> 2. An HTTP/1.1 client MUST know that that the origin server for the\n> request supports PUT with Content-Range header (but not neccessarily on\n> the specified resource) before attempting such a request. This is\n> necessary since a pre-1.1 server that does not understand PUT with\n> Content-Range header will overwrite the whole resource instead of just the\n> Content-Range the client requested.\n> \n> 3. An HTTP/1.1 origin server MAY reply to a PUT with Content-Range with a\n> \"207 Partial Update OK\" if the change in the entity body of a cached\n> response can be duplicated by a cache by merely replacing the bytes at the\n> byte positions specified in the Content-Range  with the corresponding\n> bytes in the entity-body in the request. Client and proxies that don't\n> understand \"207 Range Update\" will be correct if they treat it as \"200\n> OK\". (Recall that responses to PUT are not cachable)\n> \n> 4. An HTTP/1.1 cache receiving a \"207 Partial Update OK\" status-code in\n> the response to a PUT with Content-Range MAY update the entity in the\n> cache as described above; if it does not, or if the status-code is\n> anything else, it MUST invalidate the entity, as per section 9.6)\n=================\n\nAdd a section:\n=================\n10.2.8 207 Partial Update OK\n\nThe server has fulfilled a method that specified a partial update, and the\nchange in the resource may be mimiced by a cache that understands the\nmethod. In the case of partial PUT\nthe change in the resource can be duplicated by a cache merely by replacing\nthe bytes at the byte positions specified with the corresponding bytes in\nthe entity-body in the request.\n==================\n\nAdd a section:\n==================\n10.5.8 507 Partial Update Not Implemented\n\nThe server does not support paritial update on the specified resource.\n\n===================\n\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "On Wed, 19 Nov 1997, Paul Leach wrote:\n\n> Browsers should time out, and after a while, stop sending Authorization\n> and/or Proxy-Authorization header. (This isn't really simply optional UI\n> behavior -- it's required for security).  However, existing browsers don't.\n> And, different applications have different requirements in this regard.\n> Hence, it is necessary for servers to be able to say that their maximum idle\n> time has passed, and that the browser should require the user to enter their\n> credentials again.\n\nHello all,\n\nAuthorization credentials should be given a timeout at authentication\ntime, the same way cookies are, and UA's MUST not use stale\nauthentication.  Since there is currently no way to specify this behavior\nin protocol at challenge-time, the proposal as I understand it is to allow\nthe server to arbitrarily nix authorization that it considers stale and\nforce the browser to challenge the user for credentials again.\n\nWhile it might be useful in some applications to allow a server to behave\nthis way, it is also desirable for the timeout to be a property of the\nauth process and for the either the UA to be instructed to \"forget\" or for\nthe server to reject credentials (or both) after they become stale.  The\nuser might be alerted that this will happen.  Not all servers are going to\nwant to maintain a list of authorization ttls, but UA's could perform a\nsimple timeout check with no significant performance impact. \n\nTimeouts could be specified in the initial challenge as follows:\n\nWWW-Authenticate: BASIC realm=foo  timeout=30m\n\nBut this wouldn't allow for timeouts to be determined by credential as it\nwould be the first statement by the server in the challenge dialogue,\nbefore the credentials were sent in reply. \n\nEither way, in lieu of changing the credentials, the server is at the\nmercy of the UA when it assumes that the credentials have really been\nretyped and not taken from cache.  \n\n> Add:\n> A user agent MUST NOT reuse the same credentials if a substantial amount of\n> time has passed without any user activity -- for example, the current user\n> may have left their browser, and an unauthorized one started using it. It is\n> RECOMMENDED that this time not exceed one hour, and that it be configurable.\n\nThis is good, but there are perfectly reasonable situations where you\nwould want to do exactly the opposite.  Assume that an institution has\nkiosk UA's that operate under a license to freely access otherwise\nprotected content.  The UA keeps its keyring on disk or in memory with\ncredentials and instead of prompting a user, reads the keyring and\nformulates its response to a challenge.  There is never a case where these\nlong-lived credentials would cause a security risk, yet it would not be\nfully compliant under the above language.  \n\nIn the case of a personal workstation, a locking screen saver can perform\nthis function much more reliably than any protocol.  But in the case of\nshared stations, especially public kiosk applications where users can walk\nup and enter their own credentials, there needs to be support in the\nprotocol and perhaps the authentication scheme for authorization timeouts. \nI just want to see the work distributable between the parties in a manner\nappropriate to the application instead of being always foisted onto the\nserver and with mandatory timeouts.\n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "Whether at least existing HTTP implementations can possibly use\nmore than three digits depends strongly on implementation details:\n\na) If the implementation takes the code, and then performs a MOD 100\noperation, to get a subcode for the code type, then it makes sense to\nallow for more than three digit status codes.\n\nb) If existing implementations just take the first decimal digit,\nand then switch off of the remaining digits, more than 3 digit status\ncodes are feasible.\n\nWithout some data on whether implementations (particularly proxies) do \na) or b), any discussion of more than three digit codes is pretty silly.\n\nI suspect that a) predominates, but having never implemented a proxy\nor server, or for that matter, client library, I can't say.\n\nSo implementers, how is the jungle out there???\n\nYour editor,\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "Jim Gettys:\n>\n>Whether at least existing HTTP implementations can possibly use\n>more than three digits depends strongly on implementation details:\n>\n>a) If the implementation takes the code, and then performs a MOD 100\n>operation, to get a subcode for the code type, then it makes sense to\n>allow for more than three digit status codes.\n>\n>b) If existing implementations just take the first decimal digit,\n>and then switch off of the remaining digits, more than 3 digit status\n>codes are feasible.\n\nTrue, but the draft discusses status codes for `HTTP and HTTP-Derived\nProtocols', and I suppose that for at least some HTTP-Derived\nprotocols, compatibility with the status code parsers of plain http\nclients will not be an issue.  \n\nAnd if you really want to send a 5 digit code like 45205 to a http\nclient without prior negotiation, you could always invent some wrapping\nscheme with responses like\n\nHTTP/1.1 299 Extended status code\nStatus: 45205 Epibration complete\nContent-type: text/html\n....\n\nKoen.\n\n\n>\n>Without some data on whether implementations (particularly proxies) do \n>a) or b), any discussion of more than three digit codes is pretty silly.\n>\n>I suspect that a) predominates, but having never implemented a proxy\n>or server, or for that matter, client library, I can't say.\n>\n>So implementers, how is the jungle out there???\n>\n>Your editor,\n>- Jim Gettys\n>\n>\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "  I think that the choice of a credentials timeout may have been an\n  unfortunate one, and think that the example should be omitted from\n  the text in the standard.\n\n  One has only to read the CGI newsgroup or ask people who develop\n  applications to run over the Web to know how often people want this\n  - they want to be able to explicitly flush credentials.  It is an\n  easy thing to do.  Yes, it won't work with old browsers, but neither\n  will a number of other 1.1 innovations; we should just do the right\n  thing now and get on with it.  (There, I got my 2 cents in before\n  7EST :-)\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "Based on feedback, and the epiphany that screen savers do the same thing as\nI proposed the browser do, I withdraw the proposed modification to section\n11. Something like it should go in the security considerations section --\nJim, can you mark it as an editorial issue, not for this revision?\n\nI also added text about an error message, and what happens with browsers\nthat don't understand the new status code.\n\nRevised proposal:\n\n> Add sections 10.4.19 and 10.4.20\n> \n> ==============================\n> \n> 10.4.19 420 Reauthentication Required\n> \n> This header is similar to \"401 Unauthorized\", except that the user agent\n> MUST request credentials from the user before resubmitting the request,\n> even\n> if the challenge is the same as on a prior response or if the user agent\n> has\n> already obtained credentials from the user. The user agent should not\n> assume\n> that the current credentials are invalid if the request contained an\n> Authorization header. The server can use this status code to cause the\n> browser to verify that the current user is the same as the one who\n> supplied\n> the original credentials (say, after a period of inactivity). The server\n> SHOULD send an entity-body\nexplaining the reason for requiring reauthentication, because user agents\nthat do not understand the status code will treat it as a generic 400 error\nand display\nthe message.\n\n> 10.4.20 421 Proxy Reauthentication Required\n> \n> This header is similar to \"407 Proxy Aauthentication Required\", except\n> that\n> the user agent MUST request credentials from the user before resubmitting\n> the request, even if the challenge is the same as on a prior response or\n> if\n> the user agent has already obtained credentials from the user.  The user\n> agent should not assume that the current credentials are invalid if the\n> request contained an Proxy-Authorization header. The server can use this\n> status code to cause the browser to verify that the current user is the\n> same\n> as the one who supplied the original credentials (say, after a period of\n> inactivity). The server SHOULD send an entity-body\n> explaining the reason for requiring reauthentication, because user agents\n> that do not understand the status code will treat it as a generic 400\n> error and display\n> the message.\n> \n> \n> ==================================\n> \n\n\n\n"
        },
        {
            "subject": "Issue: TRAILER_FIELDS and Transfer Encodings last time..",
            "content": "These are the edits that I have recieved for this issue since yesterday. It\ntakes into account the following:\n\n- Replace the header name Accept-TE with simply TE [1]\n- Added fixes from Dave Kristol [2]\n- I will leave it up to Jim and Larry to decide on the\n  implications of changing a MAY to a SHOULD as \n  proposed in [3] by Ronald Tschalaer\n\nI now have implemented both Trailer and TE in libwww robot (client), and I\nhandle arbitrary headers in the trailer of chunked encoding.\n\nHenrik\n\n[1] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0217.html\n[2] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0206.html\n[3] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0220.html\n\n****\n\n3.6 Transfer Codings\n\nTransfer coding values are used to indicate an encoding transformation that\nhas been, can be, or may need to be applied to an entity-body in order to\nensure \"safe transport\" through the network. This differs from a content\ncoding in that the transfer coding is a property of the message, not of the\noriginal entity. Therefore, transfer codings only apply to the immediate\nconnection.\n\n       transfer-coding         = \"chunked\" | transfer-extension\n       transfer-extension      = token *( \";\" parameter )\n\nParameters may be in the form of attribute/value pairs.\n\n       parameter               = attribute \"=\" value\n       attribute               = token\n       value                   = token | quoted-string\n\n[***[Jim, parameter is already defined in 3.7. As it should only be in one\nplace, it can either be here or in section 3.7. In the latter case, there\nshould be a forward reference here]***]\n\nAll transfer-coding values are case-insensitive. HTTP/1.1 uses transfer\ncoding values in the TE header field (section 14.Y) and the\nTransfer-Encoding header field (section 14.40).\n\nTransfer codings are analogous to the Content-Transfer-Encoding values of\nMIME [7], which were designed to enable safe transport of binary data over\na 7-bit transport service. However, safe transport has a different focus\nfor an 8bit-clean transfer protocol. In HTTP, the only unsafe\ncharacteristic of message-bodies is the difficulty in determining the exact\nbody length (section 7.2.2), or the desire to encrypt data over a shared\ntransport.\n\nThe Internet Assigned Numbers Authority (IANA) acts as a registry for\ntransfer-coding value tokens. Initially, the registry contains the\nfollowing tokens: \"chunked\" (section 3.6.1), \"identity\" (section 3.6.2),\n\"gzip\" (section 3.5), \"compress\" (section 3.5), and \"deflate\" (section 3.5).\n\nNew transfer-coding value tokens should be registered in the same way as\nnew content-coding value tokens (section 3.5).\n\nA server which receives an entity-body with a transfer-coding it does not\nunderstand SHOULD return 501 (Unimplemented), and close the connection. A\nserver MUST NOT send transfer-codings to an HTTP/1.0 client.\n\n3.6.1 Chunked Transfer Coding\n\nThe chunked encoding modifies the body of a message in order to transfer it\nas a series of chunks, each with its own size indicator, followed by an\noptional trailer containing entity-header fields. This allows\ndynamically-produced content to be transferred along with the information\nnecessary for the recipient to verify that it has received the full message.\n\n       Chunked-Body   = *chunk\n                        last-chunk\n                        trailer\n                        CRLF\n       chunk          = chunk-size [ chunk-extension ] CRLF\n                        chunk-data CRLF\n       chunk-size     = 1*HEX \n       last-chunk     = 1*(\"0\") [ chunk-extension ] CRLF\n\n       chunk-extension= *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] ) \n       chunk-ext-name = token\n       chunk-ext-value= token | quoted-string\n       chunk-data     = chunk-size(OCTET)\n       trailer        = *entity-header\n\nThe chunk-size field is a string of hex digits indicating the size of the\nchunk. The chunked encoding is ended by any chunk whose size is zero,\nfollowed by the trailer, which is terminated by an empty line.\n\nThe trailer allows the sender to include additional HTTP header fields at\nthe end of the message. The Trailer header field can be used to indicate\nwhich header fields are included in a trailer (see section 14.Y).\n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the TE field (section 14.X).\n\nAn example process for decoding a Chunked-Body is presented in appendix\n19.4.6.\n\nAll HTTP/1.1 applications MUST be able to receive and decode the \"chunked\"\ntransfer coding, and MUST ignore chunk-extension extensions they do not\nunderstand.\n\n3.6.2 Identity Transfer Coding\n\nThe identity transfer-encoding is the default (identity) encoding; the use\nof no transformation whatsoever. This transfer-coding is used only in the\nTE header field, and SHOULD NOT be used in any Transfer-Encoding header field.\n\n14.X TE\n\nThe TE request-header field is similar to Accept-Encoding, but restricts\nthe transfer-codings (section 3.6) that are acceptable in the response.\n\n       TE               = \"TE\" \":\"\n                          #( t-codings )\nt-codings        = \"chunked\" | ( transfer-extension [ accept-params ] )\n\nExamples of its use are:\n\n       TE: deflate\n       TE:\n       TE: chunked; deflate;q=0.5\n\nThe TE header field only applies to the immediate connection. Therefore,\nthe te keyword MUST be supplied within a Connection header field (section\n14.10) whenever TE is present in an HTTP/1.1 message.\n\nA server tests whether a transfer-coding is acceptable, according to an TE\nfield, using these rules:\n\n1. If the transfer-coding is one of the transfer-codings listed in the TE\nfield, then it is acceptable, unless it is accompanied by a qvalue of 0.\n(As defined in section 3.9, a qvalue of 0 means \"not acceptable.\")\n\n2. If multiple transfer-codings are acceptable, then the acceptable\ntransfer-coding with the highest non-zero qvalue is preferred.\n\n3. The \"identity\" transfer-coding is always acceptable, unless specifically\nrefused because the TE field includes \"identity;q=0\". The \"chunked\"\ntransfer-coding is always acceptable. The Trailer header field (section\n14.Y) can be used to indicate the set of header fields included in thetrailer.\n\n4. If the TE field-value is empty, only the \"identity\" and the \"chunked\"\ntransfer-codings are acceptable.\n\nIf an TE field is present in a request, and if a server cannot send a\nresponse which is acceptable according to the TE header field, then the\nserver SHOULD send an error response with the 406 (Not Acceptable) status\ncode.\n\nIf no TE field is present, the sender MAY assume that the recipient will\naccept the \"identity\" and \"chunked\" transfer-codings.\n\nA server using chunked transfer-coding in a response MUST NOT use the\ntrailer for other header fields than Content-MD5 and Authentication-Info\nunless the \"chunked\" transfer-coding is present in the request as an\naccepted transfer-coding in the TE field.\n\n14.Y Trailer\n\nThe Trailer general field value indicates that the given set of header\nfields are present in the trailer of a message encoded with chunked\ntransfer-coding.\n\n       Trailer  = \"Trailer\" \":\" 1#field-name\n\nAn HTTP/1.1 sender MAY include a Trailer header field in a message using\nchunked transfer-coding with a non-empty trailer. Doing so allows the\nrecipient to know which header fields to expect in the trailer.\n\nIf no Trailer header field is present, the trailer SHOULD NOT include any\nother header fields than Content-MD5 and Authentication-Info.\n\nA server MUST NOT include any other header fields unless the \"chunked\"\ntransfer-coding is present in the request as an accepted transfer-coding in\nthe TE field.\n\nMessage header fields listed in the Trailer header field MUST NOT include\nthe following header fields:\n\nTransfer-Encoding\nContent-Length\nTrailer\n\n\n\n"
        },
        {
            "subject": "Re: Access control and knowledg",
            "content": "See \"Cache-Control: private\"\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "Having a document to vet and register proposed status codes is\na good idea -- one that we have discussed in the past.  Reserving\nsets of status codes for use by other protocols is a bad idea.\nHTTP status codes need to be thought-out with the rigor of an RFC.\nThe draft is insufficient for helping IANA set up a registry.\nBetter examples of that are the charset and media type registries.\n\nHTTP/1.x status codes are three digits.  Responses are examined for\nthree digits.  Any more than three digits and the response will\nbe treated as HTTP/0.9.\n\nWhen a protocol using HTTP needs a new status code, it can talk about\nit in the abstract (e.g., 4aa) until a sufficient justification can\nbe put in text and registered for that status.  There is no need to\npre-register codes that might be needed.  Fewer than 10% of the proposals\nfor new status codes have been accepted, usually because the authors\ndidn't bother to check for an existing status code that already\nserves their purpose, or simply wanted their own \"special\" code\n\"just in case\", or the proposal faded into dust long before implementation.\n\nWhat other protocols not using HTTP want to do with their own status\ncodes is not relevant.  I suggest they use more than three digits.\nLikewise, it would make sense for HTTP/2+ to use more than three\ndigits (or at least a token with wider range).\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "I basically agree with Roy on this one; don't add more digits\nto the status-code number.\n\nOne thing that has always bugged me is that an HTTP response\ncarries exactly one status code, and we often have to work hard\nto figure out which one that should be.  There are times when\nit would be better to have a way to communicate multiple dimensions\nof status information.\n\nThat's why we added the Warning header.   Maybe we should be\nencouraging protocols layered over HTTP (such as WEBDAV) to\nthink in terms of adding new headers, rather than new status\ncodes?  I.e.,\n\nHTTP/1.1 200 OK\nDAV-Status: DAV/1.3 3141592 Pie is ready\n\nThis seems to be a better way of dealing with layering.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "PROXYREDIRECT status..",
            "content": "I've received mail from both Ari and Josh this evening; it looks like they \nare not going to be able to get an updated version of 306 done (particularly \nsince 306 (set proxy) depended on the elaborated OPTIONS spec, we've been \nunable to converge on).\n\nI'm going to remove the changes made in draft 08 (rev-00) for\nthis issue, though I think I'll add a few words around limiting\n305 to origin servers, for a single request. (to deal with the\nfundamental security issue 305 raises).  Right now, anyone working\nto the last draft silly enough to try to implement it would be doing\nmore harm than good, so I don't want to leave the rev 00 wording\nin Rev 01...\n\nHaving said that:\n\nI believe the set proxy functionality is REALLY badly needed for operational \nand web evolution reasons, in my personal opinion.  The sooner the better.\n\nBut Set Proxy needs to be done right, because the potential for spoofing \nattacks is very large, and the design work better not be hurried.\n\nUnless/until an updated proposal 306 (set proxy) gets made (and soon) to \nthe working group, I'm extremely pessimistic about 306 (set proxy) making \ndraft standard of HTTP/1.1.\n\nEven then, one might argue that set proxy is new functionality, and I don't \nwant this to hang up getting HTTP/1.1 to draft standard.  So my opinion \nis at this date to undock the set proxy functionality into a separate document. \n\nWe cannot introduce new functionality between proposed standard and draft \nstandard, only fix problems found in the proposed standard; as usual IETF \nleaves this to the judgement call of the editor, working group chair, and \narea directors, and ultimately IESG; there is wiggle room, but not infinite \namounts. In many ways, I'd be happier if the Set Proxy were an independent \ndocument, particularly at this date.\n\nI suggest more serious thought be made to how to resolve this outside of \nthe base HTTP/1.1 specification, sad though that may be, and that we undock \nset proxy.  If people get cracking, such a document could go to proposed\nstandard, as HTTP/1.1 goes to draft, without risking hanging up HTTP/1.1\ngoing to draft...\n\nYour editor,\n\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: Proposed amendment to RFC210",
            "content": "I'm leaning toward calling for the formation of a \"state management\"\nsubgroup or even a separate working group, given how much\nattention this is getting, and how separable from the other\nissues around HTTP.\n\nIn the meanwhile, I will consult with IESG members and\narea directorate and get advice about how best to proceed as\nfar as revising 2109.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "I believe that there is not (and will not be) consensus for\ndraft-schulzrinne to move forward in the HTTP working group.\n\na) We recommend that new status codes go through standards track\nb) We recommend that other protocols that wish to layer ON TOP OF\n  HTTP do so by using some other header, in conjunction with\n  existing HTTP headers and warnings\nc) We recommend that other protocols that are really separate\n   but are not HTTP use other than 3-digit status codes.\n\nDo these match what everyone's talking about?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Changes to the spe",
            "content": "Let me state my agreement with Jim Gettys, Larry Masinter, and Roy Fielding\non the issue of changes being made to the HTTP/1.1 proposed draft.\n\nThe proposed draft is the standard.  Any changes we are making should only\nbe to fix known problems and should only be done in a way such that RFC 2068\ncompliant programs will remain compliant with the draft version of the\nstandard.\n\nThis means that all new optional features, such as Accept-TE, do not get\nincluded in the draft standard.\n\nThis means that any features that are not backwards compatible, such as\nchanging basic auth syntax or putting in more than 3 digit error codes, with\nRFC 2068 do not get included in the draft standard.\n\nThis means that any features that have not been independently implemented by\nat least two vendors do not get included in the draft standard.\n\nMoving from Proposed to Draft is not a chance to put in new features that we\nnow realize are useful based on our experience with RFC 2068.  This is only\na chance to fix known problems, in a backwards compatible way.  If you want\nto add a new optional feature, write up an RFC and put it on standards\ntrack.  If you want to put in a new mandatory feature or change expected\nprotocol behavior then rev the protocol number.  But if a proposed change\nfails any of the previous three tests then it has no business being put into\nthe draft standard.\n\nYaron\n\n\n\n"
        },
        {
            "subject": "Re: Regarding Authenticatio",
            "content": "Thank you Ross.\nMy other questions are answered and I am waiting for \ngetting this doubt clarified.Thanks for confirming.\n\nRegards,\nSam.\n\nOn Thu, 20 Nov 1997, Ross Patterson wrote:\n\n> Sambasiva Rao <sams@wipinfo.soft.net> writes:\n> \n> >Few issues related to Authentication  are as following :\n> >\n> >1> In the authentication credentials field defined as following\n> >\n> >       credentials  = basic-credentials\n> >                       |auth-scheme #auth-param\n> >       in RFC2068.\n> >\n> >       a> Does this mean the server must produce parse error if the client\n> >sends two or more scheme credentials ?\n> \n> Yes, that's what it means.  There is no provision in HTTP 1.1 for\n> multiple sets of credentials on an Authorization header.  By extension,\n> that also means that there is no provision multiple Authorization headers\n> in a request.\n> \n> Ross Patterson\n> Sterling Software, Inc.\n> VM Software Division\n> \n\n\n\n"
        },
        {
            "subject": "Re: Changes to the spe",
            "content": "I don't want to deep end on process, but ...\n\n> ... Any changes we are making should only\n> be to fix known problems \n\n yes\n\n> and should only be done in a way such that RFC 2068\n> compliant programs will remain compliant with the draft version of the\n> standard.\n\nThis is desirable but not a requirement. If RFC 2068 is broken,\nthen we can fix it. (I don't think that applies to either Accept-TE or\nextra error codes.)\n\n> This means that all new optional features, such as Accept-TE, do not get\n> included in the draft standard.\n\nIt's preferable to undock anything that can be, especially if it isn't\nnecessary to fix a design error.\n\n> This means that any features that are not backwards compatible, such as\n> changing basic auth syntax or putting in more than 3 digit error codes, with\n> RFC 2068 do not get included in the draft standard.\n\nBackward compatibility isn't a requirement officially, although 'consensus'\nis unlikely without it. I think what's best is to deal with the issues\non their merits.\n\n> This means that any features that have not been independently implemented by\n> at least two vendors do not get included in the draft standard.\n\nTo be precise, 'implementors' don't have to be 'vendors'.\n\n> Moving from Proposed to Draft is not a chance to put in new features that we\n> now realize are useful based on our experience with RFC 2068.\n\nThat's right. We've mainly 'undocked' these extra features.\n\n> This is only\n> a chance to fix known problems, \n\nyes \n\n> in a backwards compatible way. \n\nwhen possible.\n\n> If you want\n> to add a new optional feature, write up an RFC and put it on standards\n> track. \n\nI agree 100%. This is preferable.\n\n> If you want to put in a new mandatory feature or change expected\n> protocol behavior then rev the protocol number.\n\nIf necessary; revving the protocol number is really only necessary\nif it improves compatibility.\n\n> But if a proposed change\n> fails any of the previous three tests then it has no business being put into\n> the draft standard.\n\nThe header/trailer issue is something where 2068 proposed something\nbut it was unclear. Accept-TE seemed like a way to deal with the trailer\nissue as well as hit a few other needs. I'm wary of putting it in\nthe main spec, especially since it's optional, but moving it forward\nindependently seems like a good idea.\n\nSimilarly, the error-code draft was not being proposed as a part of the\nHTTP/1.1 spec, but as an independent draft. We're trying to tie up\nthe loose ends, and this is one of them.\n\nPlease state your opinion on the issues! It's easy to deep-end on this\nprocess stuff.\n\nDo you like \"Accept-TE\" or not? If you don't like it, why not?\n\nWe may still need to put it in a separate draft, but: Yaron, I couldn't\ntell, from your message, whether you approved or disapproved of the\nAccept-TE concept.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "Paul, I wonder how many of the browser, etc. should be clients instead?\nI don't see that this should be specific to browsers, though it\ncertainly is to deal with browser security issues.\n\nI'm adding an editor's note to the draft to this effect, and I'll add\na place holder to the security considerations section.\n\nI'm also renumbering to confuse the innocent.\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Changes to the spe",
            "content": "Please delay comments about the trailer/TE stuff until after this\nnext draft comes out.  I like what I see of it, now that I've put it in;\nit fits, it fixes a number of real problems cleanly, and leverages what's\nalready there.\n\nAnd lets not deep end on process right now; lets get the problems fixed,\npreferably in a backward's compatible way.\n- Jim\n\n\n\n"
        },
        {
            "subject": "I'm starting draft production right now..",
            "content": "And will no longer be making any edits to the document.\n\nI'll announce when the drafts are ready.\n- Jim\n\n\n\n"
        },
        {
            "subject": "Submissions of both HTTP/1.1 and Authentication drafts..",
            "content": "You all done good.  We're down to 4 substantive technical issues, by my \n(possibly optimistic) count.  Lets get this puppy put to bed soon, and complete \ninteroperability testing.\n\nI just submitted both the HTTP/1.1 and Authentication drafts to the internet \ndrafts editor; hopefully they won't drop them on the floor like they did \n60 or more drafts before the last IETF.\n\nDespite the time-stamp on my mail, I live on the east coast, (my mail lives \non the west coast, courtesy of Internet, Web, HTTP and Java magic) so it \nis rediculous in the morning....\n\nThere are several more hours work to get clean word and postscript files \nall set up on our web site, and I'm not going to do it before going to sleep.\n\nIn fact, it isn't clear it will happen necessarily happen\nbefore sometime on Monday; I may sleep all day today, and my family has\ndibs on me this weekend.\n\nHere's the summary of the issues list, which I'll post as well once\nI've got the links in it set up to the files.\n\n4 Open Technical issues:  RE-AUTHENTICATION-REQUESTED,\nPROXY-REDIRECT, CONTENT-ENCODING, OPTIONS \nMinimal OPTIONS is in Rev-01, PROXY-REDIRECT proposal that was in\nRev-00 draft (which interacts with OPTIONS) was removed from rev-01. \n\n8 Open Editorial issues: XREFS,  REQUEST_TARGET, UPDATE_ACKS,\nDISPOSITION, REQUIREMENTS, CLEAN_INDEXES, CODE_REGISTRY,\nIANA_DIRECTIONS \n\n4 Ready for Last Call (all edited into Rev-01): PUT-RANGE, \nRE-AUTHENTICATION-REQUESTED, RANGE_WITH_CONTENTCODING, TRAILER_FIELDS, \n\n12 Last call issues (all edited into Rev-01): DIGEST_SYNTAX,\nPROTECTION_SPACE, CONTRADICTION, IMS_INM_MISMATCH,\nCONNECTION_METHOD, BYTERANGE_SYNTAX,  RE-VERSION,  HOST,\nAUTH-CHUNKED,   REQUIRE-DIGEST,    DOCKDIGEST, \nGENERIC_MESSAGE, \n\n9 Technical Issues closed since Rev-00 (all edited into Rev-01): 301-302,\nREDIRECTS, CACHING-CGI, WARNINGS, DATE-IF-MODIFIED, 403VS404,\nAGE-CALCULATION (Yeah!), VARY, RANGE-ERROR \n\n12 Editorial issues closed since Rev-00 (all edited into Rev-01):\nREMOVE_19.6, GENERIC_MESSAGE, RANGE_OPTIONAL, EBNF, CODES,\nUTF-8, DOCKDIGEST, URL-SYNTAX, 1310_CACHE, MESSAGE-BODY,\nLENGTH_WORDING,  CLEANUP, \n\nHave a good day.  Have a good weekend.  See you in Washington.\nYour editor,\non behalf of the HTTP/1.1 working group,\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Question on AcceptCharse",
            "content": "I have a question on how to treat ISO 8859-1 in\nAccept-Charset and didn't find it discussed in the\narchives. \n\n> 14.2 Accept-Charset\n\n> The ISO-8859-1 character set can be assumed to be\n> acceptable to all user agents.\n>\n>        Accept-Charset = \"Accept-Charset\" \":\"\n>                  1#( ( charset | \"*\" [ \";\" \"q\" \"=\" qvalue ]\n> )\n>\n> Character set values are described in section 3.4. Each\n> charset may be given an associated quality value which\n> represents the user's preference for that charset. The\n> default value is q=1. An example is\n>\n>        Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n>\n> The special value \"*\", if present in the Accept-Charset\n> field, matches every character set (including ISO-8859-1)\n> which is not mentioned elsewhere in the Accept-Charset\n> field.    If no \"*\" is present in an Accept-Charset field,\n> then all character sets not explicitly mentioned get a\n> quality value of 0, except for ISO-8859-1, which gets a\n> quality value of 1 if not explicitly mentioned.\n\nIf a server receives\n\n  Accept-Charset: iso-8859-5, *;q=0\n\nis iso-8859-1 acceptable to the client?  What about:\n\n  Accept-Charset: iso-8859-5, iso-8859-1;q=0\n\nIn both cases I would assume it is not acceptable to the\nclient, but this seems to contradict the first sentence above.\nIf this is in fact the case, then I think the sentence\n\n> The ISO-8859-1 character set can be assumed to be\n> acceptable to all user agents.\n\nshould be removed.  Given the last paragraph, which I find\nquite clear, this sentence only adds confusion.\n\nHoward\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "At 22:35 20/11/97 +0100, Koen Holtman wrote:\n>And if you really want to send a 5 digit code like 45205 to a http\n>client without prior negotiation, you could always invent some wrapping\n>scheme with responses like\n>\n>HTTP/1.1 299 Extended status code\n>Status: 45205 Epibration complete\n>Content-type: text/html\n\nNice thought.  But if you're going to take this route you might as well go\nfurther and adopt RFC1893-style enhanced status codes, methinks.\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Re: Changes to the spe",
            "content": "Yaron slightly overstates the case:  RFC 2068 is a Proposed standard,\nnot a draft or full standard.  His statements are completely true\nfor a draft standard going to full standard.\n\nThe rule (as I understand it) for going from proposed to draft is: no new \nfunctionality; you can fix problems without having to recycle at proposed \nif changes are backwards compatible with the proposed standard.  If you \nfail this test, then you have to cycle at proposed standard.  We'd like \nto NOT have to cycle at proposed!\n\nSo I agree with Yaron's mail with one exception, Accept-TE, or TE, is to \nsolve an existing known problem in transfer codings in the proposed standard. \nIt is demonstrably broken (even in HTTP/1.0, if memory serves) with garbage \ngetting displayed on people's screen. The solution must be (I believe is) \nbackwards compatible with RFC 2068.\n\nThe first thing I did when becoming editor was to sit down with Scott\nBradner (Mr. IETF process himself), to understand what the rules of the\ngame were.  I recommend others likewise understand IETF process if they\never find themselves as editor.\n - Jim\n\n\n\n"
        },
        {
            "subject": "Re: FW: Proposed amendment to RFC210",
            "content": "I certainly hope we can discuss this in Memphis, whether\nas part of the agenda for the working group or in a bar bof.\nMy concern is that this proposal seems to address\na question not basic to the concerns about the restrictions\non domain.  In short, it seems to heighten the ability of a server\nsending a cookie to verify its identity, without doing a\nwhole lot to explicate the relationship between the cookie\nissuer and content provider.  I can see ways in which this\nmechanism could be used, but I'm not sure that my\nexamples are part of your intended design.\n\nYour base design seems to assume that Certifying\nAuthorities will emerge which will certify not just\nan organization's identity but its adherence to an\nestablished set of guidelines on the use of the data\nwhich it receives.  This seems to combine the x509\ncertificate with something which would require\na much bigger process.  Not ISO 9000, maybe, but\na significant amount of work, as it involves verifying\ninternal processes--not just proofs of identity.\nThe emergence os trustworthy CA's willing to\ntake that on seems problematic.\n\nThere may be a way around that, by drawing on the\nexisting relationships and setting things up so that\nthe assurance of certification was inherent in the\ncontent-provider/cookie issuer relationship.\nIf, for example, we imagine that cookie issuers make the\ncontent-provider the cookie-issuer's certifying authority\nfor a particular cookie, then allowing cookies when the\ncertifying authority domain matches the content-provider\nmakes a certain amount of sense.   Doing so, however, would\nrequire a whole new set of CA's, the acceptance of which\nin the cookie context should probably not be extended to\nother contexts.  It also requires a method of allowing the\nUA to display this new relationship.\n\nTo rephrase this, I don't think users have a problem\nbelieving that \"tripleclick\" is who it says it is when\nthey receive a cookie from \"tripleclick.net\".\nI think what they need to see is how tripleclick relates\nto the current and other content providers.   Using\nthe inter-relationships among x509 certifying\nauthorities may be one way of getting the relationships\nspecified, but it is a at least moderately complex way\nthat still needs to be made visible to the end user.\n\nregards,\nTed Hardie\nNASA NIC\n\nNB:  NASA isn't confused about this, I am.\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 draft rev01 and authentication draft 00 issued...",
            "content": "See http://www.w3.org/Protocols/HTTP/Issues/ for details, and links\nto a plethora of files in various formats...\n\nShould be there by 5:00 EST; Henrik is publishing the files now,\nand checking the links.\n\nThanks to the working group for all your help.\n\nLets get this wrapped up now; we're down, at least in my optimistic\ncount, to the fingers of one hand...\n\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: REVERSION discussion at Munich...",
            "content": "To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\n\n\nOn Mon, 17 Nov 1997, Jim Gettys wrote:\n\n> \n> Given the heat that this generated at Munich, I'd appreciate it\n> if others read Josh's mail, and proposed change to close out the issue.\n> Henry, do you agree?\n> \n> > \"Proxy servers MUST upgrade all requests to the highest\n> > version supported by the proxy\"\n\nI don't agree ... it makes no sense to force a proxy to subject itself\nto the requirement that it unchunk a response, etc. Replace MUST with\nMAY or even SHOULD. This should be a proxy implementor's design \ndecision.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "New mailing list on 'content negotiation",
            "content": "In preparation for the December BOF on content negotiation,\nPaul Hoffman has graciously set up a new mailing list:\n\n  ietf-conneg@imc.org\n\n\nTo subscribe, send a message with the body 'subscribe'\nto ietf-conneg-request@imc.org. You'll get a reply as follows.\n(The list is not yet open, but will be in a few days.)\n\nLarry\n-------------------------------------------------------------------\n\n> The ietf-conneg mailing list is to discuss negotiating elements of the\n> presentation of documents that are not naturally captured by the MIME Media\n> Type.\n> \n> To see what has gone on before you subscribed, please see the mailing\n> list archive at\n>   http://www.imc.org/ietf-conneg/\n> \n> To unsubscribe from the ietf-conneg mailing list, send a message to:\n>   ietf-conneg-request@imc.org\n> with the message:\n>   unsubscribe\n> \n> If you need to contact a human about this mailing list, please\n> send a message to:\n>   phoffman@imc.org\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "1. I agree that this facility is required.\n2. However, it is incomplete.  W/o a corresponding mechanism, there is\n   no practical way for a server or proxy to know when the proposed \n   420/421 reauthentication should be requested.\n\nSo, the timeout concept must be included, with the server providing the\ntimeout. At least optionally, the timeout should be specified as an\nidle period, and not absolute value. That is, if the credential isn't\nused (sent) within X seconds, it must be discarded.\n\nSecondly, an additonal mechanism is required to allow the server to\nforce flushing of credentials w/o the implication of an immediate user\nprompt. In many applications, there is an explicit notion of logoff.\nSome form of response header to say flush credentials. Might be as\nsimple as the challenge header with a 2xx response to mean this is\na valid response but a new user interaction is required before sending\ncredentials to the server.\n\nDave Morris\n\nOn Thu, 20 Nov 1997, Paul Leach wrote:\n\n> Based on feedback, and the epiphany that screen savers do the same thing as\n> I proposed the browser do, I withdraw the proposed modification to section\n> 11. Something like it should go in the security considerations section --\n> Jim, can you mark it as an editorial issue, not for this revision?\n> \n> I also added text about an error message, and what happens with browsers\n> that don't understand the new status code.\n> \n> Revised proposal:\n> \n> > Add sections 10.4.19 and 10.4.20\n> > \n> > ==============================\n> > \n> > 10.4.19 420 Reauthentication Required\n> > \n> > This header is similar to \"401 Unauthorized\", except that the user agent\n> > MUST request credentials from the user before resubmitting the request,\n> > even\n> > if the challenge is the same as on a prior response or if the user agent\n> > has\n> > already obtained credentials from the user. The user agent should not\n> > assume\n> > that the current credentials are invalid if the request contained an\n> > Authorization header. The server can use this status code to cause the\n> > browser to verify that the current user is the same as the one who\n> > supplied\n> > the original credentials (say, after a period of inactivity). The server\n> > SHOULD send an entity-body\n> explaining the reason for requiring reauthentication, because user agents\n> that do not understand the status code will treat it as a generic 400 error\n> and display\n> the message.\n> \n> > 10.4.20 421 Proxy Reauthentication Required\n> > \n> > This header is similar to \"407 Proxy Aauthentication Required\", except\n> > that\n> > the user agent MUST request credentials from the user before resubmitting\n> > the request, even if the challenge is the same as on a prior response or\n> > if\n> > the user agent has already obtained credentials from the user.  The user\n> > agent should not assume that the current credentials are invalid if the\n> > request contained an Proxy-Authorization header. The server can use this\n> > status code to cause the browser to verify that the current user is the\n> > same\n> > as the one who supplied the original credentials (say, after a period of\n> > inactivity). The server SHOULD send an entity-body\n> > explaining the reason for requiring reauthentication, because user agents\n> > that do not understand the status code will treat it as a generic 400\n> > error and display\n> > the message.\n> > \n> > \n> > ==================================\n> > \n> \n> \n\n\n\n"
        },
        {
            "subject": "[Fwd: WEBPRIV BOF for D.C. IETF",
            "content": "FYI, there may be a separate BOF (in the User Services Area)\nabout Web Browsing and user privacy.\n\nattached mail follows:\n\n\nFolks,\n\nMarcia and I (well, mostly Marcia) will be trying to cram into a slot\non the D.C. agenda for this BOF.  Larry is willing to run it.  FYI, and\nstay tuned.\n\nThanks, Joyce\n\n----- Begin Included Message -----\n\n\nSubject: IETF Agenda Request: WEBPRIV\n\nBOF Name: Web User Privacy: expectations and threats (WEBPRIV)\nArea: USV (Apps)\nConflicts: HTTP, URLREG, IPP, FAX, \nExpected Attendance: 40\n\nBOF description:\n\nThere are a number of methods in use that gather information about\nusers of products over the network. While many of these information\ngathering methods are benign, the potential for abuse is quite large.\nPopular reports of the privacy issues around web browsing have focused\non the issue of 'cookies': the use of the HTTP state management\nmechanism to send information about the user to a third party. However,\nthere are a large number of other possible threats. For example,\nsoftware packages, when downloaded and installed, may send the user's\nidentification and other information directly to the maker of the\nsoftware package, in the name of 'helping' with the identification of\nthe user's configuration.\n\nThe goal of the BOF is canvas for interest in creating a guideline to\nthreats to user privacy and an enumeration of the mechanisms and\npolicies that are necessary to avoid such threats.\n\n\n\n"
        },
        {
            "subject": "Fbk on state-man-mec04.tx",
            "content": "As I reviewed the new draft, I noted a few editorial comments which\nfollow:\n\n1.  In several places ``X'' is used, sometimes in the same paragraph\n    with \"X\". It seemed strange to use two different forms of double\n    quoting. I think it would be better to stick with one form.\n\n2.  In the first paragraph of section \"3. STATE AND SESSIONS\", the\n    phrase \"the technique\" implies a reference to a technique which\n    hasn't been defined.  And in fact, the phrase could refer to\n    either the new proposal OR the 'existing' methodology.\n\n    I was also uncomfortable with the word \"currently\" since Netscape\n    cookies have been in use now for a long time and most readers would\n    consider the current time frame to include Netscape cookies.\n\n    Perhaps replace \"Currently, HTTP servers\" with \"HTTP servers \n    conforming to RFC 2068\"\n\n3.  I believe \"4. Outline\" and the introductory phrase \"We outline\" was\n    already noted by Roy and acknowledged. \n\n4.  In the same section 4 introductory paragraph, I agree with the\n    suggestion already made that most of the paragraph could just be\n    deleted (keep 1st two sentences). The reference to CGI programs\n    should be deleted in any case.\n\n5.  In 4.2.1 I think it would be better to drop the first parenthetic\n    note which attempts to differentiate persistent connections from\n    the term session.  At least drop the first sentence and change\n    \"should have no effect\" to \"has no effect\".\n\n6.  In 4.2.2, in the description of NAME=VALUE, it is redundant to\n    say \"$ are reserved\" and \"for other users\".  Drop the second part.\n\n7.  In 4.3.3, the phrase \"then gets discarded\" isn't needed.  The\n    cookie persists until X happens is sufficient.\n\n8.  I don't see the need for the restriction stated in the last sentence\n    of section 4.5:\n      Proxies must not introduce Set-Cookie2 (Cookie) headers of their\n      own in proxy responses (requests). \n    I'd rather have this dropped completely but if not, then change must \n    to should.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "HTTP question",
            "content": "Hi Everybody!\n\nI am doing research on Web traffic modeling at G.I.T.\nWith certain Web page(www.cnnfn.com), the client continues to re-connect\nwith that page in exact every 180 second automatically. This situaion was\nfound on\ntcpdump of dial-up Web traffic. \n\nThe user in a client watches the\ntime and press the button ??  It is not a possible scenario.\n\nIf anyone is able to answer to the question, it would help me a lot.\nThank you in advance.\n\nPGP public key is available upon request.\n\nCHK\nURL: http://www.ee.gatech.edu/users/hkchoi\n\n\n\n"
        },
        {
            "subject": "Re: Fbk on state-man-mec04.tx",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n\n[General note:  I submitted state-man-mec-05 last Thursday, but I\nhaven't seen the announcement through official channels.  You can\nlook at it via <http://portal.research.bell-labs.com/~dmk/cookie.html>.\nItems that have been addressed there are labeled \"-05\".]\n\n  > As I reviewed the new draft, I noted a few editorial comments which\n  > follow:\n  > \n  > 1.  In several places ``X'' is used, sometimes in the same paragraph\n  >     with \"X\". It seemed strange to use two different forms of double\n  >     quoting. I think it would be better to stick with one form.\n\nI agree.  (They've been like that for hearly two years.  Where have you\nbeen. :-)\n\n  > \n  > 2.  In the first paragraph of section \"3. STATE AND SESSIONS\", the\n  >     phrase \"the technique\" implies a reference to a technique which\n  >     hasn't been defined.  And in fact, the phrase could refer to\n  >     either the new proposal OR the 'existing' methodology.\n\nDitto.\n\n  > \n  >     I was also uncomfortable with the word \"currently\" since Netscape\n  >     cookies have been in use now for a long time and most readers would\n  >     consider the current time frame to include Netscape cookies.\n\n-05\n\n  > \n  >     Perhaps replace \"Currently, HTTP servers\" with \"HTTP servers \n  >     conforming to RFC 2068\"\n  > \n  > 3.  I believe \"4. Outline\" and the introductory phrase \"We outline\" was\n  >     already noted by Roy and acknowledged. \n\nOkay.\n\n  > \n  > 4.  In the same section 4 introductory paragraph, I agree with the\n  >     suggestion already made that most of the paragraph could just be\n  >     deleted (keep 1st two sentences). The reference to CGI programs\n  >     should be deleted in any case.\n\n-05\n\n  > \n  > 5.  In 4.2.1 I think it would be better to drop the first parenthetic\n  >     note which attempts to differentiate persistent connections from\n  >     the term session.  At least drop the first sentence and change\n  >     \"should have no effect\" to \"has no effect\".\n\nOkay.  Now that I've removed some of the introductory stuff that mentions\n\"persistent\", there's less risk of confusing the two concepts.\n\n  > \n  > 6.  In 4.2.2, in the description of NAME=VALUE, it is redundant to\n  >     say \"$ are reserved\" and \"for other users\".  Drop the second part.\n\nOkay.\n\n  > \n  > 7.  In 4.3.3, the phrase \"then gets discarded\" isn't needed.  The\n  >     cookie persists until X happens is sufficient.\n\nI agree it's redundant, but I want to be clear that the cookie should\nbe thrown away at that point.\n\n  > \n  > 8.  I don't see the need for the restriction stated in the last sentence\n  >     of section 4.5:\n  >       Proxies must not introduce Set-Cookie2 (Cookie) headers of their\n  >       own in proxy responses (requests). \n  >     I'd rather have this dropped completely but if not, then change must \n  >     to should.\n\nBasically I'm emphasizing that cookies are end-to-end, and that proxies\nshouldn't be inserting them.  I think the restriction is appropriate.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: WG Last Call for draft-schulzrinne-http-status00.tx",
            "content": "Koen:\n\n% And if you really want to send a 5 digit code like 45205 to a http\n% client without prior negotiation, you could always invent some wrapping\n% scheme with responses like\n% \n% HTTP/1.1 299 Extended status code\n% Status: 45205 Epibration complete\n% Content-type: text/html\n% ....\n\nAgreed.\nI fear that working mod 1000 is a far worse kludge, and it risks to\nbreak current implementations.\n\n.mau.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP question",
            "content": "What you have encountered is a Netscape extension which is also\nimplemented MSIE ... the REFRESH header. This is typically generated\nusing the HTML <META> http-equiv markup rather than real HTTP headers.\n\nThe feature is not part of 'official' HTTP.\n\nThe really onerous aspect of refresh is the usage you have noticed where\nthe refresh continues to loop long after the user has gone to lunch, \netc. I discovered this usage of refresh when I went to track down\nwhy my ondemand ISDN connection was remaining active hours after anyone\nhad been knowingly using the Internet.  The page in question had\nan advertising banner which was being refreshed every 20 minutes\nthereby generating many false hits against the advertisement (my believe\nis that it was fraud but the banner company apparently didn't care as\nthey ignored my mail).\n\nBy luck, I noticed the ISDN activity, otherwise even the usage pennies\nwould have added up.\n\nIf this, or a future IETF group, should consider this header for\nstandarization, I will be quite vocal in my crusade to have limits\nset on the number of refreshes between human intervention or perhaps\nthe amount of time it continues.\n\nThe refresh header provides a very useful facility but it is also \nsubject to the potential for serious abuse in terms of useless network\ntraffic and usage cost to end users who no clue what is happening.\n\nDave Morris\n\nOn Mon, 24 Nov 1997, Choi, Hyoung-Kee wrote:\n\n> Hi Everybody!\n> \n> I am doing research on Web traffic modeling at G.I.T.\n> With certain Web page(www.cnnfn.com), the client continues to re-connect\n> with that page in exact every 180 second automatically. This situaion was\n> found on\n> tcpdump of dial-up Web traffic. \n> \n> The user in a client watches the\n> time and press the button ??  It is not a possible scenario.\n> \n> If anyone is able to answer to the question, it would help me a lot.\n> Thank you in advance.\n> \n> PGP public key is available upon request.\n> \n> CHK\n> URL: http://www.ee.gatech.edu/users/hkchoi\n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: HTTP question",
            "content": "Choi, Hyoung-Kee wrote:\n> \n> Hi Everybody!\n> \n>         I am doing research on Web traffic modeling at G.I.T.\n> With certain Web page(www.cnnfn.com), the client continues to re-connect\n> with that page in exact every 180 second automatically. This situaion was\n> found on\n> tcpdump of dial-up Web traffic.\n\nLooks like they've changed it to 600 seconds, but I think you'll find\nthat this has something to do with it:\n\n<META http-equiv=\"refresh\" content=\"600, URL=/\">\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "How the server does it's timeout is completely up to it, or more precisely,\nup to the application that uses the server.\n\nAs far as I can tell, the people who want this have quite well formed ideas\nas to how long the timeout should be, so we don't need to include\nguidelines.\n\nAs to the second suggestion, which I'll call  \"2xx Logout\", I'm agnostic,\nand await more WG feedback.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "My point is that the server HAS NO WAY to perform a timeout on its own\nwithout someform of state tracking.  By providing a timeout to the\nclient, the server doesn't need to introduce some other form of\nstate management.\n\nOn Mon, 24 Nov 1997, Paul Leach wrote:\n\n> How the server does it's timeout is completely up to it, or more precisely,\n> up to the application that uses the server.\n> \n> As far as I can tell, the people who want this have quite well formed ideas\n> as to how long the timeout should be, so we don't need to include\n> guidelines.\n> \n> As to the second suggestion, which I'll call  \"2xx Logout\", I'm agnostic,\n> and await more WG feedback.\n\n\n\n"
        },
        {
            "subject": "Administrivia: running on automati",
            "content": "Folks,\n\nAs I'm off to the bay area for a conference, the http-wg list will be running\non script power alone for two weeks.\n\nIf mail loops happen or people start pestering the list with unsubscription\nrequests, then please be patient and direct them to the correct place.\n\nThanks... and perhaps see you in Santa Clara...\n-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "How about cookies? I've heard they are useful for tracking state... :-)\n\nAs I understand it:  cookie has a magic number in it. Magic number is index\ninto a table at the server. Table has timeout information.\n\n> ----------\n> From: David W. Morris[SMTP:dwm@xpasc.com]\n> Sent: Monday, November 24, 1997 10:07 AM\n> To: Paul Leach\n> Cc: 'http-wg'; 'Jim Gettys'; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: RE: REAUTHENTICATION REQUIRED\n> \n> My point is that the server HAS NO WAY to perform a timeout on its own\n> without someform of state tracking.  By providing a timeout to the\n> client, the server doesn't need to introduce some other form of\n> state management.\n> \n> On Mon, 24 Nov 1997, Paul Leach wrote:\n> \n> > How the server does it's timeout is completely up to it, or more\n> precisely,\n> > up to the application that uses the server.\n> > \n> > As far as I can tell, the people who want this have quite well formed\n> ideas\n> > as to how long the timeout should be, so we don't need to include\n> > guidelines.\n> > \n> > As to the second suggestion, which I'll call  \"2xx Logout\", I'm\n> agnostic,\n> > and await more WG feedback.\n> \n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "On Mon, 24 Nov 1997, Paul Leach wrote:\n\n> How about cookies? I've heard they are useful for tracking state... :-)\n> \n> As I understand it:  cookie has a magic number in it. Magic number is index\n> into a table at the server. Table has timeout information.\n\nCookies are one way to maintain state, munged URLs are another. Both\nare more complex than needed if the client is simply given a timeout.\n\nServers which want more precision or actually require a stateful\ninteraction will of course maintain their own timeouts. But for basic\naccess to a secured set of WEB resources, having the client provide\nthe timing keeps everything simpler.\n\n\n> \n> > ----------\n> > From: David W. Morris[SMTP:dwm@xpasc.com]\n> > Sent: Monday, November 24, 1997 10:07 AM\n> > To: Paul Leach\n> > Cc: 'http-wg'; 'Jim Gettys'; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> > Subject: RE: REAUTHENTICATION REQUIRED\n> > \n> > My point is that the server HAS NO WAY to perform a timeout on its own\n> > without someform of state tracking.  By providing a timeout to the\n> > client, the server doesn't need to introduce some other form of\n> > state management.\n> > \n> > On Mon, 24 Nov 1997, Paul Leach wrote:\n> > \n> > > How the server does it's timeout is completely up to it, or more\n> > precisely,\n> > > up to the application that uses the server.\n> > > \n> > > As far as I can tell, the people who want this have quite well formed\n> > ideas\n> > > as to how long the timeout should be, so we don't need to include\n> > > guidelines.\n> > > \n> > > As to the second suggestion, which I'll call  \"2xx Logout\", I'm\n> > agnostic,\n> > > and await more WG feedback.\n> > \n> \n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "Two comments:\n\nCertain popular web servers have a builtin \"session\" mechanism, so that what\nthe server needs to do has already been implemented.\n\nThe guys who want this want to trust the browser as little as possible. A\nbrowser that doesn't understand the timeout directive would ignore it. A\nbrowser that doesn't understand \"4xx reauth required\" will consider it a\nfatal error. They like that default.\n\n\n> ----------\n> From: David W. Morris[SMTP:dwm@xpasc.com]\n> Sent: Monday, November 24, 1997 10:34 AM\n> To: Paul Leach\n> Cc: 'http-wg'; 'Jim Gettys'; 'http-wg'\n> Subject: RE: REAUTHENTICATION REQUIRED\n> \n> \n> \n> On Mon, 24 Nov 1997, Paul Leach wrote:\n> \n> > How about cookies? I've heard they are useful for tracking state... :-)\n> > \n> > As I understand it:  cookie has a magic number in it. Magic number is\n> index\n> > into a table at the server. Table has timeout information.\n> \n> Cookies are one way to maintain state, munged URLs are another. Both\n> are more complex than needed if the client is simply given a timeout.\n> \n> Servers which want more precision or actually require a stateful\n> interaction will of course maintain their own timeouts. But for basic\n> access to a secured set of WEB resources, having the client provide\n> the timing keeps everything simpler.\n> \n> \n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "On Mon, 24 Nov 1997, Paul Leach wrote:\n\n> Two comments:\n> \n> Certain popular web servers have a builtin \"session\" mechanism, so that what\n> the server needs to do has already been implemented.\n\nTrue, but the session mechanism I am intimately familiar with on one\npopular web server doesn't apply for all cases where basic authentication\ncan be used. To activate that session management would require the\naddition of additional logic.  Furthermore, there is quite a bit of\nserver overhead associated with session management AND there is \nsome degree of end user and developer resistance to use of cookies and\nsessions when there isn't an obvious need.\n\n\n> The guys who want this want to trust the browser as little as possible. A\n> browser that doesn't understand the timeout directive would ignore it. A\n> browser that doesn't understand \"4xx reauth required\" will consider it a\n> fatal error. They like that default.\n\nAs far as I'm concerned, that still represents a degree of trust.\n\nAs far as 'the guys' who want this is concerned, I believe you are\ndescribing a specific set of guys. I can assure you that there would\nalso be quite a bit of interest in the timeout and 2xx logoff additions\nI've suggested based on several months of following the 2500 subscriber\nmailing list associated with a common server.\n\nIn any case, I'm not objecting to the original \"4xx reauth required\"\nproposal except that I believe it is quite incomplete in terms of the\nneeds/desires of the HTTP based application community.\n\n> \n> \n> > ----------\n> > From: David W. Morris[SMTP:dwm@xpasc.com]\n> > Sent: Monday, November 24, 1997 10:34 AM\n> > To: Paul Leach\n> > Cc: 'http-wg'; 'Jim Gettys'; 'http-wg'\n> > Subject: RE: REAUTHENTICATION REQUIRED\n> > \n> > \n> > \n> > On Mon, 24 Nov 1997, Paul Leach wrote:\n> > \n> > > How about cookies? I've heard they are useful for tracking state... :-)\n> > > \n> > > As I understand it:  cookie has a magic number in it. Magic number is\n> > index\n> > > into a table at the server. Table has timeout information.\n> > \n> > Cookies are one way to maintain state, munged URLs are another. Both\n> > are more complex than needed if the client is simply given a timeout.\n> > \n> > Servers which want more precision or actually require a stateful\n> > interaction will of course maintain their own timeouts. But for basic\n> > access to a secured set of WEB resources, having the client provide\n> > the timing keeps everything simpler.\n> > \n> > \n> \n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": ">>>>> \"DWM\" == David W Morris <dwm@xpasc.com> writes:\n\nDWM> 2. However, it is incomplete.  W/o a corresponding mechanism, there is\nDWM>    no practical way for a server or proxy to know when the proposed\nDWM>    420/421 reauthentication should be requested.\n\nDWM> So, the timeout concept must be included, with the server providing the\nDWM> timeout. At least optionally, the timeout should be specified as an\nDWM> idle period, and not absolute value. That is, if the credential isn't\nDWM> used (sent) within X seconds, it must be discarded.\n\n  I'm not sure that I understand this comment.  The discussion of a\n  timeout in the proposed addition was strictly an example of the use\n  of this code, not a part of the definition of its function.\n\nDWM> Secondly, an additonal mechanism is required to allow the server to\nDWM> force flushing of credentials w/o the implication of an immediate user\nDWM> prompt. In many applications, there is an explicit notion of logoff.\nDWM> Some form of response header to say flush credentials. Might be as\nDWM> simple as the challenge header with a 2xx response to mean this is\nDWM> a valid response but a new user interaction is required before sending\nDWM> credentials to the server.\n\n  This is an excellent suggestion.  I put forward the following\n  suggested wording.\n\n   2xx Discard Credentials\n\n     This status indicates that the request succeeded, but that the user\n     credentials used to construct the Authorization header field for the\n     request MUST be discarded.  This allows web based applications to\n     implement a 'logout' function.  The body of the response should\n     in all other ways be treated as though the response code were\n     '200 Ok'.\n\n  This is completely backward compatible, since clients that don't\n  understand it should already just treat it as a 200, they just won't\n  discard cached credentials, which is the current situation anyway.\n\n  Yes - users can just walk away without using an offered 'logout'\n  function, but in my view that is no reason not to provide the\n  capability.  I've been getting 2 or 3 pieces of email a week for\n  the last couple of months from developers trying to figure out how\n  to do this, and I haven't had any answer for them.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Submissions of both HTTP/1.1 and Authentication drafts..",
            "content": "I'd like to thank Jim Gettys for his heroic efforts in getting\nthe HTTP/1.1 spec out. We may yet succeed in completing our work\nin 1997! If we can close the remaining issues and document\ninteroperable implementations, we can declare success.\n\nI imagine that we will keep the mailing list open for discussion\nof HTTP protocol issues; substantive remaining issues will\nbe candidates for forming smaller, more focused working groups.\nNew proposals can be considered as individual contributions\nas well. There's already sufficient movement for me to feel\nconfident that we can start working groups on:\n  - privacy (within USV/APPS joint)\n  - state management\n  - content negotiation\nand that HTTP-NG protocol and design are moving forward within\nthe W3C, but with an intention to bring those forward as IETF\nproposals for review.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "On Mon, 24 Nov 1997, Scott Lawrence wrote:\n\n> \n> >>>>> \"DWM\" == David W Morris <dwm@xpasc.com> writes:\n> \n> DWM> 2. However, it is incomplete.  W/o a corresponding mechanism, there is\n> DWM>    no practical way for a server or proxy to know when the proposed\n> DWM>    420/421 reauthentication should be requested.\n> \n> DWM> So, the timeout concept must be included, with the server providing the\n> DWM> timeout. At least optionally, the timeout should be specified as an\n> DWM> idle period, and not absolute value. That is, if the credential isn't\n> DWM> used (sent) within X seconds, it must be discarded.\n> \n>   I'm not sure that I understand this comment.  The discussion of a\n>   timeout in the proposed addition was strictly an example of the use\n>   of this code, not a part of the definition of its function.\n\nMy point is simply that I believe there are many cases where a site is\nsecured and the owners are concerned about stale credentials but there is\nno reason to impose any form of state management on access to the server.\nWithout some form of statemanagement, to correlate users to timeout\ntables on the server, there is no way for a server to know when to \nissue the reauthentication challange.\n\nHence my suggestion that the original proposal be extended to provide \na credential timeout implemented by the client.\n\n\n> DWM> Secondly, an additonal mechanism is required to allow the server to\n> DWM> force flushing of credentials w/o the implication of an immediate user\n>[...] \n>   This is an excellent suggestion.  I put forward the following\n>   suggested wording.\n> \n>    2xx Discard Credentials\n> \n>      This status indicates that the request succeeded, but that the user\n>      credentials used to construct the Authorization header field for the\n>      request MUST be discarded.  This allows web based applications to\n>      implement a 'logout' function.  The body of the response should\n>      in all other ways be treated as though the response code were\n>      '200 Ok'.\n> \n>   This is completely backward compatible, since clients that don't\n>   understand it should already just treat it as a 200, they just won't\n>   discard cached credentials, which is the current situation anyway.\n> \n>   Yes - users can just walk away without using an offered 'logout'\n>   function, but in my view that is no reason not to provide the\n>   capability.  I've been getting 2 or 3 pieces of email a week for\n>   the last couple of months from developers trying to figure out how\n>   to do this, and I haven't had any answer for them.\n\nYour proposed wording for 2xx sounds OK to me. At least 1 post per week\nto the MS IIS/ASP mail list asks the question as well.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Updated Issues list..",
            "content": "The April Fool's blizzard did not prevent me from getting the issues again\nbefore Memphis after all.  It did prevent me from trying to get it\nsorted into some sort of priority order.  Oh, well...\n\nThis includes (barring errors on my part), all known problems with\nthe HTTP/1.1 specification.\n\nAs usual, the issues list is at:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "As a general rule, do not use a status code to indicate something which\nis not status -- doing so breaks orthogonal design issues.\nIf the same functionality can be accomplished with a header field\n(new or existing), then do that instead.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": ">>>>> \"RTF\" == Roy T Fielding <fielding@kiwi.ics.uci.edu> writes:\n\nRTF> As a general rule, do not use a status code to indicate something which\nRTF> is not status -- doing so breaks orthogonal design issues.\nRTF> If the same functionality can be accomplished with a header field\nRTF> (new or existing), then do that instead.\n\n  You make an excellent point.  We could add this to the\n  Authentication-Info header as follows:\n\n       3.2.3 The Authentication-Info Header\n\n       When authentication succeeds, the server may optionally provide\n       an Authentication-Info header indicating that the server wants\n       to communicate some information regarding the successful\n       authentication.\n\n            AuthenticationInfo = \"Authentication-Info\" \":\"\n                                ( discard | 1#( digest | nextnonce ) )\n\n            discard            = \"discard\"\n            nextnonce          = \"nextnonce\" \"=\" nonce-value\n            digest             = \"digest\" \"=\" entity-digest\n\n       If the 'discard' indication is sent in a response, the client\n       MUST discard any user credentials which were used to\n       authenticate the corresponding request.  The client MAY retain\n       the realm value associated with the request so that it can\n       prompt the user for new credentials before transmitting a\n       subsequent request (note that this only saves a round trip for\n       authentication schemes which do not utilize a nonce from the\n       server).\n\n       [... and a corresponding addition to the description of Proxy\n        authentication]\n\n  Note that this makes the Authentication-Info header field applicable\n  to Basic authentication as well as Digest (but only the 'discard'\n  value would be meaningfull in Basic).\n\n  As Roy points out, this has the advantage that it can be combined\n  with any status value.  I believe that it is backward compatible IFF\n  deployed client parsers for the Authentication-Info header have been\n  written such that they can tolerate and ignore an unexpected value\n  (the robustness principle would suggest that they should have been).\n  Since I am aware of only a couple of clients that have implemented\n  Digest this is easy to check.  If this turns out to be the case then\n  the backward compatibility is no worse than when using status\n  values; that is, it will be ignored by existing implementations,\n  leaving credentials lying around, which is the only alternative\n  today anyway.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "Recycling existing authentication techniques, a server can change the\nrealm under which a namespace is protected over time while authenticating\nagainst the same credentials.  After the server timed out authorization,\nit could challenge a client against a different realm over the same\nPATH_INFO namespace (the realm perhaps corresponding to\n$domain.$timestamp) and force verifiable reauthentication. \n\nOnly the most reckless clients would try to guess that a set of distinct\nrealms over the same namespace were \"similar\"  enough to reuse\ncredentials.  Sloppy clients could make a much safer bet and reuse\ncredentials for the same realm, same namespace case, effectively ignoring\nthe proposed message.  I experimented with this a few years ago with\nMosaic and Netscape (v <= 2.0) and I recall that they both stacked up\nrealms and would send as many authorization responses as realms\nauthorized. \n\nThe implementation costs on the server side would be only slightly more\nexpensive than keeping enough state to know when to send a\nREAUTHENTICATION REQUIRED message.\n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "Scott answers Roy:\n\n\n% RTF> As a general rule, do not use a status code to indicate something which\n% RTF> is not status -- doing so breaks orthogonal design issues.\n% RTF> If the same functionality can be accomplished with a header field\n% RTF> (new or existing), then do that instead.\n% \n%   You make an excellent point.  We could add this to the\n%   Authentication-Info header as follows:\n% \n%        3.2.3 The Authentication-Info Header\n% \n%        When authentication succeeds, the server may optionally provide\n%        an Authentication-Info header indicating that the server wants\n%        to communicate some information regarding the successful\n%        authentication.\n\nThere is a thing I still do not understand. \n\nAs some pointed out, often it is the client, not the server, which \nwould like to forget the auth info (but this does not belong to HTTP); \nmoreover the server cannot be sure that the client forgets the infos.\n\nThis all said, shouldn't the server send a cookie (oops, wrong term :-))\nwhich the client should send back together with the usual Authentication:\ndata? \n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n\n\n\nOn Tue, 25 Nov 1997, Marc Salomon wrote:\n\n> Recycling existing authentication techniques, a server can change the\n> realm under which a namespace is protected over time while authenticating\n> against the same credentials.  After the server timed out authorization,\n> it could challenge a client against a different realm over the same\n> PATH_INFO namespace (the realm perhaps corresponding to\n> $domain.$timestamp) and force verifiable reauthentication. \n> \n> Only the most reckless clients would try to guess that a set of distinct\n> realms over the same namespace were \"similar\"  enough to reuse\n> credentials.  Sloppy clients could make a much safer bet and reuse\n> credentials for the same realm, same namespace case, effectively ignoring\n> the proposed message.  I experimented with this a few years ago with\n> Mosaic and Netscape (v <= 2.0) and I recall that they both stacked up\n> realms and would send as many authorization responses as realms\n> authorized. \n> \n> The implementation costs on the server side would be only slightly more\n> expensive than keeping enough state to know when to send a\n> REAUTHENTICATION REQUIRED message.\n\nThe problem with this suggestion is that it makes an already confusing\nand user unfriendly prompt even less obvious to the end user. Since \nthe realm is the only clue the server can give to the user as to what\nthey are being asked to login to, I dislike any scheme which requires\narbitrarily distinguished realm values.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": ">>>>> \"MC\" == Maurizio Codogno <mau@beatles.cselt.it> writes:\n\nMC> As some pointed out, often it is the client, not the server, which\nMC> would like to forget the auth info (but this does not belong to HTTP);\nMC> moreover the server cannot be sure that the client forgets the infos.\n\n  The proposal is to provide a mechanism whereby the server can direct\n  the client to discard the user credentials.\n\n  Clients should also have other mechanisms for doing the same things\n  - for example, there should always be some way for the user to\n  direct a browser to delete any stored credentials (so the user can\n  leave a shared system without leaving credentials for the next\n  user).\n\nMC> This all said, shouldn't the server send a cookie (oops, wrong term :-))\nMC> which the client should send back together with the usual Authentication:\nMC> data?\n\n  As I wrote the proposal, 'discard' can't be combined with other uses\n  of the Authentication-Info header, such as nextnonce; this may have\n  been a mistake.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: PUT with ContentRang",
            "content": "Paul Leach:\n\n% The basic idea is to allow PUT with Content-Range, and allow caches to apply\n% the update when possible to avoid having to refetch the whole entity-body,\n% and to create a model for (e.g.) WebDAV that wants to do partial updates on\n% resources.\n\nHow would it cope with plain append (which I suppose is the most \nuseful thing to do)?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: FW: Proposed amendment to RFC210",
            "content": "Quoting Ted Hardie:\n\n> My concern is that this proposal seems to address\n> a question not basic to the concerns about the restrictions\n> on domain.  In short, it seems to heighten the ability of a server\n> sending a cookie to verify its identity, without doing a\n> whole lot to explicate the relationship between the cookie\n> issuer and content provider.  I can see ways in which this\n\nI believe the idea is not only to authenticate the server, but also\nto certify a particular level of trust, and category of business\npractice in a way that, browser folks willing, will allows the user to \nset preferences that use guidelines from CA's to determine if cookies should be\naccepted or not.  Authenticating the server is really just one small\npart that guarantees that the credentials (the level of trust and catagory of\ndata handling practice) are not stolen, or misused.\n\nAs I understand it, the real problem with allowing cookies\nfrom EVERYWHERE is that it allows the collection of data about individuals\nwithout their knowledge or consent, and once collected, the user\nhas no idea what it's going to be used for, or by whom it will be used.\nThe proposal provides a mechanism of informing users of business and\ndata relationships other than just the ones dictated by a particular domain,\nand a mechanism that allows them to weigh the benefits and hazards of \naccepting cookies from entities.\n\n> Your base design seems to assume that Certifying\n> Authorities will emerge which will certify not just\n> an organization's identity but its adherence to an\n> established set of guidelines on the use of the data\n> which it receives.  This seems to combine the x509\n\nIndeed they are.  eTRUST is such a company (www.etrust.org), and there\nwill undoubtedly be others.  eTRUST will be using a combination of \nauditing by professional auditors like KPMG, and signed legal affidavits\nto track and qualify companies policies for handling data collected\nthrough their site, both through cookies and other means.\n \n> If, for example, we imagine that cookie issuers make the\n> content-provider the cookie-issuer's certifying authority\n> for a particular cookie, then allowing cookies when the\n\nThis is an interesting approach,  but it doesn't really scale very\nwell.  If you have everyone acting as a CA, there is no trust\nin the CA process.  Ideally you want to have a few CA's that can\nbe established as credible agents, otherwise there's no meaning\nin \"bob's CA and gas station\" guaranteeing you that \"evildoer.com\"\nis issuing cookies that will only be used to track clickthrough's.\n\n> To rephrase this, I don't think users have a problem\n> believing that \"tripleclick\" is who it says it is when\n> they receive a cookie from \"tripleclick.net\".\n> I think what they need to see is how tripleclick relates\n> to the current and other content providers.   Using\n\nYou're absolutely right.  But if tripleclick has signed a legal\ndocument and gone through an auditing process by a third party\nattesting that the only thing they are doing with their cookies\nis making sure that each visitor only sees the same add 3 times,\nand the third party CA says ok, you're in a \"class 1\" catagory,\nwhich means that you don't collect any personally identifiable\ninformation, you only deal with aggregate information, and \ntherefore people's privacy is pretty well protected if they\naccept tripleclick's cookies, then there should be no reason  not to\nallow cookies from tripleclick if the user chooses to accept\n\"class 1\" certified cookies.\n\nConversely, tripleclick may actually collect names and phone numbers\nand sell that information as targeted mailing lists based upon\nuser's preferences on the adds they sent, or they may work with\n\"Company B\" to compare logs with Copmany B and link the preferences \nthat tripleclick collected with user names that Company B collects.  In\nthat case, the third party CA may issue them a \"class 3\" certificate\nthat says that tripleclick actively trades personally identifiable\ninformation with other groups.  Under certain circumstances, the\nuser may want to accept class 3 certificates for the added value\nof getting more information about things they are interested in.  In \nmost cases, however, they are likely to not want to accept these cookies.\nThe point is that the user should have the choice of accepting the \ncookies and the information policies that they want.  Informed consent\nprior to divulging information is much better than arbitrarily limiting\nthe use of cookies by domains.  The use of cookies and the whole \nissue of trust and privacy change from company to company, situation \nto situation, and the user should be able to make informed decisions\nabout what they want to do in any particular set of situations.\n\nJonathan\n\n===============================================================\nJonathan Stark                             (415) 858 1930 x217\neTRUST Technical Director                  stark@eTRUST.org\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "\"Scott Lawrence\" <lawrence@agranat.com> writes:\n\n>  As I wrote the proposal, 'discard' can't be combined with other uses\n>  of the Authentication-Info header, such as nextnonce; this may have\n>  been a mistake.\n\nJust to be clear: You're proposing that a client receiving an\n\"Authentication-Info: Discard\" header in a response should discard the\ncredentials it supplied on the Authorization header in the request,\ncorrect?  Given everything Fote has been saying about the domain of\ncredentials over URLs, not to mention realm values, it's entirely\npossible for a browser to have a large cache of presumably-valid\ncredentials.  I don't think you're asking to have them all discarded, or\neven some subset of those associated with the origin server, correct?\n\nOne more point: Since Authentication-Info is defined by RFC 2069 as\nbeing sent when authentication is successful, this discard response must\nbe sent with a 2xx or 3xx status code and without a WWW-Authenticate\nheader.  In other words, an origin server is only allowed to request the\nclient to discard credentials at a time when they are acceptable to the\nserver.\n\nAnd I assume we'd want to support \"Proxy-Authentication-Info: Discard\"\nas well.\n\nI have to say that this is all starting to sound like something we\naren't allowed to do while trying to move HTTP/1.1 from Proposed to\nDraft Standard.  As much as the lack of a procedure for invalidating a\nclient's credentials has hurt my company's products (quite a bit!), we\nwould nonetheless be opposed to any change that would prevent the rapid\nadvancement of HTTP/1.1 to Draft Standard.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": ">>>>> \"RP\" == Ross Patterson <Ross_Patterson@ns.reston.vmd.sterling.com> writes:\n\nRP> \"Scott Lawrence\" <lawrence@agranat.com> writes:\n\n>> As I wrote the proposal, 'discard' can't be combined with other uses\n>> of the Authentication-Info header, such as nextnonce; this may have\n>> been a mistake.\n\nRP> Just to be clear: You're proposing that a client receiving an\nRP> \"Authentication-Info: Discard\" header in a response should discard the\nRP> credentials it supplied on the Authorization header in the request,\nRP> correct?  Given everything Fote has been saying about the domain of\nRP> credentials over URLs, not to mention realm values, it's entirely\nRP> possible for a browser to have a large cache of presumably-valid\nRP> credentials.  I don't think you're asking to have them all discarded, or\nRP> even some subset of those associated with the origin server, correct?\n\n  Yes, correct.  My intent was that the credentials used for the\n  request be discarded - regardless of what other protection spaces\n  they may have also applied to.  Credentials not used for the request\n  are not affected.\n\n  Example:\n\n    get /foo       ------------------>\n                   <----------------- 401 Unauthorized\n                                      WWW-Authenticate: realm=\"bar\"\n    (prompt user)\n    get /foo       ------------------>\n    Authorization: xxxxx\n                   <----------------- 200 Ok\n                                      ...\n    get /foo/bogus  ------------------>\n    Authorization: xxxxx\n                   <----------------- 200 Ok\n                                      ...\n    post /foo/bar  ------------------>\n    Authorization: xxxxx\n                   <----------------- 200 Ok\n                                      Authentication-Info: discard\n    (discard credentials use to build xxxxx)\n\n  At this point, if the user enters the URL for /foo/bogus, the user\n  agent may 'remember' that it needs to prompt for credentials in\n  realm 'bar' or it may just send a request and (presumably) get a 401\n  as usual.  The fact that 'discard' came in response to a request for\n  /foo/bar does not restrict the loss of credentials to that resource\n  - the credentials go away and must not be reused.\n\n  If people don't think that was clear, I invite improved wording.\n\nRP> One more point: Since Authentication-Info is defined by RFC 2069 as\nRP> being sent when authentication is successful, this discard response must\nRP> be sent with a 2xx or 3xx status code and without a WWW-Authenticate\nRP> header.  In other words, an origin server is only allowed to request the\nRP> client to discard credentials at a time when they are acceptable to the\nRP> server.\n\n  I had not made that specific, but I see no problem with it.\n\nRP> And I assume we'd want to support \"Proxy-Authentication-Info: Discard\"\nRP> as well.\n\n  I will leave that to those who believe that they understand proxy\n  authentication...\n\nRP> I have to say that this is all starting to sound like something we\nRP> aren't allowed to do while trying to move HTTP/1.1 from Proposed to\nRP> Draft Standard.  As much as the lack of a procedure for invalidating a\nRP> client's credentials has hurt my company's products (quite a bit!)...\n\n  It is clear that this capability, if it existed, would be a big help\n  to application developers.  I believe that a good case can be made\n  that its absense decreases the security of many applications, and so\n  I would argue that it is a bug in the protocol - bugs may be fixed\n  at this stage.  I also believe (as I said in the earlier note) that\n  it does not present a backward compatibility problem (web\n  applications using old browsers will be bug-for-bug compatible with\n  the old protocol :-).\n\nRP> ... we\nRP> would nonetheless be opposed to any change that would prevent the rapid\nRP> advancement of HTTP/1.1 to Draft Standard.\n\n  I could not agree more.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "Folks, pleass avoid messages like the following exerpt!\n\n>  I have to say that this is all starting to sound like something we\n>  aren't allowed to do while trying to move HTTP/1.1 from Proposed to\n>  Draft Standard.  As much as the lack of a procedure for invalidating a\n>  client's credentials has hurt my company's products (quite a bit!), we\n>  would nonetheless be opposed to any change that would prevent the rapid\n>  advancement of HTTP/1.1 to Draft Standard.\n>  \n\nLarry has said it before, and I'll say it again:\n\nLet Larry and myself worry about the process problems: focus on the\nright technical solution to the problem....  Both he and I are much\nmore up on IETF process than most of the working group.\n\nThere is almost always a way of skinning the cat; for example, rather than \nholding up the base HTTP/1.1 document, one can issue an independent document \nfor the functionality enhancement, and move it to proposed standard when \nthe base HTTP/1.1 spec goes to draft.  If we believe an issue doesn't\nmeet the criteria required for draft, we can generally split it off into\nan independent document.  I'll make this call in concert with Larry\nand the Area directors.\n\nWe don't want someone's (very often incorrect) understanding of process\nto close off discussions of the right way to solve an issue.  Ergo\nmy quick reaction to this paragraph.\n\nThe basic point of IETF process is to try to ensure interoperability (and\ntherefore backward compatibility).  You can usually get what you want done\none way or the other without much of a problem.  It might (or might not)\nrequire a separate document).  So don't sweat the process stuff or let\nit cause you to lose sleep on potential solutions.\n\nYou should, however, worry HARD about installed base and interoperability. \nIf solutions to issues don't meet those realities, then life gets much harder.  \nThis is the reality of the Internet process (and commercial products).....\n\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "If there's a problem and there's consensus on how to fix it,\nthen we can decide to undock the solution from 1.1. I think\nthere's some sympathy in the IESG to solve security problems,\neven if the solution isn't quite within the scope of what's\nallowed for moving from 'Proposed' to 'Draft'.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: REAUTHENTICATION REQUIRE",
            "content": "Some comments:\n\nI think that you need to be more explicit about the meaning of \"discard\" --\nthe credentials should be zero'd out in memory, so that even if someone\ncomes and attaches a debugger to an idle browser, they won't be able to get\nthe credentials.\n\nI think you need to explain why this is an issue for the standard, and not\njust a browser implementation issue -- the security (against certain\nthreats) of the end-to-end distributed application depends on it being done.\n\nYou may want to point out that when discarding credentials, the browser may\nalso want to erase history, etc. -- I presume that one environment is serial\nreuse of the browser by another user who may immediately sit down and start\nto use it.\n\nPaul\n\n> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Tuesday, November 25, 1997 10:51 AM\n> To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: REAUTHENTICATION REQUIRED\n> \n> \n> >>>>> \"RP\" == Ross Patterson <Ross_Patterson@ns.reston.vmd.sterling.com>\n> writes:\n> \n> RP> \"Scott Lawrence\" <lawrence@agranat.com> writes:\n> \n> >> As I wrote the proposal, 'discard' can't be combined with other uses\n> >> of the Authentication-Info header, such as nextnonce; this may have\n> >> been a mistake.\n> \n> RP> Just to be clear: You're proposing that a client receiving an\n> RP> \"Authentication-Info: Discard\" header in a response should discard the\n> RP> credentials it supplied on the Authorization header in the request,\n> RP> correct?  Given everything Fote has been saying about the domain of\n> RP> credentials over URLs, not to mention realm values, it's entirely\n> RP> possible for a browser to have a large cache of presumably-valid\n> RP> credentials.  I don't think you're asking to have them all discarded,\n> or\n> RP> even some subset of those associated with the origin server, correct?\n> \n>   Yes, correct.  My intent was that the credentials used for the\n>   request be discarded - regardless of what other protection spaces\n>   they may have also applied to.  Credentials not used for the request\n>   are not affected.\n> \n>   Example:\n> \n>     get /foo       ------------------>\n>                    <----------------- 401 Unauthorized\n>                                       WWW-Authenticate: realm=\"bar\"\n>     (prompt user)\n>     get /foo       ------------------>\n>     Authorization: xxxxx\n>                    <----------------- 200 Ok\n>                                       ...\n>     get /foo/bogus  ------------------>\n>     Authorization: xxxxx\n>                    <----------------- 200 Ok\n>                                       ...\n>     post /foo/bar  ------------------>\n>     Authorization: xxxxx\n>                    <----------------- 200 Ok\n>                                       Authentication-Info: discard\n>     (discard credentials use to build xxxxx)\n> \n>   At this point, if the user enters the URL for /foo/bogus, the user\n>   agent may 'remember' that it needs to prompt for credentials in\n>   realm 'bar' or it may just send a request and (presumably) get a 401\n>   as usual.  The fact that 'discard' came in response to a request for\n>   /foo/bar does not restrict the loss of credentials to that resource\n>   - the credentials go away and must not be reused.\n> \n>   If people don't think that was clear, I invite improved wording.\n> \n> RP> One more point: Since Authentication-Info is defined by RFC 2069 as\n> RP> being sent when authentication is successful, this discard response\n> must\n> RP> be sent with a 2xx or 3xx status code and without a WWW-Authenticate\n> RP> header.  In other words, an origin server is only allowed to request\n> the\n> RP> client to discard credentials at a time when they are acceptable to\n> the\n> RP> server.\n> \n>   I had not made that specific, but I see no problem with it.\n> \n> RP> And I assume we'd want to support \"Proxy-Authentication-Info: Discard\"\n> RP> as well.\n> \n>   I will leave that to those who believe that they understand proxy\n>   authentication...\n> \n> RP> I have to say that this is all starting to sound like something we\n> RP> aren't allowed to do while trying to move HTTP/1.1 from Proposed to\n> RP> Draft Standard.  As much as the lack of a procedure for invalidating a\n> RP> client's credentials has hurt my company's products (quite a bit!)...\n> \n>   It is clear that this capability, if it existed, would be a big help\n>   to application developers.  I believe that a good case can be made\n>   that its absense decreases the security of many applications, and so\n>   I would argue that it is a bug in the protocol - bugs may be fixed\n>   at this stage.  I also believe (as I said in the earlier note) that\n>   it does not present a backward compatibility problem (web\n>   applications using old browsers will be bug-for-bug compatible with\n>   the old protocol :-).\n> \n> RP> ... we\n> RP> would nonetheless be opposed to any change that would prevent the\n> rapid\n> RP> advancement of HTTP/1.1 to Draft Standard.\n> \n>   I could not agree more.\n> \n> --\n> Scott Lawrence           EmWeb Embedded Server\n> <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering\n> http://www.agranat.com/\n> \n\n\n\n"
        },
        {
            "subject": "Authorization draft: AuthenticationInfo synta",
            "content": "With the discussion around the \"discard\" token for the Authentication-Info\nheader I realized that this header contains no syntax for adding further\nelements (whereas, for example, most header definitions in the http spec\ndo). To rectify this I propose making the following changes.\n\nIn section 3.2.3 replace\n\n            AuthenticationInfo = \"Authentication-Info\" \":\"\n                                 1#( digest | nextnonce )\n\nby\n\n            AuthenticationInfo = \"Authentication-Info\" \":\" 1#(auth-info)\n            auth-info          = digest | nextnonce | discard | extension-info\n            extension-info     = token [ \"=\" ( token | quoted-string ) ]\n\nand in section 3.6 replace\n\n            Proxy-Authentication-Info = \"Proxy-Authentication-Info\" \":\"\n                                        nextnonce\n\nby\n\n            Proxy-Authentication-Info = \"Proxy-Authentication-Info\" \":\"\n                                        1#( proxy-auth-info)\n            proxy-auth-info           = nextnonce | discard | extension-info\n\n(or appropriately modified if \"discard\" is not accepted in the end).\n\nBut probably the definition of these headers should be moved to section 1.x\nand suitably adjusted, since \"discard\" is not restricted to the Digest\nscheme.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: Question on AcceptCharse",
            "content": "Howard Melman:\n>\n\nHi Howard,\n\n>\n>I have a question on how to treat ISO 8859-1 in\n>Accept-Charset and didn't find it discussed in the\n>archives. \n>\n>> 14.2 Accept-Charset\n>\n>> The ISO-8859-1 character set can be assumed to be\n>> acceptable to all user agents.\n>>\n>>        Accept-Charset = \"Accept-Charset\" \":\"\n>>                  1#( ( charset | \"*\" [ \";\" \"q\" \"=\" qvalue ]\n>> )\n>>\n>> Character set values are described in section 3.4. Each\n>> charset may be given an associated quality value which\n>> represents the user's preference for that charset. The\n>> default value is q=1. An example is\n>>\n>>        Accept-Charset: iso-8859-5, unicode-1-1;q=0.8\n>>\n>> The special value \"*\", if present in the Accept-Charset\n>> field, matches every character set (including ISO-8859-1)\n>> which is not mentioned elsewhere in the Accept-Charset\n>> field.    If no \"*\" is present in an Accept-Charset field,\n>> then all character sets not explicitly mentioned get a\n>> quality value of 0, except for ISO-8859-1, which gets a\n>> quality value of 1 if not explicitly mentioned.\n>\n>If a server receives\n>\n>  Accept-Charset: iso-8859-5, *;q=0\n>\n>is iso-8859-1 acceptable to the client?  What about:\n>\n>  Accept-Charset: iso-8859-5, iso-8859-1;q=0\n>\n>In both cases I would assume it is not acceptable to the\n>client, but this seems to contradict the first sentence above.\n\nYou are right. The first sentence above was left over from an earlier\nedit, and should have had some kind of `by default' qualifier.\n\n>If this is in fact the case, then I think the sentence\n>\n>> The ISO-8859-1 character set can be assumed to be\n>> acceptable to all user agents.\n>\n>should be removed.  Given the last paragraph, which I find\n>quite clear, this sentence only adds confusion.\n\nI agree, this sencence should be removed.\n\n>Howard\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Agenda for WEBPRIV BOF, Mon Dec 8 1930220",
            "content": "Monday, December 8 at 1930-2200\n      (opposite mhtml, acap, ipvbi, tcpimpl, rmonmib, issll mobileip)\n\nOverview:\n\nThere are a number of methods in use that gather information about\nInternet (and World Wide Web users). While much of the information\ngathering is benign, the potential for abuse is high.\n\nPopular reports of the privacy issues around web browsing have focused\non the issue of 'cookies': the use of the HTTP state management\nmechanism to send information about the user to a third party. However,\nthere are a large number of other possible threats. For example,\nsoftware packages, when downloaded and installed, may send the user's\nidentification and other information directly to the maker of the\nsoftware package, in the name of 'helping' with the identification of\nthe user's configuration. Maintainers of proxy caches could leave\nlogs of user activities publicly available. Companies could join\nin a consortium to share information about user preferences and behavior.\n\nWhile this kind of information can have many positive uses, it also\ncan be misused. Internet users may not be aware that their reading\nbehavior is observed, and there have been many cases where privacy\nof information about an individual's use of public libraries, video\nrentals and other media have been at issue.\n\nThe goal of the BOF is canvas for interest in a working group\n(in USV) aimed at creating a set of guidelines which will aid\nboth system administrators and protocol designers: what are the\nnature of the threats to user privacy, and what are some of the\nmechanisms and policies that are necessary to avoid such threats.\n\nTentative agenda (volunteers to present issues welcome)\n\n   15 - welcome, introduction\n   60 - review of HTTP issues\n       20 privacy and 'hit metering'\n       20 privacy and 'state management' (cookies)\n       20 review of W3C P3 initiative\n   30 - operational issues:\n       log files, privacy methods for obscuring\n       ISP policies\n       web site policies\n   15 - USV working group strategy & policy\n        Privacy vs. Security: boundaries\n   30 - Plans for working group\n\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "A common registry for MIME headers",
            "content": "I personally think it would be better if there were a common\nregistry for 'headers' that included the list of which\nwere useful in HTTP, EMail, NetNews, within particular multipart\ntypes, etc.\n\nI think it would help in future coordination between extensions\nof mail, news, other kinds of 'push', etc.\n\nDo you support using the same registry for HTTP and other applications\nof MIME?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\nattached mail follows:\nA New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the Detailed Revision/Update of Message Standards \nWorking Group of the IETF.\n\nTitle: Mail and Netnews Header Registration Procedure\nAuthor(s): J. Palme\nFilename: draft-ietf-drums-MHRegistry-02.txt\nPages: 9\nDate: 24-Nov-97\n\nVarious IETF standards and e-mail and netnews software products\nuse various e-mail and netnews header fields. This document\nspecifies a procedure for the registration of e-mail and netnews\nheader field names, to reduce the risk that two different products\nuse the same header name in different ways (homonyms) or that\nseveral different header names are used with identical meaning\n(synonyms).\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-drums-MHRegistry-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-drums-MHRegistry-02.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-drums-MHRegistry-02.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: A common registry for MIME headers",
            "content": "Larry Masinter wrote:\n>I personally think it would be better if there were a common\n>registry for 'headers' that included the list of which\n>were useful in HTTP, EMail, NetNews, within particular multipart\n>types, etc.\n>\n>I think it would help in future coordination between extensions\n>of mail, news, other kinds of 'push', etc.\n>\n>Do you support using the same registry for HTTP and other applications\n>of MIME?\n\nThat sounds like a good idea to me. I think Jacob Palme has already started\nsuch a registry for the email realm, and I suspect he might be happy to\nexpand its scope.\n\nAlex Hopmann\nMicrosoft Corporation\n\n\n\n"
        },
        {
            "subject": "Revised agenda for HTTP-WG, Dec 8, 1300150",
            "content": "Because of the various scheduling constraints, there\nis only a single two-hour session, Monday December 8, 1300-1500,\nfor HTTP-WG. However, there are several other sessions\nrelated to HTTP on the agenda: Monday 1930-2200 on \"Web Privacy\",\nand Tuesday 1545-1800 on \"Content Negotiation\".\n\nI suggest the following agenda:\n\n80 minutes: Jim Gettys: review open issues in HTTP/1.1 draft.\n10 minutes: Dave Kristol: Review state management draft\n   (Interested parties should meet after the WEBPRIV BOF to\n   plan further work on state management and privacy.)\n10 minutes: Scott Lawrence: Review status of 'OPTIONS' and 'PEP'\n20 minutes: Larry Masinter: Review of Working Group status & schedule\n   for completing work.\n\nIn addition, there will be a meeting Tuesday evening (during the\ndinner/social time) for a group to document interoperable\nimplementations of each feature of HTTP/1.1.  (Come with\nyour marked up copy of the HTTP/1.1 spec.) Location to be announced.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-state-man-mec05.txt,.p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-05.txt,.ps\nPages: 19\nDate: 26-Nov-97\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-05.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-man-mec-05.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-05.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v11-spec-rev01.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): J. Mogul, T. Berners-Lee, L. Masinter, \n                          P. Leach, R. Fielding, H. Nielsen, J. Gettys\nFilename: draft-ietf-http-v11-spec-rev-01.txt\nPages: 165\nDate: 26-Nov-97\n\n       The Hypertext Transfer Protocol (HTTP) is an application-level protocol\n       for distributed, collaborative, hypermedia information systems. It is a\n       generic, stateless, object-oriented protocol which can be used for many\n       tasks, such as name servers and distributed object management systems,\n       through extension of its request methods. A feature of HTTP is the\n       typing and negotiation of data representation, allowing systems to be\n       built independently of the data being transferred.\n \n       HTTP has been in use by the World-Wide Web global information initiative\n       since 1990. This specification defines the protocol referred to as\n       ''HTTP/1.1''.\n \n       The issues list for HTTP/1.1 can be found at:\n       http://www.w3.org/Protocols/HTTP/Issues/.\n \n       This draft does not resolve all open issues in the HTTP/1.1\n       specification requiring closure before HTTP/1.1 goes to draft standard.\n       It does, however, close most of them, and note where in the document\n       there are still significant issues under discussion.  The best way to\n       view this document is to get a copy of the Word 97 document found at:\n       http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-rev-\n       01.doc; all issues are noted as comments in the source document, with\n       hyperlinks to the Issues list.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-rev-01.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-01.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-authentication00.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP Authentication: Basic and Digest Access \n                          Authentication\nAuthor(s): J. Franks, A. Luotonen, P. Leach, J. Hostetler,\n                          P. Hallam-Baker, E. Sink, L. Stewart\nFilename: draft-ietf-http-authentication-00.txt\nPages: 27\nDate: 26-Nov-97\n\n       ''HTTP/1.0'' includes the specification for a Basic Access Authentication\n       scheme. This scheme is not considered to be a secure method of user\n       authentication (unless used in conjunction with  some external secure\n       system such as SSL [5]), as the user name and password are passed over\n       the network as clear text.\n \n       This document also provides the specification for HTTP's authentication\n       framework, the original Basic authentication scheme and a scheme based\n       on cryptographic hashes, referred to as ''Digest Access Authentication''.\n       It is therefore intended to also serve as a replacement for RFC 2069.[6]\n \n       Like Basic, Digest access authentication verifies that both parties to a\n       communication know a shared secret (a password); unlike Basic, this\n       verification can be done without sending the password in the clear,\n       which is Basic's biggest weakness. As with most other authentication\n       protocols, the greatest sources of risks are usually found not in the\n       core protocol itself but in policies and procedures surrounding its use.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-authentication-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-authentication-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-authentication-00.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "RE: emai",
            "content": "Please DO NOT send any more e-mail to: mfox@telplus.net \nThank you!\n\n\n\n"
        },
        {
            "subject": "Re: Revised agenda for HTTP-WG, Dec 8, 1300150",
            "content": "Marcia Beaulieu (IETF Meeting Coordinator) wrote:\n> \n> Larry,\n> \n> I just got word from Harald that we can schedule a second slot for\n> HTTP on Thursday, December 11 at 1300-1500.  If you are still\n> interested in having this additional slot, please let me know.\n\n\nYes, we'd very much like a second slot. I'll issue a revised\nagenda soon.\n\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: A common registry for MIME headers",
            "content": ">I personally think it would be better if there were a common\n>registry for 'headers' that included the list of which\n>were useful in HTTP, EMail, NetNews, within particular multipart\n>types, etc.\n>\n>I think it would help in future coordination between extensions\n>of mail, news, other kinds of 'push', etc.\n\n\nI think that Larry's point is a good one but the problem is\nsomewhat deeper. The problem is that the RFC structure\ndoes not really support the layered nature of the protocols,\nnor is the need for continuing extensions handled at all well.\n\nThe area in which this is most apparent is NNTP where\nalmost half the 'de-facto' NNTP standard has been outside\nthe IETF process for a decade. The recent NNTP extensions\nwork has thus been preformed in a remedial rather than a\npro-active fashion.\n\nTwo models to bear in mind, the UK and US legislative\nprocesses. The RFC process is basically that of the UK \nparliament. Bills correspond to Internet drafts. They can \nbe introduced by any member at any time and have no \nweight. RFCs correspond to 'Acts' - i.e. fully processed\nand approved bills that have made it thorugh the committee\ncycle. Except that RFCs also include documents that would\nin the UK system be considered 'orders in council' 'white\npapers' and all the rest of parliamentary clutter.\n\nThe US legislative system is rather different in that the \nacts passed are essentially a set of editing instructions for\nanother document, a consolidated statement of the law.\nThis is essentially the French Revolutionary model, impose\na uniform code.\n\nThe advantage of this second model is that it allows much \nmore scope for 'tweakage' of an extant spec. Additional\nheaders can be specified and added into the general mix\nwithout having to re-issue every spec that is dependent on\nthem.\n\nFor example consider the Digest Authentication Spec,\nshould it dock or not? Ideally I would like to be able to\n'dock' specifications of this type after the HTTP/1.1 \nspec has been fully approved and without re-opening \nHTTP.\n\nThe case of specs that cross working group boundaries \nmakes a mechanism of this type even more important.\nFor example the MIME group would never accept the\nreal and legitimate objections of the HTTP community \nthat since HTTP is 8-bit clean it was imperative that\na content length engoding be allowed. What was actually\na transport issue (SMTP implementations are habitually\nbroken meaning that content-length is not stable) became\na structural imperative.\n\nAs much of the communications infrastructure becomes\nembedded in the operating system it becomes even more\nimportant to systematize the means by which facilities can\nbe added to the general infrastructure while taking account\nof the specific limtations of particular protocols.\n\nSo a possible new structure for RFCs would be as a set of \nediting instructions. The protocol headers (or whatever)\nwould be specifie with a description for each. Then another\nsection would discuss the applicability of the headers to each \nof the MIME based transport protocols SMTP, NNTP and \nHTTP.\n\nSome centralized document would be maintained with a\nstructure designed to allow component-wise editing (i.e. \nthe addition of additional chunks). There might be cross\nreferences in this document so that additional rationale\nfor a particular change might be found in an acompanying \nRFC..\n\nIn short this is not a simple 'registration' function but a \nfundamental change in the way the IETF operates. It would \nhave many advantages, in particular avoiding the repetative\nre-writing of introductory material which may or may not\ncontain substantive changes.\n\nThe problem is getting agreement to make such a change.\nThere are still folk that turn purple at the heretical suggestion\nthat RFCs should be issued in HTML. Their rationale being \nbased on a bad experience with postscript. One ill fated \nexperiment in the middle ages having proved that for all time\nthere can be no improvement on ASCII text. This is why\nevery single person down in Washington the week after next\nwill carry at least one RFC which has been misprinted.\n\nAnd don't think for a moment that the response that everyone\nwill give to proposals for change is to agree with them but\nlament at great length that they wont be changed.\n\nSuch things are the spark of revolutions.\n\n            Phill\n\n\n\n"
        },
        {
            "subject": "DAV Searching and Locating MiniBO",
            "content": "> Subject: DAV Searching and Locating Mini-BOF\n> When: During WEBDAV WG Session on Monday, Dec. 8, 1997 (1530-1545) \n> \n> A Mini-BOF meeting will be held during the first 15 minutes of the\n> WEBDAV WG Session at the Washington IETF to introduce the proposed\n> WEBDAV-related working group: DAV Searching and Locating (DASL). The\n> purpose of this proposed working group is to develop an extensible\n> property and content based HTTP search mechanism for locating DAV\n> resources.\n> \n> Items for the brief 15-minute mini-BOF\n> - problems DASL is meant to solve\n> - DASL's relationship to WEBDAV\n> - go over general requirements\n> \n> For more information on proposed DASL WG consult the following\n> - DASL Homepage: http://www.ics.uci.edu/pub/ietf/dasl\n> - DASL Charter: http://www.ics.uci.edu/pub/ietf/dasl/charter.html\n> - DASL Requirements:\n> ftp://ftp.ietf.org/internet-drafts/draft-reddy-dasl-requirements-00.tx\n> t\n> - DASL mailing list archives:\n> http://lists.w3.org/Archives/Public/www-webdav-dasl/\n> - DASL mailing list: www-webdav-dasl@w3.org\n> \n> To join the DASL mailing list, send email with a body of \"subscribe\"\n> to:\n>  www-webdav-dasl-request@w3.org .\n> \nThanks,\nSaveen\n\n\n\n"
        },
        {
            "subject": "HTTP Interoperability Test 12",
            "content": "  The fifth HTTP/1.1 Multi-Vendor Internet Test Day is Dec 4.\n\n  !!! We do have at least one 1.1 proxy for this week !!!\n\n  This is the last test day before the December IETF - don't miss the\n  chance.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Dec 2.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  ================================================================\n    Note:\n        There is an updated list of continuously available test sites\n        at:\n             http://www.w3.org/Protocols/HTTP/Forum/#Test\n        Send me a note to update that list.\n  ================================================================\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REAUTHENTICATION REQUIRE",
            "content": "** Reply to note from \"Scott Lawrence\" <lawrence@agranat.com> Tue, 25 Nov 1997 13:51:13 -0500\n> RP> And I assume we'd want to support \"Proxy-Authentication-Info: Discard\"\n> RP> as well.\n>   \n>   I will leave that to those who believe that they understand proxy\n>   authentication...\n>   \nPretty much just like Authentication.  I agree with Ross, it should be\nadded for Proxy-Authentication also.\n\nAnd at the risk of function creep, I think there is a need for the\nability to change a password.  I do not have time to submit wording for\nthis before DC, but if there is interest and someone does not beat me\nto it, I will do so after.\n\nRichard L. Gray\nwill code for chocolate\n\ncc: \"Scott Lawrence\" <lawrence@agranat.com>\n\n\n\n"
        },
        {
            "subject": "Re: A common registry for MIME headers",
            "content": "At 09:42 29/11/97 PST, Larry Masinter wrote:\n>I personally think it would be better if there were a common\n>registry for 'headers' that included the list of which\n>were useful in HTTP, EMail, NetNews, within particular multipart\n>types, etc.\n>\n>I think it would help in future coordination between extensions\n>of mail, news, other kinds of 'push', etc.\n>\n>Do you support using the same registry for HTTP and other applications\n>of MIME?\n\nHere, here (or should that be \"hear hear\"?)\n\nI find there there seems to be much discussion about the cross-overs\nbetween protocols, so having a mechanism which avoids using the same name\nfor different purposes in different protocols seems like a good start.\n\nAnd should there also be some equivalent to the \"vnd\" tree in MIME, for\nstrictly private protocol use?\n\nGK.\n---\n\n------------\nGraham Klyne\n\n\n\n"
        },
        {
            "subject": "Request for HTML Draft to support Multi Direction languages",
            "content": "Dear sirs,\n\nI want to bring this issue up.\n\nArabic, Hebrew and other languages are wrriten from right to left. \nWhile The latest HTML draft does not have a special tag to specify the\ndirection and takes left to right as the default direction of the\nlanguage.\n\nI think it is about time that such a tag (property) will be defined.\n\nThe suggestion is to add a property called Language with the following\nfeatures:\n1. Direction - The direction from which the language is wrriten.\n2. Default font - The default font for that language as in the OS (such\nas Windows 95) installation.\n3. Wrap - Direction of wrraping according to the dominent language.\n\nSo that if we combine Arabic/Hebrew with English in the same sentence,\nwhich is most common we can write:\n\n<Wrap=right>\n<Language Direction=left font=TimesNewRoman>English text</Direction>\n<Language Direction=Right font=HebFont>Hebrew text</Direction>\n</Wrap>\n\nThe wrapping issue is also very important and should be concidered with\nthe \"Dominent\" language in which the HTML document was wrriten.\n\nI would like to get your commenct on these important issues.\n-- \n\nRegards\n-----------------------------------------------------------------\nErez Levin\nR&D manager\n\n\nDDDDDD   IIIIII NN      NN    GGGGG       OOO\nD     D    II   NNNN    NN   GG         OO   OO\nD      D   II   NN NN   NN  GG         OO     OO\nD      D   II   NN  NN  NN  GG  GGGG   OO     OO  Infosystems\nD     D    II   NN   NN NN   GG  GG     OO   OO\nDDDDDD   IIIIII NN    NNNN    GGGG        OOO\n\nEmail: erezl@dingo.co.il\nOur site:http://www.dingo.co.il\n-------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Interoperability Test 12",
            "content": "At 12:07 12/2/97 -0500, Scott Lawrence wrote:\n>  The fifth HTTP/1.1 Multi-Vendor Internet Test Day is Dec 4.\n\nIs this going on now - how can I contribute? I have libwww ready to handle\ntrailers and TE and so if anybody want to test the fastest client on the\nmarket ;-) for pipelining then I would be happy to try it out.\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Another way to pipeline cache validation...",
            "content": "For revalidating conventional Web pages, it is pretty clear what you\ncan do to take advantage of HTTP/1.1's pipelining and buffering.  We\ncertainly outline strategies in our SIGCOMM paper, and the very significant\nperformance wins that result.\n\nI now use a Java based mail system (Pachyderm), and startup time to validate \nthe Java class libraries (at least at the moment; it is a prototype system \nnot yet using JAR files), has been a problem.  And, on the surface, short \nof packaging everything up into JAR files, taking advantage of pipelining \nin HTTP/1.1 is hard for Java. A class starts executing, then at run time \nfinds it needs another class, goes to revalidate it, executes the next class, \nrevalidates, etc.  This effectively serializes cache validation for Java \napplications where classes are not packed in to a single JAR file, and\nresults in N round trips, one for each class library.  Boring.\n\nIn a discussion with Andrew Birrell (DECSRC), one of Pachyderm's authors, \nAndrew had a good idea; it is clearly applicable to things other than Java,\nthough its use to speed Java startup is obvious.\n\nAndrew's idea is to augment the cache database that a browser keeps on disk \nof things it caches, with a list of the order in which things are accessed \n(at that site, maybe with how soon they were accessed).  So when you go \nsee that you need to validate the first class library, it then becomes easy \nto figure out that there are a bunch of things that the Java application \nis going to want/need from that web site.  This makes it trivial to do a \npipelined cache validation of the set of objects likely to be needed and \nreduce this to one round trip.  The technique could be used for other forms \nof cache validation than Java classes as well, but it is obviously a good \nidea for them. \n- Jim Gettys\n\n\n\nattached mail follows:\nThe original message was received at Thu, 4 Dec 1997 11:11:28 -0800 (PST)\nfrom pachyderm.pa.dec.com [16.4.16.23]\n\n   ----- The following addresses have delivery notifications -----\n<http-wg@cuckoo.hpl.com>  (unrecoverable error)\n\n   ----- Transcript of session follows -----\n... while talking to gate.hpl.com.:\n>>> RCPT To:<http-wg@cuckoo.hpl.com>\n<<< 550 Unable to add recipient.\n550 <http-wg@cuckoo.hpl.com>... User unknown\n\n   ----- Original message follows -----\n\nReturn-Path: <jg@pa.dec.com>\nReceived: from pachyderm.pa.dec.com (pachyderm.pa.dec.com [16.4.16.23])\nby mail1.digital.com (8.7.5/UNX 1.5/1.0/WV) with SMTP id LAA01213; \nThu, 4 Dec 1997 11:11:28 -0800 (PST)\nReceived: by pachyderm.pa.dec.com; id AA10741; Thu, 4 Dec 1997 11:11:27 -0800\nDate: Thu, 4 Dec 1997 11:11:27 -0800\nFrom: jg@pa.dec.com (Jim Gettys)\nMessage-Id: <9712041911.AA10741@pachyderm.pa.dec.com>\nX-Mailer: Pachyderm (client pachyderm.pa-x.dec.com, user jg)\nTo: w3c-http@w3.org, http-wg@cuckoo.hpl.com\nSubject: Another way to pipeline cache validation....\n\nFor revalidating conventional Web pages, it is pretty clear what you\ncan do to take advantage of HTTP/1.1's pipelining and buffering.  We\ncertainly outline strategies in our SIGCOMM paper, and the very significant\nperformance wins that result.\n\nI now use a Java based mail system (Pachyderm), and startup time to validate \nthe Java class libraries (at least at the moment; it is a prototype system \nnot yet using JAR files), has been a problem.  And, on the surface, short \nof packaging everything up into JAR files, taking advantage of pipelining \nin HTTP/1.1 is hard for Java. A class starts executing, then at run time \nfinds it needs another class, goes to revalidate it, executes the next class, \nrevalidates, etc.  This effectively serializes cache validation for Java \napplications where classes are not packed in to a single JAR file, and\nresults in N round trips, one for each class library.  Boring.\n\nIn a discussion with Andrew Birrell (DECSRC), one of Pachyderm's authors, \nAndrew had a good idea; it is clearly applicable to things other than Java,\nthough its use to speed Java startup is obvious.\n\nAndrew's idea is to augment the cache database that a browser keeps on disk \nof things it caches, with a list of the order in which things are accessed \n(at that site, maybe with how soon they were accessed).  So when you go \nsee that you need to validate the first class library, it then becomes easy \nto figure out that there are a bunch of things that the Java application \nis going to want/need from that web site.  This makes it trivial to do a \npipelined cache validation of the set of objects likely to be needed and \nreduce this to one round trip.  The technique could be used for other forms \nof cache validation than Java classes as well, but it is obviously a good \nidea for them. \n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: Another way to pipeline cache validation...",
            "content": "On Thu, 4 Dec 1997, Jim Gettys wrote:\n\n> Andrew's idea is to augment the cache database that a browser keeps on disk \n> of things it caches, with a list of the order in which things are accessed \n> (at that site, maybe with how soon they were accessed).  So when you go \n> see that you need to validate the first class library, it then becomes easy \n> to figure out that there are a bunch of things that the Java application \n> is going to want/need from that web site.  This makes it trivial to do a \n> pipelined cache validation of the set of objects likely to be needed and \n> reduce this to one round trip.  The technique could be used for other forms \n> of cache validation than Java classes as well, but it is obviously a good \n> idea for them. \n\nSeems to me that there is an obvious container relationship which could\nbe exploited by the user agents. As long as the UA is going to change\nto improve revalidation performance, the cache management database \nshould keep track of the external references from an object.  Simply\nfollowing the direct external reference chain should provide a \ndirected graph of all objects likely to be required the next time\nan object was referenced. A lot better than using relative time\nassociation to guess what else to try.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Rerevised agenda for IETF HTTP meetings, Dec 8 &amp; 1",
            "content": "We were first given another agenda slot on Thursday, but\nhave been offered to swap with SCHEMA on Wednesday, 1530-1730.\nI'm assuming the swap will happen.\n\nMonday, Dec 8 1300-1500 \n70 min: Jim Gettys: review open issues in HTTP/1.1 draft.\n        see http://www.w3.org/Protocols/HTTP/Issues for list\n20 min: Dave Kristol: Review state management draft\n        (note WebPRIV bof below)\n20 min: Scott Lawrence: Review status of 'OPTIONS' and 'PEP'\n10 min: summary & plan for Wednesday\n\n<<Monday  1930-2200 WebPRIV BOF>>\n<<Tuesday 1545-1800 content negotiation BOF>>\n<<Tuesday 1800-(dinner) document interoperable implementations>>\n\nWednesday, Dec 10 1530-1730\n15 min: report on survey of interoperable implementations\n60 min: Jim Gettys: Review remaining open issues\n15 min: Content negotiation and HTTP: plans?\n15 min: Web privacy and state management: plans?\n15 min: Working group wrap-up plans\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Interoperability Test 12",
            "content": "> At 12:07 12/2/97 -0500, Scott Lawrence wrote:\n> >  The fifth HTTP/1.1 Multi-Vendor Internet Test Day is Dec 4.\n> \n> Is this going on now - how can I contribute?\n\nSign up next week. Scott sends out a list of those participating to all\nparticipants the day before the test. Then try to find any bugs you\ncan...\nNot that many clients have participated so far, so I think the server\nfolks would be especially happy if you participated.\n\nI'd like to add some long overdue good words for the tests here (and\nof course many thanx to Scott for organizing them). I've been able to\neliminate a number of bugs in my client, and there've been a few in most\nservers. I take it as a good sign that most problems have been\nimplementation bugs, but nevertheless the tests have turned up at least\none protocol problem (which has been solved).\n\nNote that no proxies have been involved in the tests (at least, not that\nI was aware of) so this is one area of the spec which could use some\ntesting.\n\n> I have libwww ready to handle\n> trailers and TE and so if anybody want to test the fastest client on the\n> market ;-) for pipelining then I would be happy to try it out.\n\nI've got my client ready too - now I'm waiting for a server to implement\nthe trailers and TE (at least I didn't find anything during the tests\nthat\nincluded trailers or the Trailer header). Any servers out there sending\ntrailers?\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-pep05.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: PEP - an Extension Mechanism for HTTP\nAuthor(s): D. Connolly, H. Nielsen, R. Khare, \n                          E. Prud'hommeaux\nFilename: draft-ietf-http-pep-05.txt\nPages: 28\nDate: 04-Dec-97\n\n       HTTP is used increasingly in applications that need more facilities\n       than the standard version of the protocol provides, ranging from\n       distributed authoring, collaboration, and printing, to various remote        procedure call mechanisms. The Protocol Extension Protocol (PEP) is an\n       extension mechanism designed to address the tension between private\n       agreement and public specification and to accommodate extension of\n       applications such as HTTP clients, servers, and proxies. The PEP\n       mechanism is designed to associate each extension with a URI[2], and\n       use a few new RFC 822[1] derived header fields to carry the extension\n       identifier and related information between the parties involved in an\n       extended transaction.\n \n       This document defines PEP and describes the interactions between PEP\n       and HTTP/1.1[7]. PEP is intended to be compatible with HTTP/1.0[5]\n       inasmuch as HTTP/1.1 is compatible with HTTP/1.0 (see [7], section\n       19.7). It is proposed that the PEP extension mechanism be included in\n       future versions of HTTP.\n \n       The PEP extension mechanism may be applicable to other information\n       exchange not mentioned in this document. It is recommended that\n       readers get acquainted with section 1.4 for a suggested reading of\n       this specification and a list of sections specific for HTTP based\n       applications.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-pep-05.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-pep-05.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-pep-05.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "comments on http-authentication0",
            "content": "I'll bring my usual set of nitpicking editorial comments to the IETF\nmeeting for personal delivery.  Meanwhile, here are some substantive\nones.\n\n1) Sect 1.2\n[Proxies] MUST forward the WWW-Authenticate and Authorization\nheaders untouched....\n\nI would like MUST to be SHOULD.  I've brought this up once before.\nThere may be services (LPWA, lpwa.com, is one such) whose legitimate\npurpose is to provide authentication services for a user, such as\nreplacing special character sequences in Authorization with a user's\ncomputed identity.  The proxy ought to be able to do so without being\nconsidered non-compliant.\n\n2) Sect 3.2.1, under \"nonce\"\n... is the dotted quad IP address ...\n\nHow to handle IPv6 addresses?\n\n3) Sect 3.2.2, syntax\nshould be\n    entity-digest = <\"> ...\n      ^\n\nThe \"date\" attribute description bears no mention here of what\ndate we're talking about.  I inferred from text much further on\nthat it's supposed to mirror the Date header of the\nrequest/response.\n\n4) Sect 3.2.2, semantics\n\nConsider sender -> proxy -> receiver.\n\nThe entity-digest incorporates information from headers from\nthe sender.  Consider, for example, Date and Content-Length.  A\nproxy could add Date if one were missing.  A proxy could add a\nContent-Length after gobbling up something that the sender sent\n\"chunked\".\n\nThe receiver wouldn't know that the proxy had added those\nheaders.  It would use the added headers in its calculation of\nentity-digest and derive a different value from what the sender\ncalculated.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: comments on http-authentication0",
            "content": "At 12:33 12/5/97 EST, Dave Kristol wrote:\n\n>1) Sect 1.2\n>[Proxies] MUST forward the WWW-Authenticate and Authorization\n>headers untouched....\n>\n>I would like MUST to be SHOULD.  I've brought this up once before.\n>There may be services (LPWA, lpwa.com, is one such) whose legitimate\n>purpose is to provide authentication services for a user, such as\n>replacing special character sequences in Authorization with a user's\n>computed identity.  The proxy ought to be able to do so without being\n>considered non-compliant.\n\nYes, this was resolved in the discussion [1] on the list. The solution that\nwas last called is stated in [2].\n\nHenrik\n\n[1] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0181.html\n[2] http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0096.html\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Proposal for new HTTP 1.1 authentication schem",
            "content": "I was hoping to polish this proposal a little more before floating it\nexternally, but alas, with the meeting on Monday, time did not permit.  I\nhope that I have at least stated my perspective well enough to stimulate\ndiscussion.\n\nProblem statement:\nThe existing HTTP authentication model does not allow authentication realms\nto be distributed across servers.  To protect user credentials. HTTP\nbrowsers associate each realm with a single IP address, and will not pass\nuser credentials to multiple servers even if they claim to belong to the\nsame realm.  This security measure, built into the browser, has created the\nundesirable side effect of requiring users to re-type their user names and\npasswords for each protected server within a multiple server site.\n\nAbstract:  A proposal is made to:\n\n   Allow the distribution of protection realms across servers\n   Protect user credentials from \"imposter\" servers which claim to belong\n   to a realm but do no\n   Pass user credentials securely to content servers which identify\n   themselves as members of those realms\n   To re-use user credentials throughout protection realms so that users\n   are challenged for user name and password only once within the context\n   of a single session.\n   Create secure, trusted relationships between servers\n   Centralize authentication, authorization, and directory services for one\n   or multiple websites\n   Centralize directory security\n   Simplify or eliminate directory services on distributed content servers\n   Make it scalable\n\n\nI propose that this new authentication scheme be named \"remote\nauthentication\".\n\n\nTheory of operation:\n\nWhen access is first attempted to a content page which is protected by\nremote authentication, the browser is redirected to the \"remote\nauthentication\" server for that realm.  This redirection should be done via\nSSL for security.  The user is then challenged for user name and password\nby basic authentication.  The server then encrypts the user name and\npassword with a secret (symmetric) key and returns the user name and\npassword to the browser where they are cached for the session.  The browser\nis then re-directed to the original content server and the browser passes\nthe encrypted user name and password to the server.  The content server,\nwhich shares the same secret key as the authentication server, is able to\ndecrypt the user name and password.\n\nWhen  the browser is challenged by another server which claims to be in the\nsame realm, the user credentials are served.  If the server is trusted, it\nwill share the same secret key as the authentication server and the user\nname and password will be decrypted.  If the server is an imposter,\ndecryption will fail and the user credentials remain secure.\n\nThe way I see it, the browser will associate each realm with an encrypted\nuser name and password, which are simply opaque strings.  In addition, I\nwould like the browser to allow the server to cache an additional string of\narbitrary length to pass state to the other servers in the realm.  It could\nbe used to store additional authentication and authorization semantics,\nsuch as an expiration time for the authentication, authorization\ninformation such as a list of groups to which the individual belongs, an\nindex to identify the identity of the encryption key, etc.  This\ninformation could be signed with an MD5 MAC or encrypted or just plain\ncleartext.\n\nI think that we could do the web a great big favor if we eliminated the\nneed to replicate directories to content servers.  The authentication piece\nis easy, but if we can solve the authorization part I believe that we could\nsimplify the common registration puzzle at the same time.\n\nThank you for your support,\n-e\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "> \n> I was hoping to polish this proposal a little more before floating it\n> externally, but alas, with the meeting on Monday, time did not permit.  I\n> hope that I have at least stated my perspective well enough to stimulate\n> discussion.\n> \n\nThis sounda a lot like the old expired draft:\n\n\"Mediated Digest Authentication\"\nhttp://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-mda-00.txt\n\nI wonder if you could do this all with one-way keyed hash functions, and\navoid the use of SSL, and thus export restrictions.\n\nSomeone would have to think about possible attacks involving\na bogus server.\n\n\n\n"
        },
        {
            "subject": "Re: Request for HTML Draft to support Multi Direction languages",
            "content": "Dear Erez,\n\nOn Sat, 5 Apr 1997, Erez Levin wrote:\n> I want to bring this issue up.\n> Arabic, Hebrew and other languages are wrriten from right to left. \n> While The latest HTML draft does not have a special tag to specify the\n> direction and takes left to right as the default direction of the\n> language.\nI don't think that the transfer protocol (HTTP) has anything to do with\nrendering the HTML document itself.\nSo, you'd better resend this mail to the HTML working group.\n\nYours,\nBertold\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n Kolics, Bertold                             E-Mail: bertold@tohotom.vein.hu\n University of Veszprem, Hungary        W3: http://tohotom.vein.hu/~bertold/\n Information Engineering Course\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "Digest authentication already includes a mechanism (the 'domain'\nattribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\nspecify that credentials may be used on multiple servers, and through the\n'digest' attribute allows for mutual authentication.  \n\nThere is also the model of Kerberos to consider - developing a\nticket-based authentication scheme (with the advantages and problems of\nany third-party mechanism) would be another area to explore.\n \n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Fri, 5 Dec 1997, Scott Lawrence wrote:\n\n> \n> Digest authentication already includes a mechanism (the 'domain'\n> attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\n> specify that credentials may be used on multiple servers, and through the\n> 'digest' attribute allows for mutual authentication.  \n> \n> There is also the model of Kerberos to consider - developing a\n> ticket-based authentication scheme (with the advantages and problems of\n> any third-party mechanism) would be another area to explore.\n>  \n\nI believe that the original intent of the \"opaque\" field in the digest\nauthentication header may have been precisely for such a ticket.  A\nrequest could be referred to an \"authentication server\" which would\nredirect to a server that could check the ticket in the opaque field\nand satisfy the request.  In this way only the authentication server\nwould need to know all user passwords.  The document servers would\nonly need to know a single secret shared with the authentication\nserver.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Oracle Web Server 3.0 with SS",
            "content": "Hi,\n\nI'm sorry for using this list for this question. If there's a better\nlist for this, I would appreciate if you could let me know.\n\nI need some help on Oracle Web Application Server 3.0 with SSL. Is there\nanyone using this?\n\n\nRegards,\nAli\n\n\n\n"
        },
        {
            "subject": "Issues list updated to reflect Monday session at the IETF in Washingto",
            "content": "I updated the issues list from my overhead slides and memory.  As always\nit is found at:  http://www.w3.org/Protocols/HTTP/Issues/\n- Jim\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "I think that the spec for \"domain\" is broken -- it specifies a list of URIs,\nbut doesn't say that these can be _prefixes_ of URIs that may also use the\nsame credentials. Without that, it is pretty uselss, IMHO.\n\n\n> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Friday, December 05, 1997 10:53 AM\n> To: Eric_Houston/CAM/Lotus@lotus.com\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: Proposal for new HTTP 1.1 authentication scheme\n> \n> \n> Digest authentication already includes a mechanism (the 'domain'\n> attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\n> specify that credentials may be used on multiple servers, and through the\n> 'digest' attribute allows for mutual authentication.  \n> \n> There is also the model of Kerberos to consider - developing a\n> ticket-based authentication scheme (with the advantages and problems of\n> any third-party mechanism) would be another area to explore.\n>  \n> \n\n\n\n"
        },
        {
            "subject": "RE: comments on http-authentication0",
            "content": "Another comment. I detected another bug in the spec.  On page 13, it says:\n\n           entity-digest<\"> KD (H(A1), unquoted nonce-value \":\" Method \":\"\n                         date \":\" entity-info \":\" H(entity-body)) <\">\n                                       ; format is <\"> *LHEX <\">\n\nshould be:\n\n           entity-digest<\"> KD (H(H(A1)), unquoted nonce-value \":\" Method\n\":\"\n                         date \":\" entity-info \":\" H(entity-body)) <\">\n                                       ; format is <\"> *LHEX <\">\n\nIt is supposed to be this way so that when you have a centralized key\nserver, you can send it the initial nonce, the user name, and the\nresponse-digest, it can validate it, and send back H(H(A1)), so that the\nserver need not ever have H(A1) -- a good thing, because otherwise it would\ngive the server the ability to authenticate as the user.\n\n\n> ----------\n> From: dmk@research.bell-labs.com[SMTP:dmk@research.bell-labs.com]\n> Sent: Friday, December 05, 1997 9:33 AM\n> To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: comments on http-authentication-00\n> \n> I'll bring my usual set of nitpicking editorial comments to the IETF\n> meeting for personal delivery.  Meanwhile, here are some substantive\n> ones.\n> \n> 1) Sect 1.2\n> [Proxies] MUST forward the WWW-Authenticate and Authorization\n> headers untouched....\n> \n> I would like MUST to be SHOULD.  I've brought this up once before.\n> There may be services (LPWA, lpwa.com, is one such) whose legitimate\n> purpose is to provide authentication services for a user, such as\n> replacing special character sequences in Authorization with a user's\n> computed identity.  The proxy ought to be able to do so without being\n> considered non-compliant.\n> \n> 2) Sect 3.2.1, under \"nonce\"\n> ... is the dotted quad IP address ...\n> \n> How to handle IPv6 addresses?\n> \n> 3) Sect 3.2.2, syntax\n> should be\n>     entity-digest = <\"> ...\n>       ^\n> \n> The \"date\" attribute description bears no mention here of what\n> date we're talking about.  I inferred from text much further on\n> that it's supposed to mirror the Date header of the\n> request/response.\n> \n> 4) Sect 3.2.2, semantics\n> \n> Consider sender -> proxy -> receiver.\n> \n> The entity-digest incorporates information from headers from\n> the sender.  Consider, for example, Date and Content-Length.  A\n> proxy could add Date if one were missing.  A proxy could add a\n> Content-Length after gobbling up something that the sender sent\n> \"chunked\".\n> \n> The receiver wouldn't know that the proxy had added those\n> headers.  It would use the added headers in its calculation of\n> entity-digest and derive a different value from what the sender\n> calculated.\n> \n> Dave Kristol\n> \n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "Al Gilman <asgilman@access.digex.net> wrote:\n>Two issues came up in today's session of the HTTP 1.1 WG that\n>left me curious.  Not that any major decisions hang on the\n>answers, but:\n>\n>Lynx came up when the fellow from MicroSoft quipped regarding the\n>305 proxy redirection message \"Lynx has implemented it.\"\n>\n>Later it appeared he meant to be humorous, as it was left as an\n>open question.\n>\n>The Conventional Wisdom in the meeting is that 305 is broken and\n>306 didn't fix it.  The group is headed in the direction that\n>this function will not be present in the \"Draft Standard\" version\n>of 1.1.\n>\n>Going, going...\n\nThe specs for 305 in the most current HTTP/1.1 draft in\neffect describe Lynx's implementation, years ago, but not\ncompletely.  Lynx's implementation:\n\n1) Accepts 305 and acts on the Location: only if a proxy\n   is not already being used, i.e., if it comes from an\n   origin server.  Otherwise, Lynx renders and displays\n   the body.\n2) The redirection is treated as \"one time\" (a new request\n   for the document will go to the orgin server, not to\n   the proxy of the previous redirection).\n3) The method is retained, and if \"unsafe\" (POST), the\n   user is prompted whether to proceed.  That is, the\n   redirection is treated like a 307, but with a proxy\n   request structure:\n     http://proxy[:port]/http://origin[:port]/path[;param?query]\n   It would be better to treat it like a 304 (always use GET),\n   IMHO.\n\nThis does not serve the originally intended purposes, better reflected\nbut still imperfect in the 306 draft.  However, it is no less useful\nthan 304 and 307 (i.e., would not be expected to be used often,\nbut is available if really needed).   In effect, the origin server\ncan redirect either to another origin server (304 or 307) or to a\nproxy (305) which may have the requested document and, for example,\nbe closer to the UA.\n\nIf 306 is revised, it would be better to treat that as\na new status, not a revision of 305, and have 306 based on only a\nSet-Proxy: header, with no Location header.  Browsers which do not\nimplement it thus will treat it as 300, and should show the body\nby virtue of no Location header being present.\n\nWhether or not the \"guys at MicroSoft\" as yet grasp the\noccassional uses to which 304, 305, and 307 might be put, they\nnonetheless can be useful ocassionally (that statement was intended\nto be humorous &#1;).  But 306 does need more work before it's\nintended uses can be achieved.\n\n \n>Second point:  CommentURL on cookies.  General tenor of comment\n>from commercial implementors was that with excellent orienteering\n>skills one would be able to unearth this thing and eventually\n>follow it.  Not in any way that was easy or obvious.  I was just\n>curious if Lynx would be more aggressive about offering the user\n>the option to follow this reference to a discussion of site\n>standards for the user of cookies.\n\nLynx implements CommentURL as links accessible via the\nCookie Jar.  Apparently, the discussions on the HTTP-WG email\nlist about this being for descriptions of the particular cookies\nwhich have a CommentURL attribute still have not gotten through.\nSince the CommentURL could return text/html, a link to yet another\ndocument which discusses the site's \"standards\" could be included,\nbut the document returned via the CommentURL, itself, should\nexplain how that particular cookie will be used (e.g., to maintain\ndisplay preference, or as a shopping cart, or to track retrievals\nand offer appropriate \"what's new\" links within requested documents,\netc.).  The user should be able to make an informed judgment about\nwhether to suffer the overhead and disk space associated with each,\nparticular, cookie.  If it's simply \"generic\" statements about the\nsite's security/privacy policies, it falls short of servering the\nneeds of users with respect to cookie management, particularly as\nmore and more sites set them, and the total number to be handled\nand stored would otherwise get rather large (this assumes other\nbrowsers will follow Lynx's lead in offering a substantive cookie\nmanagement interface by which users easily can remove particular,\npreviously or \"tentatively\" accepted, cookies from the database).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "Could the spec be fixed without interoperability trouble emerging?\n\n(Query to digest implementers???).\n- Jim\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Tue, 9 Dec 1997, Jim Gettys wrote:\n\n> Could the spec be fixed without interoperability trouble emerging?\n> \n> (Query to digest implementers???).\n> - Jim\n> \n\nMost of the suggestions by Paul and Dave seem to be clarifications\nof the original intent.  They should not cause problems.\n\nThe one significant change is Paul's suggested change of the algorithm\nfor calculating the \"entity-digest\".  If implementations exist they\nwill be incompatible.  I don't think I would describe this as\nfixing a \"bug\" in the entity-digest algorithm.  It might be an\nimprovement though.  On the other hand, if I recall correctly it\nwas Paul who wrote the entity-digest algorithm, so he may have a\nright to call it a bug.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "PEP draft 5 (21 Nov 1997",
            "content": "The latest PEP draft (draft-ieft-http-pep-05) has an inconsistency\nbetween the BNFs for <bagname> and <map>.  Specifically, <ext-decl>\nis claimed to be a <bag>, meaning that <map> must be a legal\n<bagname>.  <bagname> is defined as just a <token>, but <map> looks\nmore like a <bag> - it takes the form \"{ map ...}\".  From the rest of\nthe spec, it looks to me like the <ext-decl> BNF is defined as intended,\nand that <bagname> should be defined as something like:\n\n   bagname = token | bag\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Can I add a link to your site from mine",
            "content": "Hello,\n\nI just looked at your site and I really liked it!\nI have a site where I list this weeks top 20 sites\non the internet. I have different categories, and\neventually want to have a lot more. There's not\nmuch there now, but I'm just starting out! I will link\nthe best sites on my page every two weeks.\nWould you consider letting me link your site to my \npages?\nMy site is at:\n         http://comevisit.com/joanne/INDEX.HTML\n\nThank you very much!\n\nSincerely,\nJoanne Stevens\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "Jim Gettys wrote:\n> \n> Could the spec be fixed without interoperability trouble emerging?\n> \n> (Query to digest implementers???).\n\nApache's Digest Auth is marked as experimental, and since no\nwidely-deployed browser implements Digest, I'd guess it won't be a big\ndeal.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "John Franks wrote:\n> [...]\n> Most of the suggestions by Paul and Dave seem to be clarifications\n> of the original intent.  They should not cause problems.\n> [...]\n\nI still feel my one objection about proxy-added headers is substantive\nand unresolved.  Briefly, an origin server might omit headers that get\nfigured into the entity-digest calculation.  A proxy might subsequently\nadd those headers.  The client sees a message *with* the headers,\ncalculates an entity-digest that figures them in, and gets a different\nanswer from what the origin server calculated.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Tue, 9 Dec 1997, Dave Kristol wrote:\n\n> I still feel my one objection about proxy-added headers is substantive\n> and unresolved.  Briefly, an origin server might omit headers that get\n> figured into the entity-digest calculation.  A proxy might subsequently\n> add those headers.  The client sees a message *with* the headers,\n> calculates an entity-digest that figures them in, and gets a different\n> answer from what the origin server calculated.\n> \n> Dave Kristol\n> \n\nI agree that there is an issue here.  The current spec says the\nproxy MUST not add these headers.  If I recall you suggested the\nMUST be changed to SHOULD.  I am not sure how this helps beyond\nmaking the proxy technically \"legal.\"  It doesn't materially affect\nthe problem.\n\nWhat should a proxy do in this situation?  It seems it must either\nnot add headers or break the entity-digest.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "John Franks wrote:\n> \n> On Tue, 9 Dec 1997, Dave Kristol wrote:\n> \n> > I still feel my one objection about proxy-added headers is substantive\n> > and unresolved.  Briefly, an origin server might omit headers that get\n> > figured into the entity-digest calculation.  A proxy might subsequently\n> > add those headers.  The client sees a message *with* the headers,\n> > calculates an entity-digest that figures them in, and gets a different\n> > answer from what the origin server calculated.\n> [...]\n> I agree that there is an issue here.  The current spec says the\n> proxy MUST not add these headers.  If I recall you suggested the\n> MUST be changed to SHOULD.  I am not sure how this helps beyond\n> making the proxy technically \"legal.\"  It doesn't materially affect\n> the problem.\n\nUmmm...  I think my \"MUST -> SHOULD\" had to do with a proxy's changing\nthe content of headers.  I think I see the words to which you're\nreferring (end of p.13), and they mention Content-Length explicitly but\ndon't mention Date.  And there's a potential problem with\nContent-Length:  suppose a proxy eats chunked data and wants to create a\ncomplete entity *with* Content-Length.  Is it hereby forced to forward\nthe entity as \"chunked\" because it's forbidden to add Content-Length?\n> \n> What should a proxy do in this situation?  It seems it must either\n> not add headers or break the entity-digest.\n\nI agree it's a dilemma.  An option is to require that clients send\nContent-Length and (perhaps) not Date, and forbid proxies to add either\nwithin this context.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "Dave Kristol wrote:\n> \n> John Franks wrote:\n> >\n> > On Tue, 9 Dec 1997, Dave Kristol wrote:\n> >\n> > > I still feel my one objection about proxy-added headers is substantive\n> > > and unresolved.  Briefly, an origin server might omit headers that get\n> > > figured into the entity-digest calculation.  A proxy might subsequently\n> > > add those headers.  The client sees a message *with* the headers,\n> > > calculates an entity-digest that figures them in, and gets a different\n> > > answer from what the origin server calculated.\n> > [...]\n> > I agree that there is an issue here.  The current spec says the\n> > proxy MUST not add these headers.  If I recall you suggested the\n> > MUST be changed to SHOULD.  I am not sure how this helps beyond\n> > making the proxy technically \"legal.\"  It doesn't materially affect\n> > the problem.\n> \n> Ummm...  I think my \"MUST -> SHOULD\" had to do with a proxy's changing\n> the content of headers.  I think I see the words to which you're\n> referring (end of p.13), and they mention Content-Length explicitly but\n> don't mention Date.  And there's a potential problem with\n> Content-Length:  suppose a proxy eats chunked data and wants to create a\n> complete entity *with* Content-Length.  Is it hereby forced to forward\n> the entity as \"chunked\" because it's forbidden to add Content-Length?\n> >\n> > What should a proxy do in this situation?  It seems it must either\n> > not add headers or break the entity-digest.\n> \n> I agree it's a dilemma.  An option is to require that clients send\n> Content-Length and (perhaps) not Date, and forbid proxies to add either\n> within this context.\n\nAlternatively, you exclude those headers from the digest?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "> ----------\n> From: Dave Kristol[SMTP:dmk@bell-labs.com]\n> Sent: Tuesday, December 09, 1997 11:56 AM\n> To: John Franks\n> Cc: Jim Gettys; Paul Leach; Eric_Houston/CAM/Lotus@lotus.com; Scott\n> Lawrence; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: Proposal for new HTTP 1.1 authentication scheme\n> \n> John Franks wrote:\n> > [...]\n> > Most of the suggestions by Paul and Dave seem to be clarifications\n> > of the original intent.  They should not cause problems.\n> > [...]\n> \n> I still feel my one objection about proxy-added headers is substantive\n> and unresolved.  Briefly, an origin server might omit headers that get\n> figured into the entity-digest calculation.  A proxy might subsequently\n> add those headers.  The client sees a message *with* the headers,\n> calculates an entity-digest that figures them in, and gets a different\n> answer from what the origin server calculated.\n> \nI agree that this hasn't been addressed. I don't think it'll be a problem in\npractice -- implementors would quickly discover that Message Digest didn't\nwork if the origin server omits any headers and proxies add them.  It would\nbe (at least) nice to be clear about this, though.\n\nThere are two ways to fix the problem -- \n1. Say that origin servers can't omit the headers\n2. Say that proxies can't add them when using Message Disgest.\n\nI don't know which is best. For Date, at least, it seems silly to omit it.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "> I don't know which is best. For Date, at least, it seems silly to omit it.\n\n  We've been over this before:\n\nSome servers do not have clocks; they _can't_ send dates.\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "Sorry. Then the only fix is to say that proxies can't add Date when Digest\nis in use.\n\n> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Tuesday, December 09, 1997 1:58 PM\n> To: Paul Leach\n> Cc: John Franks; 'Dave Kristol'; Jim Gettys;\n> Eric_Houston/CAM/Lotus@lotus.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: RE: Proposal for new HTTP 1.1 authentication scheme\n> \n> \n> > I don't know which is best. For Date, at least, it seems silly to omit\n> it.\n> \n>   We've been over this before:\n> \n> Some servers do not have clocks; they _can't_ send dates.\n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: Can I add a link to your site from mine",
            "content": "At 11:41 AM 4/5/97, Joanne Stevens wrote:\n>Hello,\n>\n>I just looked at your site and I really liked it!\n>I have a site where I list this weeks top 20 sites\n>on the internet. I have different categories, and\n>eventually want to have a lot more. There's not\n>much there now, but I'm just starting out! I will link\n>the best sites on my page every two weeks.\n>Would you consider letting me link your site to my\n>pages?\n>My site is at:\n>         http://comevisit.com/joanne/INDEX.HTML\n>\n>Thank you very much!\n>\nSure.\nThanks Joanne. If you could, please visit: http://www.edgemedia.net\nIt is a sister company and would be very greatful if it could be linked as well.\nThanks Alot!\n\n==============================================================================\n                                   Eric Harley\n\n\"Of all God's creatures there is only one that cannot be made the slave of\nthe lash. That one is the cat. If man could be crossed with the cat it\nwould improve man, but deteriorate the cat.\"\n        -Mark Twain\n\nEmail:  eric.harley@powerwareintl.com\nWeb:    http://www.powerwareintl.com/staff/erich/\nPGP:    http://www.powerwareintl.com/staff/erich/pgp.txt\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Tuesday, December 09, 1997 8:30 AM\n> \n> .  On the other hand, if I recall correctly it\n> was Paul who wrote the entity-digest algorithm, so he may have a\n> right to call it a bug.\n> \nYour recollection is correct.\nI talked to Scott Lawrence, and his products implements entity-digest, but\nhe said he'd be glad to change if we (MS) implement it that way. (We\ncurrently have no implementation.) SInce WebDAV seems like it will require\nDigest, and we love WebDAV, there's a good chance we will be implementing\nit.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Tue, 9 Dec 1997, Dave Kristol wrote:\n\n> \n> Ummm...  I think my \"MUST -> SHOULD\" had to do with a proxy's changing\n> the content of headers.  \n\nSorry, my mistake.\n\n> I think I see the words to which you're\n> referring (end of p.13), and they mention Content-Length explicitly but\n> don't mention Date.  And there's a potential problem with\n> Content-Length:  suppose a proxy eats chunked data and wants to create a\n> complete entity *with* Content-Length.  Is it hereby forced to forward\n> the entity as \"chunked\" because it's forbidden to add Content-Length?\n> \n  ...snip..\n\n> I agree it's a dilemma.  An option is to require that clients send\n> Content-Length and (perhaps) not Date, and forbid proxies to add either\n> within this context.\n> \n\nHere is what the spec says:\n\n       The entity-info elements incorporate the values of the URI used\n       to request the entity as well as the associated entity headers\n       Content-Type, Content-Length, Content-Encoding, Last-Modified,\n       and Expires. These headers are all end-to-end headers (see\n       section 13.5.1 of [2]) which must not be modified by proxy\n       caches. The \"entity-body\" is as specified by section 10.13 of [2]\n       or RFC 1864. The content length MUST always be included. The\n       HTTP/1.1 spec requires that content length is well defined in all\n       messages, whether or not there is a Content-Length header.\n\nI was remembering \"which must not be modified by proxy caches\" as\n\"which MUST NOT be modified by proxy caches.\"\n\nI guess I don't see a problem with this.  On the question of length it\nsays the content length must be used in the digest even if there is no\nContent-Length header.  This seems fine and should cause no problem if\na proxy unchunks a chunked response.  The server has to calculate the\nMD5 digest of the entity so it will not be much harder to calculate\nthe length.  I guess the proxy better get the length right or the\nclient better do its own length calculation and not trust the proxy.\n\nAs for Date, I guess the only problem is servers with no clock.\nThey don't send a Date header.  Draft-v11-rev01  says \n\n       A received message that does not have a Date header field MUST be\n       assigned one by the recipient if the message will be cached by that\n       recipient or gatewayed via a protocol which requires a Date.\n\nSo it seems that it is fine for the proxy to forward the dateless\nresponse as long as it does not cache the entity.  It is unlikely\nthat an authenticated response should be cached anyway.\n\nMaybe there are problems I don't understand.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "Paul:\n\n% I talked to Scott Lawrence, and his products implements entity-digest, but\n% he said he'd be glad to change if we (MS) implement it that way. (We\n% currently have no implementation.) SInce WebDAV seems like it will require\n% Digest, and we love WebDAV, there's a good chance we will be implementing\n% it.\n\nA not-too-much-correlated thought:\n\nI was wondering why Digest does not use a salt, so that the server needn't\nkeep the password in clear.\n\nOr is it me who misread the RFC?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Wed, 10 Dec 1997, Maurizio Codogno wrote:\n\n> \n> I was wondering why Digest does not use a salt, so that the server needn't\n> keep the password in clear.\n> \n\nWell, it does and it doesn't keep the password in the clear. \nThe server keeps a one-way hash of \"username:realm:password\".\nThis means that the user can use the same username and password \nfor multiple sites/realms and someone with access to the password\nfile at one site cannot use that information to impersonate the\nuser at another site.\n\nBut you are right in the sense that gaining access to the password\nfile for one site/realm does allow an attacker to impersonate a\nuser at that site/realm.\n\nI think that adding a salt doesn't really help in any way. If by salt\nyou mean something known to the client as well as the server then it\nis either secret, and effectively part of the password, or public, and\neffectively part of the realm name.  If by salt you mean something\nknown only to the server and you don't allow the password sent in\nthe clear then you would need to encrypt *and decrypt* the password\non the server.  Because of patent restrictions and government export\nrestrictions nothing involving encryption/decryption is acceptable\nin this authentication method.  Digest authentication uses only\none-way hash functions which are unrestricted by patent and freely\nexportable.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Tue, 9 Dec 1997, Scott Lawrence wrote:\n\n> \n> > I don't know which is best. For Date, at least, it seems silly to omit it.\n> \n>   We've been over this before:\n> \n> Some servers do not have clocks; they _can't_ send dates.\n\nSo lets define a fixed pseudo-date that has the meaning that 'This server\ncan't support dates'.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": ">Sorry. Then the only fix is to say that proxies can't add Date when Digest\n>is in use.\n\nDate should not be part of the digest -- it gets updated at times\nwhen the entity does not change (think 304).  Likewise, the\nContent-Length header field should not be part of the digest;\ninstead, include the actual value for the size of the entity-body.\n\nKeep in mind the distinction between general-header fields indicating\nproperties of the message, and entity-header fields indicating properties\nof the payload of the message.  Aside from HEAD responses, Content-Length\nindicates the size of the message-body and not the entity-body.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "Paul Leach wrote:\n\n> There are two ways to fix the problem --\n> 1. Say that origin servers can't omit the headers\n> 2. Say that proxies can't add them when using Message Disgest.\n> \n> I don't know which is best. For Date, at least, it seems silly to omit it.\n\nI think we're looking at the problem from different directions.  A\n*sender* produces an entity-digest.  I look at the problem from the\nperspective of an origin server *receiving* the entity-digest.  In this\ncase the sender is a client (user-agent), possibly doing a PUT or POST. \nClients seldom send Date.  Proxies could (conceivably) add them.\n\nI like Benjamin Franz's suggestion of a fixed date that means \"this is\nnot a date\" as a placeholder.\n\nContent-length is another matter.  If the client sends the entity with\nchunked encoding, it probably does not know the content length, although\nit may calculate an entity-digest on the fly (and add it as a\ntrailer?).  But the proxy may coalesce the entity and add a\nContent-length header.  Now what?  The entity-digest as calculated by\nthe two parties will be different because of the Content-length.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Tue, 9 Dec 1997, Dave Kristol wrote:\n> \n> I like Benjamin Franz's suggestion of a fixed date that means \"this is\n> not a date\" as a placeholder.\n\nYes, this makes sense.\n\n> Content-length is another matter.  If the client sends the entity with\n> chunked encoding, it probably does not know the content length, although\n> it may calculate an entity-digest on the fly (and add it as a\n> trailer?).  But the proxy may coalesce the entity and add a\n> Content-length header.  Now what?  The entity-digest as calculated by\n> the two parties will be different because of the Content-length.\n> \n\nI believe that entity headers like Content-MD5, Content-Length,\netc. refer to the entity after any Content-encoding has been applied,\nbut before any hop-by-hop Transfer-encoding has been applied.  \n\nThe specification is pretty explicit about this:\n\n       \"Entity-header fields define optional metainformation about the\n       entity-body...\"\n\n\n       \"The entity-body is obtained from the message-body by decoding\n       any Transfer-Encoding that may have been applied to ensure safe\n       and proper transfer of the message.\"\n\nThis is important as it would be impossible to put Content-MD5 in a\nchunked trailer, if the MD5 hash was calculated on the chunked entity.\nLikewise Authentication-Info is explicitly allowed in a chunked\ntrailer and it depends on the Content-Length.\n\nIn any case, it seems the Content-length of an entity is the length\nbefore any chunking.  Thus if a proxy removes chunking and adds a\nContact-Length header that should not introduce any errors in \nentity-digest calculation.\n\nAm I missing something?\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "Having the info in two places isn't necessarily a bad thing. Each\nsolution has its place.\n\nOh the DHCP end of things I am looking at the URL as not just a place to\nget proxy configuration but as a place to get any and all configuration\ninformation I might need. It is the \"entry\" point from where I can start\nmaking requests for all sorts of interesting things.\n\nYaron\n\n> -----Original Message-----\n> From:Stuart Kwan \n> Sent:Friday, March 28, 1997 3:12 PM\n> To:'nemo/Joel N. Weber II'\n> Cc:http-wg@cuckoo.hpl.hp.com; josh@netscape.com\n> Subject:RE: ID:  Proxy autoconfig\n> \n>    2)  Using DNS will not work for mobile clients.  For example,\n> consider a\n>    laptop named SKWAN01.INTRA.MICROSOFT.COM.  While plugged into\n> the\n>    Microsoft corporate net, it queries for and receives the TXT\n> RR:\n> \n>    w3-ns-pac.intra.microsoft.com. IN TXT\n>    \"service:yp-http://proxy1.intra.microsoft.com:8080/proxy.ins\"\n> \n>    I unplug my laptop and take it on a visit to Netscape.  When\n> I plug into\n>    the Netscape corporate network, I query for the TXT RR per\n> above and the\n>    query fails.  At this point, I have no way of finding the\n> proxy servers\n>    for that network, and automatic configuration fails.\n> \n> I don't really follow this logic.\n> \n> When you visit Netscape, I assume you'll get a different IP\n> address.\n> I also assume that means you'll have a different hostname--maybe\n> visitor01.intra.netscape.com\n> \n> Pardon that.  I was assuming dynamic DNS.  Future Microsoft clients\n> (and\n> DNS servers) will be dynamic-DNS enabled.  When they receive a new IP\n> address, they will register that address under their name.\n> \n> However, you make a good point.  When I plug my laptop in at Netscape,\n> I\n> need a new IP addr.  Either 1) they are running DHCP and this\n> discussion\n> is moot, the client can receive the URL via DHCP, or 2) I have dig\n> into\n> my laptop and enter a new IP address by hand - in which case a) the\n> client could do a reverse lookup to determine it's name, or b) I could\n> hand configure the new name, or c) I could hand enter the proxy\n> information.  After all, some sysadmin is going to have to give me an\n> IP\n> (and DNS IPs, and gateways, etc etc), I might as well be given the\n> proxy\n> server info too.\n> \n> I am also not opposed to storing this information in two places.  I am\n> only concerned that we solve the automatic configuration problem.\n> \n> Cheers,\n> - Stuart Kwan\n> Microsoft Corp.\n\n\n\n"
        },
        {
            "subject": "AcceptRange",
            "content": "     Hi,\n\n      RFC2068 for HTTP/1.1 specifies Accept-Ranges to be a response header\n      field in Section 14.5. But there is no mention of this field in\n      the BNF for response header fields in Section 6.2.\n\n      Accept-Ranges seems to have been grouped along with the other Accept*\n      fields of the request-header, but does not find a place in the BNF\n      for request-header fields also.\n\n      Could anybody clarify this?\n\n      Regards,\n      Ramya T.V.\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": "> -----Original Message-----\n> From:Foteos Macrides [SMTP:MACRIDES@SCI.WFBR.EDU]\n> Sent:Tuesday, December 09, 1997 7:02 AM\n> To:lynx-dev@sig.net\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: LYNX-DEV two curiosities from IETF HTTP session.\n> \n> Al Gilman <asgilman@access.digex.net> wrote:\n> >Two issues came up in today's session of the HTTP 1.1 WG that\n> >left me curious.  Not that any major decisions hang on the\n> >answers, but:\n> >\n> >Lynx came up when the fellow from MicroSoft quipped regarding the\n> >305 proxy redirection message \"Lynx has implemented it.\"\n> >\n> >Later it appeared he meant to be humorous, as it was left as an\n> >open question.\n> >\n> >The Conventional Wisdom in the meeting is that 305 is broken and\n> >306 didn't fix it.  The group is headed in the direction that\n> >this function will not be present in the \"Draft Standard\" version\n> >of 1.1.\n> >\n> >Going, going...\n[Joshua Cohen]  \nWell, I guess its time for me to fess up.\nIm the guy who \"quipped\".  Im also the guy who wrote the 305/306\ndraft to specify \nthe 305 and 306 so that they are usable.\nUnfortunately, we simply couldnt resolve the security implications\nin time\nand leaving them in the http/1.1 draft puts the entire protocol at\nrisk from\na process point of view.\nBefore anyone jumps at dont put process over function, if we had a\ngood\nresolution, Id be pushing for it in the core protocol.  We dont, and\nuntil\nwe do, it needs to wait..\n>  \n> The specs for 305 in the most current HTTP/1.1 draft in\n> effect describe Lynx's implementation, years ago, but not\n> completely.  Lynx's implementation:\n> \n[Joshua Cohen]  [--snipped--] \n> If 306 is revised, it would be better to treat that as\n> a new status, not a revision of 305, and have 306 based on only a\n> Set-Proxy: header, with no Location header.  Browsers which do not\n> implement it thus will treat it as 300, and should show the body\n> by virtue of no Location header being present.\n> \n> Whether or not the \"guys at MicroSoft\" as yet grasp the\n> occassional uses to which 304, 305, and 307 might be put, they\n> nonetheless can be useful ocassionally (that statement was intended\n> to be humorous &#1;).  But 306 does need more work before it's\n> intended uses can be achieved.\n> \n[Joshua Cohen]  This \"guy at microsoft\" gets it, and I beleive\nthat most of my colleagues do as well.   As I said earlier, the\n305-306 draft (which was supposed to roll into http/1.1)\nwas my doing. \n\nPlease dont ascribe any higher meaning to the fact that\na \"microsoft\" guy spoke about temporarily holding\n305/306 from the http/1.1 draft.  Im still enthusiastic\nabout the 305/306 functionality but until\nwe can resolve the very real security implications\nit is prudent to withold it from the draft..\n\n\nJosh Cohen <joshco@microsoft.com>\n>   \n\n\n\n"
        },
        {
            "subject": "Unidentified subject",
            "content": "Unfortunately, the comments of the group are beyond my comprehension at\nthis time and I am unable to respond.  I do not yet understand the digest\nauthentication scheme well enough to comment...\n-e\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": ">> Digest authentication already includes a mechanism (the 'domain'\n>> attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\n>> specify that credentials may be used on multiple servers,\nMy vision is to move away from requiring directories to be replicated on\ncontent servers.  While the re-direction for authentication may seem\nawkward, it is essentially equivalent to doing the directory transport in\nreal time via LDAP on the back end.  I don't know much about digest\nauthentication just yet (but I'm getting the feeling that I am going to be\nlearning about it real soon) but I'm not convinced that it is solving the\nright problems...\n\n>> There is also the model of Kerberos to consider - developing a\n>> ticket-based authentication scheme (with the advantages and problems of\n>> any third-party mechanism) would be another area to explore.\nWhen you say Kerberos, do you mean OMTX signed URLs?  They are excellent\nfor associating arbitrary authorization semantics with a URL.  They include\nthe semantics and a MAC for security.  That is what I had in mind when I\nsaid that the browser should cache another string of arbitrary length: the\nimplementation can use it for ticketing.\n-e\n\n\n\n\nScott Lawrence <lawrence@agranat.com> on 12/05/97 01:53:46 PM\n\nTo:   Eric Houston/CAM/Lotus\ncc:   http-wg@cuckoo.hpl.hp.com\nSubject:  Re: Proposal for new HTTP 1.1 authentication scheme\n\n\n\n\n\nDigest authentication already includes a mechanism (the 'domain'\nattribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\nspecify that credentials may be used on multiple servers, and through the\n'digest' attribute allows for mutual authentication.\nThere is also the model of Kerberos to consider - developing a\nticket-based authentication scheme (with the advantages and problems of\nany third-party mechanism) would be another area to explore.\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "I don't quite get it...  But it sounds like digest authentication does some\nneat stuff.  So is this in common usage?  Do existing servers and browsers\nimplement this fully?\n-e\n\n\n\n\n\nJohn Franks <john@math.nwu.edu> on 12/07/97 08:43:59 AM\n\nTo:   Scott Lawrence <lawrence@agranat.com>\ncc:   Eric Houston/CAM/Lotus, http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject:  Re: Proposal for new HTTP 1.1 authentication scheme\n\n\n\n\nOn Fri, 5 Dec 1997, Scott Lawrence wrote:\n>\n> Digest authentication already includes a mechanism (the 'domain'\n> attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to\n> specify that credentials may be used on multiple servers, and through the\n> 'digest' attribute allows for mutual authentication.\n>\n> There is also the model of Kerberos to consider - developing a\n> ticket-based authentication scheme (with the advantages and problems of\n> any third-party mechanism) would be another area to explore.\n>\nI believe that the original intent of the \"opaque\" field in the digest\nauthentication header may have been precisely for such a ticket.  A\nrequest could be referred to an \"authentication server\" which would\nredirect to a server that could check the ticket in the opaque field\nand satisfy the request.  In this way only the authentication server\nwould need to know all user passwords.  The document servers would\nonly need to know a single secret shared with the authentication\nserver.\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "What is ContentLength",
            "content": "Studying the specification some more I see there seems to be\nsome ambiguity about the meaning of Content-length.  Here are\nsome quotes:\n\n       7.1 Entity Header Fields\n\n       Entity-header fields define optional metainformation about the entity-\n       body or, if no body is present, about the resource identified by the\n       request.\n\n              entity-header  = ...\n                             | Content-Length           ; Section 14.14\n                             ...\n       7.2.2 Length\n\n       The length of an entity-body is the length of the message-body after any\n       transfer codings have been removed...\n\n\nBut later we have\n\n       14.14 Content-Length\n\n       The Content-Length entity-header field indicates the size of the\n       message-body, in decimal number of OCTETs, sent to the recipient...\n\nThese seem inconsistent.  If Content-Length means the length after\ntransfer encodings have been applied then it is hop-by-hop and not\nend-to-end.  It also cannot be an entity header as described in\n7.1.  There probably is also a need for a header meaning entity-length.\n\nPersonally I would like to see Content-Length remain an entity header.\nAll the other Content-* headers are entity headers and apply to the\nentity before transfer encoding.\n\nOne way to do this would be to introduce a new \"Transfer-Length\"\nheader with the stipulation that its default value is the\nContent-Length.  The Content-Length would be defined as it is now in\nsection 7, i.e. the entity length.  Thus the Transfer-Length header\nwould only be needed when the message length and entity length\ndiffered.  This would give us consistent terminology (Content-* for\nentity, Transfer-* for message).  It would also not break any current\nof which I am aware.  At present the only widely deployed TE is\nchunked and it needs neither header.  If new TEs arise which need\nto have the message length specified they would have to use \nTransfer-length (or both).\n\nI see no alternative other than rewriting the specification to make\nContent-length a hop-by-hop general header and not an entity header.\nThe authentication specification would also need to be modified \nsince it is not possible to put Authentication-Info in a chunked\ntrailer as it is currently defined if Content-length is the length\nof the chunked message.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "John Franks wrote:\n> [...]\n> In any case, it seems the Content-length of an entity is the length\n> before any chunking.  Thus if a proxy removes chunking and adds a\n> Contact-Length header that should not introduce any errors in\n> entity-digest calculation.\n> \n> Am I missing something?\n\nI think I see where I've gone wrong, sort of.  The definition of\nentity-digest calls for including the content length but not necessarily\nthe Content-length (although that's how it's described).  So it sounds\nlike the sender would have to know the full length of the entity being\nsent before sending it and include that value.  Whether the entity got\nsent chunked or not would not matter (although I think it would be\nperverse to know the entity length and send the entity with chunked).\n\nNotwithstanding John Frank's new message about \"What is Content-Length\",\nI don't want to invoke some kind of apocalyptic revision of the spec.\nand what Content-length means.  I think I can live with this part of the\nauthentication document, assuming the addition of a \"this is not a date\"\nDate header.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Wed, 10 Dec 1997, Dave Kristol wrote:\n\n> \n> Notwithstanding John Frank's new message about \"What is Content-Length\",\n> I don't want to invoke some kind of apocalyptic revision of the spec.\n> and what Content-length means. \n\nI also don't want an apocalyptic revision.  But I honestly read the\nspec and believed, based on section 7, that the Content-length header\nshould contain the entity length.  Clearly others believe that it\nshould contain the message length.  The issue hasn't arisen before\nbecause in (nearly?) all implementations these two agree in all\ninstances where there is a Content-length header.\n\nThe distinction will become increasingly important, and indeed is\nalready for the entity-digest in authentication.  For interoperability\nthe spec has to be very clear on whether the Content-length header is\nthe length before or after TE is applied.  In my view it currently\nexplicitly states contradictory things in sections 7 and 14.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": "Here's what I think needs to happen:\n\nIt looks to me as though we may need a clarification to Rev-01 on 305 (use \nthis proxy for a single request) to to match the existing Lynx behavior, \nif that is actually the \"right thing\" for the desired semantics (single \ntime redirection to a proxy). (previous comments say that we're pretty close \nto that now in Rev-01, but a bit of further tweaking is needed).\n\nAnd the full \"go use this proxy forever\" functionality (i.e. what we called \n306) needs to get addressed somehow, but in an independent (not HTTP/1.1 \ndocument (and concievably, outside of HTTP altogether, if that seems the \nright solution.).  This one nees to deal with all the security/trust problems \nidentified in the discussion on 306 in the mailing list. \n\n - Jim\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "Two new refinements that I would like to make:\n\n 1) When the content server redirects the request to the authentication\nserver, it encrypts the ACL for the protected resource.  The authentication\nserver then validates the user against the (decrypted) ACL and returns the\nfirst matching entry to be cached in the browser.  When the browser is\nqueried for user credentials, the encrypted (authenticated) group\naffiliations are returned to the content server.\n\n2) Could re-directed authentication be layered on top of the existing\nschemes so that it could be used with basic, digest, and X.509?\n\nBTW, all those emails have given new meaning to \"digest authentication\".\nRight now I'm suffering from \"authentication indigestion\"!\n-e ;-)\n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "jg@pa.dec.com (Jim Gettys) wrote:\n>Here's what I think needs to happen:\n>\n>It looks to me as though we may need a clarification to Rev-01 on 305 (use \n>this proxy for a single request) to to match the existing Lynx behavior, \n>if that is actually the \"right thing\" for the desired semantics (single \n>time redirection to a proxy). (previous comments say that we're pretty close \n>to that now in Rev-01, but a bit of further tweaking is needed).\n>\n>And the full \"go use this proxy forever\" functionality (i.e. what we called \n>306) needs to get addressed somehow, but in an independent (not HTTP/1.1 \n>document (and concievably, outside of HTTP altogether, if that seems the \n>right solution.).  This one nees to deal with all the security/trust problems \n>identified in the discussion on 306 in the mailing list. \n\nI thought it had already been decided in the HTTP-WG list\n(http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0245.html) that:\n\n(1) Josh's draft-cohen-http-305-306-responses-00.txt would be\n    dropped from Rev-01;\n(2) Josh and Ari would generate a new 306 proposal (which could\n    include a \"go use this proxy forever\" directive, but has\n    other options, depending on what's in the Set-Proxy: header\n    for that status); and\n(3) 305 would become a one-shot, from-origin-server-only,\n    redirection to a proxy via a Location: header, as described\n    in Rev-01 (draft-ietf-http-v11-spec-rev-01):\n\n10.3.6 305 Use Proxy\n\n The requested resource MUST be accessed through the proxy given\nby the Location field. The Location field gives the URL of the\nproxy. The recipient is expected to repeat this single request\nvia the proxy. 305 responses MUST only be generated by origin\nservers.\n\n\nThat description doesn't say what to do about the method.  My suggestion\nwas to add a sentence about that issue, and my present bias is to make it\n303-like ;(oops, not 304 that I wrote in my earlier message): e.g.:\n\n          The redirected request SHOULD be made with a GET\nmethod.\n\nThat would make 305 utterly safe, and it's not obvious to me why a\nPOST or other non-safe request would ever be redirected to a proxy\nwith intention that the content be retained (it's most likely to be\nused by scripts homologously to 302/303), so I doubt this would pose\na functionality problem.\n\nReleased versions of Lynx prompt the user about 305 redirection\nfor a POST:\n\nP)roceed, use G)ET, or C)ancel?\n\nand users have been \"conditioned\" to choose G or C for such prompts,\nso making 305 303-like should not be a serious problem with those\nversions.  The current Lynx development code (scheduled for release\nas v2.8 this month) treats 305 as 307-like (doesn't include the\noption to use G)ET), but it would take just a few minutes to change\nthat to 303-like behavior for the actual release. \n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Way redirect POST? [was Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "On Wed, 10 Dec 1997, Foteos Macrides wrote:\n\n> That would make 305 utterly safe, and it's not obvious to me why a\n> POST or other non-safe request would ever be redirected to a proxy\n> with intention that the content be retained (it's most likely to be\n> used by scripts homologously to 302/303), so I doubt this would pose\n> a functionality problem.\n\nAs a HTTP based application builder, it is quite clear to me why I want\nto redirect a POST as a POST to another server with/without proxy\nconsiderations. If the POST isn't redirected, all the reasons for\nusing a POST method in the first place get broken IF the intent is\nto have the redirect target process the POST as submitted.\n\nIn one developer community whose mailing list I follow, a request for\nhow to accomplish this function comes up at least once a week ... which\nsuggests to me a much higher demand since each time the request comes\nup some additional percentage of the group learns that it isn't possible\nand some other mechanism is required to forward the POSTed content to\nthe processing server.\n\nI'm not proposing any changes but I wanted to make it clear that there\nis an unaddressed requirement for seamless redirection of POST requests.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Request for HTML Draft to support Multi Direction languages",
            "content": "On Sat, 5 Apr 1997, Erez Levin wrote:\n\n> Dear sirs,\n> \n> I want to bring this issue up.\n> \n> Arabic, Hebrew and other languages are wrriten from right to left. \n> While The latest HTML draft does not have a special tag to specify the\n> direction and takes left to right as the default direction of the\n> language.\n\nThis is indeed a HTML issue, not an HTTP issue. And it has been\naddressed already. Please see RFC 2070!\n\nRegards,Martin (one of the authors of RFC 2070).\n\n\n\n"
        },
        {
            "subject": "Minutes from HTTP/1.1 Implementor's dinne",
            "content": "I'm posting these for Rohit.\n\n- Jim\n\nHTTP 1.1 Implementers' Dinner\nRohit Khare, Dec 9, 1997\n\n[The general idea was to walk through the document looking for capitalized \nMUST\nand MUST NOT requirements. This discussion helped clarify that we defintely \nneed\nautomated tools to track implementations' support for Draft Standard \ndocumentation\n-- we didn't even touch the MAYs and SHOULDs.\n\nWhat follows is a semi-transcript... Rohit]\n\nPrescription of a new issue in 4.4 Message Length:\nnotification to user agents (JG:take to mlist and deal with there)\nSomething about warning to users seems egregious\n\nMUST notify the user is too strong -- is it required for robustness\nin the prot?\n\nLM rules the meta-discussion out of order : are there two interoperable \nimplementations?\nor not? Doesn't seem to be.\n\nClient guys: Henrik Frystyk Nielsen, Yaron Goland\nServer: Rich (IBM), Scott (Agranat),\nProxy: henrik, daniel, yaron, Rich\nClient: Sami Sun (URN resolver)\nServer: Dave, Henry, Jigsaw, Scott\nDave Kristol, Daniel Veillard are also present\n\nYG: there are currently 265 requirments\nYG: will be releasing a spreadsheet of all 265 with what IE4 and IE5 do, by\njanuary.\n\nSection 3.1\ncould be prefaced by leading zeros. MS doesn't know, but will fix if found.\nRich: do handle it. Henry: handles it\n\nHFN: would be more interesting to test if \"1.2\" works as a test case\n\nYG: these answers I'm giving tonght are for IE4 as I know it. BUT,\nwe have libInet, which is eqivalent to libwww\n(which means we have common support). See,\nIE4 complies with \"all IANA charsets\", but other apps ontop of INET may\nignore it.\n\n(ie. their 3rd party developers may be able to pass illegit stuff down \nthrough\nthe api. For example, it doesn't check that mimetypes passed in have no \nwhite\nspace.)\n\n----- Major and minor must be treated as separate integers. (3.1 , 3 \nparagraphs in )\n\n[Rich has a server Don't list ]\n[Henry has don'ts for must and should, but not mays]\n\nJG: WHAT we need is two separate implementations that have been tested to\ninteroperate -- NOT necc shipped, much less commercial and supported. JUST\nhad to be implemented once.\n\nJust a client and server? JG: I'm more comfortable with two each (c,s,p)\nThat's my personal belief\n\nYG: given that we don't need shipment, we may try testing against IE5.\n\nSL: the most important result of today's dinner is to list out what must be\ntested ASAP\n\nYG: now that our testers are up to speed, we're ready to hit anything that\npeople put up for testing.\n\nWe need a standardized template for testing ***\n\nNeed a suite we can automate fromthe client; send us a list of\nURLs we should hit, so we can automate the testing process (YG)\n\nLM: it would be great to do some multivendor proxy chains.\n\nRK: even if we set up a chain from the same vendor...\n\nJG: most features in here can be implemented in 1.0 and that's just fine\n(so let's all put our 1.0 proxies in the test?)\n\nHFN: put needs to be tested (with chunked, byterange, through proxys,\nput things should be tested as rigorously as GET.\n\n....\n\nDo you [ms] have any international test sites?\n\nHenry: we have our own satellite networks, but we need to have plug-nd-play\nboxes shipped in. We have a network simulator (sits between two lans and it\nsets error rate, delay, etc). If we ever do a face-to-face, we'll bring it.\n\nLM: did anyone get back to the connectathon folks.  (Quentin Clark, \ncthon@sun.com)\nScott has been in touch, but no firm plan.\n\nScott asked for a test profile.\n\n<time to order dinner>\n\nHenry: speaking of connectathon, a few years, they did a TCPIP. ALl they\nprovided was powerand space, not too useful. We'd be willing to do that.\n\nYG offers space, power, catering AND a trip to the company store...\n\nSection 3.1 continued, lots of straightfoward ones.\n\nJG suggests reading it, rather than reading aloud.\n\nRich: I'm worried about proxies up and downgrading to versions.\nHenry: I know it does that. Rich thinks so, but doesn't know for sure.\n(RE-VERSION)\n\nReference to the leach / mogul versioning draft. Final text is in the\nspec since munich (JG). Jigsaw does -- I sent a 1.0 req and it upgrades\n(Scott reports).\n\n3.2.2 has a SHOULD which is too srong (numeric IP addresses). IP Addresses\nare FQDNs and fully legal. SHOULD become should -- lowercase.\n\n3.2.3\nHFN: the main kludge there is spaces.\nYG: we're not compliant and *can't be* . Our servers are not case-sensitive\nand we're not going to change it. We had a big long meeting about it...\n\nLM: of COURSE you do this. YG: no, we don't\n\nJG: we have atleast two W3C environments which do it, though.\n\nJG: we've mad w3.org case-INsensitive... /HyperText/MarkUp was confusing \neveryone...\n\nYG: in a perfect world, the server should be case-insensitive and does the \nmapping and sends Location: . But, we work offline on DOS, so we strip case \non the\nclient.\n\nJG: my rule is protocols should be case-preserving but insensitive.\n\nSummary: do we have a third implementation which is case-preserving?\n\nHenry: if you have a client acting as a client-side cache, is the lookup\nbeyond the host-name case sensitive. IE is insensitive. IBM had to \nimplement\nan escaping system to be preserving -- so their proxy does.\n\nScott commits to adding a test case for this.\n\n3.3 -----\n\nmust accept all three date formats (henry, scott, rich, and yaron say yes)\n\n3.3.1 ----\n\nYes, they're silly, but do they work?\n\nPASSED\n\n3.3.2 ---\nSL: WHY is it here? who uses delta-seconds? Editorial issue to JIM\n\n3.4 ---\n\nYG: we are compliant\nRich: we are compliant\n\nHenry: I think the MUST is not necessary, editorial issue -- should it be\nnormative . NEVER MIND -- it's a quote from the MIME source text.\n\n3.5 ---\n\nIE supports deflate and gzip. NOT compress. NOT support x-gzip. Yaron to \nharass.\n\nJG: issue around \"identity\"\n\nRIch: we don't send idenity (though could be configured)\n\nYG: we NEVER send qvalues.\n\nHFN: w3c handles Identity - -HFN to check this.\n\nHenry: we don't know about identity, since it's post-2068.\n\nScott to check as well.\n\nHFN: C-E identity should never happen. ONLY in A-E.\n\n-----\n\nChunked transfer encoding: sent by many.\n\nYG: our proxy is 1.0, hence doesn't do chunked onward (same for Rich)\n\nHenry: IMHO, pipelined PUT and POST is looking for trouble...\n\nHenrik says W3C code is OK.\n\nIBM: has not impl trailers. Scott: no trailers\n\nTHERE ARE NOT TWO IMPLS OF TRAILERS AT THIS TABLE -- no one is generating \nthem!\nTHERE ARE INTEROPERABLE CHUNKS\nTHERE ARE INTEROPERABLE TRAILERS\nBUT NOT TOGETHER>\n\n<Dinner>\n\n\n3.6.1 ----\n\nDONE\n\n3.7 ----\n\nLWS in mime type.\n(client-side only?)\n\nYG: compliant\nW3C: compliant\n\nPROCESS QUESTION: When documenting for DRAFT standard, do you have to \ndocument\nWHICH two, or that there ARE two.\n\nin 3.7.0 -- parametrization of mimetypes, forking of viewer. Case \ninsensitive.\n\nIE ignores charset on mime-type.\neditorial [sic] - YARON Has more  -- \"to the and inform\"\n\nYG: we sniff charset from the stream. We don't do charset. Uses statistical\nalgorithms to guess charset.\n\nLM decides IE4 is NOT compliant because it ignores charset parameters.\nCharset is the RIGHT way.\n\nYG: I spent 2 hours in a life-and-death battle, and we decided life sucks.\nI had i18n experts begging me to do some fixes. We lost. URLs are OPAQUE.\n\nJG: the answer from Jon Postel is that we should have documented who has \ndone which\nthings. We should go by sections and note exceptions.\n\nRESOLVED: there ARE two W3C user-agents that passes parameters correctly\n(including lynx and netscape)\n\nLM: Are there any clients that send charset on put or post.\n(Henrik does put/post with chunked. IE5 will, too)\n\n------\n\nMultipart types\n\nIBM decodes some multiparts.\nMHTML defines a use for multipart.\nLM: do we have implementatons of multipart at all?\nYES: file upload is multipart, and thus:\nMicrosoft admits, yes on client, yes on server.\n\nIBMs proxy may not check that the epologue isn't empty.\n\n----\n3.8\nsyntax of UA -- all compliant\n(Kristol isn't yet)\n----\n3.9\nqvalues\nrathole\nEnd of dinner.\n\n\n\n"
        },
        {
            "subject": "Re: Way redirect POST? [was Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Wed, 10 Dec 1997, Foteos Macrides wrote:\n>> That would make 305 utterly safe, and it's not obvious to me why a\n>> POST or other non-safe request would ever be redirected to a proxy\n>> with intention that the content be retained (it's most likely to be\n>> used by scripts homologously to 302/303), so I doubt this would pose\n>> a functionality problem.\n>\n>As a HTTP based application builder, it is quite clear to me why I want\n>to redirect a POST as a POST to another server with/without proxy\n>considerations. If the POST isn't redirected, all the reasons for\n>using a POST method in the first place get broken IF the intent is\n>to have the redirect target process the POST as submitted.\n>\n>In one developer community whose mailing list I follow, a request for\n>how to accomplish this function comes up at least once a week ... which\n>suggests to me a much higher demand since each time the request comes\n>up some additional percentage of the group learns that it isn't possible\n>and some other mechanism is required to forward the POSTed content to\n>the processing server.\n>\n>I'm not proposing any changes but I wanted to make it clear that there\n>is an unaddressed requirement for seamless redirection of POST requests.\n\nSure, but I was referring to *one-shot* redirection of the\n*browser* to use a proxy via 305.  What to do about the method (and\nwho should act on the redirection) could be specified via the Set-Proxy:\nheader for 306.  For 306, POST or PUT content could be retained, or not,\ndepending on its directives (assuming that a new 306 proposal is\nforthcoming and its security/privacy problems are solved).  Also,\nHTTP/1.1 will have 307 available for redirection to another origin\nserver as ultimate request target with POST or PUT content retained,\nwhich I suspect is what's actually being sought in the discussions to\nwhich you refer (unless you're talking about redirections to be acted\nupon by an intermediate proxy, rather than the browser which initiated\nthe request).  If the browser already is using a proxy, the browser\nmust have been configured to use it (i.e., it can be presumed \"safe\"),\nand would continue to use that proxy for the 307 redirection with POST\nor PUT retained.  If you want to change the browser's proxy, or set one,\nfor a redirected request, plus retain an \"unsafe\" method, you have\n\"unsolved security/privacy problems\" which would preclude keeping 305\nin the upcoming HTTP/1.1 Draft Standard, so let's put that off to a\nfuture draft for 306, and keep 305 for utterly safe, 303-like and\none-shot redirection by an origin server for the browser to seek the\ndocument via a proxy.\n\nA proxy server doesn't process POST content (at least, not\nHTTP/1.0 or HTTP/1.1 proxies).  Only an origin server (or its\nscript) does that.  A one-shot, 305 redirection would be to GET a\ndocument which the origin server (or its script) has reason to\n\"think\" has been cached by the proxy specified in the Location:\nheader, and if the proxy no longer has it in cache, the request will\nbe directed by that proxy back toward the origin server, so that the\nproxy's cache will be refreshed in conjunction with filling the\nbrowser's request, i.e., the \"utterly safe\" 305 is useful (and works,\nbased on implementation experience with Lynx), albeit that a 306 with\nthe originally intended, much greater functionality should still be\nplanned on as a new, separate draft (but *not* as a replacement or\nfurther modification of 305).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: Minutes from HTTP/1.1 Implementor's dinne",
            "content": "liblnet == wininet\nI did offer space, power, and catering but I said I would have to\ninvestigate the company story option. =P\n\nYaron\n\n\n> -----Original Message-----\n> From:Jim Whitehead [SMTP:ejw@ics.uci.edu]\n> Sent:Wednesday, December 10, 1997 1:11 PM\n> To:'http-wg@cuckoo.hpl.hp.com'\n> Subject:Minutes from HTTP/1.1 Implementor's dinner\n> \n> I'm posting these for Rohit.\n> \n> - Jim\n> \n> HTTP 1.1 Implementers' Dinner\n> Rohit Khare, Dec 9, 1997\n> \n> [The general idea was to walk through the document looking for capitalized\n> \n> MUST\n> and MUST NOT requirements. This discussion helped clarify that we\n> defintely \n> need\n> automated tools to track implementations' support for Draft Standard \n> documentation\n> -- we didn't even touch the MAYs and SHOULDs.\n> \n> What follows is a semi-transcript... Rohit]\n> \n> Prescription of a new issue in 4.4 Message Length:\n> notification to user agents (JG:take to mlist and deal with there)\n> Something about warning to users seems egregious\n> \n> MUST notify the user is too strong -- is it required for robustness\n> in the prot?\n> \n> LM rules the meta-discussion out of order : are there two interoperable \n> implementations?\n> or not? Doesn't seem to be.\n> \n> Client guys: Henrik Frystyk Nielsen, Yaron Goland\n> Server: Rich (IBM), Scott (Agranat),\n> Proxy: henrik, daniel, yaron, Rich\n> Client: Sami Sun (URN resolver)\n> Server: Dave, Henry, Jigsaw, Scott\n> Dave Kristol, Daniel Veillard are also present\n> \n> YG: there are currently 265 requirments\n> YG: will be releasing a spreadsheet of all 265 with what IE4 and IE5 do,\n> by\n> january.\n> \n> Section 3.1\n> could be prefaced by leading zeros. MS doesn't know, but will fix if\n> found.\n> Rich: do handle it. Henry: handles it\n> \n> HFN: would be more interesting to test if \"1.2\" works as a test case\n> \n> YG: these answers I'm giving tonght are for IE4 as I know it. BUT,\n> we have libInet, which is eqivalent to libwww\n> (which means we have common support). See,\n> IE4 complies with \"all IANA charsets\", but other apps ontop of INET may\n> ignore it.\n> \n> (ie. their 3rd party developers may be able to pass illegit stuff down \n> through\n> the api. For example, it doesn't check that mimetypes passed in have no \n> white\n> space.)\n> \n> ----- Major and minor must be treated as separate integers. (3.1 , 3 \n> paragraphs in )\n> \n> [Rich has a server Don't list ]\n> [Henry has don'ts for must and should, but not mays]\n> \n> JG: WHAT we need is two separate implementations that have been tested to\n> interoperate -- NOT necc shipped, much less commercial and supported. JUST\n> had to be implemented once.\n> \n> Just a client and server? JG: I'm more comfortable with two each (c,s,p)\n> That's my personal belief\n> \n> YG: given that we don't need shipment, we may try testing against IE5.\n> \n> SL: the most important result of today's dinner is to list out what must\n> be\n> tested ASAP\n> \n> YG: now that our testers are up to speed, we're ready to hit anything that\n> people put up for testing.\n> \n> We need a standardized template for testing ***\n> \n> Need a suite we can automate fromthe client; send us a list of\n> URLs we should hit, so we can automate the testing process (YG)\n> \n> LM: it would be great to do some multivendor proxy chains.\n> \n> RK: even if we set up a chain from the same vendor...\n> \n> JG: most features in here can be implemented in 1.0 and that's just fine\n> (so let's all put our 1.0 proxies in the test?)\n> \n> HFN: put needs to be tested (with chunked, byterange, through proxys,\n> put things should be tested as rigorously as GET.\n> \n> ....\n> \n> Do you [ms] have any international test sites?\n> \n> Henry: we have our own satellite networks, but we need to have\n> plug-nd-play\n> boxes shipped in. We have a network simulator (sits between two lans and\n> it\n> sets error rate, delay, etc). If we ever do a face-to-face, we'll bring\n> it.\n> \n> LM: did anyone get back to the connectathon folks.  (Quentin Clark, \n> cthon@sun.com)\n> Scott has been in touch, but no firm plan.\n> \n> Scott asked for a test profile.\n> \n> <time to order dinner>\n> \n> Henry: speaking of connectathon, a few years, they did a TCPIP. ALl they\n> provided was powerand space, not too useful. We'd be willing to do that.\n> \n> YG offers space, power, catering AND a trip to the company store...\n> \n> Section 3.1 continued, lots of straightfoward ones.\n> \n> JG suggests reading it, rather than reading aloud.\n> \n> Rich: I'm worried about proxies up and downgrading to versions.\n> Henry: I know it does that. Rich thinks so, but doesn't know for sure.\n> (RE-VERSION)\n> \n> Reference to the leach / mogul versioning draft. Final text is in the\n> spec since munich (JG). Jigsaw does -- I sent a 1.0 req and it upgrades\n> (Scott reports).\n> \n> 3.2.2 has a SHOULD which is too srong (numeric IP addresses). IP Addresses\n> are FQDNs and fully legal. SHOULD become should -- lowercase.\n> \n> 3.2.3\n> HFN: the main kludge there is spaces.\n> YG: we're not compliant and *can't be* . Our servers are not\n> case-sensitive\n> and we're not going to change it. We had a big long meeting about it...\n> \n> LM: of COURSE you do this. YG: no, we don't\n> \n> JG: we have atleast two W3C environments which do it, though.\n> \n> JG: we've mad w3.org case-INsensitive... /HyperText/MarkUp was confusing \n> everyone...\n> \n> YG: in a perfect world, the server should be case-insensitive and does the\n> \n> mapping and sends Location: . But, we work offline on DOS, so we strip\n> case \n> on the\n> client.\n> \n> JG: my rule is protocols should be case-preserving but insensitive.\n> \n> Summary: do we have a third implementation which is case-preserving?\n> \n> Henry: if you have a client acting as a client-side cache, is the lookup\n> beyond the host-name case sensitive. IE is insensitive. IBM had to \n> implement\n> an escaping system to be preserving -- so their proxy does.\n> \n> Scott commits to adding a test case for this.\n> \n> 3.3 -----\n> \n> must accept all three date formats (henry, scott, rich, and yaron say yes)\n> \n> 3.3.1 ----\n> \n> Yes, they're silly, but do they work?\n> \n> PASSED\n> \n> 3.3.2 ---\n> SL: WHY is it here? who uses delta-seconds? Editorial issue to JIM\n> \n> 3.4 ---\n> \n> YG: we are compliant\n> Rich: we are compliant\n> \n> Henry: I think the MUST is not necessary, editorial issue -- should it be\n> normative . NEVER MIND -- it's a quote from the MIME source text.\n> \n> 3.5 ---\n> \n> IE supports deflate and gzip. NOT compress. NOT support x-gzip. Yaron to \n> harass.\n> \n> JG: issue around \"identity\"\n> \n> RIch: we don't send idenity (though could be configured)\n> \n> YG: we NEVER send qvalues.\n> \n> HFN: w3c handles Identity - -HFN to check this.\n> \n> Henry: we don't know about identity, since it's post-2068.\n> \n> Scott to check as well.\n> \n> HFN: C-E identity should never happen. ONLY in A-E.\n> \n> -----\n> \n> Chunked transfer encoding: sent by many.\n> \n> YG: our proxy is 1.0, hence doesn't do chunked onward (same for Rich)\n> \n> Henry: IMHO, pipelined PUT and POST is looking for trouble...\n> \n> Henrik says W3C code is OK.\n> \n> IBM: has not impl trailers. Scott: no trailers\n> \n> THERE ARE NOT TWO IMPLS OF TRAILERS AT THIS TABLE -- no one is generating \n> them!\n> THERE ARE INTEROPERABLE CHUNKS\n> THERE ARE INTEROPERABLE TRAILERS\n> BUT NOT TOGETHER>\n> \n> <Dinner>\n> \n> \n> 3.6.1 ----\n> \n> DONE\n> \n> 3.7 ----\n> \n> LWS in mime type.\n> (client-side only?)\n> \n> YG: compliant\n> W3C: compliant\n> \n> PROCESS QUESTION: When documenting for DRAFT standard, do you have to \n> document\n> WHICH two, or that there ARE two.\n> \n> in 3.7.0 -- parametrization of mimetypes, forking of viewer. Case \n> insensitive.\n> \n> IE ignores charset on mime-type.\n> editorial [sic] - YARON Has more  -- \"to the and inform\"\n> \n> YG: we sniff charset from the stream. We don't do charset. Uses\n> statistical\n> algorithms to guess charset.\n> \n> LM decides IE4 is NOT compliant because it ignores charset parameters.\n> Charset is the RIGHT way.\n> \n> YG: I spent 2 hours in a life-and-death battle, and we decided life sucks.\n> I had i18n experts begging me to do some fixes. We lost. URLs are OPAQUE.\n> \n> JG: the answer from Jon Postel is that we should have documented who has \n> done which\n> things. We should go by sections and note exceptions.\n> \n> RESOLVED: there ARE two W3C user-agents that passes parameters correctly\n> (including lynx and netscape)\n> \n> LM: Are there any clients that send charset on put or post.\n> (Henrik does put/post with chunked. IE5 will, too)\n> \n> ------\n> \n> Multipart types\n> \n> IBM decodes some multiparts.\n> MHTML defines a use for multipart.\n> LM: do we have implementatons of multipart at all?\n> YES: file upload is multipart, and thus:\n> Microsoft admits, yes on client, yes on server.\n> \n> IBMs proxy may not check that the epologue isn't empty.\n> \n> ----\n> 3.8\n> syntax of UA -- all compliant\n> (Kristol isn't yet)\n> ----\n> 3.9\n> qvalues\n> rathole\n> End of dinner.\n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "Yaron Goland <yarong@microsoft.com> wrote:\n>I doubt any commercial browser will implement 305 without some very serious\n>security provided to assure that the proxy asking for the one time redirect\n>is going to get it. I would suggest that this problem needs to be dealt with\n>in the large 305/306 context, in a stand alone spec, and that the draft\n>standard for HTTP should simply state that 305 has been deprecated and\n>SHOULD NOT be implemented.\n\nYou apparently haven't yet grasped the changes Jim already has\nmade for 305 in Rev-01.  The 305 can *only* be sent by an origin server.\nDeployed proxies will pass it through to the browser, as they do for\n300, 301, 302, 303 and 307.  Josh's 305/306 draft has been dropped\nfrom Rev-01, with expectation that he (and Ari) will generate a new,\n306-only draft (complementary to a revised OPTIONS draft).  I suppose\na proxy, if already being used by the browser, could (should?) act on\nthe 305, and there shouldn't be a security problem with that if the\n305 is to be handled always as a GET.  If unsafe methods are to be\nretained with 305, instead of postponing that functionality to a new\n306 proposal, then yes, it would be better to drop 305.  But 305 would\nbe useful if it were specified as presently in Rev-01 with the addition\nof a sentence that GET always should be used, and who knows when, if\never, the security/privacy problems with 306 will be solved.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": ">  From: Yaron Goland <yarong@microsoft.com>\n>  Date: Wed, 10 Dec 1997 11:21:51 -0800\n>  To: \"'jg@pa.dec.com'\" <jg@pa.dec.com>, Josh Cohen <joshco@microsoft.com>\n>  Cc: Foteos Macrides <MACRIDES@SCI.WFBR.EDU>, lynx-dev@sig.net,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: RE: LYNX-DEV two curiosities from IETF HTTP session.\n>  \n>  I doubt any commercial browser will implement 305 without some very serious\n>  security provided to assure that the proxy asking for the one time redirect\n>  is going to get it. I would suggest that this problem needs to be dealt with\n>  in the large 305/306 context, in a stand alone spec, and that the draft\n>  standard for HTTP should simply state that 305 has been deprecated and\n>  SHOULD NOT be implemented.\n>  \n>  Yaron\n\nI think you are confused....  In Rev-01, only an origin server is allowed\nto generate a 305 response.  It is authoritative for that resource, so\nthe spoofing problems don't come up (and is the reason for that text being\nin the document...)\n- Jim\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": "On Wed, 10 Dec 1997, Jim Gettys wrote:\n\n> \n> >  From: Yaron Goland <yarong@microsoft.com>\n> >  Date: Wed, 10 Dec 1997 11:21:51 -0800\n> >  To: \"'jg@pa.dec.com'\" <jg@pa.dec.com>, Josh Cohen <joshco@microsoft.com>\n> >  Cc: Foteos Macrides <MACRIDES@SCI.WFBR.EDU>, lynx-dev@sig.net,\n> >          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> >  Subject: RE: LYNX-DEV two curiosities from IETF HTTP session.\n> >  \n> >  I doubt any commercial browser will implement 305 without some very serious\n> >  security provided to assure that the proxy asking for the one time redirect\n> >  is going to get it. I would suggest that this problem needs to be dealt with\n> >  in the large 305/306 context, in a stand alone spec, and that the draft\n> >  standard for HTTP should simply state that 305 has been deprecated and\n> >  SHOULD NOT be implemented.\n> >  \n> >  Yaron\n> \n> I think you are confused....  In Rev-01, only an origin server is allowed\n> to generate a 305 response.  It is authoritative for that resource, so\n\nBut what is there about the protocol which allows this requirement to\nbe enforced?\n\n> the spoofing problems don't come up (and is the reason for that text being\n> in the document...)\n\nSeems to me that the protocol relies on gentle (i.e., conforming)\nbehavior by proxies and servers. The definition of spoofing includes an\nelement of malicious intent.\n\nPerhaps it would close the loop to require that the client only \naccept a 305 status from an origin server to which it is directly\nconnected?\n\nThen the potential for spoofing would be limited to IP and/or DNS\nspoofing.\n\nIn either case though, I fail to see the motivation for someone\nintending to spoof a redirect who wouldn't simply change the\norigin server or make the IP spoofed variant of the origin server\nsimply serve the nefarious content which would have be acquired\nas a result of the redirect.\n\nThus I don't see any difference in risk with/without 305.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1-TCP interactions (was Re: HTTP Connection Management(draft-ietf-http-connection00.txt)",
            "content": "Jim, Alan,\n\nBelow is the result of internal agreement in BT on a response to your draft\nin rough order of priority:\n\n1. Full burst after connection idle could break Internet\n========================================================\n>       * clients and proxies SHOULD close idle connections.  Most of the\n>         benefits of an open connection diminish the longer the\n>         connection is idle: the congestion state of the network is a\n>         dynamic and changing phenomena [Paxson]. The client, better\n>         than a server, knows when it is likely not to revisit a site.\n>         By monitoring user activity, a client can make reasonable\n>         guesses as to when a connection needs closing.  Research has\n>         shown [Mogul2] shows that most of the benefits of a persistent\n>         connection are likely to occur within approximately\n>         60 seconds. Further research in this area is needed.  On the\n>         client side, define a connection as \"idle\" if it meets at least\n>         one of these two criteria:\n>\n>             * no user-interface input events during the last 60 seconds\n>               (parameter value shouldn't be defined too precisely)\n>\n>             * user has explicitly selected a URL from a different\n>               server. Don't switch just because inlined images are from\n>               somewhere else! Even in this case, dally for some seconds\n>               (e.g., 10) in case the user hits the \"back\" button.\n>         On the server side, use a timeout that is adapted based on\n>         resource constraints: short timeout during overload, long\n>         timeout during underload.  Memory, not CPU cycles, is likely to\n>         be the controlling resource in a well-implemented system.\n>\n>\nThis problem is big because the definition of \"idle\" is an aggregate of two\nfactors.\n\nThe nub of the problem is that even if we come up with an algorithm for how\nlong to idle before close (holy grail?), as the draft says, there are *two*\nnot just one reason for leaving the connection open:\n1.1 not losing the learned congestion window (not having to go back to slow\nstart on a new connection unnecessarily)\n1.2 not having to suffer the pain of tearing down and re-establishing the\nconnection itself.\n\nIf there was just the former (e.g. if you used Transaction TCP), you would\nclose the connection after a few Round Trip Times (RTTs).\nBecause there is the latter, any algorithm will tend to leave the\nconnection open much longer than the congestion window is useful to keep.\n\nApart from the fact that this points to the need for whatever algorithm is\nproposed to be dependent on the transport (which isn't fixed as just being\nTCP, remember), there is a BIG problem with TCP itself.\n\nTCP best practice RFC2001 on \"TCP Slow Start, Congestion Avoidance, Fast\nRetransmit and Fast Recovery Algorthms\" issued in Jan '97 is silent about\nwhat TCP congestion window should do during a period of idle. This is\nprimarily because it hasn't been a problem with current apps using the\nInternet. This is not HTTP-wg problem, but the fact that HTTP/1.1\ndefinitions of \"idle\" will probably be done all sorts of ways means TCP\nneeds to protect the Internet. For instance 60secs is orders of magnitude\nabove what the network treats as idle. \n\nAs the draft says, if a TCP stack leaves the congestion window static\nduring idle, by the time the user clicks again (even after 500msec in many\ncases) the balance point of the congestion windows of all the traffic along\nthe route will typically have changed considerably from when the idle\nstarted. Thus the response gets injected at full whack into a network that\nhas probably re-used all the bandwidth share that the idling connection had\nwon during preceding transmission burst.\n\nCurrent situation is that BSD TCP drops back to slow start after (1RTT +\n(standard deviation of RTT)). However, latest Windows stacks are tending to\nbecome a lot more aggressive to appear to be faster at the expense of\neveryone else (according to Van Jacobsen in the Reliable Multicast IRTF W-G).\n\nHTTP W-G should shout loudly at the TCP Implementation W-G to get best\npractice for idle connections documented given the majority holding of HTTP\non Internet traffic.\n\nHaving a standard doesn't mean selfish implementers have to keep to it, but\nit's better than not having a standard.\n\n2. Who closes connection? - proxies\n===================================\nTo paraphrase the draft, it says \"client should close but server has the\nright to to protect itself\"\n\nThe issue here is that whoever closes the TCP connection suffers a\n(typically) 4 min hold on resources used for the connection during the\nTIME_WAIT state.\nIf the client is a big proxy, it then gets all these resources held up.\n\nSo when the user-agent send a close to the proxy, the proxy then decides\nwhether it wants to re-use the onward connection to the server for someone\nelse. If it doesn't, it would be nice if it could send an HTTP \"Connection:\nClose\" to the server in such a way to force the server to do the transport\nlayer close. If there were a further proxy, this process gets complicated.\n\nI've been tinkering with an algorithm (based on TCP initial sequence\nnumbers or on netstat calls) for the two ends to compare their numbers of\nopen connections  but this all gets very messy. It would also require a\n(small and backward compatible) change to the \"Connection: Close\" field.\n\n>       * clients and proxies SHOULD close idle connections.  Most of the\nThere's also a wording ambiguity. It should be clarified that you mean the\nclent side of a proxy - the word \"proxy\" doesn't say which side of the proxy.\n\n3. Who closes the connection? - not just normal 2xx responses.\n==============================================================\nFor most 2xx responses the draft is fine. It should say it is for 2xx\nresponses. It doesn't make sense for some other type of response:\n\n* 101 Switching protocols might require a connection close initiated by the\nserver for some cleverer uses of this.\n\n* 3xx responses and...\n>             * user has explicitly selected a URL from a different\n>               server.\nA \"different server\" should be clarified to ensure it is clear that this\nmeans a really different server, not just a different server name (which\nmight be multi-homed).\n\n4xx & 5xx responses\nI think the wording is OK for these as it's often common sense what needs\nto happen.\n\n4. Longer term\n==============\nWeb push proposals might make the logic that the client knows best less\nclear. Haven't really thought this through, but just throw it in for\ndiscussion.\n\n5. Policing\n===========\nHow can the rule about only holding 2 connections or 2N connections for\nproxies be policed? How does a server even know (for sure) whether it's a\nproxy of a user-agent making requests of it?\n\nBob\n\nAt 00:18 26/03/97 -0500, jg@zorch.w3.org wrote:\n>\n>Alan Freier and I (and others) have been worried about the fact that RFC2068\n>is silent on much of the rational and implementation detail required for\n>successful implementation of connection management policy for HTTP persistent\n>connections in HTTP/1.1.  Alan goaded me into drafting this with him\n>a few weeks ago, and we now have something presentable for working group\n>comments.  I'm submitting this to the ID drafts editor.\n>\n>I'll make this available in HTML sometime next week, but you'll have\n>to live with plain-text for the moment.\n>- Jim Gettys\n>=========\n>\n>\n>\n>\n>\n>\n>HTTP Working Group                    J. Gettys, Digital Equipment\nCorporation\n>INTERNET-DRAFT                  A. Freier, Netscape Communications\nCorporation\n>Expires September 26, 1997                                      March 26,\n1997\n>\n>\n>                       HTTP Connection Management\n>\n>                   draft-ietf-http-connection-00.txt\n>\n>Status of This Memo\n>\n>   This document is an Internet-Draft. Internet-Drafts are working\n>   documents of the Internet Engineering Task Force (IETF), its areas,\n>   and its working groups. Note that other groups may also distribute\n>   working documents as Internet-Drafts.\n>\n>   Internet-Drafts are draft documents valid for a maximum of six months\n>   and may be updated, replaced, or obsoleted by other documents at any\n>   time. It is inappropriate to use Internet-Drafts as reference\n>   material or to cite them other than as \"work in progress.\"\n>\n>   To learn the current status of any Internet-Draft, please check the\n>   \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n>   Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n>   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n>   ftp.isi.edu (US West Coast).\n>\n>   Distribution of this document is unlimited.  Please send comments to\n>   the HTTP working group at \"http-wg@cuckoo.hpl.hp.com\".  Discussions\n>   of the working group are archived at\n>   \"http://www.ics.uci.edu/pub/ietf/http/\".  General discussions about\n>   HTTP and the applications which use HTTP should take place on the\n>   \"www-talk@w3.org\" mailing list.\n>\n>1. Abstract\n>\n>   The HTTP/1.1 specification (RFC 2068) is silent about various details\n>   of TCP connection management when using persistent connections.  This\n>   document discusses some of the implementation issues discussed during\n>   HTTP/1.1's design, and introduces a few new requirements on HTTP/1.1\n>   implementations learned from implementation experience, not fully\n>   understood when RFC 2068 was issued.  This is an initial draft for\n>   working group comment, and we expect further drafts.\n>\n>\n>\n>\n>\n>\n>\n>\n>Gettys & Freier                                                 [Page 1]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>\n>\n>2. Table of Contents\n>\n>   1. Abstract ....................................................... 1\n>   2. Table of Contents .............................................. 2\n>   3. Key Words ...................................................... 2\n>   4. Connection Management for Large Scale HTTP Systems ............. 2\n>   5. Resource Usage (Who is going to pay?) .......................... 2\n>   6. Go to the Head of the Line ..................................... 6\n>   7. The Race is On ................................................. 7\n>   8. Closing Half of the Connection ................................. 8\n>   9. Capture Effect ................................................. 9\n>   10. Security Considerations ...................................... 10\n>   12. References ................................................... 12\n>   13. Acknowlegements .............................................. 13\n>   14. Authors' Addresses ........................................... 13\n>\n>3. Key Words\n>\n>   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n>   \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\n>   document are to be interpreted as described in RFC xxxx. [Bradner]\n>\n>4. Connection Management for Large Scale HTTP Systems\n>\n>   Recent development of popular protocols (such as HTTP, LDAP, ...)\n>   have demonstrated that the standards and engineering communities have\n>   not yet come to grip with the concept of connection management. For\n>   instance, HTTP/1.0 [HTTP/1.0] uses a TCP connection for carrying\n>   exactly one request/response pair. The simplistic beauty of that\n>   model has much less than optimal behavior.\n>\n>   This document focuses HTTP/1.1 implementations but the conclusions\n>   drawn here may be applicable to other protocols as well.\n>\n>   The HTTP/1.1 Proposed Standard [HTTP/1.1] specification is silent on\n>   when, or even if, the connection should be closed (implementation\n>   experience was desired before the specification was frozen on this\n>   topic). So HTTP has moved from a model that closed the connection\n>   after every request/response to one that might never close. Neither\n>   of these two extremes deal with \"connection management\" in any\n>   workable sense.\n>\n>\n>\n>\n>\n>\n>5. Resource Usage (Who is going to pay?)\n>\n>   The Internet is all about scale: scale of users, scale of servers,\n>\n>\n>Gettys & Freier                                                 [Page 2]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   scale over time, scale of traffic. For many of these attributes,\n>   clients must be cooperative with servers.\n>\n>   Clients of a network service are unlikely to communicate with more\n>   than a few servers (small number of 10s). Considering the power of\n>   desktop machines of today, maintaining that many idle connections\n>   does not appear to be overly burdensome, particularly when you\n>   consider the client is the active party and doesn't really have to\n>   pay attention to the connection unless it is expecting some response.\n>\n>   Servers will find connections to be critical resources and will be\n>   forced to implement some algorithm to shed existing connections to\n>   make room for new ones. Since this is an area not treated by the\n>   protocol, one might expect a variety of \"interesting\" efforts.\n>\n>   Maintaining an idle connection is almost entirely a local issue.\n>   However, if that local issue is too burdensome, it can easily become\n>   a network issue.  A server, being passive, must always have a read\n>   pending on any open connection.  Some implementations of the multi-\n>   wait mechanisms tend to bog down as the number of connections climbs\n>   in to the hundreds, though operating system implementations can scale\n>   this into the thousands, tens of thousands, or even beyond. Whether\n>   server implementations can also scale to so many simultaneous clients\n>   is likely much less clear than if the operating system can\n>   theoretically support such use. Implementations might be forced to\n>   use fairly bizarre mechanisms, which could lead to server\n>   instability, and then perhaps service outages, which are indeed a\n>   network issues. And despite any heroic efforts, it will all be to no\n>   avail. The number of clients that could hold open a connection will\n>   undoubtedly overwhelm even the most robust of servers over time.\n>\n>   When this happens, the server will of necessity be forced to close\n>   connections.  The most often considered algorithm is an LRU. The\n>   success of LRU algorithms in other areas of computer engineering is\n>   based on locality of reference.  I.e., in this case, if LRU is better\n>   than random, then this is because the \"typical\" client's behavior is\n>   predictable based on its recent history. Clients that have made\n>   requests recently are probably more likely to make them again, than\n>   clients which have been idle for a while. While we are not sure we\n>   can point to rigorous proof of this principle, we believe it does\n>   hold for Web service and client reference patterns are certainly a\n>   very powerful \"clue\".\n>\n>   The client has more information that could be used to drive the\n>   process.  For instance, it does not seem to much to expect that a\n>   connection be held throughout the loading of a page and all its\n>   embedded links. It could further sense user sincerity towards the\n>   page by detecting such events as mouse movement, scrolling, etc., as\n>   indicators that there is still some interest in pursing the page's\n>   content, and therefore the chance of accessing subsequent links.  But\n>   if the user has followed a number of links in succession away to a\n>   different server, it may be likely that the first connection will not\n>\n>\n>Gettys & Freier                                                 [Page 3]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   be used again soon. Whether this is significantly better than LRU is\n>   an open question, but it is clear that unlikely to be used\n>   connections should be closed, to free the server resouces involved.\n>   Server resouces are much more scarce than client resources, and\n>   clients should be frugal, if the Web is to have good scaling\n>   properties.\n>\n>   Authoritative knowledge that it is appropriate to close a connection\n>   can only come from the user. Unfortunately, that source is not to be\n>   trusted.  First, most users don't know what a connection is, and\n>   having them indicate it is okay to close it is meaningless. Second, a\n>   user that does know what a connection is probably inherently greedy.\n>   Such a user would never surrender the attention that a connection to\n>   a server implies. Research [Mogul2] does show that most of the\n>   benefits of persistent connections are gained if connections can be\n>   held open after last use approximately one minute for the HTTP\n>   traffic studied; this captures most \"click ahead\" behavior of a\n>   user's web browsing.\n>\n>   For many important services, server resources are critical resources;\n>   there are many more clients than services. For example, the AltaVista\n>   search service handles (as of this writing) tens of millions of\n>   searches per day, for millions of different clients. While it is one\n>   of the two or three most popular services on the Internet today, it\n>   is clearly small relative to future services built with Internet\n>   technology and HTTP. From this perspective, it is clear that clients\n>   need to cooperate with servers to enable servers to continue to\n>   scale.\n>\n>   System resources at a server:\n>\n>       * Server resources (open files, file system buffers, processes,\n>         memory for applications, memory for socket buffers for\n>         connections currently in use (16-64Kbytes each, data base\n>         locks). In BSD derived TCP implementations, socket buffers are\n>         only needed on active connections. This usually works because\n>         it's seldom the case that there is data queued to/from more\n>         than a small fraction of the open connections.\n>\n>       * PCB (Protocol control blocks, only ~100-140 bytes; even after a\n>         connection is closed, you can't free this data structure for a\n>         significant amount of time, of order minutes. More severe,\n>         however, is that many inferior TCP implementations have had\n>         linear or quadratic algorithms relating to the number of PCB's\n>         to find PCB's when needed.\n>\n>   These are organized from most expensive, to least.\n>\n>   Clients should read data from their TCP implementations aggressively,\n>   for several reasons:\n>\n>       * TCP implementations will delay acknowledgements if socket\n>\n>\n>Gettys & Freier                                                 [Page 4]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>         buffers are not emptied. This will lower TCP performance, and\n>         cause increased elapsed time for the end user. [Frystyk et.\n>         al.] while continuing to consume the server's resources.\n>\n>       * Servers must be able to free the resources held on behalf of\n>         the client as quickly as possible, so that the server can reuse\n>         these resources on behalf of others. These are often the\n>         largest and scarcest server system resource (processes, open\n>         files, file system buffers, data base locks, etc.)\n>\n>   When HTTP requests complete (and a connection is idle), an open\n>   connection still consumes resources some of which are not under the\n>   server's control:\n>\n>       * socket buffers (16-64KB both in the operating system, and often\n>         similar amounts in the server process itself)\n>\n>       * Protocol Control Blocks (.15 KB/PCB's). (??? Any other data\n>         structures associated with PCB's?)\n>\n>   If, for example, an HTTP server had to indefinitely maintain these\n>   resources, this memory alone for a million clients (and there are\n>   already HTTP services larger than this scale in existence today)\n>   using a single connection each would be tens of gigabytes of memory.\n>   One of the reasons the Web has succeeded is that servers can, and do\n>   delete connections, and require clients to reestablish connections.\n>\n>   If connections are destroyed too aggressively (HTTP/1.0 is the\n>   classic limiting case), other problems ensue.\n>\n>       * The state of congestion of the network is forgotten [Jacobson].\n>         Current TCP implementations maintain congestion information on\n>         a per-connection basis, and when the connection is closed, this\n>         information is lost. The consequences of this are well known:\n>         general Internet congestion, and poor user performance\n>\n>       * Round trip delays and packets to re-establish the connections.\n>         Since most objects in the Web are very small, of order half the\n>         packets in the network has been due to just the TCP open and\n>         close operation.\n>\n>       * Slow Start lowers initial throughput of the TCP connection\n>\n>       * PCB's become a performance bottleneck in some TCP\n>         implementations (and cannot be reused for a XXX timeout after\n>         the connection has been terminated).  The absolute number of\n>         PCBs in the TIME_WAIT state could be much larger than the\n>         number in the ESTABLISHED state. Closing connections too\n>         quickly can actually consume more memory than closing them\n>         slowly, because all PCBs consume memory and idle socket buffers\n>         do not.\n>\n>\n>\n>Gettys & Freier                                                 [Page 5]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   From these two extreme examples, it is obvious that connection\n>   management becomes a central issue for both clients and servers.\n>\n>   Clearly, benefits of persistent connections will be lost if clients\n>   open many connections simultaneously. RFC2068 therefore specifies no\n>   more than 2 connections from a client to a server should be open at\n>   any one time, or 2N connections (where N is the number of clients a\n>   proxy is serving) for proxies. Frystyk et. al. have shown that\n>   roughly twice the performance of HTTP/1.0 using four to six\n>   connections can be reached using HTTP/1.1 over a single TCP\n>   connection using HTTP/1.1, even over a LAN, once combined with\n>   compression of the HTML documents [Frystyk].\n>\n>6. Go to the Head of the Line\n>\n>   The HTTP/1.1 specification requires that proxies use no more than 2N\n>   connections, where N is the number of client connections being\n>   served.  Mogul has shown that persistent connections are a \"good\n>   thing\", and Frystyk et. al. show data that significant (a factor of\n>   2-8) savings in number of packets transmitted result by using\n>   persistent connections.\n>\n>   If fewer connections are better, then, why does HTTP/1.1 permit\n>   proxies to establish more than the absolute minimum of connections?\n>   In the interests of brevity, the HTTP/1.1 specification is silent on\n>   some of the motivations for some requirements of the specification.\n>   At the time HTTP/1.1 was specified, we realized that if a proxy\n>   server attempted to aggregate requests from multiple client\n>   connections onto a single TCP connection, a proxy would become\n>   vulnerable to the \"head of line\" blocking problem. If Client A, for\n>   example, asks for 10 megabytes of data (or asked for a dynamicly\n>   generated document of unlimited length), then if a proxy combined\n>   that request with requests from another Client B, Client B would\n>   never get its request processed. This would be a very \"bad thing\",\n>   and so the HTTP/1.1 specification allows proxies to scale up their\n>   connection use in proportion to incoming connections. This will also\n>   result in proxy servers getting roughly fair allocation of bandwidth\n>   from the Internet proportional to the number of clients.\n>\n>   Since the original HTTP/1.1 design discussions, we realized that\n>   there is a second, closely related denial of service security arises\n>   if proxies attempt to use the same TCPconnection for multiple\n>   clients.  An attacker could note that a particular URL of a server\n>   that they wished to attack was either very large, very slow (script\n>   based), or never returned data. By making requests for that URL, the\n>   attacker could easily block other clients from using that server\n>   entirely, due to head of line blocking, so again, simultaneously\n>   multiplexing requests from different clients would be very bad, and\n>   therefore implementations MUST not attempt such multipexing.\n>\n>   In other words, head-of-line blocking couples the fates of what\n>   should be independent interactions, which allows for both denial-of-\n>\n>\n>Gettys & Freier                                                 [Page 6]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   service attacks, and for accidental synchronization.\n>\n>   Here is another example of head-of-line blocking: imagine clients A\n>   and B are connected to proxy P1, which is connected to firewall proxy\n>   P2, which is connected to the Internet. If P1 only has one connection\n>   to P2, and A attempts to connect (via P1 and P2) to a dead server on\n>   the Internet, all of B's operations are blocked until the connection\n>   attempt from P2 to the dead server times out. This is not a good\n>   situation.\n>\n>   Note that serial reuse of a TCP connection does not have these\n>   considerations: a proxy might first establish a connection to an\n>   origin server for Client A, and possibly leave the connection open\n>   after Client A finishes and closes\n>\n>   its connection, and then use the same connection for Client B, and so\n>   on.  As in normal clients, such a proxy should close idle\n>   connections.\n>\n>   Future HTTP evolution also dictates that simultaneous multiplexing of\n>   clients over a connection should be prohibited. A number of schemes\n>   for compactly encoding HTTP rely on associating client state with a\n>   connection, which HTTP 1.X does not currently do. If proxies do such\n>   multiplexing, then such designs will be much harder to implement.\n>\n>7. The Race is On\n>\n>   Deleting a connection without authoritative knowledge that it will\n>   not be soon reused is a fundamental race that is part of any timeout\n>   mechanism.  Depending on how the decision is made will determine the\n>   penalties imposed.\n>\n>   It is intuitively (and most certainly empirically) less expensive for\n>   the active (client) partner to close a connection than the server.\n>   This is due in most part to the natural flow of events. For instance,\n>   a server closing a connection cannot know that the client might at\n>   that very moment be sending a request. The new request and the close\n>   message can pass by in the night simply because the server and the\n>   client are separated by a network. That type of failure is a network\n>   issue. The code of both the client and the server must to be able to\n>   deal with such failures, but they should not have to deal with it\n>   efficiently. A client closing a connection, on the other hand, will\n>   at least be assured that any such race conditions are mostly local\n>   issues. The flow will be natural, assuming one treats closing as a\n>   natural event. To paraphrase Butler Lampson's 1983 paper on system\n>   design, \"The events that happen normally must be efficient.  The\n>   exceptional need to make progress.\" [Lampson]\n>\n>   Having the client closing the connection will decrease the\n>   probability of the client having to do automatic connection recovery\n>   of a pipeline caused by a premature close on server side. From an\n>   client implementation point of view this is advantageous as automatic\n>\n>\n>Gettys & Freier                                                 [Page 7]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   connection recovery of a pipeline is significantly more complicated\n>   than closing an idle connection.  In HTTP, however, servers are free\n>   to close connections any time, and this observation does not help,\n>   but may simplify other protocols. It will, however, reduce the number\n>   of TCP resets observed, and make the exceptional case exceptional,\n>   and avoid a TCP window full of requests being transmitted under some\n>   circumstances.\n>\n>   On the one hand, it is a specific fact about TCP that if the client\n>   closes the connection, the server does not have to keep the TIME_WAIT\n>   entry lying around. This is goodness.\n>\n>   On the other hand, if the server has the resources to keep the\n>   connection open, then the client shouldn't close it unless there is\n>   little chance that the client will use the server again soon, since\n>   closing & then reopening adds computational overhead to the server.\n>   So allowing the server to take the lead in closing connections does\n>   have some benefits.\n>\n>   A further observation is that congestion state of the network varies\n>   with time, so the benefits of the congestion state being maintained\n>   by TCP diminishes the longer a connection is idle.\n>\n>   This discussion also shows that a client should close idle\n>   connections before the server does. Currently in the HTTP standard\n>   there is no way for a server to provide such a \"hint\" to the client,\n>   and there should be a mechanism. This memo solicits other opinions on\n>   this topic.\n>\n>8. Closing Half of the Connection\n>\n>   In simple request/response protocols (e.g. HTTP/1.0), a server can go\n>   ahead and close both recieve and transmit sides of its connection\n>   simultaneously whenever it needs to. A pipelined or streaming\n>   protocol (e.g. HTTP/1.1) connection, is more complex [Frystyk et.\n>   al.], and an implementation which does so can create major problems.\n>\n>   The scenario is as follows: an HTTP/1.1 client talking to a HTTP/1.1\n>   server starts pipelining a batch of requests, for example 15 on an\n>   open TCP connection.  The server decides that it will not serve more\n>   than 5 requests per connection and closes the TCP connection in both\n>   directions after it successfully has served the first five requests.\n>   The remaining 10 requests that are already sent from the client will\n>   along with client generated TCP ACK packets arrive on a closed port\n>   on the server. This \"extra\" data causes the server's TCP to issue a\n>   reset which makes the client TCP stack pass the last ACK'ed packet to\n>   the client application and discard all other packets. This means that\n>   HTTP responses that are either being received or already have been\n>   received successfully but haven't been ACK'ed will be dropped by the\n>   client TCP. In this situation the client does not have any means of\n>   finding out which HTTP messages were successful or even why the\n>   server closed the connection. The server may have generated a\n>\n>\n>Gettys & Freier                                                 [Page 8]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   \"Connection: Close\" header in the 5th response but the header may\n>   have been lost due to the TCP reset. Servers must therefore close\n>   each half of the connection independently.\n>\n>9. Capture Effect\n>\n>   One of the beauties of the simple single connection for each\n>   request/response pair is that it did not favor an existing client\n>   over another. In general, this natural rotation made for a fairer\n>   offering of the overall service, albeit a bit heavy handed. Our\n>   expectation is that when protocols with persistent connections get\n>   heavily deployed, that aspect of fairness will not exist. Without\n>   some moderately complex history, it might be that only the first 1000\n>   clients will ever be able to access a server (providing that your\n>   server can handle 1000 connections).\n>\n>   There needs to be some policy indicating when it is appropriate to\n>   close connections. Such a policy should favor having the client be\n>   the party to initiate the closure, but must provide some manner in\n>   which the server can protect itself from misbehaving clients. Servers\n>   can control greedy clients in HTTP/1.1 by use of the 503 (Service\n>   Unavailable) response code in concert with the Retry-After response-\n>   header field, or by not reading further requests from that client, at\n>   the cost of temporarily occupying the connection. As long as the\n>   server can afford to keep the connection open, it can delay a \"greedy\n>   client\" by simply closing the TCP receive window.  As soon as it\n>   drops the connection, it has no way to distinguish this client from\n>   any other. Either of these techniques may in fact be preferable to\n>   closing the client's connection; the client might just immediately\n>   reopen the connection, and you are unlikely to know if it is the same\n>   greedy client.\n>\n>   Implementation complexity will need to be balanced against scheduling\n>   overhead.  A number of possible server scheduling algorithms exist,\n>   with different costs and benefits. The implementation experience of\n>   one of us (jg) with the X Window System [Gettys et. al.] may be of\n>   use to those implementing Web server schedulers.\n>\n>       * Strict round robin scheduling: a operating system select or\n>         poll operation is executed for each request processed, and each\n>         request is handled in turn (across connections). Since select\n>         is executed frequently, new connections get a good chance of\n>         service sooner rather than later. Some algorithm must be chosen\n>         to avoid capture effect if the server is loaded. This is most\n>         fair, and approximates current behavior. The disadvantage is,\n>         however, a (relatively expensive) system call / request, which\n>         will likely become too expensive as Web servers become\n>         carefully optimized after HTTP/1.1 is fully implemented.\n>\n>       * Modified round robin scheduling: a operating system select or\n>         poll operation is executed. Any new connections are\n>         established, and for each connection showing data available,\n>\n>\n>Gettys & Freier                                                 [Page 9]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>         all available requests are read into buffers for later\n>         execution. Then all requests are processed, round robin between\n>         buffers. Some algorithm must be chosen to avoid capture effect\n>         if the server is loaded. This eliminates the system call per\n>         operation.  This is quite efficient, and still reasonably\n>         fairly apportions server capabilities.\n>\n>       * Some servers are likely to be multithreaded, possibly with a\n>         thread per connection. These servers will have to have some\n>         mechanism to share state so that no client can forever capture\n>         a connection on a busy server.\n>\n>   A final note: indefinite round robin scheduling may not in fact be\n>   the most desirable algorithm, due to the timesharing fallacy. If a\n>   connection makes progress more slowly than possible, not only will\n>   the client (the end user) observe poorer performance, but the\n>   connection (and the considerable system overhead each one represents)\n>   will be open longer, and more connections and server resources will\n>   be required as a result.\n>\n>   At some point, large, loaded servers will have to choose a connection\n>   to close; research [Padmanabhan and Mogul] shows that LRU may be as\n>   good as more complex algorithms for choosing which to close.\n>\n>   Further experimentation with HTTP/1.1 servers will be required to\n>   understand the most useful scheduling and connection management\n>   algorithms.\n>\n>10. Security Considerations\n>\n>   Most HTTP related security considerations are discussed in RFC2068.\n>   This document identifies a further security concern: proxy\n>   implementations that simultaneously multiplex requests from multiple\n>   clients over a TCP connection are vulnerable to a form of denial of\n>   service attacks, due to head of line blocking problems, as discussed\n>   further above.\n>\n>   The capture effect discussed above also presents opportunities for\n>   denial of service attacks.\n>\n>11. Requirements on HTTP/1.1 Implementations\n>\n>   Here are some simple observations and requirements from the above\n>   discussion.\n>\n>       * clients and proxies SHOULD close idle connections.  Most of the\n>         benefits of an open connection diminish the longer the\n>         connection is idle: the congestion state of the network is a\n>         dynamic and changing phenomena [Paxson]. The client, better\n>         than a server, knows when it is likely not to revisit a site.\n>         By monitoring user activity, a client can make reasonable\n>         guesses as to when a connection needs closing.  Research has\n>\n>\n>Gettys & Freier                                                [Page 10]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>         shown [Mogul2] shows that most of the benefits of a persistent\n>         connection are likely to occur within approximately\n>         60 seconds. Further research in this area is needed.  On the\n>         client side, define a connection as \"idle\" if it meets at least\n>         one of these two criteria:\n>\n>             * no user-interface input events during the last 60 seconds\n>               (parameter value shouldn't be defined too precisely)\n>\n>             * user has explicitly selected a URL from a different\n>               server. Don't switch just because inlined images are from\n>               somewhere else! Even in this case, dally for some seconds\n>               (e.g., 10) in case the user hits the \"back\" button.\n>         On the server side, use a timeout that is adapted based on\n>         resource constraints: short timeout during overload, long\n>         timeout during underload.  Memory, not CPU cycles, is likely to\n>         be the controlling resource in a well-implemented system.\n>\n>       * servers SHOULD implement some mechanism to avoid the capture\n>         effect.\n>\n>       * proxies MUST use independent TCPconnections to origin or futher\n>         proxy servers for different client connections, both to avoid\n>         head of line blocking between clients, and to avoid the denial\n>         of service attacks that implementations that attempt to\n>         multiplex multiple clients over the same connection would be\n>         open to.\n>\n>       * proxies MAY serially reuse connections for multiple clients.\n>\n>       * servers MUST properly close incoming and outgoing halves of TCP\n>         connections independently.\n>\n>       * clients SHOULD close connections before servers when possible.\n>         Currently, HTTP has no \"standard\" way to indicate idle time\n>         behavior to clients, though we note that the Apache HTTP/1.1\n>         implementation advertizes this information using the Keep-Alive\n>         header if Keep-Alive is requested. We note, however, that Keep-\n>         Alive is NOT currently part of the HTTP standard, and that the\n>         working group may need to consider providing this \"hint\" to\n>         clients in the future of the standard by this or other means\n>         not currently specified in this initial draft.\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>Gettys & Freier                                                [Page 11]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>12. References\n>\n>   [Apache]\n>      The Apache Authors, The Apache Web Server is distributed by The\n>      Apache Group.\n>\n>   [Bradner]\n>      S. Bradner, \"Keywords for use in RFCs to Indicate Requirement\n>      Levels\", RFC XXXX\n>\n>   [Frystyk]\n>      Henrik Frystyk Nielsen, \"The Effect of HTML Compression on a LAN\n>      \", W3C. URL:\n>\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/LAN.html\n>\n>   [Frystyk et. al]\n>      Henrik Frystyk Nielsen, Jim Gettys, Anselm Baird-Smith, Eric\n>      Prud'hommeaux, W3C, H&aring;kon Wium Lie, Chris Lilley, W3C,\n>      \"Network Performance Effects of HTTP/1.1, CSS1, and PNG\". W3C\n>      Note, February, 1997. See URL:\n>      http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/ for this and\n>      other HTTP/1.1 performance information.\n>\n>   [Gettys et. al.]\n>      Gettys, J., P.L. Karlton, and S. McGregor, \" The X Window System,\n>      Version 11.'' Software Practice and Experience Volume 20, Issue\n>      No. S2, 1990 ISSN 0038-0644.\n>\n>   [HTTP/1.0]\n>      T. Berners-Lee, R. Fielding, H. Frystyk.  \"Informational RFC 1945\n>      - Hypertext Transfer Protocol -- HTTP/1.0,\" MIT/LCS, UC Irvine,\n>      May 1996\n>\n>   [HTTP/1.1]\n>      R. Fielding, J. Gettys, J.C. Mogul, H. Frystyk, T. Berners-Lee,\n>      \"RFC 2068 - Hypertext Transfer Protocol -- HTTP/1.1,\" UC Irvine,\n>      Digital Equipment Corporation, MIT\n>\n>   [Jacobson]\n>      Van Jacobson. \"Congestion Avoidance and Control.\" In Proc. SIGCOMM\n>      '88 Symposium on Communications Architectures and Protocols, pages\n>      314-329. Stanford, CA, August, 1988.\n>\n>   [Lampson]\n>      B. Lampson, \"Hints for Computer System Design\", 9th ACM SOSP, Oct.\n>      1983, pp. 33-48.\n>\n>   [Mogul]\n>      Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP.\" In\n>      Proc. SIGCOMM '95 Symposium on Communications Architectures and\n>      Protocols, pages 299-313. Cambridge, MA, August, 1995.\n>\n>\n>\n>Gettys & Freier                                                [Page 12]\n>\n>Internet-Draft         HTTP Connection Management             March 1997\n>\n>\n>   [Mogul2]\n>      Jeffrey C. Mogul. \"The Case for Persistent-Connection HTTP\".\n>      Research Report 95/4, Digital Equipment Corporation Western\n>      Research Laboratory, May, 1995. URL:\n>      http://www.research.digital.com/wrl/techreports/abstracts/95.4.html\n>\n>   [Padmanabhan and Mogul]\n>      Venkata N. Padmanabhan and Jeffrey C. Mogul. Improving HTTP\n>      Latency. In Proc. 2nd International WWW Conf. '94: Mosaic and the\n>      Web, pages 995-1005. Chicago, IL, October, 1994. URL:\n>\nhttp://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html\n>\n>   [Padmanabhan & Mogul]\n>       V.N. Padmanabhan, J. Mogul, \"Improving HTTP Latency\", Computer\n>      Networks and ISDN Systems, v.28, pp.  25-35, Dec. 1995. Slightly\n>      revised version of paper in Proc. 2nd International WWW Conference\n>      '94: Mosaic and the Web, Oct. 1994\n>\n>   [Paxson]\n>\n>      Vern Paxson, \"End-to-end Routing Behavior in the Internet\" ACM\n>      SIGCOMM '96, August 1996, Stanford, CA.\n>\n>13. Acknowlegements\n>\n>   Our thanks to Henrik Frystyk Nielsen for comments on the first draft\n>   of this document.\n>\n>14. Authors' Addresses\n>\n>   Jim Gettys\n>   W3 Consortium\n>   MIT Laboratory for Computer Science\n>   545 Technology Square\n>   Cambridge, MA 02139, USA\n>   Fax: +1 (617) 258 8682\n>   Email: jg@w3.org\n>\n>   Alan Freier\n>   Netscape Communications Corporation\n>   Netscape Communications\n>   501 East Middlefield Rd.\n>   Mountain View, CA 94043\n>   Email: freier@netscape.com\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>Gettys & Freier                                                [Page 13]\n>\n>\n>\n____________________________________________________________________________\nBob Briscoe         http://www.labs.bt.com/people/briscorj/\n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Wed, 10 Dec 1997, Jim Gettys wrote:\n>> >  From: Yaron Goland <yarong@microsoft.com>\n>> >  Date: Wed, 10 Dec 1997 11:21:51 -0800\n>> >  To: \"'jg@pa.dec.com'\" <jg@pa.dec.com>, Josh Cohen <joshco@microsoft.com>\n>> >  Cc: Foteos Macrides <MACRIDES@SCI.WFBR.EDU>, lynx-dev@sig.net,\n>> >          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>> >  Subject: RE: LYNX-DEV two curiosities from IETF HTTP session.\n>> >  \n>> >  I doubt any commercial browser will implement 305 without some very serious\n>> >  security provided to assure that the proxy asking for the one time redirect\n>> >  is going to get it. I would suggest that this problem needs to be dealt with\n>> >  in the large 305/306 context, in a stand alone spec, and that the draft\n>> >  standard for HTTP should simply state that 305 has been deprecated and\n>> >  SHOULD NOT be implemented.\n>> >  \n>> >  Yaron\n>> \n>> I think you are confused....  In Rev-01, only an origin server is allowed\n>> to generate a 305 response.  It is authoritative for that resource, so\n>\n>But what is there about the protocol which allows this requirement to\n>be enforced?\n>\n>> the spoofing problems don't come up (and is the reason for that text being\n>> in the document...)\n>\n>Seems to me that the protocol relies on gentle (i.e., conforming)\n>behavior by proxies and servers. The definition of spoofing includes an\n>element of malicious intent.\n>\n>Perhaps it would close the loop to require that the client only \n>accept a 305 status from an origin server to which it is directly\n>connected?\n>\n>Then the potential for spoofing would be limited to IP and/or DNS\n>spoofing.\n>\n>In either case though, I fail to see the motivation for someone\n>intending to spoof a redirect who wouldn't simply change the\n>origin server or make the IP spoofed variant of the origin server\n>simply serve the nefarious content which would have be acquired\n>as a result of the redirect.\n>\n>Thus I don't see any difference in risk with/without 305.\n\nThe \"current practice\" is unintended:  The 305 is passed back\nto the browser because deployed proxies don't recognize it.\n\nThe 305 *was* intended (I think, but the original discussion\nabout it was long ago) to be handled by proxies if interposed between\nthe browser and origin server, in which case it would be the proxy first\nreceiving a response from the origin server (i.e., what you mean by\n\"directly connected\").  That would be desireable, because for 305 the\nbrowser should not act on it if it already is using a proxy (IMHO, for\nsecurity/privacy considerations, and thus how it was implemented in\nLynx).  If proxies do act on it, then it's even more important to make\n305 redirection GET-only, because the user of the browser will not be\nable to confirm or cancel redirection of a POST.  One can leave 306\nredirection of POSTs by proxies as a future possibility (not in the\nHTTP/1.1 Draft Standard), but for what it's worth, I've been\nscratching my head for some time about how to deal with the\nsecurity/privacy problems that would raise, and still don't see any\nsolutions.  For 305, it's possible that in a chain of proxies the one\nwhich first receives the origin server's response doesn't recognize it,\nand passes it on to the next proxy in the chain, but if any proxy in\nthe chain recognizes it and acts on it with a conversion to GET (if\nit's not a GET request in the first place), then there's no security\nproblem, and no serious privacy problem (certainly much less than with\nunverifiable cookie transactions :).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "Content-Length certainly has been a thorn in my side for a long time,\nfrom the very beginning.  Trying to rationalize the contradictory\ndefinitions for Content-Length in HEAD vs GET, and the fact that servers\nused it to indicate message length while browsers ignored it except\nfor measuring the size of a POST, hasn't worked very well.  We have\nskated by so long as the only transfer encoding is chunked, but John\nis right in that the basic abstractions break down when considering\ndigests or transfer codings in general.\n\n>Personally I would like to see Content-Length remain an entity header.\n>All the other Content-* headers are entity headers and apply to the\n>entity before transfer encoding.\n>\n>One way to do this would be to introduce a new \"Transfer-Length\"\n>header with the stipulation that its default value is the\n>Content-Length.  The Content-Length would be defined as it is now in\n>section 7, i.e. the entity length.  Thus the Transfer-Length header\n>would only be needed when the message length and entity length\n>differed.  This would give us consistent terminology (Content-* for\n>entity, Transfer-* for message).  It would also not break any current\n>of which I am aware.  At present the only widely deployed TE is\n>chunked and it needs neither header.  If new TEs arise which need\n>to have the message length specified they would have to use \n>Transfer-length (or both).\n\nThat is a reasonable solution.  My only concern would be for proxies,\nbut I think they'd be better off in the long run with a clear definition.\nThe one exception to the above is that Transfer-Length would default\nto zero for responses to HEAD requests, 204, and 304.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Wed, 10 Dec 1997, Roy T. Fielding wrote:\n\n> Content-Length certainly has been a thorn in my side for a long time,\n> from the very beginning.  Trying to rationalize the contradictory\n> definitions for Content-Length in HEAD vs GET, and the fact that servers\n> used it to indicate message length while browsers ignored it except\n> for measuring the size of a POST, hasn't worked very well.  We have\n> skated by so long as the only transfer encoding is chunked, but John\n> is right in that the basic abstractions break down when considering\n> digests or transfer codings in general.\n> \n> John Franks wrote\n> >Personally I would like to see Content-Length remain an entity header.\n> >All the other Content-* headers are entity headers and apply to the\n> >entity before transfer encoding.\n> >\n> >One way to do this would be to introduce a new \"Transfer-Length\"\n> >header with the stipulation that its default value is the\n> >Content-Length.  The Content-Length would be defined as it is now in\n> >section 7, i.e. the entity length.  Thus the Transfer-Length header\n> >would only be needed when the message length and entity length\n> >differed.  This would give us consistent terminology (Content-* for\n> >entity, Transfer-* for message).  It would also not break any current\n> >of which I am aware.  At present the only widely deployed TE is\n> >chunked and it needs neither header.  If new TEs arise which need\n> >to have the message length specified they would have to use \n> >Transfer-length (or both).\n> \n\n> That is a reasonable solution.  My only concern would be for proxies,\n> but I think they'd be better off in the long run with a clear definition.\n\nI think proxies should be ok.  If they understand a new TE which requires\nTransfer-length then they should also understand Transfer-length.  If\nthey don't understand the TE they have to reject it.  Proxies are not\nsupposed to touch digests so that shouldn't be a problem.\n\n> The one exception to the above is that Transfer-Length would default\n> to zero for responses to HEAD requests, 204, and 304.\n> \n\nYes, you are right.  Indeed, any request or response should have\nTransfer-length 0 if and only if it has an empty message body.  And\nan empty messge body should imply Transfer-length 0 without the header\nbeing present.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": " \n\nOn Wed, 10 Dec 1997, John Franks wrote:\n\n> On Wed, 10 Dec 1997, Roy T. Fielding wrote:\n> \n> > Content-Length certainly has been a thorn in my side for a long time,\n> > from the very beginning.  Trying to rationalize the contradictory\n> > definitions for Content-Length in HEAD vs GET, and the fact that servers\n> > used it to indicate message length while browsers ignored it except\n> > for measuring the size of a POST, hasn't worked very well.  We have\n> > skated by so long as the only transfer encoding is chunked, but John\n> > is right in that the basic abstractions break down when considering\n> > digests or transfer codings in general.\n> > \n> > John Franks wrote\n> > >Personally I would like to see Content-Length remain an entity header.\n> > >All the other Content-* headers are entity headers and apply to the\n> > >entity before transfer encoding.\n> > >\n> > >One way to do this would be to introduce a new \"Transfer-Length\"\n> > >header with the stipulation that its default value is the\n> > >Content-Length.  The Content-Length would be defined as it is now in\n> > >section 7, i.e. the entity length.  Thus the Transfer-Length header\n> > >would only be needed when the message length and entity length\n> > >differed.  This would give us consistent terminology (Content-* for\n> > >entity, Transfer-* for message).  It would also not break any current\n> > >of which I am aware.  At present the only widely deployed TE is\n> > >chunked and it needs neither header.  If new TEs arise which need\n> > >to have the message length specified they would have to use \n> > >Transfer-length (or both).\n> > \n> \n> > That is a reasonable solution.  My only concern would be for proxies,\n> > but I think they'd be better off in the long run with a clear definition.\n> \n> I think proxies should be ok.  If they understand a new TE which requires\n> Transfer-length then they should also understand Transfer-length.  If\n> they don't understand the TE they have to reject it.  Proxies are not\n> supposed to touch digests so that shouldn't be a problem.\n> \n> > The one exception to the above is that Transfer-Length would default\n> > to zero for responses to HEAD requests, 204, and 304.\n> > \n> \n> Yes, you are right.  Indeed, any request or response should have\n> Transfer-length 0 if and only if it has an empty message body.  And\n> an empty messge body should imply Transfer-length 0 without the header\n> being present.\n\nI am quite confused here. We introduced CHUNKed transfer encoding \nbecause it is difficult for some servers to know the content length\nprior to beginning to send data.\n\nExactly HOW would a server know transfer-length before sending\ndata? I can define a reasonable use of content-length in the\ntrailer of a chunk-encoded transfer ... since content length is\nthe entity length, it could serve as a double check of the\nreceipt of the chunk-encoded entity.  But clearly, transfer\nlength couldn't appear in the trailer as the length wouldn't\nbe known until after the trailer was complete.  It makes no\nsense to me for a server which knows the length of the transfer\nencoded entity to ever use transfer encoding.\n\nSounds like protocol cruft to me.  What am I missing?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Minutes from Second HTTP session at IETF4",
            "content": "Larry, all --\n\nThis is my transcript of the second session, and some abbreviated\n\"minutes.\" I did not take minutes for the first session on Monday,\nthough. I think we should submit Josh Cohen's slides as a separate\npart of the proceeding rather than part of the (limited to 6-pages)\nminutes.\n\nHTTP-WG is dead, Long live HTTP!, Rohit Khare\n\n========================================================== \nMinutes of the Dec 10th HTTP-WG meeting \nReported by Rohit Khare, UC Irvine\n\nThe second session reported on the myriad HTTP-related activities\nwhich will continue outside HTTP-WG, as well as the remainder of the\nISSUES list. In the previous days, there had been new meetings and\ndevelopments related to 1) interoperabilty of 1.1. implementations; 2)\nclosure on Authentication; 3) Privacy on the Web; 4) cookies; 5)\ncontent-negotiation; 6) and extensions to 1.1.\n\nTo advance to DRAFT standard, the group needs to stabilize updates to\n2068 and document at least two independent, interoperable\nimplementations of each feature -- not applications which support\nevery feature, not just shipping products. The previous night, a\nhalf-dozen client, server, and proxy developers sat down with the\nchair and editor to walk through the MUSTs and MUST NOTs section by\nsection. The main insight was that we'll need much more systematic\nsupport to document the hundreds of requirements in HTTP/1.1. Caching,\nin particular, seems to be the most fraught with difficulty. Scott\nLawrence agreed to continue his (very useful) Thursday testing bees;\nand there was a survey of interest in face-to-face implementation\nbake-offs or conference calls.\n\n2069 had one concern raised that Digest does not work well with\nproxies and not at all across multiple servers. Paul Leach will soon\npost to the list a small tweak which addresses both (and issue a new\nI-D?).\n\nUser Services sponsored a BOF on Web Privacy. April Marine reported\nthe broad support for investigating the nexus of trust issues around\nthe Web. A detailed charter awaits debate, though, on\nweb-priv@nasa.gov. A second BOF is projected for LA.\n\nDavid Kristol, editor of the cookie spec, reported that a small group\nof developers reviewed the outstanding issues with state management\nand found one technical and one political problem left. Technically,\ndomain matching does not work with flat (intranet) domains where\n\"foo.\" is an FQDN. The latter is user notification of cookies from\nunverifiable transactions -- should people be tracked by an external\nsource of inline images or applets or audio?\n\nRecipient Feature Profile (nee content-negotiation), weaves together\nextensibility threads from HTTP, printing, faxing, mail, and many\nother application-layer protocols.Ted Hardie reported the conviction\nthe group will set up a registry as quickly as possible. Then, it may\ntackle aggregation of features and a prototypical example of how to\nstore profiles within LDAP, etc.\n\nThe Extensions team reported a strawman charter for 1) producing an\nFYI document of guidance on adding features, headers, and methods to\nHTTP, 2) extending error response codes, 3) and simplifying PEP and\nOPTIONS into a reliable extension hook. It is NOT an HTTP/1.2\ngroup. Josh Cohen and Scott Lawrence will chair and edit,\nrespectively, an investigation into how-to-extend, not what-to-extend.\n\nFinally, Jim Gettys continued working through the ISSUES\nlist. RE-VERSION: Henry Sanders will redocument the justification for\nupgrading proxy connections to the highest supported version to the\nlist. PUT-RANGE is demoted as problematic (caches observe only a\nsubset of edits) and insufficient (can't insert or remove\nranges). This functionality will become PATCH in WebDAV, and hence\ndiscouraged or forbidden in HTTP/1.1\n\nThis concluded the final meeting of the HTTP Working Group. Up next:\nW3C has been working on experiments for HTTP-NG, which had a BOF at\n39th IETF and will surely be heard from in 1998.\n\n==========================================================\nTranscript of the second HTTP session, Dec 10th \nReported by Rohit Khare, UC Irvine.\n\nReports on interoperability check, webpriv, state, extend, conneg,\nissues on digest, closing plans (the very,very, very last meeting\never!)\n\n\n========== \nFrankenstein's Bride (JG)\n\nJG: for DRAFT, we MUST show two INDEPENDENT IMPLEMENTATIONS of EACH\nFEATURE which are INTEROPERABLE -- not products which implement ALL,\nnot necc. SHIPPING, either.\n\nKeith concurs.\n\nBut, there are a lot of MAYs, SHOULDs, and so on.\n\nAfter last night, I know how to proceed. We need something like\nMogul's requirements I-D. Even if it doesn't become Informational\nRFC. JG appeals for volunteers to contribute time and technology to\nmanage this survey.\n\nWe spent 60-90 minutes polling each section around the dinner\ntable. Got through section 3. I'm more worried about the caching\nsection, where less work has been done.\n\nAlternate solution is a f2f 2day retreat. I don't have the time\nmyself. Phone confs may help, 1-2 hrs at a time. We need to figure out\na process to satisfy IESG docs.\n\nLM: we don't need too many people. Just two examples, remember.\n\nKeith: we want assurance the spec is clean and clear enough that we\ncan get repeatable results. Second, it flushes out ambiguities.\n\nLM: I'm starting to get skeptical about automation -- we'll need phone\ncalls.\n\nJG: we need both\n\nYaron G: we need the list of requirements, though, so everyone can do\ntheir homework.\n\nProgress will happen after holidays, in January.\n\n==== \n\"In the bar last night...\"\n\nJG: Paul Leach realized how a small tweak to Digest would tighten\nsecurity on the back-end, do key distribution, and work across\nmultiple servers. Jeff Schiller was at the bar and gave a quick OK at\nfirst glance. Paul will post to the list very, very soon tweaks to\n2069 (since he has text from previous docs). Please pay attention to\nthis when it hits the list.\n\nLM: We hope that we can slip this in without affecting deployed\nimplementations.\n\nKeith urges an I-D ASAP, so it's not just on the mailing list.\n\n==== \nWEBPRIV (speaker: April Marine)\n\nbrought to you from the User Services Area.\n\nLot of people thought some work should be done, but not who and\nwhat. Mailing list to start, with possible second bof to evaluate a\ndraft charter by LA. web-priv@nasa.gov by next week.\n\n===== \nState of the State Address (Dave Kristol)\n\nAll seriousness aside, there was a small group of people who got\ntogether after WebPriv to decide what's left. Dare I say it, I think\nthere's reason to hope.\n\nWe have ONE tech problem left, which has to do with domain-matching\nrule for FLAT domains (intranets where, say, \"foo.\" IS an FQDN).\n\nThe other broad area is UI / privacy considerations. Main concern was\npolicy for unverifiable transactions. Document is one way; vendors are\ntending the other.\n\nFinally, future work might include Dan Jaye's certifiable cookies.\n\nDan Jaye takes the stage.\n\nDJ: we had a good discussion about trust labels for cookie policy\nafter the WebPriv meeting. The current dialog box situation is\nsuboptimal. It would be good to have signed labels that say,\ne.g. \"cookies only used for session management\".\n\nAdmins: there is no formal existence of state subgroup.\n\nDJ: wherever further work will happen, we'll have discussion.\n\nKeith: if it's homeless, it'll have a 4-week Last Call.\n\nLM: It makes sense to charter a new state-management group, since it's\na protocol.\n\nKeith: since there's no plans to formally charter state.\n\nDJ: where should we dicuss it?\n\nLM, Keith: http-state@w3.org and web-priv@nasa.gov.\n\nDJ: there is also a move in P3P at W3C for reconciling vocabularies\ndescribing privacy policy of applications. (affects much more than\ncookies).\n\n===== \nConneg, or, RFP (Recipient Feature Profile) (Ted Hardie)\n\nWe believe we can make progress quickly (by LA) on feature\nregistration.\n\nTed reported a slew of disposition decisions for existing drafts (see\nconneg minutes).\n\nDecision to setup registry *quickly*. Then the group will tackle\ngroups of features, to abbreviate lists.\n\nFinally, creating at least one prototypical example of how to store\nthis information (e.g. in LDAP). This may or may not survive\ncharter-bashing and App AD advice.\n\nLM: What's the earliest you can imagine advancing Informational? TH:\nas soon as mid-January. Join the list NOW!\n\n====== \nExtensions (Josh Cohen)\n\n[he has HTML slides which should be available soon:\nhttp://www.clock.org/~mutex/ietf ]\n\nTheres a draft already out on transactional http. JC: That's right:\nwe're going to cite LOTS of examples of extensions to justify these\ngeneric rules.\n\nKristol: I think people forget about interaction with proxies in\ngeneral. I'd like it explicitly considered (in the first part of the\noutline).\n\nJim Whitehead: Based on my experience reading PEP, I had to read it\nand talk to Henrik to figure it out. I think that we need a\nREQUIREMENTS document. JC: I agree, we need to spend some time looking\nat PEP.\n\nJG calls upon Henrik to change RK's original naming of PEP.\n\nKeith: there's no formal requirment for a BOF; we can just\nproceeed. Because of our stds. liason requirements, we may need a\nfeedback cycle for \"IESG is thinking of creating a WG\".\n\nJC: I hope this doesn't cause mission creep.\n\nKeith: there's no requirement that we have to answer all these pieces\nof feedback. It's just a week to notify people.\n\nThe new mailing list will be created and announced on the http-wg\nlist.\n\nLM: calls for supporting Schulzrinne's extened error codes in this\ngroup. Keith concurs; JG says the arguments against on http-wg were\nreally bogus.\n\nJC agrees to add it to the charter.\n\n======== \nThe ISSUES List Rides Again....\n\nJG resumes walking through the issues list.\n\n-------\nJG asks why RE-VERSION was called for.\n\nHenry Sanders: If you DON'T upgrade the request, you'll get a\nlower-version answer for your cache -- a subsequent 1.1 request CAN'T\nbe satisfied from a 1.0 cached response. 1.0 requests can be answered\nfrom 1.1 responses. Richer responses can always be used to answer\nsimpler queries.\n \nJG asks Henry to post his cogent answer to the list. Then, RE-VERSION\nwill be closed.\n\nLM: How many people here believe there will be a 1.2?\n\nLots of \"50-50\" hands up.\n\n------\nPUT-RANGE\n\nJG calls for discussion, not resolution (since Leach isn't here).\n\nYG defends: the use might be in a proxy which updates a byterange of\nits CACHED copy at the same time it passes back to the origin. I think\nthis is bad because of 1) in WebDAV, many proxies may have partial\nviews of the update changes. So, you'd have to recheck e-tags to\nupdate caches -- and redownload all that material. 2) more to the\npoint, editing ususally adds and removes material, which put-range\ndoes not do.\n\nYG, discussing with PL, resolves that the WebDAV group's PATCH method\nshould split off to handle this.\n\nLM, JG: likely outcome is to take put-range OUT. The draft will say\n\"don't do put with byteranges\"\n\nLM: the convincing argument I heard is that it requires a robust\nversion number to detect the feature: a server that did PUT but not\nbyteranges, would REPLACE the whole resource. That's why it should be\nforbidden, not discouraged.\n\nJG: right now it's unspecified in the RFC, so now we need to add\nlanguage discouraging this.\n\n----\nRANGE WITH CONTENTCODING and TRAILER FIELDS to LAST CALL.\n\n==========\nCONCLUSION\n\nLM's best estimate is that 20 more issues will be raised from the\ninteroperability testing. But regardless, this is the LAST\nmeeting. Scott Lawrence commits to continuing Thursday\ninteroperability tests until such time as testing becomes\nuninteresting. mailing list will remain open indefinitely (or until\nStandard status 2 years from now). You can also expect NG\nefforts. Currently, it's at W3C, which is not closed, but not public,\neither. In the long-term it will become a draft and proposal to IETF.\n\nAnd there was much rejoicing... off to the bar.\n\n\n\n\n \n\n\n\n"
        },
        {
            "subject": "Minutes from Second HTTP session at IETF4",
            "content": "Larry, all --\n\nThis is my transcript of the second session, and some abbreviated\n\"minutes.\" I did not take minutes for the first session on Monday,\nthough. I think we should submit Josh Cohen's slides as a separate\npart of the proceeding rather than part of the (limited to 6-pages)\nminutes.\n\nHTTP-WG is dead, Long live HTTP!, Rohit Khare\n\n========================================================== \nMinutes of the Dec 10th HTTP-WG meeting \nReported by Rohit Khare, UC Irvine\n\nThe second session reported on the myriad HTTP-related activities\nwhich will continue outside HTTP-WG, as well as the remainder of the\nISSUES list. In the previous days, there had been new meetings and\ndevelopments related to 1) interoperabilty of 1.1. implementations; 2)\nclosure on Authentication; 3) Privacy on the Web; 4) cookies; 5)\ncontent-negotiation; 6) and extensions to 1.1.\n\nTo advance to DRAFT standard, the group needs to stabilize updates to\n2068 and document at least two independent, interoperable\nimplementations of each feature -- not applications which support\nevery feature, not just shipping products. The previous night, a\nhalf-dozen client, server, and proxy developers sat down with the\nchair and editor to walk through the MUSTs and MUST NOTs section by\nsection. The main insight was that we'll need much more systematic\nsupport to document the hundreds of requirements in HTTP/1.1. Caching,\nin particular, seems to be the most fraught with difficulty. Scott\nLawrence agreed to continue his (very useful) Thursday testing bees;\nand there was a survey of interest in face-to-face implementation\nbake-offs or conference calls.\n\n2069 had one concern raised that Digest does not work well with\nproxies and not at all across multiple servers. Paul Leach will soon\npost to the list a small tweak which addresses both (and issue a new\nI-D?).\n\nUser Services sponsored a BOF on Web Privacy. April Marine reported\nthe broad support for investigating the nexus of trust issues around\nthe Web. A detailed charter awaits debate, though, on\nweb-priv@nasa.gov. A second BOF is projected for LA.\n\nDavid Kristol, editor of the cookie spec, reported that a small group\nof developers reviewed the outstanding issues with state management\nand found one technical and one political problem left. Technically,\ndomain matching does not work with flat (intranet) domains where\n\"foo.\" is an FQDN. The latter is user notification of cookies from\nunverifiable transactions -- should people be tracked by an external\nsource of inline images or applets or audio?\n\nRecipient Feature Profile (nee content-negotiation), weaves together\nextensibility threads from HTTP, printing, faxing, mail, and many\nother application-layer protocols.Ted Hardie reported the conviction\nthe group will set up a registry as quickly as possible. Then, it may\ntackle aggregation of features and a prototypical example of how to\nstore profiles within LDAP, etc.\n\nThe Extensions team reported a strawman charter for 1) producing an\nFYI document of guidance on adding features, headers, and methods to\nHTTP, 2) extending error response codes, 3) and simplifying PEP and\nOPTIONS into a reliable extension hook. It is NOT an HTTP/1.2\ngroup. Josh Cohen and Scott Lawrence will chair and edit,\nrespectively, an investigation into how-to-extend, not what-to-extend.\n\nFinally, Jim Gettys continued working through the ISSUES\nlist. RE-VERSION: Henry Sanders will redocument the justification for\nupgrading proxy connections to the highest supported version to the\nlist. PUT-RANGE is demoted as problematic (caches observe only a\nsubset of edits) and insufficient (can't insert or remove\nranges). This functionality will become PATCH in WebDAV, and hence\ndiscouraged or forbidden in HTTP/1.1\n\nThis concluded the final meeting of the HTTP Working Group. Up next:\nW3C has been working on experiments for HTTP-NG, which had a BOF at\n39th IETF and will surely be heard from in 1998.\n\n==========================================================\nTranscript of the second HTTP session, Dec 10th \nReported by Rohit Khare, UC Irvine.\n\nReports on interoperability check, webpriv, state, extend, conneg,\nissues on digest, closing plans (the very,very, very last meeting\never!)\n\n\n========== \nFrankenstein's Bride (JG)\n\nJG: for DRAFT, we MUST show two INDEPENDENT IMPLEMENTATIONS of EACH\nFEATURE which are INTEROPERABLE -- not products which implement ALL,\nnot necc. SHIPPING, either.\n\nKeith concurs.\n\nBut, there are a lot of MAYs, SHOULDs, and so on.\n\nAfter last night, I know how to proceed. We need something like\nMogul's requirements I-D. Even if it doesn't become Informational\nRFC. JG appeals for volunteers to contribute time and technology to\nmanage this survey.\n\nWe spent 60-90 minutes polling each section around the dinner\ntable. Got through section 3. I'm more worried about the caching\nsection, where less work has been done.\n\nAlternate solution is a f2f 2day retreat. I don't have the time\nmyself. Phone confs may help, 1-2 hrs at a time. We need to figure out\na process to satisfy IESG docs.\n\nLM: we don't need too many people. Just two examples, remember.\n\nKeith: we want assurance the spec is clean and clear enough that we\ncan get repeatable results. Second, it flushes out ambiguities.\n\nLM: I'm starting to get skeptical about automation -- we'll need phone\ncalls.\n\nJG: we need both\n\nYaron G: we need the list of requirements, though, so everyone can do\ntheir homework.\n\nProgress will happen after holidays, in January.\n\n==== \n\"In the bar last night...\"\n\nJG: Paul Leach realized how a small tweak to Digest would tighten\nsecurity on the back-end, do key distribution, and work across\nmultiple servers. Jeff Schiller was at the bar and gave a quick OK at\nfirst glance. Paul will post to the list very, very soon tweaks to\n2069 (since he has text from previous docs). Please pay attention to\nthis when it hits the list.\n\nLM: We hope that we can slip this in without affecting deployed\nimplementations.\n\nKeith urges an I-D ASAP, so it's not just on the mailing list.\n\n==== \nWEBPRIV (speaker: April Marine)\n\nbrought to you from the User Services Area.\n\nLot of people thought some work should be done, but not who and\nwhat. Mailing list to start, with possible second bof to evaluate a\ndraft charter by LA. web-priv@nasa.gov by next week.\n\n===== \nState of the State Address (Dave Kristol)\n\nAll seriousness aside, there was a small group of people who got\ntogether after WebPriv to decide what's left. Dare I say it, I think\nthere's reason to hope.\n\nWe have ONE tech problem left, which has to do with domain-matching\nrule for FLAT domains (intranets where, say, \"foo.\" IS an FQDN).\n\nThe other broad area is UI / privacy considerations. Main concern was\npolicy for unverifiable transactions. Document is one way; vendors are\ntending the other.\n\nFinally, future work might include Dan Jaye's certifiable cookies.\n\nDan Jaye takes the stage.\n\nDJ: we had a good discussion about trust labels for cookie policy\nafter the WebPriv meeting. The current dialog box situation is\nsuboptimal. It would be good to have signed labels that say,\ne.g. \"cookies only used for session management\".\n\nAdmins: there is no formal existence of state subgroup.\n\nDJ: wherever further work will happen, we'll have discussion.\n\nKeith: if it's homeless, it'll have a 4-week Last Call.\n\nLM: It makes sense to charter a new state-management group, since it's\na protocol.\n\nKeith: since there's no plans to formally charter state.\n\nDJ: where should we dicuss it?\n\nLM, Keith: http-state@w3.org and web-priv@nasa.gov.\n\nDJ: there is also a move in P3P at W3C for reconciling vocabularies\ndescribing privacy policy of applications. (affects much more than\ncookies).\n\n===== \nConneg, or, RFP (Recipient Feature Profile) (Ted Hardie)\n\nWe believe we can make progress quickly (by LA) on feature\nregistration.\n\nTed reported a slew of disposition decisions for existing drafts (see\nconneg minutes).\n\nDecision to setup registry *quickly*. Then the group will tackle\ngroups of features, to abbreviate lists.\n\nFinally, creating at least one prototypical example of how to store\nthis information (e.g. in LDAP). This may or may not survive\ncharter-bashing and App AD advice.\n\nLM: What's the earliest you can imagine advancing Informational? TH:\nas soon as mid-January. Join the list NOW!\n\n====== \nExtensions (Josh Cohen)\n\n[he has HTML slides which should be available soon:\nhttp://www.clock.org/~mutex/ietf ]\n\nTheres a draft already out on transactional http. JC: That's right:\nwe're going to cite LOTS of examples of extensions to justify these\ngeneric rules.\n\nKristol: I think people forget about interaction with proxies in\ngeneral. I'd like it explicitly considered (in the first part of the\noutline).\n\nJim Whitehead: Based on my experience reading PEP, I had to read it\nand talk to Henrik to figure it out. I think that we need a\nREQUIREMENTS document. JC: I agree, we need to spend some time looking\nat PEP.\n\nJG calls upon Henrik to change RK's original naming of PEP.\n\nKeith: there's no formal requirment for a BOF; we can just\nproceeed. Because of our stds. liason requirements, we may need a\nfeedback cycle for \"IESG is thinking of creating a WG\".\n\nJC: I hope this doesn't cause mission creep.\n\nKeith: there's no requirement that we have to answer all these pieces\nof feedback. It's just a week to notify people.\n\nThe new mailing list will be created and announced on the http-wg\nlist.\n\nLM: calls for supporting Schulzrinne's extened error codes in this\ngroup. Keith concurs; JG says the arguments against on http-wg were\nreally bogus.\n\nJC agrees to add it to the charter.\n\n======== \nThe ISSUES List Rides Again....\n\nJG resumes walking through the issues list.\n\n-------\nJG asks why RE-VERSION was called for.\n\nHenry Sanders: If you DON'T upgrade the request, you'll get a\nlower-version answer for your cache -- a subsequent 1.1 request CAN'T\nbe satisfied from a 1.0 cached response. 1.0 requests can be answered\nfrom 1.1 responses. Richer responses can always be used to answer\nsimpler queries.\n \nJG asks Henry to post his cogent answer to the list. Then, RE-VERSION\nwill be closed.\n\nLM: How many people here believe there will be a 1.2?\n\nLots of \"50-50\" hands up.\n\n------\nPUT-RANGE\n\nJG calls for discussion, not resolution (since Leach isn't here).\n\nYG defends: the use might be in a proxy which updates a byterange of\nits CACHED copy at the same time it passes back to the origin. I think\nthis is bad because of 1) in WebDAV, many proxies may have partial\nviews of the update changes. So, you'd have to recheck e-tags to\nupdate caches -- and redownload all that material. 2) more to the\npoint, editing ususally adds and removes material, which put-range\ndoes not do.\n\nYG, discussing with PL, resolves that the WebDAV group's PATCH method\nshould split off to handle this.\n\nLM, JG: likely outcome is to take put-range OUT. The draft will say\n\"don't do put with byteranges\"\n\nLM: the convincing argument I heard is that it requires a robust\nversion number to detect the feature: a server that did PUT but not\nbyteranges, would REPLACE the whole resource. That's why it should be\nforbidden, not discouraged.\n\nJG: right now it's unspecified in the RFC, so now we need to add\nlanguage discouraging this.\n\n----\nRANGE WITH CONTENTCODING and TRAILER FIELDS to LAST CALL.\n\n==========\nCONCLUSION\n\nLM's best estimate is that 20 more issues will be raised from the\ninteroperability testing. But regardless, this is the LAST\nmeeting. Scott Lawrence commits to continuing Thursday\ninteroperability tests until such time as testing becomes\nuninteresting. mailing list will remain open indefinitely (or until\nStandard status 2 years from now). You can also expect NG\nefforts. Currently, it's at W3C, which is not closed, but not public,\neither. In the long-term it will become a draft and proposal to IETF.\n\nAnd there was much rejoicing... off to the bar.\n\n\n\n\n \n\n\n\n"
        },
        {
            "subject": "Minutes from Second HTTP session at IETF4",
            "content": "Larry, all --\n\nThis is my transcript of the second session, and some abbreviated\n\"minutes.\" I did not take minutes for the first session on Monday,\nthough. I think we should submit Josh Cohen's slides as a separate\npart of the proceeding rather than part of the (limited to 6-pages)\nminutes.\n\nHTTP-WG is dead, Long live HTTP!, Rohit Khare\n\n========================================================== \nMinutes of the Dec 10th HTTP-WG meeting \nReported by Rohit Khare, UC Irvine\n\nThe second session reported on the myriad HTTP-related activities\nwhich will continue outside HTTP-WG, as well as the remainder of the\nISSUES list. In the previous days, there had been new meetings and\ndevelopments related to 1) interoperabilty of 1.1. implementations; 2)\nclosure on Authentication; 3) Privacy on the Web; 4) cookies; 5)\ncontent-negotiation; 6) and extensions to 1.1.\n\nTo advance to DRAFT standard, the group needs to stabilize updates to\n2068 and document at least two independent, interoperable\nimplementations of each feature -- not applications which support\nevery feature, not just shipping products. The previous night, a\nhalf-dozen client, server, and proxy developers sat down with the\nchair and editor to walk through the MUSTs and MUST NOTs section by\nsection. The main insight was that we'll need much more systematic\nsupport to document the hundreds of requirements in HTTP/1.1. Caching,\nin particular, seems to be the most fraught with difficulty. Scott\nLawrence agreed to continue his (very useful) Thursday testing bees;\nand there was a survey of interest in face-to-face implementation\nbake-offs or conference calls.\n\n2069 had one concern raised that Digest does not work well with\nproxies and not at all across multiple servers. Paul Leach will soon\npost to the list a small tweak which addresses both (and issue a new\nI-D?).\n\nUser Services sponsored a BOF on Web Privacy. April Marine reported\nthe broad support for investigating the nexus of trust issues around\nthe Web. A detailed charter awaits debate, though, on\nweb-priv@nasa.gov. A second BOF is projected for LA.\n\nDavid Kristol, editor of the cookie spec, reported that a small group\nof developers reviewed the outstanding issues with state management\nand found one technical and one political problem left. Technically,\ndomain matching does not work with flat (intranet) domains where\n\"foo.\" is an FQDN. The latter is user notification of cookies from\nunverifiable transactions -- should people be tracked by an external\nsource of inline images or applets or audio?\n\nRecipient Feature Profile (nee content-negotiation), weaves together\nextensibility threads from HTTP, printing, faxing, mail, and many\nother application-layer protocols.Ted Hardie reported the conviction\nthe group will set up a registry as quickly as possible. Then, it may\ntackle aggregation of features and a prototypical example of how to\nstore profiles within LDAP, etc.\n\nThe Extensions team reported a strawman charter for 1) producing an\nFYI document of guidance on adding features, headers, and methods to\nHTTP, 2) extending error response codes, 3) and simplifying PEP and\nOPTIONS into a reliable extension hook. It is NOT an HTTP/1.2\ngroup. Josh Cohen and Scott Lawrence will chair and edit,\nrespectively, an investigation into how-to-extend, not what-to-extend.\n\nFinally, Jim Gettys continued working through the ISSUES\nlist. RE-VERSION: Henry Sanders will redocument the justification for\nupgrading proxy connections to the highest supported version to the\nlist. PUT-RANGE is demoted as problematic (caches observe only a\nsubset of edits) and insufficient (can't insert or remove\nranges). This functionality will become PATCH in WebDAV, and hence\ndiscouraged or forbidden in HTTP/1.1\n\nThis concluded the final meeting of the HTTP Working Group. Up next:\nW3C has been working on experiments for HTTP-NG, which had a BOF at\n39th IETF and will surely be heard from in 1998.\n\n==========================================================\nTranscript of the second HTTP session, Dec 10th \nReported by Rohit Khare, UC Irvine.\n\nReports on interoperability check, webpriv, state, extend, conneg,\nissues on digest, closing plans (the very,very, very last meeting\never!)\n\n\n========== \nFrankenstein's Bride (JG)\n\nJG: for DRAFT, we MUST show two INDEPENDENT IMPLEMENTATIONS of EACH\nFEATURE which are INTEROPERABLE -- not products which implement ALL,\nnot necc. SHIPPING, either.\n\nKeith concurs.\n\nBut, there are a lot of MAYs, SHOULDs, and so on.\n\nAfter last night, I know how to proceed. We need something like\nMogul's requirements I-D. Even if it doesn't become Informational\nRFC. JG appeals for volunteers to contribute time and technology to\nmanage this survey.\n\nWe spent 60-90 minutes polling each section around the dinner\ntable. Got through section 3. I'm more worried about the caching\nsection, where less work has been done.\n\nAlternate solution is a f2f 2day retreat. I don't have the time\nmyself. Phone confs may help, 1-2 hrs at a time. We need to figure out\na process to satisfy IESG docs.\n\nLM: we don't need too many people. Just two examples, remember.\n\nKeith: we want assurneg, or, RFP (Recipient Feature Profile) (Ted Hardie)\n\nWe believe we can make progress quickly (by LA) on feature\nregistration.\n\nTed reported a slew of disposition decisions for existing drafts (see\nconneg minutes).\n\nDecision to setup registry *quickly*. Then the group will tackle\ngroups of features, to abbreviate lists.\n\nFinally, creating at least one prototypical example of how to store\nthis information (e.g. in LDAP). This may or may not survive\ncharter-bashing and App AD advice.\n\nLM: What's the earliest you can imagine advancing Informational? TH:\nas soon as mid-January. Join the list NOW!\n\n====== \nExtensions (Josh Cohen)\n\n[he has HTML slides which should be available soon:\nhttp://www.clock.org/~mutex/ietf ]\n\nTheres a draft already out on transactional http. JC: That's right:\nwe're going to cite LOTS of examples of extensions to justify these\ngeneric rules.\n\nKristol: I think people forget about interaction with proxies in\ngeneral. I'd like it explicitly considered (in the first part of the\noutline).\n\nJim Whitehead: Based on my experience reading PEP, I had to read it\nand talk to Henrik to figure it out. I think that we need a\nREQUIREMENTS document. JC: I agree, we need to spend some time looking\nat PEP.\n\nJG calls upon Henrik to change RK's original naming of PEP.\n\nKeith: there's no formal requirment for a BOF; we can just\nproceeed. Because of our stds. liason requirements, we may need a\nfeedback cycle for \"IESG is thinking of creating a WG\".\n\nJC: I hope this doesn't cause mission creep.\n\nKeith: there's no requirement that we have to answer all these pieces\nof feedback. It's just a week to notify people.\n\nThe new mailing list will be created and announced on the http-wg\nlist.\n\nLM: calls for supporting Schulzrinne's extened error codes in this\ngroup. Keith concurs; JG says the arguments against on http-wg were\nreally bogus.\n\nJC agrees to add it to the charter.\n\n======== \nThe ISSUES List Rides Again....\n\nJG resumes walking through the issues list.\n\n-------\nJG asks why RE-VERSION was called for.\n\nHenry Sanders: If you DON'T upgrade the request, you'll get a\nlower-version answer for your cache -- a subsequent 1.1 request CAN'T\nbe satisfied from a 1.0 cached response. 1.0 requests can be answered\nfrom 1.1 responses. Richer responses can always be used to answer\nsimpler queries.\n \nJG asks Henry to post his cogent answer to the list. Then, RE-VERSION\nwill be closed.\n\nLM: How many people here believe there will be a 1.2?\n\nLots of \"50-50\" hands up.\n\n------\nPUT-RANGE\n\nJG calls for discussion, not resolution (since Leach isn't here).\n\nYG defends: the use might be in a proxy which updates a byterange of\nits CACHED copy at the same time it passes back to the origin. I think\nthis is bad because of 1) in WebDAV, many proxies may have partial\nviews of the update changes. So, you'd have to recheck e-tags to\nupdate caches -- and redownload all that material. 2) more to the\npoint, editing ususally adds and removes material, which put-range\ndoes not do.\n\nYG, discussing with PL, resolves that the WebDAV group's PATCH method\nshould split off to handle this.\n\nLM, JG: likely outcome is to take put-range OUT. The draft will say\n\"don't do put with byteranges\"\n\nLM: the convincing argument I heard is that it requires a robust\nversion number to detect the feature: a server that did PUT but not\nbyteranges, would REPLACE the whole resource. That's why it should be\nforbidden, not discouraged.\n\nJG: right now it's unspecified in the RFC, so now we need to add\nlanguage discouraging this.\n\n----\nRANGE WITH CONTENTCODING and TRAILER FIELDS to LAST CALL.\n\n==========\nCONCLUSION\n\nLM's best estimate is that 20 more issues will be raised from the\ninteroperability testing. But regardless, this is the LAST\nmeeting. Scott Lawrence commits to continuing Thursday\ninteroperability tests until such time as testing becomes\nuninteresting. mailing list will remain open indefinitely (or until\nStandard status 2 years from now). You can also expect NG\nefforts. Currently, it's at W3C, which is not closed, but not public,\neither. In the long-term it will become a draft and proposal to IETF.\n\nAnd there was much rejoicing... off to the bar.\n\n\n\n\n \n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "I'm all for an editorial note about \"Content-Length\" that explains\nits difficulty in serving both as an entity length and a transfer\nlength. In fact, this kind of confusion is, I believe, one of the\nreasons why content-length was deprecated for use in mail!\n\nSo we're saddled with a legacy, and need some clear warnings that\n\"Content-Length\".\n\nIt's less clear what \"Transfer-Length\" would buy us, since we've\nsurvived for so long without it. It sounds like a good conceptual\naid (\"we shoulda done it that way\") but not a necessary protocol\naddition (\"we need to do it this way now\").\n\n> I see no alternative other than rewriting the specification to make\n> Content-length a hop-by-hop general header and not an entity header.\n\nUnfortunately, this seems (to me, personally) to be the only\nway out. I believe that most implementations treat it this\nway, in any case.\n\n> The authentication specification would also need to be modified \n> since it is not possible to put Authentication-Info in a chunked\n> trailer as it is currently defined if Content-length is the length\n> of the chunked message.\n\nThis seems acceptable to me; what do others think?\n\n\nWe will need two interoperable digest implementations (two clients, two servers)\nthat have been tested as interworking in conjunction with a\n1.1 proxy that does rewriting of the transfer encoding. Is there any\nhope of setting up such a test during the weekly testing?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": ">>>>> \"EH\" == Eric Houston:\n\nEH> Two new refinements that I would like to make:\n\nEH> 1) When the content server redirects the request to the authentication\nEH> server, it encrypts the ACL for the protected resource.  The authentication\nEH> server then validates the user against the (decrypted) ACL\n\n  Whoa - this is authentication, not authorization.  The purpose is to\n  provide a trustable identity for the end user without exposing the\n  means of doing so to the world, not to do access control.  Access\n  control depends on authentication, but authentication does not\n  include access control.  I believe that any discussion of ACLs is\n  out of scope for this specification.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "  This is getting out of hand.  We have multiple interoperable\n  implementations in which the current definition of content length\n  and its interaction with transfer encoding has been shown to work.\n  However flawed it may be in some theoretical sense, it does work and\n  should not be changed, nor should any other header to carry a length\n  be added, unless it can be shown that it actual practice something\n  is broken.\n\n  As for the digest authentication entity digest, the content length\n  is not one of the elements that presents a problem, since it does\n  not change based on the transfer encoding.  If the digest\n  information is sent in a chunked trailer then the length is known at\n  the time the entity-digest is computed, even if it was not known\n  when the headers were sent (in which case there is no Content-Length\n  header field).  We do have some _other_ problems to resolve with the\n  entity-digest calculation in face of proxies adding headers, but\n  this is not one of them.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Minutes from Second HTTP session at IETF4",
            "content": "LM, Keith: http-state@w3.org and web-priv@nasa.gov.\n\nIt's http-state@lists.research.bell-labs.com, I think.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, Scott Lawrence wrote:\n\n> \n>   This is getting out of hand.  We have multiple interoperable\n>   implementations in which the current definition of content length\n>   and its interaction with transfer encoding has been shown to work.\n\nThe Rev01 spec contains TWO contradictory definitions of content\nlength.  One says it is the length AFTER transfer encoding is applied\nthe other says it is the length BEFORE transfer encoding is applied.\nIt works at present only because the only current transfer encoding\nis chunked and it is self-delimiting so the Content-length header\nis omitted.  If there will ever be a transfer encoding which is\nnot self-delimiting then the spec must resolve this issue.\n\n>   However flawed it may be in some theoretical sense, it does work and\n>   should not be changed, nor should any other header to carry a length\n>   be added, unless it can be shown that it actual practice something\n>   is broken.\n> \n\nSomething is broken.  What is broken depends on which definition of\nContent-length you use.  \n\nIf content length is the length AFTER transfer encoding then digest\nauthentication definitively broken.  You can't create\nAuthentication-info in a trailer of a chunked object if it needs to\nknow the length of the chunked object.\n\nOn the other hand if Content-length is the length BEFORE transfer\nencoding is applied then there is no way to have a non-self delimiting\ntransfer encoding (e.g. a compression).  This is because\nContent-length would be the uncompressed length so the client wouldn't\nknow when the body ended.\n\n>   As for the digest authentication entity digest, the content length\n>   is not one of the elements that presents a problem, since it does\n>   not change based on the transfer encoding.  \n\nThat depends on the definition of Content-length.  Larry Masinter\nargues that Content-length should be the length AFTER chunking.\nIf that is the case then digest is broken.\n\nI would prefer Content-length to be the length BEFORE any transfer\nencoding.  From Scott's remarks it is clear this is what he\nunderstands content length to be.  But if Content-length is length\nbefore transfer encoding we must introduce a Transfer-length header or\nforbid any future transfer encodings which are not self-delimiting.\n\nI worry that defining content length to be the length after transfer\nencoding might have some serious gotcha's we haven't thought of.\nFor example, with that definition, if a client does a HEAD on \nresource and then a GET on the same resource the Content-lengths\nfor the two responses could be completey different.  Would this\nbreak anything?\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, John Franks wrote:\n\n> I would prefer Content-length to be the length BEFORE any transfer\n> encoding.  From Scott's remarks it is clear this is what he\n> understands content length to be.  But if Content-length is length\n> before transfer encoding we must introduce a Transfer-length header or\n> forbid any future transfer encodings which are not self-delimiting.\n\nAs I've already said, transfer encoding was introduced to resolve the\nproblem that some servers don't know the total length until \nthe whole document has been sent.  All future transfer encodings MUST\nbe self-delimiting. It would be stupid to introduce a new header\nwhose value can't always be computed to define the length of \nsome possible new method of encoding.\n\nIF and WHEN a new encoding is defined which can't be designed to be\nself-delimiting, a new header can be introduced coupled for use with\nthat encoding.  Easy enough to redefine the the new encoding to include\nan extra prefix record containing the length.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, David W. Morris wrote:\n\n> \n> \n> On Thu, 11 Dec 1997, John Franks wrote:\n> \n> > I would prefer Content-length to be the length BEFORE any transfer\n> > encoding.  From Scott's remarks it is clear this is what he\n> > understands content length to be.  But if Content-length is length\n> > before transfer encoding we must introduce a Transfer-length header or\n> > forbid any future transfer encodings which are not self-delimiting.\n> \n> As I've already said, transfer encoding was introduced to resolve the\n> problem that some servers don't know the total length until \n> the whole document has been sent.  \n\nThat was the reason \"chunked\" was introduced.  There are other reasons\none might want to use a Transfer-encoding, e.g. compression to conserve\nbandwidth.\n\n> All future transfer encodings MUST\n> be self-delimiting. \n\nWhat is the basis of this assertion?\n\nI find no evidence this was the intent of the protocol authors.\n\n> It would be stupid to introduce a new header\n> whose value can't always be computed to define the length of \n> some possible new method of encoding.\n> \n\nOr it might be stupid to preclude the possibility of ever using \nlots of useful transfer encodings to avoid introducing a new\nheader.\n\nHere's one scenario that I think people have in mind.  A proxy\nreceives documents, compresses them on the fly and caches the\ncompressed version (saving disk space).  When a client requests the\ndocument it is served in the compressed form (with the appropriate\nTransfer-encoding header) to save bandwidth.  The proxy knows\nthe compressed length; the client needs to know it for the transaction\nto work.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "DM> All future transfer encodings MUST be self-delimiting.\n\nJF> What is the basis of this assertion?\n\n  I believe that he was just noting the fact that given the current\n  usage of the Content-Length, that is the way it is.\n\nJF> ... preclude the possibility of ever using\nJF> lots of useful transfer encodings to avoid introducing a new\nJF> header.\n\n  At this point there are a great many reasons not to introduce a new\n  header without a compelling reason - they are called deployed\n  implementations.  There is no reason why the definition of a new\n  transfer encoding cannot be done in a way that corrects this\n  situation, and no benefit in correcting it before that time.\n\n  There are no transfer encodings in 1.1 for which the length is\n  ambiguous; we don't need to change the spec now.\n\n  I have already logged this among the caveats we will document in the\n  guidelines for extending HTTP.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "On Mon, 8 Dec 1997, Paul Leach wrote:\n\n> I think that the spec for \"domain\" is broken -- it specifies a list of URIs,\n> but doesn't say that these can be _prefixes_ of URIs that may also use the\n> same credentials. Without that, it is pretty uselss, IMHO.\n\nActually my guess is that this is what's happening already implicitly (it's\nhow I've implemented it): if you take the paragraph from the authentication\nspec\n\n     A client SHOULD assume that all paths at or deeper than the depth of the\n     last symbolic element in the path field of the Request-URI also are\n     within the protection space specified by the Basic realm value of the\n     current challenge. A client MAY send the corresponding Authorization\n     header with requests for resources in that space without receipt of\n     another challenge from the server.\n\nand apply it to Digest then you're effectively assuming all URIs to be\nprefixes.\n\nIs there a specific reason this shouldn't be done? My feeling was (and\nlooking at the digest implementation in a certain well known server seems\nto confirm this) that people will use digest as a drop in replacement for\nbasic authentication, and hence the rules for pre-emptively sending auth\ninfo should be the same for both schemes.\n\nIn a similar vein, I've never really understood the motivation for the\nlast sentence in:\n\n     domain\n       A space-separated list of URIs, as specified in RFC XURI [7]. The\n       intent is that the client could use this information to know the set\n       of URIs for which the same authentication information should be sent.\n       The URIs in this list may exist on different servers. If this keyword\n       is omitted or empty, the client should assume that the domain\n       consists of all URIs on the responding server.\n\nIt seems a little overeager to me. I'd say drop it completely (assuming\nthe application of the first quoted paragraph to digest) or replace it\nwith something like\n\n     domain\n       A space-separated list of URIs, as specified in RFC XURI [7]. The\n       intent is that the client could use this information to know the set\n       of URIs for which the same authentication information should be sent.\n       The URIs in this list may exist on different servers. If this keyword\n       is omitted or empty, the client should assume that the domain\n       consists of all URIs on the responding server with paths at or deeper\n       than the depth of the last symbolic element in the path field of the\n       Request-URI.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, Scott Lawrence wrote:\n\n>   At this point there are a great many reasons not to introduce a new\n>   header without a compelling reason - they are called deployed\n>   implementations.  \n\nAny current implementation that is compliant would remain compliant\nif a Transfer-length header is added.  \n\n>\n>   There are no transfer encodings in 1.1 for which the length is\n>   ambiguous; we don't need to change the spec now.\n> \n\nAmbiguity may be in the eye of the beholder, but many people believe\nthat the content length of a chunked object is the length AFTER\nchunking.  They have a very good case based on section 14.14 of the\nRev01 spec.  Even a smart guy like Dave Kristol expressed this view\nhere recently, but I hope we've converted him.  :) \n\nIt can also be argued based on section 7 that the content length is\nthe length before chunking.  You and I agree that this is what it\nshould be, but I don't in all honesty see how one can say the spec is\nunambiguous.\n\nThe specification is very explicit that if a Content-length header\nexists it must contain the length AFTER the transfer coding is applied.\nThe only reason this is not a problem for chunked is the spec also\nforbids the existence of a Content-length header if chunking is used.\nIt would be possible to forbid the existance of a Content-length \nheader whenever any transfer encoding exists, but then it is pretty\nstupid to say that Content-length is the length after encoding.\n\nFor Authentication-info to work content-length must be the length\nbefore a transfer encoding is applied.  If everyone could agree on\nthat as the definition of content length, I would be happy.  Then in\nthe future we would either have to introduce Transfer-length or forbid\nthe use of Content-length with all transfer encodings (as we do with\nchunked).\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "This is not &quot;this is not a date&quot",
            "content": "Dave Kristol writes:\n    I like Benjamin Franz's suggestion of a fixed date that means \"this\n    is not a date\" as a placeholder.\n\nand various other people seem to agree.\n\nI have a suggestion: instead of coming up with a new syntax\nfor \"this is not a date\" (when the existing date syntax, horribly\ninefficient and hard-to-parse as it is, has at least more or\nless been tamed by implementors) perhaps we can get by with\nsomething a little simpler.\n\nFor example, we could say\n\nIf the sender is required to send a Date header\nby some part of this specification, and is unable\nto generate a current HTTP-date value, it SHOULD\nsend a legal HTTP-date value that is provably in\nthe past.\n\nFor example, send\nDate: Thu, 01 Jan 1970 00:0:01 GMT\n\nAs far as I can tell, this will not lead to any more trouble\nthan simply omitting the Date header.  It also seems to be\nlegal according to section 14.19 of -rev-01.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "editorial glitch in section 14.1",
            "content": "It currently says:\n\n       In theory, the date SHOULD represent the moment just before the\n       entity is generated. In practice, the date can be generated at\n       any time during the message origination without affecting its\n       semantic value.\n\nI don't think it makes sense for a normative keyword such as \"SHOULD\"\nto be preceded by \"In theory\", except if this is clearly marked as\na \"Note\".  This would be very confusing to someone trying to figure\nout if this is a real SHOULD, or just a theory.\n\nHow about\n\n       In theory, the date ought to represent the moment just before the\n       entity is generated. In practice, the date can be generated at\n       any time during the message origination without affecting its\n       semantic value.\n\nIn fact, if you read 14.19 carefully, it never has any actual\nnormative requirement on what the Date value MUST or SHOULD be\n(except for this \"in theory\").  But perhaps what we really meant\nto say, before the paragraph quoted above, is:\n\nThe HTTP-date sent in a Date header SHOULD NOT represent a date\nand time subsequent to the generation of the message.  It\nSHOULD represent the best available approximation of the date\nand time of message generation, unless the implementation has\nno means of generating a reasonably accurate date and time.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "I agree with John Franks and Roy Fielding.  There seems to be no way to\nmake Transfer-Encoding work, in the general case, without some\nresolution to this problem.  We went through a discussion of *why*\nTransfer-Encoding is necessary a few weeks ago, and if we are going to\ngo ahead with the TE header (to make non-chunked transfer-codings\nfeasible), then we really can't avoid solving the length problem as\nwell.\n\nHere's a sketch of a specific proposal:\n\n   (1) The definition in 7.2.2:\nThe length of an entity-body is the length of the message-body after\nany transfer codings have been removed.\n    should be retained.  At first I thought \"maybe we should rename\n    this the 'entity-length'\", but we already use that non-terminal (in\n    Content-Range) to mean something different.  Which maybe should be\n    changed to use \"instance-length\", since the \"entity-length\" used\n    with a Content-Range has nothing to do with the length of the\n    entity-body.  But I digress.\n   \n   (2) We add this definition (somewhere)\nThe transfer-length of a message is the length of the message-body\nas it appears in the message; that is, after any transfer codings\nhave been applied.\n\n   (3) We add a new message-header\n\nTransfer-Length = \"Transfer-Length\" \":\" transfer-length\n\n   (4) We add a requirement that a non-identity transfer-coding MUST\n   either be self-delimiting or be accompanied by a Transfer-Length header.\n   \n   (5) We generalize this statement in section 4.4 (Message Length)\n       Messages MUST NOT include both a Content-Length header field and\n       the \"chunked\" transfer coding. If both are received, the\n       Content-Length MUST be ignored.\n   so that it reads\n       Messages MUST NOT include both a Content-Length header field and\n       a non-identity transfer coding. If both are received, the\n       Content-Length MUST be ignored.\n\n   (6) In 13.5.2 (Non-modifiable Headers), where it currently says\nA cache or non-caching proxy MUST NOT modify any of the following\nfields in a response:\n\n         .  Expires\n         .  Content-Length\n\n       but it may add any of these fields if not already present.\n   if one interprets \"modify\" to include \"delete\", then this would seem\n   to prevent a proxy from adding *any* transfer-coding that changes\n   the message length(!), including chunked!  And so this looks like a\n   potential ambiguity.  I think we need to add something like:\n\n       The Content-Length header MAY be deleted, but if so, the proxy\n       MUST provide some other means for the recipient to discover the\n       length of the entity (i.e., a self-delimiting transfer-coding\n       such as chunked, a Transfer-length header, or non-persistent\n       connection with no transfer-coding.)  A Content-Length header\n       MUST be added if the proxy provides no other means for the\n       recipient for discovering the length of the entity.\n\n   In other words, I guess Content-Length really is a hop-by-hop\n   header, but there are some rules to follow about if and how a proxy\n   regenerates it for the next hop.\n\nThis all means that, in section 4.4, the statement:\n     3. If a Content-Length header field (section 14.14) is present, its\n        decimal value in OCTETs represents the length of the message-body.\nis still essentially correct.  That is, if you see a Content-Length,\nthen the length of the entity-body is the same as the length of the\nmessage-body.  Since (if there is no Transfer-Encoding) we expect\nthe recipient to use Content-Length to find the end of the body, this\nstatement is true and useful.\n\nThe introduction of Transfer-Length along these lines won't cause\n*any* problems for deployed implementations, since nobody should\nbe sending a non-self-delimiting transfer coding unless the recipient\nhas, by using the TE header, indicated that it can accept such a\ntransfer coding.\n\nI'm not familiar enough with Digest Authentication to know if this\nclarification requires some debugging of the D. A. spec.\n\n-Jeff\n\nP.S.: Actually, instead of \"Transfer-Length\", which is 15 characters,\nI would suggest a header name such as \"TLen\".  After all, the obvious\nnon-self-delimiting transfer codings are for the purpose of data\ncompression, so why add unnecessary bytes?\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, Jeffrey Mogul wrote:\n\n> \n> Here's a sketch of a specific proposal:\n> \n\nThis looks good\n\n>    (1) The definition in 7.2.2:\n> The length of an entity-body is the length of the message-body after\n> any transfer codings have been removed.\n>     should be retained.  At first I thought \"maybe we should rename\n>     this the 'entity-length'\", but we already use that non-terminal (in\n>     Content-Range) to mean something different.  Which maybe should be\n>     changed to use \"instance-length\", since the \"entity-length\" used\n>     with a Content-Range has nothing to do with the length of the\n>     entity-body.  But I digress.\n\nIt would be good to do this renaming or at least name the length of\nthe entity-body something.  This is the number which Digest\nauthentication needs to use.  The current D-A spec uses\n\"Content-Length\" and \"content length\" with the intended meaning of\nlength of the entity-body (the ambiguity of this is how we got into\nthis discussion).  Anyway, *some term* should be defined meaning\nlength of the entity-body and that term should replace \"content\nlength\" and \"Content-Length\" in the DA spec.\n\nInterestingly, the D-A spec contains the sentence.\n\n       \"The HTTP/1.1 spec requires that content length is well defined\n       in all messages, whether or not there is a Content-Length header.\"\n\nSounds like wishful thinking at this point, but I guess this sentence\nshould be removed if content length is replaced by entity-length.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "Paul Leach <paulle@microsoft.com> wrote:\n>> ----------\n>> From: jg@pa.dec.com[SMTP:jg@pa.dec.com]\n>> Sent: Wednesday, December 10, 1997 4:48 PM\n>> \n><snip>\n>\n>> I think you are confused....  In Rev-01, only an origin server is allowed\n>> to generate a 305 response.  It is authoritative for that resource, so\n>> the spoofing problems don't come up (and is the reason for that text being\n>> in the document...)\n>> \n>And exactly how can the browser tell that it was the origin server that sent\n>the 305? And not the untrustworthy proxy in between the client and the\n>server?\n>\n>I know that normally one trusts one's proxy, but since security issues are\n>being raised here, the question needs to be asked.\n\nThat's not a problem for the Lynx implementation because it\nwill show the body instead of acting on the 305 if it already is using\na proxy, on the assumption that the UA which receives it from the origin\nserver should act on it, and that can't be the browser if it already\nis using a proxy (plus, the browser's current proxy may be obligatory\nfor a firewall).  Also note, as was raised in the recent discussion,\nthat if a proxy acts on it, you need a GET-only requirement, because a\nPOST should not be redirected without confirmation by the human user of\nthe browser.\n\nBut this all seems academic upon reading the statement in the\nrecently posted minutes of the IETF meeting that to be retained in the\nDraft Standard there must be two independent implementations, because\nthere is only one.  So I guess it's indeed bye bye 305. (i.e., put it\noff with 306 to a new draft :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "remov",
            "content": "On Thu, 11 Dec 1997, Monette wrote:\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "  I'm all for fixing any words in the spec that make the definition of\n  Content-Length ambiguous, of course.\n\n>>>>> \"JF\" == John Franks <john@math.nwu.edu> writes:\n\nJF> For Authentication-info to work content-length must be the length\nJF> before a transfer encoding is applied.  If everyone could agree on\nJF> that as the definition of content length, I would be happy.  Then in\nJF> the future we would either have to introduce Transfer-length or forbid\nJF> the use of Content-length with all transfer encodings (as we do with\nJF> chunked).\n\n  Agreed.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: LYNXDEV two curiosities from IETF HTTP session",
            "content": "On Thu, 11 Dec 1997, Foteos Macrides wrote:\n\n>         But this all seems academic upon reading the statement in the\n> recently posted minutes of the IETF meeting that to be retained in the\n> Draft Standard there must be two independent implementations, because\n> there is only one.  So I guess it's indeed bye bye 305. (i.e., put it\n> off with 306 to a new draft :)\n\nJust for the record, I implement the 305 too. However, I haven't seen\nany server which sends it so far, so you're conclusion is probably\nright.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "> >    domain\n> >      A space-separated list of URIs, as specified in RFC XURI [7]. The\n> >      intent is that the client could use this information to know the set\n> >      of URIs for which the same authentication information should be sent.\n> >      The URIs in this list may exist on different servers. If this keyword\n> >      is omitted or empty, the client should assume that the domain\n> >      consists of all URIs on the responding server with paths at or deeper\n> >      than the depth of the last symbolic element in the path field of the\n> >      Request-URI.\n> >\n> What does \"last symbolic element in the path field of the Request-URI\" mean?\n\nMaybe an example is best. Assume the Request-URI is\n\"http://somewhere/the/path/index.html\" then you want to to talk about all\nURIs with a prefix of \"http://somewhere/the/path/\", i.e. the scheme, the\nsite component and the path component of the Request-URI minus any trailing\nsegment. I assumed \"symbolic element\" to refer to \"/\".\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, Scott Lawrence wrote:\n\n>   There are no transfer encodings in 1.1 for which the length is\n>   ambiguous; we don't need to change the spec now.\n\nI'm not sure this is true. In the latest draft I discovered the\nfollowing\nin Section 3.6:\n\n     The Internet Assigned Numbers Authority (IANA) acts as a registry\nfor\n     transfer-coding value tokens. Initially, the registry contains the\n     following tokens: \"chunked\" (section 3.6.1), \"identity\" (section\n3.6.2),\n     \"gzip\" (section 3.5), \"compress\" (section 3.5), and \"deflate\"\n(section\n     3.5).\n\nTwo questions:\n\n1) Are gzip, compress and deflate really to be used as both transfer\n   encodings and content encodings? What's the rationale behind that?\n\n2) I'm not very familiar with the details of these encodings, but I\n   believe they aren't self delimiting. Is this true?\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Life is hard... and then you die. wrote:\n\n> On Thu, 11 Dec 1997, Scott Lawrence wrote:\n> \n> >   There are no transfer encodings in 1.1 for which the length is\n> >   ambiguous; we don't need to change the spec now.\n> \n> I'm not sure this is true. In the latest draft I discovered the\n> following\n> in Section 3.6:\n> \n>      The Internet Assigned Numbers Authority (IANA) acts as a registry\n> for\n>      transfer-coding value tokens. Initially, the registry contains the\n>      following tokens: \"chunked\" (section 3.6.1), \"identity\" (section\n> 3.6.2),\n>      \"gzip\" (section 3.5), \"compress\" (section 3.5), and \"deflate\"\n> (section\n>      3.5).\n> \n> Two questions:\n> \n> 1) Are gzip, compress and deflate really to be used as both transfer\n>    encodings and content encodings? What's the rationale behind that?\n> \n\nYes.  A proxy may wish to compress an object before transmitting it\nto a client to improve bandwith utilization or perceived speed.\nThe proxy can add a transfer encoding, but not a content encoding.\n\n> 2) I'm not very familiar with the details of these encodings, but I\n>    believe they aren't self delimiting. Is this true?\n> \n\nI am not sure.  But from the recent discussion it is very clear\nthat it is cruicially important that all transfer encodings be\nexplicitly defined to be self-delimiting or non-self-delimiting.\nI am not sure this is done in the IANA registry.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, Paul Leach wrote:\n\n> An alternate proposal, which I believe is simpler and requires less\n> modification:\n\n\nI like this proposal.  It does seem simpler.\n\n> \n> Content-Length, if validly present, is the length of the entity-body (which\n> is the message-body after transfer codings are removed). It is also the\n> length of the message-body if no transfer-coding is used.\n> Content-Length MUST NOT be present if a transfer coding is used. If it is\n> present in such cases, it is invalid, and the robustness principle says it\n> should be ignored.\n> \n\n<snip>\n\n> \n> Transfer-Length, if validly present, is the length of the message-body.\n> Transfer-Length MUST NOT be present on self-delimiting transfer codings. If\n> it is present in this case, it is invalid, and the robustness principle says\n> it should be ignored.\n> \n\nThis makes it crucially important for every transfer encoding to be\nEXPLICITLY DEFINED as self-delimiting or non-self-delimiting.\n\n> Under these rules, Content-Length is still logically end-to-end -- the\n> header may not physically be present, but its value if it is ever present is\n> well-defined end-to-end and the same end-to-end.\n> \n\nThis is a virtue of Paul's proposal.\n\n> \n> Digest Auth should drop all reference to Content-Length and replaces it with\n> \"length of entity-body\", which is always well-defined and always\n> determinable given the above rules.\n> \n\nAgreed.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: This is not &quot;this is not a date&quot",
            "content": ">>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> If the sender is required to send a Date header\nJM> by some part of this specification, and is unable\nJM> to generate a current HTTP-date value, it SHOULD\nJM> send a legal HTTP-date value that is provably in\nJM> the past.\n\nJM> For example, send\nJM> Date: Thu, 01 Jan 1970 00:0:01 GMT\n\nJM> As far as I can tell, this will not lead to any more trouble\nJM> than simply omitting the Date header.  It also seems to be\nJM> legal according to section 14.19 of -rev-01.\n\n  I'm concerned that this approach would confuse proxies when combined\n  with max-age; anything that attempts to use the Date value to figure\n  out how old a response _really_ is using the date value will be\n  hopelessly messed up (your excellent work on clock unsyncronization\n  has already shown that this is essentially hopeless, Jeff, but I\n  don't think that will stop people from trying).\n\n  We have the same problem with Date and Expires; proxies may add them\n  (section 13.5.2) even when the origin server used an empty value\n  when the digest was generated.  As I see it, we have four\n  alternatives:\n\n   1) Change the rule for whether or not these may be added.\n\n      I include this for completeness - I don't think it would work\n      because existing (even 1.0) implementations would still do it.\n\n      1a) Don't allow the additions when authentication is in use.\n\n          Doesn't work because the authentication fields may be in the\n          trailer where the proxy can't see them when building the\n          header (we should allow for proxies that forward and cache\n          in parallel).\n\n   2) Have the origin server put in bogus values and use those in the\n      calculation.\n\n      I'm concerned that this would cause confusion with age\n      calculations and perhaps cache validity problems.\n\n   3) Put a cleartext attribute in the digest headers to pass the\n      value used by the origin server when computing the digest.  We\n      do this now with the digest-uri-value for the same reason (the\n      value in the received URI line may not be what the sender\n      sent).\n\n   4) Remove these components from the entity digest calculation.  If\n      we accept Paul Leachs suggestion that we correct/redefine the\n      digest to allow for 3rd party authentication anyway, this\n      becomes an option to consider.\n\n  I don't believe that 1 or 2 are good ideas for the reasons above.\n\n  Number 3 is workable, but makes digest that much more trouble to do\n  and I'd hate to create any more barrier to getting this\n  widely implemented.\n\n  My own inclination is to bite the bullet and have a hard look at 4;\n  this means figuring out whether we really need all those fields in\n  the entity digest.  If we removed the problematic fields and made\n  Pauls change, the definition (without my <<<< markers) would become:\n\n    entity-digest<\"> KD ( H( H( A1 ) )              <<<<< was H(A1)\n                         ,unquoted nonce-value \":\"\n                          Method \":\"\n                                                    <<<<< date was here\n                          entity-info \":\"\n                          H(entity-body)\n                         )\n                 <\">\n\n       entity-info       =\n         H(\n           digest-uri-value \":\"\n           media-type       \":\"   ; Content-Type, see section 3.7 of [2]\n           *DIGIT           \":\"   ; Content-Length, see 10.12 of [2]\n           content-coding   \":\"   ; Content-Encoding, see 3.5 of [2]\n                 <<<<<< l-m and expires were here\n           )\n\n  Talking off line with Paul, he said that the expires and\n  last-modified values were originally included here to provide replay\n  protection; even with a repeated nonce-value these values would\n  make replay harder.  We already have text in there on the importance\n  of good nonce selection, and mechanisms for changing the nonce\n  without adding round trips.\n\n  With abject apologies to everyone else who has already implemented\n  this, I suggest that we make this change in the interest of making\n  the mechanism work and getting others to join us in eliminating\n  cleartext passwords with a low cost authentication scheme.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "Ronald.Tschalaer@psi.ch (Life is hard... and then you die.) wrote:\n>> >    domain\n>> >      A space-separated list of URIs, as specified in RFC XURI [7]. The\n>> >      intent is that the client could use this information to know the set\n>> >      of URIs for which the same authentication information should be sent.\n>> >      The URIs in this list may exist on different servers. If this keyword\n>> >      is omitted or empty, the client should assume that the domain\n>> >      consists of all URIs on the responding server with paths at or deeper\n>> >      than the depth of the last symbolic element in the path field of the\n>> >      Request-URI.\n>> >\n>> What does \"last symbolic element in the path field of the Request-URI\" mean?\n>\n>Maybe an example is best. Assume the Request-URI is\n>\"http://somewhere/the/path/index.html\" then you want to to talk about all\n>URIs with a prefix of \"http://somewhere/the/path/\", i.e. the scheme, the\n>site component and the path component of the Request-URI minus any trailing\n>segment. I assumed \"symbolic element\" to refer to \"/\".\n\nYour example is correct, but \"symbolic elements\" refers to\nthe \"substrings in a URL path that comprise a hierarchy delimited\nby slashes\".  They are symbolic because the server's configuration\nfile normally maps them appropriately for the platform's physical\nfile system, whereas the URL paths, themselves, have a platform\nindependent syntax.  The \"last symbolic element\" in a Request-URI\nsuch as  http://somewhere/the/path/  is a zero-length substring\nfollowing the last slash, which by convention is interpreted as a\n\"symbol\" for the configured index filename, or for a directory\nlisting if allowed by the server and the configured index filename\nis not found.  The logic is homologous to that for resolving\npartial references versus a base URL.  The \"template\" terminology\nused originally, when Ari was developing Basic authentication for\nthe CERN server, may be more clear, but didn't catch on.  The\n\"template\" is everything up to the last slash of the path (or\nimplied lead slash for an http(s) Request-URI with no explicit\npath), and anything which has that as a prefix is considered to\nbe in the protection space specified by the realm value.  You're\nbasically suggusting that the items in a \"domain\" list should be\nconsidered \"templates\" (prefixes) for (symbolic) protection spaces,\nand that the default is to use a \"template\" derived from the\nRequest-URI.\n\nI'm not sure how well that would work in conjunction with\nnounce handing, and other aspects of Digest authentication which\nare more complicated than Basic, but agree with you that most\nimplementors would like it to be as \"drop in\" as possible with\nrespect to existing Basic implementations.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Thu, 11 Dec 1997, John Franks wrote:\n\n> Transfer-encoding header) to save bandwidth.  The proxy knows\n> the compressed length; the client needs to know it for the transaction\n> to work.\n\nAs I said ... redefine the encoding to include a header (within the\noutput of the encoding process) which gives\nthe length.  Of introduce a transfer-length header at that time.\n\nThe length should be part of the encoding. Otherwise the next problem\nwill be when a nested encoding is introduced. Where to put the\nlength of that encoding.\n\nThere is no risk to future extensibility if we ignore this potential\nat this time.  Any such new encoding must be a 'closed' system in that\nboth ends must understand it.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "** Reply to note from John Franks <john@math.nwu.edu> Thu, 11 Dec 1997 10:36:52 -0600 (CST)\n>  \n> I would prefer Content-length to be the length BEFORE any transfer\n> encoding.  From Scott's remarks it is clear this is what he\n> understands content length to be.\n\nAnd it is how we (IBM) understand it also.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "After thinking about this some more, I believe we've (almost) all\nmissed an obvious solution.\n\nWe're concerned about how the sender specifies the actual length\nof the message body when a transfer-coding is used.\n\nDave Morris has been saying \"redefine the encoding to include a header\n(within the output of the encoding process) which gives the length.\"\nWhich I had been thinking of as a bad idea, because it means that\nHTTP would be in the business of specifying the format of the\nmessage bodies.\n\nBut then I remembered something that Dave had written earlier:\n    We introduced CHUNKed transfer encoding because it is difficult for\n    some servers to know the content length prior to beginning to send\n    data.\n(in this case, he was discussing trailers, but it's an important\nobservation.)\n\nAnd we already have the ability to specify multiple transfer codings\nin the Transfer-Encoding header.\n\nSo: Instead of defining a Transfer-Length header, we could simply\nadd this rule:\n\nIf a message is sent on a persistent connection using\na transfer-coding that does not exactly preserve the\nlength of the data being encoding, then the \"chunked\"\ntransfer-coding MUST be used, and MUST be the last\ntransfer-coding applied.\n\nI.e., instead of sending\n\nHTTP/1.1 200 OK\nDate: Thu, 11 Dec 1997 20:33:51 GMT\nTransfer-Encoding: compress\nTransfer-Length: 12345\n\n... compressed data ...\n\na server could send\n\nHTTP/1.1 200 OK\nDate: Thu, 11 Dec 1997 20:33:51 GMT\nTransfer-Encoding: compress, chunked\n\n3039\n... compressed data ...\n0\n\n[Note: 12345 = 0x3039]\n\nSince the chunked encoding by definition includes the \"transfer\nlength\", we don't need another header field.  And all HTTP/1.1\nimplementations are required to support (\"receive and decode\")\nthe chunked transfer-coding, so we don't need to argue about\nwhether this is interoperable.\n\nWe don't have to require a chunked transfer-coding when the\nend-of-message is marked by end-of-connection (not that we\nwant to encourage it, of course) since current HTTP/1.0 practice\nsuggests that this basically works.\n\nAside from that, I don't have much to complain about Paul Leach's\n\"simplified\" proposal, although I'm not sure it would really be\nany simpler once all the definitions are made consistent.\n\nWhen Paul writes:\n    Under these rules, Content-Length is still logically end-to-end --\n    the header may not physically be present, but its value if it is\n    ever present is well-defined end-to-end and the same end-to-end.\nI'm not sure it makes sense to talk about a header being \"end-to-end\"\nif it isn't actually transmitted on some hops.  The content-length\n*value* is certainly end-to-end (although this is really an\nentity-length value), but I'm not sure we should be calling a\nheader end-to-end if it is possible to remove it.  After all,\nthe definition in 13.5.1 says:\n       .  End-to-end headers, which must be transmitted to the ultimate\n  recipient of a request or response. End-to-end headers in\n  responses must be stored as part of a cache entry and\n  transmitted in any response formed from a cache entry.\n\nDo you really want to change this definition?  Or admit that\nContent-Length is another category?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Jeffrey Mogul wrote:\n\n> After thinking about this some more, I believe we've (almost) all\n> missed an obvious solution.\n> \n> We're concerned about how the sender specifies the actual length\n> of the message body when a transfer-coding is used.\n> \n> Dave Morris has been saying \"redefine the encoding to include a header\n> (within the output of the encoding process) which gives the length.\"\n> Which I had been thinking of as a bad idea, because it means that\n> HTTP would be in the business of specifying the format of the\n> message bodies.\n\nActually, I thought I was discussing the property of the encoding and\nasserting by example that I believed it was trival for the implementer\nof the transfer encoding to meet the requirement.\n\n> \n> But then I remembered something that Dave had written earlier:\n>     We introduced CHUNKed transfer encoding because it is difficult for\n>     some servers to know the content length prior to beginning to send\n>     data.\n> (in this case, he was discussing trailers, but it's an important\n> observation.)\n\nI'll take your word about the context ... I think I've attempted to\nmake that point more than once ... but what I've been trying to\nsay is that there is a catch-22 ... either the length of the\ntransfer encoding result is self defining OR we are right back where we\ninvented chunked encoding because we don't know the length til the\nend of the transfer.\n\n\n> \n> And we already have the ability to specify multiple transfer codings\n> in the Transfer-Encoding header.\n> \n> So: Instead of defining a Transfer-Length header, we could simply\n> add this rule:\n> \n> If a message is sent on a persistent connection using\n> a transfer-coding that does not exactly preserve the\n> length of the data being encoding, then the \"chunked\"\n> transfer-coding MUST be used, and MUST be the last\n> transfer-coding applied.\n\nSounds like a complete solution to me.  Of course, I think there\nmight still be a few words about content-length to bring into\nalignment.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": ">>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> After thinking about this some more, I believe we've (almost) all\nJM> missed an obvious solution.\n\nJM> If a message is sent on a persistent connection using\nJM> a transfer-coding that does not exactly preserve the\nJM> length of the data being encoding, then the \"chunked\"\nJM> transfer-coding MUST be used, and MUST be the last\nJM> transfer-coding applied.\n[...]\nJM> a server could send\n\nJM> HTTP/1.1 200 OK\nJM> Date: Thu, 11 Dec 1997 20:33:51 GMT\nJM> Transfer-Encoding: compress, chunked\n\nJM> 3039\nJM> ... compressed data ...\nJM> 0\n\n  Simple and elegant.  Let's make it so.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Jeffrey Mogul wrote:\n\n> \n> a server could send\n> \n> HTTP/1.1 200 OK\n> Date: Thu, 11 Dec 1997 20:33:51 GMT\n> Transfer-Encoding: compress, chunked\n> \n> 3039\n> ... compressed data ...\n> 0\n> \n\nThis raises the whole issue of stacked transfer encodings.  \nAre you suggesting that arbitrary stackings be allowed, or\njust two, the second of which is chunked?  This would need to\nbe clarified.  But generally, at first glance I don't see any\nproblems.\n\n> When Paul writes:\n>     Under these rules, Content-Length is still logically end-to-end --\n>     the header may not physically be present, but its value if it is\n>     ever present is well-defined end-to-end and the same end-to-end.\n> I'm not sure it makes sense to talk about a header being \"end-to-end\"\n> if it isn't actually transmitted on some hops.\n\nWhat I would most like to see is the assertion that if a Content-length\nheader is present its value is the length of the entity-body.  It\nmay not be too important that it be officially end-to-end.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, John Franks wrote:\n\n> What I would most like to see is the assertion that if a Content-length\n> header is present its value is the length of the entity-body.  It\n\nAfter removing all transfer encodings. Then there would seem to be\nno conflict between Content-length and transfer-encoding of any form.\nAnother simplification.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "John Franks wrote:\n    This raises the whole issue of stacked transfer encodings.  \n    Are you suggesting that arbitrary stackings be allowed, or\n    just two, the second of which is chunked?  This would need to\n    be clarified.  But generally, at first glance I don't see any\n    problems.\n\nI'm not \"suggesting\" anything.  It's already in the spec, and\nhas been there since at least RFC2068:\n\n14.40 Transfer-Encoding\n\n   The Transfer-Encoding general-header field indicates what (if any)\n   type of transformation has been applied to the message body in order\n   to safely transfer it between the sender and the recipient. This\n   differs from the Content-Encoding in that the transfer coding is a\n   property of the message, not of the entity.\n\n          Transfer-Encoding     \n  = \"Transfer-Encoding\" \":\" 1#transfer-coding\n\nthe BNF clearly allows any number of transfer-codings.\n\nThe current -rev-01 draft adds:\n       If multiple encodings have been applied to an entity, the transfer\n       codings MUST be listed in the order in which they were applied.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Jeffrey Mogul wrote:\n\n> \n> If a message is sent on a persistent connection using\n> a transfer-coding that does not exactly preserve the\n> length of the data being encoding, then the \"chunked\"\n> transfer-coding MUST be used, and MUST be the last\n> transfer-coding applied.\n> \n\nIs there a reason to require that chunked be applied after a\nself-delimiting transfer encoding?  There would be a (probably\nslight)  performance penality for doing it and I don't see the\npurpose.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "    > If a message is sent on a persistent connection using\n    > a transfer-coding that does not exactly preserve the\n    > length of the data being encoding, then the \"chunked\"\n    > transfer-coding MUST be used, and MUST be the last\n    > transfer-coding applied.\n    > \n    \n    Is there a reason to require that chunked be applied after a\n    self-delimiting transfer encoding?  There would be a (probably\n    slight)  performance penality for doing it and I don't see the\n    purpose.\n\nIt seems like a mistake to get into the business of specifying\nself-delimiting transfer codings (aside from chunked, which is\na generic way to do that).  This way, we have some modularity\nin the protocol design.  I.e., we have only three ways to find\nthe end of a message (EOF, Content-Length, chunked); why add\nmore?\n\nI can see a small performance penalty for parsing the chunk size,\nbut this is hex (not decimal) so it's actually cheaper than parsing\ncontent-length, and much cheaper than parsing the silly HTTP-date format.\nAnd it's likely that any self-delimiting transfer coding would have\nnearly as much added overhead.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Paul Leach wrote:\n\n> \n> \n> > ----------\n> > From: David W. Morris[SMTP:dwm@xpasc.com]\n> > Sent: Friday, December 12, 1997 2:34 PM\n> > > \n> > > If a message is sent on a persistent connection using\n> > > a transfer-coding that does not exactly preserve the\n> > > length of the data being encoding, then the \"chunked\"\n> > > transfer-coding MUST be used, and MUST be the last\n> > > transfer-coding applied.\n> > \n> > Sounds like a complete solution to me.  Of course, I think there\n> > might still be a few words about content-length to bring into\n> > alignment.\n> > \n> There's a problem -- if no one implements any transfer coding other than\n> identity or chunked, then we don't have the necessary two implementations to\n> go to Draft.  If they do, then I'll be they don't follow this rule -- they\n> probably believe that Content-length is the length of the message body, not\n\nI would take that bet ... we've had multiple implementors report that\nthey thought 'content-length' was entity length, not message length.\nNONE who reported the converse.\n\n\n> the entity-body.\n> \n> I also don't like having to impose chunked when it isn't needed. If a cache\n> recieves a .txt file, and gzips it for later use in serving it to clients,\n> it perfectly well knows the length, and can send it out with a TE of gzip\n> and a Content-length (or Transfer-length, if we want to introduce that and\n> get it implemented twice).\n\nCan't speak to implementations ...\n\nbut imposing chunked is an almost ZERO overhead operation.  NO MORE\noverhead than adding a transfer-length .... probably less:\n\n     3039[crlf]\n     encoded message content\n     [crlf]\n     0[crlf]\n\nNothing about chunked encoding requires more than a single chunk.\n\nThis also cleanly covers the case where the encoded length (as \nin compressed) is unknown UNTIL after encoding is complete. Just\nuse multiple chunks.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "On 12 Dec 1997, Foteos Macrides wrote:\n> \n> >> What does \"last symbolic element in the path field of the Request-URI\"\n> >> mean?\n> >\n> >Maybe an example is best. Assume the Request-URI is\n> >\"http://somewhere/the/path/index.html\" then you want to to talk about all\n> >URIs with a prefix of \"http://somewhere/the/path/\", i.e. the scheme, the\n> >site component and the path component of the Request-URI minus any trailing\n> >segment. I assumed \"symbolic element\" to refer to \"/\".\n> \n> Your example is correct, but \"symbolic elements\" refers to\n> the \"substrings in a URL path that comprise a hierarchy delimited\n> by slashes\".\n[snip]\n\nThanx for the (very clear) explanation. I suppose one could just substitute\n\"symbolic element\" by \"segment\".\n\n> I'm not sure how well that would work in conjunction with\n> nounce handing, and other aspects of Digest authentication which\n> are more complicated than Basic, but agree with you that most\n> implementors would like it to be as \"drop in\" as possible with\n> respect to existing Basic implementations.\n\nThe nonce is not (that much of) a problem. If you previously got a\nnextnonce then use that, otherwise just use the previously used nonce.\nWhen pre-emptively sending auth info the main difference between Basic\nand Digest authentication is that you have to recalculate the response\ndigest (and possibly entity-digest) and adjust the various parameters,\ninstead of just sending exact same header as you can do in Basic. So\nthe prefix idea works just fine with Digest.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: This is not &quot;this is not a date&quot",
            "content": ">I have a suggestion: instead of coming up with a new syntax\n>for \"this is not a date\" (when the existing date syntax, horribly\n>inefficient and hard-to-parse as it is, has at least more or\n>less been tamed by implementors) perhaps we can get by with\n>something a little simpler.\n>\n>For example, we could say\n>\n>If the sender is required to send a Date header\n>by some part of this specification, and is unable\n>to generate a current HTTP-date value, it SHOULD\n>send a legal HTTP-date value that is provably in\n>the past.\n>\n>For example, send\n>Date: Thu, 01 Jan 1970 00:0:01 GMT\n>\n>As far as I can tell, this will not lead to any more trouble\n>than simply omitting the Date header.  It also seems to be\n>legal according to section 14.19 of -rev-01.\n\nUmmm, we already have a syntax for \"this is not a date\"  -- the lack\nof a Date header field.  Why should we send garbage on the wire?\nI think we should just change the specification so that it properly\nhandles the case where there is no Date header field.  That would be\nthe more robust way of specifying it in any case.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 97, Jeffrey Mogul wrote:\n\n> It seems like a mistake to get into the business of specifying\n> self-delimiting transfer codings (aside from chunked, which is\n> a generic way to do that).  This way, we have some modularity\n> in the protocol design.  I.e., we have only three ways to find\n> the end of a message (EOF, Content-Length, chunked); why add\n> more?\n\nI agree 100%. I'd like to see the lowest level of handling (determining\nthe end of a response) kept as simple as possible.\n\n\n  Cheers,\n\n  Ronald\n\n\nP.S. There are _four_ ways to determine eom: multipart/byteranges is\n     also one.\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "On Fri, 12 Dec 1997, Jeffrey Mogul wrote:\n\n> John Franks wrote:\n>     This raises the whole issue of stacked transfer encodings.  \n>     Are you suggesting that arbitrary stackings be allowed, or\n>     just two, the second of which is chunked?\n> \n> I'm not \"suggesting\" anything.  It's already in the spec, and\n> has been there since at least RFC2068:\n> \n> 14.40 Transfer-Encoding\n> \n>    The Transfer-Encoding general-header field indicates what (if any)\n>    type of transformation has been applied to the message body in order\n>    to safely transfer it between the sender and the recipient. This\n>    differs from the Content-Encoding in that the transfer coding is a\n>    property of the message, not of the entity.\n> \n>           Transfer-Encoding     \n>   = \"Transfer-Encoding\" \":\" 1#transfer-coding\n> \n> the BNF clearly allows any number of transfer-codings.\n> \n> The current -rev-01 draft adds:\n>        If multiple encodings have been applied to an entity, the transfer\n>        codings MUST be listed in the order in which they were applied.\n> \n\nInteresting.  I wonder how many current implementations can handle\n\nTransfer-Encoding: chunked, chunked, chunked\n\nI know mine can't.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Friday, December 12, 1997 8:17 AM\n> \n> > 2) I'm not very familiar with the details of these encodings, but I\n> >    believe they aren't self delimiting. Is this true?\n> > \n> \n> I am not sure.  But from the recent discussion it is very clear\n> that it is cruicially important that all transfer encodings be\n> explicitly defined to be self-delimiting or non-self-delimiting.\n> I am not sure this is done in the IANA registry.\n> \nMumble. I'd guess that they aren't all self-delimiting. If so, and if\nthey're implemented (likely, otherwise, how could they make it into the\nDraft Standard), then that means that Content-Length is de-facto the length\nof the message body.\n\nThe same argument applies, by the way, the Transfer-Length -- if we add it,\nthere won't be any implementations, so we'd have to recycle at Proposed (OK\nby me, but not necessarily to others).\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "> ----------\n> From:\n> Eric_Houston/CAM/Lotus@lotus.com[SMTP:Eric_Houston/CAM/Lotus@lotus.com]\n> \n> Two new refinements that I would like to make:\n> \n>  1) When the content server redirects the request to the authentication\n> server, it encrypts the ACL for the protected resource.  The\n> authentication\n> server then validates the user against the (decrypted) ACL and returns the\n> first matching entry to be cached in the browser.  When the browser is\n> queried for user credentials, the encrypted (authenticated) group\n> affiliations are returned to the content server.\n> \nYou may do this if you want, but I'm not planning to spec the content server\n(CS) to authentication server *AS) protocol as part of the proposal. I'll\ngive a sample one, perhaps in a separate draft, to make sure that one is\npossible.\n\n(Personally, I don't see why the content server can't evaluate the ACL\nitself. But that just proves that if we do try to spec the CS<->AS protocol,\nit'll slow down the client->server protocol finalization.)\n\n> 2) Could re-directed authentication be layered on top of the existing\n> schemes so that it could be used with basic, digest, and X.509?\n> \nRe-directed authentication is totally transparent to the client, so talking\nabout \"on top of existing schemes\" is not meaningful.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "An alternate proposal, which I believe is simpler and requires less\nmodification:\n\nContent-Length, if validly present, is the length of the entity-body (which\nis the message-body after transfer codings are removed). It is also the\nlength of the message-body if no transfer-coding is used.\nContent-Length MUST NOT be present if a transfer coding is used. If it is\npresent in such cases, it is invalid, and the robustness principle says it\nshould be ignored.\n\nIf we want, we can provide for the future existence of non-self-delimited\ntransfer codings by adding Transfer-length:\n\nTransfer-Length, if validly present, is the length of the message-body.\nTransfer-Length MUST NOT be present on self-delimiting transfer codings. If\nit is present in this case, it is invalid, and the robustness principle says\nit should be ignored.\n\nUnder these rules, Content-Length is still logically end-to-end -- the\nheader may not physically be present, but its value if it is ever present is\nwell-defined end-to-end and the same end-to-end.\n\nNote that the definition of Content-length is independent of the existence\nof Transfer-length, and we could omit it.\n\nUnder these rules, proxies may delete Content-length, as long as the add\nTransfer-length or a self-delimited transfer coding. So, we remove it from\nthe list of \"must not modify\" headers.\n\nDigest Auth should drop all reference to Content-Length and replaces it with\n\"length of entity-body\", which is always well-defined and always\ndeterminable given the above rules.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": "> ----------\n> From: jg@pa.dec.com[SMTP:jg@pa.dec.com]\n> Sent: Wednesday, December 10, 1997 4:48 PM\n> \n<snip>\n\n> I think you are confused....  In Rev-01, only an origin server is allowed\n> to generate a 305 response.  It is authoritative for that resource, so\n> the spoofing problems don't come up (and is the reason for that text being\n> in the document...)\n> \nAnd exactly how can the browser tell that it was the origin server that sent\nthe 305? And not the untrustworthy proxy in between the client and the\nserver?\n\nI know that normally one trusts one's proxy, but since security issues are\nbeing raised here, the question needs to be asked.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Friday, December 12, 1997 3:41 PM\n> \n<snip>\n\n> > When Paul writes:\n> >     Under these rules, Content-Length is still logically end-to-end --\n> >     the header may not physically be present, but its value if it is\n> >     ever present is well-defined end-to-end and the same end-to-end.\n> > I'm not sure it makes sense to talk about a header being \"end-to-end\"\n> > if it isn't actually transmitted on some hops.\n> \n> What I would most like to see is the assertion that if a Content-length\n> header is present its value is the length of the entity-body.\n> \nThat cat may be out of the bag, if anyone has implemented any TEs other than\nchunked or identity. They will almost for sure have used Content-length as\nthe length of the message-body.\n\n>  It may not be too important that it be officially end-to-end.\n> \nI don't care either. I only care that \"length of entity-body\" be\nwell-defined and determinable at each hop.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> ----------\n> From: David W. Morris[SMTP:dwm@xpasc.com]\n> Sent: Friday, December 12, 1997 2:34 PM\n> > \n> > If a message is sent on a persistent connection using\n> > a transfer-coding that does not exactly preserve the\n> > length of the data being encoding, then the \"chunked\"\n> > transfer-coding MUST be used, and MUST be the last\n> > transfer-coding applied.\n> \n> Sounds like a complete solution to me.  Of course, I think there\n> might still be a few words about content-length to bring into\n> alignment.\n> \nThere's a problem -- if no one implements any transfer coding other than\nidentity or chunked, then we don't have the necessary two implementations to\ngo to Draft.  If they do, then I'll be they don't follow this rule -- they\nprobably believe that Content-length is the length of the message body, not\nthe entity-body.\n\nI also don't like having to impose chunked when it isn't needed. If a cache\nrecieves a .txt file, and gzips it for later use in serving it to clients,\nit perfectly well knows the length, and can send it out with a TE of gzip\nand a Content-length (or Transfer-length, if we want to introduce that and\nget it implemented twice).\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: This is not &quot;this is not a date&quot",
            "content": "There's another solution -- declare that proxies can't add \"Date\" when\ndigest is in use, just like we say that they can't change the content\ncoding. We could allow \"Date\" to be in the trailer, so that if it really\nwants to add it it can. But that seems overkill -- the question really is:\nwhy do we have to allow Date to be added by a proxy?\n\nLet me give some background:\n\nFundamentally, good crypto practice says that EVERYTHING should be in the\ndigest. It is almost impossible to figure out every possible attack that\nsomeone might make by being able to modify an undigested field, so the only\nsafe thing to do is digest them all.\n\nFor HTTP that proved to be infeasible. Some fields really have to be\nmodified by proxies. (Those could still be included in the Proxy-Auth,\nthough... I hadn't thought of that, because the proxy auth was added\nlater... but anyway...) The fields that _really_ have to be modifed can't be\nin the digest.  I see no compelling reason for L-M or Expires to be changed\nby a proxy, and it's plausible that severe service degradation (forcing lots\nof cache misses) could be caused by an attacker changing them. Or that an\nattacker could feed you an old response with recent L-M. Maybe these aren't\nactually problems, but it's a lot of work to validate that they aren't, and\nwe haven't demonstrated harm by including them. So, I see no reason to\nremove them from the digest.\n\nThe only problem I can see is that an existing proxy that doesn't understand\nDigest and add Date will break digest. But so will an existing proxy that\ndoesn't understand Digest and reformats Date without changing the value. I\ndon't see either as a showstopper.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: This is not &quot;this is not a date&quot",
            "content": "On Fri, 12 Dec 1997, Paul Leach wrote:\n\n> For HTTP that proved to be infeasible. Some fields really have to be\n> modified by proxies. (Those could still be included in the Proxy-Auth,\n> though... I hadn't thought of that, because the proxy auth was added\n> later... but anyway...) The fields that _really_ have to be modifed can't be\n> in the digest.  I see no compelling reason for L-M or Expires to be changed\n\nI've not done enough homework to be sure this comment makes sense, but \nit is reasonable for a document to expire, be revalidated and have a new\nexpiration applied. If the proxy can't merge in a new expires header then\neither a new digest value or whole new copy of the entity would be\nrequired.\n\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: LYNXDEV two curiosities from IETF HTTP session",
            "content": "It is still an attack as the origin server, if it has not been\nauthenticated, is just some random server. To remind folks of the problems\nwith click tracking and cookies, a bunch of servers could choose to have\nrequests to them redirected to indicated proxies where advertising and other\ninformation will be inserted as needed. This very effectively gets around\ncookie issues.\nYaron\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Wednesday, December 10, 1997 7:48 PM\n> To:Yaron Goland\n> Cc:jg@pa.dec.com; Josh Cohen; Foteos Macrides; lynx-dev@sig.net;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:RE: LYNX-DEV two curiosities from IETF HTTP session.\n> \n> \n> >  From: Yaron Goland <yarong@microsoft.com>\n> >  Date: Wed, 10 Dec 1997 11:21:51 -0800\n> >  To: \"'jg@pa.dec.com'\" <jg@pa.dec.com>, Josh Cohen\n> <joshco@microsoft.com>\n> >  Cc: Foteos Macrides <MACRIDES@SCI.WFBR.EDU>, lynx-dev@sig.net,\n> >          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> >  Subject: RE: LYNX-DEV two curiosities from IETF HTTP session.\n> >  \n> >  I doubt any commercial browser will implement 305 without some very\n> serious\n> >  security provided to assure that the proxy asking for the one time\n> redirect\n> >  is going to get it. I would suggest that this problem needs to be dealt\n> with\n> >  in the large 305/306 context, in a stand alone spec, and that the draft\n> >  standard for HTTP should simply state that 305 has been deprecated and\n> >  SHOULD NOT be implemented.\n> >  \n> >  Yaron\n> \n> I think you are confused....  In Rev-01, only an origin server is allowed\n> to generate a 305 response.  It is authoritative for that resource, so\n> the spoofing problems don't come up (and is the reason for that text being\n> in the document...)\n> - Jim\n\n\n\n"
        },
        {
            "subject": "RE: This is not &quot;this is not a date&quot",
            "content": "> -----Original Message-----\n> From:Scott Lawrence [SMTP:lawrence@agranat.com]\n> Sent:Friday, December 12, 1997 9:00 AM\n> To:HTTP Working Group\n> Subject:Re: This is not \"this is not a date\"\n> \n>   We have the same problem with Date and Expires; proxies may add them\n>   (section 13.5.2) even when the origin server used an empty value\n>   when the digest was generated.  As I see it, we have four\n>   alternatives:\n> \n[Joshua Cohen]  Im confused.\nIf the purpose of this discussion is simply to have a date\nplaceholder,\nwhat are the effects of using a token marker date, such as the \nepoch beginning , ie 0 unix time.\n\nI imagine that existing 1.0 proxies will be using last-modified to\ndetermine\nthe age of an entity, not date:.\nPresently, many proxies will not cache an item without a L-M header\nor expires:.  \n\nWe cant fix any old 1.0 proxies that are deployed today, but I cant\nimagine how they will be worse off in this case.\nFor 1.1, we can define the epoch time as a value that\nmeans \"I have no idea what time it is\".\nNewer 1.1 proxies which implement the new age-calculation formula\nshould be aware of the null date value.\n\nAs far as a proxy adding a date: header...\nAssuming the proxy adds a date header from its own clock on a\nresponse,\nand there are nested proxies, will each successive proxy be\nincorrectly\ncalculating the age? \nsomeone tell me if Im wrong, but we are saying:\n\"Age = age of the locally cached entity\" NOT \"Age= age of the entity\nin question ( since it was created/modified)\"\nright?\n\nJosh Cohen\nObjfun: We should keep dating to a minimum in the wg, \"This is not a\ndate\", we're just friends :)\n  \n>  \n\n\n\n"
        },
        {
            "subject": "RE: HTTP/1.1-TCP interactions (was Re: HTTP Connection Management(draft-ietf-http-connection00.txt)",
            "content": ">HTTP W-G should shout loudly at the TCP Implementation W-G to get best\n>practice for idle connections documented given the majority holding of HTTP\n>on Internet traffic.\n\n>Having a standard doesn't mean selfish implementers have to keep to it, but\n>it's better than not having a standard.\n\nPerhaps its worth noting that one of the major browser vendors (Microsoft)\nis now a major ISP (MSN). \n\nThis may well be a usefull observation with respect to getting ISPs to turn\non RED routing. \n\nPhill\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Wednesday, December 10, 1997 8:23 PM\n> To:John Franks\n> Cc:Roy T. Fielding; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: What is Content-Length? \n> \n> On Wed, 10 Dec 1997, John Franks wrote:\n> \n> Exactly HOW would a server know transfer-length before sending\n> data? I can define a reasonable use of content-length in the\n> trailer of a chunk-encoded transfer ... since content length is\n> the entity length, it could serve as a double check of the\n> receipt of the chunk-encoded entity.  But clearly, transfer\n> length couldn't appear in the trailer as the length wouldn't\n> be known until after the trailer was complete.  It makes no\n> sense to me for a server which knows the length of the transfer\n> encoded entity to ever use transfer encoding.\n> \n> Sounds like protocol cruft to me.  What am I missing?\n> \n[Joshua Cohen]  \nI didnt realize that we intended all future TEs to be used for\nunknown length\nentities.  It seems logical that someone might use TE for\nencryption, which\ncould still have a known length at transmission time.\n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "Re: This is not &quot;this is not a date&quot",
            "content": ">>>>> \"DWM\" == David W Morris <dwm@xpasc.com> writes:\n\nDWM> On Fri, 12 Dec 1997, Paul Leach wrote:\n\n>> For HTTP that proved to be infeasible. Some fields really have to be\n>> modified by proxies. (Those could still be included in the Proxy-Auth,\n\nDWM> I've not done enough homework to be sure this comment makes sense, but\nDWM> it is reasonable for a document to expire, be revalidated and have a new\nDWM> expiration applied. If the proxy can't merge in a new expires header then\nDWM> either a new digest value or whole new copy of the entity would be\nDWM> required.\n\n  I think that we don't need to worry about the case of preserving\n  cachability in shared caches of a digest-authenticated response;\n  because of the nonce it just can't be done (and I don't think that\n  we would want that changed - as an origin server I can't trust the\n  proxy anyway).\n\n  The goal is to allow the athenticated message to pass through the\n  proxy uncorrupted - that's all.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": ">  1) When the content server redirects the request to the authentication\n> server, it encrypts the ACL for the protected resource.  The authentication\n> server then validates the user against the (decrypted) ACL and returns the\n> first matching entry to be cached in the browser.  When the browser is\n> queried for user credentials, the encrypted (authenticated) group\n> affiliations are returned to the content server.\n> \n\nSince there are no standardized ACLs, I don't think this can be\naddressed in the HTTP spec. Or did I miss the part where ACLs were\nadded to HTTP?\nMez\n\n\n\n"
        },
        {
            "subject": "Customizing the authentication dialo",
            "content": "Could the spec allow for customization of the authentication dialog?\nJust wondering,\n-e\n\n\n\n"
        },
        {
            "subject": "Re: Customizing the authentication dialo",
            "content": "> Could the spec allow for customization of the authentication dialog?\n\n  The only customization allowed for is the value of the realm, which\n  should be displayed to the user (if any) if challenging for the\n  credentials.  In thinking about customizing this, bear in mind that some\n  clients will not be browsers and will not have human users.\n\n\n\n"
        },
        {
            "subject": "RE: This is not &quot;this is not a date&quot",
            "content": "> ----------\n> From: David W. Morris[SMTP:dwm@xpasc.com]\n> Sent: Saturday, December 13, 1997 9:06 PM\n> \n> I've not done enough homework to be sure this comment makes sense, but \n> it is reasonable for a document to expire, be revalidated and have a new\n> expiration applied. If the proxy can't merge in a new expires header then\n> either a new digest value or whole new copy of the entity would be\n> required.\n> \nIndeed, a proxy can not just \"merge in\" a new Expires header and have the\ndigest check. So your conclusion is true -- it needs to get at least a new\ndigest that corresponds to the changed Expires header value, and maybe a\nwhole new copy of the entity.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Customizing the authentication dialo",
            "content": "Scott Lawrence wrote:\n> \n> > Could the spec allow for customization of the authentication dialog?\n> \n>   The only customization allowed for is the value of the realm, which\n>   should be displayed to the user (if any) if challenging for the\n>   credentials.  In thinking about customizing this, bear in mind that some\n>   clients will not be browsers and will not have human users.\n\nFWIW, ages ago I asked for (and was denied) the addition of a \"prompt\"\nattribute, which would have been (one of) the thing the user saw in the\ndialog box.  The argument against at the time was, I think, that such an\nattribute could be used by a malicious server to fool the user into\ngiving credentials for a spoofed authentication domain.\n\nNotwithstanding that valid criticism, I still think a \"prompt\" attribute\ncould be useful.  In one application I wrote, users have to register\nbefore they can gain access to \"protected\" documents.  The project, and\nhence the realm, is \"SEPTEMBER\".  But to remind users that they have to\nregister first, I had to make the HTTP realm attribute be \"SEPTEMBER\n(You must have registered)\", so browsers would present that string, and\nusers would get the useful hint.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "the state of Stat",
            "content": "At the IETF meeting I promised to send out a summary of where things\nstand and where things ought to go to nail down a revised HTTP State\nManagement.  This is it.\n\nPlease keep the discussion on http-state@lists.research.bell-labs.com.\n(See <http://www.bell-labs.com/mailing-lists/http-state/>.)  I'm sending\nthis message to http-wg only as a courtesy.\n\nStep 1:  Nail down the \"wire protocol\".\n\nAt the IETF meeting I believe we narrowed down the outstanding\nissue(s) to the domain-matching rules.  There are two\nsub-problems:  a) flat namespaces in intranets; and b) using\nthe n-dot rule to restrict the set of servers to which a cookie\ncan be returned.\n\nI believe we actually resolved the flat namespace problem.  The\nresolution is that, if the Domain attribute is omitted in\nSet-Cookie2, the client is restricted to return the cookie only\nto the server from which it came.  That rule applies regardless\nof the name (and how many dots it contains).\n\nThat leaves the domain-matching restriction rules to address.\n\nStep 2:  reach consensus on the \"privacy rules\"\n\nAttendees thought they could arrive at an agreement about the\ninteraction of the (previous) privacy rules and the user\ninterface.\n\nThe rules for unverifiable transactions, or at least the\ndefault user agent setting for them, remain contentious.\n\n\nFor now I would like to restrict the discussion \"agenda bashing\" for\nStep 1 only.  Does anyone believe there are other outstanding issues?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "(Fwd) webpriv list set u",
            "content": "--- Forwarded mail from April Marine <amarine@nic.nasa.gov>\n\nDate: Mon, 15 Dec 1997 14:22:21 -0800 (PST)\nFrom: April Marine <amarine@nic.nasa.gov>\nSubject: web-priv list set up\n\nHi all,\n\nThis is to let you know that the mailing list for the Web Privacy group\n(not working group) is set up.  It is web-priv@nasa.gov.  To join, send to\nweb-priv-request@nasa.gov with 'subscribe' as the body of the msg.\n\nFor those of you unable to make Washington or who did not attend the\nweb-priv BOF:   I will append the original description of the BOF at the\nend of this msg.  Larry Masinter chaired the BOF, which was convened to\nlook at concerns related to the privacy of user information collected via\nthe Web and whether there should be a group to develop some guidelines or\nrecommendations for site administrators about better ways to collect or\nhandle such information.  (I also think there may be some work aimed at\nend-users alerting them to privacy \"threats.\")\n\nThe BOF was very well attended.  There was a lot of interest in the topic,\nbut there was not enough time (in my opinion) to flesh out what tasks a\ngroup would actually take on.  Therefore, not surprisingly, there was a\ndearth of volunteers to do actual work.  However, many people felt work\nshould be done.  So, there was no working group formed as a result of the\nBOF, but this mailing list has been set up to continue the discussion and\nhopefully make it more concrete.  Personally, I think that as the work\ntopics are clarified, people will be willing to pitch in, but this list is\na test of that theory!\n\nI am sending this msg to the uswg list.  The HTTP list is also being\nalerted.  Feel free to let anyone interested in this area know about the\nlist and encourage them to join.  Soon, the minutes from the BOF will be\nposted here (Larry, can you make sure of that?  I'm not sure who took\nminutes at the BOF) and we can start from there.\n\nActually, as this msg got a bit long, I will send Larry's original\ndescription as a separate msg.\n\nPlease join in the discussion!\n\nthanks,\nApril Marine\nNASA Network Information Center\n\n\n\n\n---End of forwarded mail from April Marine <amarine@nic.nasa.gov>\n\n\n\n"
        },
        {
            "subject": "Re: Customizing the authentication dialo",
            "content": "On Mon, 15 Dec 1997, Dave Kristol wrote:\n\n> Scott Lawrence wrote:\n> > \n> > > Could the spec allow for customization of the authentication dialog?\n> > \n> >   The only customization allowed for is the value of the realm, which\n> >   should be displayed to the user (if any) if challenging for the\n> >   credentials.  In thinking about customizing this, bear in mind that some\n> >   clients will not be browsers and will not have human users.\n> \n> FWIW, ages ago I asked for (and was denied) the addition of a \"prompt\"\n> attribute, which would have been (one of) the thing the user saw in the\n> dialog box.  The argument against at the time was, I think, that such an\n> attribute could be used by a malicious server to fool the user into\n> giving credentials for a spoofed authentication domain.\n> \n> Notwithstanding that valid criticism, I still think a \"prompt\" attribute\n> could be useful.  In one application I wrote, users have to register\n> before they can gain access to \"protected\" documents.  The project, and\n> hence the realm, is \"SEPTEMBER\".  But to remind users that they have to\n> register first, I had to make the HTTP realm attribute be \"SEPTEMBER\n> (You must have registered)\", so browsers would present that string, and\n> users would get the useful hint.\n\nI agree, I think quite a few web applications end up doing their own\nauthentication simply because the default prompt is unfriendly.\n\nSpecification of a prompt doesn't need to mean replacement of the\nexisting prompt. Perhaps rather than prompt, what could be specified\nwould be a comment to be included in the login dialog box. By calling\nit a comment (or realm description) and requiring continued presentation\nof the server name and realm I think there is no valid criticism.\n\nSince the REALM still governs the process, non-human clients would\nhave no conflict.\n\nIt would also be very useful if user agents would allow the user to\nreview the body associated with the 401 response WITHOUT canceling\nthe authentication prompt.\n\nIn any case, this seems well out of bounds for what we can consider\nfor HTTP/1.1 ... or does the apparent undocking of authentication\nleave more wiggle room?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "BTW, a hallway conversation today with Fred Baker indicates there may\nbe serious security problems with serverloc....\n\nNow I just have to try to button hole Jeff Schiller to understand if this\nwill make serverloc useless for this application, or whether serverloc is\nlikely to get fixed in finite time.  I'll try to catch Jeff Tuesday....\n- jim\n\n\n\n"
        },
        {
            "subject": "Digest mes",
            "content": "Where is that \"eject\" button when you need one?\n\nI think the time has come to declare Digest as dead.  Dead because\nit was stuffed with featuritis in an attempt to solve multiple\nauthentication and security concerns instead of the only one for which\nit was originally designed.  The current definition seems to have lost\nall of the advantages it had over transport-level encryption, without\nany of the additional security/privacy attributes of TLS/SSL, and has\nbecome just as difficult to implement.  Now it doesn't even support\nnon-shared caching.\n\n>Fundamentally, good crypto practice says that EVERYTHING should be in the\n>digest. It is almost impossible to figure out every possible attack that\n>someone might make by being able to modify an undigested field, so the only\n>safe thing to do is digest them all.\n\nGood crypto practice usually involves well-defined encapsulation of what\nis being digested/signed, preferably at multiple levels so that the\nsignature can be signed as well.  There are many ways to do this effectively,\nand several proposed standards already exist for doing it with MIME\nmultiparts.  Why not separate these features (which nobody currently\nuses because no UA supports it) from the comparatively simple act\nof exchanging authentication credentials?\n\nIt's no surprise that Digest doesn't do this well, since originally \nit was not intended to do it at all.\n\n>For HTTP that proved to be infeasible. Some fields really have to be\n>modified by proxies. (Those could still be included in the Proxy-Auth,\n>though... I hadn't thought of that, because the proxy auth was added\n>later... but anyway...) The fields that _really_ have to be modifed can't be\n>in the digest.  I see no compelling reason for L-M or Expires to be changed\n>by a proxy, and it's plausible that severe service degradation (forcing lots\n>of cache misses) could be caused by an attacker changing them. Or that an\n>attacker could feed you an old response with recent L-M. Maybe these aren't\n>actually problems, but it's a lot of work to validate that they aren't, and\n>we haven't demonstrated harm by including them. So, I see no reason to\n>remove them from the digest.\n\nIf you don't separate message-level metadata (like Date) from what you\nare digesting, then the mechanism has broken its transfer-independence\nquality.  Losing that quality means that Digest has no advantages over\njust using TLS (SSL, whatever).\n\nService degradation is not an issue -- that can be accomplished more\neffectively by changing the digested part so that it no longer matches\nthe digest and thus force a reset of the whole process.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "HTT",
            "content": "Can anyone tell me how to get a complete and latest draft of HTTP client\nand server implmentation\n\nThank you\n\nPatrick Sim\nEmail  : patrick.sim@pacific.net.sg\nTel     : (714)  531-9638\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "> The current definition seems to have lost\n> all of the advantages it had over transport-level encryption,\n\nExcept that it doesn't require encryption.\n\nThis is critically important. Don't go off half-cocked.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: Digest mes",
            "content": "Cant you run SSL on TLS in \"null cipher\" mode,\nwhere you just do authentication, not encryption ?\nI know you can do this with ssl, Im imagining\nthat you can do it in TLS as well..\n\nAny SSL experts?\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Tuesday, December 16, 1997 12:04 AM\n> To:Roy T. Fielding\n> Cc:HTTP Working Group\n> Subject:Re: Digest mess\n> \n> > The current definition seems to have lost\n> > all of the advantages it had over transport-level encryption,\n> \n> Except that it doesn't require encryption.\n> \n> This is critically important. Don't go off half-cocked.\n> \n> Larry\n> -- \n> http://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": ">> The current definition seems to have lost\n>> all of the advantages it had over transport-level encryption,\n>\n>Except that it doesn't require encryption.\n\nSorry, I meant transport-level security (TLS).\n\n>This is critically important. Don't go off half-cocked.\n\nI agree it is critically important.  It was critically important \nthree years ago.  That doesn't change the fact that it is dead.\nWe keep fiddling with a dead mechanism as if adding one more feature\nwill suddenly create the installed base necessary for its deployment.\nWe can't deploy the thing because it is too complicated to require\nof all HTTP clients, and yet there is no negotiation framework for\nthe client to signal its ability to handle the mechanism other than\nthe HTTP-version.\n\nWe (or somebody) should be working on a way to do this kind of thing\nin a clean manner, without reliance on aspects of HTTP over which the\nauthentication process has no control, and in a manner that is separate\nfrom the very orthogonal concept of digest authentication.  The stupid\nthing is that we could have required Digest two years ago if it hadn't\npicked up all this other cruft.\n\nAnd the first person who says \"You can do that in PEP\" had better\nbe prepared to prove it (in some future WG to be named later).\n\n....Roy  [feeling particularly grumpy this morning, sorry]\n\n\n\n"
        },
        {
            "subject": "doub",
            "content": "     Hi,\n      I am sorry if this is not the right mailing list to ask this doubt,\n      but I hope someone can help me out with this.\n\n      RFC2068 for HTTP/1.1 specifies Accept-Ranges to be a response header\n      field in Section 14.5. But there is no mention of this field in\n      the BNF for response header fields in Section 6.2.\n\n      Accept-Ranges seems to have been grouped along with the other Accept*\n      fields of the request-header, but does not find a place in the BNF\n      for request-header fields also. Is it an extension header?\n\n      Could anybody please clarify this?\n\n      Thanks in advance,\n      Ramya T.V.\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "Jeffrey Mogul:\n>\n>    > If a message is sent on a persistent connection using\n>    > a transfer-coding that does not exactly preserve the\n>    > length of the data being encoding, then the \"chunked\"\n>    > transfer-coding MUST be used, and MUST be the last\n>    > transfer-coding applied.\n>    > \n>    \n>    Is there a reason to require that chunked be applied after a\n>    self-delimiting transfer encoding?  There would be a (probably\n>    slight)  performance penality for doing it and I don't see the\n>    purpose.\n>\n>It seems like a mistake to get into the business of specifying\n>self-delimiting transfer codings (aside from chunked, which is\n>a generic way to do that).\n\nI agree, but requiring chunked on top will get us in the much nastier\nbusiness of forbidding self-delimiting transfer codings specified by\nothers.\n\nIt should be legal to use something like gzip as the top transfer\nencoding.  If a server has to put chunking on top of a gzipped stream\nwithout knowing the size of the zipped data beforehand, this could be\nquite expensive in terms of either memory copy operations or software\ncomplexity.  The same is true for decoding such a thing.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Roy T. Fielding wrote:\n> \n> >> The current definition seems to have lost\n> >> all of the advantages it had over transport-level encryption,\n> >\n> >Except that it doesn't require encryption.\n> \n> Sorry, I meant transport-level security (TLS).\n\nTLS requires encryption.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "httpext wg mailing list read",
            "content": "As discussed at the IETF meeting last week in Washington DC, we now have a\nmailing list for the extensions group:\n\n<ietf-http-ext@w3.org>\n\nWG Chair is Josh Cohen, Editor is Scott Lawrence.\n\nTo subscribe, send a mail containing the following:\n\nTo: ietf-http-ext-request@w3.org\nSubject: subscribe\n\nSee furthermore\n\nhttp://www.w3.org/Mail/Request.html\n\nfor more information on how to unsubscribe etc.\n\nNOTE: Nobody has been subscribed yet - if you are interested in this\ngroup's work then you must subscribe yourself.\n\nThe mailing list is archived at\n\nhttp://lists.w3.org/Archives/Public/ietf-http-ext/\n\nThe home page for the working group is\n\nhttp://www.w3.org/Protocols/HTTP/ietf-ext-wg/\n\n(this is still under construction).\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": ">>> RF == \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n\nRF> I think the time has come to declare Digest as dead.  Dead because\nRF> it was stuffed with featuritis in an attempt to solve multiple\nRF> authentication and security concerns instead of the only one for which\nRF> it was originally designed.\n\n  Whoa; we have, for the first time, gotten a major client to say that\n  they may implement it and we have several protocols being layered on\n  HTTP that are specifying it as required.  There are definitly\n  problems, but let's not get frustrated by them, let's solve them.\n\n  I share your concern with respect to feature creap, and will go on\n  the record now as opposed to _any_ change that does not directly\n  serve the authentication goal.  Applications are using Basic today\n  without any prompts; they live without it with digest.  If an\n  application really wants to display explanations or reminders, they\n  can have an entire unauthenticated page to get it - it is not\n  required to prevent passwords being sent in the clear on the\n  Internet; let's not loose sight of that goal.\n\n  However, some entity digest capability is absolutely required\n  because it is the message we are authenticating - it does no good to\n  send credentials if the entity is not tied to them.\n\nPL> Fundamentally, good crypto practice says that EVERYTHING should be in the\nPL> digest. It is almost impossible to figure out every possible attack that\nPL> someone might make by being able to modify an undigested field, so the only\nPL> safe thing to do is digest them all.\n\n  But we also face the backward compatibility problem - we know\n  for sure that the current scheme is broken in the face of proxies.\n\nRF> Service degradation is not an issue -- that can be accomplished more\nRF> effectively by changing the digested part so that it no longer matches\nRF> the digest and thus force a reset of the whole process.\n\n  I agree that service degradation and cache efficiency should not be\n  our concern here - what is desperatly needed is a lightweight\n  mechanism for authentication, and if that means that caches can't\n  cache, well so be it.\n\n  I posted a proposal last Friday\n\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0390.html\n\n  to simplify the entity digest calculation by removing all the\n  metadata headers; should I rewrite this in the form of an Internet\n  Draft, or can we just debate it and possible other solutions here?\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "I talked to Jeff Schiller (IETF area director for security)\nabout the serverloc security problems....\n\nThe problem is that anyone, anywhere, can advertize the service, and \npotentially arrange to get a client to use a service (in this case,\na proxy) of the attacker's choosing.\n\nThe security compromise for serverloc to go to proposed standard is to\nallow one, via public key crypto, verify the new advertisements.\n\nThe problem from our point of view is that the solution to serverloc's \nsecurity problem reduces to the previously unsolved problem: having to \ndistribute different keys to different browsers, which isn't better than \nwhat we have now, having to distribute the first proxy configuration \ninformation to the browser. A solution to this might be to use DHCP to \ndistribute the keys required, but is not yet specified.\n\nNow, DHCP has its own set of security problems, but I note that people\nare already trusting it in the first place to get their IP addresses (for the\nmass market).\n\nSo without being expert at either protocol, it sure sounded like in\nthe short term a DHCP based solution might be best, though in the longer\nterm, something that is more dynamic than just information acquired\nat boot time would be a better solution (so that when a browser\ngets restarted, you can pick up the current proxy, or fail-over if a failure\nis detected while talking to a proxy.  And in the long term, this may\nnot be either/or.\n\nHope this helps discussions....\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "** Reply to note from \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> Tue, 16 Dec 1997 00:40:30 -0800\n\nAs a server implementer, let me point out that our problem with\nimplementing it has little, if anything, to do with the complexity of\nthe protocol or featuritis. \n\nThe basic (err, pun intended) problem we have is that we have an\ninstalled base using existing password files, which store one-way\nderivatives of the passwords.\n\nThere is NO way to get the plaintext password or H(A1) (as suggested\nin the digest draft) from these databases.  Not all of the databases\navailable to use for authentication use the same algorithms to store\nthe passwords.\n\nSo, for Digest to be even remotely interesting, we would need the\nbrowser to build H(A1) from the password derivative, not the plaintext\npassword itself.  And this only works for a subset of the databases\navailable (i.e. the ones we administer, not necessarily the system\nones).\n\nDigest also breaks with 3rd party authentication schemes such as\nKerberos. (I think somebody already pointed this out)\n\nOur users who care about passwords flowing in the clear use SSL.\n\nRichard L. Gray\nwill code for chocolate\n\n\n\n"
        },
        {
            "subject": "RE: Digest mes",
            "content": "> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Tuesday, December 16, 1997 6:46 AM\n> \n<snip>\n\n>   However, some entity digest capability is absolutely required\n>   because it is the message we are authenticating - it does no good to\n>   send credentials if the entity is not tied to them.\n> \nAbsolutely correct.\n\n> PL> Fundamentally, good crypto practice says that EVERYTHING should be in\n> the\n> PL> digest. It is almost impossible to figure out every possible attack\n> that\n> PL> someone might make by being able to modify an undigested field, so the\n> only\n> PL> safe thing to do is digest them all.\n> \n>   But we also face the backward compatibility problem - we know\n>   for sure that the current scheme is broken in the face of proxies.\n> \nWhich proxies?  I've asked a couple of time whether there are any proxies\nthat will add Date if it doesn't exist -- no answer yet.\n\n> RF> Service degradation is not an issue -- that can be accomplished more\n> RF> effectively by changing the digested part so that it no longer matches\n> RF> the digest and thus force a reset of the whole process.\n> \n>   I agree that service degradation and cache efficiency should not be\n>   our concern here - what is desperatly needed is a lightweight\n>   mechanism for authentication, and if that means that caches can't\n>   cache, well so be it.\n> \nFine.\n\n>   I posted a proposal last Friday\n> \n>     http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0390.html\n> \n>   to simplify the entity digest calculation by removing all the\n>   metadata headers; should I rewrite this in the form of an Internet\n>   Draft, or can we just debate it and possible other solutions here?\n> \nLet's debate it here. Just as with Date, I've seen no reason to remove LM\nand Expires, becuase no one has said that their proxy mucks with them.\n\n\n\n"
        },
        {
            "subject": "RE: doub",
            "content": "Ramya,\n\nTo my eye, this is an error in RFC 2068.  Fortunately, this error has been \ncaught, and draft-ietf-http-v11-spec-rev-01 does list Accept-Ranges in \nSection 6.2 as a response header.\n\n- Jim\n\nOn Tuesday, December 16, 1997 1:27 AM, Ramya T V \n[SMTP:ramyatv@wipinfo.soft.net] wrote:\n>\n>\n>      Hi,\n>       I am sorry if this is not the right mailing list to ask this doubt,\n>       but I hope someone can help me out with this.\n>\n>       RFC2068 for HTTP/1.1 specifies Accept-Ranges to be a response \nheader\n>       field in Section 14.5. But there is no mention of this field in\n>       the BNF for response header fields in Section 6.2.\n>\n>       Accept-Ranges seems to have been grouped along with the other \nAccept*\n>       fields of the request-header, but does not find a place in the BNF\n>       for request-header fields also. Is it an extension header?\n>\n>       Could anybody please clarify this?\n>\n>       Thanks in advance,\n>       Ramya T.V.\n\n\n\n"
        },
        {
            "subject": "remov",
            "content": "Patrick Sim\nEmail  : patrick.sim@pacific.net.sg\nTel     : (714)  531-9638\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "why not make the spec say something like:\n\n( the mode of http is to specify length whenever possible )\n\nIf there is no content length header and if the outermost TE is not chunked\nand the client/server doesnt understand that TE, it may respond\n4XX unsupported transfer encoding..\n\nJosh Cohen <joshco@microsoft.com>\nProgram Manager - Internet Technologies\n\n> -----Original Message-----\n> From:koen@win.tue.nl [SMTP:koen@win.tue.nl]\n> Sent:Tuesday, December 16, 1997 4:26 AM\n> To:mogul@pa.dec.com\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: What is Content-Length?\n> \n> Jeffrey Mogul:\n> >\n> >    > If a message is sent on a persistent connection using\n> >    > a transfer-coding that does not exactly preserve the\n> >    > length of the data being encoding, then the \"chunked\"\n> >    > transfer-coding MUST be used, and MUST be the last\n> >    > transfer-coding applied.\n> >    > \n> >    \n> >    Is there a reason to require that chunked be applied after a\n> >    self-delimiting transfer encoding?  There would be a (probably\n> >    slight)  performance penality for doing it and I don't see the\n> >    purpose.\n> >\n> >It seems like a mistake to get into the business of specifying\n> >self-delimiting transfer codings (aside from chunked, which is\n> >a generic way to do that).\n> \n> I agree, but requiring chunked on top will get us in the much nastier\n> business of forbidding self-delimiting transfer codings specified by\n> others.\n> \n> It should be legal to use something like gzip as the top transfer\n> encoding.  If a server has to put chunking on top of a gzipped stream\n> without knowing the size of the zipped data beforehand, this could be\n> quite expensive in terms of either memory copy operations or software\n> complexity.  The same is true for decoding such a thing.\n> \n> >-Jeff\n> \n> Koen.\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new HTTP 1.1 authentication schem",
            "content": "I don't see why a standard ACL protocol cannot be specified, it would add\nTREMENDOUS value.\n-e\n\n---------------------- Forwarded by Eric Houston/CAM/Lotus on 12/15/97\n09:51 AM ---------------------------\n\n\nMary Ellen Zurko <zurko@opengroup.org> on 12/11/97 08:41:29 AM\n\nTo:   Eric Houston/CAM/Lotus\ncc:   jg@pa.dec.com (Jim Gettys) ,\n      http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com, zurko@opengroup.org\nSubject:  Re: Proposal for new HTTP 1.1 authentication scheme\n\n\n\n\n>  1) When the content server redirects the request to the authentication\n> server, it encrypts the ACL for the protected resource.  The\nauthentication\n> server then validates the user against the (decrypted) ACL and returns\nthe\n> first matching entry to be cached in the browser.  When the browser is\n> queried for user credentials, the encrypted (authenticated) group\n> affiliations are returned to the content server.\n>\nSince there are no standardized ACLs, I don't think this can be\naddressed in the HTTP spec. Or did I miss the part where ACLs were\nadded to HTTP?\n     Mez\n\n\n\n"
        },
        {
            "subject": "Hitmetering implementation",
            "content": "Hello,\n\nPlease redirect me, if this isn't the right forum for this question.\n\nI am looking for Web server implementations of Hit-metering for testing against our Proxy server implementation.\n\nI tried searching the working group archive for such information and I could not find any announcements regarding implementation of RFC 2227.\n\nThanks in advance,\njayakumar\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "As the person who orginially proposed digest I have a few comments:\n\n1) The spec has been arround for three years, people who have built\nup large databases of non system passwords hardly deserve much\nconsideration. In any case passwords should be changed regularly,\nshaddow the damn database.\n\n1a) If you are using Kerberos the last thing you want is BASIC\nauthentication... \n\n2) The purpose was to prevent the need to EVER send passwords \nover the net in the clear, not to provide cast iron security. \n\nThe problem with BASIC is that pinheads chose the same password \nfor their Financial times subscription as their office computer account.\nIf I can snoop a companies external traffic for BASIC passwords I can\nprobably use this for an attack.\n\n3) It is astonishing how people will tolerate the incredibly broken (BASIC)\nand simultaneously spend their time inventing new hoops for attempts to\nprovide a fix. I stopped adding whistles and bells when people told me\nthey were concerned about the difficulty of implementing it.\n\n4) The idea of password based authentication is inherently flawed. If\none is going to use public key, certificates are the means to establish\nidentity. Sending passwords to an untrustworthy server does not solve\nthe 'pinhead' problem.\n\n\n            Phill\n\n\n\n"
        },
        {
            "subject": "Testing 121",
            "content": "  The next Test Day is 12-18; please send registrations as usual to\n  httptest@agranat.com.  If your information hasn't changed, just say\n  so - you needn't repeat it all.\n\n  I am interested in hearing from people who have test systems\n  available more or less continuously - I am considering putting more\n  detail on the W3C HTTP Implementors Forum page on these systems.  If\n  you think this would be a good thing, drop me a note and say so - if\n  not, drop me a note and say why not or how we should change for that\n  sort of setup.  There really is no reason why all this should happen\n  only on Thursdays (I try to have my latest version up by Thursday,\n  but something is there more or less all the time).\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "ContentBase bugs in User Agent",
            "content": "  Our testing has found that some browsers don't use the value in a\n  Content-Base header.  I've put together test cases - if I'm\n  misinterpreting the way this header should work I'd love to hear\n  about it.\n\n  These tests are up now at:\n\n    http://test11.agranat.com/basetest/\n\n  and\n\n    http://noclock11.agranat.com/basetest/\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "Yes  your point is valid..\nActually, I had suggested using their intermim DNS\nsolution in the meantime, until they get that and other issues\nworked out.\n\nThe DNS solution isnt vulnderable to the issues they point out.\n(IMHO , if im wrong, slap me )\n\nAccording to Jim Gettys,\n> \n> I talked to Jeff Schiller (IETF area director for security)\n> about the serverloc security problems....\n> \n> The problem is that anyone, anywhere, can advertize the service, and \n> potentially arrange to get a client to use a service (in this case,\n> a proxy) of the attacker's choosing.\n> \n> The security compromise for serverloc to go to proposed standard is to\n> allow one, via public key crypto, verify the new advertisements.\n> \n> The problem from our point of view is that the solution to serverloc's \n> security problem reduces to the previously unsolved problem: having to \n> distribute different keys to different browsers, which isn't better than \n> what we have now, having to distribute the first proxy configuration \n> information to the browser. A solution to this might be to use DHCP to \n> distribute the keys required, but is not yet specified.\n> \n> Now, DHCP has its own set of security problems, but I note that people\n> are already trusting it in the first place to get their IP addresses (for the\n> mass market).\n> \n> So without being expert at either protocol, it sure sounded like in\n> the short term a DHCP based solution might be best, though in the longer\n> term, something that is more dynamic than just information acquired\n> at boot time would be a better solution (so that when a browser\n> gets restarted, you can pick up the current proxy, or fail-over if a failure\n> is detected while talking to a proxy.  And in the long term, this may\n> not be either/or.\n> \n> Hope this helps discussions....\n> - Jim\n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp.        josh@geeks.org\n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Speaking as someone who is using HTTP\nfor Internet Printing, I would like to\nsee security mechanisms removed from the\ncore HTTP specification. \"Basic\" could\nbe included to support legacy environments,\nbut I do not think that HTTP digest provides\nadequate security for payloads being\ncarried by HTTP (HTML, IPP, Webdav, etc..)\n\nIf we're going to adequately address security,\nI would like to see it solved more\nrobustly. Transport Layer Security (TLS)\nseems to address most, if not all, security\nrequirements of most applications using HTTP.\n\nThere seem to be two problems to solve for\nHTTP using TLS; 1) We need an in-band \nsignaling mechanism so that clients and\nservers can negotiate whether or not the\noverhead of a security encapsulation is\nrequired, and 2) We need a TLS profile\nthat specifies unencumbered cipher suites\nto be used. \n\nI think a SASL profile for HTTP would solve\nthe first point, and some combination of\nDiffie-Hellman, Triple-DES, and MD5/SHA\nmechanisms would solve the 2nd point.\n\nLegacy environments could be supported by\nrunning TLS-enabled services on port 443\n(HTTPS), but this would be up to a site\nadministrator, and not necessarily be \naddressed by normative text in the \nstandard document.\n\nI and other groups have had a difficult\ntime with the compound support for\nsecurity over HTTP. \"Compound\" in this\ncontext means negotiating TLS/SSL3\nsecurity mechanisms at the beginning of\nan HTTP session, and then later on hitting\nHTTP security (both basic and potentially\ndigest). Its difficult to administer and\npotentially difficult for end users.\n\nIt appears that TLS would be a good \ncandidate for handling security for most\ntypes of application protocols being\ndeveloped by the IETF. Also, SASL, or some\nequivalent, could be used to negotiate\nwhether security is required. With\nthese mechanisms, working groups developing\napplication protocols (or higher layer\ntransport protocols) would have a common\nbase to work from and it would ease the\nburden of authoring the \"security \nconsiderations\" sections of their respective\ndocuments. All working groups could rely\non the expertise of others who developed\nTLS, and NOT have to become security experts.\nBasically, the working group members could\nconcentrate on the \"applications\", without\nhaving to worry about \"other issues\".\n\nRandy\n\n\n\n\nRandy Turner\nSharp Labs of America\nrturner@sharplabs.com\n\nPhillip M. Hallam-Baker wrote:\n> \n> As the person who orginially proposed digest I have a few comments:\n> \n> 1) The spec has been arround for three years, people who have built\n> up large databases of non system passwords hardly deserve much\n> consideration. In any case passwords should be changed regularly,\n> shaddow the damn database.\n> \n> 1a) If you are using Kerberos the last thing you want is BASIC\n> authentication...\n> \n> 2) The purpose was to prevent the need to EVER send passwords\n> over the net in the clear, not to provide cast iron security.\n> \n> The problem with BASIC is that pinheads chose the same password\n> for their Financial times subscription as their office computer account.\n> If I can snoop a companies external traffic for BASIC passwords I can\n> probably use this for an attack.\n> \n> 3) It is astonishing how people will tolerate the incredibly broken (BASIC)\n> and simultaneously spend their time inventing new hoops for attempts to\n> provide a fix. I stopped adding whistles and bells when people told me\n> they were concerned about the difficulty of implementing it.\n> \n> 4) The idea of password based authentication is inherently flawed. If\n> one is going to use public key, certificates are the means to establish\n> identity. Sending passwords to an untrustworthy server does not solve\n> the 'pinhead' problem.\n> \n>             Phill\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Yea, and now Internet Explorer 4.0 has broken their digest implementation\nform 3.0. Of course, netscape doesn't do digests.\n\nOf course, digests never authenticated the transaction and return codes,\nleaving them vulnerable to man-in-the-middle attacks.\n\nQuite the mess.\n\nA couple of simple fixes and this would be very useful.\n\nWhat gives?\n\n\n\n"
        },
        {
            "subject": "Administrivia: httpwg mailing list outag",
            "content": "Folks,\n\nDue to a combination of Xmas site shutdown and physical re-location of the\nlist server, the http-wg mailing list will be unavailable from the 18th of\nDecember until the 5th of January.\n\nMail sent to the list server will be queued up and hopefully should not be\ndropped on the floor nor returned.\n\nApologies in advance.\n--\n(http-wg mailing list administrator)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "On Wed, 17 Dec 1997, John C. Mallery wrote:\n\n> Yea, and now Internet Explorer 4.0 has broken their digest implementation\n> form 3.0. Of course, netscape doesn't do digests.\n\n  Internet Explorer doesn't do digest and never has.\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "On Wed, 17 Dec 1997, John C. Mallery wrote:\n\n> Yea, and now Internet Explorer 4.0 has broken their digest implementation\n> form 3.0. Of course, netscape doesn't do digests.\n> \n> Of course, digests never authenticated the transaction and return codes,\n> leaving them vulnerable to man-in-the-middle attacks.\n> \n> Quite the mess.\n> \n> A couple of simple fixes and this would be very useful.\n> \n> What gives?\n> \n> \nDid you read the spec?\n\n\n       The Digest Access Authentication scheme is not intended to be a complete\n       answer to the need for security in the World Wide Web. This scheme\n       provides no encryption of object content. The intent is simply to create\n       a weak access authentication method, which avoids the most serious flaws\n       of Basic authentication.\n\nThe design objectives of digest were 1) replace the clear text passwords\nof Basic, 2) no patent or export restrictions, i.e. NO ENCRYPTION.\n\nIt is not fair to criticize a bicycle because it does a rotten job\nas a school bus.  If you want to keep Basic forever, so be it. But\nBasic is currently used at least an order of magnitude more than \nany other authentication or security system.  This will continue\nindefinitely if digest is abandoned. \n\nI have no idea what \"digests never authenticated the transaction and\nreturn codes\" means.  The spec allows authentication of all return\nheaders which proxies don't change and the entity body. This works and\nthere are implementations. It is optionaly since usually it is not\nworth the overhead.\n\nPeople need to keep in mind what this is for.  It is designed for\nsomething like the NY Times reader registration.  It is not suitable\nfor electronic commerce.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "RT> From: Randy Turner <rturner@sharplabs.com>\nRT> Subject: Re: Digest mess\n\nRT> Speaking as someone who is using HTTP for Internet Printing, I would\nRT> like to see security mechanisms removed from the core HTTP\nRT> specification.\n\n  See the latest drafts - they have been.  This does not mean that\n  they should not be supported, just that they are specified separatly.\n\nRT> If we're going to adequately address security, I would like to see it\nRT> solved more robustly. Transport Layer Security (TLS) seems to address\nRT> most, if not all, security requirements of most applications using\nRT> HTTP.\n\n  Digest is not and never has been designed to support all possible\n  security requirements of HTTP or anything else - it is intended to\n  provide an unencumbered lightweight authentication mechanism.\n\n  As has been pointed out many times on the IPP list, there is a\n  substantial segment of the market for whom authentication only is\n  plenty of security service.  'Security' does not mean 'Encryption'\n  (nor the other way around).\n\nRT> There seem to be two problems to solve for HTTP using TLS; 1) We need\nRT> an in-band signaling mechanism so that clients and servers can\nRT> negotiate whether or not the overhead of a security encapsulation is\nRT> required, and 2) We need a TLS profile that specifies unencumbered\nRT> cipher suites to be used.\n\n  (1) is beyond our current scope; Keith Moore has asked for (and, I\n  expect, will get) volunteers to work on this, but we can't look\n  forward to an implementable specification in the very short term.\n\n  (2) is problematic because at the core of TLS is public key crypto,\n  for which there are no unencumbered algorithms (before that\n  statement starts a debate, let me qualify it - I am not interested\n  in shipping a product for the purpose of contesting what someone\n  else thinks is an indefensible patent claim - if you would like to\n  risk your business on that sort of thing then I salute you, but I\n  absolutly can not do it).\n\n  Even if some usefull unencumbered suite can be assembled, the\n  computational cost of doing _just_ authentication and integrity (the\n  goal of Digest, remember) using TLS is dramatically higher than the\n  Digest mechanism - MD5 is computationally _very_ inexpensive.\n\n  Before you reject Digest as a mechanism you want to exist, I suggest\n  that you do some work to really figure out the relative costs in\n  technology licensing, development time, required CPU, and required\n  memory in a product and run those numbers past your product\n  management people - see what thier reaction is to the floor you have\n  placed on product cost.\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Please see my comments below...\n\nRandy\n\nScott Lawrence wrote:\n> \n> RT> From: Randy Turner <rturner@sharplabs.com>\n> RT> Subject: Re: Digest mess\n> \n> RT> Speaking as someone who is using HTTP for Internet Printing, I would\n> RT> like to see security mechanisms removed from the core HTTP\n> RT> specification.\n> \n>   See the latest drafts - they have been.  This does not mean that\n>   they should not be supported, just that they are specified separatly.\n\nOk, I will take a gander at the latest draft.\n> \n> RT> If we're going to adequately address security, I would like to see it\n> RT> solved more robustly. Transport Layer Security (TLS) seems to address\n> RT> most, if not all, security requirements of most applications using\n> RT> HTTP.\n> \n>   Digest is not and never has been designed to support all possible\n>   security requirements of HTTP or anything else - it is intended to\n>   provide an unencumbered lightweight authentication mechanism.\n> \n>   As has been pointed out many times on the IPP list, there is a\n>   substantial segment of the market for whom authentication only is\n>   plenty of security service.  'Security' does not mean 'Encryption'\n>   (nor the other way around).\n\nI know the rationale for including a lightweight\nauth capability for HTTP, but I think the\noriginal rationale for including digest within HTTP\nno longer exists.\n\n> \n> RT> There seem to be two problems to solve for HTTP using TLS; 1) We need\n> RT> an in-band signaling mechanism so that clients and servers can\n> RT> negotiate whether or not the overhead of a security encapsulation is\n> RT> required, and 2) We need a TLS profile that specifies unencumbered\n> RT> cipher suites to be used.\n> \n>   (1) is beyond our current scope; Keith Moore has asked for (and, I\n>   expect, will get) volunteers to work on this, but we can't look\n>   forward to an implementable specification in the very short term.\n> \n>   (2) is problematic because at the core of TLS is public key crypto,\n>   for which there are no unencumbered algorithms (before that\n>   statement starts a debate, let me qualify it - I am not interested\n>   in shipping a product for the purpose of contesting what someone\n>   else thinks is an indefensible patent claim - if you would like to\n>   risk your business on that sort of thing then I salute you, but I\n>   absolutly can not do it).\n> \n>   Even if some usefull unencumbered suite can be assembled, the\n>   computational cost of doing _just_ authentication and integrity (the\n>   goal of Digest, remember) using TLS is dramatically higher than the\n>   Digest mechanism - MD5 is computationally _very_ inexpensive.\n> \n>   Before you reject Digest as a mechanism you want to exist, I suggest\n>   that you do some work to really figure out the relative costs in\n>   technology licensing, development time, required CPU, and required\n>   memory in a product and run those numbers past your product\n>   management people - see what thier reaction is to the floor you have\n>   placed on product cost.\n\nI don't reject the notion of \"digest\", I just don't think\nwe should solve the security problem <n> different times.\nIf you support any other protocol (other than HTTP),\nthen you're faced with potentially solving the\nsecurity problem for every protocol. Therefore, I don't\nthink the code space, development time, and required\nmemory is an issue. In fact, I think it is one of the\ndisadvantages of including security in HTTP.\n\nSupport for extensions to the available ciphersuites\nand options for TLS have been included in the latest\ndraft. If their is a less CPU-intensive way to do\nsecurity, then we could define a new ciphersuite for\nTLS.\n\nMy argument is for proper layering, I only want to\ninclude security capabilities once in my products,\nand then write profiles for all applications that\nuse it.\n\nRandy\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": ">>>>> \"PL\" == Paul Leach\n>>>>> \"EH\" == Eric Houston\n\nPL> (Personally, I don't see why the content server can't evaluate the ACL\nPL> itself.\nPL>\nThe goal is to separate the directory server from the content server; do\nnot replicate\n the directory onto the content server; do  not use LDAP for authentication\nOR authorization\n(on the back end).  Do authentication and authorization on the\n\"authentication/authorization\" server.  When\nvisitors are registered on your site, they are instantly \"registered\"\n(authorized) on all content\nservers because there is only one authentication/authorization server.\nEH> 2) Could re-directed authentication be layered on top of the existing\nEH> schemes so that it could be used with basic, digest, and X.509?\nEH>\nPL> Re-directed authentication is totally transparent to the client, so\ntalking\nPL> about \"on top of existing schemes\" is not meaningful.\nPL>\nThe point is, regardless of the scheme, to separate the directory services\nfrom the content services.\nCan webmake this authentication/authorization protocol generic enough to\n(optionally) use X.509 certs?\nIf that is possible, I don't want to require them to be on the content\nserver...\n\nEric Houston\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "At 10:14 PM 12/16/97 -0500, Phillip M. Hallam-Baker wrote:\n> 2) The purpose was to prevent the need to EVER send passwords \n> over the net in the clear, not to provide cast iron security. \n\nRegardless of the original purpose, cast-iron protection for\nauthentication secrets seems quite important, especially in\nlight of ...\n\n> The problem with BASIC is that pinheads chose the same password \n> for their Financial times subscription as their office computer account.\n> If I can snoop a companies external traffic for BASIC passwords I can\n> probably use this for an attack.\n\nAbsolutely.  Password reuse between weak and strong methods\nmust be discouraged, as long as weak methods continue to be used.\nBut memorized secrets remain important.\n\n> 3) It is astonishing how people will tolerate the incredibly broken (BASIC)\n> and simultaneously spend their time inventing new hoops for attempts to\n> provide a fix. I stopped adding whistles and bells when people told me\n> they were concerned about the difficulty of implementing it.\n>\n> 4) The idea of password based authentication is inherently flawed. If\n> one is going to use public key, certificates are the means to establish\n> identity. ...\n\nI disagree with the conclusion, which goes too far.\nSome password-based certificate-free methods work quite well.\n\n> ... Sending passwords to an untrustworthy server does not solve\n> the 'pinhead' problem.\n\nSure, but you can verify a small password without exposing it\nto an untrustworthy party, even in digested form, using a\nstronger method.\n\nClear-text and digest-style authentication are flawed, and\nit sure is discouraging that they are so widely deployed and\nused.  This should motivate the deployment of stronger methods.\n\nThe \"pinhead\" problem is that a PIN code is all that seems\nto fit in an average human head, and this code is small\nenough for brute force attack.  Methods like SPEKE and EKE\nsolve this problem w.r.t. the network.  For example, these\ndon't leak PIN-sized secrets to eavesdroppers.\n\nMemorized factors remain important for personal\nauthentication, and a total dependence on stored keys\nand certificates unnecessarily weakens a system.\nThe goal of replacing broken password methods with\nstronger password methods seems quite important.\n\n------------------------------------\nDavid Jablon\nIntegrity Sciences, Inc.\ndpj@world.std.com\n<http://world.std.com/~dpj/>\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Excuse me.  Did I say encryption?\n\nIt has to provide a hash of the return codes and a hash of\nthe entity to achieve its full potential.  This allows client\nto know that you have the right entity body and it allows \nthe client to know how the server processed the request,\ni.e. the entire transaction is authenticated. This point\nhas been raised before on the list.  I can't why it isn't\ndead obvious.\n\nEarlier versions of Internet Explorer did indeed do digests because they \nwere based on the Spyglass code, which did.\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "On Wed, 17 Dec 1997, John C. Mallery wrote:\n\n> \n> It has to provide a hash of the return codes and a hash of\n> the entity to achieve its full potential.  This allows client\n> to know that you have the right entity body and it allows \n> the client to know how the server processed the request,\n> i.e. the entire transaction is authenticated. This point\n> has been raised before on the list.  I can't why it isn't\n> dead obvious.\n> \n\nLet me repeat: ALL OF THIS IS IN THE CURRENT DRAFT.  And there\nare implementations.\n\nI don't understand your point.  Are you arguing that the \nAuthentication-info header should not be optional?  In that\ncase it would not be feasible to use digest for things like\nregistering newspaper readers where authenticating every article\nwould not be worth the overhead.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "Expanding on my previous idea we could actually use the URL in DHCP to\ndirect us to a \"services server\". Once there we can then interrogate the\nresource specified by the URL to get such interesting things as\nautoproxy scripts. These scripts can contain all sorts of fascinating\ninformation, such as time outs on the script's lifetime as well as\nprovisions for how to get updated proxy information. Rather than filling\nDHCP up with endless URLs for every service, why not just use it as an\nentry point?\n\nYaron\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Tuesday, April 08, 1997 1:39 PM\n> To:Yaron Goland\n> Cc:Stuart Kwan; http-wg@cuckoo.hpl.hp.com; josh@netscape.com\n> Subject:RE: ID:  Proxy autoconfig\n> \n> I talked to Jeff Schiller (IETF area director for security)\n> about the serverloc security problems....\n> \n> The problem is that anyone, anywhere, can advertize the service, and \n> potentially arrange to get a client to use a service (in this case,\n> a proxy) of the attacker's choosing.\n> \n> The security compromise for serverloc to go to proposed standard is to\n> allow one, via public key crypto, verify the new advertisements.\n> \n> The problem from our point of view is that the solution to serverloc's\n> \n> security problem reduces to the previously unsolved problem: having to\n> \n> distribute different keys to different browsers, which isn't better\n> than \n> what we have now, having to distribute the first proxy configuration \n> information to the browser. A solution to this might be to use DHCP to\n> \n> distribute the keys required, but is not yet specified.\n> \n> Now, DHCP has its own set of security problems, but I note that people\n> are already trusting it in the first place to get their IP addresses\n> (for the\n> mass market).\n> \n> So without being expert at either protocol, it sure sounded like in\n> the short term a DHCP based solution might be best, though in the\n> longer\n> term, something that is more dynamic than just information acquired\n> at boot time would be a better solution (so that when a browser\n> gets restarted, you can pick up the current proxy, or fail-over if a\n> failure\n> is detected while talking to a proxy.  And in the long term, this may\n> not be either/or.\n> \n> Hope this helps discussions....\n> - Jim\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "At 11:32 AM -0600 97-12-17, John Franks wrote:\n>On Wed, 17 Dec 1997, John C. Mallery wrote:\n>\n>> \n>> It has to provide a hash of the return codes and a hash of\n>> the entity to achieve its full potential.  This allows client\n>> to know that you have the right entity body and it allows \n>> the client to know how the server processed the request,\n>> i.e. the entire transaction is authenticated. This point\n>> has been raised before on the list.  I can't why it isn't\n>> dead obvious.\n>> \n>\n>Let me repeat: ALL OF THIS IS IN THE CURRENT DRAFT.  And there\n>are implementations.\n\nGood. I'll go fetch the latest draft and update my implementation,\nthank you. Didn't realize there was a document past the RFC.\n\n\n\n"
        },
        {
            "subject": "Unidentified subject",
            "content": "The digest authentication specification has been stable for quite some\ntime.  There are numerous implementations which interoperate.  Some\nof them (e.g. Apache) are widely deployed.\n\nThe recent spate of posts have criticized it because 1) \"it suffers\nfrom featuritis\", and 2) \"with just a couple of additions it could\nbe really useful.\"\n\nAs Roy Fielding pointed out the primary failing is that it is not\nimplemented in MSIE and Netscape.  The only thing we can do to remedy\nthis is keep the specification on track.\n\nThe only reason this came up at this point was that because a hash\nof the Date, L-M and Expires headers can be part of the response\nthere could be a problem for servers with no clock if a proxy added\na Date header.  There is a simple answer to this which is that\nproxies should not be allowed to add or change Date, L-M or Expires\nheaders.  There are no known implementations which do so and no one\nhas suggested any reason it is necessary to do so.\n\nIn short there is nothing wrong with digest.  It works; there are\nimplementations; they interoperate.  We would like more implementations\nand the way to get that is keep the specification on track.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "[Administrative note: I'm trying to arrange for an alternate\nHTTP-WG mailing list host so that we can continue discussions\nduring the nearly 3 weeks that 'cuckoo.hpl.hp.com' is not\navailable.]\n\nI've reviewed the mail on this topic. On the general issue\nof whether Digest authentication is 'worth it', this is a topic\nthat we've debated at length and come to a conclusion long ago.\n\nThe argument that it isn't worth the bother ('not better than\nTLS') doesn't hold. That we could and should be working on\na replacement is not a good reason for not going forward with\nwhat we have.\n\nThe original rationale for including digest authentication within\nHTTP still holds: We cannot progress an Internet\nStandard which ONLY has Basic Authentication as its authentication\nmethod, and any other solution than Digest has more difficulties\nthan Digest.\n\nWe do not want to put HTTP/1.1 as Draft Standard on hold for a\nyear or two while we try to come up with an acceptable replacement\nfor Digest Authentication.\n\nWe're going to proceed with Digest Authentication as a separately\nspecified but mandatory part of HTTP/1.1. If there is a consensus\nto make some changes to Digest Authentication in order to improve\nits utility, then we'll make those changes. However, the subject\nof \"let's not do it at all\" is closed. We need to either ship what\nwe have or modify it in a way that is generally acceptable.\n\nLarry Masinter\n(as chair, HTTP working group)\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Existance of Proxies that Add Header",
            "content": "On Wed, 17 Dec 1997, John Franks wrote:\n\n> The only reason this came up at this point was that because a hash\n> of the Date, L-M and Expires headers can be part of the response\n> there could be a problem for servers with no clock if a proxy added\n> a Date header.  There is a simple answer to this which is that\n> proxies should not be allowed to add or change Date, L-M or Expires\n> headers.  There are no known implementations which do so and no one\n> has suggested any reason it is necessary to do so.\n\n  Unfortunatly, this last is incorrect.  I just tested against my\n  clockless server at noclock11.agranat.com through the jigsaw proxy at\n  jigsaw.w3.org; it not only added a Date header, it replace the\n  Last-Modified header in the original response with a different value\n  (which is broken no matter how you figure it).\n\n  As I have pointed out before, we cannot interpret the fact that proxy\n  implementors won't step up and tell us what they are doing as meaning\n  that they are only doing what we hope is true.  I repeat my call for\n  other proxies we can use for testing.\n\n\n\n"
        },
        {
            "subject": "RE: Digest mes",
            "content": "> ----------\n> From: Randy Turner[SMTP:rturner@sharplabs.com]\n> Sent: Wednesday, December 17, 1997 12:08 AM\n> \n> If we're going to adequately address security,\n> I would like to see it solved more\n> robustly. Transport Layer Security (TLS)\n> seems to address most, if not all, security\n> requirements of most applications using HTTP.\n> \n> Sure you can use SSL/TLS for all Web security -- and you can use atom\n> bombs to kill ants, too.\n> \n> There is no way to use TLS w/o encryption; and encryption is expensive and\n> often not needed.\n> \n> There is no way to use TLS for client authentication without client\n> certificates. Getting everyone to have a certificate is non-trivial,\n> whereas everyone has passwords.\n> \nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Digest mes",
            "content": "Damn Exchange! It messed up the indenting when I cut and pasted... and hence\nyou can't tell what Randy said and what I said. See below to correct that:\n\n> ----------\n> From: Paul Leach\n> Sent: Wednesday, December 17, 1997 10:42 AM\n> To: Phillip M. Hallam-Baker; 'Randy Turner'\n> Cc: rlgray@us.ibm.com; HTTP Working Group\n> Subject: RE: Digest mess\n> \n> \nThis is what Randy said:\n\n> > ----------\n> > From: Randy Turner[SMTP:rturner@sharplabs.com]\n> > Sent: Wednesday, December 17, 1997 12:08 AM\n> > \n> > If we're going to adequately address security,\n> > I would like to see it solved more\n> > robustly. Transport Layer Security (TLS)\n> > seems to address most, if not all, security\n> > requirements of most applications using HTTP.\n> \n> \nThis was my reply:\n> > \n> > Sure you can use SSL/TLS for all Web security -- and you can use atom\n> > bombs to kill ants, too.\n> > \n> > There is no way to use TLS w/o encryption; and encryption is expensive\n> and\n> > often not needed.\n> > \n> > There is no way to use TLS for client authentication without client\n> > certificates. Getting everyone to have a certificate is non-trivial,\n> > whereas everyone has passwords.\n> > \n> Paul\n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Well, it's nice to know that people who want Digest are still listening.\nThere are two reasons why I am pushing all the buttons in this thread.\n\n  1) People seem to be bandying about suggested changes to Digest\n     without consideration for how those changes will affect\n     deployment.  If you change the entity-digest specification,\n     is the recipient of the Digest supposed to decode it using the\n     rules in RFC 2069 or RFC HTTPAA-eventually?  How is the recipient\n     going to tell the difference between them?  Or should we just assume\n     that everyone will instantly update their applications as soon as\n     the new RFC is published (just as they didn't do for the last RFC)?\n\n     HTTP/1.1 was designed for deployment.  What is the deployment\n     design for Digest mkIII?  How do we get implementers to include\n     it in their *current* products?\n\n  2) The current draft, <draft-ietf-http-authentication-00>, is\n     a pile of fresh manure.  The text format is mangled, it fails\n     to define the general HTTP authentication header fields and\n     responses, mis-defines WWW-Authenticate and Authorization as\n     Digest-only fields, mixes HTTP-BNF and pseudo-math at random\n     and without distinction, and needs a complete reorganization\n     before anyone can be sensibly expected to give it a serious\n     technical review.\n\nIf at least one of the seven authors currently listed on the draft\nwill step up to the plate and start editing, then maybe Digest can\nbe resurrected.  I suggest the authors discuss this amongst themselves\nand have a clean draft submitted by mid-January.\n\nIf not, then either Basic AA will move back into the main spec and\nDigest will be dropped, or I'll do the edit myself according to what\nis actually implemented.  [Believe me, none of us want this to happen].\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: more on digest (was: Unidentified subject!",
            "content": ">The only reason this came up at this point was that because a hash\n>of the Date, L-M and Expires headers can be part of the response\n>there could be a problem for servers with no clock if a proxy added\n>a Date header.  There is a simple answer to this which is that\n>proxies should not be allowed to add or change Date, L-M or Expires\n>headers.  There are no known implementations which do so and no one\n>has suggested any reason it is necessary to do so.\n\nAn HTTP/1.1 cache is required to change Date and Expires upon receipt\nof a 304 response containing updated values for those fields.  This\ndoes impact non-shared caches, so you will need to add something to the\neffect of the digest should be removed if those fields are updated.\n\nThe Apache proxy canonicalizes the response field-values of Date,\nLast-Modified, and Expires to the required HTTP-date format.  I have\nno idea what effect this would have on entity-digest.  If it caches\nthe response, the cache will add Date and Content-Length if they are\nmissing, but it won't normally cache the response if the request\nincluded Authorization (this would not be the case if we ever\ndeveloped a personal, non-shared proxy).\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "RE: Digest mes",
            "content": "If the current set of ciphersuites are not suitable, we\ncould always register a new ciphersuite and write a\nprofile for using HTTP w/TLS that references how \nimplementations should minimally interoperate with\na set of ciphersuites.\n\nBasically, I agree with Larry  M. in that the specs\nare basically done and we shouldn't cry over spilled\nmilk. Its just bad timing that we couldn't try and use\nTLS now that its here.\n\nRandy\n\n\n> -----Original Message-----\n> From:Paul Leach [SMTP:paulle@microsoft.com]\n> Sent:Wednesday, December 17, 1997 10:54 AM\n> To:Phillip M. Hallam-Baker; 'Randy Turner'\n> Cc:rlgray@us.ibm.com; HTTP Working Group\n> Subject:RE: Digest mess\n> \n> Damn Exchange! It messed up the indenting when I cut and pasted... and\n> hence\n> you can't tell what Randy said and what I said. See below to correct\n> that:\n> \n> > ----------\n> > From: Paul Leach\n> > Sent: Wednesday, December 17, 1997 10:42 AM\n> > To: Phillip M. Hallam-Baker; 'Randy Turner'\n> > Cc: rlgray@us.ibm.com; HTTP Working Group\n> > Subject: RE: Digest mess\n> > \n> > \n> This is what Randy said:\n> \n> > > ----------\n> > > From: Randy Turner[SMTP:rturner@sharplabs.com]\n> > > Sent: Wednesday, December 17, 1997 12:08 AM\n> > > \n> > > If we're going to adequately address security,\n> > > I would like to see it solved more\n> > > robustly. Transport Layer Security (TLS)\n> > > seems to address most, if not all, security\n> > > requirements of most applications using HTTP.\n> > \n> > \n> This was my reply:\n> > > \n> > > Sure you can use SSL/TLS for all Web security -- and you can use\n> atom\n> > > bombs to kill ants, too.\n> > > \n> > > There is no way to use TLS w/o encryption; and encryption is\n> expensive\n> > and\n> > > often not needed.\n> > > \n> > > There is no way to use TLS for client authentication without\n> client\n> > > certificates. Getting everyone to have a certificate is\n> non-trivial,\n> > > whereas everyone has passwords.\n> > > \n> > Paul\n> > \n> > \n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "We need to resolve the issue of content-length and cascaded\ntransfer encodings quickly. We need two interoperable\nimplementations of this 'feature', even if it is implicit in\nthe spec, or else we should restrict the allowed set.\n\nI don't think we should explore too many degrees of freedom.\n\n* chunked is only allowed once, as the last transfer encoding\n  applied.\n* before chunked is applied, only one T-E should be sent,\n  but recipients should accept all combinations (as long as\n  there are no duplicates).\n* no T-E's other than 'chunked' may be applied to multipart/\n  content-types, but T-Es are allowed within a multipart type\n  (e.g., multipart/byte-ranges, multipart/form-data).\n\nThe entire transmission is required to be either with content-length\nor else self-delimited where multipart is the only self-delimited\nmedia type, but chunked, gzip are self-delimited T-Es.\n\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "Before we can just \"resolve quickly\", I'm worried about the possibility of\nexisting implementations of (e.g.)\n\n   Content-Length: XXX\n   T-E: gzip\n\n   <gzipped stuff, XXX bytes long>\n\nwhich means that C-L is, defacto, the length of the message-body.\n\nAbsent info on such an implementation(s), we can invent lots of internally\nconsistent schemes, but they wouldn't conform to existing (presumably) RFC\n2068 compliant implementations.\n\nAnybody know of such implementations?\n\nPaul\n\n> ----------\n> From: Larry Masinter[SMTP:masinter@parc.xerox.com]\n> Sent: Wednesday, December 17, 1997 10:43 AM\n> To: Josh Cohen\n> Cc: 'koen@win.tue.nl'; mogul@pa.dec.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: What is Content-Length?\n> \n> We need to resolve the issue of content-length and cascaded\n> transfer encodings quickly. We need two interoperable\n> implementations of this 'feature', even if it is implicit in\n> the spec, or else we should restrict the allowed set.\n> \n> I don't think we should explore too many degrees of freedom.\n> \n> * chunked is only allowed once, as the last transfer encoding\n>   applied.\n> * before chunked is applied, only one T-E should be sent,\n>   but recipients should accept all combinations (as long as\n>   there are no duplicates).\n> * no T-E's other than 'chunked' may be applied to multipart/\n>   content-types, but T-Es are allowed within a multipart type\n>   (e.g., multipart/byte-ranges, multipart/form-data).\n> \n> The entire transmission is required to be either with content-length\n> or else self-delimited where multipart is the only self-delimited\n> media type, but chunked, gzip are self-delimited T-Es.\n> \n> \n> -- \n> http://www.parc.xerox.com/masinter\n> \n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": ">* chunked is only allowed once, as the last transfer encoding\n>  applied.\n\nokay by me\n\n>* before chunked is applied, only one T-E should be sent,\n>  but recipients should accept all combinations (as long as\n>  there are no duplicates).\n\nNo, that's unnecessarily restrictive.\n\n>* no T-E's other than 'chunked' may be applied to multipart/\n>  content-types, but T-Es are allowed within a multipart type\n>  (e.g., multipart/byte-ranges, multipart/form-data).\n\nNo, that would violate the whole model.  multipart types are payload\nin HTTP.  Furthermore, MIME does not allow T-Es within body-parts,\nonly C-T-Es.\n\n>The entire transmission is required to be either with content-length\n>or else self-delimited where multipart is the only self-delimited\n>media type, but chunked, gzip are self-delimited T-Es.\n\nThat would be a new requirement.  Non-delimited response bodies are\nstill allowed in HTTP/1.1.  I do agree that if a non-delimited T-E\nis used, chunked should be required on top.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: What is ContentLength",
            "content": "At 10:43 12/17/97 PST, Larry Masinter wrote:\n>We need to resolve the issue of content-length and cascaded\n>transfer encodings quickly. We need two interoperable\n>implementations of this 'feature', even if it is implicit in\n>the spec, or else we should restrict the allowed set.\n\nlibwww is certainly designed to handle multiple transfer encodings as well\nas multiple content encodings on client side. The application can register\nnew handlers for encodings dynamically - they just get plugged into the\nread stream.\n\n>I don't think we should explore too many degrees of freedom.\n\nWhen a client says that it understands multiple encodings using TE, it\nshould also be able to plug them together.\n\nI actually don't think we have to provide sanity rules here - there are\nmany other places in the spec where we expect people to use the \"don't\nforget to think\" rule. The only rule we need is that a message must be self\ndelimiting according to section 4.4, but this is already stated there.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "1xx Clarificatio",
            "content": "Section 8.2 of RFC 2068 has the following requirement:\n\no  An HTTP/1.1 (or later) client MUST be prepared to accept a 100\n   (Continue) status followed by a regular response.\n\nI have gone around asking people if the following behavior is legal:\n\nUser Establishes Connection\n\nServer sends first 1xx Response\n\nUser sends GET\n\nServer sends second 1xx Response\n\nServer Sends 200 Response\n\nServer sends third 1xx Response\n\nUser Closes Connection\n\nI have heard two different responses. Some people claim that the\nrequirement in 8.2 means that only the second 1xx Response is legal,\nthat 1xx responses may only be sent when a method is outstanding.\nOthers, however, have claimed that the requirement only means that IF a\nmethod is outstanding THEN the client must be able to accept a 100\nresponse. However, they add, it is completely legal for a server to send\na 1xx Response any time it wants and that proxies must pass them and\nclients must, at worst, ignore them. In fact, Roy Fielding has told me\nthat he had language like that in a previous version of the draft and\nonly noticed it was missing when I pointed the issue out to him.\n\nIt seems a clarification is in order. As such I propose that the\nfollowing paragraph be added to the end of section 10.1:\n\n1xx responses MAY be sent independently of requests. Clients MUST always\nbe able to accept 1xx responses and MUST ignore any 1xx response they do\nnot understand. In addition, proxies MUST pass through any 1xx response\nthey do not understand.\n\nThanks,\nYaron\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "On Wed, 17 Dec 1997, Paul Leach wrote:\n\n> Before we can just \"resolve quickly\", I'm worried about the possibility of\n> existing implementations of (e.g.)\n> \n>    Content-Length: XXX\n>    T-E: gzip\n> \n>    <gzipped stuff, XXX bytes long>\n> \n> which means that C-L is, defacto, the length of the message-body.\n> \n> Absent info on such an implementation(s), we can invent lots of internally\n> consistent schemes, but they wouldn't conform to existing (presumably) RFC\n> 2068 compliant implementations.\n\nBut then \"Transfer-encoding: gzip\" would be an extension to RFC2068. \nIt seems to me that by the rules, an extension we haven't heard about\ncan be ignored as experimental and subject to the risk of requiring\nadjustment as standards change.\n\nDave\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "On Wed, 17 Dec 1997, Paul Leach wrote:\n\n> Before we can just \"resolve quickly\", I'm worried about the possibility of\n> existing implementations of (e.g.)\n> \n>    Content-Length: XXX\n>    T-E: gzip\n> \n>    <gzipped stuff, XXX bytes long>\n> \n> which means that C-L is, defacto, the length of the message-body.\n\nJust so that silence need not leave anyone guessing, our server does not\never use any transfer encoding other than chunked.  The proposal for\ncascaded encodings seems good to us.\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> ----------\n> From: David W. Morris[SMTP:dwm@xpasc.com]\n> Sent: Wednesday, December 17, 1997 1:21 PM\n> \n> \n> On Wed, 17 Dec 1997, Paul Leach wrote:\n> \n> > Before we can just \"resolve quickly\", I'm worried about the possibility\n> of\n> > existing implementations of (e.g.)\n> > \n> >    Content-Length: XXX\n> >    T-E: gzip\n> > \n> >    <gzipped stuff, XXX bytes long>\n> > \n> > which means that C-L is, defacto, the length of the message-body.\n> > \n> > Absent info on such an implementation(s), we can invent lots of\n> internally\n> > consistent schemes, but they wouldn't conform to existing (presumably)\n> RFC\n> > 2068 compliant implementations.\n> \n> But then \"Transfer-encoding: gzip\" would be an extension to RFC2068. \n> It seems to me that by the rules, an extension we haven't heard about\n> can be ignored as experimental and subject to the risk of requiring\n> adjustment as standards change.\n> \nI last time I looked at the latest draft for HTTP/1.1, I thought I saw it\nspecify T-E: gzip, so that that will be in the RFC that will be the Draft\nStandard. So you're right that it's an extension to RFC 2069, but the\nconclusion you draw from that is incorrect.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: more on digest (was: Unidentified subject!",
            "content": "Does it (Apache proxy) canonicalize any other headers? If the incoming Date,\nL-M, and Expires are already canonical, does the exact string value change\n(spaces inserted, e.g.)?\n\n> ----------\n> From: Roy T. Fielding[SMTP:fielding@kiwi.ics.uci.edu]\n> Sent: Wednesday, December 17, 1997 11:11 AM\n> To: John Franks\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: more on digest (was: Unidentified subject!)\n> \n> >The only reason this came up at this point was that because a hash\n> >of the Date, L-M and Expires headers can be part of the response\n> >there could be a problem for servers with no clock if a proxy added\n> >a Date header.  There is a simple answer to this which is that\n> >proxies should not be allowed to add or change Date, L-M or Expires\n> >headers.  There are no known implementations which do so and no one\n> >has suggested any reason it is necessary to do so.\n> \n> An HTTP/1.1 cache is required to change Date and Expires upon receipt\n> of a 304 response containing updated values for those fields.  This\n> does impact non-shared caches, so you will need to add something to the\n> effect of the digest should be removed if those fields are updated.\n> \n> The Apache proxy canonicalizes the response field-values of Date,\n> Last-Modified, and Expires to the required HTTP-date format.  I have\n> no idea what effect this would have on entity-digest.  If it caches\n> the response, the cache will add Date and Content-Length if they are\n> missing, but it won't normally cache the response if the request\n> included Authorization (this would not be the case if we ever\n> developed a personal, non-shared proxy).\n> \n> ....Roy\n> \n\n\n\n"
        },
        {
            "subject": "Draft standard rules (was RE: What is ContentLength?",
            "content": "I'll say it again; don't presume that a given solution will/will\nnot result in recycling at proposed; usually, when I see such comments made,\nand it usually isn't true...\n\nThe point of draft standard is to show that all features of the\nprotocol have actually been tested a couple times by someone, somewhere,\nand that the spec can be independently implemented successfully\n(success is defined by interoperability.\n\nThe rules for draft standard are pretty clear:\nNo INCOMPATIBLE protocol changes.\nChanges are to fix bugs in the proposed standard protocol.\nNo new functionality.\n2 tested interoperable implementations (doesn't have to\nbe shipped as product.)\n\nIt isn't that hard to get two implementations; as usual, Henrik has\none in his pocket for almost everything in Rev-01.  Everything\nthat goes to draft standard has to have two interoperable implementations,\nand they can be by anyone. You don't have to show a single implementation\nof everything (in HTTP's case, this would be very unlikely in any case,\nsince we worry about clients, servers, and proxies).\nThe IESG is pragmatic about interpreting the rules.\n\nIf you have to break compatibility (at least once there are significant \nimplementations out there, so that interoperability is disturbed), you have \nto recycle at proposed.\n\nThis is why Larry called the process \"The Frankenstein's Monster\"\nprocess; you get to take pieces from anywhere to put the monster together.\n- Jim\n\n\n\n"
        },
        {
            "subject": "RE: What is ContentLength",
            "content": "> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Wednesday, December 17, 1997 1:46 PM\n> \n> Just so that silence need not leave anyone guessing, our server does not\n> ever use any transfer encoding other than chunked.  The proposal for\n> cascaded encodings seems good to us.\n> \nDoes anyone implement T-E (not C-E) of  gzip, deflate? Is anyone going to do\nit so that two implentations exist so that we can keep them in the standard?\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Proposal for new HTTP 1.1 authentication schem",
            "content": "I believe ACLs are being discussed by the WEBDAV group. In any event, \nI agree that a general purpose ACL mechanism for HTTP would be of \ngreat value.\n\nGregory Woodhouse gregory.woodhouse@med.va.gov\nMay the dromedary be with you.\n\n\n----------\nFrom:  Mary Ellen Zurko [SMTP:zurko@opengroup.org]\nSent:  Thursday, December 11, 1997 5:41 AM\nTo:  http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nCc:  jg@pa.dec.com; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; \nzurko@opengroup.org\nSubject:  Re: Proposal for new HTTP 1.1 authentication scheme\n\n>  1) When the content server redirects the request to the \nauthentication\n> server, it encrypts the ACL for the protected resource.  The \nauthentication\n> server then validates the user against the (decrypted) ACL and \nreturns the\n> first matching entry to be cached in the browser.  When the browser \nis\n> queried for user credentials, the encrypted (authenticated) group\n> affiliations are returned to the content server.\n>\n\nSince there are no standardized ACLs, I don't think this can be\naddressed in the HTTP spec. Or did I miss the part where ACLs were\nadded to HTTP?\nMez\n\n\n\n"
        },
        {
            "subject": "RE: more on digest (was: Unidentified subject!",
            "content": "On Wed, 17 Dec 1997, Paul Leach wrote:\n\n> Does it (Apache proxy) canonicalize any other headers? If the incoming Date,\n> L-M, and Expires are already canonical, does the exact string value change\n> (spaces inserted, e.g.)?\n> \n\n\nI remain convinced that the problem lies not with the digest\nauthentication but with proxy behavior.  However, it does seem that\nthe problems we have involving a conflicting behavior of the two could\nbe fixed by some modest changes in digest.\n\nFirst an assumption: the problem lies with responses, not requests\n(correct me if this is wrong).  I.e. proxies don't change the Date\nheader and won't change L-M or expires if any such ever exists \nin a request.\n\nIf this is the case then adding a field to the Authentication-Info\nheader could solve the problem by duplicating the headers which\na proxy might change.  I have in mind something like\n\n   dheaders = \"date:content_len:L-M-date:expires\"\n\nJohn Mallery points out that it would be good to have the response\nstatus code digested as well.  If it is included here that becomes\npossible (which it wasn't before since proxies change it from\nsay 304 to 200).  This header would also eliminate problems of \nproxies canonicalizing headers.\n\nAn extra header (and not a short one) does add to overhead, but it\neliminates a lot of headaches here.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Draft standard rules (was RE: What is ContentLength?",
            "content": "Yaron Goland gets credit for the 'Frankenstein' metaphor.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "It isn't 'too late' to do something else BESIDES digest;\nit's just too late to leave Digest out. It's not all that much\nimplementation hair, really, and it will give us something\nthat we can hope that every server can offer and that every compliant\nclient will implement. So Digest fills a real need. If there's\nsome other authentication mechanism that's better, then offer\nit too.\n\n[[If anyone *else* is able to host HTTP-WG for three weeks, let\nme know.]]\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "> If at least one of the seven authors currently listed on the draft\n> will step up to the plate and start editing, then maybe Digest can\n> be resurrected.  I suggest the authors discuss this amongst themselves\n> and have a clean draft submitted by mid-January.\n> \n> If not, then either Basic AA will move back into the main spec and\n> Digest will be dropped, or I'll do the edit myself according to what\n> is actually implemented.  [Believe me, none of us want this to happen].\n> \n\nRoy,\n\nJim Gettys is now the lead editor of the authentication draft,\ndespite his absence the list of authors at the top of the draft\nhe recently submitted. Jim was charged with integrating Basic\nand Digest together; in our rush to get all of the issues integrated\ninto the main HTTP/1.1 spec, we didn't spend much time on the\nauthentication draft.\n\nYour willingness to volunteer to help with editing is greatly\nappreciated, and I'm sure we will coordinate with you to take\nadvantage of your willingness to help. But please don't go\noff on your own.\n\nThanks!\n\nLarry Masinter\n(as chair, HTTP working group)\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: ID:  Proxy autoconfi",
            "content": "On Tue, 8 Apr 1997, Yaron Goland wrote:\n\n> provisions for how to get updated proxy information. Rather than filling\n> DHCP up with endless URLs for every service, why not just use it as an\n> entry point?\n\nOne problem I see is that this approach would require yet another server\nto be available on a 7x24 basis (in addition to DHCP and DNS). This must\nbe OK, but the reliability issues should be considered carefully.\n\nDave MOrris\n\n\n\n"
        },
        {
            "subject": "Re: Digest mes",
            "content": "Note the temporary mailing list: ietf-http-wg@w3.org\n\nOn Fri, 19 Dec 1997, Scott Lawrence wrote:\n\n> \n>> John Franks:\n> \n>> It is the client who must be concerned about reused nonces to avoid\n>> a replay attack.  To avoid a replay attack the client would have to\n>> keep a data base of all previous nonces and make sure they are not \n>> reused.\n> \n> No - it only needs to keep the nonce it used for the outstanding\n> request; if that does not produce the correct digest then it is not\n> valid even if it would have been valid for some earlier request.  \n> \n\nMaybe I am not understanding you.  It seems to me that if a client,\nfor example, regularly places an order and receives and acknowledgement\nthere is a possible replay attack.  \n\nIf an intermediary attacker intercepts the order he can impersonate\nthe server and offer a nonce from a previous transaction.  The client\nthen submits the order which is grabbed by the attacker and then the\nattacker replays a previous acknowledgement with the dates changed.  A\nsimilar attack might involve sending a fake 304 when in fact the\nresource has changed.  This is what hash dates is supposed to prevent.\nAm I missing something.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "Yaron Goland wrote:\n\n> It seems a clarification is in order. As such I propose that the\n> following paragraph be added to the end of section 10.1:\n> \n> 1xx responses MAY be sent independently of requests. Clients MUST always\n> be able to accept 1xx responses and MUST ignore any 1xx response they do\n> not understand. In addition, proxies MUST pass through any 1xx response\n> they do not understand.\n\nI'm really uneasy about this. I can imagine an implementation that\njust doesn't read from the connection at all unless there's some\nrequest outstanding, and a server that sends a 1xx 'response' [sic] \nwithout a pending request might wind up with stuff stuck in buffers \nthat never gets read. \n\nSeems like it puts unnecessary requirements on implementations,\nand it doesn't seem justified. Just what ARE these unanticpated\n'responses'?\n\n\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 - Retryafter for 3xx as well as 503",
            "content": "At the HTTP WG at Memphis I was asked to explain why I proposed this (see\noriginal quoted at end).\n\n>- push caching\nThis was described in an excellent poster at WWW5 in Paris. With Netscape's\n305/306 Use Proxy proposal Retry-after: would allow the server some time\n(based on Content-length?) to push the content to a proxy if it needed\nupdating (which the server is best placed to know) before re-directing to\nthe proxy.\n\n>- switching to deferred (poss. multicast) transport (e.g. SMTP) under\nheavy server load\nI was thinking here of the server protecting itself from overload (e.g.\nduring flash crowds) by sending to a mailing list arranged to get the\ncontent out to localised mail archives. It could redirect different clients\nto pick up different URLs (posting ids) from their nearest archive (based\non some CGI with a rule set for allocating different ranges of client\naddresses to different archives). This would require the re-direct to be\ndeferred, hence the need for Retry-after:\nAs a second idea, say for instance, congestion control for reliable\nmulticast gets sorted out in the next year. Then to prevent the \"flash\ncrowd\" problem, you might want to re-direct all requests in time to\nstaggercasts of the content by specifying how long until the next scheduled\npush. The URL re-directed to would obviously be of a URL scheme which\nidentified it as a multicast which didn't need another request.\n\nI have no intention of implementing this, so take it as an idea. If an\nimplementer wants to pick it up... great.\n\nI just thought the sording of the Retry-after: header section in the spec.\nwas overly restrictive in defining the semantics just for one class of\ndeferred client access (503) when there was a whole potential other class\n(3xx).\n\nI would suggest it is a SHOULD which allows people to leave it out if they\nhave good reason, but as I say, I can't see it's difficult to implement in\na browser at all.\nA compromise might be to make this a MAY.\n\nBob\n\n>Date: Tue, 04 Mar 1997 17:02:19 +0100\n>To: http-wg@cuckoo.hpl.hp.com\n>From: Bob Briscoe <rbriscoe@jungle.bt.co.uk>\n>Subject: HTTP/1.1 - Retry-after for 3xx as well as 503?\n>\n>Issue for HTTP/1.1 before becomes draft standard?...\n>\n>It would be a simple change to the spec. and fairly tidy to implement in a\nbrowser to make it clear that the Retry-After header should apply to:\n>- 3xx Redirect class responses\n>- as well as 503 Service Unavailable\n>\n>This would make it much easier to implement versions of\n>- push caching\n>- switching to deferred (poss. multicast) transport (e.g. SMTP) under\nheavy server load\n>- etc.\n>\n>Bob\n____________________________________________________________________________\nBob Briscoe         http://www.labs.bt.com/people/briscorj/\n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "Well, most often, the resource is being retreived from an HTTP server\n(in our case) or a proxy.\n\nIf the proxy is down, the pac file is useless anyway.\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy\nRanger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Summary, HTTPWG meeting in Memphi",
            "content": "The HTTP working group is making good progress on resolving\nissues with an eye toward progressing HTTP/1.1 from \"Proposed\"\nto \"Draft\" status. We discussed 38 minor issues that were\nidentified on the mailing list over the last several months,\nand seemed close to resolving many of them. We also have four\nor five major issues that had originally been planned for\nHTTP/1.1 but were not finished in time.\n\nWe're hopeful that our work will be completed in time that\nthe last meeting of the working group will be in Munich.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: A common registry for MIME headers?",
            "content": "> On Sat, 29 Nov 1997, Alex Hopmann wrote:\n>\n> > That sounds like a good idea to me. I think Jacob Palme has already started\n> > such a registry for the email realm, and I suspect he might be happy to\n> > expand its scope.\n\nThe registry I am trying to start is only for header fields, not for HTML\nelements. However, a registry for HTML element might also be useful.\n\nMy proposal for the header field registry can be found at URL:\nftp://ftp.dsv.su.se/users/jpalme/draft-ietf-drums-MHRegistry-03.txt\n\nAn example of how the header field registry could look like can be found\nat URL:\nhttp://www.dsv.su.se/~jpalme/ietf/iana-header-field-registry.html\n\nSo far, I have only produced an incomplete list of e-mail headers,\nwhich would be the basis of developing the starting set for the\nheader-field registry. That incomplete list can be found at URL:\nftp://ds.internic.net/rfc/rfc2076.txt, I plan to submit a revised\nversion shortly, but still only containing e-mail and netnews,\nnot HTTP header fields.\n\nIf you reply to this message with comments which I should read, then\ninclude me in the recipient list, since I am not a subscriber to\nthe http-wg mailing list. There is also a specific mailing list\nfor the header registry issue. This mailing list is presently not\nactive, but could of course become active and be used for a discussion\nof header field registry issues, and possibly also HTML tag registry\nissues. The list is named \"mail-headers\", since initially we planned\na registry for only e-mail header fields, but now we believe that\nthe internet community is best served by a common registry for\ne-mail, netnews and http header fields.\n\nInformation about the mailing list:\n\nSend contributions to the \"mail-headers@segate.sunet.se\" mailing list. You\ncan subscribe to the list by sending a message to\n\"listserv@segate.sunet.se\" containing in the text a line with \"subscribe\nmail-headers \" followed by our name (not your e-mail address), and\nunsubscribe with a message \"unsubscribe mail-headers\".\n\nArchives of this list will be (but are not yet) available\nby anonymous FTP from\n   ftp://segate.sunet.se/lists/mail-headers/\n\nby HTTP from\n   http://segate.sunet.se/archives/mail-headers.html\n\nby E-MAIL\n   send a message to\n   LISTSERV@SEGATE.SUNET.SE with the text \"INDEX mail-headers\"\n   to get a list of the archive files, and then a new message\n   \"GET <file name>\" to retrieve the archive files.\n\nThe FTP and E-MAIL archives are best if you want to retrieve\nall messages during a month or more, while the HTTP archives\nare better if you want to browse and find particular messages\nto download.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "It would make us broken because of the following scenario:\n\nA server recieves a GET from a client, the request is marked HTTP/1.1. The\nserver now knows it is dealing with a 1.1 client. So, no problem, according\nto the HTTP/1.1 Draft Standard a 1.1 client MUST honor the content-base\nheader. So the server sends down a content-base expecting that the body will\nbe interpreted using the content-base header to resolve relative URIs. Of\ncourse now the wrong thing will happen, the IE client will NOT honor the\ncontent-base header because it is based on the proposed standard where\ncontent-base is a MAY and the whole situation falls apart.\n\nSo therefore changing the MAY to MUST breaks HTTP/1.1 proposed standard\ncompliant implementations which choose to honor the may, as is their right,\nby ignoring the header.\n\nYaron\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Thursday, January 08, 1998 12:56 PM\n> To:Yaron Goland\n> Cc:jg@pa.dec.com; Henrik Frystyk Nielsen; Foteos Macrides;\n> koen@win.tue.nl; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:RE: MUST use Content-Base\n> \n> \n> >  From: Yaron Goland <yarong@microsoft.com>\n> >  Date: Thu, 8 Jan 1998 11:45:59 -0800 \n> >  To: \"'jg@pa.dec.com'\" <jg@pa.dec.com>\n> >  Cc: Henrik Frystyk Nielsen <frystyk@w3.org>,\n> >          Foteos Macrides\n> >   <MACRIDES@SCI.WFBR.EDU>, koen@win.tue.nl,\n> >          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> >  Subject: RE: MUST use Content-Base\n> >  \n> >  To be clear, I fully understand that RFC 2068 was a proposed, not\n> draft,\n> >  standard. As such it is subject to changes and bug fixes. The point I\n> am\n> >  trying to make is that we should go to great pains to ensure that\n> nothing we\n> >  do in going from proposed to draft will render proposed based\n> >  implementations unusable.\n> \n> Sure.  That is the point of IETF standards track processes...\n> \n> >  \n> >  If we raise content-base to a MUST then we make my client unusable. A\n> >  server, seeing my client's claim to be HTTP/1.1, will think it can rely\n> upon\n> >  sending a content-base and having it read and processed. That will not\n> be\n> >  the case with my client. My client will ignore the header and the\n> result\n> >  will be something other than what the server expected. Thus changing\n> >  content-base from may to MUST results in my client doing the \"wrong\"\n> thing\n> >  vis a vie the standard. Hence my client has been defined, by fiat, as\n> >  broken.\n> >  \n> \n> If what you do is ignore the header, then you are no worse off than any \n> HTTP/1.0 client.  I don't see this as a very serious problem, as a result.\n> If you implemented it, and did something different, then we'd have a\n> bigger\n> problem.\n> \n> But we aren't declaring anyone's implementation broken here; we're\n> declaring \n> the specification broken here.  The spec wasn't clear that the intent was \n> that everyone implement Content Base.  So I disagree with the assertion \n> that you are \"broken\".  The fault isn't in the code, it it's the\n> specification \n> that's busted.  Not the first time, and (unfortunately) probably not\n> the last time.\n> \n> >  To repeat, if we are to convince companies to implement proposed\n> standards\n> >  then we must go as far as possible to ensure that moving to draft\n> doesn't\n> >  render their implementation broken. Sometime, I understand, this is\n> totally\n> >  unavoidable. That is the risk of implementing a proposed standard. But\n> this\n> >  case is not one of those instances.\n> > \n> \n> I'd like to understand the consequences of different possible courses of\n> action.\n>  \n> >  Yaron\n> >  \n> >  PS Is it appropriate to get into a conversation about why content-base\n> is\n> >  just a bad idea, from a design point of view? There was a reason why we\n> >  didn't implement it.\n> >  \n> \n> Yes, though it sure would have been better to raise it as an issue last\n> year.\n> We wouldn't be having this mail exchange if it had been.\n> \n> If the right solution is to remove it from the spec, (and have an\n> independent\n> document for a replacement for Content-Base), that is another possible\n> course\n> of action, for example.  Let's explore the possible solutions...\n> \n> >  > -----Original Message-----\n> >  > From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> >  > Sent:Thursday, January 08, 1998 7:39 AM\n> >  > To:Yaron Goland\n> >  > Cc:Henrik Frystyk Nielsen; Foteos Macrides; koen@win.tue.nl;\n> >  > http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> >  > Subject:RE: MUST use Content-Base\n> >  > \n> >  > \n> >  > >  From: Yaron Goland <yarong@microsoft.com>\n> >  > >  Date: Wed, 7 Jan 1998 22:00:42 -0800 \n> >  > >  To: \"'Henrik Frystyk Nielsen'\" <frystyk@w3.org>,\n> >  > >          Foteos Macrides\n> >  > >   <MACRIDES@SCI.WFBR.EDU>, koen@win.tue.nl\n> >  > >  Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> >  > >  Subject: RE: MUST use Content-Base\n> >  > >  \n> >  > >  Nothing like defining current implementations as broken to really\n> get\n> >  > >  companies excited about the open standards process.\n> >  > >  \n> >  > >  \"Be standards compliant\" I said. \"It's an RFC\" I said.\n> Sigh...\n> >  > >  \n> >  > >  Yaron\n> >  > >  \n> >  > \n> >  > Yaron,\n> >  > \n> >  > You conformed as well as you knew how to RFC 2068.  No one said that\n> you \n> >  > didn't.  You can with a straight face continue to claim compliance\n> with \n> >  > 2068 (and I, for one, will defend your claims for Content-Base).\n> Please \n> >  > also read the IETF definition of what a proposed standard is.  We are\n> not\n> >  > declaring anyone broken (except the specification, that is, which is\n> >  > broken).\n> >  > \n> >  > Unfortunately, 2068 has an ambiguity.  You are far from alone in\n> believing\n> >  > \n> >  > it to be capable of multiple interpretations by well minded people\n> with \n> >  > the best of intentions. (I certainly believe it capable of multiple \n> >  > interpretations).  Other people have the other interpretation...  Are\n> they\n> >  > \n> >  > supposed to not be \"standards complient\"? Or are you saying that the\n> >  > Microsoft implemetation is the only implementation that matters?  I\n> >  > think that Netscape might have some other opinion here, for example.\n> >  > \n> >  > In the future you'll be complient to RFC 2XXX (the sucessor to 2068)\n> >  > (which \n> >  > one way or the other won't have the ambiguity).  And the process\n> ensures \n> >  > that compliance to RFC 2XXX, as a draft standard, should mean you\n> remain \n> >  > compliant with 2068.\n> >  > \n> >  > This is why there is interoperability testing; to find and fix\n> problems \n> >  > in the specification, and end up with multiple INTEROPERABLE\n> >  > implementations \n> >  > that can be implemented from the specification successfully.  Given\n> the \n> >  > multiple possible interpretations, and multiple implementations, it\n> isn't \n> >  > always possible to fix a problem without someone having to change an \n> >  > implementation.\n> >  > \n> >  > What we're trying to figure out here is what the right solution is to\n> >  > remove \n> >  > the abiguity, with minimum short term AND long term impact. My\n> previous \n> >  > mail stated a PERSONAL opinion of what should happen; it is not a\n> \"done \n> >  > deal\" (none has been made as yet, though I'll take one eventually if\n> there\n> >  > \n> >  > is no \"rough consensus\", so that we make progress in a timely\n> fashion). \n> >  > This is why I often keep quiet until relatively late in discussions\n> on a \n> >  > topic in HTTP; I take the responsibility of trying to reflect the\n> working \n> >  > group \"rough consensus\" very seriously.  I do not act against the\n> working \n> >  > group opinions.\n> >  > \n> >  > This having been said, we (the working group) need to figure out what\n> the \n> >  > right solution is.  So:\n> >  > \n> >  > 1) data on what current implementations do is very useful.\n> >  > 2) understanding what options we have is useful\n> >  > 3) understanding what the consequences of following each options is\n> >  > useful.\n> >  > \n> >  > Hand wringing, and worrying about whether one is \"compliant\" is not\n> >  > useful.  \n> >  > Note that understanding the consequences of a change are covered in\n> 3.\n> >  > \n> >  > I'd like to hear from others who can help on these points,\n> particularly\n> >  > those who've implemented it.\n> >  > \n> >  > - Jim\n> >  > \n> >  > \n> >  > --\n> >  > Jim Gettys\n> >  > Industry Standards and Consortia\n> >  > Digital Equipment Corporation\n> >  > Visting Scientist, World Wide Web Consortium, M.I.T.\n> >  > http://www.w3.org/People/Gettys/\n> >  > jg@w3.org, jg@pa.dec.com\n> --\n> Jim Gettys\n> Industry Standards and Consortia\n> Digital Equipment Corporation\n> Visting Scientist, World Wide Web Consortium, M.I.T.\n> http://www.w3.org/People/Gettys/\n> jg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "draft minutes, HTTPWG meetings April ",
            "content": "Minutes reported by Ted Hardie. Please send corrections,\nadditions, or requests for clarifications to me, so that\nI can incorporate them in the final minutes.\n\nNote that there are a large number of \"action items\"\nthat various people committed to do, and that we need\nto see these items completed quickly, since we also still\nhave a large number of unresolved issues.\n\nThanks,\n\nLarry\n\nMinutes, HTTP Working Group, April 7, 1997, Memphis, TN.\n\nThe group concentrated on closing outstanding issues.  Jim Gettys led\na point-by-point review of the issues list\n(http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/).\n   \nIn the discussion, the following issues seemed to have sufficient\nconsensus in the meeting that \"last call\" will be sent to the \nmailing list for each of them:\n   \"chunked encoding\" clarification\n   the caching issue raised by Dingle\n   accept-charset wildcarding\n   proxy authorization\n   the FIN-WAIT2 issue\n   Jeff Mogul's resolution for connection\n   and the impermissability of CRLF in a quoted string\n\nJeff Mogul's drafts on HTTP versions and proxy-revalidate will also go\nto last call in their current forms.\n\nThe following issues are resolved in principle, but require language\nto cover the needed editorial changes or clarifications:\n\n-- Content-Disposition will be added to the Appendix, as one\n   of a group of common MIME headers about which implementors should\n   be aware; Koen Holtman will draft the addition.\n\n-- The draft by Gettys et al on who should close the connection\n   represents valuable information and advice, but needs some\n   continued development.  A new version is expected shortly.\n\n-- Richard Gray will provide language on sending 100 series response\n   codes with no date headers; Jim Gettys will then fold these into\n   the main document.  \n\n-- Jeff Mogul will draft an explanation of how to cache resources\n   containing a \"?\" in the context of HTTP 1.1.\n\n-- Paul Leach will provide an examination of the use of byte-ranges\n   with PUT, with the understanding that there was mild applause in\n   the working group for eliminating the possibility of PUT with\n   byte-ranges.\n\n-- Ted Hardie will draft a missive on how to respond to a request for\n   a range of bytes for a resource which contains none of the bytes in\n   the range.\n\n-- Roy Fielding's solution for how to define max-age for responses is\n   accepted in principle, subject to minor wording changes to be\n   worked out between Jeff Mogul and Roy.\n\n-- Larry Masinter will author an implementation note on the\n   desirability of including charsets in responses.\n\n-- Paul Leach will pen a note on a proxy being able to serve a\n   resource with a content-length when it received that resource with\n   chunked-encoding.\n\n-- Scott Lawrence will provide a sentence or two which will cover the\n   possibility of leading zeros in a content-length.\n\n-- Koen Holtman\n   will draft a clarification that a qvalue of 0.0 means \"Don't send\n   me this.\"\n\n-- After consulting the language in use in RFC822, Koen Holtman will\n   also draft an editorial correction on the use of LWS in headers\n   like VIA.\n\n-- Jeff Mogul will respond to syntactic changes proposed by Koen\n   Holtman for the HTTP-warning draft; if appropriate, a new version\n   of the draft will appear.\n\n-- The Hit-metering draft will go last call after Jeff checks it for\n   one last set of editorial revisions.\n\nNot all issues reached closure.  For those which did not, particular\nindividuals may have accepted responsibility for providing next steps.\nIn all cases, however, input from others is solicited.\n\nJosh Cohen will take responsibility for re-raising the issue of using an\nequality check for Date If Modified on the list.\n\nWhile there is general agreement that 305 should be limited to use by\norigin servers, the proposal by Josh Cohen for 306 needs both concrete\nlanguage and further discussion of the implications of a proxy-managed\nproxy redirect.  Josh Cohen will provide a draft explaining Netscape's\nvision for 306.\n\nThe issue of how to handle age calculations remains contentious; Jim\nGettys has suggested that a small group interested in the problem should\nget together and work out a solution.  Those interested in being part of\nthat group should volunteer on the list; implementors of proxy caches\nare particularly encouraged to share their experience.  To help minimize\nthis issue, Jim will draft language which strongly encourages proxies to\nrun with synchronized clocks.\n\nJeff and Henrik raised the issue of inappropriate client behavior on\nreceipt of a content-encoding that it does not understand.  Henrik will\ndraft a document a proposal to fix the problem; since this touches on\nthe difference between different content- header types, careful review\nof the proposal by the working group will be needed.\n\nJosh and Henrik will work off-line on the question of what a client\nshould use in the HOST: header when it has a url with a host part that\nis not a fully qualified domain name.  Regardless of the outcome of that\ndiscussion, Paul Hoffman will draft language proposing that the client\nbe allowed to chop the host part out of the URL and use that.\nDiscussion of the two proposals should consider in particular the\ndifficulties faced by clients which would have to resolve hosts into\nfully-qualified domains and the difficulties faced by proxies without\naccess to fqdns as identifiers.\n\nHenry Sanders will check Microsoft's implementation of chunked encoding\nto ascertain whether the Digest Authentication headers can be placed in\nthe chunked encoding trailer.  If it does and there is no contrary\nimplementation, the issue is closed; if it does not, the working group\nwill need to examine the interoperability issues of allowing those\nheaders in the trailer.\n\nThe question of sending a Retry-After with 503 and 3xx response codes\nhas been tabled, pending Mr. Briscoe's providing a compelling reason for\nallowing the Retry-After.\n\nJeff and Henrik will write up a short draft on how to optimize returns\non range requests by removing meta-data which is not needed to assure\nthe client that the range returned is part of the correct resource.\nHenry Sanders will review based on implementation experience at\nMicrosoft.\n\nThe issue raised by Koen Holtman regarding the asymetry in matching\nrule for accept language in 14.4 produced a great deal of debate on the\nappropriate matching semantics.  Dirk van Gulik will identify the\nappropriate ISO documents which indicate why matching semantics based on\nwildcards will not handle all cases.  Since HTTP's use of language tags\nis derived from RFC 1766, changes needed in HTTP might, in fact, reflect\nchanges needed in RFC 1766.  Exactly how much of the matching algorithm\nbelongs in the protocol is not clear, and further discussion will be\nneeded on the mailing list.\n\nDiscussion of Koen Holtman's draft on Safe Post and Dave Morris' draft\non UA-hint focused on two issues: whether safe post and history list\nmanipulation belonged in the same mechanism and which of the two\nproposals best handled the issue.  No consensus emerged, but interest in\nthis in certain user communities (Banks etc.) is high enough to warrant\nfurther work.  Some indications were given progress might be faster\nafter splitting off the history list issue from the safe post issues\nrequired for i18n, but, again, no real consensus emerged.\n\nDan Connoly presented the new PEP draft.  Three major issues were\nraised: PEP can be overkill for some small, lightweight extensions; the\ncost of discovering whether or not a server understands PEP and a\nparticular extension can be significant; and the draft's language for\nhow to handle these extensions through HTTP 1.0 proxies does not seem\nadequate.  Koen Holtman suggests that the language in the hit-metering\ndraft is a good model for how to handle PEP with 1.0 Proxies.  Dan will\nlook at that language and the other objections; a new draft is expected.\n\nKoen Holtman presented a portion of the TCN drafts; time constraints\ndid not allow the group to consider the full set.  The base portion of\nTCN is now stable, and there are server side (Apache) and client side\n(Tango) implementations.  Preliminary indications from Dirk's\nexperience as a server implementor are that this is a very successful\nmechanism for negotiation on things like image format, but that\nlanguage and character set encoding need more implementation before\nthey can be fully evaluated.  Yaron Golan (not speaking officially for\nMicrosoft) noted that he did not feel that TCN was rich enough for the\napplications he envisioned; he felt that script-based solutions would\nbe required for any meaningful content negotiation, and these\nsolutions would both enable better server/author control and would\nshift the computational locus to the client.  Scott Lawrence and Ted\nHardie spoke in favor of the TCN framework, noting that it was richer\nand leaner than Accept headers while being fully interoperable.  A\ncounter-proposal by Mr. Briscoe will be sent to the list in the form\nof a paper.  The chair ruled after discussion that the working group\nis not ready for last call on these drafts.\n\nDave Kristol presented his new Cookie draft and discussed, briefly,\nthe controversy which has surrounded the subject. (\"In the case of RFC\n2109, it was a Request For Comments, and it got some.\")  Two classes\nof problems were raised: interoperability problems and objections to\nthe default settings required by 2109.  The interoperability problems\nare being addressed in the new draft.  Those objecting have been\ninvited to present alternative proposals.  Dan Jaye, of Engage\nTechnologies, has proposed one solution, but it requires a public key\ninfrastructure that would delay deployment too long.  His work and\nother solutions for resolving the tension between user privacy and\ntraffic can go forward independently, but, based on Area Director\ninput, we should not repeal what we have in favor of a technology\nwhich will take a year or more to put in place.  Interested parties\nshould note that the W3C is putting together a forum to address this\nissue.\n\nThe session closed with the Chair noting that the main work of the\ngroup should be progressing HTTP 1.1 from Proposed to Draft Standard\nand that he hopes to close the working group by the next IETF meeting\nin Munich.  Other groups working on specific problems may be needed,\nbut he believes we have reached a stage where they can operate\nindependently.\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "At 17:35 01/10/98 -0800, Yaron Goland wrote:\n>A server recieves a GET from a client, the request is marked HTTP/1.1. The\n>server now knows it is dealing with a 1.1 client. So, no problem, according\n>to the HTTP/1.1 Draft Standard a 1.1 client MUST honor the content-base\n>header. So the server sends down a content-base expecting that the body will\n>be interpreted using the content-base header to resolve relative URIs. Of\n>course now the wrong thing will happen, the IE client will NOT honor the\n>content-base header because it is based on the proposed standard where\n>content-base is a MAY and the whole situation falls apart.\n\nYes, this is no surprise and is yet another example showing that version\nnumbers don't work on features. Content-base is always at risk of not\nworking, even if all HTTP/1.1 applications understood it:\n\nA HTTP/1.1 caching proxy sends a request to an HTTP/1.1\nserver, gets back an entity with a content-base and\nputs it into the cache. Now an HTTP/1.0 client comes\nalong and gets the entity from the cache, ignores the\ncontent-base and the links will break.\n\nYou can substitute content-base with content-encoding, content-type,\ncontent-language, and pretty much any other characteristic of the resource.\n\nThe real problem is that sometimes this is OK and sometimes it's not - it\ndepends on the type of client asking. In the case of content-base, I don't\ncare if my client doesn't parse the darn thing.\n\nWe went through a lot of hickups solving this for content-encoding but I\ndoubt that anybody is willing to do the same for content-base. To me, this\nleaves two  ways of dealing with the problem:\n\n  1) leave it as is admitting that this was an evolutionary mistake\n  2) try an patch it and mention that \"your milage will vary\"\n\nI think I read from the discussion that people see a (limited) need for the\nfeature, so maybe the right thing is to use a SHOULD and then include a\nnote like this:\n\nNote: Many applications based on RFC 2068 or\nprevious versions of HTTP ignore the content-base\nheader field when parsing relative URIs in\ndocuments.\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "I can live with the SHOULD followed by a note.\n\nYaron (Declare Victory and Go Home)\n\n> -----Original Message-----\n> From:Henrik Frystyk Nielsen [SMTP:frystyk@w3.org]\n> Sent:Sunday, January 11, 1998 10:27 PM\n> To:Yaron Goland; 'jg@pa.dec.com'\n> Cc:Foteos Macrides; koen@win.tue.nl;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:RE: MUST use Content-Base\n> \n> At 17:35 01/10/98 -0800, Yaron Goland wrote:\n> >A server recieves a GET from a client, the request is marked HTTP/1.1.\n> The\n> >server now knows it is dealing with a 1.1 client. So, no problem,\n> according\n> >to the HTTP/1.1 Draft Standard a 1.1 client MUST honor the content-base\n> >header. So the server sends down a content-base expecting that the body\n> will\n> >be interpreted using the content-base header to resolve relative URIs. Of\n> >course now the wrong thing will happen, the IE client will NOT honor the\n> >content-base header because it is based on the proposed standard where\n> >content-base is a MAY and the whole situation falls apart.\n> \n> Yes, this is no surprise and is yet another example showing that version\n> numbers don't work on features. Content-base is always at risk of not\n> working, even if all HTTP/1.1 applications understood it:\n> \n> A HTTP/1.1 caching proxy sends a request to an HTTP/1.1\n> server, gets back an entity with a content-base and\n> puts it into the cache. Now an HTTP/1.0 client comes\n> along and gets the entity from the cache, ignores the\n> content-base and the links will break.\n> \n> You can substitute content-base with content-encoding, content-type,\n> content-language, and pretty much any other characteristic of the\n> resource.\n> \n> The real problem is that sometimes this is OK and sometimes it's not - it\n> depends on the type of client asking. In the case of content-base, I don't\n> care if my client doesn't parse the darn thing.\n> \n> We went through a lot of hickups solving this for content-encoding but I\n> doubt that anybody is willing to do the same for content-base. To me, this\n> leaves two  ways of dealing with the problem:\n> \n>   1) leave it as is admitting that this was an evolutionary mistake\n>   2) try an patch it and mention that \"your milage will vary\"\n> \n> I think I read from the discussion that people see a (limited) need for\n> the\n> feature, so maybe the right thing is to use a SHOULD and then include a\n> note like this:\n> \n> Note: Many applications based on RFC 2068 or\n> previous versions of HTTP ignore the content-base\n> header field when parsing relative URIs in\n> documents.\n> \n> Henrik\n> --\n> Henrik Frystyk Nielsen,\n> World Wide Web Consortium\n> http://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "On Mon, 12 Jan 1998, Henrik Frystyk Nielsen wrote:\n\n> I think I read from the discussion that people see a (limited) need for the\n> feature, so maybe the right thing is to use a SHOULD and then include a\n> note like this:\n> \n> Note: Many applications based on RFC 2068 or\n> previous versions of HTTP ignore the content-base\n> header field when parsing relative URIs in\n> documents.\n\nSome note of that sort should certainly be included, but I still think\nthat this needs to be a MUST or be omitted.  Granted, all implementations\nearlier than 2068 and some (including important ones) based on 2068 will\nnot do this.  The point is that it is a good thing (IMHO) to have in the\nprotocol in the future and if we make it a must then the day will come\nwhen it can be assumed to work more or less universally; if we do not make\nit a MUST then that day will not come, and the protocol feature is\nuseless.  I was most carefull in my original post - this should either be\na MUST or it should be removed altogether; I don't think that compromise\nis helpfull here.\n\n\n\n"
        },
        {
            "subject": "Multiple ContentLocation header",
            "content": "The MHTML group has decided to allow more than one Content-Location\nin the same message heading. This can be used, for example, if the\nsame object can be located by several different URLs, and you want\nto specify all of them. A particular case is the following\n\n-- border --\nContent-Location: http://www.dsv.su.se/a.gif\nContent-Location: http://www.dsv.su.se/images/a.gif\n\n-- border --\n<HTML>\n<img src=\"http://www.dsv.su.se/a.gif\">\n\n-- border --\n<HTML>\n<img src=\"http://www.dsv.su.se/images/a.gif\">\n\n-- border --\n\nSupposes a.gif and images/a.gif actually refer to the same image.\nAnd suppose the two HTML objects above have digital seals on them.\nThen, if you did not allow multiple Content-Location headers in\nthe first body part, you would have to send the image twice, or\nyou would have to modify the HTML invalidating the digital seals!\n\nThe reason I am sending this message to the http working group\nmailing list, is that this decision in the MHTML working group\nmay modify the HTTP spec, which presently says\n\n  14.15Content-Location\n\n  The Content-Location entity-header field MAY be used to supply the resource\n  location for the entity enclosed in the message  when that entity is\n  accessible from a location separate from the requested resource's URI.. In\n  the case where a resource has multiple entities associated with it, and\n  those entities actually have separate locations by which they might be\n  individually accessed, the server should provide a Content-Location for the\n  particular variant which is returned. In addition, a server SHOULD provide\n  a Content-Location for the resource corresponding to the response entity.\n\nThe MHTML group further has decided, that if there is more than one\nContent-Location in the same message heading, then neither of them can\nbe used to establish a base (unless both specify the same base).\n\nI suggest that the editors of the HTTP draft go through all cases where\nContent-Location is mentioned in the HTTP draft to check that it agrees\nwith the above decisions in the MHTML group.\n\nPlease do not send replies to this message only to the\nhttp-wg@cuckoo.hpl.hp.com mailing list, since I will not\nthen get them, I am not a member of that mailing list.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Could you enlighten us as to why you would want to specify multiple names\nfor the same object?  It would help us understand if this is applicable\nto HTTP as well.\nThanks,\nJim Gettys\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "    Supposes a.gif and images/a.gif actually refer to the same image.\n    And suppose the two HTML objects above have digital seals on them.\n    Then, if you did not allow multiple Content-Location headers in\n    the first body part, you would have to send the image twice, or\n    you would have to modify the HTML invalidating the digital seals!\n    \nI have been having discussions with some of the people behind\nthe \"DRP\" protocol proposal\nhttp://www.w3.org/TR/NOTE-drp-19970825.html\nabout how to extend HTTP to support \"duplicate suppression\".\nUsing multiple Content-Location headers is an interesting approach,\nbut it might not entirely solve the problem.  (For example, some\nthings, such as the Netscape logo, are duplicated thousands of\ntimes in the Web; one could not list these all in a Content-Location\nheader).\n\nAt any rate, I suggest that this is an important issue to solve,\nbut we are not ready to embed any specific solution in the HTTP/1.1\nstandard.  And the solution(s) for MIME and HTTP might be different,\nsince these are very different protocols.  (MIME, for example, \ndoes not have to worry about interactions with proxy caches,\npartial retrievals, etc.)\n\n    The reason I am sending this message to the http working group\n    mailing list, is that this decision in the MHTML working group\n    may modify the HTTP spec, which presently says\n\nHTTP is not MIME.  The MHTML working group shouldn't be thinking\nabout \"modifying the HTTP spec\", especially since we are trying\nvery hard to push HTTP/1.1 to Proposed Standard status ASAP.  We\ncan make changes to HTTP/1.1 that are required to satisfy the IESG\nthat it actually works as advertised; we can't keep making changes\nto keep track of new features being discussed in other working groups.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "I was skimming the minutes of the IPng WG from the Washington, DC IETF\nmeeting, and found this:\n\n[Start of excerpt]\n    IPv6 Addresses in URL's / B. Carpenter\n    --------------------------------------\n    \n    Design team meet in the bar a few nights ago.\n    \n    Need numeric address in URL's for emergency operations (or robotic apps).\n    \n    Colons break URL parsers \"hostname\" syntax\n    \n    Proposals:\n    \n      http://--ABCD-EF12-192.100.1.2.ipv6:80/\n      http://[ABCD:EF12:192.100.1.2]:80/\n    \n    Issue:  Should IPng w.g. reopen the \"colon\" notation?\n    \n    Heated discussion.  Most comments that this is stupid, we should\n    not reopen IPv6 text notation.  Long discussion.  Issue seems to be\n    that many parsers that take URL's are very limited.\n\n    No one was in favor of changing current text representation.\n    Extremely strong consensus!\n\n    It was noted that the issue is probably only relevant for complete\n    web browsers (e.g., Netscape, Microsoft, etc.), not all other\n    applications that use URL's.  If the complete web browsers can be\n    changed it is very likely to be sufficient.  Recommend that the\n    primary preferred syntax for IPv6 addresses in URL's be:\n    \n      http://[ABCD.EF01::2345:6789]:80/\n    \n    The IPv6 address should be enclosed in brackets.  URL parsers that\n    can not support this notation can either support the proposed\n    alternative syntax:\n    \n      http://--ABCD-EF12-192.100.1.2.ipv6:80/\n    \n    or not allow IPv6 addresses to be entered directly.\n\n[End of excerpt]\n\nI'm not sure if this is really an \"issue\" for HTTP/1.1, but I suspect\nthat the IESG will want to be sure that HTTP/1.1 syntax is compatible\nwith IPv6, and if there are conflicts, we should probably make sure\nthey are addressed.  Or make an explicit statement that we are not\ngoing to address them in this version of the protocol (and why not).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">     Supposes a.gif and images/a.gif actually refer to the same image.\n...\n> I have been having discussions with some of the people behind\n> the \"DRP\" protocol proposal\n> http://www.w3.org/TR/NOTE-drp-19970825.html\n> about how to extend HTTP to support \"duplicate suppression\".\n...\n> At any rate, I suggest that this is an important issue to solve,\n\nSorry if I am missing something, but is duplicate suppression really a\nbig issue ? And for what reason ?\n\nLast time I checked (which was perhaps in summer '96, and looking at\ncache performance) on our proxy, the amount of duplicate objects\nwas very small -- at least in size, I think it was maybe between\n1 and 2% of the total cache size.\n\nCheers\nLuigi\n-----------------------------+--------------------------------------\nLuigi Rizzo                  |  Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it    |  Universita' di Pisa\ntel: +39-50-568533           |  via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522           |  http://www.iet.unipi.it/~luigi/\n_____________________________|______________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "A quick check of our search engine software shows that most\nof our spidering will also break with these URLs.  That's\nnot as big a deal as the \"complete browsers\" breaking, from\nour perspective, but I do think it indicates that these\nparsers are built into a lot more than they thought.  I\nwould bet, for example, that it would complete break some\nof the firmware \"browsers\" being built into phones by the\npocketnet people.\nregards,\nTed Hardie\nNASA NIC\n\n> Our man Mogul writes:\n>     It was noted that the issue is probably only relevant for complete\n>     web browsers (e.g., Netscape, Microsoft, etc.), not all other\n>     applications that use URL's.  If the complete web browsers can be\n>     changed it is very likely to be sufficient.  Recommend that the\n>     primary preferred syntax for IPv6 addresses in URL's be:\n>     \n>       http://[ABCD.EF01::2345:6789]:80/\n>     \n>     The IPv6 address should be enclosed in brackets.  URL parsers that\n>     can not support this notation can either support the proposed\n>     alternative syntax:\n>     \n>       http://--ABCD-EF12-192.100.1.2.ipv6:80/\n>     \n>     or not allow IPv6 addresses to be entered directly.\n> \n> [End of excerpt]\n> \n> I'm not sure if this is really an \"issue\" for HTTP/1.1, but I suspect\n> that the IESG will want to be sure that HTTP/1.1 syntax is compatible\n> with IPv6, and if there are conflicts, we should probably make sure\n> they are addressed.  Or make an explicit statement that we are not\n> going to address them in this version of the protocol (and why not).\n> \n> -Jeff\n> \n\n\n\n"
        },
        {
            "subject": "Re: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "Jeffrey Mogul:\n>\n>I was skimming the minutes of the IPng WG from the Washington, DC IETF\n>meeting, and found this:\n>\n>[Start of excerpt]\n[...deleted...]\n>    changed it is very likely to be sufficient.  Recommend that the\n>    primary preferred syntax for IPv6 addresses in URL's be:\n>    \n>      http://[ABCD.EF01::2345:6789]:80/\n>    \n>    The IPv6 address should be enclosed in brackets.  URL parsers that\n>    can not support this notation can either support the proposed\n>    alternative syntax:\n>    \n>      http://--ABCD-EF12-192.100.1.2.ipv6:80/\n>    \n>    or not allow IPv6 addresses to be entered directly.\n>\n>[End of excerpt]\n>\n>I'm not sure if this is really an \"issue\" for HTTP/1.1, but I suspect\n>that the IESG will want to be sure that HTTP/1.1 syntax is compatible\n>with IPv6, and if there are conflicts, we should probably make sure\n>they are addressed.  Or make an explicit statement that we are not\n>going to address them in this version of the protocol (and why not).\n\nI remember that we had a discussion about ipv6 urls some time ago on\nwww-talk, see for example the thread starting with\n\nhttp://lists.w3.org/Archives/Public/www-talk/1996JulAug/0093.html\n\nIt was noted in that thread that [ and ] were illegal in URLs.\n\nWe did not really reach a consensus, but one intermediate conclusion\nwas that a notation like\n\n http://1080::8:800:200C:417A.8000/blah\n\nwith the . separating the address from the port number, would do the\ntrick.  I see however that you quoted some ipv6 addresses which have a\n. in them above, I think we assumed at the time that the ipv6 notation\nwould use : only. \n\nAnyway, this is probably an issue between the ipv6 people and whoever\nfeels responsible for maintaining URLs (Larry(?)).  I am just\nproviding some pointers.  I think we should at least add a note to the\n1.1 spec to warn implementers of this issue.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "Ill also add this to the http-ext issues for the extensions documents..\n\n--\nJosh Cohen <joshco@microsoft.com>\nProgram Manager - Internet Technologies \n\n> -----Original Message-----\n> From: koen@win.tue.nl [mailto:koen@win.tue.nl]\n> Sent: Monday, January 12, 1998 12:52 PM\n> To: mogul@pa.dec.com\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Re: Heads-up re: IPv6 addresses in URLs (from IPng-WG minutes)\n> \n> \n> Jeffrey Mogul:\n> >\n> >I was skimming the minutes of the IPng WG from the Washington, DC IETF\n> >meeting, and found this:\n> >\n> >[Start of excerpt]\n> [...deleted...]\n> >    changed it is very likely to be sufficient.  Recommend that the\n> >    primary preferred syntax for IPv6 addresses in URL's be:\n> >    \n> >      http://[ABCD.EF01::2345:6789]:80/\n> >    \n> >    The IPv6 address should be enclosed in brackets.  URL parsers that\n> >    can not support this notation can either support the proposed\n> >    alternative syntax:\n> >    \n> >      http://--ABCD-EF12-192.100.1.2.ipv6:80/\n> >    \n> >    or not allow IPv6 addresses to be entered directly.\n> >\n> >[End of excerpt]\n> >\n> >I'm not sure if this is really an \"issue\" for HTTP/1.1, but I suspect\n> >that the IESG will want to be sure that HTTP/1.1 syntax is compatible\n> >with IPv6, and if there are conflicts, we should probably make sure\n> >they are addressed.  Or make an explicit statement that we are not\n> >going to address them in this version of the protocol (and why not).\n> \n> I remember that we had a discussion about ipv6 urls some time ago on\n> www-talk, see for example the thread starting with\n> \nhttp://lists.w3.org/Archives/Public/www-talk/1996JulAug/0093.html\n\nIt was noted in that thread that [ and ] were illegal in URLs.\n\nWe did not really reach a consensus, but one intermediate conclusion\nwas that a notation like\n\n http://1080::8:800:200C:417A.8000/blah\n\nwith the . separating the address from the port number, would do the\ntrick.  I see however that you quoted some ipv6 addresses which have a\n. in them above, I think we assumed at the time that the ipv6 notation\nwould use : only. \n\nAnyway, this is probably an issue between the ipv6 people and whoever\nfeels responsible for maintaining URLs (Larry(?)).  I am just\nproviding some pointers.  I think we should at least add a note to the\n1.1 spec to warn implementers of this issue.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "    Sorry if I am missing something, but is duplicate suppression really a\n    big issue ? And for what reason ?\n    \n    Last time I checked (which was perhaps in summer '96, and looking at\n    cache performance) on our proxy, the amount of duplicate objects\n    was very small -- at least in size, I think it was maybe between\n    1 and 2% of the total cache size.\n\nA recent paper showed that 18% of the references in a trace were\nfor duplicated content (sorry, I don't have figures based on # of\nbytes).  See\n\nAlso, the DRP people are thinking in terms of doing software\ndistribution via HTTP.  A lot of programs are composed of\na small core plus a lot of library modules; the DRP people would\nlike to avoid retransmitting the same library over the network\nmore than once, while still being able to ensure that different\nversions of a library are properly managed.  (I.e., the duplicate\nsuppression mechanism should not substitute one version of a library\nfor another, since this substitution might break the program that\nuses the library.)   If this use of HTTP becomes popular, the number\nof bytes of duplicated content could increase dramatically.\n\nThis is still in the early stages of discussion, and it probably\nshouldn't waste bandwidth on the HTTP-WG mailing list at this point.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "[Oops, sent this out before filling in the citation to the paper.]\n\n    Sorry if I am missing something, but is duplicate suppression really a\n    big issue ? And for what reason ?\n    \n    Last time I checked (which was perhaps in summer '96, and looking at\n    cache performance) on our proxy, the amount of duplicate objects\n    was very small -- at least in size, I think it was maybe between\n    1 and 2% of the total cache size.\n\nA recent paper showed that 18% of the references in a trace were\nfor duplicated content (sorry, I don't have figures based on # of\nbytes).  See\nhttp://www.usenix.org/\n     publications/library/proceedings/usits97/douglis_rate.html\n\nAlso, the DRP people are thinking in terms of doing software\ndistribution via HTTP.  A lot of programs are composed of\na small core plus a lot of library modules; the DRP people would\nlike to avoid retransmitting the same library over the network\nmore than once, while still being able to ensure that different\nversions of a library are properly managed.  (I.e., the duplicate\nsuppression mechanism should not substitute one version of a library\nfor another, since this substitution might break the program that\nuses the library.)   If this use of HTTP becomes popular, the number\nof bytes of duplicated content could increase dramatically.\n\nThis is still in the early stages of discussion, and it probably\nshouldn't waste bandwidth on the HTTP-WG mailing list at this point.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "This whole issue is really one for the URI documents, rather than HTTP...\nOf course, we'd like them to get to draft standard soon as well, so\nthat we can reference them rather than incorporating duplication.  Yet\nanother headache of mine of last week...\n\nAt the IETF, a number of us (quite interesting collection of people),\ntalked through this issue (over beer in the bar, of course; was the most\nfun conversation of the IETF I had...)...  This was alluded to in the minutes\n(the \"design team\").\nAttending included: \nBrian Carpenter, (IAB chair)\nLarry Masinter\nmyself\nHenrik Frystyk\nSteve Deering (Cisco, and IPNG heavyweight).\nprobably one or two more I forget.\n\nBottom line was that the browsers were the least of the problem;\nwe almost certainly could get the big players to change the browsers...\n\nBut...  The browser is the tip of the iceberg...  The first proposal (things \nlike brackets or other syntactic changes to URI's) would break a large number \nof tools that know about HTML, XML and the like, and that therefore something \nlike the .ipv6 domain hack proposal was the only viable approach.  These \ntools vastly outnumber the browsers in the world.\n\nSteve was going to take the []less proposal back to the working group meeting.  \nSounds like the proposal got taken back, but not everyone understood the \nissue (in particular that the browsers were the least of the problem) and \nthink that URI syntax changes are feasible....  (wish they were, as URI \nsyntax is far from ideal, but reality can be very hard...).\n\nLooks like some further education of the IPV6 community may be in order,\nif Steve Deering  or Brian Carpenter thinks that things may go wrong...\n- Jim\n\n\n>  From: Jeffrey Mogul <mogul@pa.dec.com>\n>  Date: Mon, 12 Jan 98 12:11:41 PST\n>  To: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Heads-up re: IPv6 addresses in URLs (from IPng-WG minutes)\n>  \n>  I was skimming the minutes of the IPng WG from the Washington, DC IETF\n>  meeting, and found this:\n>  \n>  [Start of excerpt]\n>      IPv6 Addresses in URL's / B. Carpenter\n>      --------------------------------------\n>  \n>      Design team meet in the bar a few nights ago.\n>  \n>      Need numeric address in URL's for emergency operations (or robotic apps).\n>  \n>      Colons break URL parsers \"hostname\" syntax\n>  \n>      Proposals:\n>  \n>        http://--ABCD-EF12-192.100.1.2.ipv6:80/\n>        http://[ABCD:EF12:192.100.1.2]:80/\n>  \n>      Issue:  Should IPng w.g. reopen the \"colon\" notation?\n>  \n>      Heated discussion.  Most comments that this is stupid, we should\n>      not reopen IPv6 text notation.  Long discussion.  Issue seems to be\n>      that many parsers that take URL's are very limited.\n>  \n>      No one was in favor of changing current text representation.\n>      Extremely strong consensus!\n>  \n>      It was noted that the issue is probably only relevant for complete\n>      web browsers (e.g., Netscape, Microsoft, etc.), not all other\n>      applications that use URL's.  If the complete web browsers can be\n>      changed it is very likely to be sufficient.  Recommend that the\n>      primary preferred syntax for IPv6 addresses in URL's be:\n>  \n>        http://[ABCD.EF01::2345:6789]:80/\n>  \n>      The IPv6 address should be enclosed in brackets.  URL parsers that\n>      can not support this notation can either support the proposed\n>      alternative syntax:\n>  \n>        http://--ABCD-EF12-192.100.1.2.ipv6:80/\n>  \n>      or not allow IPv6 addresses to be entered directly.\n>  \n>  [End of excerpt]\n>  \n>  I'm not sure if this is really an \"issue\" for HTTP/1.1, but I suspect\n>  that the IESG will want to be sure that HTTP/1.1 syntax is compatible\n>  with IPv6, and if there are conflicts, we should probably make sure\n>  they are addressed.  Or make an explicit statement that we are not\n>  going to address them in this version of the protocol (and why not).\n>  \n>  -Jeff\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "The pointer Jeff gave was to the USENIX site, which won't give full access to \nthe paper without a password.  You can get the paper itself at \n\nhttp://www.research.att.com/~douglis/papers/roc.ps.gz\n\nFred\n\n\n\n"
        },
        {
            "subject": "Re: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": ">    Colons break URL parsers \"hostname\" syntax\n>    \n>    Proposals:\n>    \n>      http://--ABCD-EF12-192.100.1.2.ipv6:80/\n>      http://[ABCD:EF12:192.100.1.2]:80/\n>    \n>    Issue:  Should IPng w.g. reopen the \"colon\" notation?\n>    \n>    Heated discussion.  Most comments that this is stupid, we should\n>    not reopen IPv6 text notation.  Long discussion.  Issue seems to be\n>    that many parsers that take URL's are very limited.\n>\n>    No one was in favor of changing current text representation.\n>    Extremely strong consensus!\n\n\nThis is an interoperation issue. The strength of consensus inside\nthe IP-NG group is frankly not very relevant. Their proposal will \nbreak a substantial piece of existing infrastructure.\n\nAt the very least we deserve some form of argument for the IP-NG\ngroup's preference. Until one is provided I don't think we can have\nmuch of an argument here.\n\nI am not even sure where the colon notation was intended to be\nused. If they intend it to be used within URIs then the question is\ndefacto reopened. If they don't want to reopen the discussion it \nis likely that the URI group will address the issue by creating a \nnew (and incompatible) text representation.\n\nIt is acceptable for a group to introduce a notation that has limited\nutility, it is also acceptable for a group to negotiate with others to\nachieve a consensus about a solution that will meet the needs of a \nwider constituency. It is not acceptable for a group to declare that\nthey want a notation to be general but they don't want to talk about it\n\n\nAs Jim points out the question is not one for the IP-NG group to \nmake, it is within the URI group's remit and ultimately is the type\nof matter where the IESG may have to rule.\n\n\n            Phill\n\n\n\n"
        },
        {
            "subject": "Change to Chunk Length Synta",
            "content": "  In Memphis, we discussed allowing leading zeros in the\n  specification of a chunk length.\n\n  For the benefit of those who were not there, and to more fully\n  explain the reason for this change:\n\n  Our server implementation is for embedded systems:\n\n    - Usually the response to any request will consist partly\n      of dynamically generated content whose length cannot be known\n      before it is generated (hence the use of chunked encoding).\n\n    - Embedded systems generally have less flexible buffering schemes\n      (no scatter-gather buffers, for example).\n\n  We must therefor leave room for the length in a buffer and then\n  fill it when the dynamic content is complete; note that this space\n  does not necessarily fall at the beginning of the buffer.\n\n  Since we can't even know how many digits the length will require, it\n  is convenient to be allowed to use leading zeros to consume any\n  unused digits (a more complex workaround is available without this\n  change, but it requires more bytes and is unsightly).\n\n  I propose that the syntax description of chunked encoding be changed\n  to:\n  ================================================================\n\n       Chunked-Body   = *chunk\n                        last-chunk\n                        footer\n                        CRLF\n\n       chunk          = chunk-size [ chunk-ext ] CRLF\n                        chunk-data CRLF\n\n       chunk-size     = 1*HEX\n\n       last-chunk     = 1*(\"0\") [ chunk-ext ] CRLF\n\n       chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] )\n       chunk-ext-name = token\n       chunk-ext-val  = token | quoted-string\n       chunk-data     = chunk-size(OCTET)\n\n       footer         = *entity-header\n\n  The chunked encoding is ended by any chunk whose size is zero,\n  followed by the footer, which is terminated by an empty line.\n\n  ================================================================\n\n  This modified syntax also allows a chunk-ext on the last chunk,\n  which I added just for symmetry, and for the length of the final\n  chunk to be multiple digits as long as they are all zero.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "Perhaps we can get it right in HTTP-NG?\n\nBill\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 08.59 -0800 98-01-12, Jim Gettys wrote:\n> Could you enlighten us as to why you would want to specify multiple names\n> for the same object?\n\nBecause in real life, there are often different URLs referring to the\nsame file. And if a message contains several HTML documents, one of\nthem may use on of the names, the other may use the other name.\nIf we do not allow multiple Content-Location headers in the heading\nof an object with more than one URL, one would either have to:\n\n- Include the differently labelled object twice in the composite\n  message, which might increase transfer and download time.\n\nor\n\n- Rewrite the HTML, which we have decide to try to avoid in order\n  not to break digital seals.\n\n> It would help us understand if this is applicable to HTTP as well.\n\nThe next version of the MHTML standard will specify that it is\napplicable to composite MIME objects when such objects are sent\nthrough SMTP, NNTP or HTTP. There are obvious reasons why the\nsame format for composite objects should as much as possible be\nused in all three standards. The next version of the URL standard\n(draft-fielding-uri-syntax-01.txt) has omitted most of the text\nabout composite MIME objects in order to avoid unintended conflict\nwith MHTML.\n\nSince the next MHTML standard will apply to composite MIME objects,\nalso when these are sent via HTTP, it is of course of importance\nthat you in the HTTP group review it, too. Do not download the\nlatest IETF draft version of the MHTML standard now, however,\nbecause that version was before the Washington IETF meeting.\nI will notify your mailing list when a new MHTML Internet draft\nis ready.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "The only way this thing can be a must is if we change the protocol number.\n[Insert standard Henrik lecture here =)]\n\nYaron\n\n> -----Original Message-----\n> From:Scott Lawrence [SMTP:lawrence@agranat.com]\n> Sent:Monday, January 12, 1998 8:06 AM\n> To:Henrik Frystyk Nielsen\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:RE: MUST use Content-Base\n> \n> \n> On Mon, 12 Jan 1998, Henrik Frystyk Nielsen wrote:\n> \n> > I think I read from the discussion that people see a (limited) need for\n> the\n> > feature, so maybe the right thing is to use a SHOULD and then include a\n> > note like this:\n> > \n> > Note: Many applications based on RFC 2068 or\n> > previous versions of HTTP ignore the content-base\n> > header field when parsing relative URIs in\n> > documents.\n> \n> Some note of that sort should certainly be included, but I still think\n> that this needs to be a MUST or be omitted.  Granted, all implementations\n> earlier than 2068 and some (including important ones) based on 2068 will\n> not do this.  The point is that it is a good thing (IMHO) to have in the\n> protocol in the future and if we make it a must then the day will come\n> when it can be assumed to work more or less universally; if we do not make\n> it a MUST then that day will not come, and the protocol feature is\n> useless.  I was most carefull in my original post - this should either be\n> a MUST or it should be removed altogether; I don't think that compromise\n> is helpfull here.\n> \n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "On Mon, 12 Jan 1998, Yaron Goland wrote:\n\n> The only way this thing can be a must is if we change the protocol number.\n> [Insert standard Henrik lecture here =)]\n\nrom the fence, I've slowly moved to the DELETE completely opinion.\n\nPart of my reasoning is that I can't think of any content where \nthe addition of Content-base couldn't be handled by a possible\nslight extention to the data format.\n\nThe remainder is that I'm becoming convinced that the functionality\nprovided is inadequate and perhaps the order of inheritence is the\nreverse of the most useful.  I use the HTML <BASE> tag today for\ntwo reasons:\n   a.  Add image references to error content generated by proxies w/o\n       making all images absolute URLs\n   b.  Create an internal document name space which is all relative\n       to the same origin, not the document origin. This makes it\n       possible to move pages within a hieararchy w/o re-writing\n       internal links.\nIn both of these situations, I've used dynamically generated HTML\nso inserting a BASE tag was more efficient than finding and fixing\nevery link using the site scripting language.\n\nWith the background, what I would find more useful would be to\nover-ride the internally specified document base with one added by\nthe server.  Such an option would allow easy move of my document\ncollections to other locations.\n\nThis is the opposite form of inheritence from that currently specified.\nI'm not proposing that the current form be eliminated, rather I'm \nattempting to prove by example that the current specification misses\na major subset of the problem space.  Thus, it is better to \nremove it completely until it can be thought out more thoroughly\nand specified with more functionality.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "On Mon, 12 Jan 1998, Jacob Palme wrote:\n\n> The MHTML group has decided to allow more than one Content-Location\n> in the same message heading. This can be used, for example, if the\n> same object can be located by several different URLs, and you want\n> to specify all of them. A particular case is the following\n> \n> Supposes a.gif and images/a.gif actually refer to the same image.\n> And suppose the two HTML objects above have digital seals on them.\n> Then, if you did not allow multiple Content-Location headers in\n> the first body part, you would have to send the image twice, or\n> you would have to modify the HTML invalidating the digital seals!\n\nThe usage of Content-Location within HTTP is specifically to allow the\nspecification of which one of some number of alternate versions of an\nentity is in this response.  This would appear to me to be in conflict\nwith the use being suggested by MHTML.  I don't believe that this should\nbe changed at this late date, so I'd be carefull about assuming that\nchanges proposed by MHTML will apply to HTTP usage.\n\nYou describe the motivation for this as being a multipart message\ncontaining multiple different references to the same entity.  For example,\nmessage contains A, B, and C where A refers to C and B refers to D but D\nis really another name for C.  Rather than adding an attribute to C to\ngive it the additional name (from the HTTP point of view overloading the\nsemantics of Content-Location), why not add another part to the message\nthat provides the equivalent of an HTTP redirect.  Something like (for\nD):\n\nContent-Type: mime/alternate-name\nLocation: C\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Thanks Jacob and Jim for digging into this issue.\n\nAt a meta level, the basic problem is that putting HTML into Mail\nobjects leads inevitibly to composing composite objects which then\ncauses us to rrun up against all the little shortcuts that developers\n(of both standards and products) and that makes our separate WGs rub\nup against each other in abrasive ways, but that is the nature of\nrubbing each other smooth so our users can interwork with our tools.\n\nMore directly, after all the work on how to do MHTML for MAIL, we also\ndiscovered interesting uses for composite objects that no one had\nthought of, like creating archive objects that record the state of an\nevery changing composite object (e.g., a weathermap) at a point in\ntime, and it is very easy to see the value of such use.\n\nAnd, we can esily see that one might want to ship that archive object\nvia HTTP, or EMAIL, or even FTP, or SneakerNet, and always get the\nsame result.\n\nSo, back at the meta level, we have discovered that we areally do have\nto very seriousl consider our sister protocols and arrange for them to\ninterwork.\n\nCheers...\\Stef\n\n\n\nFrom your message Tue, 13 Jan 1998 01:04:24 +0100:\n}\n}At 08.59 -0800 98-01-12, Jim Gettys wrote:\n}> Could you enlighten us as to why you would want to specify multiple names\n}> for the same object?\n}\n}Because in real life, there are often different URLs referring to the\n}same file. And if a message contains several HTML documents, one of\n}them may use on of the names, the other may use the other name.\n}If we do not allow multiple Content-Location headers in the heading\n}of an object with more than one URL, one would either have to:\n}\n}- Include the differently labelled object twice in the composite\n}  message, which might increase transfer and download time.\n}\n}or\n}\n}- Rewrite the HTML, which we have decide to try to avoid in order\n}  not to break digital seals.\n}\n}> It would help us understand if this is applicable to HTTP as well.\n}\n}The next version of the MHTML standard will specify that it is\n}applicable to composite MIME objects when such objects are sent\n}through SMTP, NNTP or HTTP. There are obvious reasons why the\n}same format for composite objects should as much as possible be\n}used in all three standards. The next version of the URL standard\n}(draft-fielding-uri-syntax-01.txt) has omitted most of the text\n}about composite MIME objects in order to avoid unintended conflict\n}with MHTML.\n}\n}Since the next MHTML standard will apply to composite MIME objects,\n}also when these are sent via HTTP, it is of course of importance\n}that you in the HTTP group review it, too. Do not download the\n}latest IETF draft version of the MHTML standard now, however,\n}because that version was before the Washington IETF meeting.\n}I will notify your mailing list when a new MHTML Internet draft\n}is ready.\n}\n}------------------------------------------------------------------------\n}Jacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\n}for more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 08.52 -0500 98-01-13, Scott Lawrence wrote:\n> I don't believe that this should\n> be changed at this late date, so I'd be carefull about assuming that\n> changes proposed by MHTML will apply to HTTP usage.\n\nThe next MHTML draft will say that MHTML applies to the formatting\nof MIME composite objects for sending through e-mail, netnews or\nHTTP. I think it is obvious that the format of MIME composite\nobjects should be the same, independent of how they are sent.\n\n> You describe the motivation for this as being a multipart message\n> containing multiple different references to the same entity.  For example,\n> message contains A, B, and C where A refers to C and B refers to D but D\n> is really another name for C.  Rather than adding an attribute to C to\n> give it the additional name (from the HTTP point of view overloading the\n> semantics of Content-Location), why not add another part to the message\n> that provides the equivalent of an HTTP redirect.  Something like (for\n> D):\n>\n> Content-Type: mime/alternate-name\n> Location: C\n\nA simpler solution might be to have two headers:\nContent-Location: .. one single primary location ..\nAlternate-Locations: .. list of other locations with the same content ..\n\n> The usage of Content-Location within HTTP is specifically to allow the\n> specification of which one of some number of alternate versions of an\n> entity is in this response.\n\nI do not quite understand how you are able to designate one location\nas the primary one, if you are sending two copies of exactly the\nsame object, referenced in two different ways. In the case you\ndescribe, how can you say that C is primary and D is secondary?\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Heads-up re: IPv6 addresses in URLs (from IPngWG minutes",
            "content": "At 21:52 01/12/98 +0100, Koen Holtman wrote:\n>We did not really reach a consensus, but one intermediate conclusion\n>was that a notation like\n>\n> http://1080::8:800:200C:417A.8000/blah\n>\n>with the . separating the address from the port number, would do the\n>trick.  I see however that you quoted some ipv6 addresses which have a\n>. in them above, I think we assumed at the time that the ipv6 notation\n>would use : only. \n\nThis is correct. IPV6 can \"contain\" IPV4 addresses and this can be done by\nconcatenating IPV6 syntax and IPV4 syntax so that an IP address can contain\nboth : and . as separators.\n\nUgly, huh?\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Jacob Palme wrote:\n> I do not quite understand how you are able to designate one location\n> as the primary one, if you are sending two copies of exactly the\n> same object, referenced in two different ways. In the case you\n> describe, how can you say that C is primary and D is secondary?\n\nIt seems to me that in at least some cases it is obvious which object is\nprimary. For example, I give Apache-SSL to both Oxford and Cambridge\nUniversities, who then publish it on their FTP sites. These are then\nmirrored by various other sites. The primary version is the one that I\ngenerated (which, unfortunately for this discussion, doesn't have a URL\nat all, at least, not one I'm aware of, certainly not one that is\naccessible by the general public), and all the others are secondary,\nright?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "YG> It seems a clarification is in order. As such I propose that the\nYG> following paragraph be added to the end of section 10.1:\nYG>\nYG> 1xx responses MAY be sent independently of requests. Clients MUST always\nYG> be able to accept 1xx responses and MUST ignore any 1xx response they do\nYG> not understand. In addition, proxies MUST pass through any 1xx response\nYG> they do not understand.\n\n>>>>> \"LM\" == Larry Masinter <masinter@parc.xerox.com> writes:\n\nLM> I'm really uneasy about this. I can imagine an implementation that\nLM> just doesn't read from the connection at all unless there's some\nLM> request outstanding, and a server that sends a 1xx 'response' [sic]\nLM> without a pending request might wind up with stuff stuck in buffers\nLM> that never gets read.\n\nLM> Seems like it puts unnecessary requirements on implementations,\nLM> and it doesn't seem justified. Just what ARE these unanticpated\nLM> 'responses'?\n\n  The only apparent purpose we could find in the RFC for the 100\n  response was in a discussion of client retry behaviour when the\n  connection closed before the client had finished sending the body of\n  a POST:\n\n     Clients SHOULD remember the version number of at least the most\n     recently used server; if an HTTP/1.1 client has seen an HTTP/1.1 or\n     later response from the server, and it sees the connection close\n     before receiving any status from the server, the client SHOULD retry\n     the request without user interaction so long as the request method is\n     idempotent (see section 9.1.2); other methods MUST NOT be\n     automatically retried, although user agents MAY offer a human\n     operator the choice of retrying the request.. If the client does\n     retry the request, the client\n\n       o  MUST first send the request header fields, and then\n\n       o  MUST wait for the server to respond with either a 100 (Continue)\n          response, in which case the client should continue, or with an\n          error status.\n\n     If an HTTP/1.1 client has not seen an HTTP/1.1 or later response from\n     the server, it should assume that the server implements HTTP/1.0 or\n     older and will not use the 100 (Continue) response. If in this case\n\n  This wait between the headers and the body of a POST is the only\n  purpose we can see for a 100 response.  The only time our\n  implementation sends one is after receiving an acceptable 1.1 POST.\n  This is usefull in that it allows for us to reject a POST on\n  authentication or other grounds, hopefully before the client starts\n  sending the body (which could be large, and which the server would\n  have to just throw away).  Since most of the time a POST will have\n  been preceeded by a GET from the same server (to obtain the page\n  containing the form), there is a good chance that the client has had\n  an opportunity to know that it is dealing with a 1.1 server so it\n  can know to wait.\n\n  We would actually prefer to see this set of rules made more general\n  in that we'd like it to apply to any POST, not just one being\n  retried (which may or may not have been what was intended).\n\n  As to Yolands' suggestion above, I don't really see the point (we\n  certainly won't be sending all those extra responses), but since it\n  specifies only client behaviour we don't care much.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "On Tue, 13 Jan 1998, Jacob Palme wrote:\n\n> At 08.52 -0500 98-01-13, Scott Lawrence wrote:\n> > The usage of Content-Location within HTTP is specifically to allow the\n> > specification of which one of some number of alternate versions of an\n> > entity is in this response.\n> \n> I do not quite understand how you are able to designate one location\n> as the primary one, if you are sending two copies of exactly the\n> same object, referenced in two different ways. In the case you\n> describe, how can you say that C is primary and D is secondary?\n\nThe point is that a single resource may exist in english and french\nversions (or an image in high, medium, and low resolution, etc.) and\nbased on other information, primarily HTTP header content such as\nAccept-language, the server picks one of the versions to return as the\nresponse. The server notifies the client and any intervening caches\nthat the identity of the response is some other URL via the\nContent-location field. In other words, the versions are not the\nexact same object.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se>\n>  Date: Tue, 13 Jan 1998 16:46:11 +0100\n>  To: Scott Lawrence <lawrence@agranat.com>\n>  Cc: IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Re: Multiple Content-Location headers\n>  \n>  At 08.52 -0500 98-01-13, Scott Lawrence wrote:\n>  > I don't believe that this should\n>  > be changed at this late date, so I'd be carefull about assuming that\n>  > changes proposed by MHTML will apply to HTTP usage.\n>  \n>  The next MHTML draft will say that MHTML applies to the formatting\n>  of MIME composite objects for sending through e-mail, netnews or\n>  HTTP. I think it is obvious that the format of MIME composite\n>  objects should be the same, independent of how they are sent.\n>  \n\nI don't really want to stir up a firestorm here, but there are several\nissues (mostly procedural), I'd like to raise with your statement about\nyour next draft applying to HTTP...\n\n1) the HTTP working group tries not to lay constraints on the\nMHTML group.  In particular, not without negotiation with the\nMHTML group.  I suspect we'd like a similar attitude from\nthe MHTML group.  Hopefully, this discussion is that negotiation...\nOur specs certainly makes no statement about HTTP messages\nbeing the format that mail messages must conform to.\n\n2) HTTP is NOT a MIME prototol; it is really a \"MIME-like\"\nprotocol, where we've tried not to be gratuitously different.\n(not always successfully).  HTTP != mail. \n\nBut thanks for raising the topic, in any case, as this is how\nwe can avoid being gratuitously different.\n\n3) there is NO requirement that HTTP implementations support\n composite objects (e.g. multipart is NOT a requirement of HTTP).  It was settled \nThe HTTP working group long ago that for HTTP, such a \nrequirement was both unneeded, and actually \nunwise (for example, the caching consequences), though transmitting \nmultipart as the payload of an HTTP message is certainly not \nforbidden in HTTP (and used in a very small number of optional\nplaces).  \n\nSo the Content-Location discussion (on the HTTP mailing list, I think,\nshould be framed in the context of HTTP requests for single objects, \nnot the transmission of composite objects (which HTTP really \ndoesn't know about at all).\n\nNote that the security issues that attempting to deal with problems\nof caching composite objects (as independent, named objects)\nare far from trivial for HTTP; you'd have to worry about what\nhappens if a composite object claims the name of some part\nof the namespace not under the origin server's control, for\nexample.  This is a problem that Mail does not have, but that\nwould make HTTP's life difficult...  I get headaches just thinking\nabout the spoofing problems possible, and the problems caching\nproxy servers would have implementing such a model....\n\nI therefore question how much the MHTML specification should say about\nwhat goes over HTTP, and am reacting to the the statement in\nyour paragraph above (possibly not justifiably, since I haven't seen\nyour not yet released draft or previews of the language). \n\nYour paranoid HTTP/1.1 editor who'd like\nto get to draft standard Real Soon Now...\n- Jim Gettys\n \n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MUST use ContentBas",
            "content": ">So therefore changing the MAY to MUST breaks HTTP/1.1 proposed standard\n>compliant implementations which choose to honor the may, as is their right,\n>by ignoring the header.\n\nSorry Yaron, there is no MAY in the Content-Base definition of RFC 2068\naside from the optional decision of the server.  If the browser does not\nimplement Content-Base as specified, then the browser is not compliant\nwith HTTP/1.1 as specified in RFC 2068.  Scott's proposal was merely a\nclarification.\n\nIf, on the other hand, you want to argue that Content-Base is not going\nto be implemented, and therefore should not be in the HTTP/1.1 standard,\nthen that's a valid concern.\n\nWhen I was rewriting the URI specification and arguing with the MHTML\ngroup, I came to the conclusion that Content-Base is not needed provided\nthat Content-Location is implemented as specified.  The reasoning was\nsimilar to what Dave Morris mentioned: the only person capable of knowing\nwhether or not the embedded references in a document are relative to\nsome other namespace is the document creator, and they are better-off\nmaking that distinction within the document.  Granted, some formats may\nnot have the equivalent of HTML's BASE, but I would argue that those\nformats are very unlikely to contain relative references.\n\nDoes MSIE implement Content-Location as specified?  Note that this\nwill eventually be very important, since it enables the reduction of\nexternal redirect messages for internally redirected content.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">Supposes a.gif and images/a.gif actually refer to the same image.\n>And suppose the two HTML objects above have digital seals on them.\n>Then, if you did not allow multiple Content-Location headers in\n>the first body part, you would have to send the image twice, or\n>you would have to modify the HTML invalidating the digital seals!\n\nOr you could use the equivalent of an external-body part, or a\npart that serves as a namespace catalog, or something similar to\nthe Alternates header field.  This would not be a frequent occurrence,\nso the representation could be verbose.\n\nHTTP cannot allow multiple Content-Location header fields unless the\nsyntax for the field-value is changed to allow multiple URLs separated\nby a comma, which in turn would require that each URL be delimited.\nSuch a change is not going to happen at this point in the process.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "David W. Morris wrote:\n> \n> On Tue, 13 Jan 1998, Jacob Palme wrote:\n> \n> > At 08.52 -0500 98-01-13, Scott Lawrence wrote:\n> > > The usage of Content-Location within HTTP is specifically to allow the\n> > > specification of which one of some number of alternate versions of an\n> > > entity is in this response.\n> >\n> > I do not quite understand how you are able to designate one location\n> > as the primary one, if you are sending two copies of exactly the\n> > same object, referenced in two different ways. In the case you\n> > describe, how can you say that C is primary and D is secondary?\n> \n> The point is that a single resource may exist in english and french\n> versions (or an image in high, medium, and low resolution, etc.) and\n> based on other information, primarily HTTP header content such as\n> Accept-language, the server picks one of the versions to return as the\n> response. The server notifies the client and any intervening caches\n> that the identity of the response is some other URL via the\n> Content-location field. In other words, the versions are not the\n> exact same object.\n\nEh? Aren't you describing variants here?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Hi Jim -- Speaking as MHTML Chair, I must observe that you sound a lot\nlike I sounded when I first looked at a lot of the stuff going on in\nHTTP;-)...  Just ask Larry Masinter;-)...  He bore the brunt;-)...\n\nSince then MHTML and HTTP have been cooperating at least at the Chair\n& Editor levels.  I expect that more of the MHTML participants have\nbeen aware of this inter-WG negotiation because most of it was\nbroadcast to the MHTML list.  Larry Masinter and Roy Fielding were the\nprimary negotiators on the HTTP side.  I expect that most of the\nissues seemed to be minor adjustments not needing broad HTTP WG\nefforts, but they were more important to MHTML at the time.\n\nSo, welcome to the club;-)...  Some of us have been waiting a long\ntime to get this MHTML/HTTP discussion under way;-)...  We really do\nneed to sort this out among us and try to find Rough Consensus across\nboth WGs, if not more WGs.\n\nNow then, a detail:\n\nHow are you going to handle HTTP retrieval of archived ever-changing\nweather maps (that normally change within a given URI which stands for\n\"the current weather\") that have been archived as compound MHTML \"MIME\nObjects\"?  If you don't retrieve each of them with all of their\narchived parts, how are you ever going to reconstruct what the weather\nwas at any time in the past?\n\nSo, having found at least one case that is clearly going to make good\nuse of MHTML for archiving and transfering compound web objects, how\nare we going to enable this across the whole spectrum of transports\n(HTTP, RFC822, FTP, SneakerNet, CDROM, EtcNet) for such compound WEB\nobjects whose parts need to be bound to each other?\n\nBest...\\Stef\n\n\nrom your message Tue, 13 Jan 1998 14:20:21 -0800:\n}\n}>  From: Jacob Palme <jpalme@dsv.su.se>\n}>  Date: Tue, 13 Jan 1998 16:46:11 +0100\n}>  To: Scott Lawrence <lawrence@agranat.com>\n}>  Cc: IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n}>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n}>  Subject: Re: Multiple Content-Location headers\n}>\n}>  At 08.52 -0500 98-01-13, Scott Lawrence wrote:\n}>  > I don't believe that this should\n}>  > be changed at this late date, so I'd be carefull about assuming that\n}>  > changes proposed by MHTML will apply to HTTP usage.\n}>\n}>  The next MHTML draft will say that MHTML applies to the formatting\n}>  of MIME composite objects for sending through e-mail, netnews or\n}>  HTTP. I think it is obvious that the format of MIME composite\n}>  objects should be the same, independent of how they are sent.\n}>\n}\n}I don't really want to stir up a firestorm here, but there are several\n}issues (mostly procedural), I'd like to raise with your statement about\n}your next draft applying to HTTP...\n}\n}        1) the HTTP working group tries not to lay constraints on the\n}        MHTML group.  In particular, not without negotiation with the\n}        MHTML group.  I suspect we'd like a similar attitude from\n}        the MHTML group.  Hopefully, this discussion is that negotiation...\n}        Our specs certainly makes no statement about HTTP messages\n}        being the format that mail messages must conform to.\n}\n}        2) HTTP is NOT a MIME prototol; it is really a \"MIME-like\"\n}        protocol, where we've tried not to be gratuitously different.\n}        (not always successfully).  HTTP != mail.\n}\n}        But thanks for raising the topic, in any case, as this is how\n}        we can avoid being gratuitously different.\n}\n}        3) there is NO requirement that HTTP implementations support\n}        composite objects (e.g. multipart is NOT a requirement of HTTP).        It was se\n} ttled\n}        The HTTP working group long ago that for HTTP, such a\n}        requirement was both unneeded, and actually\n}        unwise (for example, the caching consequences), though transmitting\n}        multipart as the payload of an HTTP message is certainly not\n}        forbidden in HTTP (and used in a very small number of optional\n}        places).\n}\n}        So the Content-Location discussion (on the HTTP mailing list, I think,\n}        should be framed in the context of HTTP requests for single objects,\n}        not the transmission of composite objects (which HTTP really\n}        doesn't know about at all).\n}\n}Note that the security issues that attempting to deal with problems\n}of caching composite objects (as independent, named objects)\n}are far from trivial for HTTP; you'd have to worry about what\n}happens if a composite object claims the name of some part\n}of the namespace not under the origin server's control, for\n}example.  This is a problem that Mail does not have, but that\n}would make HTTP's life difficult...  I get headaches just thinking\n}about the spoofing problems possible, and the problems caching\n}proxy servers would have implementing such a model....\n}\n}I therefore question how much the MHTML specification should say about\n}what goes over HTTP, and am reacting to the the statement in\n}your paragraph above (possibly not justifiably, since I haven't seen\n}your not yet released draft or previews of the language).\n}\n}                        Your paranoid HTTP/1.1 editor who'd like\n}                        to get to draft standard Real Soon Now...\n}                                - Jim Gettys\n}\n}--\n}Jim Gettys\n}Industry Standards and Consortia\n}Digital Equipment Corporation\n}Visting Scientist, World Wide Web Consortium, M.I.T.\n}http://www.w3.org/People/Gettys/\n}jg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "It seems clear that we should now explore the idea of having:\n\nNo More Than One Content-Base; No More Than One Content-Location; and\nMultiple instances of Content-Location-Alternative headers, where-in\nit is required that the Content-Location-Alternative values must be\nalternative URIs for the exact same content as that which is inclosed\nin the same MIME PART of an Compound MHTML Object.\n\nWe must remember that all MIME headers must start with \"Content-\"\n\nCheers...\\Stef\n\nrom your message Tue, 13 Jan 1998 13:32:40 -0800:\n}\n}>Supposes a.gif and images/a.gif actually refer to the same image.\n}>And suppose the two HTML objects above have digital seals on them.\n}>Then, if you did not allow multiple Content-Location headers in\n}>the first body part, you would have to send the image twice, or\n}>you would have to modify the HTML invalidating the digital seals!\n}\n}Or you could use the equivalent of an external-body part, or a\n}part that serves as a namespace catalog, or something similar to\n}the Alternates header field.  This would not be a frequent occurrence,\n}so the representation could be verbose.\n}\n}HTTP cannot allow multiple Content-Location header fields unless the\n}syntax for the field-value is changed to allow multiple URLs separated\n}by a comma, which in turn would require that each URL be delimited.\n}Such a change is not going to happen at this point in the process.\n}\n}....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  Sender: stef@nma.com\n>  From: Einar Stefferud <Stef@nma.com>\n>  Date: Tue, 13 Jan 1998 15:26:31 -0800\n>  To: Jim Gettys <jg@pa.dec.com>\n>  Cc: Jacob Palme <jpalme@dsv.su.se>, Scott Lawrence <lawrence@agranat.com>,\n>          IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Re: Multiple Content-Location headers \n>  \n>  Hi Jim -- Speaking as MHTML Chair, I must observe that you sound a lot\n>  like I sounded when I first looked at a lot of the stuff going on in\n>  HTTP;-)...  Just ask Larry Masinter;-)...  He bore the brunt;-)...\n>  \n>  Since then MHTML and HTTP have been cooperating at least at the Chair\n>  & Editor levels.  I expect that more of the MHTML participants have\n>  been aware of this inter-WG negotiation because most of it was\n>  broadcast to the MHTML list.  Larry Masinter and Roy Fielding were the\n>  primary negotiators on the HTTP side.  I expect that most of the\n>  issues seemed to be minor adjustments not needing broad HTTP WG\n>  efforts, but they were more important to MHTML at the time.\n>  \n>  So, welcome to the club;-)...  Some of us have been waiting a long\n>  time to get this MHTML/HTTP discussion under way;-)...  We really do\n>  need to sort this out among us and try to find Rough Consensus across\n>  both WGs, if not more WGs.\n>  \n>  Now then, a detail:\n>  \n>  How are you going to handle HTTP retrieval of archived ever-changing\n>  weather maps (that normally change within a given URI which stands for\n>  \"the current weather\") that have been archived as compound MHTML \"MIME\n>  Objects\"?  If you don't retrieve each of them with all of their\n>  archived parts, how are you ever going to reconstruct what the weather\n>  was at any time in the past?\n>  \n\nThere is a presumption you have made here: that the archiving is being done \nvia compount MHTML documents.  A case has yet to be made that this will \nhappen.  It certainly doesn't happen today.  \n\nYou can try to make a case that it might happen if the facilities were there, \nof course).  Equally likely in my opinion though is the inverse, that mail \nas we know it becomes pretty integrated to the Web, rather than the inverse.  \nThis message is being composed on a prototype mail system which has many \nof these properties already, for example.  All mail messages I get end up \nwith a URL, the mail user agent only uses HTTP (it is written in Java), \nand I can say from first hand experience that this has much to commend it.\n(The mail is delivered to the web server via SMTP, but that\ncould be fixed :-).)\n\nBut back to the present: Mail archives in the Web are typically handled \nby a program that takes mail messages as input and generates HTML as a set \nof Web documents. An equally plausible extension to handle mhtml is to retrieve \nthe attached documents at the time the HTML is generated from the mail message, \nrather than presuming the data is inline.  This requires no protocol\nsupport beyond what exists today (though arguably is not as atomic in nature).\n\n>  So, having found at least one case that is clearly going to make good\n>  use of MHTML for archiving and transfering compound web objects, how\n>  are we going to enable this across the whole spectrum of transports\n>  (HTTP, RFC822, FTP, SneakerNet, CDROM, EtcNet) for such compound WEB\n>  objects whose parts need to be bound to each other?\n>  \n>  Best...\\Stef\n\nFundamentally, HTTP talks about a single document at a time; this\nis inherent throughout the protocol; in the caching sections, and\nall over.  All methods take a single URI as an argument; not a list\nof URI's.  This presumption is inherent throughout the design.\n\nThere is alot of work on what are called \"collections\" going on in Webdav.\nWhile I have my reservations on details of what they are proposing, the\nconcept is cleaner, in my humble opinion: it should be possible to\ndefine a document which is a collection of related documents.\nThis would fill the scenario you outline, as I understand it,\nalong with many others.\n\nNote since such collections can have URI's of their own, it fits\nwell into the model of the Web.\n\nThe transport of MHTML message as a mime type over the HTTP protocol, is \nperfectly possible and a solution everyone would happy with.  It is perfectly \nreasonable in this model that your MHTML document will be retrieved via \nHTTP (as the HTTP entity), passed to an MHTML viewer, and everything will \nwork fine. (\"Any problem in computer science can be solved by an extra level \nof indirection\" - Roger Needham).  In this scenario, there is no issue at \nall that I can see.  And it has the appropriate \"atomic\" properties you \nwould like to have for that applications, which would be difficult in any \ndesign in which the component pieces are mixed.\n\nBut to try to introduce the idea that an object is compound by its very \nnature to the web at this date, and trying to mix the MHTML metadata with \nHTTP metadata, even if possible, does not seem feasible or desirable to \nme. My complexity alarm is going off... \n- Jim Gettys\n\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Test Day 1998-011",
            "content": "  The next HTTP test day is Jan 14th - reply as usual.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n"
        },
        {
            "subject": "Correction  Test Day on the 15t",
            "content": "  Had the date wrong on the previous mail.\n\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 Pipelinin",
            "content": "Hey everybody!\n\nAfter the wg meetings I was reviewing the notes on pipelining in the\nconnection mgt draft and rfc 2068.\nI have a few questions which relate to proxy server behavior..\n\nWhen a client is talking to a proxy server and is pipelining requests,\nshould it use a single pipeline connection to issue requests to \ndifferent origin servers ?\nie should GET http://www.ups.com/ HTTP/1.1\n   and    GET http://www.fedex.com/ HTTP/1.1 \nbe sent over the same pipeline or should a new connection be established.\nA common occurence of this is when advertisement gifs come from a \ndifferent server than the html file.\n\nI guess the question becomes :\nCan requests in the same pipeline be to different origin servers?\n\nWith the advent of pipelined persistent connections ( and to a lesser\nextend 1.0 keep alives ), the distinction of 'who the client is\ntalking to' is confusing to me.  Since while the client may be\npipelining to a proxy, and the proxy can go ahead and do an\nold style connection to an origin server, how does the client deal\nwith old responses?\nAssuming the answer to the previous question,  is yes...\n\nIE: client pipelines:\n  GET http://www.foo.com/ HTTP/1.1\n  GET http://www.bar.com/ HTTP/1.1\nwhat happens if foo.com is a 1.1 server ( the proxy can do a\npersistent conenction ) and bar.com is 1.0 (proxy cannot)?\n\nAlso, when the responses come back to the client, the first is\na 1.1 response, and the second is a 1.0 response ..\n\nHow should the client or proxy behave ?\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">But back to the present: Mail archives in the Web are typically handled\n>by a program that takes mail messages as input and generates HTML as a set\n>of Web documents. An equally plausible extension to handle mhtml is\n>to retrieve the attached documents at the time the HTML is generated\n>from the mail message, rather than presuming the data is inline.\n>This requires no protocol support beyond what exists today (though\n>arguably is not as atomic in nature). \n\nInterestingly enough, one MIME-SGML proposal (the one that MIME-HTML\nis *not* based upon) could handle both cases perfectly well, even\nwithout multipart/related.\n\n\n\n"
        },
        {
            "subject": "Re: MUST use ContentBas",
            "content": ">  From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n\n(material elided...)\n\n>  When I was rewriting the URI specification and arguing with the MHTML\n>  group, I came to the conclusion that Content-Base is not needed provided\n>  that Content-Location is implemented as specified.  The reasoning was\n>  similar to what Dave Morris mentioned: the only person capable of knowing\n>  whether or not the embedded references in a document are relative to\n>  some other namespace is the document creator, and they are better-off\n>  making that distinction within the document.  Granted, some formats may\n>  not have the equivalent of HTML's BASE, but I would argue that those\n>  formats are very unlikely to contain relative references.\n>  \n\nDo others agree with Roy's analysis?  Is this true in the face of\nnegotiated resources, where Content-Location might be used to tell you\nwhere the underlying version is found? \n\nThe minimalist in me says if we don't actually need a mechanism, or a different \nmechanism we do need can be used to solve a problem, we shouldn't have it...\n\nAnd we haven't heard other opinions (e.g. lynx, etc....).  I'd like to hear\nfrom others who've formed opinions.\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MUST use ContentBas",
            "content": "Jim Gettys:\n>\n>\n>>  From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n>\n>(material elided...)\n>\n>>  When I was rewriting the URI specification and arguing with the MHTML\n>>  group, I came to the conclusion that Content-Base is not needed provided\n>>  that Content-Location is implemented as specified.  The reasoning was\n>>  similar to what Dave Morris mentioned: the only person capable of knowing\n>>  whether or not the embedded references in a document are relative to\n>>  some other namespace is the document creator, and they are better-off\n>>  making that distinction within the document.  Granted, some formats may\n>>  not have the equivalent of HTML's BASE, but I would argue that those\n>>  formats are very unlikely to contain relative references.\n>>  \n>\n>Do others agree with Roy's analysis?  \n\nI agree that Content-Base is not really needed.\n\n>Is this true in the face of\n>negotiated resources, where Content-Location might be used to tell you\n>where the underlying version is found?\n\nAs far as I can see, the removal of Content-Base will not break any\nproposed content negotiation scheme.\n\n>- Jim\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 16.21 -0800 98-01-13, Einar Stefferud wrote:\n> It seems clear that we should now explore the idea of having:\n>\n> No More Than One Content-Base; No More Than One Content-Location; and\n> Multiple instances of Content-Location-Alternative headers, where-in\n> it is required that the Content-Location-Alternative values must be\n> alternative URIs for the exact same content as that which is inclosed\n> in the same MIME PART of an Compound MHTML Object.\n>\n> We must remember that all MIME headers must start with \"Content-\"\n\nRight! That seems to be the right way to handle this issue.\nAnd then we can avoid the problem which the MHTML group had in\nwhether to be able to derive a base from the Content-Location\nheader if there was more than one of them.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 21.20 +0000 98-01-13, Ben Laurie wrote:\n> It seems to me that in at least some cases it is obvious which object is\n> primary. For example, I give Apache-SSL to both Oxford and Cambridge\n> Universities, who then publish it on their FTP sites. These are then\n> mirrored by various other sites. The primary version is the one that I\n> generated (which, unfortunately for this discussion, doesn't have a URL\n> at all, at least, not one I'm aware of, certainly not one that is\n> accessible by the general public), and all the others are secondary,\n> right?\n\nMHTML was mainly meant for the case where two different URLs actually\nget to the same physical file. But it could, of course, also be used\nin the case of mirrors. This might be very useful, to send an object\nwith a list of mirror URLs. Sometimes, the sender knows which is\nthe primary, and could then indicate this. But in many cases, the\npoor mail client which produces the message will have no idea which\nof several URLs referring to the same physical file is the primary\none.\n\nAt 14.15 -0800 98-01-13, David W. Morris wrote:\n> The point is that a single resource may exist in english and french\n> versions (or an image in high, medium, and low resolution, etc.) and\n> based on other information, primarily HTTP header content such as\n> Accept-language, the server picks one of the versions to return as the\n> response. The server notifies the client and any intervening caches\n> that the identity of the response is some other URL via the\n> Content-location field. In other words, the versions are not the\n> exact same object.\n\nIn that case, there would be no need for multiple Content-Location headers.\nSo that would not be any problem.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "editoral issues raised by Ross Patterso",
            "content": "To get into the mailing list archives....\n\nWill be editorial issue \"PATTERSON\"...\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n\nattached mail follows:\nI just finished a cover-to-cover reread of draft-ieft-http-v11-spec-rev-01,\nthe 21 November 1997 HTTP 1.1 draft.  I don't know how many of these you've\nheard about, but I thought I'd pass them along.\n\n   1) (TOC) Lots of section-numbering errors (\"1.1.2\" following \"8.1.1\", etc.)\n\n   2) (4.1) The note reads \"... client implementations generate an extra\n      CRLF's after ...\".  That's either \"an extra CRLF\" or \"some extra CRLF's\".\n\n   3) (8.1.3) The third paragraph contains an unresolved reference for\n      \"information about the Keep-Alive header ...\".\n\n   4) (8.1.4) The third paragraph reads \"For example, a client MAY have\n      started to send ...\".  I think you mean \"may\", not \"MAY\", as this isn't\n      a requirement statement.\n\n   5) (8.1.4) The fourth paragram ends with \"However, this automatic retry\n      SHOULD NOT be repeated if the second request fails.\"  I think you mean\n      the second retry of the sequence of requests, not the second request of\n      the sequence.\n\n   6) (11.1) There is a dangling \"Scheme\" at the end of the sentence.\n\n   7) (12.2) The first paragraph contains an unresolved reference for\n      \"... field-name Alternates, as described in appendix ...\".\n\n   8) (13.2.3) The third paragraph claims that \"... HTTP/1.1 requires\n      origin servers to send a Date header with every response ...\", but that\n      contradicts (14.19) where there are rules for when a server doesn't\n      have to supply a Date header.\n\n   9) (13.2.3) The fourth paragraph contains the repeated phrase \"HTTP/1.1\n      uses the Age response header to\".\n\n  10) (13.2.6) The second paragraph reiterates the claim that \"... the HTTP/1.1\n      specification requires the transmission of Date headers on every\n      response\".\n\n  11) (13.5.1) There is an extra bullet in the list of hop-by-hop headers, and\n      again in the list of headers that a non-caching proxy MUST NOT modify.\n\n  12) (13.6) The fifth paragraph reads \"A Vary header field-value of \"*\"\n      always fail to match ...\".  It should read \"... always fails to ...\".\n\n  13) (14.15) There is an extra period at the end of the first sentence.\n\n  14) (14.32) The second paragraph ends \"Clients SHOULD include both header\n      fields when a no-cache request is sent to a server not known to be\n      HTTP/1.1 compliant.\"  The fourth paragraph beings \"HTTP/1.1 clients\n      SHOULD NOT send the Pragma request-header.\"  This seems to be a\n      contradiction.\n\n  15) (14.48) The production for <t-codings> doesn't allow \"identity\", but\n      rule 3 seems to allow \"identity;q=0\".\n\n  16) (15.1) This section looks like it no longer belongs in the document,\n      since there is no actual discussion of Basic authentication in the\n      HTTP/1.1 spec.  It is also duplicated in the authentication draft\n      (draft-ietf-http-authentication-00, 21 November 1997, section 4.1).\n\n  17) (19.3) The fourth paragraph states \"... no label is preferred over the\n      labels US-ASCII or ISO-8859-1.\"  That can either be read as \"There is\n      no label that we prefer more than US-ASCII and ISO-8859-1\" or \"We\n      prefer an unlabeled character set over the US-ASCII and ISO-8859-1\n      labels.\"  I'm confused enough that I can't even guess which was meant.\n\n  18) (19.8.1) There are two odd line breaks in the phase \"part of a\n      quoted-string\".\n\n  19) (19.8.3) This section begs for either a citation of the old\n      specification or a description of it.\n\nMany thanks to you and the rest of the editting group - this is a mammoth\nwork and of great importance to a lot of us.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Since there is so much discussion about this, I have put up the\nlatest editing copy of the MHTML standards draft by FTP, in case\none of you HTTP people want to look at it and find if there\nare other cases where we have unnecessary discrepancies between\nHTTP and MHTML.\n\nYou can get the document from URL\n<ftp://ftp.dsv.su.se/users/jpalme/draft-ietf-mhtml-rev-04X.txt>\n\nNote: This is not an official IETF draft, and the present text\nhas not yet been approved by all three editors, but I am putting\nit up immediately so that you can see it while the discussion\nis current.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 14.20 -0800 98-01-13, Jim Gettys wrote:\n> I don't really want to stir up a firestorm here, but there are several\n> issues (mostly procedural), I'd like to raise with your statement about\n> your next draft applying to HTTP...\n>\n> 1) the HTTP working group tries not to lay constraints on the\n> MHTML group.  In particular, not without negotiation with the\n> MHTML group.  I suspect we'd like a similar attitude from\n> the MHTML group.  Hopefully, this discussion is that negotiation...\n> Our specs certainly makes no statement about HTTP messages\n> being the format that mail messages must conform to.\n\nYes, of course. I think there are many cases where mail and http people\nshould communicate better in order to avoid unnecessary differences\nbetween the standards they produce. Especially since combined or\nintegrated mail and web browsers are so common, it would seem very\nsilly if a module for the display of HTML should be forced to use\ndifferent algorithms if the HTML arrived via HTTP than via SMTP.\n>\n> 2) HTTP is NOT a MIME prototol; it is really a \"MIME-like\"\n> protocol, where we've tried not to be gratuitously different.\n> (not always successfully).  HTTP != mail.\n\nHere is the charter of the MHTML group\n--- --- --- --- --- --- start charter --- --- --- --- --- ---\nMIME Encapsulation of Aggregate HTML Documents (mhtml)\n------------------------------------------------------\n\n Current Status: Active Working Group\n\n Chair(s):\n     Einar Stefferud <stef@nma.com>\n\n Applications Area Director(s):\n     Keith Moore  <moore@cs.utk.edu>\n     Harald Alvestrand  <Harald.T.Alvestrand@uninett.no>\n\n Applications Area Advisor:\n     Keith Moore  <moore@cs.utk.edu>\n\n Mailing Lists:\n     General Discussion:mhtml@segate.sunet.se\n     To Subscribe:      listserv@segate.sunet.se\n         In Body:       subscribe mhtml <full name>\n     Archive:           ftp://segate.sunet.se/lists/mhtml/\n\nDescription of Working Group:\n\nWorld Wide Web documents are most often written using Hyper Text Markup\nLanguage (HTML). HTML is notable in that it contains \"embedded\ncontent\"; that is, HTML documents often contain pointers or links to\nother objects (images, external references) which are to be presented\nto the recipient. Currently, these compound structured Web documents\nare transported almost exclusively via the interactive HTTP protocol.\nThe MHTML working group has developed three Proposed Standards (RFCs\n2110, 2111 and 2112) which permit the transport of such compound\nstructured Web documents via Internet mail in MIME multipart/related\nbody parts.\n\nThe Proposed Standards are intended to support interoperability between\nseparate HTTP-based systems and Internet mail systems, as well as being\nsuitable for combined mail/HTTP browser systems.\n\nIt is beyond the scope of this working group to come up with standards\nfor document formats other than HTML Web documents.  However, the\nProposed Standards so far produced by the working group have been\ndesigned to allow other such formats to use similar strategies.\n--- --- --- --- --- --- end charter --- --- --- --- --- ---\n\nThere is nothing in the charter which says that MTHML is only for\nmail. It talks about MIME, and MIME is certainly common to both\nSMTP and HTTP, even if there may be minor differences in usage.\n\n> But thanks for raising the topic, in any case, as this is how\n> we can avoid being gratuitously different.\n>\n> 3) there is NO requirement that HTTP implementations support\n>  composite objects (e.g. multipart is NOT a requirement of HTTP).\n>>       It was settled\n> The HTTP working group long ago that for HTTP, such a\n> requirement was both unneeded, and actually\n> unwise (for example, the caching consequences), though transmitting\n> multipart as the payload of an HTTP message is certainly not\n> forbidden in HTTP (and used in a very small number of optional\n> places).\n>\n> So the Content-Location discussion (on the HTTP mailing list, I think,\n> should be framed in the context of HTTP requests for single objects,\n> not the transmission of composite objects (which HTTP really\n> doesn't know about at all).\n\nI believe this will soon change, whether you say it in your standard\nor not. Major browsers already accept or will soon accept receipt\nof multipart/mime, and senders will want to use them in order to\nensure that a recipient gets a full object, with all its parts, in\none single file and not as a multiple of separately retrieved files.\n\nIn some cases, this is a security issue. Suppose I send a HTML resource\nwhich contains a link to another message, and suppose both message\nchange exactly at 0:00 every day. Suppose now that the recipient\ngets the first HTML resource at 23:59:59 and the second at 0:00:01\nthe day after. They will then not be related, which might cause\nserious problems in some cases.\n>\n> Note that the security issues that attempting to deal with problems\n> of caching composite objects (as independent, named objects)\n> are far from trivial for HTTP; you'd have to worry about what\n> happens if a composite object claims the name of some part\n> of the namespace not under the origin server's control, for\n> example.  This is a problem that Mail does not have, but that\n> would make HTTP's life difficult...  I get headaches just thinking\n> about the spoofing problems possible, and the problems caching\n> proxy servers would have implementing such a model....\n\nIn the case of composite objects, MHTML very clearly says that\nif you send a composite object, then you can only cache them\ntogether. An object labelled Content-Location: foo in a multipart\nMIME object, may not at a later time be identical to the object\nyou might be able to retrieve using the URL in the Content-\nLocation. The composite object reflects the state of the resource\nat the single time it is sent, and later changes in the parts\ndoes not change what the recipient has already received. This\nis very important, because this allows MHTML to be used as\nan archiving format for full, complete HTML resources in one\nsingle archiving file, showing the status at one particular\npoint in time.\n>\n> I therefore question how much the MHTML specification should say about\n> what goes over HTTP, and am reacting to the the statement in\n> your paragraph above (possibly not justifiably, since I haven't seen\n> your not yet released draft or previews of the language).\n\nThe MHTML specification is a specification for using MIME multipart\nto construct composite objects. It is not directly connected to any\nparticular protocol (SMTP, HTTP, NNTP, FTP, or whatever) used to\ntransport the MHTML object.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Minor errors in http/1.1 draft, rev. 01, from Ronald Tschalaer",
            "content": "To get them into the mailing list archive (and so that people can\ncomplain if they think the changes are not just of editorial nature).\n\nI'm assigning it editorial issue RONALD.\n- Jim Gettys\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n\nattached mail follows:\n\nHere are few typos in the latest draft (draft-ietf-http-v11-spec-rev-01.txt)\nI've noticed.\n\n\n1) Section 10.3.7:\n\n       If the 302 status code is received in response to a request other than\n       GET or HEAD, the user agent MUST NOT automatically redirect the request\n       unless it can be confirmed by the user, since this might change the\n       conditions under which the request was issued.\n\n   ought to be\n\n       If the 307 status code is received in response to a request other than\n       GET or HEAD, the user agent MUST NOT automatically redirect the request\n       unless it can be confirmed by the user, since this might change the\n       conditions under which the request was issued.\n\n   (i.e. \"307\" instead of \"302\")\n\n\n2) Section 14.3:\n\n              Accept-Encoding: gzip;q=1.0; identity=0.5; *;q=0\n   ought to be\n              Accept-Encoding: gzip;q=1.0, identity;q=0.5, *;q=0\n\n   (various \";\" where \",\" should be used, and a missing \";q\").\n\n\n3) Section 14.48:\n\n              TE: chunked; deflate;q=0.5\n   ought to be\n              TE: chunked, deflate;q=0.5\n\n   (\";\" where \",\" should be used)\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "> \n>   We would actually prefer to see this set of rules made more general\n>   in that we'd like it to apply to any POST, not just one being\n>   retried (which may or may not have been what was intended).\n> \nIf the 100 is only supposed to happen on a retried request, then \nhow does a server know if its a retried request or not ?\n\n>   As to Yolands' suggestion above, I don't really see the point (we\n>   certainly won't be sending all those extra responses), but since it\n>   specifies only client behaviour we don't care much.\nIt raises some interesting issues for async notifications.\n\nBTW: was 'Yoland' meant to be an abbreviation for Yaron Goland ? :)\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: MUST use ContentBas",
            "content": "jg@pa.dec.com (Jim Gettys) wrote:\n>>  From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n>\n>(material elided...)\n>\n>>  When I was rewriting the URI specification and arguing with the MHTML\n>>  group, I came to the conclusion that Content-Base is not needed provided\n>>  that Content-Location is implemented as specified.  The reasoning was\n>>  similar to what Dave Morris mentioned: the only person capable of knowing\n>>  whether or not the embedded references in a document are relative to\n>>  some other namespace is the document creator, and they are better-off\n>>  making that distinction within the document.  Granted, some formats may\n>>  not have the equivalent of HTML's BASE, but I would argue that those\n>>  formats are very unlikely to contain relative references.\n>>  \n>\n>Do others agree with Roy's analysis?  Is this true in the face of\n>negotiated resources, where Content-Location might be used to tell you\n>where the underlying version is found? \n>\n>The minimalist in me says if we don't actually need a mechanism, or a\n>different mechanism we do need can be used to solve a problem, we shouldn't\n>have it...\n>\n>And we haven't heard other opinions (e.g. lynx, etc....).  I'd like to hear\n>from others who've formed opinions.\n\nI'm away for most of this month, with only occassional Internet\naccess, so I can't participate in any lengthy WG discussions, but happen\nto be logged in here today, so I'll summarize my opinions as someone\nassociated with Lynx, based on what I've read so far in this thread.\n\nLynx long ago implemented use of Content-Base or absolute\nURLs in Content-Location headers as a means of specifying the base\nfor resolving partial references when a BASE element is not present\nin HTML documents.  I disgree that if retained in the HTTP/1.1\nDraft Standard support for it by clients should be considered optional,\nbut have no strong opinion on whether it should be retained or deleted.\nIf deleted, this should not be with the intention of later reviving\nit with a change in rules such that it takes precedence over an actual\nBASE element in the entity-body.\n\nI don't know if ability to specify the base via an HTTP header\nis more important for XML than for HTML, but it doesn't seem to be a\ncritical need for HTML accessed via HTTP, and I've never seen either\nContent-Base or Content-Location used seriously for that purpose\nwith simple (non-multipart) HTML documents.  Both Dave's and Roy's\nposts seem to reflect thoughts about such simple HTML documents.\nThe use of these headers for multipart documents should correspond\nas well as possible with the MHTML-WG's specs, and those seem to\nbe undergoing radical change, such that I don't yet have any firm\nopinions in that regard.\n\nI doubt that anyone else associated with Lynx has strong\nopinions about these matters.  Klaus might have, but is also\nInternet-deprived this month.\n\nMy only schtick this month is that the URL -> URI draft\nstate explicitly that only one unescaped hash ('#\") is allowed\nin URIs, which if present must be a fragment delimiter, and\nthat the direction of parsing for it is irrelevant (explicitly,\nwith words in the draft, not just posts to WG lists :).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: MUST use ContentBas",
            "content": "On Wed, 14 Jan 1998, Foteos Macrides wrote:\n\n> in HTML documents.  I disgree that if retained in the HTTP/1.1\n> Draft Standard support for it by clients should be considered optional,\n> but have no strong opinion on whether it should be retained or deleted.\n> If deleted, this should not be with the intention of later reviving\n> it with a change in rules such that it takes precedence over an actual\n> BASE element in the entity-body.\n\nI don't think we need to decide at this point in time if a revival\nchanges the semantics or offers the server control over precedence.\n\nThat said, it has finally sunk into my thick skull that there is a\nsemantic difference between Content-location and <base>/Content-base.\n\nThe origin for relative URL interpretation is acquired from\nContent-location by parsing off the 'file name piece' where as the origin\nfor <base> is explicit.  The <base>/content-base allows an application\nto utilize a relative base for the whole application and to use\nrelative URL references within the document without consideration of where\na particular page appears within the structure.  Content-location is\nan alternative to the request URL and requires that all links within\npages be written with full knowledge of their own context.\n\nI think Content-base should be removed from the specification but I \ndid want to note that use of Content-location really isn't equivalent.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 08.09 -0800 98-01-14, Jim Gettys wrote:\n> You can try to make a case that it might happen if the facilities were\n>there,\n> of course).  Equally likely in my opinion though is the inverse, that mail\n> as we know it becomes pretty integrated to the Web, rather than the\n>inverse.\n> This message is being composed on a prototype mail system which has many\n> of these properties already, for example.  All mail messages I get end up\n> with a URL, the mail user agent only uses HTTP (it is written in Java),\n> and I can say from first hand experience that this has much to commend it.\n\nWhy is that an inverse to what the MHTML group is proposing? If mail gets\nmore integrated with the web, that is only more reason that aggregate\nMIME objects have the same format whatever transport method is used\nto deliver them! We are also working on a web-based e-mail system, and\nthe very popular HOTMAIL service has the same basis.\n\n> But back to the present: Mail archives in the Web are typically handled\n> by a program that takes mail messages as input and generates HTML as a set\n> of Web documents. An equally plausible extension to handle mhtml is to\n> retrieve  the attached documents at the time the HTML is generated from\n> the mail message, rather than presuming the data is inline. This requires\n> no protocol support beyond what exists today (though arguably is not as\n> atomic in nature).\n\nThe sender of a message can choose to indicate that s/he is sending the\nfull content as it looks like at send time (by including them in the\naggregate MIME object sent) or to indicate that the content of the body\nparts are to retrieved from the web at read time (by only including\nreferences to them in the MIME object sent).\n\n> Fundamentally, HTTP talks about a single document at a time; this\n> is inherent throughout the protocol; in the caching sections, and\n> all over.  All methods take a single URI as an argument; not a list\n> of URI's.  This presumption is inherent throughout the design.\n\nYes, but a single \"document\" in HTML very often consists of multiple\nparts. There was no equivalence between \"document\" and \"file\" until\nwe got the MHTML standard.\n>\n> There is alot of work on what are called \"collections\" going on in Webdav.\n> While I have my reservations on details of what they are proposing, the\n> concept is cleaner, in my humble opinion: it should be possible to\n> define a document which is a collection of related documents.\n> This would fill the scenario you outline, as I understand it,\n> along with many others.\n\nTo me it seems much cleaner to archive each document in a single file.\nRetrieving a document from a backup storage will be much easier\nif you need only retrieve one single file. The risk that parts\nget mislaid is also smaller.\n>\n> Note since such collections can have URI's of their own, it fits\n> well into the model of the Web.\n\nOf course a composite MIME object can also have a URI of its own.\nIt is *not* the same as the URI of its start object, since the URI\nof its start object will display today's weather map, while the\ncomposite MIME object will display the weather map of the day\nwhen it was generated.\n\n> But to try to introduce the idea that an object is compound by its very\n> nature to the web at this date, and trying to mix the MHTML metadata with\n> HTTP metadata, even if possible, does not seem feasible or desirable to\n> me. My complexity alarm is going off...\n\nThey are already mixed. We are for example using many headers with\nidentical names, like \"Content-Base\" and \"Content-Location\", and in that\ncase, of course, we should try to define their syntax and semantics\nin the same way. If HTTP strongly needs a header field which is not\nsuitable for SMTP or the reverse, these header fields should not\nbe given the same name in HTTP as in objects sent by SMTP.\n\nNote also that these objects can be sent through other protocols\nthan HTTP or SMTP. We have NNTP, FTP, remote file access protocols,\nPOP and IMAP. Many people believe that IMAP is going to become a\ngeneral format for access to archived document data bases, i.e.\nnot only a mail retrieval protocol. So a format for compound\ndocuments should be independent of the protocol used to transport\nthese documents.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "I want to strongly endorse Jacob's positions on why HTTP and SMTP\ntransport of MIME should not require different MIME headers for the\nsame purposes, and add the point that not all users are going to\nalways have full IP access to the whole Internet so they can at all\ntimes just reach out and grab any web page that is wished.\n\nThis is going to be especially true for archived information which by\ndefinition is a recording of what existed at the time of the archiving\naction, and MUST NOT be subject to version changes whether such\nchanges be intentional or accidental.\n\nI also want to add that WEB technology is analogous to Libraries,\nwhere the User Goes to the Materials, while Mail is orthogonal where\nthe Materials go from a sender to a receiver by means of transport by\na third party.  If the EMail SENDER will not let go of the sent\nobject, the EMail Transfer Agent Promises to not DELIVER it to the\nRecipient.\n\nI note that both Postal Systems and Libraries have existed for many\ncenturies, and that neither has yet replaced the other.  Further, I\nnote that it has been a very important aspect of both libraries and\npostal systems that most objects in libraries may easily be\ntransferred via postal services, and the objects sent through postal\nsystems can also most often be retrieved from libraries.\n\nWould that Internet Mail and Internet WEB services could work together\nso well.  I do not look forward to the day when EMail (the ultimate\nPush Technology) is replaced by the need to always go OUT to the\nlibrary to fetch my mail.  Using the web for mail reminds me of the\nconcept of agreeing on which rock in a field, under which to leave\nmessages for each other.  I call it RockMail.  Primitive at best...\n\nSo, back to our objectives.  They is simple: To agree on MIME headers\nand their definitions between WEB, and MAIL, and any other related\ntransport technologies so that we may go forward with the knowledge\nthat when we create MIME objects, the composer need not know by what\ntransport they will be moved.  This of course includes the notion of\nnot having to rewrite or restructure objects because they are about to\nbe transported via a different transport than the one in which they\narrived.\n\nI also wish to note that this kind of separation to achieve\nindepenence between protocol layers is the great value of the IP/TCP.\nAlmost all protocols above IP do not need to know what underlying\nmedia will be used to move the bits.  And. the more the merrier!\n\nCheers...\\Stef\n\nrom your message Thu, 15 Jan 1998 03:34:33 +0100:\n}\n}At 08.09 -0800 98-01-14, Jim Gettys wrote:\n}> You can try to make a case that it might happen if the facilities were\n}>there,\n}> of course).  Equally likely in my opinion though is the inverse, that mail\n}> as we know it becomes pretty integrated to the Web, rather than the\n}>inverse.\n}> This message is being composed on a prototype mail system which has many\n}> of these properties already, for example.  All mail messages I get end up\n}> with a URL, the mail user agent only uses HTTP (it is written in Java),\n}> and I can say from first hand experience that this has much to commend it.\n}\n}Why is that an inverse to what the MHTML group is proposing? If mail gets\n}more integrated with the web, that is only more reason that aggregate\n}MIME objects have the same format whatever transport method is used\n}to deliver them! We are also working on a web-based e-mail system, and\n}the very popular HOTMAIL service has the same basis.\n}\n}> But back to the present: Mail archives in the Web are typically handled\n}> by a program that takes mail messages as input and generates HTML as a set\n}> of Web documents. An equally plausible extension to handle mhtml is to\n}> retrieve  the attached documents at the time the HTML is generated from\n}> the mail message, rather than presuming the data is inline. This requires\n}> no protocol support beyond what exists today (though arguably is not as\n}> atomic in nature).\n}\n}The sender of a message can choose to indicate that s/he is sending the\n}full content as it looks like at send time (by including them in the\n}aggregate MIME object sent) or to indicate that the content of the body\n}parts are to retrieved from the web at read time (by only including\n}references to them in the MIME object sent).\n}\n}> Fundamentally, HTTP talks about a single document at a time; this\n}> is inherent throughout the protocol; in the caching sections, and\n}> all over.  All methods take a single URI as an argument; not a list\n}> of URI's.  This presumption is inherent throughout the design.\n}\n}Yes, but a single \"document\" in HTML very often consists of multiple\n}parts. There was no equivalence between \"document\" and \"file\" until\n}we got the MHTML standard.\n}>\n}> There is a lot of work on what are called \"collections\" going on in Webdav.\n}> While I have my reservations on details of what they are proposing, the\n}> concept is cleaner, in my humble opinion: it should be possible to\n}> define a document which is a collection of related documents.\n}> This would fill the scenario you outline, as I understand it,\n}> along with many others.\n}\n}To me it seems much cleaner to archive each document in a single file.\n}Retrieving a document from a backup storage will be much easier\n}if you need only retrieve one single file. The risk that parts\n}get mislaid is also smaller.\n}>\n}> Note since such collections can have URI's of their own, it fits\n}> well into the model of the Web.\n}\n}Of course a composite MIME object can also have a URI of its own.\n}It is *not* the same as the URI of its start object, since the URI\n}of its start object will display today's weather map, while the\n}composite MIME object will display the weather map of the day\n}when it was generated.\n}\n}> But to try to introduce the idea that an object is compound by its very\n}> nature to the web at this date, and trying to mix the MHTML metadata with\n}> HTTP metadata, even if possible, does not seem feasible or desirable to\n}> me. My complexity alarm is going off...\n}\n}They are already mixed. We are for example using many headers with\n}identical names, like \"Content-Base\" and \"Content-Location\", and in that\n}case, of course, we should try to define their syntax and semantics\n}in the same way. If HTTP strongly needs a header field which is not\n}suitable for SMTP or the reverse, these header fields should not\n}be given the same name in HTTP as in objects sent by SMTP.\n}\n}Note also that these objects can be sent through other protocols\n}than HTTP or SMTP. We have NNTP, FTP, remote file access protocols,\n}POP and IMAP. Many people believe that IMAP is going to become a\n}general format for access to archived document data bases, i.e.\n}not only a mail retrieval protocol. So a format for compound\n}documents should be independent of the protocol used to transport\n}these documents.\n}\n}------------------------------------------------------------------------\n}Jacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\n}for more info see URL: http://www.dsv.su.se/~jpalme\n}\n}\n\n\n\n"
        },
        {
            "subject": "deleted spa",
            "content": "spam deleted by maintainer\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se>\n>  Date: Thu, 15 Jan 1998 03:34:33 +0100\n>  To: jg@pa.dec.com (Jim Gettys), Stef@nma.com\n>  Cc: Scott Lawrence <lawrence@agranat.com>,\n>          IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Re: Multiple Content-Location headers\n>  \n>  At 08.09 -0800 98-01-14, Jim Gettys wrote:\n>  > You can try to make a case that it might happen if the facilities were\n>  >there,\n>  > of course).  Equally likely in my opinion though is the inverse, that mail\n>  > as we know it becomes pretty integrated to the Web, rather than the\n>  >inverse.\n>  > This message is being composed on a prototype mail system which has many\n>  > of these properties already, for example.  All mail messages I get end up\n>  > with a URL, the mail user agent only uses HTTP (it is written in Java),\n>  > and I can say from first hand experience that this has much to commend it.\n>  \n>  Why is that an inverse to what the MHTML group is proposing? If mail gets\n>  more integrated with the web, that is only more reason that aggregate\n>  MIME objects have the same format whatever transport method is used\n>  to deliver them! We are also working on a web-based e-mail system, and\n>  the very popular HOTMAIL service has the same basis.\n\nWe would question that they should be aggregated into a single object.\nCollections, along with what the URN folks are trying to do, might\nbe a more interesting way to achieve the end you are looking for.\n\n>  \n>  > But back to the present: Mail archives in the Web are typically handled\n>  > by a program that takes mail messages as input and generates HTML as a set\n>  > of Web documents. An equally plausible extension to handle mhtml is to\n>  > retrieve  the attached documents at the time the HTML is generated from\n>  > the mail message, rather than presuming the data is inline. This requires\n>  > no protocol support beyond what exists today (though arguably is not as\n>  > atomic in nature).\n>  \n>  The sender of a message can choose to indicate that s/he is sending the\n>  full content as it looks like at send time (by including them in the\n>  aggregate MIME object sent) or to indicate that the content of the body\n>  parts are to retrieved from the web at read time (by only including\n>  references to them in the MIME object sent).\n>  \n>  > Fundamentally, HTTP talks about a single document at a time; this\n>  > is inherent throughout the protocol; in the caching sections, and\n>  > all over.  All methods take a single URI as an argument; not a list\n>  > of URI's.  This presumption is inherent throughout the design.\n>  \n>  Yes, but a single \"document\" in HTML very often consists of multiple\n>  parts. There was no equivalence between \"document\" and \"file\" until\n>  we got the MHTML standard.\n>  >\n>  > There is alot of work on what are called \"collections\" going on in Webdav.\n>  > While I have my reservations on details of what they are proposing, the\n>  > concept is cleaner, in my humble opinion: it should be possible to\n>  > define a document which is a collection of related documents.\n>  > This would fill the scenario you outline, as I understand it,\n>  > along with many others.\n>  \n>  To me it seems much cleaner to archive each document in a single file.\n>  Retrieving a document from a backup storage will be much easier\n>  if you need only retrieve one single file. The risk that parts\n>  get mislaid is also smaller.\n\nDocuments != files in the web.  A document may have many representations,\neither different Content-type, Content-language, etc.  A single name\nmay have N variants.\n\n>  >\n>  > Note since such collections can have URI's of their own, it fits\n>  > well into the model of the Web.\n>  \n>  Of course a composite MIME object can also have a URI of its own.\n>  It is *not* the same as the URI of its start object, since the URI\n>  of its start object will display today's weather map, while the\n>  composite MIME object will display the weather map of the day\n>  when it was generated.\n>  \n>  > But to try to introduce the idea that an object is compound by its very\n>  > nature to the web at this date, and trying to mix the MHTML metadata with\n>  > HTTP metadata, even if possible, does not seem feasible or desirable to\n>  > me. My complexity alarm is going off...\n>  \n>  They are already mixed. We are for example using many headers with\n>  identical names, like \"Content-Base\" and \"Content-Location\", and in that\n>  case, of course, we should try to define their syntax and semantics\n>  in the same way. If HTTP strongly needs a header field which is not\n>  suitable for SMTP or the reverse, these header fields should not\n>  be given the same name in HTTP as in objects sent by SMTP.\n>  \n\nWe are all in absolute agreement here about attempting to not (further) \ndefining headers with the same name that are not identical.  This is\nin fact at the root of the issue.\n\nPart of the reaction you are seeing is that the HTTP group is trying to \nget to draft standard and is focussed on this intently (the sooner the better), \nbut there is another issue you need to understand:\n\nRoy Fielding points out \n(http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0152.html) that your \nproposal would require syntactic change to Content-Location, as specified \nin HTTP/1.1 (RFC 2068).  I (and I believe others) suspect that the syntactic \nchanges required would break running, deployed implementations.\n\nHTTP/1.1 is already in very significant deployment. It is essentially \nimpossible at this date to introduce incompatible change to the HTTP protocol, \nboth on (proper) process grounds, but more importantly on pragmatic grounds \nof not breaking deployed code (which is what the process is attempts to \nensure). \n\nThis is reality; it has nothing to do with \"cleanlyness\" in the abstract.\n\nSo right now, most people (including myself) in the HTTP group aren't even \nexamining the proposal on its merits: with the name Content-Location, it \nis almost impossible on both pragmatic implementation grounds and on IETF \nprocess grounds (which are driven by these realities of deployment of running \ncode) to deal with the proposal.\n\nNow, to go further in this discussion, we need to be convinced that the \nproposal won't break deployed code. The burden of proof is on you at this \npoint, as the one proposing the change.  So as I see it, either show that \n1) the change won't break running deployed HTTP code, or 2) change the name \nto something else, so it won't break deployed code, and therefore HTTP might \nbe able to implement it.  \n\nOnce we get beyond this point, forced by the existing deployment of HTTP/1.1, \nthe HTTP working group might actually try to understand the problem and \nproposal in more detail....  Then we might be able to reach a consensus \non the merits of the case.  But right now we're stuck.\n\nRegards,\n- Jim Gettys\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "RE: MUST use ContentBas",
            "content": "Actually we don't support ANY mechanism which defines base information\noutside the scope of the document and it is unlikely that we ever will. It\nscrews up the protocol stack (as if http-equiv wasn't bad enough) and we\nthink it is just confusing and unnecessary.\n\nNote that we don't explicitly define the require-uri as the default base,\nrather we allow for the concept of references relative to \"current\nlocation\", whatever that may be. In the case of a net resource, the current\nlocation is its URI.\n\nYaron\n\n\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Wednesday, January 14, 1998 11:23 AM\n> To:Roy T. Fielding\n> Cc:Yaron Goland; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: MUST use Content-Base\n> \n> \n> >  From: \"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu>\n> \n> (material elided...)\n> \n> >  When I was rewriting the URI specification and arguing with the MHTML\n> >  group, I came to the conclusion that Content-Base is not needed\n> provided\n> >  that Content-Location is implemented as specified.  The reasoning was\n> >  similar to what Dave Morris mentioned: the only person capable of\n> knowing\n> >  whether or not the embedded references in a document are relative to\n> >  some other namespace is the document creator, and they are better-off\n> >  making that distinction within the document.  Granted, some formats may\n> >  not have the equivalent of HTML's BASE, but I would argue that those\n> >  formats are very unlikely to contain relative references.\n> >  \n> \n> Do others agree with Roy's analysis?  Is this true in the face of\n> negotiated resources, where Content-Location might be used to tell you\n> where the underlying version is found? \n> \n> The minimalist in me says if we don't actually need a mechanism, or a\n> different \n> mechanism we do need can be used to solve a problem, we shouldn't have\n> it...\n> \n> And we haven't heard other opinions (e.g. lynx, etc....).  I'd like to\n> hear\n> from others who've formed opinions.\n> - Jim\n> --\n> Jim Gettys\n> Industry Standards and Consortia\n> Digital Equipment Corporation\n> Visting Scientist, World Wide Web Consortium, M.I.T.\n> http://www.w3.org/People/Gettys/\n> jg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": ">> We would actually prefer to see this set of rules made more general\n>> in that we'd like it to apply to any POST, not just one being\n>> retried (which may or may not have been what was intended).\n\n>>>>> \"JC\" == Josh Cohen <josh@netscape.com> replies:\n\nJC> If the 100 is only supposed to happen on a retried request, then\nJC> how does a server know if its a retried request or not ?\n\n  I was unclear - what I meant was that I would like to see the client\n  always wait for a 100 response following the headers on a 1.1 POST,\n  not just when it is retrying one that was interrupted.\n\n  Our current behaviour is that we will send the 100 Continue after\n  reading the headers on any 1.1 POST which we think is ok (it may\n  still be rejected if the authentication digest does not check out,\n  but we can't know that until we get the body).\n\nJC> BTW: was 'Yoland' meant to be an abbreviation for Yaron Goland ? :)\n\n  Yes, my apologies, Yaron - my fingers are faster than my brain\n  sometimes.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: editorial glitch in section 14.19 (now INTHEORY editorial issue",
            "content": "I've been wading through filed mail since IETF; unless I hear complaints,\nthe changes in this message Jeff proposes will be adopted.\nThere has been no comment on this since Jeff's original posting.\nAs it is a clarification involving normative wording, I wanted to flag\nit for people's attention.\n\n- Jim\n\n(http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q4/0378.html)\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "New content negotiation draft availabl",
            "content": "I have submitted a slightly updated version of the main transparent\ncontent negotiation draft, draft-ietf-http-negotiation-05.txt, and\nmade it available at the usual place:\n\n   http://gewis.win.tue.nl/~koen/conneg/\n\nTo make room for independent developments on feature tag registration,\nsome small changes were made to the feature tag parts of the draft:\nfeature tags may now be quoted, so that one can use URLs as feature\ntags, a reference to work on feature tag registration procedures has\nbeen added, and the syntax has been changed slightly to make\nfault-tolerant parsing of feature tags which are URLs easier.\n\nThe web page above has also been updated to reflect recent revisions\nof other drafts by Ted Hardie.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Has the ContentLength issue been resolved",
            "content": "Has the Content-Length issue be resolved?  I.e. is the Content-Length\nvalue the length before or after a Transfer-encoding is supplied?\n\nWhichever it is, it must be clearly stated in the specification and\nI don't see this on the issues list.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 08.34 -0800 98-01-15, Jim Gettys wrote:\n> Now, to go further in this discussion, we need to be convinced that the\n> proposal won't break deployed code. The burden of proof is on you at this\n> point, as the one proposing the change.  So as I see it, either show that\n> 1) the change won't break running deployed HTTP code, or 2) change the name\n> to something else, so it won't break deployed code, and therefore HTTP might\n> be able to implement it.\n\nMost implementors I have communicated with use the same code to display\naggregate HTML whether this HTML arrives via e-mail or via HTTP. So their\ncode is not broken, but aided by having the same format for aggregate\nHTML objects, independent of transport protocol.\n\nAssuming that we define a new header Content-Alternate or something\nlike that, what existing code would be broken by MHTML and in what way?\nI do not think that even the MHTML group original proposal would break\nany existing code, but if the HTTP group so wants, we are willing to\ndefine Content-Alternate to avoid multiple Content-Location headers.\n\nCan you give an example of what would break existing code in what way?\n Of course HTTP clients which are not capable of handling multipart\nMIME objects may have problems, but you cannot stop all development\nof new features just because old implementations will have problems\nhandling new features. The common method in HTML browsers to handle\nnew features is just to ignore the HTML elements they cannot understand.\nThis would work rather well in our case; the effect would be that\nif a document contained the same picture in several places, the\nrendering with such HTML browser would show the picture only in\none of the places, and that is not very unreasonable, is it?\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\nLocation\nfield MAY be used to supply the     header need not refer to an\nresource location for the entity    resource which is globally\nenclosed in the message  when that  available for retrieval using this\nentity is accessible from a         URI (after resolution of relative\nlocation separate from the          URIs). However, URI-s in\nrequested resource's URI.           Content-Location headers (if\n                                    absolute, or resolvable to\n                                    absolute URIs) SHOULD still be\n                                    globally unique.\n\nA cache cannot assume that an       When processing (rendering) a\nentity with a Content-Location      text/html body part in an MHTML\ndifferent from the URI used to      multipart/related structure, all\nretrieve it can be used to respond  URIs in that text/html body part\nto later requests on that Content-  which reference subsidiary\nLocation URI. However, the Content- resources within the same\nLocation can be used to             multipart/related structure SHALL\ndifferentiate between multiple      be satisfied by those resources\nentities retrieved from a single    and not by resources from any\nrequested resource, as described    another local or remote source.\nin section Caching Negotiated\nResponses.                          Therefore, If a sender wishes a\n                                    recipient to always retrieve an\n...                                 URI referenced resource from its\n                                    source, an URI labeled copy of\nIf a single server supports         that resource MUST NOT be included\nmultiple organizations that do not  in the same multipart/related\ntrust one another, then it must     structure.\ncheck the values of Location and\nContent-Location headers in         In addition, since the source of a\nresponses that are generated under  resource received in\ncontrol of said organizations to    multipart/related structure can be\nmake sure that they do not attempt  misrepresented (see 12.1 above),\nto invalidate resources over which  if a resource received in\nthey have no authority.             multipart/related structure is\n                                    stored in a cache, it MUST NOT be\n                                    retrieved from that cache other\n                                    than by a reference contained in a\n                                    body part of the same\n                                    multipart/related structure.\n                                    Failure to honor this directive\n                                    will allow a multipart/related\n                                    structure to be employed as a\n                                    Trojan Horse. For example, to\n                                    inject bogus resources (i.e. a\n                                    misrepresentation of a\n                                    competitor's Web site) into a\n                                    recipient's generally accessible\n                                    Web cache.\n\nMy feeling is that the use of Content-Location as defined in the HTTP\nand MHTML spec is not so different as to require us to use different\nheaders. But could the HTTP people please examine the quotes above\nand check what you feel about this.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se> >  Date: Thu, 15 Jan 1998 20:55:42 \n+0100 >  To: Nick Shelness <shelness@lotus.com>, jg@pa.dec.com (Jim Gettys) \n>  Cc: IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>, >          \nhttp-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com >  Subject: Re: Multiple \nContent-Location headers >  >  At 17.21 +0000 98-01-15, \nNick_Shelness@motorcity2.lotus.com wrote: >  > Could I suggest that to break \nthis impasse, that MHTML switches to a new >  > header field Content-Label \nto replace its use of Content-Location. This >  > would better capture the \nMHTML role of the header field, and would also >  > allow the simplifications \nI argued for last week on the MHTML list to >  > proceed. I.e., Content-Label \ncould only specify an absolute URI, and would >  > not establish a base. \n>  >  I am not very happy with changing an existing and already implemented \n>  IETF proposed standard in such a radical way. But maybe it is necessary. \n>  Let us examine the differences between how MHTML and HTTP uses Content- \n>  Location to see if they really need to be split into two different >  \nheader fields. >  >  HTTP 1.1 spec says                  MHTML spec says \n(I have removed >                                      the controversial \ntext allowing >                                      multiple Content-Location \nheaders, >                                      since we all agree to remove \n>                                      this.) >  >  In HTTP, multipart \nbody-parts MAY   A Content-Location header >  contain header fields which \nare     specifies an URI that labels the >  significant to the meaning of \nthat  content of a body part in whose >  part. A Content-Location header     \nheading it is placed. Its value >  field SHOULD be included in the     CAN \nbe an absolute or a relative >  body-part of each enclosed entity   URI. \n>  that can be identified by a URL. >                                      \nA Content-Location header field is >                                      \nallowed in any message or content >                                      \nheading, in addition to one >                                      Content-ID \nheader (as specified in >                                      [MIME1]) \nand, in Message headings, >                                      one Message-ID \n(as specified in >                                      [RFC822]) >  >  \nThe Content-Location entity-header  An URI in a Content-Location >  field \nMAY be used to supply the     header need not refer to an >  resource location \nfor the entity    resource which is globally >  enclosed in the message  \nwhen that  available for retrieval using this >  entity is accessible from \na         URI (after resolution of relative >  location separate from \nthe          URIs). However, URI-s in >  requested resource's URI.           \nContent-Location headers (if >                                      absolute, \nor resolvable to >                                      absolute URIs) SHOULD \nstill be >                                      globally unique. >  >  A \ncache cannot assume that an       When processing (rendering) a >  entity \nwith a Content-Location      text/html body part in an MHTML >  different \nfrom the URI used to      multipart/related structure, all >  retrieve it \ncan be used to respond  URIs in that text/html body part >  to later requests \non that Content-  which reference subsidiary >  Location URI. However, the \nContent- resources within the same >  Location can be used to             \nmultipart/related structure SHALL >  differentiate between multiple      \nbe satisfied by those resources >  entities retrieved from a single    and \nnot by resources from any >  requested resource, as described    another \nlocal or remote source. >  in section Caching Negotiated >  \nResponses.                          Therefore, If a sender wishes a \n>                                      recipient to always retrieve an >  \n...                                 URI referenced resource from its \n>                                      source, an URI labeled copy of >  \nIf a single server supports         that resource MUST NOT be included >  \nmultiple organizations that do not  in the same multipart/related >  trust \none another, then it must     structure. >  check the values of Location \nand >  Content-Location headers in         In addition, since the source \nof a >  responses that are generated under  resource received in >  control \nof said organizations to    multipart/related structure can be >  make sure \nthat they do not attempt  misrepresented (see 12.1 above), >  to invalidate \nresources over which  if a resource received in >  they have no \nauthority.             multipart/related structure is \n>                                      stored in a cache, it MUST NOT be \n>                                      retrieved from that cache other \n>                                      than by a reference contained in \na >                                      body part of the same \n>                                      multipart/related structure. \n>                                      Failure to honor this directive \n>                                      will allow a multipart/related \n>                                      structure to be employed as a \n>                                      Trojan Horse. For example, to \n>                                      inject bogus resources (i.e. a \n>                                      misrepresentation of a \n>                                      competitor's Web site) into a \n>                                      recipient's generally accessible \n>                                      Web cache. \n>  \n>  My feeling is that the use of Content-Location as defined in the HTTP \n>  and MHTML spec is not so different as to require us to use different \n>  headers. But could the HTTP people please examine the quotes above \n>  and check what you feel about this. \n> \n\nThe problem we have is syntax and implementation, not semantics.  \nLets clear this hurdle before we get into the meat of what you are trying \nto achieve, and whether your suggestion fits into the architecture of the \nWeb, and my apologies of jumping into the meat in some of my early messages \non this topic.\n\nRoy Fielding's point is that the syntax change required to allow the header \nname Content-Location to have multiple fields (needed as that is what proxies \ntypically do if they find multiple headers of the same name), is a problem, \nand one that may (likely) break exisiting implementations.  It is also \npossible/likely this would break existing applications of HTTP, particularly \nclients and proxies.  To include the URI in a comma separated list would \nrequire quoting of the URI's, as Roy points out; parsers may not be coded \ncorrectly to deal with this.  It is quite likely that existing implementations \nwill get the wrong answer, or even die, if one attempts to have multiple \nContent-Location headers, or that would not understand the quoting that\nthis would require.  And then there are the proxy issues....\n\nTo quote from section 4.2 of the HTTP spec:\n\n\"Multiple message-header fields with the same field-name may be present in \na message if and only if the entire field-value for that header field is \ndefined as a comma-separated list [i.e., #(values)]. It MUST be possible \nto combine the multiple header fields into one \"field-name: field-value\" \npair, without changing the semantics of the message, by appending each \nsubsequent field-value to the first, each separated by a comma. The order \nin which header fields with the same field-name are received is therefore \nsignificant to the interpretation of the combined field value, and thus \na proxy MUST NOT change the order of these field values when a message is \nforwarded.\"\n\nThese are the cruxes of the problem.  So we're trying to follow the doctor's \nmaxim \"first, do no harm\". We aren't worrying (yet) about the semantic issues \nthat may or may not exist between how Content-Location is defined in the \ntwo different specs, but pointing out that allowing multiple of \nContent-Location headers is an incompatible change which may break \nimplementations, and we have no data which shows this change is harmless.\n\nSo until it is shown to be harmless, we must presume harm.  IETF process\nattempts to avoid regression; we're worried that existing, deployed software\nwould stop working, possibly in significant ways.\n\nSo, please, as in my previous message, either present data that it\ndoesn't break implementations, or don't argue about the name.  Otherwise\nwe're going to continue to bog down.  I think that will let us all\nmake faster progress.\n\nI hope this clarifies where the difficulty lies.\n\n- Jim Gettys\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>To quote from section 4.2 of the HTTP spec:\n>\n>\"Multiple message-header fields with the same field-name may be present in\n>a message if and only if the entire field-value for that header field is\n>defined as a comma-separated list [i.e., #(values)]. It MUST be possible\n>to combine the multiple header fields into one \"field-name: field-value\"\n>pair, without changing the semantics of the message, by appending each\n>subsequent field-value to the first, each separated by a comma.\n\nAnd to add from the long-expired-but-still-regarded-as-authoritative\nCGI Internet Draft (draft-robinson-www-interface-01.txt, 15 February\n1996) in section 5 \"Environment Variables\" (pp. 8):\n\n   \"If multiple headers with the same field-name are received then\n   they must be rewritten as a single header having the same\n   semantics.\"\n\nNot only must it be *possible* to normalize headers, when running a CGI\nprogram all headers *must* be normalized for its pool of variables.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: New content negotiation draft availabl",
            "content": "I had a couple of comments on the content negotiation draft. I am new to\nthis discussion, so I apologize if these comments have been addressed\nbefore.\n\n1. I am curious why Section 5.4 makes the inclusion of type, language,\ncharset, and length attributes optional. Are there some cases where it\nwould be difficult for a web server/proxy to determine these\ncharacteristics at the time it generates the variant list? The only\nexamples I can think of are cgi-bin scripts and streaming documents that\ndo not have a fixed size. Additionally, I am also curious why the server\ndoes not have to maintain a one-to-one correspondence between the\nattributes in the variant description and the relevant HTTP Content-*\nheaders. Obviously, making these fields required and identical to those in\nthe HTTP header would make a client's job of choosing the most appropriate\nvariant much easier, and unless there is a reason that this is difficult,\nit might be useful to make these MUSTs.\n\n2. The list of alternates in an \"Alternates:\" header currently uses the\nURI of documents for identification. It seems that there might be some\nadvantages to using the URL of the document instead, e.g. the example in\nSection 4.3:\n\n>Alternates: {\"paper.1\" 0.9 {type text/html} {language en}},\n>                 {\"paper.2\" 0.7 {type text/html} {language fr}},\n>                 {\"paper.3\" 1.0 {type application/postscript}\n\nbecomes:\n\n>Alternates: {\"http://x.org/paper.1\" 0.9 {type text/html} {language en}},\n>                 {\"http://x.org/paper.2\" 0.7 {type text/html} {language\nfr}},\n>                 {\"http://x.org/paper.3\" 1.0 {type\napplication/postscript}\n\nUsing URLs instead of URIs has the significant advantage that it provides\na simple mechanism for pointing clients to mirror locations that replicate\nthe same content. For example, the Internet Movie Database could have an\n\"Alternates:\" header like:\n\nAlternates: {\"http://www.imdb.com/\" 1.0 {type text/html} {language en}},\n                 {\"http://uk.imdb.com/\" 0.7 {type text/html} {language\nen}},\n                 {\"http://italy.imdb.com/\" 0.7 {type text/html} {language\nit}}\n\nI don't know of any existing mechanism to do replication of this nature\nother than mapping a DNS name to multiple addresses, which doesn't handle\nreplicas on sites that are not a part of the same administrative domain\nand does not allow fine-grained replica support.\n\nI'd appreciate your comments, and thanks.\n\n        --Mark\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Hi Jim -- \n\nI hope the garbling of all that included text was an accident;-)...  I\nwas gretly revlied to discover that there were no comments inserted\nthere-in;-)...  (I trust such garbelling is not considered to be a\nuseful feature of WEB Mail UAs;-)...\n\nNext, I think we may be out of synch in the discussion.\n\nMHTML folk almost immediately gave up on the ideas of allowing\nmultiple Content-location headers, or of giving them multiple\nvalues...  \n\nThis is no longer any kind of an issue between HTTP and MHTML!!!!\nWe are now looking for two other new things:\n\n1.  Are there any other gotchas lurking in the HTTP/MHTML wood pile\n    that we have not noticed before, since all us woodpile residents\n    would like to avoid all possible hidden gotchas???\n\n2.  Can we use the new idea articulated by Nick Shelness to use a new\n    Content-Label header, or allow a new Content-Alternate-Location?\n\nI think MHTML is leaning toward Content-Alternate-Location, but lets\nconsider both in looking for gotchas.\n\nCheers...\\Stef\n\nrom your message Thu, 15 Jan 1998 12:57:56 -0800:\n}\n}>  From: Jacob Palme <jpalme@dsv.su.se> >  Date: Thu, 15 Jan 1998 20:55:42\n}+0100 >  To: Nick Shelness <shelness@lotus.com>, jg@pa.dec.com (Jim Gettys)\n}>  Cc: IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>, >\n}http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com >  Subject: Re: Multiple\n}Content-Location headers >  >  At 17.21 +0000 98-01-15,\n}Nick_Shelness@motorcity2.lotus.com wrote: >  > Could I suggest that to break\n}this impasse, that MHTML switches to a new >  > header field Content-Label\n}to replace its use of Content-Location. This >  > would better capture the\n}MHTML role of the header field, and would also >  > allow the simplifications\n}I argued for last week on the MHTML list to >  > proceed. I.e., Content-Label\n}could only specify an absolute URI, and would >  > not establish a base.\n}>  >  I am not very happy with changing an existing and already implemented\n}>  IETF proposed standard in such a radical way. But maybe it is necessary.\n}>  Let us examine the differences between how MHTML and HTTP uses Content-\n}>  Location to see if they really need to be split into two different >\n}header fields. >  >  HTTP 1.1 spec says                  MHTML spec says\n}(I have removed >                                      the controversial\n}text allowing >                                      multiple Content-Location\n}headers, >                                      since we all agree to remove\n}>                                      this.) >  >  In HTTP, multipart\n}body-parts MAY   A Content-Location header >  contain header fields which\n}are     specifies an URI that labels the >  significant to the meaning of\n}that  content of a body part in whose >  part. A Content-Location header\n}heading it is placed. Its value >  field SHOULD be included in the     CAN\n}be an absolute or a relative >  body-part of each enclosed entity   URI.\n}>  that can be identified by a URL. >\n}A Content-Location header field is >\n}allowed in any message or content >\n}heading, in addition to one >                                      Content-ID\n}header (as specified in >                                      [MIME1])\n}and, in Message headings, >                                      one Message-ID\n}(as specified in >                                      [RFC822]) >  >\n}The Content-Location entity-header  An URI in a Content-Location >  field\n}MAY be used to supply the     header need not refer to an >  resource location\n}for the entity    resource which is globally >  enclosed in the message\n}when that  available for retrieval using this >  entity is accessible from\n}a         URI (after resolution of relative >  location separate from\n}the          URIs). However, URI-s in >  requested resource's URI.\n}Content-Location headers (if >                                      absolute,\n}or resolvable to >                                      absolute URIs) SHOULD\n}still be >                                      globally unique. >  >  A\n}cache cannot assume that an       When processing (rendering) a >  entity\n}with a Content-Location      text/html body part in an MHTML >  different\n}from the URI used to      multipart/related structure, all >  retrieve it\n}can be used to respond  URIs in that text/html body part >  to later requests\n}on that Content-  which reference subsidiary >  Location URI. However, the\n}Content- resources within the same >  Location can be used to\n}multipart/related structure SHALL >  differentiate between multiple\n}be satisfied by those resources >  entities retrieved from a single    and\n}not by resources from any >  requested resource, as described    another\n}local or remote source. >  in section Caching Negotiated >\n}Responses.                          Therefore, If a sender wishes a\n}>                                      recipient to always retrieve an >\n}...                                 URI referenced resource from its\n}>                                      source, an URI labeled copy of >\n}If a single server supports         that resource MUST NOT be included >\n}multiple organizations that do not  in the same multipart/related >  trust\n}one another, then it must     structure. >  check the values of Location\n}and >  Content-Location headers in         In addition, since the source\n}of a >  responses that are generated under  resource received in >  control\n}of said organizations to    multipart/related structure can be >  make sure\n}that they do not attempt  misrepresented (see 12.1 above), >  to invalidate\n}resources over which  if a resource received in >  they have no\n}authority.             multipart/related structure is\n}>                                      stored in a cache, it MUST NOT be\n}>                                      retrieved from that cache other\n}>                                      than by a reference contained in\n}a >                                      body part of the same\n}>                                      multipart/related structure.\n}>                                      Failure to honor this directive\n}>                                      will allow a multipart/related\n}>                                      structure to be employed as a\n}>                                      Trojan Horse. For example, to\n}>                                      inject bogus resources (i.e. a\n}>                                      misrepresentation of a\n}>                                      competitor's Web site) into a\n}>                                      recipient's generally accessible\n}>                                      Web cache.\n}>\n}>  My feeling is that the use of Content-Location as defined in the HTTP\n}>  and MHTML spec is not so different as to require us to use different\n}>  headers. But could the HTTP people please examine the quotes above\n}>  and check what you feel about this.\n}>\n}\n}The problem we have is syntax and implementation, not semantics.\n}Lets clear this hurdle before we get into the meat of what you are trying\n}to achieve, and whether your suggestion fits into the architecture of the\n}Web, and my apologies of jumping into the meat in some of my early messages\n}on this topic.\n}\n}Roy Fielding's point is that the syntax change required to allow the header\n}name Content-Location to have multiple fields (needed as that is what proxies\n}typically do if they find multiple headers of the same name), is a problem,\n}and one that may (likely) break exisiting implementations.  It is also\n}possible/likely this would break existing applications of HTTP, particularly\n}clients and proxies.  To include the URI in a comma separated list would\n}require quoting of the URI's, as Roy points out; parsers may not be coded\n}correctly to deal with this.  It is quite likely that existing implementations\n}will get the wrong answer, or even die, if one attempts to have multiple\n}Content-Location headers, or that would not understand the quoting that\n}this would require.  And then there are the proxy issues....\n}\n}To quote from section 4.2 of the HTTP spec:\n}\n}\"Multiple message-header fields with the same field-name may be present in\n}a message if and only if the entire field-value for that header field is\n}defined as a comma-separated list [i.e., #(values)]. It MUST be possible\n}to combine the multiple header fields into one \"field-name: field-value\"\n}pair, without changing the semantics of the message, by appending each\n}subsequent field-value to the first, each separated by a comma. The order\n}in which header fields with the same field-name are received is therefore\n}significant to the interpretation of the combined field value, and thus\n}a proxy MUST NOT change the order of these field values when a message is\n}forwarded.\"\n}\n}These are the cruxes of the problem.  So we're trying to follow the doctor's\n}maxim \"first, do no harm\". We aren't worrying (yet) about the semantic issues\n}that may or may not exist between how Content-Location is defined in the\n}two different specs, but pointing out that allowing multiple of\n}Content-Location headers is an incompatible change which may break\n}implementations, and we have no data which shows this change is harmless.\n}\n}So until it is shown to be harmless, we must presume harm.  IETF process\n}attempts to avoid regression; we're worried that existing, deployed software\n}would stop working, possibly in significant ways.\n}\n}So, please, as in my previous message, either present data that it\n}doesn't break implementations, or don't argue about the name.  Otherwise\n}we're going to continue to bog down.  I think that will let us all\n}make faster progress.\n}\n}I hope this clarifies where the difficulty lies.\n}\n                        - Jim Gettys\n}\n}\n}--\n}Jim Gettys\n}Industry Standards and Consortia\n}Digital Equipment Corporation\n}Visting Scientist, World Wide Web Consortium, M.I.T.\n}http://www.w3.org/People/Gettys/\n}jg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "chunked enc. (Was: Re: draft minutes, HTTPWG meetings April 7",
            "content": "> In the discussion, the following issues seemed to have sufficient\n> consensus in the meeting that \"last call\" will be sent to the\n> mailing list for each of them:\n>    \"chunked encoding\" clarification\n\nThe clarification was that leading zeroes would be allowed in the hexadecimal\nbyte count.\n\nAs I thought about this a little, I realized some additional tweaking would\nbe required in the draft, both in the syntax and the description.  In\nparticular, the syntax probably becomes\nchunk-size= 1*HEX\nbut the description will need a caveat that the value of chunk-size must be\nnon-zero, except for the last chunk.  Should the last chunk always be denoted\nby \"0\" CRLF, or by 1*HEX CRLF, where the value of 1*HEX value is zero?  If\nthe former, are we comfortable with zero-length, non-final chunks?  There\ncould be the ambiguous case of chunk-size being, say \"00\".\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 20.55 +0000 98-01-15, Nick_Shelness@motorcity2.lotus.com wrote:\n> The reason for changing from my previous align MHTML with HTTP 1.1 position\n> (which you also enunciate), to my employ a new header field position, was\n> because I was concerned that the MIME folding algorithm we apply to header\n> fields containing invalid URIs would be incompatible with HTTP 1.1. HTTP\n> 1.1 can outlaw invalid URIs, MHTML has to both make them RFC822/MIME safe\n> and cope with them.\n\nThis would then require that Content-Base be replaced by a new header\nname, too. And maybe also other header fields.\n\nI think it would be a great pity if the same object could not be sent\nvia HTTP and SMTP just because of such possible syntax problems.\nFor example, you may want to receive an object via one protocol\nand further transport it via another protocol. And we have in the\nMHTML group carefully tried to avoid the need for changes in the\nobject in this case, so that digital seals are not broken.\n\nWould existing HTTP implementations get into deep trouble if they\nget header fields which are folded across several lines according\nto the conventions used in e-mail?\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "At 12.57 -0800 98-01-15, Jim Gettys wrote:\n> The problem we have is syntax and implementation, not semantics.\n> Lets clear this hurdle before we get into the meat of what you are trying\n> to achieve, and whether your suggestion fits into the architecture of the\n> Web, and my apologies of jumping into the meat in some of my early messages\n> on this topic.\n>\n> Roy Fielding's point is that the syntax change required to allow the header\n> name Content-Location to have multiple fields (needed as that is what\n>proxies\n> typically do if they find multiple headers of the same name), is a problem,\n> and one that may (likely) break exisiting implementations.\n\nBut what I suggested what to allow only one field, and one value, with the\nname Content-Location in each heading, and to define a new header field\nContent-Location-Alternate for cases where more than one is needed.\nThat would avoid your problem.\n\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "All,\n\nI have reviewed the relevant Content-Base and Content-Location language in\n<draft-ietf-http-v11-spec-rev-01.txt>. All that would be required to fully\nalign <draft-ietf-mhtml-rev-04X.txt> with this usage would be a reversion\nto allowing only 0 or 1 Content-Location header field, per content or\nmessage header. If we wish to continue to allow a resource, carried as an\nMHTML body part, to be labeled with additional URIs, then I suggest we\nadopt Stef's suggestion, though I would opt for Content-Alternate-Location\nin place of Content-Location-Alternate. This would allow us to associate\nadditional URIs with a body part for the purpose of satisfying multiple URI\nreferrences with a single body part. Note, that I am not a great fan of\nthis additional complexity, but others hav argued strongly for its\ninclusion, and rough concensus was reached.\n\nRight now, I can see no role for a Content-Alternate-Location header field\nin *single object* HTTP, but I leave for others to argue otherwise.\n\nNick\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Jacob,\n\n> I am not very happy with changing an existing and already implemented\n> IETF proposed standard in such a radical way. But maybe it is necessary.\n> Let us examine the differences between how MHTML and HTTP uses Content-\n> Location to see if they really need to be split into two different\n> header fields.\n\nThe reason for changing from my previous align MHTML with HTTP 1.1 position\n(which you also enunciate), to my employ a new header field position, was\nbecause I was concerned that the MIME folding algorithm we apply to header\nfields containing invalid URIs would be incompatible with HTTP 1.1. HTTP\n1.1 can outlaw invalid URIs, MHTML has to both make them RFC822/MIME safe\nand cope with them. The problematic text is:\n\n4.4 Encoding and decoding of URIs in MIME header fields\n\n4.4.1 Encoding of URIs containing inappropriate characters\n\nSome documents may contain URIs with characters that are inappropriate for\nan RFC 822 header, either because the URI itself has an incorrect syntax\naccording to [URL] or the URI syntax standard has been changed to allow\ncharacters not previously allowed in MIME headers. These URIs cannot be\nsent directly in a message header. If such a URI occurs, all spaces and\nother illegal characters in it must be encoded using one of the methods\ndescribed in [MIME3] section 4. This encoding MUST only be done in the\nheader, not in the HTML text. Receiving clients must decode the [MIME3]\nencoding in the heading before comparing URIs in body text to URIs in\nContent-Location headers.\n\nThe charset parameter value \"US-ASCII\" SHOULD be used if the URI contains\nno octets outside of the 7-bit range. If such octets are present, the\ncorrect charset parameter value (derived e.g. from information about the\nHTML document the URI was found in) SHOULD be used. If this cannot be\nsafely established, the value \"UKNOWN-8BIT\" [RFC 1428] MUST be used.\n\nNote, that for the matching of URIs in text/html body parts to URIs in\nContent-Location headers, the value of the charset parameter is irrelevant,\nbut that it may be relevant for other purposes, and that incorrect labeling\nMUST, therefore, be avoided. Warning: Irrelevance of the charset parameter\nmay not be true in the future, if different character encodings of the same\nnon-English filename are used in HTML.\n\n\n4.4.2 Folding of long URIs\n\nSince MIME header fields have a limited length and long URIs can result in\nContent-Location and Content-Base headers that exceed this length ,\nContent-Location and Content-Base headers may have to be folded.\n\nEncoding as discussed in clause 4.4.1 MUST be done before such folding.\nAfter that, the folding can be done, using the algorithm defined in\n[URLBODY] section 3.1.\n\n4.4.3 Unfolding and decoding of received URLs in MIME header fields\n\nUpon receipt, folded MIME header fields should be unfolded, and then any\nMIME encoding should be removed, to retrieve the original URI.\n\nNick\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "Jim,\n\n> Roy Fielding points out\n> (http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0152.html) that\nyour\n> proposal would require syntactic change to Content-Location, as specified\n> in HTTP/1.1 (RFC 2068).  I (and I believe others) suspect that the\nsyntactic\n> changes required would break running, deployed implementations.\n\n> HTTP/1.1 is already in very significant deployment. It is essentially\n> impossible at this date to introduce incompatible change to the HTTP\nprotocol,\n> both on (proper) process grounds, but more importantly on pragmatic\ngrounds\n> of not breaking deployed code (which is what the process is attempts to\n> ensure).\n\nCould I suggest that to break this impasse, that MHTML switches to a new\nheader field Content-Label to replace its use of Content-Location. This\nwould better capture the MHTML role of the header field, and would also\nallow the simplifications I argued for last week on the MHTML list to\nproceed. I.e., Content-Label could only specify an absolute URI, and would\nnot establish a base.\n\nWe also have a pre-existing definition (see RFC 2110), but the state of\nMHTML implementation may allow this late breaking change to take effect\nwithout too much negative impact. Speaking for Lotus, we can accomodate it.\n\nNick\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  Sender: stef@nma.com\n>  From: Einar Stefferud <Stef@nma.com>\n>  Date: Thu, 15 Jan 1998 16:51:09 -0800\n>  To: Jim Gettys <jg@pa.dec.com>\n>  Cc: Jacob Palme <jpalme@dsv.su.se>, Nick Shelness <shelness@lotus.com>,\n>          IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Re: Multiple Content-Location headers \n>  \n>  Hi Jim -- \n>  \n>  I hope the garbling of all that included text was an accident;-)...  I\n>  was gretly revlied to discover that there were no comments inserted\n>  there-in;-)...  (I trust such garbelling is not considered to be a\n>  useful feature of WEB Mail UAs;-)...\n> \n\nI did mention in a previous message I was using an EXPERIMENTAL mail UA. :-).\n\nI had hit the \"wrap\" button to rewrap a paragraph that Pachyderm has, and \nit did the wrong thing.  I manually undid the damage I saw, but it did alot \nmore than I saw (I didn't scroll the window back far enough).\n\n \n>  Next, I think we may be out of synch in the discussion.\n>  \n>  MHTML folk almost immediately gave up on the ideas of allowing\n>  multiple Content-location headers, or of giving them multiple\n>  values...  \n> \n\nOK, great. I think I may have added the discussion to the HTTP issues list \nyesterday; I'll go mark it closed then.  \n\n>  This is no longer any kind of an issue between HTTP and MHTML!!!!\n>  We are now looking for two other new things:\n>  \n>  1.  Are there any other gotchas lurking in the HTTP/MHTML wood pile\n>      that we have not noticed before, since all us woodpile residents\n>      would like to avoid all possible hidden gotchas???\n\nProbably.  The problem is finding people who really understand both\nto go over the specs looking for trouble.  I know I don't have time\nto catch up on MHTML at the moment, in the push to get to draft\nstandard, and due to personal events that have reduced my available time.\nIf there are people who could go looking for trouble it would be\nvery good.  Sooner rather than later, for both our sakes...\n\n>  \n>  2.  Can we use the new idea articulated by Nick Shelness to use a new\n>      Content-Label header, or allow a new Content-Alternate-Location?\n>  \n\nYes, though try to keep the header names short.  There is an important\ndifference between HTTP and mail that many overlook; HTTP is real time,\nand each byte adds to user latency (and reduces operations per second).\nMail is store and forward, and not latency sensitive.\n\nClearly for this kind of application, allowing multiple values on a single \nheader reduces the overhead of the header name.  This is usually preferable \nthan multiple headers.  (We had real trouble with naive implementations \nof Accept headers early in the Web listing each content type one per header, \ncompounding the number of bytes sent by a good factor, and causing lousy \ninteractive \"feel\"); ergo the language to allow a proxy to \"undo\" the\ndamage somewhat.\n\nDon't go overboard on this recommendation, but bear it in mind.  It is\none more area where HTTP has different requirements than mail.\n\n>  I think MHTML is leaning toward Content-Alternate-Location, but lets\n>  consider both in looking for gotchas.\n>  \n\nAs you look at this problem, you should understand the need HTTP has for \nwhat we call \"Alternates\".  \n\nThe issue in HTTP is that there may be multiple representations for an object \n(different languages, content types, etc.).  A request returns only one \nof these.  But the client is the only place where you fully understand what \nthe application is, and you'd like to know of alternate representations. \n\nA good example is that you are looking a web page for which there is both \nHTML and Postscript and Acrobat available (the Postscript is probably higher \nquality, as well). If you are about to print a document, you'd like to know \nthat there are these other representations available.  \n\nI think the HTTP working group has consensus on the need for Alternates, \nthough I don't think the exact details have been finalized. So besides the \ndocument URL, you need other information: e.g. type, size (so you can figure \nout how long it will take to download), maybe some quality metric. \n\nIt isn't clear that MHTML and HTTP needs will exactly match, but\nyou should be aware of it anyway.\n- Regards,\nJim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se>\n>  Date: Fri, 16 Jan 1998 04:00:24 +0100\n>  To: jg@pa.dec.com (Jim Gettys)\n>  Cc: Nick Shelness <shelness@lotus.com>, jg@pa.dec.com (Jim Gettys),\n>          IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n>          http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n>  Subject: Re: Multiple Content-Location headers\n>  \n>  At 12.57 -0800 98-01-15, Jim Gettys wrote:\n>  > The problem we have is syntax and implementation, not semantics.\n>  > Lets clear this hurdle before we get into the meat of what you are trying\n>  > to achieve, and whether your suggestion fits into the architecture of the\n>  > Web, and my apologies of jumping into the meat in some of my early messages\n>  > on this topic.\n>  >\n>  > Roy Fielding's point is that the syntax change required to allow the header\n>  > name Content-Location to have multiple fields (needed as that is what\n>  >proxies\n>  > typically do if they find multiple headers of the same name), is a problem,\n>  > and one that may (likely) break exisiting implementations.\n>  \n>  But what I suggested what to allow only one field, and one value, with the\n>  name Content-Location in each heading, and to define a new header field\n>  Content-Location-Alternate for cases where more than one is needed.\n>  That would avoid your problem.\n>  \n\nSorry...  That would work.  Please see the note I just sent out that\nincluded a discussion of the need HTTP has for \"Alternates\", which might\nor might not be grist for that mill.\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "RE: chunked enc. (Was: Re: draft minutes, HTTPWG meetings April  7",
            "content": "> non-zero, except for the last chunk.  Should the last chunk always be\n> denoted\n> by \"0\" CRLF, or by 1*HEX CRLF, where the value of 1*HEX value is zero?\n> If\n> the former, are we comfortable with zero-length, non-final chunks?\n> There\n> could be the ambiguous case of chunk-size being, say \"00\".\n> \nI would prefer the second option of \"1*HEX CRLF, where the value of\n1*HEX is zero.\" It seems cleaner than having to treat \"0\" and \"00\"\ndifferently, and I can't think of a case where intermediate zero length\nchunks are useful.\n\nHenry\n\n\n\n"
        },
        {
            "subject": "Re: New content negotiation draft availabl",
            "content": "Mark,\n\nOn at least one of the revisions of the \"Alternates\"draft,\nthe full URL syntax was made mandatory.  I believe that\nthat syntax continues to be the consensus of the conneg list.\nregards,\nTed Hardie\nNASA NIC\n\n> 2. The list of alternates in an \"Alternates:\" header currently uses the\n> URI of documents for identification. It seems that there might be some\n> advantages to using the URL of the document instead, e.g. the example in\n> Section 4.3:\n> \n> >Alternates: {\"paper.1\" 0.9 {type text/html} {language en}},\n> >                 {\"paper.2\" 0.7 {type text/html} {language fr}},\n> >                 {\"paper.3\" 1.0 {type application/postscript}\n> \n> becomes:\n> \n> >Alternates: {\"http://x.org/paper.1\" 0.9 {type text/html} {language en}},\n> >                 {\"http://x.org/paper.2\" 0.7 {type text/html} {language\n> fr}},\n> >                 {\"http://x.org/paper.3\" 1.0 {type\n> application/postscript}\n> \n> Using URLs instead of URIs has the significant advantage that it provides\n> a simple mechanism for pointing clients to mirror locations that replicate\n> the same content. For example, the Internet Movie Database could have an\n> \"Alternates:\" header like:\n> \n> Alternates: {\"http://www.imdb.com/\" 1.0 {type text/html} {language en}},\n>                  {\"http://uk.imdb.com/\" 0.7 {type text/html} {language\n> en}},\n>                  {\"http://italy.imdb.com/\" 0.7 {type text/html} {language\n> it}}\n> \n>\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "On Fri, 16 Jan 1998, Jacob Palme wrote:\n\n> At 12.57 -0800 98-01-15, Jim Gettys wrote:\n> > The problem we have is syntax and implementation, not semantics.\n> > Lets clear this hurdle before we get into the meat of what you are trying\n> > to achieve, and whether your suggestion fits into the architecture of the\n> > Web, and my apologies of jumping into the meat in some of my early messages\n> > on this topic.\n> >\n> > Roy Fielding's point is that the syntax change required to allow the header\n> > name Content-Location to have multiple fields (needed as that is what\n> >proxies\n> > typically do if they find multiple headers of the same name), is a problem,\n> > and one that may (likely) break exisiting implementations.\n> \n> But what I suggested what to allow only one field, and one value, with the\n> name Content-Location in each heading, and to define a new header field\n> Content-Location-Alternate for cases where more than one is needed.\n> That would avoid your problem.\n\nMy confusion is that I just read mail which appeared to come from you\nwhich was complaining about adding header fields and how that would\nsomehow make it difficult to use HTTP to transfer your compound document\nand now you seem to be endorsing such an approach.  \n\nIt would be easier to make progress if you would take one approach and\nstick with it....\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "deleted spa",
            "content": "spam deleted by maintainer\n\n\n\n"
        },
        {
            "subject": "InternetDrafts related to delta encoding in HTT",
            "content": "Some of you are probably aware of the SIGCOMM '97 paper I co-authored\nwith Fred Douglis, Anja Feldmann, and Bala Krishnamurthy of AT&T, on\nthe subject of \"delta encoding.\"  The idea is certainly not original\nwith us, but we seem to have become the first to propose a specific\nset of protocol extensions to HTTP to support delta encoding.  At\nabout the same time, a group of people from a long list of companies\nsubmitted a proposal to W3C for a \"Distribution and Replication\nProtocol\" (DRP) that had a similar feature.\n\nSo we pooled our efforts, and we've now generated an Internet-Draft\ndescribing the proposed extension:\n\n    ftp://ietf.org/internet-drafts/draft-mogul-http-delta-00.txt\n    \n    \"Delta encoding in HTTP\"\n    J Mogul, Y. Goland, Arthur van Hoff, Fred Douglis, Anja Feldmann,\n    Balachander Krishnamurthy\n    01/13/1998. (104930 bytes) \n\n     Many HTTP requests cause the retrieval of slightly modified\n     instances of resources for which the client already has a cache\n     entry. Research has shown that such modifying updates are\n     frequent, and that the modifications are typically much smaller\n     than the actual entity. In such cases, HTTP would make more\n     efficient use of network bandwidth if it could transfer a minimal\n     description of the changes, rather than the entire new instance of\n     the resource. This is called ``delta encoding.'' This document\n     describes how delta encoding can be supported as a compatible\n     extension to HTTP/1.1.\n\nThis is not a work item for the HTTP-WG (at least, not under the\ncurrent charter), but we thought it would be a good idea to encourage\ncomments from the HTTP-WG.  Note that this is a first draft, not\na final specification, so constructive comments are especially\nwelcome.  This is a fairly long draft (sorry, I tend to err on the\nside of including too much) but the actual set of extensions proposed\nfor HTTP is fairly compact.\n\nFor reasons that are explained in draft-mogul-http-delta-00, some\nof us believe that we needed a different kind of integrity mechanism\nthan is currently provided in HTTP/1.1.  A simple mechanism is\nproposed in another Internet-Draft:\n\n    ftp://ietf.org/internet-drafts/draft-mogul-http-digest-00.txt\n\n    \"Instance Digests in HTTP\"\n    J Mogul, Arthur van Hoff\n    01/13/1998. (24829 bytes) \n\n     HTTP/1.1 defines a Content-MD5 header that allows a server to\n     include a digest of the response body. However, this is\n     specifically defined to cover the body of the actual message, not\n     the contents of the full file (which might be quite different, if\n     the response is a Content-Range, or uses a delta encoding). Also,\n     the Content-MD5 is limited to one specific digest algorithm; other\n     algorithms, such as SHA-1, may be more appropriate in some\n     circumstances. Finally, HTTP/1.1 provides no explicit mechanism by\n     which a client may request a digest. This document proposes HTTP\n     extensions that solve these problems.\n\nAgain, constructive comments are welcome.  Please be careful to\ndistinguish between this \"Digest\" proposal and the \"Digest\nAuthentication\" part of HTTP/1.1, which are two entirely different\nthings.\n\nWe expect to generate a few more drafts as a consequence of trying\nto recast DRP as a set of relatively independent extensions to HTTP.\nThis does not mean that the DRP proposal is being abandoned, but it\nis quite likely that it will be possible to simplify DRP by adding\nsome generally-useful extensions to HTTP instead.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Multiple ContentLocation header",
            "content": "rom Jim Gettys's message Fri, 16 Jan 1998 08:55:11 -0800:\n}\n}...............................Please see the note I just sent out that\n}included a discussion of the need HTTP has for \"Alternates\", which might\n}or might not be grist for that mill.\n}                        - Jim\n\n\nHi Jim -- I think the term you are thinking of is \"alternative\" as in\nMutipart-alternative, which I believe serves the very same purpose for\nEMail use of MIME to present the recipient with different\nalternatives.\n\nHowever, in your case, the \"alternates\" are not identical content\nversions with alternative names, but alternative (i.e., different)\ncontent versions with distinctive names, as should be the case for\ndifferent content.\n\nMHTML is trying to specifically ponly deal the case of identical\ncontent with alternative URIs.  \n\nPerhaps the header name \"Content-alias:\" will better convey the\nmeaning and also be a distinctive header that will not be coonfused\nwith other kids of alternatives.\n\nAnyway, I really think we should jointly (MHTML+HTTP) sort these\nissues out before you go to DRAFT, cause after that we can't even make\nminor changes.\n\nI can certainly appreciate your conceren and desire to ush to the\nfinish line for DRAFT status, but it seems to me that the payoff from\nreal INTERWORKABILITY is too great to so easily give it up.\n\nCheers...\\Stef\n\n\n\n"
        },
        {
            "subject": "Some suggested changes to the HTTP draf",
            "content": "The HTTP draft says:\n\n   If a cache receives a successful response whose Content-Location field\n   matches that of an existing cache entry for the same Request-URI, whose\n   entity-tag differs from that of the existing entry, and whose Date is\n   more recent than that of the existing entry, the existing entry SHOULD\n   NOT be returned in response to future requests and should be deleted\n   from the cache.\n\nI suggest this is changed to:\n\n   If a cache receives a successful response where the Content-Location\n   field in the *outermost* HTTP heading matches that of an existing cache\n   entry for the same Request-URI, and whose entity-tag differs from that\n   of the existing entry, and whose Date is more recent than that of the\n   existing entry, the existing entry SHOULD NOT be returned in response\n   to future requests and should be deleted from the cache.\n\nReason: I think this is what you really mean. To use Content-Locations\nin headings inside MIME Multipart objects for cache matching can be\ndangerous.\n\nLater on, the HTTP draft says:\n\n   The Content-Location entity-header field MAY be used to supply the\n   resource location for the entity enclosed in the message when that\n   entity is accessible from a location separate from the requested\n   resource's URI. In the case where a resource has multiple entities\n   associated with it, and those entities actually have separate\n   locations by which they might be individually accessed, the server\n   should provide a Content-Location for the particular variant which\n   is returned. In addition, a server SHOULD provide a\n   Content-Location for the resource corresponding to the response\n   entity.\n\nSuggested change:\n\n   The Content-Location entity-header field, in the outermost HTTP\n   heading of a response, MAY be used to supply the resource location\n   for the entity enclosed in the message when that entity is\n   accessible from a location separate from the requested\n   resource's URI. In the case where a resource has multiple entities\n   associated with it, and those entities actually have separate\n   locations by which they might be individually accessed, the server\n   should provide a Content-Location for the particular variant which\n   is returned. In addition, a server SHOULD provide a\n   Content-Location for the resource corresponding to the response\n   entity.\n\n   The Content-Location header field in a body part inside a returned\n   multipart MIME structure is used as defined in [46].\n\n   [46] Palme, J., Hopmann, A., Shelness, N.: MIME Encapsulation of\n   Aggregate Documents, such as HTML (MHTML), March 1997.\n\nReason: To avoid the risk of unintended discrepancies between the HTTP\nand the MHTML standards.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Some suggested changes to the HTTP draf",
            "content": "On Sat, 17 Jan 1998, Jacob Palme wrote:\n\n> \n> I suggest this is changed to:\n> \n>    If a cache receives a successful response where the Content-Location\n>    field in the *outermost* HTTP heading matches that of an existing cache\n>    entry for the same Request-URI, and whose entity-tag differs from that\n>    of the existing entry, and whose Date is more recent than that of the\n>    existing entry, the existing entry SHOULD NOT be returned in response\n>    to future requests and should be deleted from the cache.\n> \n> Reason: I think this is what you really mean. To use Content-Locations\n> in headings inside MIME Multipart objects for cache matching can be\n> dangerous.\n\nWith a couple of minor well specified exceptions HTTP does not deal\nwith Multipart objects as multipart, but as a single object.  There\nare no outer vs inner headers; only one set of headers.  What you\nmight call inner headers are just part of the data to HTTP. The\ncontent of a MIME Multipart object is no different than the content of\na binary file to HTTP.  An HTTP cache should be no more likely to use\na heading inside a MIME Multipart object than to use a string inside\nan object of type application/octet-stream.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Some suggested changes to the HTTP draf",
            "content": "On Sat, 17 Jan 1998, John Franks wrote:\n> \n> With a couple of minor well specified exceptions HTTP does not deal\n> with Multipart objects as multipart, but as a single object.  There\n> are no outer vs inner headers; only one set of headers.  What you\n> might call inner headers are just part of the data to HTTP. The\n> content of a MIME Multipart object is no different than the content of\n> a binary file to HTTP.  An HTTP cache should be no more likely to use\n> a heading inside a MIME Multipart object than to use a string inside\n> an object of type application/octet-stream.\n> \n\nI was wrong.  The paragraph above may describe current practice but it\nis not what the spec says.  I just reread the relevant specification\nsections, which I should have done before my post.\n\nI don't know of any caches that parse multipart documents and\nseparately cache body parts, but I suppose they might exist.  I am not\nsure why a transport protocol should be mucking about inside an entity\nbeing transported -- even a multipart entity, but apparently it is\npossible.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "> \n> That was my objection ... to the suggestion the DHCP or DNS provide a\n> single URL pointing to a services name / location server. I think\n> it a possibly undesirable design to add yet another server to the\n> 7x24 list. The original post had been that multiple URLs could identify\n> the locations of different server types.\n> \n> Dave Morris\n\nthe original post, ie the draft, was from me, and that wasnt what I said.\nI meant that the DNS record was a pointer to the PAC URL.\n\nIf you meant yaron's original post, then I see your point.\nHowever, it seems that a service location server is inevitable.\nIt may be SVRLOC or something else, but I've come to accept that\nthere willl be some sort of svrloc server..\n\n(yes, it suffers from point of failure problems.. )\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Some comments on Digest Aut",
            "content": "So I have finished my analysis of digest auth and am recommending that IE\n5.0 implement it as a password protection mechanism, nothing more. While I\nlike to think my recommendation carries some weight the reality is that it\ndoes not assure implementation, now or ever.\n\nThe following spec issues were brought up:\n\n1) In section 3.3 of\nftp://ietf.org/internet-drafts/draft-ietf-http-authentication-00.txt, 4th\nparagraph, it says \"The client will follow the redirection, and pass the\nsame Authorization header, including the <opaque> data which the second\nserver may require.\" This sounds like a typo. If the client were to pass the\nsame authentication header it used before the redirection then the\ninformation, such as digest-uri, in the header would be incorrect. This\ncould be used as the basis for a replay attack.\n2) The proxy-authenticate headers seem to all use 401s, shouldn't these be\n407s?\n\n3) Assuming using a new nonce for every request becomes a MUST it will be\nimpossible to use pipelining or concurrent requests. The performance\nimplications are chilling. As such we were thinking of an addition to the\nstandard which would be 100% backwards compatible but would help with\nperformance. The idea is to send a \"next-nonce\" header which includes a list\nof nonces. The client MUST use the nonces in order, this is necessary to\nmake the server's job of tracking nonces tractable. Current clients would\nignore the header and just use the nonce provided in the authentication\nresponse header. I am not privy to the black corridors of security so I\ncan't be sure this wouldn't weaken the algorithm but it seems to make the\nmechanism no weaker than providing the next nonce in the authentication\nresponse header.\n\n4) We wish to apply the same preauthorization rules we now use for basic.\nSo, for example, a URI in the domain list would imply that all that URI's\nchildren are part of the same domain. In the worst case we end up sending in\na bad response and get a new challenge. It doesn't seem to hurt and it\ndefinitely could help.\n\n5) We will not support the digest-required option on a digest-challenge. If\nwe receive such a challenge we will look for another authentication option\nin the www-authenticate. Failing to find one, we will inform the user that\nwe can't authenticate. There are a number of reasons for this decision but\nthe primary one is performance. Because the body authentication information\nmust be placed at the beginning of a message, with the exception of chunked\ntransfers, we would be required to buffer an entire message before sending\nit off. The performance ramifications when compared to the security\nadvantage provided by hashing the body was not seen as a sufficiently good\ntrade off to justify the cost.\n\nI suspect that digest is the best hope for reasonable security in the next\ntwelve to eighteen months. However work should begin now on a new standard,\ndigest-ng. It should provide for message based authentication of arbitrary\nheaders as well both public key and single key encryption. Connection based\nsecurity mechanisms like TLS require smashing through firewalls and proxies.\nA message based mechanism would be significantly preferable for these very\nreasons.\n\nJust some thoughts,\nfrom someone working on a Saturday,\nYaron\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Yaron Goland wrote:\n> 3) Assuming using a new nonce for every request becomes a MUST it will be\n> impossible to use pipelining or concurrent requests. The performance\n> implications are chilling. As such we were thinking of an addition to the\n> standard which would be 100% backwards compatible but would help with\n> performance. The idea is to send a \"next-nonce\" header which includes a list\n> of nonces. The client MUST use the nonces in order, this is necessary to\n> make the server's job of tracking nonces tractable. Current clients would\n> ignore the header and just use the nonce provided in the authentication\n> response header. I am not privy to the black corridors of security so I\n> can't be sure this wouldn't weaken the algorithm but it seems to make the\n> mechanism no weaker than providing the next nonce in the authentication\n> response header.\n\nSeems to me that the problem with this is that you can't enforce \"the\nclient MUST use the nonces in order\", partly because multiple\nsimultaneous connections, for example, could foil the client's best\nefforts to obey, by exhibiting variable delay, and partly because the\nclient may be multithreaded, so synchronizing the sending of the nonces\nwith each other (as opposed to the acquisition of them, which is\ntrivially synchronized) would be onerous.\n\nThe way to fix this is to say \"the client MUST use each nonce at most\nonce\", perhaps? I think this still makes life easy for the server,\nright?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "On Sat, 17 Jan 1998, Ben Laurie wrote:\n\n> Yaron Goland wrote:\n> > 3) Assuming using a new nonce for every request becomes a MUST it will be\n> > impossible to use pipelining or concurrent requests. The performance\n> > implications are chilling. As such we were thinking of an addition to the\n> > standard which would be 100% backwards compatible but would help with\n> > performance. The idea is to send a \"next-nonce\" header which includes a list\n> > of nonces. The client MUST use the nonces in order, this is necessary to\n> > make the server's job of tracking nonces tractable. Current clients would\n> > ignore the header and just use the nonce provided in the authentication\n> > response header. I am not privy to the black corridors of security so I\n> > can't be sure this wouldn't weaken the algorithm but it seems to make the\n> > mechanism no weaker than providing the next nonce in the authentication\n> > response header.\n> \n> Seems to me that the problem with this is that you can't enforce \"the\n> client MUST use the nonces in order\", partly because multiple\n> simultaneous connections, for example, could foil the client's best\n> efforts to obey, by exhibiting variable delay, and partly because the\n> client may be multithreaded, so synchronizing the sending of the nonces\n> with each other (as opposed to the acquisition of them, which is\n> trivially synchronized) would be onerous.\n\nWhat if instead if the NONCE per REQUEST were modified to per ATOMIC\nCONNECTION with the server. All requests within a connection could use\nthe same NONCE or in the case of multiple NONCEs sent on responses\nwithin the connection, the client would be permitted to use the latest\nNONCE it had received which essentially means any NONCE sent on the\nconnection. Either Yaron's original multiple NONCE proposal of this\nvariant would have some risk of an undectect MIM attack with a hijacked\nconnection but I would put my amateur security hat on and suggest that\nif the content is valuable enough to be worth that level of sophistication\nin the attack then SSL or something stronger should have been used\nanyway. \n\nAlso, what if each NONCE' used where derrived from the previous\nNONCE using a known algorithm.  Like the next NONCE was a function\nof the prior NONCE and the shared secret?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Some suggested changes to the HTTP draf",
            "content": "At 08.27 -0600 98-01-17, John Franks wrote:\n> On Sat, 17 Jan 1998, Jacob Palme wrote:\n>\n> >\n> > I suggest this is changed to:\n> >\n> >    If a cache receives a successful response where the Content-Location\n> >    field in the *outermost* HTTP heading matches that of an existing cache\n> >    entry for the same Request-URI, and whose entity-tag differs from that\n> >    of the existing entry, and whose Date is more recent than that of the\n> >    existing entry, the existing entry SHOULD NOT be returned in response\n> >    to future requests and should be deleted from the cache.\n> >\n> > Reason: I think this is what you really mean. To use Content-Locations\n> > in headings inside MIME Multipart objects for cache matching can be\n> > dangerous.\n>\n> With a couple of minor well specified exceptions HTTP does not deal\n> with Multipart objects as multipart, but as a single object.  There\n> are no outer vs inner headers; only one set of headers.  What you\n> might call inner headers are just part of the data to HTTP. The\n> content of a MIME Multipart object is no different than the content of\n> a binary file to HTTP.  An HTTP cache should be no more likely to use\n> a heading inside a MIME Multipart object than to use a string inside\n> an object of type application/octet-stream.\n\nWe both agree that Content-Location should not be used for caching\nexcept when in the outermost header. But since Content-Location\n(the way the MHTML group uses it) can often appear inside multiparts,\nI thought there was a need to specify in the HTTP spec that the HTTP\nuse of Content-Location to control caching only is valid for the\noutermost heading.\n\nOne might also say that it only is valid for HTTP headings. The headings\ninside a multipart are MIME content-headings, not HTTP headings (except\nin the special case of Content-Type: message/http). I am not sure if\nyou make this distinction in the HTTP spec between HTTP headings and\nMIME headings.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "I wish someone would explain to me how having multiple outstanding attacks\ncould lead to degraded security assuming that the multiple nonces do NOT\nhave lifetimes greater than that of the previous single nonce. That is, the\nserver sends you a nonce and starts a hypothetical count down. Once the\ncount down is expired that nonce will not be accepted. My proposal is to\nallow the server, using a separate header, to return multiple nonces.\nHowever I suspect that the server should use the same count down for ALL the\npreviously returned nonces. Use 'em or lose 'em, as it were.\n\nAs for ordering of requests, I'm still not sure how big an issue this is. It\nwould be great if some server side implementers could weigh in on the issue.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Saturday, January 17, 1998 5:23 PM\n> To:Ben Laurie\n> Cc:Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Some comments on Digest Auth\n> \n> \n> \n> On Sat, 17 Jan 1998, Ben Laurie wrote:\n> \n> > Yaron Goland wrote:\n> > > 3) Assuming using a new nonce for every request becomes a MUST it will\n> be\n> > > impossible to use pipelining or concurrent requests. The performance\n> > > implications are chilling. As such we were thinking of an addition to\n> the\n> > > standard which would be 100% backwards compatible but would help with\n> > > performance. The idea is to send a \"next-nonce\" header which includes\n> a list\n> > > of nonces. The client MUST use the nonces in order, this is necessary\n> to\n> > > make the server's job of tracking nonces tractable. Current clients\n> would\n> > > ignore the header and just use the nonce provided in the\n> authentication\n> > > response header. I am not privy to the black corridors of security so\n> I\n> > > can't be sure this wouldn't weaken the algorithm but it seems to make\n> the\n> > > mechanism no weaker than providing the next nonce in the\n> authentication\n> > > response header.\n> > \n> > Seems to me that the problem with this is that you can't enforce \"the\n> > client MUST use the nonces in order\", partly because multiple\n> > simultaneous connections, for example, could foil the client's best\n> > efforts to obey, by exhibiting variable delay, and partly because the\n> > client may be multithreaded, so synchronizing the sending of the nonces\n> > with each other (as opposed to the acquisition of them, which is\n> > trivially synchronized) would be onerous.\n> \n> What if instead if the NONCE per REQUEST were modified to per ATOMIC\n> CONNECTION with the server. All requests within a connection could use\n> the same NONCE or in the case of multiple NONCEs sent on responses\n> within the connection, the client would be permitted to use the latest\n> NONCE it had received which essentially means any NONCE sent on the\n> connection. Either Yaron's original multiple NONCE proposal of this\n> variant would have some risk of an undectect MIM attack with a hijacked\n> connection but I would put my amateur security hat on and suggest that\n> if the content is valuable enough to be worth that level of sophistication\n> in the attack then SSL or something stronger should have been used\n> anyway. \n> \n> Also, what if each NONCE' used where derrived from the previous\n> NONCE using a known algorithm.  Like the next NONCE was a function\n> of the prior NONCE and the shared secret?\n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Yaron Goland wrote:\n> \n> I wish someone would explain to me how having multiple outstanding attacks\n> could lead to degraded security assuming that the multiple nonces do NOT\n> have lifetimes greater than that of the previous single nonce. That is, the\n> server sends you a nonce and starts a hypothetical count down. Once the\n> count down is expired that nonce will not be accepted. My proposal is to\n> allow the server, using a separate header, to return multiple nonces.\n> However I suspect that the server should use the same count down for ALL the\n> previously returned nonces. Use 'em or lose 'em, as it were.\n> As for ordering of requests, I'm still not sure how big an issue this is. It\n> would be great if some server side implementers could weigh in on the issue.\n\nMy point is that you can't require ordering unless you add some other\nrequirements, too (like using the nonces on the same keptalive session).\nAnyway, checking ordering in Apache would be troublesome, and I imagine\nin other servers, too, at least without additional restrictions. In\nfact, Apache won't even be able to prevent the client from using the\nsame nonce multiple times (up until the nonce times out, anyway).\n\nBut most importantly, I can't see what value the ordering requirement\nhas, so it may as well be dropped. \n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Has the ContentLength issue been resolved",
            "content": "On Thu, 15 Jan 1998, John Franks wrote:\n> \n> Has the Content-Length issue be resolved?  I.e. is the Content-Length\n> value the length before or after a Transfer-encoding is supplied?\n\nI don't believe so, and I for one would be glad to get it decided. As I\nsee it there are basically four solutions to this problem:\n\n1) Content-Length is the length after t-e.\n\n2) Add a new header Transfer-Length for t-e's other than chunked and\n   identity.\n\n3) Require all t-e's to be self delimiting.\n\n3a) If a t-e is not self delimiting then either use a Transfer-length,\n    Content-length or chunked t-e on top\n\n4) require the \"outermost\" t-e to be the chunked t-e\n\nI think that 1) is bad and unnecessary. Content-length has never been\nthe length after t-e is applied, as previously the only t-e's were\nidentity and chunked (Content-length was ignored when the chunked t-e\nwas used). Let's leave Content-* as applying to the entity before\nt-e.\n\nI also don't like 3) and 3a) for implementation reasons: in a client\nthe partitioning of the input stream into responses is a very low level\noperation. If self delimiting t-e's are allowed then supporting a new\nt-e would require changes to the core code, making it harder to write\nextensible clients. I'd rather see the number of ways the body can be\ndelimited frozen once and for all (the currently 4 ways are quite\nenough, IMHO). Also, this solution means clients can't just use\nexisting decompression libraries (such zlib) to decode the body;\ninstead they have to implement at least parts of the code themselves.\nAnd lastly, this solution would prevent the existence of clients which\naccept all t-e's, possibly storing the response for later processing by\nsome other software (such as spiders and similar clients).\n\nThis leaves 2) and 4). The disadvantage of 4) is that it might use a few\nbytes more than 2). Furthermore, some people have expressed concern that\n4) would require extra copying of the data on the server side, but I'm\nnot sure how true this is if implemented correctly. On the other hand I\npersonally like 4) because it requires the least changes to client code,\nbut I can live with either solution. Note that if t-e's are going to be\ncomputed on the fly then servers will have to resort to chunking anyway.\n\nJust my 2 Rp.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "Re: New content negotiation draft availabl",
            "content": "Mark Stemm:\n>\n>I had a couple of comments on the content negotiation draft. I am new to\n>this discussion, so I apologize if these comments have been addressed\n>before.\n>\n>1. I am curious why Section 5.4 makes the inclusion of type, language,\n>charset, and length attributes optional. Are there some cases where it\n>would be difficult for a web server/proxy to determine these\n>characteristics at the time it generates the variant list?\n\nYes, there are such times.  First, an document author may simply not\nhave specified the language of a document, or even the charset, so the\nserver is allowed to leave them out.  Second, as you note below,\ncomputing the length can be expensive.\n\n The only\n>examples I can think of are cgi-bin scripts and streaming documents that\n>do not have a fixed size. Additionally, I am also curious why the server\n>does not have to maintain a one-to-one correspondence between the\n>attributes in the variant description and the relevant HTTP Content-*\n>headers. Obviously, making these fields required and identical to those in\n>the HTTP header would make a client's job of choosing the most appropriate\n>variant much easier, and unless there is a reason that this is difficult,\n>it might be useful to make these MUSTs.\n\nIn the transparent negotiation scheme, the client uses the alternates\nheader field only to select content, it never needs to look at the\nheader fields of variants for this, so a MUST would not make the life\nof a client any easier, while it might make the life of a server\n(implementer) somewhat harder.\n\n>\n>2. The list of alternates in an \"Alternates:\" header currently uses the\n>URI of documents for identification. It seems that there might be some\n>advantages to using the URL of the document instead, e.g. the example in\n>Section 4.3:\n\nHmm, I think you are confusing terminology here; an URI is not a\nrelative URL, it is an old term which is supposed to be somewhat more\ngeneral than an URL.  The URI can be both absolute and relative, as the\ndraft says, so your examples below are already allowed.\n\n>\n>>Alternates: {\"paper.1\" 0.9 {type text/html} {language en}},\n>>                 {\"paper.2\" 0.7 {type text/html} {language fr}},\n>>                 {\"paper.3\" 1.0 {type application/postscript}\n>\n>becomes:\n>\n>>Alternates: {\"http://x.org/paper.1\" 0.9 {type text/html} {language en}},\n>>                 {\"http://x.org/paper.2\" 0.7 {type text/html} {language\n>fr}},\n>>                 {\"http://x.org/paper.3\" 1.0 {type\n>application/postscript}\n>\n>Using URLs instead of URIs has the significant advantage that it provides\n>a simple mechanism for pointing clients to mirror locations that replicate\n>the same content. For example, the Internet Movie Database could have an\n>\"Alternates:\" header like:\n>\n>Alternates: {\"http://www.imdb.com/\" 1.0 {type text/html} {language en}},\n>                 {\"http://uk.imdb.com/\" 0.7 {type text/html} {language\n>en}},\n>                 {\"http://italy.imdb.com/\" 0.7 {type text/html} {language\n>it}}\n>\n>I don't know of any existing mechanism to do replication of this nature\n>other than mapping a DNS name to multiple addresses, which doesn't handle\n>replicas on sites that are not a part of the same administrative domain\n>and does not allow fine-grained replica support.\n\nNobody ever did a good mirroring system for the web (and managed to\nget is deployed), and I would not argue that the alternates\nheader is a good mirroring mechanism, but as you note it can be abused\nto do mirroring.\n\n>I'd appreciate your comments, and thanks.\n\nThanks for your comments.  Note, by the way, that nobody is currently\nworking on developing the protocol in draft-ietf-http-negotiation any\nfurther, we are going to take the current version and release it as an\nexperimental protocol.\n\n>        --Mark\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiation05.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: Transparent Content Negotiation in HTTP\nAuthor(s): K. Holtman, A. Mutz\nFilename: draft-ietf-http-negotiation-05.txt\nPages: 43\nDate: 16-Jan-98\n\n        HTTP allows web site authors to put multiple versions of the\n        same information under a single URL.  Transparent content\n        negotiation is an extensible negotiation mechanism, layered on\n        top of HTTP, for automatically selecting the best version when\n        the URL is accessed.  This enables the smooth deployment of\n        new web data formats and markup tags.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-negotiation-05.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiation-05.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-negotiation-05.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: chunked enc. (Was: Re: draft minutes, HTTPWG meetings April 7",
            "content": "On Fri, 11 Apr 1997, Dave Kristol wrote:\n\n> > In the discussion, the following issues seemed to have sufficient\n> > consensus in the meeting that \"last call\" will be sent to the\n> > mailing list for each of them:\n> >    \"chunked encoding\" clarification\n> \n> The clarification was that leading zeroes would be allowed in the hexadecimal\n> byte count.\n> \n> As I thought about this a little, I realized some additional tweaking would\n> be required in the draft, both in the syntax and the description.  In\n> particular, the syntax probably becomes\n> chunk-size= 1*HEX\n> but the description will need a caveat that the value of chunk-size must be\n> non-zero, except for the last chunk.  Should the last chunk always be denoted\n> by \"0\" CRLF, or by 1*HEX CRLF, where the value of 1*HEX value is zero?  If\n> the former, are we comfortable with zero-length, non-final chunks?  There\n> could be the ambiguous case of chunk-size being, say \"00\".\n\nWell, I think the end flag should simply be a length value of ZERO. I\ndon't see any percentage in making the specification more complex so \nwhy not treat a length of 0 or 00 or 000 etc. as the end indicator. \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Yaron Goland wrote:\n> \n> I wish someone would explain to me how having multiple outstanding attacks\n> could lead to degraded security assuming that the multiple nonces do NOT\n> have lifetimes greater than that of the previous single nonce. That is, the\n> server sends you a nonce and starts a hypothetical count down. Once the\n> count down is expired that nonce will not be accepted. My proposal is to\n> allow the server, using a separate header, to return multiple nonces.\n> However I suspect that the server should use the same count down for ALL the\n> previously returned nonces. Use 'em or lose 'em, as it were.\n> \n> As for ordering of requests, I'm still not sure how big an issue this is. It\n> would be great if some server side implementers could weigh in on the issue.\n\nHere's my two grams' worth. :-)\n\nI'm ill-qualified to address the issue of degraded security, but let me\ntry to address the server implementation issues.\n\nA nonce can be self-describing.  That is, the server can choose a form\nfor a nonce that encodes its lifetime.  That's attractive, because it\nmeans the server can avoid having a database of nonces.  The lifetime\ncan be made arbitrarily long or short, as the server's needs require.\n\nIf, instead, the server provides a *list* of nonces in a\nWWW-Authenticate header, then it would be obliged to remember what they\nwere.  Otherwise a rogue client could return a digest for a nonce of its\nown choosing, along with the nonce chosen, disregarding the list.  More\nparticularly, it could replay a previous nonce.  If the nonces from the\nlist are meant to be single-use, then the server must at least remember\nthe set of nonces already used, or not yet used, so it can detect\nresponses for nonces that have been used.\n\nNow consider the proposal where a set of nonces must be received by a\nserver in the order it sent them.  A browser may open multiple\nconnections.  On its first connection it gets a base page, a\nWWW-Authenticate, and a list of nonces.  It responds with an\nAuthorization header and opens multiple new connections to retrieve\nimages for the page.  Which nonce should the browser use on which\nconnection?  How can it be sure that the server will process the\nconnections and accompanying requests in the order that it sends them? \nIf the server's receive-order is different from the browser's\nsend-order, the server must reject the out-of-order requests, even\nthough the nonces are otherwise good.  If \"the server\" is really a farm\nof servers, the connections may be to multiple machines, and the\ninter-server communication may affect the apparent sequence of nonce\nuse.\n\nConclusions:  requiring a client and server to honor a sequence of\nnonces is hard.  Allowing them to pick from a list of nonces, as long as\nthere's no reuse is easier, but still hard.  I don't see any particular\nadded value in sending a list of nonces, all of which have independent\n(or simultaneous) time-out properties, instead of a single such nonce.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "On Mon, 19 Jan 1998, Dave Kristol wrote:\n> \n> A nonce can be self-describing.  That is, the server can choose a form\n> for a nonce that encodes its lifetime.  That's attractive, because it\n> means the server can avoid having a database of nonces.  The lifetime\n> can be made arbitrarily long or short, as the server's needs require.\n> \n\nEmbedding items in a nonce (e.g. by making the nonce a hash of these\nitems) is often useful.  The nonce is then checked when the request\nwith credentials arrives to make sure that it is a valid nonce.  Dave\nmentions using an embedded timestamp which can be used to expire\nnonces.  It is also a good idea to embed the requestor's IP address.\nOne thing that I would like to do, but which would conflict with a\npre-delivered list of nonces, is to embed the (strong) ETag of a\ndocument in the nonce.  This is simpler than timestamping and\nguarantees that a replay can only retrieve exactly the same document\n(which a MITM has presumably already seen when he captured the nonce.)\n\n>\n> Conclusions:  requiring a client and server to honor a sequence of\n> nonces is hard.  Allowing them to pick from a list of nonces, as long as\n> there's no reuse is easier, but still hard.  I don't see any particular\n> added value in sending a list of nonces, all of which have independent\n> (or simultaneous) time-out properties, instead of a single such nonce.\n> \n\nThe point of Yaron's suggestion is to get pipelining to work.  This\nis certainly important if we can figure out how to do it.  A list of\nnonces which must be returned in order is problematic if they\nare not all used in the same pipeline because there is no guarantee\nthat they arrive in the order they were sent (or even to the same\nserver in some cases).  Fortunately, to make pipelining work we are\nessentially talking about one connection.\n\nI would prefer letting the client generate a sequence of nonces\n(based on the first) valid only when pipelined.  They would embed\nthe shared secret.  E.g. \n\n   new_nonce := H( username:password:last_nonce:URI )\n\nThe server gets to create the first nonce and can embed things like IP\naddress and timestamp in it.  Subsequent nonces are guaranteed to be\nmade by someone knowing the secret and have the first nonce embedded.\nThe server only needs to remember the previous nonce in a pipeline.\n\nI don't think this would break anything.  An old client which did not\nunderstand this would simply not pipeline.  A new client understanding\nthis and sending a request to an old server would have the pipelined\nrequests after the first fail and would retry them without pipelining.\n\nWhat do you think?\n\nOne problem is what does \"in the same pipeline\" mean if HTTP were\nto use something other than TCP/IP as undelying transport.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "Yaron Goland <yarong@microsoft.com> wrote:\n\n  > ASSUMPTION: Avoiding replay attacks is important enough to most implementers\n  > that either the standard will require or implementers will voluntarily\n  > refuse to accept the same nonce twice.\n  > \n  > GOAL OF THIS MESSAGE: To demonstrates that the current digest auth\n  > mechanism, from the point of view of performance in situations where we wish\n  > to prevent replay attacks, is unacceptably sub-optimal.\n\nAh, excellent that you set those forth, because I disagree with the\nassumption.\n\nThe purpose of Digest is to replace Basic, with its cleartext\npasswords.  Basic is already subject to replay attacks.  Digest should\nbe no more susceptible, and it isn't more susceptible.  By clever\nchoice of time-limited nonces, it can easily be less so.  But it isn't\nperfect.  We've known that for a long time.\n\nSo let me hark back to the discussion of a few weeks ago.  Let's not\ntry to make Digest do something it was not intended to do.  Let's\nhold replay-proof Digest for digest-ng discussions.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Dave Kristol wrote:\n> \n> Yaron Goland <yarong@microsoft.com> wrote:\n> \n>   > ASSUMPTION: Avoiding replay attacks is important enough to most implementers\n>   > that either the standard will require or implementers will voluntarily\n>   > refuse to accept the same nonce twice.\n>   >\n>   > GOAL OF THIS MESSAGE: To demonstrates that the current digest auth\n>   > mechanism, from the point of view of performance in situations where we wish\n>   > to prevent replay attacks, is unacceptably sub-optimal.\n> \n> Ah, excellent that you set those forth, because I disagree with the\n> assumption.\n> \n> The purpose of Digest is to replace Basic, with its cleartext\n> passwords.  Basic is already subject to replay attacks.  Digest should\n> be no more susceptible, and it isn't more susceptible.  By clever\n> choice of time-limited nonces, it can easily be less so.  But it isn't\n> perfect.  We've known that for a long time.\n\nAgreed. Another way of saying much the same thing is that HTTP is\nstateless by design. Strictly avoiding the reuse of nonces makes it\nstateful (at least, I can't see a way of doing it that doesn't).\nHowever, we can get a very similar effect by time-limiting nonces,\nwithout changing the nature of HTTP.\n\n> So let me hark back to the discussion of a few weeks ago.  Let's not\n> try to make Digest do something it was not intended to do.  Let's\n> hold replay-proof Digest for digest-ng discussions.\n\nExactly. Also remember that for those cases where Digest is\ninsufficient, there is always SSL/TLS.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: New content negotiation draft availabl",
            "content": "I wrote:\n>\n> Note, by the way, that nobody is currently\n>working on developing the protocol in draft-ietf-http-negotiation any\n>further, we are going to take the current version and release it as an\n>experimental protocol.\n\nThis is a bit ambiguous: I meant to say that nobody in this working\ngroup is working on changes to the text of\ndraft-ietf-http-negotiation.  However, some people in the\nimplementation community *are* working on developing implementatations\nof the protocol in this draft.\n\nThanks to Larry Masinter for pointing out this ambiguity.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "Yaron Goland <yarong@microsoft.com> wrote:\n\n  > Oh wait, I thought we were requiring that nonces never be re-used. If not\n  > then that is cool, the next-nonce header should go into a SEPARATE\n  > specification from the draft digest auth proposal. Since it is 100%\n  > compatible with RFC 2069 and the draft digest auth proposal I don't see any\n  > reason to shove it into the main digest auth spec. It can ride on its own.\n\nMy understanding is that the behavior of \"nonce\" is at the origin\nserver's discretion.  It can be one-time, time-limited, eternal,\nwhatever.  It's up to the implementation.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "\"I don't see any particular added value in sending a list of nonces, all of\nwhich have independent (or simultaneous) time-out properties, instead of a\nsingle such nonce.\" - So Spoke Dave Kristol\n\nASSUMPTION: Avoiding replay attacks is important enough to most implementers\nthat either the standard will require or implementers will voluntarily\nrefuse to accept the same nonce twice.\n\nGOAL OF THIS MESSAGE: To demonstrates that the current digest auth\nmechanism, from the point of view of performance in situations where we wish\nto prevent replay attacks, is unacceptably sub-optimal.\n\nLet us say, for the sake of argument, that we were willing to rewrite our\nentire browser implementation to maximize its performance with the current\ndigest auth proposal. In that case, because we know a server will not accept\nthe same nonce twice, we would never pipeline a request that needed auth and\nnever make simultaneous requests that needed auth. Thus using auth\nessentially removes all the performance benefits of using HTTP/1.1.\n\nNow, back in the real world, we are not going to change our entire browser\narchitecture just to make digest happy. When we download an HTML page with\nlinks in it we open two connections and start making both simultaneous and\npipelined requests, regardless of the need for authentication. This means\nthat all the requests will have the same nonce. Assuming the server wishes\nto protect itself from replay attacks it will only accept one of the\nrequests and reject all the others. This means that if N requests are made\none is accepted and the other N-1 are rejected. Assuming the server is\nitself optimal it will give each of the rejected requests a unique nonce and\nthe next N-1 requests will be accepted. Thus ONLY adding somewhere between 1\nto N-1 additional round trips depending upon how you measure round trip\ncosts in dual connection pipelined situations. \n\nI have sent out a proposal which demonstrates how to implement a server such\nthat it will never accept the same nonce twice and doesn't require any\nordering. Hopefully we can use that post as the basis for further\ndiscussion.\n\nYaron\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Monday, January 19, 1998 6:58 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Some comments on Digest Auth\n> \n> Yaron Goland wrote:\n> > \n> > I wish someone would explain to me how having multiple outstanding\n> attacks\n> > could lead to degraded security assuming that the multiple nonces do NOT\n> > have lifetimes greater than that of the previous single nonce. That is,\n> the\n> > server sends you a nonce and starts a hypothetical count down. Once the\n> > count down is expired that nonce will not be accepted. My proposal is to\n> > allow the server, using a separate header, to return multiple nonces.\n> > However I suspect that the server should use the same count down for ALL\n> the\n> > previously returned nonces. Use 'em or lose 'em, as it were.\n> > \n> > As for ordering of requests, I'm still not sure how big an issue this\n> is. It\n> > would be great if some server side implementers could weigh in on the\n> issue.\n> \n> Here's my two grams' worth. :-)\n> \n> I'm ill-qualified to address the issue of degraded security, but let me\n> try to address the server implementation issues.\n> \n> A nonce can be self-describing.  That is, the server can choose a form\n> for a nonce that encodes its lifetime.  That's attractive, because it\n> means the server can avoid having a database of nonces.  The lifetime\n> can be made arbitrarily long or short, as the server's needs require.\n> \n> If, instead, the server provides a *list* of nonces in a\n> WWW-Authenticate header, then it would be obliged to remember what they\n> were.  Otherwise a rogue client could return a digest for a nonce of its\n> own choosing, along with the nonce chosen, disregarding the list.  More\n> particularly, it could replay a previous nonce.  If the nonces from the\n> list are meant to be single-use, then the server must at least remember\n> the set of nonces already used, or not yet used, so it can detect\n> responses for nonces that have been used.\n> \n> Now consider the proposal where a set of nonces must be received by a\n> server in the order it sent them.  A browser may open multiple\n> connections.  On its first connection it gets a base page, a\n> WWW-Authenticate, and a list of nonces.  It responds with an\n> Authorization header and opens multiple new connections to retrieve\n> images for the page.  Which nonce should the browser use on which\n> connection?  How can it be sure that the server will process the\n> connections and accompanying requests in the order that it sends them? \n> If the server's receive-order is different from the browser's\n> send-order, the server must reject the out-of-order requests, even\n> though the nonces are otherwise good.  If \"the server\" is really a farm\n> of servers, the connections may be to multiple machines, and the\n> inter-server communication may affect the apparent sequence of nonce\n> use.\n> \n> Conclusions:  requiring a client and server to honor a sequence of\n> nonces is hard.  Allowing them to pick from a list of nonces, as long as\n> there's no reuse is easier, but still hard.  I don't see any particular\n> added value in sending a list of nonces, all of which have independent\n> (or simultaneous) time-out properties, instead of a single such nonce.\n> \n> Dave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Yaron Goland wrote:\n> ASSUMPTION: Avoiding replay attacks is important enough to most implementers\n> that either the standard will require or implementers will voluntarily\n> refuse to accept the same nonce twice.\n\nAs I mentioned in another message, requiring that nonces are only\naccepted once makes HTTP stateful, and will be difficult to implement in\nsome servers. However, since some servers may want to (at least in some\nmodes) make this requirement, it would seem we need a mechanism to\nsupport it. It seems to me that the list-of-nonces (unencumbered by any\nordering requirements) is a way to achieve this, which, so long as it is\noptional, has no impact on servers and clients that do not wish to\nimplement it.\n\nI should point out that a server that implements it is likely to have an\nawful lot of nonces to track.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Test Day 1998-012",
            "content": "  The next test day will be Thursday, Jan 22.\n\n  Please send registration information as usual.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Pipelinin",
            "content": "On Fri, 11 Apr 1997, Josh Cohen wrote:\n\n> Hey everybody!\n> \n> After the wg meetings I was reviewing the notes on pipelining in the\n> connection mgt draft and rfc 2068.\n> I have a few questions which relate to proxy server behavior..\n> \n> When a client is talking to a proxy server and is pipelining requests,\n> should it use a single pipeline connection to issue requests to \n> different origin servers ?\n> ie should GET http://www.ups.com/ HTTP/1.1\n>    and    GET http://www.fedex.com/ HTTP/1.1 \n> be sent over the same pipeline or should a new connection be established.\n> A common occurence of this is when advertisement gifs come from a \n> different server than the html file.\n> \n> I guess the question becomes :\n> Can requests in the same pipeline be to different origin servers?\n\nYes ... but there are some reasons why it may not be advisable from a\nimplementation design perspective.  Consider the case where request\nbelow to foo.com takes forever to discover the DNS name is invalid. \nThe response from bar.com must wait and the requesting client has no\nclue why.\n\n> \n> With the advent of pipelined persistent connections ( and to a lesser\n> extend 1.0 keep alives ), the distinction of 'who the client is\n> talking to' is confusing to me.  Since while the client may be\n> pipelining to a proxy, and the proxy can go ahead and do an\n> old style connection to an origin server, how does the client deal\n> with old responses?\n> Assuming the answer to the previous question,  is yes...\n> \n> IE: client pipelines:\n>   GET http://www.foo.com/ HTTP/1.1\n>   GET http://www.bar.com/ HTTP/1.1\n> what happens if foo.com is a 1.1 server ( the proxy can do a\n> persistent conenction ) and bar.com is 1.0 (proxy cannot)?\n> \n> Also, when the responses come back to the client, the first is\n> a 1.1 response, and the second is a 1.0 response ..\n\nShould not matter from a functional perspective.  The 1.1 proxy simply\nreturns the responses to the client in the order requested. Since the\nproxy->client response must conform to http/1.1, I would expect it to\nadjust the 1.0 response headers if needed ... can't imagine what the \nneed would be.  And of course the connection management headers are\nhop-hop and as such must be adjusted before the response is forwarded.\n\n\n> \n> How should the client or proxy behave ?\n\nThe proxy can contact both servers (and may already have an open \npersistent connection to the 1.1 server idle after handling another\nclient's request) and the only requirement is that the proxy return the\nresponses in the same order as received.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "Paul Leach wrote:\n  > > [DMK:]\n  > > So let me hark back to the discussion of a few weeks ago.  Let's not\n  > > try to make Digest do something it was not intended to do.  Let's\n  > > hold replay-proof Digest for digest-ng discussions.\n  > > \n  > No.\n  > \n  > A replayable Digest is just as bad as Basic.\n\nLet me say the same thing differently:  A replayable Digest is no worse\nthan Basic.  And it has the merit that it eliminates cleartext passwords.\nThat's all we were trying to do.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Has the ContentLength issue been resolved",
            "content": "On Thu, 15 Jan 1998, John Franks wrote:\n\n>> Has the Content-Length issue be resolved?  I.e. is the Content-Length\n>> value the length before or after a Transfer-encoding is supplied?\n\n>>>>> \"RT\" == Life is hard and then you die <Ronald.Tschalaer@psi.ch> writes:\n\n  [nice summary of alternatives]\n\nRT> 4) require the \"outermost\" t-e to be the chunked t-e\n\nRT> I personally like 4) because it requires the least changes to client code,\nRT> but I can live with either solution. Note that if t-e's are going to be\nRT> computed on the fly then servers will have to resort to chunking anyway.\n\n  Agreed - I thought that Jeffs note proposing this as the easy way\n  out was a simple solution that is almost certainly a\n  backward-compatible solution, and we've heard nothing to the contrary.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Dave Kristol wrote:\n> \n> Paul Leach wrote:\n>   > > [DMK:]\n>   > > So let me hark back to the discussion of a few weeks ago.  Let's not\n>   > > try to make Digest do something it was not intended to do.  Let's\n>   > > hold replay-proof Digest for digest-ng discussions.\n>   > >\n>   > No.\n>   >\n>   > A replayable Digest is just as bad as Basic.\n> \n> Let me say the same thing differently:  A replayable Digest is no worse\n> than Basic.  And it has the merit that it eliminates cleartext passwords.\n> That's all we were trying to do.\n\nA replayable Digest is by no means as bad as Basic:\n\n1. The replay is likely to be time-limited in any sensible\nimplementation, unlike in Basic.\n\n2. The replay is only applicable to a single URL, unlike Basic.\n\n3. The attacker is likely to have already seen the content, in the\nprocess of stealing the material necessary for the replay.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Mandatory draft first revision  please comment",
            "content": "Here's the ID that I just submitted to the IETF under the name\n\ndraft-frystyk-http-mandatory\n\nI have also made it available as\n\"http://www.w3.org/Protocols/HTTP/ietf-http-ext/draft-frystyk-http-mandatory\n-00.txt\" so that we at least can point to it.\n\nHenrik\n\n\n\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n\ntext/plain attachment: draft-frystyk-http-mandatory-00.txt\n\n\n\n\n"
        },
        {
            "subject": "Re: Mandatory draft first revision  please comment",
            "content": "Sorry - WRONG LIST :(\n\nI should have sent it to <ietf-http-ext@w3.org> - sorry for the trouble.\n\nPLEASE DO NOT SEND ANY COMMENTS ABOUT THIS DRAFT TO THIS LIST!\n\nHenrik\n--\nHenrik Frystyk Nielsen,\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "Paul Leach <paulle@microsoft.com> wrote:\n  > > [DMK]\n  > > Let me say the same thing differently:  A replayable Digest is no worse\n  > > than Basic.  And it has the merit that it eliminates cleartext passwords.\n  > > \n  > A distinction without a difference. The fact that they are not plaintext is\n  > irrelevant. The important property about plaintext is that it can be\n  > replayed. If Digest can be replayed, then it has the property of plaintext\n  > that we're trying to get rid of, and so we will have accomplished nothing.\n  > NOTHING!\n\nTo echo Ben Laurie's point:  the replay is good only for the object\nalready fetched (and, presumably, seen by the intruder), because the\nURL is in the digest.  With Basic, the intruder could look at *any*\nprotected object by using the username/password.  Digest is stronger.\nAnd, with suitable time-dependent nonce generation, the potential\ndamage is limited.\n\nMaybe we should be stepping back to identify the kind of application\nwhere we think Digest makes sense.  In my mind, Digest is meant for a\nrelatively simple application where the server wants to limit who can\nlook at some of the content, but that it wouldn't be TEOTWAWKI (the end\nof the world as we know it) if someone actually saw the stuff.  If the\nmaterial were truly important, after all, we would be using SSL or\nother encryption.\n\nAfter all, if someone can sniff passwords, then they can also already\nsee the material flying by.  If they had a username/password under\nBasic, then they could look at stuff at will.  So Digest merely limits\nwhat an intruder can look at anyway, even with a tighter\nspecification.  It doesn't prevent the intruder from looking.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Paul Leach wrote:\n> \n> > From:         Ben Laurie[SMTP:ben@algroup.co.uk]\n> > A replayable Digest is by no means as bad as Basic:\n> >\n> > 1. The replay is likely to be time-limited in any sensible\n> > implementation, unlike in Basic.\n> >\n> > 2. The replay is only applicable to a single URL, unlike Basic.\n> >\n> > 3. The attacker is likely to have already seen the content, in the\n> > process of stealing the material necessary for the replay.\n> >\n> If you can do the above, then you've got _some_ replay prevention.\n\nYou can.\n\n> Dave is arguing that no replay protection is necessary. I'm willing to\n> discuss how much is needed, but I'm tired of statements about \"eliminating\n> plaintext is all we have to do\". (I'll remind everyone that even Basic\n> _doen't_ use plaintext -- it uses a Base64 encoding.)\n\nAgreed.\n\n> I also do not believe that we can rely on \"any sensible implementation\".\n> When it comes to security, we need to require sensible impllementations,\n> because it is well proven that even well intentioned implentors frequently\n> fail to acheive \"sensible implementations\".\n\nI'll limit the obvious snipe to this sentence :-)\n\n> That means we need to precisely describe the algorithms for at least one\n> sensible implementaiton.\n\nFair enough, but I don't think we can go so far as to mandate the\nalgorithm, because...\n\n> Finally, I believe that if we can solve the pipelining problem, then we can\n> solve the replay problem.\n\n...this, I believe, can only be solved by requiring servers to keep\nstate, which is a Bad Thing. I have no objection to those servers that\nwant to (and can) doing this, but I really don't see the point - if you\nare _that_ concerned about the content, you should've SSLed it (because\nthe Bad Guy who is failing to get usephul stuph for a replay attack is\ngetting the content anyway).\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n> \n> > From: John Franks[SMTP:john@math.nwu.edu]\n> \n> > It is also a good idea to embed the requestor's IP address.\n> > \n\n> This will be broken when there is a proxy farm, each with its own IP\n> address, and where the client chooses the particular proxy based on the\n> URL.\n> \n\nIf the client chooses the proxy based on URL it will work because the\nURL requested without credentials (which elicits the nonce) will be\nthe same as the URL requested with credentials.  If the first request\nwithout credentials and the second with credentials are from different\nproxies, then you are right it will break.\n\n\n> > One thing that I would like to do, but which would conflict with a\n> > pre-delivered list of nonces, is to embed the (strong) ETag of a\n> > document in the nonce.  This is simpler than timestamping and\n> > guarantees that a replay can only retrieve exactly the same document\n> > (which a MITM has presumably already seen when he captured the nonce.)\n> > \n> Both would be good -- otherwise you can retreive the same document\n> indefinitely into the future.\n> \n\nYou could only receive *exactly* the same document indefinitely into\nthe future as any update of the document changes the ETag.  I don't\nsee repeatedly obtaining exactly the same document as a problem with\nidempotent methods like GET.  Of course, PUT and POST are a different\nmatter, but I don't think they have ETags.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Digest and pipelinin",
            "content": "If we can rely on requests being delivered in order, then I have a simple\nalgorithm to do replay prevention with pipelining.\n\n1. The server sends a nonce which is a random number cocatenated with a\ntimestamp. It sends an \"opaque\" value (part of) which is a \"client ID\".\n1. The client uses a nonce which is the concatentation of the nonce the\nserver sent it and a 3 digit count (mod 1000) of the number of requests it\nhas sent to the server, plus a client chosen random string (this last\nprevents dictionary attacks). It sends back the 3 digits plus the client\nchosen random string.\n2. The server uses (part of) the opaque field to identify the client, and\nconcatenates the nonce it keeps associated with the opaque field with the\nnonce sent by the client to get the complete nonce that it uses to check the\nclient's request. It keeps a counter of the requests associated with the\nclient ID; this count and that received from the client must match; after\nmatching it is incremented by one. The timestamp is checked to make sure it\nisn't too old. When it expires, the association between the client ID and\nthe nonce and count can be discarded.\n\nCan we depend on the requests being in order?\n\nIf we can't, then the server would need to keep a (say) 64 bit mask plus a\nbase count, and check that the count in the request hasn't been seen before;\nwhen the count gets to be 0 mod 32, then the base count gets bumped by 32\nand the mask gets shifted right by 32 bits. This would handle requests as\nlong as they weren't too out of order.\nA little harder, but still not really tough. I'd be happy to code it for\ninclusion.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Digest and pipelinin",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n> If we can rely on requests being delivered in order, then I have a simple\n> algorithm to do replay prevention with pipelining.\n> \n> 1. The server sends a nonce which is a random number cocatenated with a\n> timestamp. It sends an \"opaque\" value (part of) which is a \"client ID\".\n> 1. The client uses a nonce which is the concatentation of the nonce the\n> server sent it and a 3 digit count (mod 1000) of the number of requests it\n> has sent to the server, plus a client chosen random string (this last\n> prevents dictionary attacks). It sends back the 3 digits plus the client\n> chosen random string.\n> 2. The server uses (part of) the opaque field to identify the client, and\n> concatenates the nonce it keeps associated with the opaque field with the\n> nonce sent by the client to get the complete nonce that it uses to check the\n> client's request. It keeps a counter of the requests associated with the\n> client ID; this count and that received from the client must match; after\n> matching it is incremented by one. The timestamp is checked to make sure it\n> isn't too old. When it expires, the association between the client ID and\n> the nonce and count can be discarded.\n> \n> Can we depend on the requests being in order?\n\n\nOrder can only be depended upon if the client ONLY uses a single\nconnection at a time to connect with a server. Irrespective of whether\nthat connection is persistent or uses pipelining.\n\n\n> \n> If we can't, then the server would need to keep a (say) 64 bit mask plus a\n> base count, and check that the count in the request hasn't been seen before;\n> when the count gets to be 0 mod 32, then the base count gets bumped by 32\n> and the mask gets shifted right by 32 bits. This would handle requests as\n> long as they weren't too out of order.\n\nThis algorithm will break as soon as request#32 arrives while #29 \nis pending. The algorithm I think would work if the increment rule is\nthat slots 0-31 have been used, then increment the base and shift...\n\nBut given a web page as complex as some from a representative site\nsuch as Microsoft where I've counted more than 30 requests as a\nresult of a single user URL click, and given 4 persistent connections,\nI'd have a hard time accepting a very small limit like 32 on the\norder of randomness. Perhaps 128 (with 256 bits in the mask)  would work\nfor the near term but even that seems like a poor protical choice.\n\nInstead limit the scope of counter derrived nonce usage to insure\nthat requests are seen in order:\n\nRequire re-authentication by the client for each parallel connection. Each\npersistent connection would thus get a unique client ID and hence nonce\nsequence.\n \nIn the case of multiple connections for single requests,\nthe client may use a counter nonce IFF all requests have completed and\nthe counter-nonce is derrived from the parallel connection with the\nmost recent Date: header. Or because of the complexity of this approach,\ndon't support sequencial nonces outside of a single persistent connection.\n\nBUT ... I'm not sure this is an improvement on the suggestion from\nsomeone (JohnF or BenL?) that each sequencial nonce be computed from\nthe previous nonce by rehashing the prior nonce and shared secret, perhaps\nwith a counter.\n\nOf course, this all becomes a delightful challenge when a proxy gets in\nthe middle and doesn't maintain a one-one relationship between client and\nserver connections. This would raise havoc with order expectations as\nwell.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n> > \n> Actually, my comment (that both Etag and timestamp are good) was wrong. You\n> can't use an Etag in the nonce, because nonces aren't per-resource. \n\nThey certainly can be.  This is purely an implementation decision.\nSome existing implementations work this way.  Nothing in the spec\nprohibits this and I doubt if that will change.\n\nIncidentally, whether an implementation is stateful (e.g. remembers all\nnonces used) or stateless is also an implementation decision.  I very\nmuch doubt that any consensus could be reached on a specification change\nwhich either requires the server to be stateful or prohibits it from \nbeing so.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: dmk@research.bell-labs.com[SMTP:dmk@research.bell-labs.com]\n> Sent: Tuesday, January 20, 1998 12:59 PM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Some comments on Digest Auth\n> \n> Paul Leach wrote:\n>   > > [DMK:]\n>   > > So let me hark back to the discussion of a few weeks ago.  Let's not\n>   > > try to make Digest do something it was not intended to do.  Let's\n>   > > hold replay-proof Digest for digest-ng discussions.\n>   > > \n>   > No.\n>   > \n>   > A replayable Digest is just as bad as Basic.\n> \n> Let me say the same thing differently:  A replayable Digest is no worse\n> than Basic.  And it has the merit that it eliminates cleartext passwords.\n> \nA distinction without a difference. The fact that they are not plaintext is\nirrelevant. The important property about plaintext is that it can be\nreplayed. If Digest can be replayed, then it has the property of plaintext\nthat we're trying to get rid of, and so we will have accomplished nothing.\nNOTHING!\n\nPaul\n\n\n\n"
        },
        {
            "subject": "HTTP practice and Year2000: the bad(?) new",
            "content": "At one of the HTTP-WG sessions at the IETF meeting in Memphis, we had a\nbrief discussion of Year-2000 issues (although I don't think this\nappeared in the minutes, so I can't remember who brought it up).\n\nAlthough HTTP/1.1 requires the use of \"rfc1123-date\", which has\na four-digit year field (section 3.3.1):\n\n   HTTP/1.1 clients and servers [...] MUST\n   only generate the RFC 1123 format for representing HTTP-date values\n   in header fields.\n\nolder implementations used RFC 850 dates, which have a 2-digit\nyear field (e.g., \"Sunday, 06-Nov-94 08:49:37 GMT\").\n\nThe question that came up was \"how prevalent is the use of the\n2-digit year field?\".  I.e., what is the likelihood that users\nwill be faced with buggy results after the end of 1999?\n\nIn December, I made a trace of the contents of the HTTP requests\nand responses flowing through our proxy servers.  This trace covered\nabout 2 days, and about 500K requests (from several thousand different\nusers).  Since I have full header information, I realized that I could\nfind out roughly how prevalent the use of 2-digit year fields is.\n\nAs a crude test, I looked at one subset of this trace (about 2% of\nthe total responses), and pulled out all of the Date, Last-Modified,\nand Expires headers seen there.  This resulted in 6282 separate values.\nI then used \"grep\" to find the values that had a 2-digit year field\nbetween 1993 and 1997; about 1247 values used this obsolete format.\nIn other words, around 20% of the values are not \"Year-2000 ready\".\n\nIt would take somewhat more effort for me to answer other questions,\nsuch as\n\nhow many different sites on the Internet are not Y2K-ready?\n\nwhich server implementations are not Y2K-ready?\n\nbut I would rather not get into a public naming of names (i.e., listing\nthe non-compliant server implementations).\n\nAnyway, this may not be a true disaster.  Any new HTTP client\nought to recognize that if the current year is 2002, and it\nreceives an \"Expires: Sunday, 06-Nov-94 08:49:37 GMT\", then\nthat \"94\" means \"1994\".  My guess is that most of the client\nand proxy population will be updated during the next 3 years,\nexcept perhaps for some embedded systems.  Unfortunately, it's\nhard to tell from these traces whether any naive client\nimplementations are lurking out there.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "I think the proposal to allow clients to generate nonces is intriguing but\nit does concern me from a long term security point of view. Experience has\ntaught that allowing clients to generate what is essentially server side\ninformation leads to trouble. For example in DAV we learned a long time ago\nto not let clients generate URIs, it tends to break things. In this case\nallowing the client to generate nonces remove's flexibility on the server's\npart in how it generates and manages nonces. Furthermore I'm concerned with\nbehavior through proxies where a proxy may have multiple connections to a\nserver and may put a client's request into any one of its connections to\nthat server.\n\nI guess i'm just old fashioned but I always like to err on the side of\nmaximum flexibility. In this case that means only giving the server the\nright to generate nonces.\n\nYaron\n\n> -----Original Message-----\n> From:John Franks [SMTP:john@math.nwu.edu]\n> Sent:Monday, January 19, 1998 10:42 AM\n> To:Dave Kristol\n> Cc:Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Some comments on Digest Auth\n> \n> On Mon, 19 Jan 1998, Dave Kristol wrote:\n> > \n> > A nonce can be self-describing.  That is, the server can choose a form\n> > for a nonce that encodes its lifetime.  That's attractive, because it\n> > means the server can avoid having a database of nonces.  The lifetime\n> > can be made arbitrarily long or short, as the server's needs require.\n> > \n> \n> Embedding items in a nonce (e.g. by making the nonce a hash of these\n> items) is often useful.  The nonce is then checked when the request\n> with credentials arrives to make sure that it is a valid nonce.  Dave\n> mentions using an embedded timestamp which can be used to expire\n> nonces.  It is also a good idea to embed the requestor's IP address.\n> One thing that I would like to do, but which would conflict with a\n> pre-delivered list of nonces, is to embed the (strong) ETag of a\n> document in the nonce.  This is simpler than timestamping and\n> guarantees that a replay can only retrieve exactly the same document\n> (which a MITM has presumably already seen when he captured the nonce.)\n> \n> >\n> > Conclusions:  requiring a client and server to honor a sequence of\n> > nonces is hard.  Allowing them to pick from a list of nonces, as long as\n> > there's no reuse is easier, but still hard.  I don't see any particular\n> > added value in sending a list of nonces, all of which have independent\n> > (or simultaneous) time-out properties, instead of a single such nonce.\n> > \n> \n> The point of Yaron's suggestion is to get pipelining to work.  This\n> is certainly important if we can figure out how to do it.  A list of\n> nonces which must be returned in order is problematic if they\n> are not all used in the same pipeline because there is no guarantee\n> that they arrive in the order they were sent (or even to the same\n> server in some cases).  Fortunately, to make pipelining work we are\n> essentially talking about one connection.\n> \n> I would prefer letting the client generate a sequence of nonces\n> (based on the first) valid only when pipelined.  They would embed\n> the shared secret.  E.g. \n> \n>    new_nonce := H( username:password:last_nonce:URI )\n> \n> The server gets to create the first nonce and can embed things like IP\n> address and timestamp in it.  Subsequent nonces are guaranteed to be\n> made by someone knowing the secret and have the first nonce embedded.\n> The server only needs to remember the previous nonce in a pipeline.\n> \n> I don't think this would break anything.  An old client which did not\n> understand this would simply not pipeline.  A new client understanding\n> this and sending a request to an old server would have the pipelined\n> requests after the first fail and would retry them without pipelining.\n> \n> What do you think?\n> \n> One problem is what does \"in the same pipeline\" mean if HTTP were\n> to use something other than TCP/IP as undelying transport.\n> \n> John Franks\n> john@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: dmk@research.bell-labs.com[SMTP:dmk@research.bell-labs.com]\n> Sent: Monday, January 19, 1998 11:13 AM\n> To: Yaron Goland\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Some comments on Digest Auth\n> \n> Yaron Goland <yarong@microsoft.com> wrote:\n> \n>   > ASSUMPTION: Avoiding replay attacks is important enough to most\n> implementers\n>   > that either the standard will require or implementers will voluntarily\n>   > refuse to accept the same nonce twice.\n>   > \n>   > GOAL OF THIS MESSAGE: To demonstrates that the current digest auth\n>   > mechanism, from the point of view of performance in situations where\n> we wish\n>   > to prevent replay attacks, is unacceptably sub-optimal.\n> \n> Ah, excellent that you set those forth, because I disagree with the\n> assumption.\n> \n> The purpose of Digest is to replace Basic, with its cleartext\n> passwords.  Basic is already subject to replay attacks.  Digest should\n> be no more susceptible, and it isn't more susceptible.  By clever\n> choice of time-limited nonces, it can easily be less so.  But it isn't\n> perfect.  We've known that for a long time.\n> \n> So let me hark back to the discussion of a few weeks ago.  Let's not\n> try to make Digest do something it was not intended to do.  Let's\n> hold replay-proof Digest for digest-ng discussions.\n> \nNo.\n\nA replayable Digest is just as bad as Basic.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: Ben Laurie[SMTP:ben@algroup.co.uk]\n> Sent: Tuesday, January 20, 1998 3:09 PM\n> To: Paul Leach\n> Cc: Dave Kristol; http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Some comments on Digest Auth\n> \n> Paul Leach wrote:\n> > \n> > > From:         Ben Laurie[SMTP:ben@algroup.co.uk]\n> > > A replayable Digest is by no means as bad as Basic:\n> > >\n> > > 1. The replay is likely to be time-limited in any sensible\n> > > implementation, unlike in Basic.\n> > >\n> > > 2. The replay is only applicable to a single URL, unlike Basic.\n> > >\n> > > 3. The attacker is likely to have already seen the content, in the\n> > > process of stealing the material necessary for the replay.\n> > >\n> > If you can do the above, then you've got _some_ replay prevention.\n> \n> You can.\n> \n> > Dave is arguing that no replay protection is necessary. I'm willing to\n> > discuss how much is needed, but I'm tired of statements about\n> \"eliminating\n> > plaintext is all we have to do\". (I'll remind everyone that even Basic\n> > _doen't_ use plaintext -- it uses a Base64 encoding.)\n> \n> Agreed.\n> \n> > I also do not believe that we can rely on \"any sensible implementation\".\n> > When it comes to security, we need to require sensible impllementations,\n> > because it is well proven that even well intentioned implentors\n> frequently\n> > fail to acheive \"sensible implementations\".\n> \n> I'll limit the obvious snipe to this sentence :-)\n> \nNone of us has ever used a weak random number generator to create keys, I'm\nsure.\n\n> > That means we need to precisely describe the algorithms for at least one\n> > sensible implementaiton.\n> \n> Fair enough, but I don't think we can go so far as to mandate the\n> algorithm, because...\n> \n> > Finally, I believe that if we can solve the pipelining problem, then we\n> can\n> > solve the replay problem.\n> \n> ...this, I believe, can only be solved by requiring servers to keep\n> state, which is a Bad Thing. I have no objection to those servers that\n> want to (and can) doing this, but I really don't see the point - if you\n> are _that_ concerned about the content, you should've SSLed it (because\n> the Bad Guy who is failing to get usephul stuph for a replay attack is\n> getting the content anyway).\n> \nIf the servers keep no state, and just accepts the nonce that the client\nquotes back at it, then you get no replay protection at all. The bad guy can\nget an updated version of any page they have previously overheard, any time\nthey want. If the application is (e.g.) one where the user pays to get\nperiodically update information which has a constant URL, then this is a\nserious problem.\n\nThat isn't what is being proposed, I realize. If the nonce the server gives\nout contains a time stamp, it can refuse to accept old nonces. But I think\nwe should specify that it MUST contain a timestamp, if that's all the replay\nprotection we're going to have. And we could specify that the client include\na timestamp in the nonce...\n\nEureka!\n\nLet the server send a nonce which contains a timestamp S. The client checks\nto see that it isn't too old.\n\nLet the client return a nonce which is the server's nonce S concatenated\nwith a timestamp C (plus some more random stuff to prevent dictionary\nattacks). The server checks that S and C are both not too old. S can be used\nfor (say) 10 minutes; C has to be less than (say) 30 seconds old. (C is\ninitially set to S, and subsequent C are equal to S plus the amount of time\nsince S was received. This pretty much synchs the two clocks, so the\nout-of-dateness of C is bounded by transit time through the\nnetwork/proxies...)\n\nWhen the 60 minutes expires, the server has to respond with a 401 to refresh\nS. Or, we could allow WWW-Authenticate to be sent with 200 OK; it would\nupdate all the fields that were present (so could refresh the nonce, opaque,\nrealm, etc.)\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Tuesday, January 20, 1998 6:23 PM\n> To: Paul Leach\n> Cc: Dave Kristol; Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Some comments on Digest Auth\n> \n> On Tue, 20 Jan 1998, Paul Leach wrote:\n> \n> > > \n> > Actually, my comment (that both Etag and timestamp are good) was wrong.\n> You\n> > can't use an Etag in the nonce, because nonces aren't per-resource. \n> \n> They certainly can be.  This is purely an implementation decision.\n> \nOK, it is, but not a practical one. It would require that every initial\nrequest for a URL return 401. That will essentially double the number of\nround trips.\n\n> Some existing implementations work this way.  Nothing in the spec\n> prohibits this and I doubt if that will change.\n> \n> Incidentally, whether an implementation is stateful (e.g. remembers all\n> nonces used) or stateless is also an implementation decision.  I very\n> much doubt that any consensus could be reached on a specification change\n> which either requires the server to be stateful or prohibits it from \n> being so.\n> \nAs long as the stateless one can actually be made more than trivially more\nsecure than Basic. I think we might be well on the way, but let's not forget\nthe priorities.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Monday, January 19, 1998 10:41 AM\n> To: Dave Kristol\n> Cc: Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Some comments on Digest Auth\n> \n<snip>\n\n> It is also a good idea to embed the requestor's IP address.\n> \nThis will be broken when there is a proxy farm, each with its own IP\naddress, and where the client uses chooses the particular proxy based on the\nURL.\n\n> One thing that I would like to do, but which would conflict with a\n> pre-delivered list of nonces, is to embed the (strong) ETag of a\n> document in the nonce.  This is simpler than timestamping and\n> guarantees that a replay can only retrieve exactly the same document\n> (which a MITM has presumably already seen when he captured the nonce.)\n> \nBoth would be good -- otherwise you can retreive the same document\nindefinitely into the future.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Mandatory draft first revision  please comment",
            "content": "Why does this spec insist on being more than one paragraph long?\n\nAll we need is a \"Mandatory\" header followed by the names of header fields\nthat MUST be understood. That is it. No fancy namespaces, no extensions,\nNOTHING.\n\nNow if you want to add those fancy features, fine, define a \"fancy-features\"\nheader and PUT ITS NAME IN MANDATORY.\n\nWhile we are at it, what happened to the section on changing method names to\n\"PEP-*\"? We need this if we are ever going to deploy.\n\nAll this having been said I want to congratulate Henrik et all on really\nparing down the spec. It is much better than before. With just a little more\ncutting we will finally have something we can deploy.\n\nYaron\n\n> -----Original Message-----\n> From:Henrik Frystyk Nielsen [SMTP:frystyk@w3.org]\n> Sent:Tuesday, January 20, 1998 1:43 PM\n> To:ietf-http-ext@w3.org\n> Subject:Mandatory draft first revision - please comment!\n> \n> \n> Here's the ID that I just submitted to the IETF under the name\n> \n> draft-frystyk-http-mandatory\n> \n> I have also made it available as\n> \"http://www.w3.org/Protocols/HTTP/ietf-http-ext/draft-frystyk-http-mandato\n> ry\n> -00.txt\" so that we at least can point to it.\n> \n> Henrik\n> \n> \n> --\n> Henrik Frystyk Nielsen,\n> World Wide Web Consortium\n> http://www.w3.org/People/Frystyk << File:\n> draft-frystyk-http-mandatory-00.txt >> \n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: Ben Laurie[SMTP:ben@algroup.co.uk]\n> Sent: Tuesday, January 20, 1998 1:33 PM\n> To: Dave Kristol\n> Cc: Paul Leach; http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Some comments on Digest Auth\n> \n> Dave Kristol wrote:\n> > \n> > Paul Leach wrote:\n> >   > > [DMK:]\n> >   > > So let me hark back to the discussion of a few weeks ago.  Let's\n> not\n> >   > > try to make Digest do something it was not intended to do.  Let's\n> >   > > hold replay-proof Digest for digest-ng discussions.\n> >   > >\n> >   > No.\n> >   >\n> >   > A replayable Digest is just as bad as Basic.\n> > \n> > Let me say the same thing differently:  A replayable Digest is no worse\n> > than Basic.  And it has the merit that it eliminates cleartext\n> passwords.\n> > That's all we were trying to do.\n> \n> A replayable Digest is by no means as bad as Basic:\n> \n> 1. The replay is likely to be time-limited in any sensible\n> implementation, unlike in Basic.\n> \n> 2. The replay is only applicable to a single URL, unlike Basic.\n> \n> 3. The attacker is likely to have already seen the content, in the\n> process of stealing the material necessary for the replay.\n> \nIf you can do the above, then you've got _some_ replay prevention.\nDave is arguing that no replay protection is necessary. I'm willing to\ndiscuss how much is needed, but I'm tired of statements about \"eliminating\nplaintext is all we have to do\". (I'll remind everyone that even Basic\n_doen't_ use plaintext -- it uses a Base64 encoding.)\n\nI also do not believe that we can rely on \"any sensible implementation\".\nWhen it comes to security, we need to require sensible impllementations,\nbecause it is well proven that even well intentioned implentors frequently\nfail to acheive \"sensible implementations\".\n\nThat means we need to precisely describe the algorithms for at least one\nsensible implementaiton.\n\nFinally, I believe that if we can solve the pipelining problem, then we can\nsolve the replay problem.\n\nPaul\n\nPaul \n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "Oh wait, I thought we were requiring that nonces never be re-used. If not\nthen that is cool, the next-nonce header should go into a SEPARATE\nspecification from the draft digest auth proposal. Since it is 100%\ncompatible with RFC 2069 and the draft digest auth proposal I don't see any\nreason to shove it into the main digest auth spec. It can ride on its own.\nYaron\n\n> -----Original Message-----\n> From:dmk@research.bell-labs.com [SMTP:dmk@research.bell-labs.com]\n> Sent:Monday, January 19, 1998 11:13 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:RE: Some comments on Digest Auth\n> \n> Yaron Goland <yarong@microsoft.com> wrote:\n> \n>   > ASSUMPTION: Avoiding replay attacks is important enough to most\n> implementers\n>   > that either the standard will require or implementers will voluntarily\n>   > refuse to accept the same nonce twice.\n>   > \n>   > GOAL OF THIS MESSAGE: To demonstrates that the current digest auth\n>   > mechanism, from the point of view of performance in situations where\n> we wish\n>   > to prevent replay attacks, is unacceptably sub-optimal.\n> \n> Ah, excellent that you set those forth, because I disagree with the\n> assumption.\n> \n> The purpose of Digest is to replace Basic, with its cleartext\n> passwords.  Basic is already subject to replay attacks.  Digest should\n> be no more susceptible, and it isn't more susceptible.  By clever\n> choice of time-limited nonces, it can easily be less so.  But it isn't\n> perfect.  We've known that for a long time.\n> \n> So let me hark back to the discussion of a few weeks ago.  Let's not\n> try to make Digest do something it was not intended to do.  Let's\n> hold replay-proof Digest for digest-ng discussions.\n> \n> Dave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: ID:  Proxy autoconfi",
            "content": "On Thu, 10 Apr 1997, Josh wrote:\n\n> Well, most often, the resource is being retreived from an HTTP server\n> (in our case) or a proxy.\n> \n> If the proxy is down, the pac file is useless anyway.\n\nThat was my objection ... to the suggestion the DHCP or DNS provide a\nsingle URL pointing to a services name / location server. I think\nit a possibly undesirable design to add yet another server to the\n7x24 list. The original post had been that multiple URLs could identify\nthe locations of different server types.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: John Franks[SMTP:john@math.nwu.edu]\n> Sent: Tuesday, January 20, 1998 3:25 PM\n> To: Paul Leach\n> Cc: Dave Kristol; Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Some comments on Digest Auth\n> \n> On Tue, 20 Jan 1998, Paul Leach wrote:\n> \n> > \n> > > From: John Franks[SMTP:john@math.nwu.edu]\n> > \n> > > It is also a good idea to embed the requestor's IP address.\n> > > \n> \n> > This will be broken when there is a proxy farm, each with its own IP\n> > address, and where the client chooses the particular proxy based on the\n> > URL.\n> > \n> \n> If the client chooses the proxy based on URL it will work because the\n> URL requested without credentials (which elicits the nonce) will be\n> the same as the URL requested with credentials.  If the first request\n> without credentials and the second with credentials are from different\n> proxies, then you are right it will break.\n> \nAnd the third and subsequent, with credentials, through a URL-dependent\nproxy, will also break.\n\n> > > One thing that I would like to do, but which would conflict with a\n> > > pre-delivered list of nonces, is to embed the (strong) ETag of a\n> > > document in the nonce.  This is simpler than timestamping and\n> > > guarantees that a replay can only retrieve exactly the same document\n> > > (which a MITM has presumably already seen when he captured the nonce.)\n> > > \n> > Both would be good -- otherwise you can retreive the same document\n> > indefinitely into the future.\n> > \n> \n> You could only receive *exactly* the same document indefinitely into\n> the future as any update of the document changes the ETag.  I don't\n> see repeatedly obtaining exactly the same document as a problem with\n> idempotent methods like GET.  Of course, PUT and POST are a different\n> matter, but I don't think they have ETags.\n> \nActually, my comment (that both Etag and timestamp are good) was wrong. You\ncan't use an Etag in the nonce, because nonces aren't per-resource. I do a\nGET on http://www.foo.com/bar.html, and get a nonce. I use the same nonce\nwhen I do a GET on http://www.foo.com/waz.html.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "[re: embedding client IP addr in the nonce]\n\n>If the client chooses the proxy based on URL it will work because the\n>URL requested without credentials (which elicits the nonce) will be\n>the same as the URL requested with credentials.  If the first request\n>without credentials and the second with credentials are from different\n>proxies, then you are right it will break.\n\nthis will be more common in the field as protocols such as\nhttp://ircache.nlanr.net/Cache/ICP/draft-vinod-carp-v1-02.txt become more\npopular....\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n\n> If Digest can be replayed, then it has the property of plaintext\n> that we're trying to get rid of, and so we will have accomplished nothing.\n> NOTHING!\n> \n\nThis is not right.  No one is advocating this, but even if each server\nused only *one* publicly available nonce for all transactions\n(different nonces for different servers) we would have accomplished quite\na lot.\n\nReplay attacks on different servers or with different types of service\nwould not be possible.  Think of a user who chooses the same password\nfor his SSL/password protected bank account services and for\nregistration to a Web chat site.  With Basic on the chat site a MITM\ngets access to this guy's bank account with digest he doesn't.\n\nIn fact a replay attack would only be good for updated versions of\na document the MITM had already seen.  This is an important base\nand we need to keep it in mind as something already accomplished.\nBut we can certainly do better.\n\nRight now we are wrestling with some competing requirements.\nLet's list our goals:\n\n1) Minimize or eliminate replay attacks on updated documents.\n\n2) Preserve the benefits of pipelining.\n\n3) Preserve the statelessness of servers (but state in the nonce is ok).\n\nThe order of importance for these goals might vary for different \napplications.  That is why I would like to leave a lot to the \ndiscretion of the implementor.  As Yaron says we should maximize\nthe server's flexibility.\n\nIt seems to me that a server which embeds a short lived timestamp and\na server secret (but nothing else) in the nonce does pretty well on these\ngoals.  It is not perfect, but it satisfies 2), and 3).  And it leaves\nonly a small window for replay attacks on updates.\n\nNote, however, that we can't require a timestamp in the nonce because\nsome servers don't have clocks.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": ">>>>> \"PL\" == Paul Leach <paulle@MICROSOFT.com> writes:\n\nPL> If the servers keep no state, and just accepts the nonce that the client\nPL> quotes back at it, then you get no replay protection at all.\n\n  This is something of a digression, but it _is_ possible for the\n  server to construct nonces which are not reusable and which require\n  no per-nonce state in the server.\n\nPL> But I think we should specify that it MUST contain a timestamp, if\nPL> that's all the replay protection we're going to have. And we could\nPL> specify that the client include a timestamp in the nonce...\nPL> [description of nonce generation rules using timestamps]\n\n  First, I must repeat my favorite refrain: Not all HTTP\n  implementations have clocks - you can't require the use of\n  timestamps.\n\n  More important for the current discussion... the standard should not\n  specify how nonces are constructed.  There are very good reasons for\n  this:\n\n    - Any specified algorithm (no matter how clever) tells an attacker\n      how the nonce space is limited, thereby weakening the security.\n\n    - RFC 2069 specifies that the nonce may be constructed in any way\n      the server chooses, and specifies that the client just uses that\n      value.  Any change that requires specific algorithms for either\n      will break existing deployed implementations.\n\n  If we are going to break existing implementations, then it seems to\n  me that we should just forget the current spec altogether and go on\n  to digest-ng (which I don't think we can get done soon enough to\n  make the IESG happy with advancing 1.1).\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n> \n> A replayable Digest is just as bad as Basic.\n\nThere is obviously some disagreement here .... that statement may be\ntrue if you limit your considerations to HTTP requests .... but \nwhen you consider that all users of some operating systems are\nforced to use the same userid and password for HTTP *AND* for login\nto those systems there is a hugh difference between basic which\nallows for trivial recovery of login credentials and digest\nwhich doesn't.\n\nAnd the vendor furnished limitations are just one problem.  It is well\nknown that humans tend to use the same passwords in unrelated\ncontexts when given the opportunity to choose their own.\n\nIt sure seemed like we had concenus that getting rid of plain text\npasswords (and I'm sorry but base64 encoding is plain text, just like\ntranslating it to ebccic would be) was essential and that all the other\ndesirable functionality would be covered in digest-ng.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Tue, 20 Jan 1998, Paul Leach wrote:\n\n> irrelevant. The important property about plaintext is that it can be\n> replayed. If Digest can be replayed, then it has the property of plaintext\n> that we're trying to get rid of, and so we will have accomplished nothing.\n> NOTHING!\n\nNo, the important property is that it allows recovery of passwords for\nattack on other systems.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "There is no harm in allowing BOTH client and server to each generate part of\nthe nonce, and much good -- it makes precomputed dictionary attacks harder.\n\n> ----------\n> From: Yaron Goland\n> Sent: Monday, January 19, 1998 10:45 AM\n> To: 'John Franks'; Dave Kristol\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: RE: Some comments on Digest Auth\n> \n> I think the proposal to allow clients to generate nonces is intriguing but\n> it does concern me from a long term security point of view. Experience has\n> taught that allowing clients to generate what is essentially server side\n> information leads to trouble. For example in DAV we learned a long time\n> ago\n> to not let clients generate URIs, it tends to break things. In this case\n> allowing the client to generate nonces remove's flexibility on the\n> server's\n> part in how it generates and manages nonces. Furthermore I'm concerned\n> with\n> behavior through proxies where a proxy may have multiple connections to a\n> server and may put a client's request into any one of its connections to\n> that server.\n> \n> I guess i'm just old fashioned but I always like to err on the side of\n> maximum flexibility. In this case that means only giving the server the\n> right to generate nonces.\n> \n> Yaron\n> \n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "> ----------\n> From: Scott Lawrence[SMTP:lawrence@agranat.com]\n> Sent: Wednesday, January 21, 1998 7:07 AM\n> To: Paul Leach\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: Some comments on Digest Auth\n> \n> \n> >>>>> \"PL\" == Paul Leach <paulle@MICROSOFT.com> writes:\n> \n> PL> If the servers keep no state, and just accepts the nonce that the\n> client\n> PL> quotes back at it, then you get no replay protection at all.\n> \n>   This is something of a digression, but it _is_ possible for the\n>   server to construct nonces which are not reusable and which require\n>   no per-nonce state in the server.\n> \nIts not a digression. If we had such an algorithm, then that's what we'd\ndescribe in the draft.\n\n> PL> But I think we should specify that it MUST contain a timestamp, if\n> PL> that's all the replay protection we're going to have. And we could\n> PL> specify that the client include a timestamp in the nonce...\n> PL> [description of nonce generation rules using timestamps]\n> \n>   First, I must repeat my favorite refrain: Not all HTTP\n>   implementations have clocks - you can't require the use of\n>   timestamps.\n> \nI apologize for being loose. We may need to describe more than one algorithm\n-- one for systems with clocks, and one without.\n\n>   More important for the current discussion... the standard should not\n>   specify how nonces are constructed.  There are very good reasons for\n>   this:\n> \n>     - Any specified algorithm (no matter how clever) tells an attacker\n>       how the nonce space is limited, thereby weakening the security.\n> \nIf it's \"limited\" to a space of, say, 128 bits, that's adequate to cause\nbrute force attacks to take millions of years. Not a problem.  Besides\nwhich, I carefully said that the nonce _contains_ a time stamp, not that it\n_is_ a timestamp; any server can always include any additional random bits\nthat it wants to make the space as big as it would like.\n\n>     - RFC 2069 specifies that the nonce may be constructed in any way\n>       the server chooses, and specifies that the client just uses that\n>       value. Any change that requires specific algorithms for either\n>       will break existing deployed implementations.\n> \nI'm not talking about requiring different algorithms. We need to describe at\nleast ONE way to build a secure implementation. (Maybe one for with and\nwithout clocks.) Implementers should be free to try to do better, but\nhistory shows that without that guidance, they will often do worse.\n\nAlso, we need to display at least one good solution just to convince\nourselves that one is possible given the protocol. And it needs to be\nclosely examined -- for example, the suggestion about IP addresses in the\nnonce that is currently in the draft breaks with proxy farms. If its only a\n\"suggestion\", people tend to ignore it, because they tink \"that's not how\nI'm going to implement it\".\n\n>   If we are going to break existing implementations, then it seems to\n>   me that we should just forget the current spec altogether and go on\n>   to digest-ng (which I don't think we can get done soon enough to\n>   make the IESG happy with advancing 1.1).\n> \nWhich is why we aren't going to go straight to digest-ng. If there is any\nchange needed to the protocol, then it should be small so that the change to\nexisting implementations can be slight. \n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "There are authentication algorithms that get rid of plain-text and that\nstill allow a recovered password to be used against other systems. (Digest\nwould be one if it didn't mix the realm name into the key used to compute\nthe response. I wouldn't be suprised if that weren't accidental -- the CRAM\nMD5 protocol being used in POP3 and other mail protocols does not have that\nproperty.)\n\nSo, I'm not complaining about the current digest spec -- I just don't want\nthe criterion to be \"it's not plaintext, so it's OK\".\n\nThe following criteria are fine by me:\n1. Recovery of the password on one system doesn't allow its use on another\n2. Replay attacks are limited to a reasonably small time window, and\nimplementations can practically make it quite small.\n3. Brute force attack is infeasible on well chosen passwords.\n \nPaul\n\n\n\n"
        },
        {
            "subject": "Minutes of 1/21 editorial teleconference..",
            "content": "Larry Masinter, Jim Gettys, Paul Leach, Jeff Mogul, Henrik Frysyk, and Scott \nLawrence joined the call.  We're going back on a weekly telephone schedule to\ntry to get this wrapped up soon.\n\nAs always, the updated issues list is found at:\nhttp://www.w3.org/Protocols/HTTP/Issues/\n\n#AUTH-INFO-SYNTAX has been quiet on the mailing list, and we didn't\nget to discussing it at today's meeting.  Please look at it and comment.\n\n#DIGEST-PROBS has been quiet on the mailing list, and we didn't\nget to discussing it at today's meeting.  Please look at it and comment.\n\n#CONTENT-LENGTH has been reclassified to a technical issue from editorial; \nthere is a contradiction in the spec that needs clarification, and hopefully, \nclarifying this will also unconfuse a number of working group members who've \nbeen confused as a result. It is important we get this right... Jeff and \nHenrik are working on language here.\n\n#MULTIPART has been created; the spec is not clear as the interrelationship \nof MIME multipart being transported and/or cached by HTTP.  Roy Fielding \nhas volunteered to try to draft language to clarify this (after Feb. 1, \nas he has a different deadline then).  Our thanks to him.\n\n#MULTIPLE-CONTENT-LOCATION has been closed; the MHTML group is looking at\nother solutions.\n\n#DIGEST-MESS is still a mess, though some light is visible; there is\nconsensus that we must get it implemented and get passwords in\nthe clear off the net. Larry Masinter and Paul Leach are putting\ntogether another draft to get out soon (hopefully next week), reflecting\nall the mailing list action.\n\n#CONTENT-BASE Content-Base will be removed from the next draft (02).  Without \na feature mechanism and some required extension mechanism, we don't believe \nit is recoverable; solutions would be analogous to the problems we had with \nmaking transfer encoding work, and the payoff isn't worth that level of \ntrouble; better to solve the extensibility mess right and then deal with \nit.  Sigh...\n\n#IPV6-ADDR-URLS has been closed; solutions to it belong in the URI syntax\ndocument, which is a different group, and being driven independently of\nHTTP, so the HTTP working group should not worry about it.  Not to mention\nthe IPV6 community hasn't ever actually raised the issue with us.\n\n#DIGEST-SCALING has been subsumed by Digest mess, and whether the change\nrequired to be able to scale up will happen may hinge on whether incompatible\nchanges have to happen in Digest, which might force digest to recycle at\nproposed.  (This is not the end of the world, however, if it had to, in\nmy humble opinion; what is important is to get something implemented and\nout there sooner rather than later; don't get hung up worrying about\nprocess stuff; Larry and I will handle the details...).\n\nI've been working hard on the next draft (rev-02), at least as hard as I \ncan with a cold and a disk drive dying on the system I do the editing on.  \nMost of the editorial grunt work has been completed that will be required \nto get the document to draft standard, so I'm hoping the next draft could \nconcievably (though not likely) go to draft standard, if we can close out \nissues faster than new ones arise and I catch up with the moving target \nsoon.  If not, I'll put out an interim draft in a couple weeks (I'd like \nto avoid it if I can; it costs me about a day of work to get one done and \nsubmitted; I was too hurried the last time, and the TOC and index suffered \nas a result.) Experience shows there is always at least one more draft than \nyou hope for...\n\nThe editorial work completed includes a section on changes since RFC 2068, \nwork on cleaning up and improving the index, instructions to IANA and a \nnumber of other issues and I'm continuing work on the few remaining editorial \nissues. Editorial issues that require changes to BNF, normative language, \nor which I'm not completely sure I'll get right I send mail to the mailing \nlist, so you can complain at me. I still have to finish going through the \nmy folder since IETF to make sure none have been dropped on the floor.\n\nAs always, let me know of what issues you believe should exist.\n- Jim Gettys\n\n\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: HTTP practice and Year2000: the bad(?) new",
            "content": "> I then used \"grep\" to find the values that had a 2-digit year field\n> between 1993 and 1997; about 1247 values used this obsolete format.\n> In other words, around 20% of the values are not \"Year-2000 ready\".\n\nI recall the change was made to the HTTP spec sometime in 1992.\nPerhaps you could state which clients were producing the buggy\nfields.\n\n> that \"94\" means \"1994\".  My guess is that most of the client\n> and proxy population will be updated during the next 3 years,\n> except perhaps for some embedded systems. \n\nI wouldn't bet on people being that smart... If people could produce\nbuggy  code despite being warned in 1995 they can probably continue \nto do so in 1999. After all the problem will go away in 2000 for the\nrest of our lifetime.\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": "On Wed, 21 Jan 1998, Jim Gettys wrote:\n\n> \n> #AUTH-INFO-SYNTAX has been quiet on the mailing list, and we didn't\n> get to discussing it at today's meeting.  Please look at it and comment.\n> \n\nI believe we reached consensus to eliminate the Authentication-info\nheader from the authentication specification.  If this is done its\nsyntax should be moot.\n\n> \n> #DIGEST-MESS is still a mess, though some light is visible; there is\n> consensus that we must get it implemented and get passwords in\n> the clear off the net. Larry Masinter and Paul Leach are putting\n> together another draft to get out soon (hopefully next week), reflecting\n> all the mailing list action.\n> \n\nI believe we reached consensus to eliminate the entity digest and all\nits associated paraphenalia.  After that consensus I volunteered to\ntry to cut down the previous version.  The results of that effort\n(with a few changes since I sent it to Jim) are available at\n\n     http://hopf.math.nwu.edu/digest.revised\n\n> \n> #DIGEST-SCALING has been subsumed by Digest mess, and whether the change\n> required to be able to scale up will happen may hinge on whether incompatible\n> changes have to happen in Digest, which might force digest to recycle at\n> proposed.  (This is not the end of the world, however, if it had to, in\n> my humble opinion; what is important is to get something implemented and\n> out there sooner rather than later; don't get hung up worrying about\n> process stuff; Larry and I will handle the details...).\n> \n\nI hope we will not forget the consensus and start the debate all over\nagain.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": "Wasn't trying to stir anything up; just minutes as I understood them,\nand I haven't been able to keep up on the digest discussion (a cold,\nand a dead system disk has put me behind)..\n\nIn the case of Digest, my understanding is much less than in other\nareas, and I'm not editing the next draft (thankfully), so I don't\nneed to understand it.  I leave it to you, Larry, Paul and Scott\nto get it together.\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Wed, 21 Jan 1998, Paul Leach wrote:\n\n> \n> The following criteria are fine by me:\n> 1. Recovery of the password on one system doesn't allow its use on another\n> 2. Replay attacks are limited to a reasonably small time window, and\n> implementations can practically make it quite small.\n> 3. Brute force attack is infeasible on well chosen passwords.\n>  \n\nWe are all in agreement here.  I think that the last official\nversion of the spec with all references to entity-digest and\nAuthentication-info removed pretty well meets these needs if\na timestamp is used in the nonce.\n\nI don't see that further requirements in the spec will buy us\nmuch more.  \n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Some comments on Digest Aut",
            "content": "On Wed, 21 Jan 1998, John Franks wrote:\n\n> On Wed, 21 Jan 1998, Paul Leach wrote:\n> \n> > \n> > The following criteria are fine by me:\n> > 1. Recovery of the password on one system doesn't allow its use on another\n> > 2. Replay attacks are limited to a reasonably small time window, and\n> > implementations can practically make it quite small.\n> > 3. Brute force attack is infeasible on well chosen passwords.\n> >  \n> \n> We are all in agreement here.  I think that the last official\n> version of the spec with all references to entity-digest and\n> Authentication-info removed pretty well meets these needs if\n> a timestamp is used in the nonce.\n\nI don't think there should be a normative requirement that a timestamp\nbe used in the nonce. The server owns the nonce and could easily\nimplement some other mechanism including tracking nonces issued and\nsetting timelimits, perhaps even a moving window reset with each use.\n\nI have no objection for a SHOULD level requirement that the server\nimplement some form of replay protection, and I have no objection\nto text which describes a timestamp and clock-less server version of\nnonce handling which achieves Pauls #2 objective.\n\nAll given that the WG otherwise agrees that #2 should be part of\nthe digest specification.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Some comments on Digest Aut",
            "content": "Paul Leach <paulle@microsoft.com> writes:\n\n>>   More important for the current discussion... the standard should not\n>>   specify how nonces are constructed.  There are very good reasons for\n>>   this:\n>>\n>>     - Any specified algorithm (no matter how clever) tells an attacker\n>>       how the nonce space is limited, thereby weakening the security.\n>>\n>If it's \"limited\" to a space of, say, 128 bits, that's adequate to cause\n>brute force attacks to take millions of years. Not a problem.  Besides\n>which, I carefully said that the nonce _contains_ a time stamp, not that it\n>_is_ a timestamp; any server can always include any additional random bits\n>that it wants to make the space as big as it would like.\n\nRFC 2069, while suggesting that a good nonce value might involve a timestamp,\ndoes not specify what form a timestamp should take.  I dare say that some of\nus will use the System/370 64-bit clock, while others of you will use an\n<asctime-date> or even a Triple-DES-encrypted <rfc850-date> with a reading\nfrom the Gita as the key.  All are perfectly valid, and unpredictable from the\nspec.  While a particular variety of server may have a limited set of nonces,\nthe HTTP world will not.  At least, not unless you count Apache's market share\n;-)\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": ">>>>> \"JF\" == John Franks <john@math.nwu.edu> writes:\n\n>> #AUTH-INFO-SYNTAX has been quiet on the mailing list, and we didn't\n>> get to discussing it at today's meeting.  Please look at it and comment.\n\nJF> I believe we reached consensus to eliminate the Authentication-info\nJF> header from the authentication specification.  If this is done its\nJF> syntax should be moot.\n\n  The consensus was to remove the entity digest - the\n  Authentication-info header field is still needed for the nextnonce\n  attribute - a big performance win.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": "John Franks wrote:\n> I believe we reached consensus to eliminate the entity digest and all\n> its associated paraphenalia.  After that consensus I volunteered to\n> try to cut down the previous version.  The results of that effort\n> (with a few changes since I sent it to Jim) are available at\n> \n>      http://hopf.math.nwu.edu/digest.revised\n\nI've just glanced through this, perhaps I've missed something. If nonces\nare going to be time-limited, we need a response that means \"your nonce\nhas expired\" so the user is not prompted for a password again.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": ">>>>> \"BL\" == Ben Laurie <ben@algroup.co.uk> writes:\n\nBL> I've just glanced through this, perhaps I've missed something. If nonces\nBL> are going to be time-limited, we need a response that means \"your nonce\nBL> has expired\" so the user is not prompted for a password again.\n\n  It's in there... the server sends a 401 response, with the\n  WWW-Authenticate header to provide nonce and 'stale=true' to\n  indicate that it was the expired nonce that was the problem rather\n  than the credentials.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": "On Thu, 22 Jan 1998, Ben Laurie wrote:\n\n> John Franks wrote:\n> > I believe we reached consensus to eliminate the entity digest and all\n> > its associated paraphenalia.  After that consensus I volunteered to\n> > try to cut down the previous version.  The results of that effort\n> > (with a few changes since I sent it to Jim) are available at\n> > \n> >      http://hopf.math.nwu.edu/digest.revised\n> \n> I've just glanced through this, perhaps I've missed something. If nonces\n> are going to be time-limited, we need a response that means \"your nonce\n> has expired\" so the user is not prompted for a password again.\n> \n\nThat's what the \"stale=true\" field will do.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Pipelinin",
            "content": "> \n> Yes ... but there are some reasons why it may not be advisable from a\n> implementation design perspective.  Consider the case where request\n> below to foo.com takes forever to discover the DNS name is invalid. \n> The response from bar.com must wait and the requesting client has no\n> clue why.\n> \n> Dave Morris\n> \n\nI agree that its a difficult implementation to handle, but the case you\nmention isnt the one I was thinking about.  \nWhat do we do with sticky headers?  Should each origin server get\n1 copy of the sticky headers? Should the client have to specify\nsticky headers for the 1st request to a new origin server in a pipeline?\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy Ranger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of 1/21 editorial teleconference..",
            "content": "On Wed, 21 Jan 1998, Scott Lawrence wrote:\n\n> \n> >>>>> \"JF\" == John Franks <john@math.nwu.edu> writes:\n> \n> >> #AUTH-INFO-SYNTAX has been quiet on the mailing list, and we didn't\n> >> get to discussing it at today's meeting.  Please look at it and comment.\n> \n> JF> I believe we reached consensus to eliminate the Authentication-info\n> JF> header from the authentication specification.  If this is done its\n> JF> syntax should be moot.\n> \n>   The consensus was to remove the entity digest - the\n>   Authentication-info header field is still needed for the nextnonce\n>   attribute - a big performance win.\n> \n\nYou are right.  There was not a consensus to remove \nAuthentication-info/nextnonce.\n\nI am sceptical of its utility however.  It would be useful if\na typical session looked like \n\n request - response - request - response -  request - response...\n\nrather than\n\n 5 piplined requests - 5 piplined responses\n\nBut the latter is much more likely to be typical.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "MHTML/HTTP 1.1 Conflict",
            "content": "All,\n\nNow that the dust has settled, we may be able to look more clearly at\npotential conflicts between the latest MHTML and HTTP 1.1 draft documents.\n\n\n1. Content-Base and Content-Location\n------------------------------------\n\nMHTML and HTTP 1.1 both employ two syntactically identical MIME header\nfields. These fields are Content-Base and Content-Location. In both MHTML\nand HTTP 1.1, either or both can be employed in any top-level message\nheader, any embedded MIME message header, and any  MIME content header.\nContent-Base has identical semantics in both MHTML and HTTP 1.1.\nContent-Location has different semantics (see below), though in both cases\nit can be used to establish a content base.\n\n\n1.1 HTTP 1.1 Content-Location\n-----------------------------\n\nHTTP 1.1 defines the semantics of Content-Location as follows.\n\nrom Section 3.7.2\n\n  In HTTP, multipart body-parts MAY contain header fields which are\n  significant to the meaning of that part. A Content-Location header field\n  (section 14.15) SHOULD be included in the body-part of each enclosed\n  entity that can be identified by a URL.\n\nrom Section 14.15\n\n  The Content-Location entity-header field MAY be used to supply the\n  resource location for the entity enclosed in the message when that\n  entity is accessible from a location separate from the requested\n  resource's URI..\n\n  ...\n\n  The Content-Location value is not a replacement for the original\n  requested URI; it is only a statement of the location of the resource\n  corresponding to this particular entity at the time of the request.\n  Future requests MAY use the Content-Location URI if the desire is to\n  identify the source of that particular entity.\n\nGiven the definition of Content-Location from Section 14.15, it is not\nclear what purpose the specification of Content-Location in a content\nheader as per Section 3.7.2 serves.\n\n\n1.2 MHTML Content-Location\n--------------------------\n\nA Content-Location header field is employed in MHTML to label a MIME body\npart with an URI, and this URI is used only to satisfy URI references from\nText/HTML objects in the same MIME Multipart/Related structure.  While an\nMHTML Content-Location may be viewed as a *hint* to the original source of\na MIME body part, it is explicitly constrained from carrying any more\nweight than that. It does not, as in HTTP, \"identify the source\" of a body\npart.\n\n\n1.3 Resolving the Semantic Conflict\n-----------------------------------\n\nThere seem to be three possible approaches.\n\n1. We can merely accept that the semantics of Content-Location\n   are different in the two documents and proceed.\n\n2. HTTP can distinguish between the use of Content-Location in\n   message headers and MIME content headers. In message headers\n   it would have existing HTTP semantics. In content headers\n   it would have MHTML semantics.\n\n3. MHTML can introduce a new header field (Content-Label ?) which\n   would inherit current MHTML Content-Location semantics.\n   Content-Location could, therefore, fully retain its HTTP\n   semantics.\n\nHopefully, we can rapidly move to concensus on which approach to adopt.\n\n\n1.4 MIME Line Length Considerations\n-----------------------------------\n\nHeader fields in top level HTTP message headers are not subject to MIME\nline length constraints, and none is introduced in the HTTP 1.1 draft.\nHeader fields in MIME Message/HTTP message headers and MIME content headers\nare subject to MIME line length constraints. This is not addressed by the\nHTTP draft, though header fields can be folded and unfolded as per RFC 822.\nIt is addressed by the MHTML draft. The MHTML draft explicitly addresses\nthe encoding/folding and unfolding/decoding of long URIs as per MIME, to\nsatisfy MIME line length constraints. It could be argued that the HTTP 1.1\ndraft should employ identical language.\n\nNick\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "1. Content-Base\n\nNote that Content-Base is being removed from the HTTP/1.1 specification,\nper the minutes of our last editorial meeting.  I think that closes\nout any conflict between the specifications for Content-Base.\n\n1.1 Content-Location\n\nContent-Location in HTTP is used when resources have multiple \nrepresentations (i.e. you can get the same document back in multiple languages \nor datatypes, depending on Accept headers, for example); it isn't clear \nthat the definitions for Content-Location should match in the two uses.  \n(e.g. you do a GET on an object, and the Content-Location gives you the \nhint about where to find the version that you actually got, possibly for \nediting purposes). We can either accept the differing definitions, or you \ncan change the name in MHTML to confuse the innocent....\n\nNote that Roy is going to try to draft language for the HTTP spec to\nclarify the distinction between HTTP message headers and MIME content\nheaders.  As far as HTTP is (mostly) concerned, multipart mime\nis just more content.\n\nSo I'm not sure I have a strong feeling on which of the\nthree approaches is best.  Might be best to see Roy's words first\n(won't happen before sometime next week.\n\n1.4 MIME Line Length Considerations\n\nHTTP, being a binary 8-bit clean transport, does not have the same line \nlength limitations that are forced on E-mail systems for gateway operations \n(which is where the line length limitation for mail comes from), that force \nyou into URL wrapping (ugh, shudder; my condolences to you....).\n\nI don't think that it is wise (or even possible) to force this complexity on \nHTTP implementations at this late date (and I think it would have been\na hard sell, even 4-5 years ago). I do think that it would be wise to \nhave a note or editorial cross reference to the MIME spec, so that HTTP\nimplementations that expect to support and share code between HTTP and MIME \nwill be aware of the issue.  My opinion is that a sentence or three should \nbe drafted and added to the HTTP spec to point this out to people.\nSuggested words to make this cross reference gratefully accepted...\n\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "On Fri, 23 Jan 1998, Jim Gettys wrote:\n\n> 1. Content-Base\n> \n> Note that Content-Base is being removed from the HTTP/1.1 specification,\n> per the minutes of our last editorial meeting.  I think that closes\n> out any conflict between the specifications for Content-Base.\n\n  Actually Jim, I think that it creates one - MHTML agents will expect it\n  to be honored, while HTTP agents will ignore it.  Another sentence or\n  two in the interoperation notes...\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">  From: Scott Lawrence <lawrence@agranat.com>\n>  Date: Fri, 23 Jan 1998 11:51:13 -0500 (EST)\n>  To: Jim Gettys <jg@pa.dec.com>\n>  Cc: Nick Shelness <shelness@lotus.com>,\n>          IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n>          http-wg@cuckoo.hpl.hp.com, Nick_Shelness/SSW/Lotus@lotus.com\n>  Subject: Re: MHTML/HTTP 1.1 Conflicts\n>  \n>  On Fri, 23 Jan 1998, Jim Gettys wrote:\n>  \n>  > 1. Content-Base\n>  > \n>  > Note that Content-Base is being removed from the HTTP/1.1 specification,\n>  > per the minutes of our last editorial meeting.  I think that closes\n>  > out any conflict between the specifications for Content-Base.\n>  \n>    Actually Jim, I think that it creates one - MHTML agents will expect it\n>    to be honored, while HTTP agents will ignore it.  Another sentence or\n>    two in the interoperation notes...\n\nThe reality today is that Content-Base is ignored in HTTP (wasn't in HTTP/1.0, \nand isn't honored by any current browser of note).    Without a robust \nextension mechanism, we didn't see how to get it introduced reliably into \nthe Web.\n\nAnd as you pass off the message to an MHTML agent, the MHTML agent can do \nwhat it likes with Content-Base.\n\nAt most, Content-Base is providing additional information that can be\npassed to external viewers.  This was the reason Roy gave as to why\nhe hadn't said \"MUST\" in the first place when drafting Content-Base.\nIt was a statement of fact, as opposed to something required to make\nthe protocol work.\n\nThere really isn't anything at the protocol level that makes the\nprotocol depend on it; it is additional meta-data for the content type\nrederer to use.\n\nRemoving Content-Base does require referencing 2068, if only to say that \nit isn't in the draft standard.  It may be that we should also call\nout a reference to the MHTML spec that it uses it in interpreting content.\n\n- Jim\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "At 08.37 -0800 98-01-23, Jim Gettys wrote:\n> Content-Location in HTTP is used when resources have multiple\n> representations (i.e. you can get the same document back in multiple\n>languages\n> or datatypes, depending on Accept headers, for example); it isn't clear\n> that the definitions for Content-Location should match in the two uses.\n> (e.g. you do a GET on an object, and the Content-Location gives you the\n> hint about where to find the version that you actually got, possibly for\n> editing purposes). We can either accept the differing definitions, or you\n> can change the name in MHTML to confuse the innocent....\n\nDo I understand you rightly, that Content-Location for this purpose can\nonly occur on the outermost header of a HTTP message (or possibly\nit could occur on a subobject of Content-Type: message/http?).\n\nThe MHTML usage of Content-Location only occurs inside Content-Type:\nMultipart/related. Thus, there is in reality no conflict. We could\neven say that Content-Location can have multiple values inside\nMultipart/related but that only single values are allowed in HTTP headers!\n>\n> 1.4 MIME Line Length Considerations\n>\n> HTTP, being a binary 8-bit clean transport, does not have the same line\n> length limitations that are forced on E-mail systems for gateway operations\n> (which is where the line length limitation for mail comes from), that force\n> you into URL wrapping (ugh, shudder; my condolences to you....).\n>\n> I don't think that it is wise (or even possible) to force this complexity on\n> HTTP implementations at this late date (and I think it would have been\n> a hard sell, even 4-5 years ago). I do think that it would be wise to\n> have a note or editorial cross reference to the MIME spec, so that HTTP\n> implementations that expect to support and share code between HTTP and MIME\n> will be aware of the issue.  My opinion is that a sentence or three should\n> be drafted and added to the HTTP spec to point this out to people.\n> Suggested words to make this cross reference gratefully accepted...\n\nIn the MHTML group, we believe that more and more the same objects will be\ntransported by different protocols. For example, you get a resource through\nHTTP from your intranet, and post it via e-mail to someone outside the\nintranet who could not otherwise get it because of the firewall.\n\nIn these cases, it would be an advantage if the resource need not be\nmodified to different format in e-mail than in http. Such format changes\nmight invalidate digital seals and signatures. Thus, we think it would\nbe nice to recommend (even if not to require) that also in http the following\nrules are followed:\n\n(1) Header lines are kept shorter than 76 characters\n(2) Header lines can be split into multiple lines\n(3) In the special case of long URLs, such can be split into\n    multiple lines, and the receiving software should concatenate\n    the parts (removing entirely all whitespace) before using the\n    URL\n(4) Line breaks should be CRLF, not bare CR or bare LF\n\nAt 09.44 -0800 98-01-23, Jim Gettys wrote:\n> Removing Content-Base does require referencing 2068, if only to say that\n> it isn't in the draft standard.  It may be that we should also call\n> out a reference to the MHTML spec that it uses it in interpreting content.\n\nI think it would be very nice if the HTTP spec referenced the MHTML\nspec saying something like this:\n\n   When aggregates of linked objects (for example HTML plus in-line images)\n   are transported, they should be formatted using the MHTML standard.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se>\n>  Date: Fri, 23 Jan 1998 21:12:53 +0100\n>  To: jg@pa.dec.com (Jim Gettys), Nick Shelness <shelness@lotus.com>\n>  Cc: IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n>          http-wg@cuckoo.hpl.hp.com, Nick_Shelness/SSW/Lotus@lotus.com\n>  Subject: Re: MHTML/HTTP 1.1 Conflicts\n>  \n>  At 08.37 -0800 98-01-23, Jim Gettys wrote:\n>  > Content-Location in HTTP is used when resources have multiple\n>  > representations (i.e. you can get the same document back in multiple\n>  >languages\n>  > or datatypes, depending on Accept headers, for example); it isn't clear\n>  > that the definitions for Content-Location should match in the two uses.\n>  > (e.g. you do a GET on an object, and the Content-Location gives you the\n>  > hint about where to find the version that you actually got, possibly for\n>  > editing purposes). We can either accept the differing definitions, or you\n>  > can change the name in MHTML to confuse the innocent....\n>  \n>  Do I understand you rightly, that Content-Location for this purpose can\n>  only occur on the outermost header of a HTTP message (or possibly\n>  it could occur on a subobject of Content-Type: message/http?).\n>  \n\nYes.\n\nHTTP doesn't know beans from subobjects.  HTTP transports a rendered\nrepresentation of a single resource.  What is inside the entity (payload) isn't\nof interest to HTTP.\n\nHTTP's use of Multipart is very limited, and doesn't violate this principle. \n(e.g. the use of multipart in range requests are referring ranges of the \nsame resource as the one requested, not multiple resources).\n\n>  The MHTML usage of Content-Location only occurs inside Content-Type:\n>  Multipart/related. Thus, there is in reality no conflict. We could\n>  even say that Content-Location can have multiple values inside\n>  Multipart/related but that only single values are allowed in HTTP headers!\n\nI believe that is true.\n\nRoy will be working on drafting changes to the HTTP specFrom Albert-Lunde@nwu.edu Sat Jan 24 21:47:26 1998\nReceived: from otter.hpl.hp.com (otter.hpl.hp.com [15.144.59.2])\nby hplb.hpl.hp.com (8.8.6/8.8.6 HPLabs Relay) with ESMTP id FAA25192;\nSun, 25 Jan 1998 05:46:37 GMT\nReceived: from cuckoo.hpl.hp.com by otter.hpl.hp.com with ESMTP\n(1.37.109.16/15.6+ISC) id AA129207192; Sun, 25 Jan 1998 05:46:32 GMT\nReceived: (from procmail@localhost) by cuckoo.hpl.hp.com (8.7.6/8.7.1) id FAA00922; Sun, 25 Jan 1998 05:46:05 GMT\nResent-Date: Sun, 25 Jan 1998 05:46:05 GMT\nX-Sender: lunde@nuinfo.acns.nwu.edu (Unverified)\nMessage-Id: <v03110700b0f079aee70b@[129.105.110.129]>\nMime-Version: 1.0\nContent-Type: text/plain; charset=\"us-ascii\"\nDate: Sat, 24 Jan 1998 23:45:02 -0600\nTo: IETF working group on HTML in e-mail <mhtml@segate.sunet.se>, \n    http-wg@cuckoo.hpl.hp.com\nFrom: Albert Lunde <Albert-Lunde@nwu.edu>\nSubject: Re: MHTML/HTTP 1.1 Conflicts \nResent-Message-Id: <\"CdVIX.0.JC.x5joq\"@cuckoo>\nResent-From: http-wg@cuckoo.hpl.hp.com\nX-Mailing-List: <http-wg@cuckoo.hpl.hp.com> archive/latest/5274\nX-Loop: http-wg@cuckoo.hpl.hp.com\nPrecedence: list\nResent-Sender: http-wg-request@cuckoo.hpl.hp.com\n\nI'm not a great protocol maven, but I'm going to put in my two cents worth...\n\nIt seems like the issues you are raising are central to why HTTP is\nreferred to as \"MIME-like\" and contrasted with srict MIME in the specs.\n\n>I am reading the HTTP spec now, to check for possible problems with\n>MHTML. Since I have not read all of it yet, can you say if there is\n>anything at all in the HTTP spec which says anything about the format\n>of bodies, i.e. of what comes after the blank line which ends the\n>HTTP heading. If the HTTP spec just regards this as an arbitrary string\n>of octets, formatted according to its MIME content type, then there\n>will probably not be any risk of conflict between MHTML and HTTP.\n>\n>In particular, does specifications about header line length, header folding,\n>end-of-line characters, etc., in the HTTP spec clearly say that these\n>specs only apply to lines in the HTTP header! If it does not say so,\n>but this is the intention, you need only say this more clearly, and\n>all conflicts with MHTML will disappear.\n\nIn _most_ respects, I think HTTP regards the body as a stream of bytes...\nbut a big exception and an important difference from MIME is the treatment\nof end-of-line for text/* types.\n\nSee RFC2068 sections  3.7.1 and 19.4.1 (which I see you've read..)\n\nIn section 3.7.1 it says \"This flexibility regarding line breaks applies\nonly to text media in the entity-body; a bare CR or LF MUST NOT be\nsubstituted for CRLF within any of the HTTP control structures (such as\nheader fields and multipart boundaries).\"\n\nSo the HTTP spec says that one of its wacky non-MIME rules applies only to\nthe entity body.\n\n>It is e-mail, rather than MHTML, which has limitiations. You could\n>write something like this, perhaps as added text in chapter 3.7.1\n>of the HTTP spec?\n>\n>   The same content may sometimes be sent through e-mail, sometimes\n>   through http. E-mail has different rules than http regarding\n>   line length (preferred less than 76 characters in headings,\n>   long lines are more often folded, in particular long URLs\n>   are sometimes folded by inserting LWS which must be removed\n>   before using the URL, line breaks must be CRLF, not bare CR\n>   or bare LF). If an object is retrieved through http and then\n>   forwarded through e-mail, this may require conversion. Such\n>   conversion may invalidate checksums used for digital seals,\n>   digitals signatures, etc. This can be avoided if the resource\n>   is formatted, also in its http version, according to e-mail\n>   rules.\n>\n>> We can't at this date even contemplate splitting long URL's; it would break\n>> huge numbers of implementations.  You need to get in your head that HTTP\n>> is a binary, 8 bit clean transport (streaming RPC system) of arbitrary\n>> datatypes; it uses MIME like message syntax, but isn't really MIME.\n>\n>Certainly not in HTTP headings. But what about headings inside multipart\n>bodies, transported through HTTP?\n>\n>> The long line problem really doesn't apply to HTTP at all.\n[..]\n>Is there no user requirement among http users to be able to retrieve\n>resources through http and forward them through e-mail? If there is such\n>a user requirement, and if there is another user requirement that\n>security checksums should work accross such forwarding, then you do\n>have a problem with long lines, even if I can understand that you would\n>much prefer that there was no such problem.\n\nI think HTTP makes a distinction between its requirements and those of a\npure MIME environment. Thus these quotes from 19.4.1:\n\n>Where it is possible, a proxy or gateway from HTTP to a strict MIME\n>environment SHOULD translate all line breaks within the text media\n>types described in section 3.7.1 of this document to the MIME\n>canonical form of CRLF. Note, however, that this may be complicated\n>by the presence of a Content-Encoding and by the fact that HTTP\n>allows the use of some character sets which do not use octets 13 and\n>10 to represent CR and LF, as is the case for some multi-byte\n>character sets.\n\nand from 19.4.4:\n\n>Proxies and gateways from HTTP to MIME-compliant protocols are\n>responsible for ensuring that the message is in the correct format\n>and encoding for safe transport on that protocol, where \"safe\n>transport\" is defined by the limitations of the protocol being used.\n>Such a proxy or gateway SHOULD label the data with an appropriate\n>Content-Transfer-Encoding if doing so will improve the likelihood of\n>safe transport over the destination protocol.\n\nMy reading of this is that HTTP only imposes its own requirements on the\nHTTP headers and body: which are those of an almost-binary transport\n(almost because of the CR/LF/CRLF rules), with no line length limits.\n\nEspecially the paragraph from 19.4.4 puts the responsibity on HTTP-> mail\n(and mail-> HTTP) gateways for unscrewing the real incompatabilties with\nMIME.\n\nI'm not sure what the best fix is for some of the issues you raise, but I\ndon't think you will be able to completely allign HTTP and pure MIME\nrequirements on message bodies. HTTP is not going to start line wrapping\neverything on the off-chance responses (even signed ones) will get\ngatewayed to mail somewhere.\n\nSome of the HTTP-> mail gateway problems might be solved by applying a\nbase64 encoding of the whole thing... but this may not solve everything;\nI'm not sure.\n\nMaybe it is desirable to be more explict about what such gateways could do.\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: draft minutes, HTTPWG meetings April ",
            "content": "> Minutes, HTTP Working Group, April 7, 1997, Memphis, TN.\n[...]\n> Dave Kristol presented his new Cookie draft and discussed, briefly, the\n> controversy which has surrounded the subject.  [...]\n> Interested parties should note that the W3C is putting\n> together a forum to address this issue. \n\nThe only link I could find at www.w3.org mentioning this,\n<http://www.w3.org/pub/WWW/Member/9704/P3Call.html>, returns:\n\n> Sorry, Unauthorized. \n>\n> The URL you are requesting requires proper authentication. \n>\n> The member-only section of the W3C website is not publicly viewable.\n> Password authorization is needed for access to the W3C Member Site. \n\nWould someone fom the W3C like to comment as to the purpose/charter of the\nforum? \n\nThanks,\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "I am fast losing confidence that we can ever resolve our MHTML/HTTP\ninterworking problems, as long as the IETF allows HTTP to claim to\nonly be MIME-like, while using MIME headers, but with differences from\nthe MIME standard?  \n\nWithout the surrounding HTTP wrapper, how are we supposed to know\nwhich kind of MIME object we are dealing with?  Are we supposed to\nsniff it to see if there is any trace of HTTP smell to it?\n\nI raise this issue now because we need a reading on this situation\nfrom our APP Area Directors, and perhaps from the APP Area\nDirectorate.\n\nI do not see how we can ever sort things out when any IETF standard\nclaims to be MIME, but not quite, while it references the MIME\nstandard, and uses MIME standard headers that do not conform to the\nMIME standard.\n\nThis is a sure recipe for a long term (like continuing forever) series\nof interworking problems.\n\nIt seems to me that if any standard claims to be MIME-like, that it\nshould have been required to choose new names for its headers and to\nstrcitly conform to the MIME standard in its use of any MIME headers.\n\nI have no idea what to do about this situation, but I am having great\ndifficulty trying to figure out how we are ever going to be able to\nclose on our MHTML standard and hope for consistent interworking with\nMIME objects created for HTTP tansport, without our adopting the HTTP\nMIME-like standard for our MIME standard.\n\nAre we supposed to just give up because of some serious mistakes made\nby HTTP in the distant past?\n\nWill someone please explain how this is supposed to work?\n\n\nThanks...\\Stef\n\n\n\n"
        },
        {
            "subject": "FW: Reauthentication Requested Revisite",
            "content": "I raised this on http-ext, but there doesnt seem to be much\nfeedback, so Im posting this here.\n\nIf you have an opinion on this, I would request that\nyou join the ext list (ietf-http-ext@w3.org) and comment.\n\nthanks alot\n\njosh\n\n-----Original Message-----\nFrom: Josh Cohen \nSent: Friday, January 23, 1998 5:51 PM\nTo: 'Scott Lawrence'\nCc: ietf-http-ext@w3.org\nSubject: RE: Reauthentication Requested Revisited\n\n\n\n\n> -----Original Message-----\n> From: Scott Lawrence [mailto:lawrence@agranat.com]\n> Sent: Friday, January 23, 1998 1:46 PM\n> To: Josh Cohen\n> Cc: ietf-http-ext@w3.org\n> Subject: Re: Reauthentication Requested Revisited\n> \n> \n> \n> >>>>> \"JC\" == Josh Cohen <joshco@microsoft.com> writes:\n> \n> JC> Reauthenticarion required revisited.\n> \n>   This discussion got all mixed up.  The original requirement is that\n>   the server wants the client to discard the current credentials (that\n>   is, those used in the request to which this is a response).\n> \nmixed up ? I dont think so.  I understand your reason and I agree with it.\nThats been my motivation all along.\n\n>   There are (at least) three reasons why the server might want to do\n>   this:\n> \n>    1) The server wishes to force the user to reenter credentials (it\n>       has been too long, or too many requests since those credentials\n>       were originally obtained - make sure the same human being is\n>       still there).  This would normally accompany a 401 response.\n> \n>    2) The user has indicated (by pushing a 'logout' button or\n>       following an off-site link of some kind) that the authenticated\n>       part of the session is over; the server wants the user agent to\n>       get the credentials out of cache so that new ones will be\n>       obtained next time (eg.  student is doing registration for\n>       next semester from a public browser - pushes the 'commit\n>       schedule' button).  Most often will accompany a 2xx response.\n>       This is the one that people on the CGI newsgroups ask for\n>       several times a week.\n> \nok.. see below..\n\n>    3) Those credentials are known by the server to be no longer valid\n>       (the password just got changed).  This might be either a positive\n>       or negative response.\n>\nI beleive that this is not a problem.  If your creds fail the browsers\nbehavior is to pop up the box and ask again anyway..\n(I think this issue is presently behaving as expected without any of this )\n\n>   This also serves to illustrate that the feature should not be a\n>   status code.\n> \n> JC> Introduce a new response header action-request:\n> \n> JC>  action-request \":\" ActionID \",\" \"type\" \"=\" value\n> JC>   ActionID = OpaqueString\n> JC>   value = \"AUTH\" | \"EXEC\" | \"ECHO\"\n>\n(see below = here )\nchange to \nvalue = \"REAUTH\" | \"EXEC\" | \"PURGEAUTH\" | \"ECHO\"\n\n> JC> EXEC means \"execute\" the content body, which presumably\n> JC>  is a script, ie javascript\n> \n>   And if I (the user) don't allow execution of arbitrary code shipped\n>   to my browser by strangers (no, I don't have Java enabled)?\nWell, it would be disabling javascript, VB or whatever was in the body.\nThe EXEC can fail, but the server should be aware that it may fail.\nAll this is saying is 'you need to do something before you can\n  do this request, go look for instructions in the body'..\n\n> \n> JC> ECHO perform no action, just echo the ActionID in the\n> JC>  next request to this URI\n> \n>   Yet another form of cookie?\n> \nOne might agree, but an extremely limited cookie.\n1. only works for the exact same request URI\n2. only for the next request only, no persist\n\n> JC> Essentially this is a server to client request acknowledgement\n> JC> system.\n> \n>   And why is that any more assurance that it actually did anything\n>   than you had before? (the kid can still _claim_ to have asked Mom)\n> \nI guess really what happens here is that the kid is knowingly lying.\nThats ok, the browser must actively \"claim\" and that is enough.\nclient is saying \"I understand what you said and I dealt with it \n appropriately\"\n\nObviously a client can be coded to lie, but what Im assuming that the \nclient is coded honestly.\nIf the request failed with 4xx because something was missing, \n it will fail again even if the client is lying.\nThe exception is if its a reauth request, in which case I dont\nbeleieve there is any way to verify that.\n\n\n> --\n> Scott Lawrence           EmWeb Embedded Server\n<lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering\nhttp://www.agranat.com/\n> \n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "At 23.45 -0600 98-01-24, Albert Lunde wrote:\n> and from 19.4.4:\n>\n> >Proxies and gateways from HTTP to MIME-compliant protocols are\n> >responsible for ensuring that the message is in the correct format\n> >and encoding for safe transport on that protocol, where \"safe\n> >transport\" is defined by the limitations of the protocol being used.\n> >Such a proxy or gateway SHOULD label the data with an appropriate\n> >Content-Transfer-Encoding if doing so will improve the likelihood of\n> >safe transport over the destination protocol.\n>\n> My reading of this is that HTTP only imposes its own requirements on the\n> HTTP headers and body: which are those of an almost-binary transport\n> (almost because of the CR/LF/CRLF rules), with no line length limits.\n>\n> Especially the paragraph from 19.4.4 puts the responsibity on HTTP-> mail\n> (and mail-> HTTP) gateways for unscrewing the real incompatabilties with\n> MIME.\n\nCertainly HTTP must *allow* all the current variants of document formats\nwith various end-of-line characters. But the HTTP spec could still\n*mention* the problem that security checksums will not work if messages\nare converted from HTTP to mail, unless they already in HTTP are in\nthe canonical form. Security checksums will probably  be more and more\nimportant in the future, and more and more people will want to provide\ndocuments with digital seals and digital signatures, and which can\nbe moved from HTTP to mail.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">  From: Jacob Palme <jpalme@dsv.su.se>\n>  Date: Sat, 24 Jan 1998 19:00:45 +0100\n>  To: jg@pa.dec.com (Jim Gettys)\n>  Cc: Nick Shelness <shelness@lotus.com>,\n>          IETF working group on HTML in e-mail <mhtml@SEGATE.SUNET.SE>,\n>          http-wg@cuckoo.hpl.hp.com, Nick_Shelness/SSW/Lotus@lotus.com\n>  Subject: Re: MHTML/HTTP 1.1 Conflicts\n>  \n>  > I believe that recommending things that decrease interactive HTTP\n>  >performance\n>  > is unlikely to fly.  Each line split is of order 1% of a HTTP request (~200\n>  > bytes), which is already too verbose and hurting interactive feel and\n>  > performance.  Naive implementations that tried to do this would be hurt\n>  > the most...  The scars of the \"Accept\" disaster are still on everyone's\n>  >back...\n>  \n>  I am reading the HTTP spec now, to check for possible problems with\n>  MHTML. Since I have not read all of it yet, can you say if there is\n>  anything at all in the HTTP spec which says anything about the format\n>  of bodies, i.e. of what comes after the blank line which ends the\n>  HTTP heading. If the HTTP spec just regards this as an arbitrary string\n>  of octets, formatted according to its MIME content type, then there\n>  will probably not be any risk of conflict between MHTML and HTTP.\n>  \n\nYou've caught on to HTTP....  Great...\n\nHTTP can transport absolutely arbitrary content.  There is NOTHING which\nrestricts the bytes that come after the heading.\n\nWith the possible exception of transforming caching proxies, nothing looks \nat the payload (entity, in HTTP speak).  This is close to a hypothetical \ncase; while transforming caching proxies exist (e.g. AOL), I find it unlikely \nthat they would look inside an MHTML message and attempt to transform the \nimages they found. (and you could inhibit this, by having the HTTP origin \nserver add a \"no-transform\" cache-control directive).\n\nOtherwise, we couldn't transport the arbitrary datatypes (often binary) \nin use in the Web.\n\nSo as far as HTTP is concerned, MHTML is just another bag of bits to\nbe transported.\n\n\n>  In particular, does specifications about header line length, header folding,\n>  end-of-line characters, etc., in the HTTP spec clearly say that these\n>  specs only apply to lines in the HTTP header! If it does not say so,\n>  but this is the intention, you need only say this more clearly, and\n>  all conflicts with MHTML will disappear.\n>  \n\nHopefully, Roy's words will make this clear when he has a chance to\ndraft them this week.  This is why we've added the whole multipart thing\nas a full fledged issue on our issues list; it wasn't clear to you,\nand therefore the HTTP spec failed; by definition, then, the HTTP spec's\nuse and non-use of Multipart needs clarification.\n\n>  A possible exception from what I write above is the message/http\n>  body part. I have scanned through the HTTP 1.1 draft and found message/http\n>  mentioned twice: Once in the description of Trace, and once in chapter\n>  19.1 which specifies the message/http MIME part.\n>  \n>  Certainly, if a message/http body part is transported through e-mail,\n>  it must follow e-mail rules for heading lines, line folding, line\n>  breaks, etc. So possible chapter 19.1 should have added to it (to\n>  avoid confusion) that if a message/http body part is transported\n>  through HTTP (in the trace) then its use of line length, line folding\n>  and line breaks can follow HTTP conventions, but if a message/http\n>  body part is transported through e-mail, then it must adhere to\n>  e-mail rules in this respect.\n>  \n\nYes.  Exactly.  I suspect that is a worthwhile thing to add to our spec,\nas it is the definition of the http message type.\n\n>  If you do not specify Content-Base in the HTTP spec, but if Content-\n>  Base is specified in MHTML, then there is of course a possibility\n>  that software based on MHTML will put Content-Base headers into\n>  HTTP headings (unless you explicitly forbid this).\n\nWhy would they do this?  Content-Base isn't going to have any meaning\nto HTTP clients, in any case. It would be ignored by a receiving client.\n\n>  \n>  > I think the best we can do is point out to implementers who hope to\n>  > share MHTML and HTTP code that MHTML has limitations in this area, so\n>  > that they can write the code right the first time, rather than twice.\n>  \n>  It is e-mail, rather than MHTML, which has limitiations. You could\n>  write something like this, perhaps as added text in chapter 3.7.1\n>  of the HTTP spec?\n\nSeems like a good suggestion as to where to put it.\n\n>  \n>     The same content may sometimes be sent through e-mail, sometimes\n>     through http. E-mail has different rules than http regarding\n>     line length (preferred less than 76 characters in headings,\n>     long lines are more often folded, in particular long URLs\n>     are sometimes folded by inserting LWS which must be removed\n>     before using the URL, line breaks must be CRLF, not bare CR\n>     or bare LF). If an object is retrieved through http and then\n>     forwarded through e-mail, this may require conversion. Such\n>     conversion may invalidate checksums used for digital seals,\n>     digitals signatures, etc. This can be avoided if the resource\n>     is formatted, also in its http version, according to e-mail\n>     rules.\n\nYes.  If the text content to be transported via HTTP is in e-mail rules,\nnothing should break.  As far as that goes, the bytes could be arbitrary,\nand nothing would break.\n>  \n>  > We can't at this date even contemplate splitting long URL's; it would break\n>  > huge numbers of implementations.  You need to get in your head that HTTP\n>  > is a binary, 8 bit clean transport (streaming RPC system) of arbitrary\n>  > datatypes; it uses MIME like message syntax, but isn't really MIME.\n>  \n>  Certainly not in HTTP headings. But what about headings inside multipart\n>  bodies, transported through HTTP?\n>  \n\nAgain, with a few specific exceptions like range requests and multipart\nform data, HTTP doesn't use multipart for anything.  HTTP isn't going\nto look at the headings inside of multipart bodies, with the\nnoted exceptions of multipart types defined by and used by HTTP itself.\n\n>  > The long line problem really doesn't apply to HTTP at all.  The HTTP\n>  >protocol\n>  > (as opposed to \"smart\" proxies speaking HTTP that might attempt to transform\n>  > content, of which there are a few) does not look inside the payload (even\n>  > these proxies would be unlikely to try to transform things inside of a MHTML\n>  > message).  If the content is multipart, most HTTP implementations won't\n>  > look at the content at all, or any of your headers (with this possible\n>  > exception of transforming proxies, though I expect it is unlikely they\n>  > will bother).\n>  \n>  Is there no user requirement among http users to be able to retrieve\n>  resources through http and forward them through e-mail? If there is such\n>  a user requirement, and if there is another user requirement that\n>  security checksums should work accross such forwarding, then you do\n>  have a problem with long lines, even if I can understand that you would\n>  much prefer that there was no such problem.\n\nThe reality of the web is that there is tons of content without any line \nlength enforced already exisiting...   It just isn't possible to get the \nhundreds of millions of documents reformatted at this date.  Think of generic \nHTML on the Web as just another binary data type rather than as text, and \nyou'll see how to deal with it.  To forward this HTML through email safely, \nyou are certainly have to encode it before transmission. If the bags of \nbits get messed with, the security checksums will clearly fail.\n\nWhat this means, is that there are two possible cases:\n1) the HTML to be mailed could be reformatted to email rules, if no \nsecurity measures have been applied...  \n2) the HTML gets encoded and transmitted, and treated as a bag\nof bits, if it has any security wrappers.\n\nEasier might be just to say that all HTML forwarded via email must be\nencoded before transfer (at the cose of some bytes on the wire, but\nmail isn't real time), rather than having two cases to handle.  Any HTML\nrenderer people use is likely to be willing to accept HTTP relaxed rules\nfor line length, line termination and URL length.\n\nThe world is a jungle out there...  Ugh...\n\n>  \n>  > >  I think it would be very nice if the HTTP spec referenced the MHTML\n>  > >  spec saying something like this:\n>  > >\n>  > >     When aggregates of linked objects (for example HTML plus in-line\n>  >images)\n>  > >     are transported, they should be formatted using the MHTML standard.\n>  > >\n>  >\n>  > This is a simple plug for MHTML, and doesn't address the issue for which\n>  > it is intended (to let implementers of 2068 be aware that there are two\n>  > definitions for Content-Base out there; the 2068 definition and the MHTML\n>  > definition).  I don't think that the Microsoft Word world would necessarily\n>  > appreciate that language, for example.  It is inappropriate for a protocol\n>  > specification.\n>  \n>  The importance is that there should not be two, possibly conflicting\n>  standards for the format of body contents (what comes after the end\n>  of the HTTP heading). By referencing MHTML and MIME, you just say that\n>  the format of body contents is not specifed by the HTTP specs but by\n>  the MIME and MHTML specs. This is already implied in the HTTP 1.1 spec\n>  in the following quote:\n>  \n>     3.7.1Canonicalization and Text Defaults\n>  \n>     Internet media types are registered with a canonical form. In\n>     general, an entity-body transferred via HTTP messages MUST be\n>     represented in the appropriate canonical form prior to its\n>     transmission; the exception is \"text\" types, as defined in\n>     the next paragraph.\n>  \n>     When in canonical form, media subtypes of the \"text\" type use\n>     CRLF as the text line break. HTTP relaxes this requirement and\n>     allows the transport of text media with plain CR or LF alone\n>     representing a line break when it is done consistently for an\n>     entire entity-body. HTTP applications MUST accept CRLF, bare CR,\n>     and bare LF as being representative of a line break in text\n>     media received via HTTP. In addition, if the text is\n>     represented in a character set that does not use octets 13 and\n>     10 for CR and LF respectively, as is the case for some\n>     multi-byte character sets, HTTP allows the use of whatever\n>     octet sequences are defined by that character set to represent\n>     the equivalent of CR and LF for line breaks. This flexibility\n>     regarding line breaks applies only to text media in the\n>     entity-body; a bare CR or LF MUST NOT be substituted for\n>     CRLF within any of the HTTP control structures (such as\n>     header fields and multipart boundaries).\n>  \n\nYes, but as I note above, HTTP has relaxed these requirements (or rather, \nthe reality of what people have put up as content on the Web has forced \nus to acknowledge the relaxation of the MIME text requirements, at least \nas used by HTTP).  \n\n>  > What I need is to know how to reference Content-Base in MHTML, so that when\n>  > I delete Content-Base from the HTTP spec, I can put a reference to the MHTML\n>  > spec as well as 2068 (needs to go in the \"Changes from RFC 2068\" section\n>  > I have written for the next HTTP draft).  E.g. title of the RFC, authors,\n>  > what section of MHTML is appropriate, etc.\n>  \n>  You might also mention that Content-Location is used both in MHTML and\n>  in HTTP in similar ways, but that the exact usage should be taken from\n>  the appropriate standard.\n>  \n>  The present official version of MHTML is in RFC 2110:\n>  \n>  2110 MIME E-mail Encapsulation of Aggregate Documents, such as HTML\n>       (MHTML). J. Palme, A. Hopmann. March 1997. (Format: TXT=41961 bytes)\n>       (Status: PROPOSED STANDARD)\n>  \n>  We will submit a new, revised version soon, whether it will be before\n>  or after the HTTP 1.1 draft standard is difficult to say. Best, of course,\n>  is if you reference the new version. \n\n> There are no big differences, but\n>  there are differences. For example, in RFC 2110, Content-Base is only\n>  valid for its immediate content, not for subparts. In the new spec,\n>  Content-Base can give a base also for subparts (unless these have\n>  their own base indication). The description of how relative URLs\n>  are handled is also slightly different. Most of these changes have\n>  been made in order to better agree with the new RFC 1808.\n>  \n>  The present reference to the new version of RFC 2110 is\n>  MIME Encapsulation of Aggregate Documents, such as HTML (MHTML).\n>  J. Palme, A. Hopmann, N. Shellness, November 1997.\n>  \n>  If you reference that document, the RFC editor will change the\n>  reference to the RFC number of our new version of the MHTML\n>  proposed standard.\n>  \n>  MTHML is also based on two other proposed standards, but I do not think\n>  you need reference them. They are:\n>  \n>  2112 The MIME Multipart/Related Content-type. E. Levinson. February\n>       1997. (Format: TXT=17052 bytes) (Obsoletes RFC1872) (Status: PROPOSED\n>       STANDARD)\n>  \n>  2111 Content-ID and Message-ID Uniform Resource Locators. E. Levinson.\n>       February 1997. (Format: TXT=9099 bytes) (Status: PROPOSED STANDARD)\n>  \n>  --- --- ---\n\nThanks.  The reference won't be normative in the HTTP case, so I don't think\nit matters too much.\n\nThe RFC editor generally updates references to the latest document\nas part of his/her work.\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">  Sender: stef@nma.com\n>  From: Einar Stefferud <Stef@nma.com>\n>  Date: Sun, 25 Jan 1998 00:13:37 -0800\n>  To: IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n>          http-wg@cuckoo.hpl.hp.com\n>  Subject: Re: MHTML/HTTP 1.1 Conflicts \n>  \n>  I am fast losing confidence that we can ever resolve our MHTML/HTTP\n>  interworking problems, as long as the IETF allows HTTP to claim to\n>  only be MIME-like, while using MIME headers, but with differences from\n>  the MIME standard?  \n>  \n>  Without the surrounding HTTP wrapper, how are we supposed to know\n>  which kind of MIME object we are dealing with? \n\nBy its MIME type.  HTTP has adopted the HTTP type registry, lock stock and \nbarrel.  There is one place, where HTTP has relaxed this: text types, \nreflecting the reality of the Web, where you don't (necessarily) get CRLF's \nfor line delimiters, and don't have line length restrictions. This is \nacknowleging existing practice, not something we believe is necessarily \ndesirable (though it is pretty clear that one of the reasons the Web succeeded \nwas that pretty well arbitrary plaintext documents could be served up without \nmodification, early in the Web).  This reflects the reality of the text \ncontent on the Web, and how it was prepared, and that HTTP servers treat \nall datatypes the same, as bags of bits (possibly with some metadata).\n\nNote that any fully conformant (i.e. email) text body and/or message is, \nhowever, a valid HTTP text entity.  This is one saving grace for interoperability.\n\n> Are we supposed to\n>  sniff it to see if there is any trace of HTTP smell to it?\n>  \n>  I raise this issue now because we need a reading on this situation\n>  from our APP Area Directors, and perhaps from the APP Area\n>  Directorate.\n>  \n>  I do not see how we can ever sort things out when any IETF standard\n>  claims to be MIME, but not quite, while it references the MIME\n>  standard, and uses MIME standard headers that do not conform to the\n>  MIME standard.\n>  \n\nI don't think HTTP claims to be MIME.  It certainly borrows both\nsyntax and lots of usage, and the type registry from MIME.\n\n>  This is a sure recipe for a long term (like continuing forever) series\n>  of interworking problems.\n>  \n\nYup.  Best we can do is try to avoid new ones.\n\n>  It seems to me that if any standard claims to be MIME-like, that it\n>  should have been required to choose new names for its headers and to\n>  strcitly conform to the MIME standard in its use of any MIME headers.\n\nYou won't get any argument out of us; we certainly agree.  But it is water under\nthe dam, in some cases.\n\n>  \n>  I have no idea what to do about this situation, but I am having great\n>  difficulty trying to figure out how we are ever going to be able to\n>  close on our MHTML standard and hope for consistent interworking with\n>  MIME objects created for HTTP tansport, without our adopting the HTTP\n>  MIME-like standard for our MIME standard.\n>  \n>  Are we supposed to just give up because of some serious mistakes made\n>  by HTTP in the distant past?\n>\n\nI don't think things are quite a bad as you make out.\n\nYou are still trying to think of HTTP as MIME; it isn't...\n\nNote that the HTTP standard has been trying as best it can to mitigate the\ndamage for a long time.\n \n>  Will someone please explain how this is supposed to work?\n>  \n>  \n\nThink of HTTP as a generic \"bag of bits transport\" protocol, using\nMIME types to type the bags of bits, and you'll get the general idea.\n(FTP with types on steroids).\n\nMHTML and MIME as far as HTTP is concerned is just one more datatype, one that\nhappens to strongly resemble HTTP in syntax.\n\nThat it looks like MIME is pretty much an accident of history, in my\nhumble opinion; the confusion this has caused has haunted HTTP and now will\nhaunt MIME and MHTML for years.\n\nHTTP is enough like MIME to confuse even the non-innocent (or maybe\nparticularly the non-innocent) into believing it is MIME.  So people who\ndon't know MIME, just find HTTP a baroque and wierd transport protocol, while\nthose who come to HTTP from MIME get all confused.  So, in our experience,\nit is the MIME experts who have the biggest trouble getting their\nheads around HTTP.\n\nI wish there were a magic wand to \"fix\" this, and the resulting confusion; \nthere isn't one I can find.  If I had been the one to design HTTP, it wouldn't \nhave looked like RFC 822...\n\n- Jim Gettys\n\n\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: editoral issues raised by Ross Patterso",
            "content": ">  From: \"Ross Patterson\" <Ross_Patterson@ns.reston.vmd.sterling.com>\n>  Date: Tue, 30 Dec 97 13:01:50 EST\n>  To: jg@w3.org\n>  Cc: masinter@parc.xerox.com\n>  Subject: HTTP editorial issues (21Nov97)\n>  \n>  I just finished a cover-to-cover reread of draft-ieft-http-v11-spec-rev-01,\n>  the 21 November 1997 HTTP 1.1 draft.  I don't know how many of these you've\n>  heard about, but I thought I'd pass them along.\n>  \n>     1) (TOC) Lots of section-numbering errors (\"1.1.2\" following \"8.1.1\", etc.)\n>  \n\nMicrosoft Word often loses its cookies in section numbering; I didn't\ncatch that its cookies were lost when I generated the postscript in the\nlast draft (may have to do with what time of night I generated it).  I give\nWord at best a B- as a word processing tool...\n\nI'll try to make sure the next draft doesn't have the problem.\n\n>     2) (4.1) The note reads \"... client implementations generate an extra\n>        CRLF's after ...\".  That's either \"an extra CRLF\" or \"some extra CRLF's\".\n>  \n\n\"extra CRLF's\" seems to be right.\n\n>     3) (8.1.3) The third paragraph contains an unresolved reference for\n>        \"information about the Keep-Alive header ...\".\n> \n\nI deleted the KeepAlive section from this document, and forgot the change\nthe reference to the section to RFC 2068 instead.  We don't need to\nkeep things not part of the standard that we may need to reference\nonce we have them in an RFC somewhere (in this case, 2068).\nThanks....\n \n>     4) (8.1.4) The third paragraph reads \"For example, a client MAY have\n>        started to send ...\".  I think you mean \"may\", not \"MAY\", as this isn't\n>        a requirement statement.\n>  \n\nYup. You are right...\n\n>     5) (8.1.4) The fourth paragram ends with \"However, this automatic retry\n>        SHOULD NOT be repeated if the second request fails.\"  I think you mean\n>        the second retry of the sequence of requests, not the second request of\n>        the sequence.\n>  \n\nI modified the last sentence to say:\n\"The automatic retry SHOULD NOT be repeated if the second sequence of requests\nfails.\"\n\n>     6) (11.1) There is a dangling \"Scheme\" at the end of the sentence.\n>  \n\nDeleted.\n\n>     7) (12.2) The first paragraph contains an unresolved reference for\n>        \"... field-name Alternates, as described in appendix ...\".\n>  \nDeleted the dangling reference, as it isn't needed...\n\n>     8) (13.2.3) The third paragraph claims that \"... HTTP/1.1 requires\n>        origin servers to send a Date header with every response ...\", but that\n>        contradicts (14.19) where there are rules for when a server doesn't\n>        have to supply a Date header.\n>  \n\nCovered in a different message to come.\n\n>     9) (13.2.3) The fourth paragraph contains the repeated phrase \"HTTP/1.1\n>        uses the Age response header to\".\n\nYup.  Fixed.\n\n>  \n>    10) (13.2.6) The second paragraph reiterates the claim that \"... the HTTP/1.1\n>        specification requires the transmission of Date headers on every\n>        response\".\n\nCovered in a different message to come.\n\n>  \n>    11) (13.5.1) There is an extra bullet in the list of hop-by-hop headers, and\n>        again in the list of headers that a non-caching proxy MUST NOT modify.\n>  \n\nShould be fixed now.\n\n>    12) (13.6) The fifth paragraph reads \"A Vary header field-value of \"*\"\n>        always fail to match ...\".  It should read \"... always fails to ...\".\n>  \n\nFixed.\n\n>    13) (14.15) There is an extra period at the end of the first sentence.\n> \n\nFixed.\n\n>    14) (14.32) The second paragraph ends \"Clients SHOULD include both header\n>        fields when a no-cache request is sent to a server not known to be\n>        HTTP/1.1 compliant.\"  The fourth paragraph beings \"HTTP/1.1 clients\n>        SHOULD NOT send the Pragma request-header.\"  This seems to be a\n>        contradiction.\n>  \n\nCovered in a separate message to come.\n\n>    15) (14.48) The production for <t-codings> doesn't allow \"identity\", but\n>        rule 3 seems to allow \"identity;q=0\".\n> \nHere's Henrik's clarification:\nit is not that \"identity\" isn't allowed - the problem\nis that chunked is special - due to backwards compatibility concerns.\n\nIn general, all transfer-codings can have parameters associated with them\n(both in Transfer-Encoding and TE header fields) and they can be assigned a\nq-value when they are listed in the TE header.\n\nHowever, chunked is special in that it can't have any parameters and it\ncan't have a qvalue when listed in the TE header field. This is why chunked\nis treated specially in the BNF for both Transfer-Encoding and TE. All\nother transfer-codings, including \"identity\" are included in the\ntransfer-extension contruct.\n\nThe current wording is therefore correct - but it merits a note like\nthis in the TE section:\n\n        Note: Because of backwards compatibility considerations\n        with RFC 2068, the \"chunked\" transfer-coding can not be used\n        with parameter or accept-params.\n \n>    16) (15.1) This section looks like it no longer belongs in the document,\n>        since there is no actual discussion of Basic authentication in the\n>        HTTP/1.1 spec.  It is also duplicated in the authentication draft\n>        (draft-ietf-http-authentication-00, 21 November 1997, section 4.1).\n>  \n\nYup.  You are right.  I like deleting material :-).\n\n>    17) (19.3) The fourth paragraph states \"... no label is preferred over the\n>        labels US-ASCII or ISO-8859-1.\"  That can either be read as \"There is\n>        no label that we prefer more than US-ASCII and ISO-8859-1\" or \"We\n>        prefer an unlabeled character set over the US-ASCII and ISO-8859-1\n>        labels.\"  I'm confused enough that I can't even guess which was meant.\n\n\nI've clarified this to:\n\"The character set of an entity-body should be labeled as the lowest common \ndenominator of the character codes used within that body, with the exception \nthat not labeling the entity is preferred over labeling the entity with \nthe labels US-ASCII or ISO-8859-1. See section 3.7.1.\"\n\n>  \n>    18) (19.8.1) There are two odd line breaks in the phase \"part of a\n>        quoted-string\".\n>  \n\nYes, that is exactly the point.  But indenting the lines may make it clearer,\nso I've indented it...\n\n>    19) (19.8.3) This section begs for either a citation of the old\n>        specification or a description of it.\n>  \n>  Many thanks to you and the rest of the editting group - this is a mammoth\n>  work and of great importance to a lot of us.\n>  \n>  Ross Patterson\n>  Sterling Software, Inc.\n>  VM Software Division\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n\nattached mail follows:\nI just finished a cover-to-cover reread of draft-ieft-http-v11-spec-rev-01,\nthe 21 November 1997 HTTP 1.1 draft.  I don't know how many of these you've\nheard about, but I thought I'd pass them along.\n\n   1) (TOC) Lots of section-numbering errors (\"1.1.2\" following \"8.1.1\", etc.)\n\n   2) (4.1) The note reads \"... client implementations generate an extra\n      CRLF's after ...\".  That's either \"an extra CRLF\" or \"some extra CRLF's\".\n\n   3) (8.1.3) The third paragraph contains an unresolved reference for\n      \"information about the Keep-Alive header ...\".\n\n   4) (8.1.4) The third paragraph reads \"For example, a client MAY have\n      started to send ...\".  I think you mean \"may\", not \"MAY\", as this isn't\n      a requirement statement.\n\n   5) (8.1.4) The fourth paragram ends with \"However, this automatic retry\n      SHOULD NOT be repeated if the second request fails.\"  I think you mean\n      the second retry of the sequence of requests, not the second request of\n      the sequence.\n\n   6) (11.1) There is a dangling \"Scheme\" at the end of the sentence.\n\n   7) (12.2) The first paragraph contains an unresolved reference for\n      \"... field-name Alternates, as described in appendix ...\".\n\n   8) (13.2.3) The third paragraph claims that \"... HTTP/1.1 requires\n      origin servers to send a Date header with every response ...\", but that\n      contradicts (14.19) where there are rules for when a server doesn't\n      have to supply a Date header.\n\n   9) (13.2.3) The fourth paragraph contains the repeated phrase \"HTTP/1.1\n      uses the Age response header to\".\n\n  10) (13.2.6) The second paragraph reiterates the claim that \"... the HTTP/1.1\n      specification requires the transmission of Date headers on every\n      response\".\n\n  11) (13.5.1) There is an extra bullet in the list of hop-by-hop headers, and\n      again in the list of headers that a non-caching proxy MUST NOT modify.\n\n  12) (13.6) The fifth paragraph reads \"A Vary header field-value of \"*\"\n      always fail to match ...\".  It should read \"... always fails to ...\".\n\n  13) (14.15) There is an extra period at the end of the first sentence.\n\n  14) (14.32) The second paragraph ends \"Clients SHOULD include both header\n      fields when a no-cache request is sent to a server not known to be\n      HTTP/1.1 compliant.\"  The fourth paragraph beings \"HTTP/1.1 clients\n      SHOULD NOT send the Pragma request-header.\"  This seems to be a\n      contradiction.\n\n  15) (14.48) The production for <t-codings> doesn't allow \"identity\", but\n      rule 3 seems to allow \"identity;q=0\".\n\n  16) (15.1) This section looks like it no longer belongs in the document,\n      since there is no actual discussion of Basic authentication in the\n      HTTP/1.1 spec.  It is also duplicated in the authentication draft\n      (draft-ietf-http-authentication-00, 21 November 1997, section 4.1).\n\n  17) (19.3) The fourth paragraph states \"... no label is preferred over the\n      labels US-ASCII or ISO-8859-1.\"  That can either be read as \"There is\n      no label that we prefer more than US-ASCII and ISO-8859-1\" or \"We\n      prefer an unlabeled character set over the US-ASCII and ISO-8859-1\n      labels.\"  I'm confused enough that I can't even guess which was meant.\n\n  18) (19.8.1) There are two odd line breaks in the phase \"part of a\n      quoted-string\".\n\n  19) (19.8.3) This section begs for either a citation of the old\n      specification or a description of it.\n\nMany thanks to you and the rest of the editting group - this is a mammoth\nwork and of great importance to a lot of us.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Change to Chunk Length Synta",
            "content": "Scott,\n\nAt last this fine improvement is moving into the spec where\nit should have beeen all along.\n\nPlease note that a number of http implementations already addressed this\nproblem not known content length in advance due to dynamic content\ngeneration by padding with spaces on the right of the HEX length.\n\nYour change to the BNF should make space padding on the right explicit \nin order to ensure interoperability of the installed base. Typically, these \nimplmentations perform all character translation and chunking using windows into a\nsingle buffer, which on some platforms is the TCP packet.\n\nYou can find discussions about this in the working group archives back\nabout 6-8 months. Roy Fielding proposed some syntax at the time.\n\nWhen is this spec change expected to be deployed?\n\nJohn C. Mallery\nArtificial Intelligence Laboratory, Massachusetts Institute of Technology\n545 Technology Square, NE43-797, Cambridge, MA 02139-4301 USA\nEmail: JCMa@ai.mit.edu, Phone: 617-253-5966, Fax: 617-253-5060 \nWWW: http://www.ai.mit.edu/people/jcma/jcma.html\n\n\n\n"
        },
        {
            "subject": "Re: HTTP editorial issues (21Nov97",
            "content": ">  \n>     8) (13.2.3) The third paragraph claims that \"... HTTP/1.1 requires\n>        origin servers to send a Date header with every response ...\", but that\n>        contradicts (14.19) where there are rules for when a server doesn't\n>        have to supply a Date header.\n>  \n>    10) (13.2.6) The second paragraph reiterates the claim that \"... the HTTP/1.1\n>        specification requires the transmission of Date headers on every\n>        response\".\n>  \n\nThis is left over from previous issues around allowing HTTP to be used\nwith clockless origin servers  (see issues list issue: NO_CLOCK).\n\nI think the right fix is in the third paragraph of 13.2.3, which I've\nrewritten to:\n\n\"HTTP/1.1 origin servers should send a Date header with every response if \npossible, giving the time at which the response was generated (see section \n14.19).\"\n\nAnd rewriting the paragraph of 13.2.6 to just say:\n\n\"Neither the entity tag nor the expiration value can impose an ordering \non responses, since it is possible that a later response intentionally carries \nan earlier expiration time. The Date values are ordered to a granularity \nof one second.\"\n\n>    14) (14.32) The second paragraph ends \"Clients SHOULD include both header \n>        fields when a no-cache request is sent to a server not known to be\n>        HTTP/1.1 compliant.\"  The fourth paragraph beings \"HTTP/1.1 clients \n>        SHOULD NOT send the Pragma request-header.\"  This seems to be a \n>        contradiction. \n>  \n\nYup; you are right.  The best thing to do, Henrik and I think, is just to \ndelete the sentence \"HTTP/1.1 clients SHOULD NOT send the Pragma \nrequest-header.\"  We think the other cases having to do with proxies are \ncovered under the upgrade requirements already in the draft.  I think any \nrational implementation will likely just play it safe and always send \"Pragma: \nnocache\", rather than trying to optimize out the deprecated feature (Pragma); \nafter all, you are cache busting, and can't expect stirling performance \nunder these circumstances...\n\nThanks for your careful read; I really appreciate readers who\ndo as fine a job as you do.  And everyone everwhere benefits from\nthe reduction of errors in the standard.\n- Jim\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "I understand that we are dealing with a legacy of bad HTTP choices\nback when there was no IETF involvement, adn the people developing\nythe \"standard\" understood tha the way to set standards was to \"just\ndo what you wnat to do\" adn get it over with.\n\nAll this menas that it is just an accident of history and so we have\nto just grin and bear it and live with all the bad fallout effects.\n\nI am looking for some way to provide published expanations for the\nroots of all the confusion and so educate peple on two fronts:\n\n1.  How to cope wth the existing confustion.\n\n2.  How and why to avoid this kind of thing in the future.\n\nI see no reason to leave this kind of knowledge hidden to as to invite\nmore fo the same.  Do we need some kind of IETF/IAB purblicatron that\nexplains why one protocol should not use the same verbs as other\ndiffernet protocols use for different meanings?\n\nIs this a higher meta level issue for our Architecture Board to work\non?\n\nCheers...\\Stef\n\nrom your message Mon, 26 Jan 1998 11:08:23 -0800:\n}\n}\n}\n}>  Sender: stef@nma.com\n}>  From: Einar Stefferud <Stef@nma.com>\n}>  Date: Sun, 25 Jan 1998 00:13:37 -0800\n}>  To: IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n}>          http-wg@cuckoo.hpl.hp.com\n}>  Subject: Re: MHTML/HTTP 1.1 Conflicts \n}>  \n}>  I am fast losing confidence that we can ever resolve our MHTML/HTTP\n}>  interworking problems, as long as the IETF allows HTTP to claim to\n}>  only be MIME-like, while using MIME headers, but with differences from\n}>  the MIME standard?  \n}>  \n}>  Without the surrounding HTTP wrapper, how are we supposed to know\n}>  which kind of MIME object we are dealing with? \n}\n}By its MIME type.  HTTP has adopted the HTTP type registry, lock stock and \n}barrel.  There is one place, where HTTP has relaxed this: text types, \n}reflecting the reality of the Web, where you don't (necessarily) get CRLF's \n}for line delimiters, and don't have line length restrictions. This is \n}acknowleging existing practice, not something we believe is necessarily \n}desirable (though it is pretty clear that one of the reasons the Web succeeded \n}was that pretty well arbitrary plaintext documents could be served up without \n}modification, early in the Web).  This reflects the reality of the text \n}content on the Web, and how it was prepared, and that HTTP servers treat \n}all datatypes the same, as bags of bits (possibly with some metadata).\n}\n}Note that any fully conformant (i.e. email) text body and/or message is, \n}however, a valid HTTP text entity.  This is one saving grace for interoperability.\n}\n}> Are we supposed to\n}>  sniff it to see if there is any trace of HTTP smell to it?\n}>  \n}>  I raise this issue now because we need a reading on this situation\n}>  from our APP Area Directors, and perhaps from the APP Area\n}>  Directorate.\n}>  \n}>  I do not see how we can ever sort things out when any IETF standard\n}>  claims to be MIME, but not quite, while it references the MIME\n}>  standard, and uses MIME standard headers that do not conform to the\n}>  MIME standard.\n}>  \n}\n}I don't think HTTP claims to be MIME.  It certainly borrows both\n}syntax and lots of usage, and the type registry from MIME.\n}\n}>  This is a sure recipe for a long term (like continuing forever) series\n}>  of interworking problems.\n}>  \n}\n}Yup.  Best we can do is try to avoid new ones.\n}\n}>  It seems to me that if any standard claims to be MIME-like, that it\n}>  should have been required to choose new names for its headers and to\n}>  strcitly conform to the MIME standard in its use of any MIME headers.\n}\n}You won't get any argument out of us; we certainly agree.  But it is water under\n}the dam, in some cases.\n}\n}>  \n}>  I have no idea what to do about this situation, but I am having great\n}>  difficulty trying to figure out how we are ever going to be able to\n}>  close on our MHTML standard and hope for consistent interworking with\n}>  MIME objects created for HTTP tansport, without our adopting the HTTP\n}>  MIME-like standard for our MIME standard.\n}>  \n}>  Are we supposed to just give up because of some serious mistakes made\n}>  by HTTP in the distant past?\n}>\n}\n}I don't think things are quite a bad as you make out.\n}\n}You are still trying to think of HTTP as MIME; it isn't...\n}\n}Note that the HTTP standard has been trying as best it can to mitigate the\n}damage for a long time.\n} \n}>  Will someone please explain how this is supposed to work?\n}>  \n}>  \n}\n}Think of HTTP as a generic \"bag of bits transport\" protocol, using\n}MIME types to type the bags of bits, and you'll get the general idea.\n}(FTP with types on steroids).\n}\n}MHTML and MIME as far as HTTP is concerned is just one more datatype, one that\n}happens to strongly resemble HTTP in syntax.\n}\n}That it looks like MIME is pretty much an accident of history, in my\n}humble opinion; the confusion this has caused has haunted HTTP and now will\n}haunt MIME and MHTML for years.\n}\n}HTTP is enough like MIME to confuse even the non-innocent (or maybe\n}particularly the non-innocent) into believing it is MIME.  So people who\n}don't know MIME, just find HTTP a baroque and wierd transport protocol, while\n}those who come to HTTP from MIME get all confused.  So, in our experience,\n}it is the MIME experts who have the biggest trouble getting their\n}heads around HTTP.\n}\n}I wish there were a magic wand to \"fix\" this, and the resulting confusion; \n}there isn't one I can find.  If I had been the one to design HTTP, it wouldn't \n}have looked like RFC 822...\n}\n}- Jim Gettys\n}\n}\n}\n}\n}--\n}Jim Gettys\n}Industry Standards and Consortia\n}Digital Equipment Corporation\n}Visting Scientist, World Wide Web Consortium, M.I.T.\n}http://www.w3.org/People/Gettys/\n}jg@w3.org, jg@pa.dec.com\n}\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "Well, the problem is that there is no \"Guide to Developing Internet protocols\"\ndocument that I've seen, that sets out enough of the lore required.\n\nThe closest I've seen is an Internet Draft, that I saw some months ago, \non application protocols; it was about 80% correct, but incomplete. (I don't \nhave the reference handy)...  It drew some conclusions I don't think were \nwarranted:  e.g. text protocols are good, while the real point is that \nprotocols should be designed for extensibility up front, whether text or \nbinary based. It happens that text protocols (e.g. RFC 822 decendends) are \noften easy to extend (at least in certain ways, as HTTP has been finding \nout, hard to extend in some other ways); we have another set of lessons \nthat we've been learning the hard way.  As author of a binary applications\nprotocol that has been extended in many, many ways successfully, that\nis the real lesson (note that the X Window System lacked a formal extension\nmechanism until Version 11; thankfully, we were able to kill all older\nversions of X, and survived to tell the tale).\n\nI think it would be worthwhile to have such a document, and would be willing \nto help raise it as an issue with the IAB.  Ultimately, it is the kind of \nguidance that the IAB should provide (or endorse someone's attempt at such \nguidance).\n- Jim\n\n\nIt would be good for such a document to try to collect the lore\nof Internet Protocol design to help out newcomers...\n- Jim\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">  Sender: stef@nma.com\n>  From: Einar Stefferud <Stef@nma.com>\n>  Date: Mon, 26 Jan 1998 13:13:40 -0800\n>  To: Jim Gettys <jg@pa.dec.com>\n>  Cc: IETF working group on HTML in e-mail <mhtml@segate.sunet.se>,\n>          http-wg@cuckoo.hpl.hp.com\n>  Subject: Re: MHTML/HTTP 1.1 Conflicts \n>  \n>  I understand that we are dealing with a legacy of bad HTTP choices\n>  back when there was no IETF involvement, adn the people developing\n>  ythe \"standard\" understood tha the way to set standards was to \"just\n>  do what you wnat to do\" adn get it over with.\n>  \n\nBTW, this is a bit unfair and an overstatement...  The folks who designed \nHTTP have/had different goals than mail, and different expertise than networks. \nPart of the Web's success was that it allowed existing and future content \nto be transported (including MHTML). Demanding all text files be converted \nbefore being served by a web server would not likely have helped the Web \ntake off, for example, so it seems to me it was a perfectly pragmatic decision, \nin that case.\n\nAdopting the MIME type registry was clearly a win.  \n\nYou can question whether modelling the HTTP protocol after MIME was a good \nidea; I think history is showing it wasn't, that the accumulated baggage \nsince 822 is causing serious confusion.  Systems also age, and RFC822's \ndecendents are showing serious signs of senesence. \n\nBut things might have been even worse had they (TimBL, et. al. etc.) gone \noff completely on their own.  There are relatively few people who've designed \neven one protocol at all well. I know in my background it took several \nredesigns to design one (the X protocol) even partially right for long term \nstability (and I certainly know of many problems with that protocol).  Given \nthe speed with which the Web took off, it is unlikely Tim et. al. would \nhave had the luxury we did of fixing the worst mistakes we made in early\nversions of X.\n\nSo you're beating up on people with 20-20 hindsight.  Things might be much \nworse.  All in all, I think they did a pretty decent job for a first try.\nWhere were we when the Web should have been invented (arguably, all the\npieces to invent it have been around since around 1986).\n- Jim Gettys\n\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "tcodings and identit",
            "content": "jg@pa.dec.com (Jim Gettys) writes:\n\n>>    15) (14.48) The production for <t-codings> doesn't allow \"identity\", but\n>>        rule 3 seems to allow \"identity;q=0\".\n>>\n>Here's Henrik's clarification:\n>it is not that \"identity\" isn't allowed - the problem\n>is that chunked is special - due to backwards compatibility concerns.\n>\n>...\n\nI see where I got lost now.  The production for <t-codings> allows\n\"chunked\" or <transfer-extension>, but I missed the fact that in this\ncase, we use \"-extension\" to include choices defined in the spec.  In\nall other cases we use \"-extension\" for extensions beyond the current\nspec and define current values in the BNF (viz. <accept-params>,\n<cache-response-directive> et al.)  So while the BNF is technically\ncorrect, it deviates from normal usage in the rest of the spec.\n\nI can live with leaving <t-codings> as it is, but I wonder if others will\nalso miss the subtly different use of \"-extension\" in transfer-extension.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "> \n> I understand that we are dealing with a legacy of bad HTTP choices\n> back when there was no IETF involvement, adn the people developing\n> ythe \"standard\" understood tha the way to set standards was to \"just\n> do what you wnat to do\" adn get it over with.\n> \n> All this menas that it is just an accident of history and so we have\n> to just grin and bear it and live with all the bad fallout effects.\n\nA historical note....\n\nI'd like to say that _some_ of the differences between HTTP and\nMIME were not entirely accidents of history: they were also\njustified as being optimized for different transports.\n\nI think I started following the http working group list\nnot long before the question of tolernance for different\nkinds of end-of-line in text was being considered (or\nreconsidered.)\n\nAt that time, there _were_ existing browsers which implemented\ntolerance for different end of lines in various ways; so there\nwere legacy code issues. But there were also server authors\nwho wanted to optimize for speed: they wanted to be able to\njust throw the bytes of a text file out on the net without\nmaking the end-of-lines into some cannonical format.\n\nThe discussion was in December 1994 on http-wg list, much of it\nunder the Subject: \"Re: Comments on the HTTP/1.0 draft.\"\n\nSee for example,\nChuck Shotton's comments at:\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1994q4/0101.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1994q4/0119.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1994q4/0122.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1994q4/0127.html\n\nand other remarks in the same thread (I'm pointing out\nChuck mostly because I recall him as a server author who\nspoke up at the time, not because he's the only one\nwho said something.)\n\nSee also:\nSubject: \"Closure on canonicalization, I hope\"\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1994q4/0300.html\n\n(I admit to having said some questionable things in the same\nthreads because I was new to the process; but my personal involement\nmakes me remember it.)\n\nYou can view some of the other differences in a similar light\nof optimizing for different transport.\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "Thanks Jim -- I will accept your judgement that I am beating up on the\nwrong people with hindsight, and the implication tha doing so is\nunfair.  But, I was fishing for an understanding of how we got here\nfrom there and that is what I got.  Thank you very much;-)...\n\nThe problem of converting hindsight into foresight has always been\ndaunting, so merely clarifying hindsight, in and of itself, is not of\nmuch interest if we cannot convert it with some kind of meta process\ninto a way to visualize, in this case, the architecual meanings in the\nfuture.\n\nrom what I have seen here and elsewhere, it seems very clear that any\nnew protocol should never ever use the same names for anything that is\nnot infact the same thing, just in case it somehow gets into that\nalternate universe and is there mistaken for what it is not.\n\nI suspect that the ISO/OSI custom of incorporating the protocol names\nby abreviation into the protocol element labels was a method for\nachieving this precise goal, as it clearly prevnets the use of any\nnaming label from bearing the same name as some otherwise defined\nelement.\n\nIf this is all there is to be gleaned from the HTTP/RFC822/MIME\nescapade, so be it.  Perhaps it does not look like a very big lesson,\nbut if it can help prevent future accidents, it does not look too\nexpensive related to the possibly huge payoffs.\n\nCheers...\\Stef\n\nrom Jim Gettys's message Mon, 26 Jan 1998 14:35:11 -0800:\n}\n}>  From: Einar Stefferud <Stef@nma.com>\n}>  Date: Mon, 26 Jan 1998 13:13:40 -0800\n}>  Subject: Re: MHTML/HTTP 1.1 Conflicts \n}>  \n}>  I understand that we are dealing with a legacy of bad HTTP choices\n}>  back when there was no IETF involvement, adn the people developing\n}>  ythe \"standard\" understood tha the way to set standards was to \"just\n}>  do what you wnat to do\" adn get it over with.\n}>  \n}\n}BTW, this is a bit unfair and an overstatement...  The folks who designed \n}HTTP have/had different goals than mail, and different expertise than networks. \n}Part of the Web's success was that it allowed existing and future content \n}to be transported (including MHTML). Demanding all text files be converted \n}before being served by a web server would not likely have helped the Web \n}take off, for example, so it seems to me it was a perfectly pragmatic \n}decision, in that case.\n}\n}Adopting the MIME type registry was clearly a win.  \n}\n}You can question whether modelling the HTTP protocol after MIME was a good \n}idea; I think history is showing it wasn't, that the accumulated baggage \n}since 822 is causing serious confusion.  Systems also age, and RFC822's \n}decendents are showing serious signs of senesence. \n}\n}But things might have been even worse had they (TimBL, et. al. etc.) gone \n}off completely on their own.  There are relatively few people who've designed \n}even one protocol at all well. I know in my background it took several \n}redesigns to design one (the X protocol) even partially right for long term \n}stability (and I certainly know of many problems with that protocol).  Given \n}the speed with which the Web took off, it is unlikely Tim et. al. would \n}have had the luxury we did of fixing the worst mistakes we made in early\n}versions of X.\n}\n}So you're beating up on people with 20-20 hindsight.  Things might be much \n}worse.  All in all, I think they did a pretty decent job for a first try.\n}Where were we when the Web should have been invented (arguably, all the\n}pieces to invent it have been around since around 1986).\n}- Jim Gettys\n}\n}--\n}Jim Gettys\n}Industry Standards and Consortia\n}Digital Equipment Corporation\n}Visting Scientist, World Wide Web Consortium, M.I.T.\n}http://www.w3.org/People/Gettys/\n}jg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Pipelinin",
            "content": ">When a client is talking to a proxy server and is pipelining requests,\n>should it use a single pipeline connection to issue requests to \n>different origin servers ?\n>ie should GET http://www.ups.com/ HTTP/1.1\n>   and    GET http://www.fedex.com/ HTTP/1.1 \n>be sent over the same pipeline or should a new connection be established.\n>A common occurence of this is when advertisement gifs come from a \n>different server than the html file.\n>\n>I guess the question becomes :\n>Can requests in the same pipeline be to different origin servers?\n\nThat would depend on the nature of the proxy.  A \"personal proxy\" (which\nis essentially what a good user agent cache becomes) would probably\nuse multiple request threads (the equivalent of connections).  A small\ngroup proxy would probably maintain a continuous two connections to\nevery client in its domain, and thus whether or not the two are pipelined\nwould depend on the presence of other outstanding requests.  Likewise,\na national proxy would likely maintain feeder connections to reqional\nproxies.\n\nIn other words, HTTP is capable of doing either, and the best choice \nwill depend on the context in which the proxy is configured.  A good\nimplementation plan would be to make such decisions configurable by\nthe proxy maintainer.\n\n>With the advent of pipelined persistent connections ( and to a lesser\n>extend 1.0 keep alives ), the distinction of 'who the client is\n>talking to' is confusing to me.  Since while the client may be\n>pipelining to a proxy, and the proxy can go ahead and do an\n>old style connection to an origin server, how does the client deal\n>with old responses?\n>Assuming the answer to the previous question,  is yes...\n>\n>IE: client pipelines:\n>  GET http://www.foo.com/ HTTP/1.1\n>  GET http://www.bar.com/ HTTP/1.1\n>what happens if foo.com is a 1.1 server ( the proxy can do a\n>persistent conenction ) and bar.com is 1.0 (proxy cannot)?\n\nThe proxy is always sending HTTP/1.1 responses, even if the origin\nis an HTTP/1.0 server.  The proxy is capable of \"converting\" any HTTP/1.0\nresponse into a valid HTTP/1.1 response.  The only question in such\ncases is how much conversion should be done.  Adding \"Connection: close\"\nis the simplest, but it is generally more efficient (for the proxy)\nto do a full conversion (i.e., add chunked encoding if no content-length\nis present).\n\n>Also, when the responses come back to the client, the first is\n>a 1.1 response, and the second is a 1.0 response ..\n\nNot if the proxy is 1.1 compliant.  It would be a very, very bad idea\nfor the proxy to change its HTTP-version based on the origin of the\nresponse.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-rvsa-v1003.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: HTTP Remote Variant Selection Algorithm -- RVSA/1.0\nAuthor(s): K. Holtman, A. Mutz\nFilename: draft-ietf-http-rvsa-v10-03.txt\nPages: 13\nDate: 26-Jan-98\n\n        HTTP allows web site authors to put multiple versions of the\n        same information under a single URL.  Transparent content\n        negotiation is a mechanism for automatically selecting the\n        best version when the URL is accessed.  A remote variant\n        selection algorithm can be used to speed up the transparent\n        negotiation process. This document defines the remote variant\n        selection algorithm with the version number 1.0.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-rvsa-v10-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-rvsa-v10-03.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-rvsa-v10-03.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiation06.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: Transparent Content Negotiation in HTTP\nAuthor(s): K. Holtman, A. Mutz\nFilename: draft-ietf-http-negotiation-06.txt\nPages: 57\nDate: 26-Jan-98\n\n        HTTP allows web site authors to put multiple versions of the\n        same information under a single URL.  Transparent content\n        negotiation is an extensible negotiation mechanism, layered on\n        top of HTTP, for automatically selecting the best version when\n        the URL is accessed.  This enables the smooth deployment of\n        new web data formats and markup tags.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-negotiation-06.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiation-06.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-negotiation-06.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-state-man-mec06.txt,.p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-06.txt,.ps\nPages: 19\nDate: 26-Jan-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-06.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-man-mec-06.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-06.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "New transparent content negotiation draft",
            "content": "As usual, I have made html and changebar versions of the new drafts\ndraft-ietf-http-negotiation-06.txt and draft-ietf-http-rvsa-v10-03.txt\navailable at\n\n  http://gewis.win.tue.nl/~koen/conneg/\n\nThese are supposed to be the \"camera-ready\" versions which will be\nfrozen as experimental RFCs.  \n\nThe only protocol-level change in the drafts is that the description\nfield in the Alternates header now has an optional language tag as\nrequired by the new IETF i18n policies\n(draft-alvestrand-charset-policy-02.txt).  I have also added a note\nmentioning other negotiation efforts going on in the IETF.  For the\nrest, there are a lot of little layout changes to conform to the RFC\nformatting rules.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Test Day 1998-012",
            "content": "  Next test day is Jan 29th.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "Why did the HTTP group decide to remove Content-Base from the HTTP\nspecification? Does this mean that the sender must use the\nbase element inside the HTML text, in all cases where relative\nURLs are not matched by the base taken from the request URI.\n\nIf the HTTP group has decided to remove Content-Base, are the\nsame reasons valid for removing Content-Base from MHTML? The\ndifference is that when you send by e-mail, you have no request\nURI, so this might be a reason why Content-Base is more important\nin e-mail than in HTTP.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "At 09.18 -0800 98-01-26, Jim Gettys wrote:\n> The reality of the web is that there is tons of content without any line\n> length enforced already exisiting...   It just isn't possible to get the\n> hundreds of millions of documents reformatted at this date.  Think of\n>generic\n> HTML on the Web as just another binary data type rather than as text, and\n> you'll see how to deal with it.  To forward this HTML through email safely,\n> you are certainly have to encode it before transmission. If the bags of\n> bits get messed with, the security checksums will clearly fail.\n\nCertainly you cannot ask people to reformat existing documents.\nBut a company which buys a WYSIWYG HTML editor like Frontpage\nor Pagemill might require that they produce \"canonical\" HTML\nwhich agrees with both e-mail and HTTP rules. This would allow\ncompany employees to, for example, take pages from the local\nIntranet and send them by mail outside of this Intranet,\nwithout having digital seals and signatures broken.\n>\n> What this means, is that there are two possible cases:\n> 1) the HTML to be mailed could be reformatted to email rules, if no\n> security measures have been applied...\n> 2) the HTML gets encoded and transmitted, and treated as a bag\n> of bits, if it has any security wrappers.\n\nWhat kind of encoding are you thinking of? E-mail only has Content-\nTransfer-Encoding, and this can in e-mail only be applied to individual\nbody parts, not to a whole multi-part as one single object to be encoded.\nEncoding of a whole multipart as a single large object using\nBase64 och Quoted-Printable is explicitly forbidden in e-mail.\n\nI am no expert on security issues and in particular how the security\nchecksums are computed. If security checksums are not to be broken,\nthen either the original document must be in canonical form, or\nall content-transfer-encoding must be removed at receipt *before*\ncomputing a security checksum. In the second case, the algorithm\nfor applying content-transfer-encoding (before sending by e-mail)\nand removing it again (after receipt by e-mail) must be so exact\nthat the identical octet sequence is returned at receipt.\n\n(1) Are the algorithms for encoding and deconding with content-\ntransfer-encoding defined exactly enough to ensure this?\n\n(2) Do the security standards say that content-transfer-encoding\nis to be removed before computing security checksums?\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "On Tue, 27 Jan 1998, Jacob Palme wrote:\n\n> Why did the HTTP group decide to remove Content-Base from the HTTP\n> specification? Does this mean that the sender must use the\n> base element inside the HTML text, in all cases where relative\n> URLs are not matched by the base taken from the request URI.\n\nRFC 2068 did not use the normative MUST in the description of the header,\nleading some implementors to decide that it was optional.  From the\nperspective of the server it is not usefull unless the client is\nguaranteed to use it (if you have to also put the base into the entity\nthen there is no point in putting it into the headers).\n\n> If the HTTP group has decided to remove Content-Base, are the \n> same reasons valid for removing Content-Base from MHTML? \n\nThe discussion on the http list began with:\n\n http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0024.html\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "On Tue, 27 Jan 1998, Jacob Palme wrote:\n\n> Certainly you cannot ask people to reformat existing documents.\n> But a company which buys a WYSIWYG HTML editor like Frontpage\n> or Pagemill might require that they produce \"canonical\" HTML\n> which agrees with both e-mail and HTTP rules. This would allow\n> company employees to, for example, take pages from the local\n> Intranet and send them by mail outside of this Intranet,\n> without having digital seals and signatures broken.\n\nMuch of the discussion on this topic in this forum seems to be based\non the mistaken assumption that the HTTP specification has something\nto say about the format of an HTML document.\n\nIn fact the HTTP spec *cannot* create or remove any restrictions on\nHTML format.  There is another working group dealing with that.\nWhether HTML has line length restrictions or EOL requirements is none\nof our business as an HTTP working group.  It is also the case that\nHTTP cannot enforce restrictions on HTML adopted elsewhere.  Just as\nit cannot enforce the correct format of JPEG or PDF.\n\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "Jacob Palme wrote:\n\n> Certainly you cannot ask people to reformat existing documents.\n> But a company which buys a WYSIWYG HTML editor like Frontpage\n> or Pagemill might require that they produce \"canonical\" HTML\n> which agrees with both e-mail and HTTP rules. This would allow\n> company employees to, for example, take pages from the local\n> Intranet and send them by mail outside of this Intranet,\n> without having digital seals and signatures broken.\n\nI know of no Unix text editor that produces text files that\nuse the \"canonical\" format for text/plain using CRLF instead\nof just plain LF, and I've never seen a Macintosh text editor\nthat produces text files using the \"canonical\" format with\nCRLF instead of just plain CR. An editor produces files that\nare appropriate for the native environment, and people with\nInternet mail who mail documents using SMTP expect the mailer\nto convert the file from the local convention to the canonical\none on transmission, and to do the inverse conversion when\nsaving the document.\n\nThe HTTP specification says that at least for the case of converting\nend-of-line, such conversion is not necessary. The canonical form is\nstill CRLF, but conversion is not necessary.\n\nSecurity checksums _should_ be applied to the canonical form,\nbut in truth, it is feasible to apply it to the transmitted form,\nand it is the choice of the originator whether to do this. Since\nan HTTP server should never actually modify the end-of-line convention,\nthe data as represented by the choice of the original sender can\nremain intact. That is, this is not really an issue.\n\nWe've been through this topic endlessly in the past, and wrangled\nout some specific wording in the HTTP draft that was believed generally\nto be adequate to explain the situation, at least as far as\nthe 'end of line convention' discussion in the HTTP/1.1 specification\nwas concerned.\n\nIf there was some demand, a separate document on \"HTTP/mail gateway\nimplementation considerations\" could elaborate on the topic, but the\ndemand for that seems to be low. (Perhaps IMAP/WebDAV interoperability\nmight be an interesting application (isn't a mail folder a 'collection'?)).\n\n\nI believe there's a need for addressing the 'MIME vs MIME-like'\nissue, as far as the applicability of future specifications that\nmight revise MIME and their applicability to HTTP. For example,\nif there's a revision of 'content-disposition', does it apply\nto HTTP too? I don't think that would be reasonable if the spec\nweren't reviewed by the HTTP community, but on the other hand,\nkeeping HTTP and the rest of MIME in sync is highly desirable.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "Regarding the 100 response, Scott Lawrence writes:\n\n>  The only apparent purpose we could find in the RFC for the 100\n>  response was in a discussion of client retry behaviour when the\n>  connection closed before the client had finished sending the body of\n>  a POST:\n\nI think this can be blamed on reducing the HTTP/1.1 description to\nthose elements we already knew would be needed.  The general class of\n1xx responses was intended to carry asynchronous information from\nthe server to the client.  I reintroduced them after talking to TimBL\nwhen I was at the W3C, but somewhere in the year between that conversation\nand the final call we overly contrained the definition of 1xx.\nI will try to formulate a new description, along with an update to\nthe \"treat unrecognized responses as the x00 response of that class\"\nrule which is insufficient to describe what was intended.\n\n>  We would actually prefer to see this set of rules made more general\n>  in that we'd like it to apply to any POST, not just one being\n>  retried (which may or may not have been what was intended).\n\nThe 100 response must be sent by an HTTP/1.1 server upon receipt of\nany HTTP/1.1 request containing a message body, after it receives the\nheader fields and determines that it wishes to receive that body.  The\nRFC 2068 says this in section 8.2, but in a rather confusing way.\n\n>  As to [Yaron's] suggestion above, I don't really see the point (we\n>  certainly won't be sending all those extra responses), but since it\n>  specifies only client behaviour we don't care much.\n\nThe client should not care either, since it should be ignoring any\n1xx class of response.  A client that is not looking for such a response\nwill simply see it (and ignore it) upon its next request, or never see\nit at all if it just drops the connection.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">If the HTTP group has decided to remove Content-Base, are the\n>same reasons valid for removing Content-Base from MHTML? The\n>difference is that when you send by e-mail, you have no request\n>URI, so this might be a reason why Content-Base is more important\n>in e-mail than in HTTP.\n\nYes, the same reasons are valid for MHTML.  In fact, they came out\nof our earlier discussion regarding the confusion between Content-Location\nand Content-Base within a hierarchical context.\n\nThe reasoning is that the only principal who is capable of knowing\nwhether or not the embedded relative URLs of a document are relative\nto a base URL that is different from the document location (the only\nreason we have Content-Base) is the person/process that created that\ndocument.  Since it is reasonable to assume that any media type\ncapable of using relative URLs in that manner will also have its\nown means of assigning a base URL (e.g., the BASE element in HTML),\nwe don't need a special field for that purpose at higher levels in\nthe document hierarchy.  Furthermore, since an embedded BASE is known\nto be more reliable for this purpose than any other mechanism, we\ndon't need Content-Base for the single-document case either.  In addition,\nit greatly simplifies the problem of \"how to get the Base URL\", since\nwe remove any ambiguity between Content-Base and Content-Location\nprecedence.\n\nSince it is unlikely that anyone is currently sending MHTML documents\nwith Content-Base (they only need Content-Location), I think\nthat MHTML can safely drop Content-Base at this point.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": ">I am fast losing confidence that we can ever resolve our MHTML/HTTP\n>interworking problems, as long as the IETF allows HTTP to claim to\n>only be MIME-like, while using MIME headers, but with differences from\n>the MIME standard?  \n>\n>Without the surrounding HTTP wrapper, how are we supposed to know\n>which kind of MIME object we are dealing with?  Are we supposed to\n>sniff it to see if there is any trace of HTTP smell to it?\n\nStef, I have explained this before.  HTTP is not MIME compliant.\nThis was not an accident, or a \"misunderstanding\" of what the IETF\nallows -- it was a deliberate and well-considered design decision\nin order to maximize the advantages of shared protocol mechanisms\nand registries without sacrificing the distinct performance and\nadaptability considerations of a directly-connected transfer protocol.\nWe made that decision not just because it was the way almost all\nHTTP servers were implemented, but because it is the right way to\nimplement an HTTP service.  Most of the work put into RFC 1945 was\nto make HTTP/1.x as MIME-friendly as possible, but it is unreasonable\nto force bad design decisions onto HTTP just because they were\nnecessary design decisions for E-mail.\n\nAll of the questions you have asked are answered in section 19.4 of\nRFC 2068.  In particular, the presence of a MIME-Version header field\nindicates that the message content is in complete compliance with the\nMIME protocol.  Likewise, the absence of a MIME-Version header field\nindicates that a gateway from HTTP to any MIME-compliant protocol\n(including e-mail) must provide the necessary conversion.  That solves\nall of the interworking problems between HTTP and MIME, and does so\nat the most appropriate access point.\n\nMeanwhile, please keep in mind that both Content-Base and Content-Location\nwere originally designed for HTTP, before the MHTML working group began.\nI believe they are the best possible mechanism for doing what MHTML\nwants to do, and further that all of MHTML's decisions prior to the\nnotion of allowing multiple Content-Location header fields (now gone)\nhave matched those made by HTTP.\n\nI have yet to see any case where the MHTML specifications and HTTP\nspecifications collide.  I have seen cases where MHTML has deliberately\nspecified limitations in the transfer syntax in order to satisfy\nnonexistent gateway limitations in e-mail, which I believe to be a poor\ndesign.  If MHTML is to be truly independent of differences between\nprotocols, then it should only require line-length and encoding\nwhen they are necessary for safe transfer.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "This will be really cool, if we can all agree to it;)...\\Stef\n\nAt 14:18 28/1/98 -0800, Roy T. Fielding wrote:\n>\n>Since it is unlikely that anyone is currently sending MHTML documents\n>with Content-Base (they only need Content-Location), I think\n>that MHTML can safely drop Content-Base at this point.\n>\n>.....Roy\n\n\n\n"
        },
        {
            "subject": "Issue: message/http or application/htt",
            "content": ">Certainly, if a message/http body part is transported through e-mail,\n>it must follow e-mail rules for heading lines, line folding, line\n>breaks, etc. So possible chapter 19.1 should have added to it (to\n>avoid confusion) that if a message/http body part is transported\n>through HTTP (in the trace) then its use of line length, line folding\n>and line breaks can follow HTTP conventions, but if a message/http\n>body part is transported through e-mail, then it must adhere to\n>e-mail rules in this respect.\n\nI've always found this one of the most annoying things about how\nMIME was specified. If all message types must obey the same rules\nas an RFC 822 message, then why would you ever need more than\nmessage/rfc822?\n\nHTTP does have the ability to send proper MIME line folding, line breaks,\netc.  However, for various other reasons it might be better to use\napplication/http instead, particularly when the entire message needs\nto be content-transfer-encoded in order to pass through e-mail.\nPerhaps we should change the media type, or simply define both?\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "Thanks Roy for punctuating our discussons with very clear marks.\n\nUntil we have a very clear conclsion that MHTML and HTTP do not collide, it\ncontinued to make me very nervous about havig to recycles again at\nProposed.\nnone of us want that to happen, and none of us want to later be found to\nhave ignored any collision that should hav been found at this stage.\n\nPer your suggestions that MHTML might be further simplified, I suggest that\nour editing team lok very carfully at your suggestions to see if we can use\nthem as you predict.\n\nCheers...\\Stef\n\nAt 14:52 28/1/98 -0800, Roy T. Fielding wrote:\n>>I am fast losing confidence that we can ever resolve our MHTML/HTTP\n>>interworking problems, as long as the IETF allows HTTP to claim to\n>>only be MIME-like, while using MIME headers, but with differences from\n>>the MIME standard?  \n>>\n>>Without the surrounding HTTP wrapper, how are we supposed to know\n>>which kind of MIME object we are dealing with?  Are we supposed to\n>>sniff it to see if there is any trace of HTTP smell to it?\n>\n>Stef, I have explained this before.  HTTP is not MIME compliant.\n>This was not an accident, or a \"misunderstanding\" of what the IETF\n>allows -- it was a deliberate and well-considered design decision\n>in order to maximize the advantages of shared protocol mechanisms\n>and registries without sacrificing the distinct performance and\n>adaptability considerations of a directly-connected transfer protocol.\n>We made that decision not just because it was the way almost all\n>HTTP servers were implemented, but because it is the right way to\n>implement an HTTP service.  Most of the work put into RFC 1945 was\n>to make HTTP/1.x as MIME-friendly as possible, but it is unreasonable\n>to force bad design decisions onto HTTP just because they were\n>necessary design decisions for E-mail.\n>\n>All of the questions you have asked are answered in section 19.4 of\n>RFC 2068.  In particular, the presence of a MIME-Version header field\n>indicates that the message content is in complete compliance with the\n>MIME protocol.  Likewise, the absence of a MIME-Version header field\n>indicates that a gateway from HTTP to any MIME-compliant protocol\n>(including e-mail) must provide the necessary conversion.  That solves\n>all of the interworking problems between HTTP and MIME, and does so\n>at the most appropriate access point.\n>\n>Meanwhile, please keep in mind that both Content-Base and Content-Location\n>were originally designed for HTTP, before the MHTML working group began.\n>I believe they are the best possible mechanism for doing what MHTML\n>wants to do, and further that all of MHTML's decisions prior to the\n>notion of allowing multiple Content-Location header fields (now gone)\n>have matched those made by HTTP.\n>\n>I have yet to see any case where the MHTML specifications and HTTP\n>specifications collide.  I have seen cases where MHTML has deliberately\n>specified limitations in the transfer syntax in order to satisfy\n>nonexistent gateway limitations in e-mail, which I believe to be a poor\n>design.  If MHTML is to be truly independent of differences between\n>protocols, then it should only require line-length and encoding\n>when they are necessary for safe transfer.\n>\n>.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "> I have yet to see any case where the MHTML specifications and HTTP\n> specifications collide.  I have seen cases where MHTML has deliberately\n> specified limitations in the transfer syntax in order to satisfy\n> nonexistent gateway limitations in e-mail, which I believe to be a poor\n> design.  If MHTML is to be truly independent of differences between\n> protocols, then it should only require line-length and encoding\n> when they are necessary for safe transfer.\n\nI agree with Roy about this. Line length and encoding are necessary because of\ntransport limitations, they are not MIME limitations per se. As such, these\nlimitations should not be built into MHTML unconditionally, since MIME\ntransports exist that don't have these limitations.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: Issue: message/http or application/htt",
            "content": "> I've always found this one of the most annoying things about how\n> MIME was specified. If all message types must obey the same rules\n> as an RFC 822 message, then why would you ever need more than\n> message/rfc822?\n\nI agree that it is annoying. However, the minute you try and lift these\nrules you get into the area of nested encoding. Simply put, we could\nnot reach consensus on a standard that allowed for this possibility.\n\nEven today my assessment of the present situation is that there is no chance of\nlifting these restrictions.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: MHTML/HTTP 1.1 Conflict",
            "content": "At 07.40 -0800 98-01-28, Larry Masinter wrote:\n> I know of no Unix text editor that produces text files that\n> use the \"canonical\" format for text/plain using CRLF instead\n> of just plain LF, and I've never seen a Macintosh text editor\n> that produces text files using the \"canonical\" format with\n> CRLF instead of just plain CR. An editor produces files that\n> are appropriate for the native environment, and people with\n> Internet mail who mail documents using SMTP expect the mailer\n> to convert the file from the local convention to the canonical\n> one on transmission, and to do the inverse conversion when\n> saving the document.\n\nI believe BBEDIT, a popular Macintosh text editor with special\nfeatures for (not WYSIWYG) HTML, has some support for this.\nMany HTML editors have built-in features for updating the\nmaster document on a HTTP server. I believe they usually\nfollow the FTP TEXT convention of converting line breaks to\nthe convention used on the server platform.\n\nI am no expert on security features. Is it true that these\nalways convert line breaks to CRLF format before computing\nthe checksums? If that is the case, the problem becomes\nless important. Is it also true that security features\ndecode Content-Transfer-Encoding before computing their\nchecksums. That would further reduce the problem of HTML\ndocuments in formats unsuitable for mail transmission without\nContent-Transfer-Encoding.\n\n> Security checksums _should_ be applied to the canonical form,\n> but in truth, it is feasible to apply it to the transmitted form,\n> and it is the choice of the originator whether to do this. Since\n> an HTTP server should never actually modify the end-of-line convention,\n> the data as represented by the choice of the original sender can\n> remain intact. That is, this is not really an issue.\n\nChoice of the originator or choice of the security algorithm?\nDo you mean that the security algorithms allows a sealed\ndocument to contain information on which conversions and\nuncodings should be done on a document before computing\nchecksums? Or do you mean that different security schemes\nhandle this in different ways?\n\n> We've been through this topic endlessly in the past, and wrangled\n> out some specific wording in the HTTP draft that was believed generally\n> to be adequate to explain the situation, at least as far as\n> the 'end of line convention' discussion in the HTTP/1.1 specification\n> was concerned.\n\nI am sorry if I am taking time to discuss an issue which you have\nalready resolved. I agree that HTTP servers should not change the\ndocuments before sending them. This means that the documents stored\nin the HTTP server should be in a form which allows gatewaying\nto e-mail without loss of security checksums.\n\n> If there was some demand, a separate document on \"HTTP/mail gateway\n> implementation considerations\" could elaborate on the topic, but the\n> demand for that seems to be low. (Perhaps IMAP/WebDAV interoperability\n> might be an interesting application (isn't a mail folder a 'collection'?)).\n\nIn the MHTML group, we have carefully tried to make a standard which\nwould break security checksums as little as possible. We believed\nthis was a user requirement; when you say \"demand for that seems to\nbe low\", do you mean that this is not, and will not, be an important\nuser requirement?\n\nWe already have som text about this in the MHTML document. Below\nis a quote of what we have written:\n\n   -  The MIME standard [MIME2] requires that e-mailed documents of\n      \"Content-Type: Text/ MUST be in canonical form before a\n      Content-Transfer-Encoding is applied, i.e. that line breaks are\n      encoded as CRLFs, not as bare CRs or bare LFs or something else.\n      This is in contrast to [HTTP] where section 3.6.1 allows other\n      representations of line breaks.\n\n   Note that this might cause problems with integrity checks based on\n   checksums, which might not be preserved when moving a document from\n   the HTTP to the MIME environment. If a document has to be converted\n   in such a way that a checksum based message integrity check becomes\n   invalid, then this integrity check header SHOULD be removed from the\n   document.\n\n   Other sources of problems are Content-Encoding used in HTTP but not\n   allowed in MIME, and character sets that are not able to represent\n   line breaks as CRLF. A good overview of the differences between\n   HTTP and MIME with regards to Content-Type: \"text\" can be found\n   in [HTTP], appendix C.\n\n   If the original document has line breaks in the canonical form\n   (CRLF), then the document SHOULD remain unconverted so that\n   integrity check sums are not invalidated.\n\n   A provider of HTML documents SHOULD always provide original documents\n   in the canonical form with CRLF for line breaks, so that they will be\n   transferable via both HTTP and SMTP without invalidating checksum\n   integrity checks.\n\nSo rather than writing a new document on this, MHTML could be used\nor extended to cover this issue. The HTTP spec could just mention\nthat MHTML contains some discussion on how to create documents which\ncan be sent through both e-mail and HTTP without breaking security\nchecksums.\n\nMIME recommends using Content-Transfer-Encoding to break lines longer\nthan 76 characters when transporting documents through e-mail.\nThis might also break security checksums, unless the security\nchecksum algorithms are defined in ways which will not be broken\nby Content-Transfer-Encoding.\n\nIs it permitted to use e-mail-type Content-Transfer-Encoding\non documents sent through HTTP? From what you say, I believe it\nmust be permitted if the Content-Transfer-Encoding header is inside\nthe body, since HTTP only handles the outermost (HTTP) header.\nBut is a Content-Transfer-Encoding header field allowed in the\noutermost (HTTP) heading?\n\n\n\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 : Chunkin",
            "content": "Hi all, sorry if I am not following correct protocol in mailing to this\ngroup.  First a bit of background - Our company develops WinGate, a fairly\npopular proxy server for NT/95, and so we have a lot of vested interest in\nthe HTTP/1.1 protocol.\n\nReading through the proposed standard again today, and some of the issues\nraised on http://www.w3.org/Protocols/HTTP/Issues/Overview.html I would just\nlike to raise a small suggestion with regard to chunking.  I have another\nwith regard to real-time streamed media sent through HTTP, but depending on\nthe response to this, that may form the entity body of another posting.\n\nTo my understanding, chunking was included in HTTP/1.1 for one reason only -\nthat being where the server could not know beforehand, the size of the\nentity it was sending, most likely because it was being created by some\nother process (e.g CGI etc).  So, Chunking was added to allow the client to\nKNOW when it had received the entire entity.\n\nHowever, this adds a couple of problems/questions:\n\n1. The chunk size may be arbitrary, and the resulting latency introduced at\nthe server (since chunking at the server requires some form of buffering)\nmay cause performance degradation in applications such as streamed media\nthrough HTTP.  Is there any recommendation on how to chunk the data?\n\n2. Any intermediary (proxy) must continually monitor the transfer through\nitself. Is it allowed to re-chunk the data, and if so under what guidelines.\n\nSo, I have a proposal to achieve the same desired objective as chunking.\n\nProposal:\n\nAddition of a new option to specify that transmission of the entity is\nterminated by a certain sequence of octets (like an end tag) transmitted by\nthe server before closing the TCP connection.  The receiver would then know\nif had received the whole entity by examining the last packet received\nbefore the connection was closed.  If the end tag was there, then it got the\nlot, else it didn't - simple. \n\nThis method provides no less information to the client than the current\nchunking method, since there is no progress information in chunking anyway\n(one does not know how many chunks are coming through), but is MUCH less\nintrusive into the entity body data, and would therefore be much more\nrobust.  A proxy for instance could simply pass the data as it received it,\nand if talking to an HTTP/1.0 client would simply strip out the end tag.  \n\nThis would involve holding back the last n bytes (length of end tag) from\nevery packet relayed through itself until it received the next packet, which\nit would hold back the last n bytes of.  When notified of a close, if would\nlook at those n bytes, and if they were the tag, it would discard them, and\nclose the client connection, else it would send them to the client and let\nthe client deal with the problem of not having received all the data.  \n\nFor an HTTP/1.1 client, it would simply pass everything blindly.  If the\nentity were cachable, it could be written directly to file as received, and\nthe end tag removed at the end of the session.  Even if the end tag were\nsent through to an HTTP/1.0 client, chances are it would not cause problems,\nunlike the current chunking system.\n\nA proposal for the end tag could be say a 4 byte magic number. So there\nwould be a 1 in 4 billion chance of there being a natural conflict caused by\na premature close on the connection with the last 4 bytes of the last packet\nbeing the magic number.  If this were thought too risky, simply increasing\nthe length would serve to render this event less likely. \n\nI believe this method would simplify client, server and intermediary\nimplementations, achieving the same objective, and thereby increase software\nreliability, reducing the opportunities for bugs.\n\nRegards\n\nAdrien de Croy\n\n\n\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": ">Section 8.2 of RFC 2068 has the following requirement:\n>\n>o  An HTTP/1.1 (or later) client MUST be prepared to accept a 100\n>   (Continue) status followed by a regular response.\n>\n>I have gone around asking people if the following behavior is legal:\n>\n>User Establishes Connection\n>\n>Server sends first 1xx Response\n>\n>User sends GET\n\nJust a clarification: this would be assuming that the server knows\nthat the client supports HTTP/1.1 before it makes the first request\n(as would be the case for some non-typical-web services, which is what\nI think Yaron is envisioning).\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "On Thu, 29 Jan 1998, Adrien de Croy wrote:\n> \n> 1. The chunk size may be arbitrary, and the resulting latency introduced at\n> the server (since chunking at the server requires some form of buffering)\n> may cause performance degradation in applications such as streamed media\n> through HTTP.  Is there any recommendation on how to chunk the data?\n> \n\nThe server may chunk anyway it chooses.  It can choose to do so in a\nway it finds most efficient.\n\n> 2. Any intermediary (proxy) must continually monitor the transfer through\n> itself. Is it allowed to re-chunk the data, and if so under what guidelines.\n> \n\nIt is not only allowed, but expected to be the most common practice\nfor caching proxies.  The proxy can re-chunk anyway it chooses.\n\n> \n> Proposal:\n> \n> Addition of a new option to specify that transmission of the entity is\n> terminated by a certain sequence of octets (like an end tag) transmitted by\n> the server before closing the TCP connection.  The receiver would then know\n> if had received the whole entity by examining the last packet received\n> before the connection was closed.  If the end tag was there, then it got the\n> lot, else it didn't - simple. \n> \n\nAs you said, the point of chunking is so the receiver can know when\nthe entity ends.  But the point of knowing when the entity ends is not\nonly to recognize if the connection closed prematurely.  The main\npoint is to allow multiple HTTP transactions to occur in one TCP/IP\nconnection.  This is a big performance win.  This means, though, that\nthe entity does not end with the last packet and, in fact, entities\nwill typically end in the middle of a packet.  This makes it less\nefficient (among other problems) to use an end marker.  The receiver\nwould have to read all the incoming data and look for the marker.\nChunking is both more robust and more efficient.\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Adrien de Croy wrote:\n> A proposal for the end tag could be say a 4 byte magic number. So there\n> would be a 1 in 4 billion chance of there being a natural conflict caused by\n> a premature close on the connection with the last 4 bytes of the last packet\n> being the magic number.  If this were thought too risky, simply increasing\n> the length would serve to render this event less likely.\n\nThat's a pretty dodgy bit of maths. What you mean is that for any\nparticular 4-byte sequence there's a 1 in 4 billion chance, so in 1 MB\nof data, the probability is (roughly) 1 in 4000. Or, over a 64 kb/s link\nat full capacity, about one mistake a week.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Adrien de Croy <adrien@qbik.com> writes:\n\n>Hi all, sorry if I am not following correct protocol in mailing to this\n>group.\n\nNope, that's exactly the protocol.\n\n>Proposal:\n>\n>Addition of a new option to specify that transmission of the entity is\n>terminated by a certain sequence of octets (like an end tag) transmitted by\n>the server before closing the TCP connection.\n\nAs John Franks has pointed out, chunked encoding is intended to allow\na TCP connection to be used for several HTTP transactions in sequence\n(described in the spec as a \"persistent connection\").  The major goal was\nto cope with content of initially-unknown length, which is handled just\nfine in HTTP 1.0 by closing the TCP connection after the last byte of\ndata.  That still works in HTTP 1.1, but only for the last entity sent on\nthe connection (of course).  So persistent connections require that every\nentity sent have a determinable length, even if only upon completion of\nthe entity.  Chunked encoding does exactly that.\n\n>                                               The receiver would then know\n>if had received the whole entity by examining the last packet received\n>before the connection was closed.\n\nUnfortunately, TCP connections are byte streams, not packet streams.\nWhile it would be possible to check the last N bytes of the last entity\non a connection for some tag, it would not be possible on any prior\nentities.  So this new type of encoding wouldn't help persistent\nconnections, which are believed to be an important part of HTTP 1.1.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Issue: message/http or application/htt",
            "content": "At 15.05 -0800 98-01-28, Roy T. Fielding wrote:\n> HTTP does have the ability to send proper MIME line folding, line breaks,\n> etc.  However, for various other reasons it might be better to use\n> application/http instead, particularly when the entire message needs\n> to be content-transfer-encoded in order to pass through e-mail.\n> Perhaps we should change the media type, or simply define both?\n\nE-mail does not permit content-transfer-encoding of a multipart,\nonly of individual parts, I believe.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "On Thu, 29 Jan 1998, John Franks wrote:\n\n> On Thu, 29 Jan 1998, Adrien de Croy wrote:\n> > \n> > 1. The chunk size may be arbitrary, and the resulting latency introduced at\n> > the server (since chunking at the server requires some form of buffering)\n> > may cause performance degradation in applications such as streamed media\n> > through HTTP.  Is there any recommendation on how to chunk the data?\n> > \n> \n> The server may chunk anyway it chooses.  It can choose to do so in a\n> way it finds most efficient.\n> \n> > 2. Any intermediary (proxy) must continually monitor the transfer through\n> > itself. Is it allowed to re-chunk the data, and if so under what guidelines.\n> > \n> \n> It is not only allowed, but expected to be the most common practice\n> for caching proxies.  The proxy can re-chunk anyway it chooses.\n\nAnd required to be removed when forwarding the response to an HTTP/1.0\nclient.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Hi\n\nAt 18:21 29/01/98 +0000, Ben Laurie wrote:\n>Adrien de Croy wrote:\n>> A proposal for the end tag could be say a 4 byte magic number. So there\n>> would be a 1 in 4 billion chance of there being a natural conflict caused by\n>> a premature close on the connection with the last 4 bytes of the last packet\n>> being the magic number.  If this were thought too risky, simply increasing\n>> the length would serve to render this event less likely.\n>\n>That's a pretty dodgy bit of maths. What you mean is that for any\n>particular 4-byte sequence there's a 1 in 4 billion chance, so in 1 MB\n>of data, the probability is (roughly) 1 in 4000. Or, over a 64 kb/s link\n>at full capacity, about one mistake a week.\n>\n\nI would have thought that in a random data stream, the chances of any\nparticular 4 byte sequence would have been 1 in 4 Billion, but we are only\nlooking at the last 4 bytes before a close.  So, given there is only one\nclose per session, there is only one inspected 4 byte sequence, hence the\nchance of any one file causing confusion over an OEF marker is 1 in 4 billion.\n\nHowever, as pointed out by John Franks, this method would not, and cannot\nallow for multiple transactions per connection, since it relies on the close\nevent being a signal as well.\n\nHowever, reflecting more on that issue, the chances of a client requiring\nmultiple created entitities (i.e those where the server cannot know a priori\nthe size) in a single connection is rather low, at least at the moment.\nMultiple normal requests per connection would still be possible, and\nunaffected by this proposal.  \n\n\n>Cheers,\n>\n>Ben.\n>\n>-- \n>Ben Laurie            |Phone: +44 (181) 735 0686|Apache Group member\n>Freelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\n>and Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\n>A.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\n>London, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Hi\n\nAt 11:53 29/01/98 -0600, John Franks wrote:\n>On Thu, 29 Jan 1998, Adrien de Croy wrote:\n>> \n>> 1. The chunk size may be arbitrary, and the resulting latency introduced at\n>> the server (since chunking at the server requires some form of buffering)\n>> may cause performance degradation in applications such as streamed media\n>> through HTTP.  Is there any recommendation on how to chunk the data?\n>> \n>\n>The server may chunk anyway it chooses.  It can choose to do so in a\n>way it finds most efficient.\n>\n>> 2. Any intermediary (proxy) must continually monitor the transfer through\n>> itself. Is it allowed to re-chunk the data, and if so under what guidelines.\n>> \n>\n>It is not only allowed, but expected to be the most common practice\n>for caching proxies.  The proxy can re-chunk anyway it chooses.\n>\n>> \n>> Proposal:\n>> \n>> Addition of a new option to specify that transmission of the entity is\n>> terminated by a certain sequence of octets (like an end tag) transmitted by\n>> the server before closing the TCP connection.  The receiver would then know\n>> if had received the whole entity by examining the last packet received\n>> before the connection was closed.  If the end tag was there, then it got the\n>> lot, else it didn't - simple. \n>> \n>\n>As you said, the point of chunking is so the receiver can know when\n>the entity ends.  But the point of knowing when the entity ends is not\n>only to recognize if the connection closed prematurely.  The main\n>point is to allow multiple HTTP transactions to occur in one TCP/IP\n>connection.  This is a big performance win.  This means, though, that\n>the entity does not end with the last packet and, in fact, entities\n>will typically end in the middle of a packet.  This makes it less\n>efficient (among other problems) to use an end marker.  The receiver\n>would have to read all the incoming data and look for the marker.\n>Chunking is both more robust and more efficient.\n>\n\nI see your point here.\n\nHowever, reflecting more on that issue, the chances of a client requiring\nmultiple created entitities (i.e those where the server cannot know a priori\nthe size) in a single connection is rather low, at least at the moment.\nMultiple normal requests per connection would still be possible, and\nunaffected by this proposal.  So, overall, the performance gains by allowing\nfor maintained connections in this scenario may be outweighed by the data\noverhead in chunking.\n\n\nCheers\n\nAdrien\n\n\n>John Franks\n>john@math.nwu.edu\n>\n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "On Fri, 30 Jan 1998, Adrien de Croy wrote:\n\n> \n> However, reflecting more on that issue, the chances of a client requiring\n> multiple created entitities (i.e those where the server cannot know a priori\n> the size) in a single connection is rather low, at least at the moment.\n> Multiple normal requests per connection would still be possible, and\n> unaffected by this proposal.  So, overall, the performance gains by allowing\n> for maintained connections in this scenario may be outweighed by the data\n> overhead in chunking.\n> \n\nConnections with transfers of multiple entities of unknown size will be\nvery common with HTTP/1.1.  They would be very common today if there\nwere widely deployed HTTP/1.1 clients.  Remember, any document with\nserver side includes (e.g. a counter) can be considered of unknown size.\nIt is much easier to chunk than to calculate the length before any\ndata is sent so it can be put in a Content-length header.\n\nChunking has very low overhead.  From the point of view of efficiency\nthere would be no problem for clients or servers if ALL transactions\nwere required to be chunked.  There is not much performance difference\nbetween\n\n  Content-length: 123456\n      <123456 bytes of data>\n\nand \n\n  Content-encoding: chunked\n\n  123456\n      <123456 bytes of data>\n  0\n\nJohn Franks\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "John Franks writes:\n    Chunking has very low overhead.  From the point of view of efficiency\n    there would be no problem for clients or servers if ALL transactions\n    were required to be chunked.  There is not much performance difference\n    between\n    \n      Content-length: 123456\n  <123456 bytes of data>\n    \n    and \n    \n      Content-encoding: chunked\n    \n      123456\n  <123456 bytes of data>\n      0\n    \nUnfortunately, there is a slight correctness difference here!\n\nrom 3.6.1, \"The chunk-size field is a string of hex digits\"\nso the correct form is\n\n      Content-encoding: chunked\n\n      1E240\n          <123456 bytes of data>\n      0\n\nOtherwise, I agree entirely with John.  (And note that parsing\nhex numbers is usually less expensive than parsing decimals,\nby a few instruction cycles.)\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": ">>>>> \"RF\" == \"Roy T Fielding\" <fielding@kiwi.ICS.UCI.EDU> writes:\n\nRF> The 100 response must be sent by an HTTP/1.1 server upon receipt of\nRF> any HTTP/1.1 request containing a message body, after it receives the\nRF> header fields and determines that it wishes to receive that body.  The\nRF> RFC 2068 says this in section 8.2, but in a rather confusing way.\n\nRF> The client should not care either, since it should be ignoring any\nRF> 1xx class of response.  A client that is not looking for such a response\nRF> will simply see it (and ignore it) upon its next request, or never see\nRF> it at all if it just drops the connection.\n\n  Our server does send a 100 Continue under those conditions; the rule\n  I would like to see stated more directly is that:\n\n    When sending a request with a body to a server it has reason to\n    believe implements HTTP/1.1, a client SHOULD send the headers and\n    then wait for a 100 Continue or an error status before\n    transmitting the body of the request.\n\n  the current spec essentially says this only for the case where the\n  client is retrying a request, not for the first attempt.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: HTTP/1.1 : Chunkin",
            "content": "> -----Original Message-----\n> From: Adrien de Croy [mailto:adrien@qbik.com]\n> Sent: Thursday, January 29, 1998 2:09 PM\n> To: Ben Laurie\n> Cc: http-wg@cuckoo.hpl.hp.com\n> Subject: Re: HTTP/1.1 : Chunking\n> \n> \n> However, as pointed out by John Franks, this method would \n> not, and cannot\n> allow for multiple transactions per connection, since it \n> relies on the close\n> event being a signal as well.\n> \n> However, reflecting more on that issue, the chances of a \n> client requiring\n> multiple created entitities (i.e those where the server \n> cannot know a priori\n> the size) in a single connection is rather low, at least at \n> the moment.\n> Multiple normal requests per connection would still be possible, and\n> unaffected by this proposal.  \n> \n> \nI think its quite likely that a client might request 2 or 3 dynamically\ngenerated content entities of unknown length on th same connection.\n\nEven if your argument that its not likely is true, it still doesnt\nmean we can dump chunked because that possibility is still nonzero.\n\n> >Cheers,\n> >\n> >Ben.\n> >\n> >-- \n> >Ben Laurie            |Phone: +44 (181) 735 0686|Apache Group member\n> >Freelance Consultant  |Fax:   +44 (181) 735 \n> 0689|http://www.apache.org\n> >and Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\n> >A.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\n> >London, England.      |\"Apache: TDG\" \nhttp://www.ora.com/catalog/apache\n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at\nhttp://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Adrien de Croy wrote:\n> I would have thought that in a random data stream, the chances of any\n> particular 4 byte sequence would have been 1 in 4 Billion, but we are only\n> looking at the last 4 bytes before a close.  So, given there is only one\n> close per session, there is only one inspected 4 byte sequence, hence the\n> chance of any one file causing confusion over an OEF marker is 1 in 4 billion.\n\nSorry, my mistake, I was taking into account the below, which means\nyou'd have to check every byte.\n\n> However, as pointed out by John Franks, this method would not, and cannot\n> allow for multiple transactions per connection, since it relies on the close\n> event being a signal as well.\n> \n> However, reflecting more on that issue, the chances of a client requiring\n> multiple created entitities (i.e those where the server cannot know a priori\n> the size) in a single connection is rather low, at least at the moment.\n> Multiple normal requests per connection would still be possible, and\n> unaffected by this proposal.\n\nIt'd have to server the created entity last, which seems unlikely.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: Issue: message/http or application/htt",
            "content": ">E-mail does not permit content-transfer-encoding of a multipart,\n>only of individual parts, I believe.\n\nI was only referring to single parts.  The message type has its own\nencoding restrictions, which is why it may be more appropriate to\nrecommend use of \"application/http\" even though the semantics of\n\"message/http\" seemed like the right choice at the time.\n\nThis is actually fairly important, since it determines the most likely\nmethod of performing WebDAV operations via E-mail.  The other advantage\nof \"application/http\" is that it can be defined as a pipeline of requests\nor a pipeline of responses.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Issue: casesensitive date forma",
            "content": "Dean Gaudet has pointed out to me that Section 3.3.1 needs to include\na sentence to the effect of\n\n   HTTP-date is case sensitive and does not allow additional LWS\n   beyond that specifically included as SP in the grammar.\n\nsince \"literal\" defaults to case-insensitive.\n\nJust in case anyone wants to argue this point, I should mention that\nall servers that I have tested with If-Modified-Since do treat the\nvalue as case-sensitive, including Apache.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Hi\n\nAt 22:42 29/01/98 +0000, Ben Laurie wrote:\n>Adrien de Croy wrote:\n>> I would have thought that in a random data stream, the chances of any\n>> particular 4 byte sequence would have been 1 in 4 Billion, but we are only\n>> looking at the last 4 bytes before a close.  So, given there is only one\n>> close per session, there is only one inspected 4 byte sequence, hence the\n>> chance of any one file causing confusion over an OEF marker is 1 in 4\nbillion.\n>\n>Sorry, my mistake, I was taking into account the below, which means\n>you'd have to check every byte.\n\nI see - that would make this method extremely inefficient :)\n\nBasically my reasoning for raising this, is as part of ongoing development\nof WinGate, we are looking at implementing HTTP/1.1 support in our HTTP proxy.\n\nWe originally considered HTTP/1.0 to be relatively complex for a proxy, but\nHTTP/1.1 seems an order of magnitude more complex.  I seem to remember a\ncouple of my engineering lecturers reiterating the old KISS maxim, and since\nthis is still a proposed standard, I thought \"why not\" see if some redundant\ncomplexity could be removed from the spec.\n\nWith the reasons given, I have removed chunking from my list of redundant\n(?) complexities in HTTP/1.1 (which I am still building, although I will\nraise a couple here).\n\nFirst one.\n\n4.2 Message headers\n\nThe allowing of multiple headers if their values form a single list seems a\nredundant complexity.  Unless you are catering for people debugging by hand\nwith telnet,  There is no reason why any client or server software should\never NEED to split a field value over multiple lines.\n\nSupport for multiple fields of the same name is actually quite complex in a\nclient (including a proxy) however, since the fields can no longer be stored\nin a map structure indexed by field name, and so field lookups get slower -\nyou also have to then assume that there could be another one there.\n\nPersonally I would remove allowing this from the spec.  I don't believe any\nclient software would implement it (unless they are programming in Pascal or\nsomething with a string length limitation - UGH!).\n\nNext one\n\n2.2 Basic rules - Folding\n\nSame goes for folding of header fields - just adds complexity to a parser.\nOne can no longer use the CLRF as an end of line marker, because the next\nline may start with LWS.  What's the point? Manual telnet debugging support?\n\nSub-entity negotiation.\n\nThis is a whole can of worms for caching proxies.  All of a sudden a URL is\nnot unique any more, and caches need to index on Accept header fields as\nwell.  Also, a caching proxy may make a different decision about what\nresource is acceptable to a client or not.  I would have thought that there\nwould be VERY few implementations of this, if any server needed to send a\ndifferent entity due to some Accept-x field (say language) it could do it\nwith a redirect to another URI.  This would also be much more manageable by\nthe content provider, and MUCH easier to implement and manage in a server.\nThere needs to be something to uniquely identify every entity in the system,\na URL is not a bad one.  Would multi-entity resources be treated by all\nintermediaries (proxies gateways etc) as a single resource indexed to the\nURI that requested it?\n\nProxy Authentication.\n\nThere are some problems with the proposed method where it relates to chained\nproxies, or a \"use proxy\" response.  Basically it may be necessary for a\nclient to include many Proxy-Authorization fields in a request, since it is\npossible that only the client holds the credentials of the proxies involved.\nOtherwise ALL agents in the chain (including the end server) MUST support\npersistent connections.\n\nIf it needs multiple authorisation fields, how can each proxy tell which one\nis intended for it.  It would have to try them all for a match, strip it\nout, and pass the request on.\n\nMultiple byte-range requests?  \n\nNormally I guess these would only be made by caches, as clients would\ngenerally always need the end of a file, rather than chunks out of the\nmiddle of it.  Are overlapping byte ranges meant to be condensed by the\nserver? or supplied as requested.\n\n\nThe only general thing I would say is that in the main, the new HTTP/1.1\nadds a whole bunch of useful stuff, especially wrt caching, validating\ncached files, and efficiency improvements.  However, there are some aspects\nthat would be quite complex to code, and therefore cause problems for\nsoftware developers, which in the end affects the lives of the software\nusers - who the protocol was developed for.  With the content negotiation\nconcept, the concept of a file on a disk seems to have been lost.  I don't\nsee any unique identifier for this, which is what is required for efficient\ncaching, and serving of files.  Since the protocol is changing, why not make\nit REAL easy to validate cache files by assigning every cachable resource a\nglobally unique identifier which changes with modifications to the resource.\nThen a validation would simply involve sending the identifier\nSay in the form of URL: File Version, or system file time.  This would\ncompletely remove the complexity of validation with servers, and caches\ncould still use freshness concepts for efficiency.  It would also remove the\ndependency on synchronised clocks etc.\n\n\nAnyway, just a few thoughts.\n\nCheers\n\nAdrien de Croy\n\n\n>\n>> However, as pointed out by John Franks, this method would not, and cannot\n>> allow for multiple transactions per connection, since it relies on the close\n>> event being a signal as well.\n>> \n>> However, reflecting more on that issue, the chances of a client requiring\n>> multiple created entitities (i.e those where the server cannot know a priori\n>> the size) in a single connection is rather low, at least at the moment.\n>> Multiple normal requests per connection would still be possible, and\n>> unaffected by this proposal.\n>\n>It'd have to server the created entity last, which seems unlikely.\n>\n>Cheers,\n>\n>Ben.\n>\n>-- \n>Ben Laurie            |Phone: +44 (181) 735 0686|Apache Group member\n>Freelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\n>and Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\n>A.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\n>London, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "RE: HTTP/1.1 : Chunkin",
            "content": "-> -----Original Message-----\n-> From: Adrien de Croy [mailto:adrien@qbik.com]\n-> Sent: Thursday, January 29, 1998 11:37 PM\n-> To: Ben Laurie\n-> Cc: http-wg@cuckoo.hpl.hp.com\n-> Subject: Re: HTTP/1.1 : Chunking\n-> \n-> \n-> 4.2 Message headers\n-> \n-> The allowing of multiple headers if their values form a \n-> single list seems a\n-> redundant complexity.  Unless you are catering for people \n-> debugging by hand\n[...snip...]\n-> \n-> Personally I would remove allowing this from the spec.  I \n-> don't believe any\n-> client software would implement it (unless they are \n-> programming in Pascal or\n-> something with a string length limitation - UGH!).\n-> \nDoes the spec allow you to collapse multiple headers into\na single line? If so, you can put them in your name/value\nlist just the same.\n-> \n-> Proxy Authentication.\n-> \n-> There are some problems with the proposed method where it \n-> relates to chained\n-> proxies, or a \"use proxy\" response.  Basically it may be \n-> necessary for a\n-> client to include many Proxy-Authorization fields in a \n-> request, since it is\n-> possible that only the client holds the credentials of the \n-> proxies involved.\n-> Otherwise ALL agents in the chain (including the end server) \n-> MUST support\n-> persistent connections.\n-> \n\n\n-> If it needs multiple authorisation fields, how can each \n-> proxy tell which one\n-> is intended for it.  It would have to try them all for a \n-> match, strip it\n-> out, and pass the request on.\n-> \nHuh? (put 305 aside for the moment)\nIf the client is going through multiple chained (auth requesting )\nproxies then it must include credentials for each of them.\nAs a side affect, each credential is readable by all\nproxies in front of it.\nThe proxy knows which credential set is for it\nby looking for the realm value that it generated.\n\nAs a former proxy implementor, I am happy to agree that\nthe proxy-auth system is a bit ugly and could use some work.\nA proposal to include more specific realm info (for the chained\n case) was rejected because it could cause backward compat problems.\n\n-> Multiple byte-range requests?  \n-> \n-> Normally I guess these would only be made by caches, as clients would\n-> generally always need the end of a file, rather than chunks \n-> out of the\n-> middle of it.  Are overlapping byte ranges meant to be \nNot really.  If a client is doing pipelined range requests,\nit may very well, retreive bits of an entity at a time,\nie bytes \n1) 1-100 of entity A\n2) 1-100 of entity B\n3) 1-100 of entity C\n4) 101-200 of entity A\n5) 101-200 of entity B\n6) 101-200 of entity C\nto affect a sort of multiplex...  This is critical\nfor user experience to perform well with only 1\nor two connections..\n\n-> condensed by the\n-> server? or supplied as requested.\n-> \n-> Since the protocol is \n-> changing, why not make\n-> it REAL easy to validate cache files by assigning every \n-> cachable resource a\n-> globally unique identifier which changes with modifications \n-> to the resource.\n-> Then a validation would simply involve sending the identifier\n-> Say in the form of URL: File Version, or system file time.  \n-> This would\n-> completely remove the complexity of validation with servers, \n-> and caches\n-> could still use freshness concepts for efficiency.  It would \n-> also remove the\n-> dependency on synchronised clocks etc.\n->\nDoesnt Etags meet this requirement?\n\nfurther, be careful saying that \"since the protocol is changing\"..\nNo, the protocol isnt changing necessarily.  \nWe are bound that all changes must be backward compatible.\nAdditionally, we are past the time for new features to be added\nin the protocol. (at least in v1.1).  You might look to the extensions\nworking group for a mechanism to plug in new features...\nthat list is ietf-http-ext@w3.org\n\n---\nJosh Cohen <josh@microsoft.com>\nProgram Manager - Internet Technologies \n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Adrien de Croy <adrien@qbik.com> writes:\n\n>With the reasons given, I have removed chunking from my list of redundant\n>(?) complexities in HTTP/1.1 (which I am still building, although I will\n>raise a couple here).\n\nI suggest you go back to the HTTP 1.1 Proposed Standard (RFC 2068) and\nread section 3.1 \"HTTP Version\".  It describes the rules under which\nthis group makes changes to the protocol, and what limits we place upon\nourselves.  Pay particular attention to the first paragraph:\n\n   \"HTTP uses a \"<major>.<minor>\" numbering scheme to indicate versions\n   of the protocol. The protocol versioning policy is intended to allow\n   the sender to indicate the format of a message and its capacity for\n   understanding further HTTP communication, rather than the features\n   obtained via that communication. No change is made to the version\n   number for the addition of message components which do not affect\n   communication behavior or which only add to extensible field values.\n   The <minor> number is incremented when the changes made to the\n   protocol add features which do not change the general message parsing\n   algorithm, but which may add to the message semantics and imply\n   additional capabilities of the sender. The <major> number is\n   incremented when the format of a message within the protocol is\n   changed.\"\n\nAs you can see, one of the rules in developing HTTP 1.1 was that HTTP 1.0\nclients and servers have to be able to process HTTP 1.1 requests and\nresponses as if they were HTTP 1.0.  Any incompatible change to headers\nor to basic rules that were in place in HTTP 1.0 is disallowed a priori.\nWe've deferred a number of good ideas for exactly that reason already.\n\nWhile some of your suggestions might bear considering for a future\nreplacement protocol (HTTP 2.0 or later?), they all fly in the face of\nthe rules for extending HTTP 1.x.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "RE: HTTP/1.1 : Chunkin",
            "content": "Hi\n\nAt 03:23 30/01/98 -0800, Josh Cohen wrote:\n>\n>\n>-> -----Original Message-----\n>-> From: Adrien de Croy [mailto:adrien@qbik.com]\n>-> Sent: Thursday, January 29, 1998 11:37 PM\n>-> To: Ben Laurie\n>-> Cc: http-wg@cuckoo.hpl.hp.com\n>-> Subject: Re: HTTP/1.1 : Chunking\n>-> \n>-> \n>-> 4.2 Message headers\n>-> \n>-> The allowing of multiple headers if their values form a \n>-> single list seems a\n>-> redundant complexity.  Unless you are catering for people \n>-> debugging by hand\n>[...snip...]\n>-> \n>-> Personally I would remove allowing this from the spec.  I \n>-> don't believe any\n>-> client software would implement it (unless they are \n>-> programming in Pascal or\n>-> something with a string length limitation - UGH!).\n>-> \n>Does the spec allow you to collapse multiple headers into\n>a single line? If so, you can put them in your name/value\n>list just the same.\n\nIt does, and that is definitely what I will be doing, but the thing is that\nthe spec allows it, which means implementors must check for it.\n\n>-> \n>-> Proxy Authentication.\n>-> \n>-> There are some problems with the proposed method where it \n>-> relates to chained\n>-> proxies, or a \"use proxy\" response.  Basically it may be \n>-> necessary for a\n>-> client to include many Proxy-Authorization fields in a \n>-> request, since it is\n>-> possible that only the client holds the credentials of the \n>-> proxies involved.\n>-> Otherwise ALL agents in the chain (including the end server) \n>-> MUST support\n>-> persistent connections.\n>-> \n>\n>\n>-> If it needs multiple authorisation fields, how can each \n>-> proxy tell which one\n>-> is intended for it.  It would have to try them all for a \n>-> match, strip it\n>-> out, and pass the request on.\n>-> \n>Huh? (put 305 aside for the moment)\n>If the client is going through multiple chained (auth requesting )\n>proxies then it must include credentials for each of them.\n>As a side affect, each credential is readable by all\n>proxies in front of it.\n>The proxy knows which credential set is for it\n>by looking for the realm value that it generated.\n\nOK.\n\n>\n>As a former proxy implementor, I am happy to agree that\n>the proxy-auth system is a bit ugly and could use some work.\n>A proposal to include more specific realm info (for the chained\n> case) was rejected because it could cause backward compat problems.\n>\n\nAre these backward compatibility problems real - how many implementations\nwould need to significantly alter their behavior due to a change here, given\nthat proxy authentication was not even a part of HTTP/1.0\n\n>-> Multiple byte-range requests?  \n>-> \n>-> Normally I guess these would only be made by caches, as clients would\n>-> generally always need the end of a file, rather than chunks \n>-> out of the\n>-> middle of it.  Are overlapping byte ranges meant to be \n>Not really.  If a client is doing pipelined range requests,\n>it may very well, retreive bits of an entity at a time,\n>ie bytes \n>1) 1-100 of entity A\n>2) 1-100 of entity B\n>3) 1-100 of entity C\n>4) 101-200 of entity A\n>5) 101-200 of entity B\n>6) 101-200 of entity C\n>to affect a sort of multiplex...  This is critical\n>for user experience to perform well with only 1\n>or two connections..\n\nOK\n\n>\n>-> condensed by the\n>-> server? or supplied as requested.\n>-> \n>-> Since the protocol is \n>-> changing, why not make\n>-> it REAL easy to validate cache files by assigning every \n>-> cachable resource a\n>-> globally unique identifier which changes with modifications \n>-> to the resource.\n>-> Then a validation would simply involve sending the identifier\n>-> Say in the form of URL: File Version, or system file time.  \n>-> This would\n>-> completely remove the complexity of validation with servers, \n>-> and caches\n>-> could still use freshness concepts for efficiency.  It would \n>-> also remove the\n>-> dependency on synchronised clocks etc.\n>->\n>Doesnt Etags meet this requirement?\n>\n\nIt would if the presence of ETag was mandatory, and there was some\ndefinition of the content such that the ETag could be compared by the\nrecipient to determine precedence.\n\n>further, be careful saying that \"since the protocol is changing\"..\n>No, the protocol isnt changing necessarily.  \n>We are bound that all changes must be backward compatible.\n>Additionally, we are past the time for new features to be added\n>in the protocol. (at least in v1.1).  You might look to the extensions\n>working group for a mechanism to plug in new features...\n>that list is ietf-http-ext@w3.org\n\nHmmm, OK.\n\nAs for backward compatibility, that is pretty subjective.  In software\ndevelopment, it is more easy (read more reliable) to implement a simple\nprotocol than one which is backwardly compatible and complex.\n\nCheers\n\nAdrien\n\n>\n>---\n>Josh Cohen <josh@microsoft.com>\n>Program Manager - Internet Technologies \n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "midcourse error",
            "content": "Here's something that has puzzled me about HTTP server\nimplementations.  What happens if the server encounters an error after\nit sends, say, a \"200 OK\" response and part of the entity body?\nExample cases include an origin server that gets a timeout from a CGI\npart way through relaying the CGI's output, or when a proxy acts as a\npure relay and, for example, its connection to the next hop server (is\nthat upstream or downstream? :-) breaks.  The server has no way to\nsignal to its client that the previously claimed success has now turned\ninto an error.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": ">As for backward compatibility, that is pretty subjective.  In software\n>development, it is more easy (read more reliable) to implement a simple\n>protocol than one which is backwardly compatible and complex.\n\nIn reality, it is easier and more reliable to deploy a protocol that\nis backwards compatible.  HTTP/1.0 was almost as complex as HTTP/1.1 --\nthe only big difference is that HTTP/1.0 was unable to accomplish what\nit tried to do, whereas HTTP/1.1 is barely sufficient without making\nincompatible changes.\n\nKeep in mind that HTTP is intended for many more applications than\njust the one that you are working on today.  Even with all of its\napparent complexity, it is still possible to write a simple HTTP server\nin just a few hours, and a simple HTTP client in a few days.  The\ncomplexity is only needed by complex applications, such as caching,\nbut failure to account for that complexity results in failed systems.\n\nKia ora,\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issue: casesensitive date forma",
            "content": "Roy Fielding writes:\n\n    Dean Gaudet has pointed out to me that Section 3.3.1 needs to include\n    a sentence to the effect of\n    \n       HTTP-date is case sensitive and does not allow additional LWS\n       beyond that specifically included as SP in the grammar.\n    \n    since \"literal\" defaults to case-insensitive.\n    \nI would phrase it as:\n       HTTP-date is case sensitive and does not allow additional LWS\n       beyond that specifically included as SP in the grammar.\n       HTTP/1.1 implementations that generate HTTP-date values MUST use\n       the correct case and LWS.  However, HTTP/1.1 clients and servers\n       that parse HTTP-date value SHOULD accept values with extra LWS\n       and without regard to case.\n\nI went back to the 500K-message proxy trace I made about 14 months\nago, and went looking for what the then-existing HTTP implementations\nactually send.\n\nI started by looking for all header lines containing the string \"GMT\",\nand took these lines apart into the individual tokens.  In this\ngroup, I didn't find any lower-case month or day-of-the-week names,\nalthough that doesn't prove that no implementations send \"GMT\"\ndates with improper case; this survey only covers about 22K servers\nand 7K clients.\n\nI then looked for Date, Expires, and If-Modified-Since headers\n(using a case-insensitive match for header names), and concentrated\non the ones that were not in GMT.  Here I found some interesting\noddities:\nExpires: {ts '1996-12-06 11:44:58'}\nExpires: 01-Jan-70\nExpires: Now\nDate: FRIDAY, 12/06/96 13:29:11 EST\nDate:Friday, 06-Dec-1996\nDate:         1996/08/26\nDate: Thu, 05 Dec 1996 18:08:25 +0000\nDate: March 1979\nDate: February 26, 1979\nIt's possible that some of these are actually from the bodies\nof messages (sometimes my header-parser gets confused by interrupted\nretrievals).  However, it does appear that at least one implementation\nis sending RFC-850 style dates with non-standard case:\nDate: FRIDAY, 06-DEC-96 4:48:43 CST\nWhether or not this should be rejected outright because of the \"CST\"\nis another question.\n\n(All of the If-Modified-Since headers in my trace seem to include\n\"GMT\", although many are in RFC-850 format.)\n\nBottom line: there aren't many examples of implementations that\nsend otherwise legal dates but in the \"wrong\" case.  However,\nit seems reasonable to suggest that implementations not choke\non these.\n\nBy the way, I found a few other date-related oddities in HTTP\nheaders:\nSatday, 30-Nov-96 19:59:25 GMT\nmiercoles, 14-ago-96 08:20:33 GMT\nviernes, 06-dic-96 00:38:02 GMT\n\nI can't imagine a date-parser that would really care that \"Saturday\" is\nspelled \"Satday\" (instead of just ignoring the day-of-the-week\nentirely).  On the other hand, trying to get a parser to understand the\nmonth-names in all possible different languages is probably\nimpossible.  (I wonder if there is any three-letter string that\nabbreviates two different month-names in two different lanuages?)\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": "Tena koe\n\nAt 09:39 30/01/98 -0800, Roy T. Fielding wrote:\n>>As for backward compatibility, that is pretty subjective.  In software\n>>development, it is more easy (read more reliable) to implement a simple\n>>protocol than one which is backwardly compatible and complex.\n>\n>In reality, it is easier and more reliable to deploy a protocol that\n>is backwards compatible.  HTTP/1.0 was almost as complex as HTTP/1.1 --\n>the only big difference is that HTTP/1.0 was unable to accomplish what\n>it tried to do, whereas HTTP/1.1 is barely sufficient without making\n>incompatible changes.\n\nWhen I look at what coding I will have to do to implement HTTP/1.1 support\nin a caching proxy which also does tunneling and gatewaying, and serving,\ngiven that I need to support HTTP/1.0 and 0.9 as well, there are going to be\nso many switches in the code that it will be as if I implemented another\nprotocol anyway.  I guess I am mainly self-interested in the points I have\nraised, but it would be interesting to hear say the reasoning behind\nsomething like multiple possible entities for a single resource (rather than\nsay a redirection mechanism, which probably more people will deploy).  Files\nin a file system are unique.  Machines on the internet are unique. Time is\nunique. What is wrong with the concept of a unique addressing scheme?\n\nI do realise that HTTP/1.1 is intended for a raft of different applications,\nthere are also a raft of protocols being developed for these applications as\nwell.  I also understand that a large part of the complexity of 1.1 is to do\nwith interactions between caches, unfortunately that is what I need to\nimplement.  Also, there are a great number of users who obtain all Internet\naccess through a proxy server, and this number is growing rapidly.  So these\nmore complex systems are becoming ever more prevalent.  That means they have\na greater impact on more people, which means software vendors had better get\nit right.  It is easier (more likely) to get it right if it is simpler.\nSuch things as allowing 3 date formats for HTTP/1.1 seems to fly against\nthis.  Sure it is necessary for HTTP/1.0, but can't we mandate that clients\nuse a fixed format if they are to be HTTP/1.1 compliant?  That way in the\nfuture, the other two can be removed, and code can be simplified.\n\n>\n>Keep in mind that HTTP is intended for many more applications than\n>just the one that you are working on today.  Even with all of its\n>apparent complexity, it is still possible to write a simple HTTP server\n>in just a few hours, and a simple HTTP client in a few days.  The\n>complexity is only needed by complex applications, such as caching,\n>but failure to account for that complexity results in failed systems.\n>\n\nI also see complexity in things like:\n\nStrong vs Weak comparisons.  I wonder what human will ever decide that they\ndon't mind if people don't get the latest version of their work, because it\ndidn't change appreciably, or what system administrator is going to flag a\nfile as having hardly changed much, or what server developer is going to\nallow support for this (no mean task). \n\nCache Staleness, and warnings.  I wonder what software vendor is not going\nto curse every time a user calls up or emails complaining about warning\nmessages, and wondering what to do about them.  That costs real money.  I\nrealise in terms of coping with connectivity problems and saving bandwidth,\nit can be useful, but what user is not going to force an update anyway.   As\nfor saving bandwidth, the protocol overhead in HTTP is huge.  And HTTP/1.1\nadds even more.  You get about 2k of headers to transfer a 256 byte file.\nAnalysis of our caches and customer caches shows that the majority of files\ncached are under 4 k.  That means header bandwidth accounts for about 50%.\nIf you want to save bandwidth, look there.\n\nAnyway, sorry for my ranting.  I don't really expect that this will really\nchange anything.  However, I do have a couple of ideas that could be useful,\nand added in easily.\n\n1. Interceptive HTTP caching.\n\nThe way this works in WinGate is this.  Clients may be using SOCKS to\ncommunicate directly with a server through a SOCKS firewall.  Our SOCKS\nserver when it receives a connection request for port 80, or 8080 etc, looks\nat the first packet of the client data.  If it sees HTTP, it intercepts the\nrequest, and passes it to the caching web proxy built into WinGate as well.\nthe important thing to note here is that the client thinks it is talking to\nthe end server, and does not know it is talking through a proxy.  This flies\nagainst certain requirements in the HTTP/1.1 spec.  The reason we did it\nthough is obvious enough - sharing a single cache between SOCKS clients.  I\nalso see this in the future in router products.  Imagine a router that could\nintercept and cache HTTP - what a bandwidth saving you would have there.  It\nwould be real easy to do too.\n\nThe addition of the Host: tag in HTTP/1.1 is really good now, because it\nallows the proxy to build up a full URL for cache indexing, whereas before\nit perhaps only had the IP address of the host, which causes caching\nefficiency problems for big sites that serve a single domain name out of\nmany machines.\n\nA respone tag or something that could tell the client their request had been\nintercepted would be useful here.\n\n2. Condensing real-time streams.\n\nWe are seeing a couple of clients that use HTTP and TCP flow control to get\nrealtime audio.  The data rate is controlled by how fast the client chooses\nto read the data off the TCP buffers.  It would be useful to be able to\nrecognise in a proxy when such data was real-time or not, perhaps a major\ncontent-type.  Then if anyone else requested the same resource, they could\nbe branched in.  This could provide enormous bandwidth savings in large\nhierarchical caching proxy architectures.\n\n\nAnyay, a couple of thoughts.\n\nCheers\n\nAdrien\n\n\n>Kia ora,\n>\n>....Roy\n>\n----------------------------------------------------------------------------\n------\nAdrien de Croy - adrien@qbik.com.  Qbik New Zealand Limited, Auckland, New\nZealand\n                 See our pages and learn about WinGate at http://www.qbik.com/\n----------------------------------------------------------------------------\n------\n\n\n\n"
        },
        {
            "subject": "deleted spa",
            "content": "TEXT/PLAIN charset=US-ASCII attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: Issue: casesensitive date forma",
            "content": ">I would phrase it as:\n>       HTTP-date is case sensitive and does not allow additional LWS\n>       beyond that specifically included as SP in the grammar.\n>       HTTP/1.1 implementations that generate HTTP-date values MUST use\n>       the correct case and LWS.  However, HTTP/1.1 clients and servers\n>       that parse HTTP-date value SHOULD accept values with extra LWS\n>       and without regard to case.\n\nI'd rather not phrase it that way, since I don't want a protocol\nrequirement that suggests we should treat invalid date formats as\nsomehow valid.  I have no problem with systems trying to read the\nfield as robustly as possible, but that isn't a protocol requirement.\n\nRight now we consider those formats to be invalid, and thus ignore\nthem when processing the If-Modified-Since field (i.e., the response\nis 200 if the date is invalid).  This is fine for an origin server,\nsince it results in faster conversion and failure is safe.  I'd expect\na robust client to do better.\n\nIn any case, we always ignore the weekday.\n\n>By the way, I found a few other date-related oddities in HTTP\n>headers:\n>Satday, 30-Nov-96 19:59:25 GMT\n>miercoles, 14-ago-96 08:20:33 GMT\n>viernes, 06-dic-96 00:38:02 GMT\n\nYep, that tends to be caused by a date formatter that uses one of\nthe standard libraries affected by the locale setting.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 : Chunkin",
            "content": ">>>>> \"AdC\" == Adrien de Croy <adrien@qbik.com> writes:\n\nAdC> ... it would be interesting to hear say the reasoning behind\nAdC> something like multiple possible entities for a single resource\nAdC> (rather than say a redirection mechanism, which probably more\nAdC> people will deploy).\n\n  If you have (for\n  example) an interface that has alternate versions for different\n  languages, you can use just one set of (bookmarkable) URLs, but each\n  URL will serve the best available language for each user that\n  follows it (so I don't get the french version I can't read just\n  because I got the URL from a french speaker).\n\n  Doing this with redirection costs an extra round trip for every\n  request, and is harder for proxies to use - if the origin server\n  includes the correct Vary information in the response, proxies can\n  correctly cache the multiple versions.\n\nAdC> Such things as allowing 3 date formats for HTTP/1.1 seems to fly\nAdC> against this.  Sure it is necessary for HTTP/1.0, but can't we\nAdC> mandate that clients use a fixed format if they are to be\nAdC> HTTP/1.1 compliant?  That way in the future, the other two can be\nAdC> removed, and code can be simplified.\n\n  The requirement is already there in the 1.1 spec - there is only one\n  format allowed whenever dates are transmitted.\n\nAdC> Cache Staleness, and warnings.  I wonder what software vendor is\nAdC> not going to curse every time a user calls up or emails\nAdC> complaining about warning messages, and wondering what to do\nAdC> about them.  That costs real money.  I realise in terms of coping\nAdC> with connectivity problems and saving bandwidth, it can be\nAdC> useful, but what user is not going to force an update anyway.\n\n  The point is exactly that it allows a proxy to provide _some_\n  response even when an update is impossible, but let the user know\n  that is what they got - what is the problem?\n\nAdC> ... the protocol overhead in HTTP is huge.  And HTTP/1.1 adds\nAdC> even more.  You get about 2k of headers to transfer a 256 byte\nAdC> file.\n\n  Not in any of the tests we've run.  The latest generation of both\n  browsers and servers have greatly reduced the headers they send.\n\nAdC> 1. Interceptive HTTP caching.\n\nAdC> A respone tag or something that could tell the client their\nAdC> request had been intercepted would be useful here.\n\n  See the required 'Via' header.\n\nAdC> 2. Condensing real-time streams.\n\nAdC> We are seeing a couple of clients that use HTTP and TCP flow control to get\nAdC> realtime audio.  The data rate is controlled by how fast the client chooses\nAdC> to read the data off the TCP buffers.  It would be useful to be able to\nAdC> recognise in a proxy when such data was real-time or not, perhaps a major\nAdC> content-type.  Then if anyone else requested the same resource, they could\nAdC> be branched in.  This could provide enormous bandwidth savings in large\nAdC> hierarchical caching proxy architectures.\n\n  This is really getting into multicast support under HTTP, which is a\n  very interesting idea, but (IMHO) way out of scope for the current\n  standard.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Issue: message/http or application/htt",
            "content": "At 15.05 -0800 98-01-28, Roy T. Fielding wrote:\n> I've always found this one of the most annoying things about how\n> MIME was specified. If all message types must obey the same rules\n> as an RFC 822 message, then why would you ever need more than\n> message/rfc822?\n\nI thought the difference between message/rfc822 and message/http\nwas that the outermost heading, the header immediately following\nthe Content-Type:Message heading, was to be formatted and\ninterpreted according to e-mail versus http rules. For example,\na message/http header can contain an \"Age:\" header field, but\nsuch a field is undefined in a message/rfc822 heading.\n\nHowever, it is probably not permitted to include Content-Encoding\nor Transfer-Encoding in headings transported through e-mail,\neven if the heading is of type message/http, since those encoding\nformat may not be permitted in e-mail?\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Issue: message/http or application/htt",
            "content": "Excerpts from mail: 2-Feb-98 Re: Issue: message/http or .. Jacob\nPalme@dsv.su.se (1058*)\n\n> However, it is probably not permitted to include Content-Encoding\n> or Transfer-Encoding in headings transported through e-mail,\n> even if the heading is of type message/http, since those encoding\n> format may not be permitted in e-mail?\n\nSure, you could do that.  You'd just have to encode it a layer up.  If\nyou had the equivalent of:\n\nContent-type: message/http\nContent-Transfer-Encoding: binary\n\n[binary data]\n\nand you wanted to send it through email as a message/http type, you\ncould do something like this:\n\nContent-type: message/rfc822\nContent-Transfer-Encoding: base64\n\nQ29udGVudC10eXBlOiBtZXNzYWdlL2h0dHAKQ29udGVudC1UcmFuc2Zlci1FbmNvZGlu\nZzogYmluYXJ5CgpbYmluYXJ5IGRhdGFdCg==\n\nWhen you decoded this, you'd have the original message/http data.  \n\nWhat's less clear to me is *why* you'd want to do this....  -- Nathaniel\n--------\nImagination was given to man to compensate him for what he is not;\na sense of humor to console him for what he is.  -- Francis Bacon\n\nNathaniel Borenstein <nsb@fv.com>          |  FAQ & PGP key:\nChief Scientist, First Virtual Holdings    |  nsb+faq@nsb.fv.com\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "OK, don't laugh... For CGI, WebSite spools the output by default (you can \ndisable it) into a file which is implemented as a privately backed block of \nexpandable virtual memory. This has a few advantages and the obvious CPU/disk \nhit and reduction of capacity. The advantages are (1) you can rewind the \noutput stream and send a non-200 response at any time, and (2) Keep-Alive is \nretained without chunking because you can get the content-length. The latter \n\"advantage\" will fade with time, of course.\n\n  -- Bob\n\nOn Apr 14, Dave Kristol wrote:\n> Subject: mid-course errors\n> Here's something that has puzzled me about HTTP server\n> implementations.  What happens if the server encounters an error after\n> it sends, say, a \"200 OK\" response and part of the entity body?\n\n\n\n"
        },
        {
            "subject": "Re: Reauthentication Requested Revisite",
            "content": "JC> This provides a general mechanism for a \"retry request\" from the server\nJC> to the client along with a way to acknowledge receipt of the retry\nJC> request.\n\n  Which may or may not be a good thing, but is, I think, orthogonal to\n  the question of invalidating cached user credentials.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: Reauthentication Requested Revisite",
            "content": "-> -----Original Message-----\n-> From: Scott Lawrence [mailto:lawrence@agranat.com]\n-> Sent: Monday, February 02, 1998 4:06 PM\n-> To: Josh Cohen\n-> Subject: Re: Reauthentication Requested Revisited\n-> \n-> \n-> \n-> JC> This provides a general mechanism for a \"retry request\" \n-> from the server\n-> JC> to the client along with a way to acknowledge receipt of \n-> the retry\n-> JC> request.\n-> \n->   Which may or may not be a good thing, but is, I think, \n-> orthogonal to\n->   the question of invalidating cached user credentials.\n-> \nTechnically yes.  However to make the 'reauth request' actually\nwork and be useable, both are necessary from a system view.\n\n1) the server needs a way to send a message to the client saying\n  please revalidate your credentials with the user\n2) the server needs a way to detect that the client has\n   or is at least claiming to knowingly complete the task\n   (revalidate the credentials)\n\nSo, I guess Im lumping two things together in a sense.  I see the\n second part as an infrastructure item needed by the first to make it\n useable.\n\n(else how would you know if the client actually revalidated?)\n\n-> Scott Lawrence           EmWeb Embedded Server       \n-> <lawrence@agranat.com>\n-> Agranat Systems, Inc.        Engineering            \n-> http://www.agranat.com/\n-> \n\n\n\n"
        },
        {
            "subject": "Re: Reauthentication Requested Revisite",
            "content": ">>>>> \"JC\" == Josh Cohen <joshco@MICROSOFT.com> writes:\n\nJC> 1) the server needs a way to send a message to the client saying\nJC>   please revalidate your credentials with the user\n\n  I know that I sound like a broken record here, but the minimal\n  requirement is to instruct the user agent to discard the current\n  credentials - whether or not it should then obtain new ones depends\n  on whether or not it has another request to send that requires\n  them, which might be immediatly or next month.\n\n  A 'Logout' function does not require that new credentials be\n  obtained - in fact, doing so would defeat the very purpose of\n  discarding the current set.\n\n  A 'Revalidate' function can be accomplished by instructing the user\n  agent to discard current credentials in any redirection or\n  authentication-required response.\n\nJC> 2) the server needs a way to detect that the client has\nJC>    or is at least claiming to knowingly complete the task\nJC> ... (else how would you know if the client actually revalidated?)\n\n  But the assurance means nothing; in neither case can the server know\n  anything about what the user agent did.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: Reauthentication Requested Revisite",
            "content": "-> -----Original Message-----\n-> From: Scott Lawrence [mailto:lawrence@agranat.com]\n-> Sent: Monday, February 02, 1998 6:52 PM\n-> To: http-wg@cuckoo.hpl.hp.com\n-> Subject: Re: Reauthentication Requested Revisited\n-> \n-> \n-> \n-> >>>>> \"JC\" == Josh Cohen <joshco@MICROSOFT.com> writes:\n-> \n-> JC> 1) the server needs a way to send a message to the client saying\n-> JC>   please revalidate your credentials with the user\n-> \n->   I know that I sound like a broken record here, but the minimal\n->   requirement is to instruct the user agent to discard the current\n->   credentials - whether or not it should then obtain new ones depends\n->   on whether or not it has another request to send that requires\n->   them, which might be immediatly or next month.\n-> \n->   A 'Logout' function does not require that new credentials be\n->   obtained - in fact, doing so would defeat the very purpose of\n->   discarding the current set.\n-> \n->   A 'Revalidate' function can be accomplished by instructing the user\n->   agent to discard current credentials in any redirection or\n->   authentication-required response.\n-> \nThe issue was \"reauthentication requested\", which implies the ability\nto obtain new (or the same) creds again.\n\nI disagree, a simple \"discard\" mechanism is not sufficient.  What we\nneed is something which will work with existing browsers.  By that I mean\nthat the server can tell if the client understood the reauth request.\nAn old browser will ignore the echo request, and the server will know\nthat the client understood, at least in some part, the request.\n\n-> JC> 2) the server needs a way to detect that the client has\n-> JC>    or is at least claiming to knowingly complete the task\n-> JC> ... (else how would you know if the client actually revalidated?)\n-> \n->   But the assurance means nothing; in neither case can the \n-> server know\n->   anything about what the user agent did.\n-> \nHrm.. technically, your right, you dont know for sure that the client\nisnt lying.  However, the idea is that by responding to the echo,\nthe client is taking responsibility and acknowledging the request..\n\nThe echo response works hand in hand with a discard or reauth to \nprovide some acknowledgement.  I guess, in a way, you could look at\nthis as a client capability mechanism.\n\nAs far as the minimal functionality needed, Im not thinking in \nterms of \"logout\".  I joined paul leach in raising this issue\nbecause I want to see a way for a proxy or server to timeout\na credentials \"instance\".\n\nWe are willing to trust the client if it acknowledges the request.\nThis also provides a safe way to automatically retry a POST.\n\n\n-> --\n-> Scott Lawrence           EmWeb Embedded Server       \n-> <lawrence@agranat.com>\n-> Agranat Systems, Inc.        Engineering            \n-> http://www.agranat.com/\n-> \n\n\n\n"
        },
        {
            "subject": "Test Day 1998-020",
            "content": "  I have updated the W3C Implementors Forum page to list many of the\n  regular participants in these tests, including several servers (both\n  origin and proxy) that are available most or all of the time.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "proposed solution for CONTENTLENGT",
            "content": "[This proposal has been circulated among the editorial group\nwith no dissent.  -Jeff]\n\nMost of the changes below are editorial, in that they clarify\nthe language of the specification but don't actually make any\nnormative changes.  The only real normative change is that\nwe require the use \"chunked\" to delimit any other kind of\ntransfer-coding.  We also remove the \"proxies MUST NOT\nmodify Content-Length\" rule, which makes no sense when\ncombined with (for example) \"Transfer-Encoding: gzip\".\n\nI think we need to clean up the terminology in 14.36.1 (Byte Ranges),\nbut that's probably a separate editorial issue.\n\nThe following edits are based on draft-ietf-http-v11-spec-rev-01.txt,\nwhich you might need to refer to when reviewing these edits.\n\n(1) In section 3.6 (Transfer Codings), after:\n       All transfer-coding values are case-insensitive. HTTP/1.1 uses\n       transfer coding values in the TE header field (section 14.48) and\n       in the Transfer-Encoding header field (section 14.40).\n\nAdd (as a new paragraph) [NORMATIVE CHANGES]:\nWhenever a transfer-coding other than \"identity\" is applied to\na message-body, the set of transfer-codings MUST include\n\"chunked\", unless the message is terminated by closing the\nconnection.  When the \"chunked\" transfer-coding is used, it\nMUST be the last transfer-coding applied to the message-body.\nThe \"chunked\" transfer-coding MUST NOT be applied more than\nonce to a message-body.  These rules allow the recipient to\ndetermine the transfer-length of the message (section 4.4).\n\n(2) In section 4.4 (Message Length), insert at the beginning:\nThe transfer-length of a message is the length of the\nmessage-body as it appears in the message; that is, after any\ntransfer codings have been applied.\n\nChange\nWhen a message-body is included with a message, the length of\nthat body is determined by one of the following (in order of\nprecedence):\nTo\nWhen a message-body is included with a message, the\ntransfer-length of that body is determined by one of\nthe following (in order of precedence):\n\nChange\n 2. If a Transfer-Encoding header field (section 14.40) is\n present and indicates that the \"chunked\" transfer coding has\n been applied, then the length is defined by the chunked\n encoding (section 3.6).\nTo\n 2. If a Transfer-Encoding header field (section 14.40) is\n present and indicates that any non-identity transfer\n coding has been applied, then the transfer-length is defined\n by use of the \"chunked\" transfer coding (section 3.6),\n unless the message is terminated by closing the connection.\n\nChange\n 3. If a Content-Length header field (section 14.14) is\n present, its decimal value in OCTETs represents the length of\n the message-body.\nTo\n 3. If a Content-Length header field (section 14.14) is\n present, its decimal value in OCTETs represents both the\n entity-length and the transfer-length.  The Content-Length\n header field MUST NOT be used if these two lengths are\n different (i.e., if a Transfer-Encoding header field is\n present).\n\nChange\n 4. If the message uses the media type \"multipart/byteranges\",\n which is self-delimiting, then that defines the length. This\n media type MUST NOT be used unless the sender knows that the\n recipient can parse it; the presence in a request of a Range\n header with multiple byte-range specifiers implies that the\n client can parse multipart/byteranges responses.\nTo\n 4. If the message uses the media type \"multipart/byteranges\"\n and the transfer-length is not otherwise specified, then this\n self-delimiting media type defines the transfer-length. This\n media type MUST NOT be used unless the sender knows that the\n recipient can parse it; the presence in a request of a Range\n header with multiple byte-range specifiers implies that the\n client can parse multipart/byteranges responses.\n\nChange\n       Messages MUST NOT include both a Content-Length header field and\n       the \"chunked\" transfer coding. If both are received, the\n       Content-Length MUST be ignored.\nTo\n       Messages MUST NOT include both a Content-Length header field and\n       a non-identity transfer coding.  If the message does include\n       a non-identity transfer code, the Content-Length header field\n       MUST be ignored.\n\n(3) Change section 7.2.2 from\n\n7.2.2 Length\n\nThe length of an entity-body is the length of the message-body\nafter any transfer codings have been removed. Section 4.4\ndefines how the length of a message-body is determined.\n\nTo\n7.2.2 Entity-Length\n\nThe entity-length of a message is the length of the\nmessage-body before any transfer codings have been applied.\nSection 4.4 defines how the transfer-length of a message-body\nis determined.\n\n(4) In section 13.5.2 (Non-modifiable Headers)\n\nChange [NORMATIVE CHANGE]:\nA cache or non-caching proxy MUST NOT modify any of the\nfollowing fields in a response:\n \n  .  Expires\n  .  Content-Length\n \nbut it may add any of these fields if not already present. If an\nExpires header is added, it MUST be given a field-value\nidentical to that of the Date header in that response. If a\nContent-Length header is added, it MUST correctly reflect the\nlength of the entity-body.\n \n  Note: a typical reason for adding the Content-Length header is\n  that the origin server sent the content chunked encoded.\n \nTo\nA cache or non-caching proxy MUST NOT modify any of the\nfollowing fields in a response:\n \n  .  Expires\n \nbut it may add any of these fields if not already present. If an\nExpires header is added, it MUST be given a field-value\nidentical to that of the Date header in that response.\n \nat the end of section 13.5.2, add this new paragraph:\n\nThe Content-Length field of a request or response is added or\ndeleted according to the rules in section 4.4.  A cache or\nnon-caching proxy MUST preserve the entity-length (section\n7.2.2) of the entity-body, although it MAY change the\ntransfer-length (section 4.4).\n\n(5) In section 14.14 (Content-Length)\n\nChange\nThe Content-Length entity-header field indicates the size of\nthe message-body, in decimal number of OCTETs, sent to the\nrecipient or, in the case of the HEAD method, the size of the\nentity-body that would have been sent had the request been a\nGET.\nTo\nThe Content-Length entity-header field indicates the size of\nthe entity-body, in decimal number of OCTETs, sent to the\nrecipient or, in the case of the HEAD method, the size of the\nentity-body that would have been sent had the request been a\nGET.\n\n[NB: there is exactly one word changed above.]\n\nChange\nApplications SHOULD use this field to indicate the size of the\nmessage-body to be transferred, regardless of the media type\nof the entity. It must be possible for the recipient to\nreliably determine the end of HTTP/1.1 requests containing an\nentity-body, e.g., because the request has a valid\nContent-Length field, uses Transfer-Encoding: chunked or a\nmultipart body.\nTo\nApplications SHOULD use this field to indicate the\ntransfer-length of the message-body, unless this is prohibited\nby the rules in section 4.4.\n\nChange\n Note: The meaning of this field is significantly different\n from the corresponding definition in MIME, where it is an\n optional field used within the \"message/external-body\"\n content-type. In HTTP, it SHOULD be sent whenever the\n message's length can be determined prior to being\n transferred.\nTo\n Note: The meaning of this field is significantly different\n from the corresponding definition in MIME, where it is an\n optional field used within the \"message/external-body\"\n content-type. In HTTP, it SHOULD be sent whenever the\n message's length can be determined prior to being\n transferred, unless this is prohibited by the rules in\n section 4.4.\n\n(6) In section 14.17 (Content-Range), change all occurrences\nof \"entity-length\" to \"instance-length\".  (The non-terminal\n\"entity-length\" appears nowhere else in the -rev-01 version\nof the HTTP/1.1 specification.)\n\n[end]\n\n\n\n"
        },
        {
            "subject": "state-man-mec0",
            "content": "[Cc-ed to http-wg as a courtesy.  Discussion to take place on http-state.]\n\nThe above I-D was released on Jan 27.  If there are no comments by the\nend of Friday, Feb 6, I would like to declare discussion of the\nprotocol-only aspects of the draft closed at that time.  Thereafter (or\nwhenever those discussions end), we will return to the discussion of\nthe privacy aspects, and discussion of the protocol aspects will be out\nof bounds.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: proposed solution for CONTENTLENGT",
            "content": "> On Tue, 03 Feb 98, Jeffrey Mogul (mogul@pa.dec.com) wrote:\n> \n[snip]\n> The following edits are based on draft-ietf-http-v11-spec-rev-01.txt,\n> which you might need to refer to when reviewing these edits.\n\nLooks good to me. I have one question, though.\n\n> (2) In section 4.4 (Message Length), insert at the beginning:\n>         The transfer-length of a message is the length of the\n>         message-body as it appears in the message; that is, after any\n>         transfer codings have been applied.\n[snip]\n> Change\n>          3. If a Content-Length header field (section 14.14) is\n>          present, its decimal value in OCTETs represents the length of\n>          the message-body.\n> To\n>          3. If a Content-Length header field (section 14.14) is\n>          present, its decimal value in OCTETs represents both the\n>          entity-length and the transfer-length.  The Content-Length\n>          header field MUST NOT be used if these two lengths are\n>          different (i.e., if a Transfer-Encoding header field is\n>          present).\n[snip]\n> Change\n>        Messages MUST NOT include both a Content-Length header field and\n>        the \"chunked\" transfer coding. If both are received, the\n>        Content-Length MUST be ignored.\n> To\n>        Messages MUST NOT include both a Content-Length header field and\n>        a non-identity transfer coding.  If the message does include\n>        a non-identity transfer code, the Content-Length header field\n>        MUST be ignored.\n\nI'm not particular about this, but is there a specific reason why the\nContent-Length header must not be sent with a non-identity t-e? I just\nthink it might be useful for a recipient if it can pre-allocate a\nbuffer of the correct size (assuming no Content-Encoding was applied).\nI think the rules are clear that the Content-Encoding should not be\nused to determine the length of the entity (i.e. the transfer length)\nwhen a non-identity t-e is present, and I can't see any compatibility\nproblems with older clients (they don't send the TE header), so I don't\nsee what harm the Content-length header would do (apart from adding a\nfew extra bytes to the headers).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "SSL tunneling statu",
            "content": "My notes from the 12/96 IETF meeting say that a new I-D would be\nprovided to move SSL tunneling to Proposed Standard.  Did that ever\nhappen?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: proposed solution for CONTENTLENGT",
            "content": "Is there any way to say the same thing without referring to \"identity\"\nas a transfer coding?  It is far less confusing to say \"no transfer coding\",\nin my opinion.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: proposed solution for CONTENTLENGT",
            "content": "Jeffrey Mogul wrote:\n> Change\n>          2. If a Transfer-Encoding header field (section 14.40) is\n>          present and indicates that the \"chunked\" transfer coding has\n>          been applied, then the length is defined by the chunked\n>          encoding (section 3.6).\n> To\n>          2. If a Transfer-Encoding header field (section 14.40) is\n>          present and indicates that any non-identity transfer\n>          coding has been applied, then the transfer-length is defined\n>          by use of the \"chunked\" transfer coding (section 3.6),\n>          unless the message is terminated by closing the connection.\n\nThis confused me ... \"indicates that any non-identity transfer coding\nhas been applied\" suggests that there is more than one identity transfer\nencoding but my understanding is that there is not. Would it not be\nclearer to say:\n\n          2. If a Transfer-Encoding header field (section 14.40) is\n          present and has any value other than \"identity\", then\nthe           transfer-length is defined\n          by use of the \"chunked\" transfer coding (section 3.6),\n          unless the message is terminated by closing the connection.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 735 0686|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 735 0689|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: proposed solution for CONTENTLENGT",
            "content": "Jeffrey Mogul wrote:\n> Change\n>          2. If a Transfer-Encoding header field (section 14.40) is\n>          present and indicates that the \"chunked\" transfer coding has\n>          been applied, then the length is defined by the chunked\n>          encoding (section 3.6).\n> To\n>          2. If a Transfer-Encoding header field (section 14.40) is\n>          present and indicates that any non-identity transfer\n>          coding has been applied, then the transfer-length is defined\n>          by use of the \"chunked\" transfer coding (section 3.6),\n>          unless the message is terminated by closing the connection.\n\nBen Laurie wrote:\n\n    This confused me ... \"indicates that any non-identity transfer coding\n    has been applied\" suggests that there is more than one identity transfer\n    encoding but my understanding is that there is not. Would it not be\n    clearer to say:\n    \n      2. If a Transfer-Encoding header field (section 14.40) is\n      present and has any value other than \"identity\", then\n              the transfer-length is defined\n      by use of the \"chunked\" transfer coding (section 3.6),\n      unless the message is terminated by closing the connection.\n    \nI think Ben's proposal makes sense, although I don't\nthink anyone would (or should) ever send\nTransfer-Encoding: identity\n\nalthough the spec doesn't explicitly say this.  It does say\nthat the \"identity\" content-coding \"SHOULD NOT be used in\nContent-Encoding header\", but the same statement is not repeated\nwith respect to the \"identity\" transfer-coding and Transfer-Encoding.\nHowever, I think this was Henrik's intention, and perhaps we need\nto add that statement.\n\nAt about the same time, Roy Fielding wrote:\n\n    Is there any way to say the same thing without referring to\n    \"identity\" as a transfer coding?  It is far less confusing to say\n    \"no transfer coding\", in my opinion.\n    \nalthough I don't think Roy had seen Ben's message (judging from the\nDate headers on their messages, if I did the math right).\n\nUnless we are going to say that \"identity\" \"MUST NOT be used\nin a Transfer-Encoding header\" then I think Ben's is the clearest\nway to put things.  If we were to ban the sending of\nTransfer-Encoding: identity\nthen I guess we could simply say\n      2. If a Transfer-Encoding header field (section 14.40) is\n      present, then the transfer-length is defined\n      by use of the \"chunked\" transfer coding (section 3.6),\n      unless the message is terminated by closing the connection.\n    \nBut I think there is no specific reason to include an outright\nban on \"Transfer-Encoding: identity\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Removing ContentBas",
            "content": "The MHTML group will probably want to handle Content-Base in the same\nway as the HTTP group. We understand that the HTTP group has decided\nto remove the Content-Base header field, and we will probably do the\nsame.\n\nQuestion: RFC 2068 did contain a Content-Base header. How will\nthis circumstance be handled in the new HTTP standard?\n\n(1) Write nothing at all about Content-Base.\n\n(2) Mention somewhere that an earlier version of this standard\n    had a header Content-Base hwich has been removed.\n\n(3) Indicate Content-Base as an obsolete syntax, which servers\n    are recommended to accept for receipt during an interim\n    period, but not use in new HTTP headers.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Removing ContentBas",
            "content": "Jacob Palme <jpalme@dsv.su.se> wrote:\n>The MHTML group will probably want to handle Content-Base in the same\n>way as the HTTP group. We understand that the HTTP group has decided\n>to remove the Content-Base header field, and we will probably do the\n>same.\n>\n>Question: RFC 2068 did contain a Content-Base header. How will\n>this circumstance be handled in the new HTTP standard?\n>\n>(1) Write nothing at all about Content-Base.\n>\n>(2) Mention somewhere that an earlier version of this standard\n>    had a header Content-Base hwich has been removed.\n>\n>(3) Indicate Content-Base as an obsolete syntax, which servers\n>    are recommended to accept for receipt during an interim\n>    period, but not use in new HTTP headers.\n\nThis presumeably also should be addressed in the URL -> URI\ndraft before that becomes a Draft Standard.  It includes Content-Base\nwith a diagram and textual description of the priorities for using\nit versus Content-Location, versus headers or a BASE element in the\nbody, for the base, parallel to what is in the MHTML Proposed Standard.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Server initiated connection",
            "content": "Hello,\n\nI understand that HTTP is a request/response protocol that normally\nworks over a single connection where the client (browser) creates the\nconnection and the server closes it after sending the response.\n\nMy question is: does the server _ever_ initiate a connection to a client\nthat requires that the client to have previously established a listener\nsocket or does the HTTP server _always_ return responses over the\nconnection that was established by the client?\n\nThanks in advance,\nWyatt Williams\niCall inc.\n864-654-4322\n\n\n\n"
        },
        {
            "subject": "Re: Server initiated connection",
            "content": "On Mon, Feb 09, 1998 at 03:34:04AM +0000, Wyatt Williams wrote:\n> My question is: does the server _ever_ initiate a connection to a client\n> that requires that the client to have previously established a listener\n> socket or does the HTTP server _always_ return responses over the\n> connection that was established by the client?\n\nThis is something that I also have looked for.  From what I've read from the\n1.1 RFC and the relevant implementations, this is not done yet.\n\nI do not think that there exists any client that can accept server initiated\nconnections yet...\n--\nYiorgos Adamopoulos         -- #include <std/disclaimer.h>\nadamo@dblab.ece.ntua.gr     -- Knowledge and Data Base Systems Laboratory, NTUA\n\n\n\n"
        },
        {
            "subject": "Re: Removing ContentBas",
            "content": "I removed it from the main section.\n\nAdded a line or two in the \"changes since RFC 2068\" section I wrote.  As\nit wasn't typically implemented, not much had to be said about it.\n\nAnd put an index entry to RFC 2068 in the index, as well as indexing\nit in line.\n- Jim\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "New Proposed Chunked Rule",
            "content": "  In addtion to the change to chunk length syntax, we discussed in\n  Memphis clarifying the rules on what 'headers' are valid in the\n  footer of a chunk encoded response.  Merging these two (because the\n  affect the same paragraphs in the spec), I propose that the\n  following text replace a portion of section 3.6 beginning with the\n  chunked encoding syntax up to but not including the paragraph which\n  begins \"An example process for\":\n\n  ================================================================\n\n       Chunked-Body   = *chunk\n                        last-chunk\n                        footer\n                        CRLF\n\n       chunk          = chunk-size [ chunk-ext ] CRLF\n                        chunk-data CRLF\n\n       chunk-size     = 1*HEX\n\n       last-chunk     = 1*(\"0\") [ chunk-ext ] CRLF\n\n       chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] )\n       chunk-ext-name = token\n       chunk-ext-val  = token | quoted-string\n       chunk-data     = chunk-size(OCTET)\n\n       footer         = *entity-header\n\n  The chunked encoding is ended by any chunk whose size is zero,\n  followed by the footer, which is terminated by an empty line.  The\n  purpose of the footer is to provide an efficient way to supply\n  information about an entity that is generated dynamically.\n  Applications MUST NOT send header fields in the footer which are not\n  explicitly defined as being appropriate for the footer.\n\n  The Content-MD5 header (section 14.16) is appropriate for the footer.\n\n  The Authentication-Info header defined by RFC 2069 (An Extension to\n  HTTP: Digest Access Authentication), or its successor is appropriate\n  for the footer.\n\n  ================================================================\n\n  The modified syntax above also allows a chunk-ext on the zero length\n  last-chunk, which I added for symmetry, and for the length of the\n  final chunk to be multiple digits as long as they are all zero.\n\n  There has been some discussion of whether or not LWS is allowed\n  in the chunk length line.  I believe that the following, taken from\n  section 2.1 of the current spec, specifically allows LWS:\n\n    implied *LWS\n         The grammar described by this specification is word-based. Except\n         where noted otherwise, linear whitespace (LWS) can be included\n         between any two adjacent words (token or quoted-string), and\n         between adjacent tokens and delimiters (tspecials), without\n         changing the interpretation of a field. At least one delimiter\n         (tspecials) must exist between any two tokens, since they would\n         otherwise be interpreted as a single token.\n\n  My reading is that all of the following would be valid:\n\n11CRLF\n12 CRLF\n13;foo=barCRLF\n14;foo=bar CRLF\n15; foo=barCRLF\n16; foo=bar CRLF\n17 ; foo=barCRLF\n18 ; foo=bar CRLF\n\n  I do not believe that the rule covers (or needs to cover) LWS\n  preceeding the length (that is not a boundary between a word and\n  either another word or a tspecial), especially now that we allow\n  leading zeros.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "revised proposed solution for CONTENTLENGT",
            "content": "[Updated with minor change from\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1998q1/0343.html]\n\nMost of the changes below are editorial, in that they clarify\nthe language of the specification but don't actually make any\nnormative changes.  The only real normative change is that\nwe require the use \"chunked\" to delimit any other kind of\ntransfer-coding.  We also remove the \"proxies MUST NOT\nmodify Content-Length\" rule, which makes no sense when\ncombined with (for example) \"Transfer-Encoding: gzip\".\n\nI think we need to clean up the terminology in 14.36.1 (Byte Ranges),\nbut that's probably a separate editorial issue.\n\nThe following edits are based on draft-ietf-http-v11-spec-rev-01.txt,\nwhich you might need to refer to when reviewing these edits.\n\n(1) In section 3.6 (Transfer Codings), after:\n       All transfer-coding values are case-insensitive. HTTP/1.1 uses\n       transfer coding values in the TE header field (section 14.48) and\n       in the Transfer-Encoding header field (section 14.40).\n\nAdd (as a new paragraph) [NORMATIVE CHANGES]:\nWhenever a transfer-coding other than \"identity\" is applied to\na message-body, the set of transfer-codings MUST include\n\"chunked\", unless the message is terminated by closing the\nconnection.  When the \"chunked\" transfer-coding is used, it\nMUST be the last transfer-coding applied to the message-body.\nThe \"chunked\" transfer-coding MUST NOT be applied more than\nonce to a message-body.  These rules allow the recipient to\ndetermine the transfer-length of the message (section 4.4).\n\n(2) In section 4.4 (Message Length), insert at the beginning:\nThe transfer-length of a message is the length of the\nmessage-body as it appears in the message; that is, after any\ntransfer codings have been applied.\n\nChange\nWhen a message-body is included with a message, the length of\nthat body is determined by one of the following (in order of\nprecedence):\nTo\nWhen a message-body is included with a message, the\ntransfer-length of that body is determined by one of\nthe following (in order of precedence):\n\nChange\n 2. If a Transfer-Encoding header field (section 14.40) is\n present and indicates that the \"chunked\" transfer coding has\n been applied, then the length is defined by the chunked\n encoding (section 3.6).\nTo\n 2. If a Transfer-Encoding header field (section 14.40) is\n present and has any value other than \"identity\", then the\n transfer-length is defined by use of the \"chunked\" transfer\n coding (section 3.6), unless the message is terminated by\n closing the connection.\n\nChange\n 3. If a Content-Length header field (section 14.14) is\n present, its decimal value in OCTETs represents the length of\n the message-body.\nTo\n 3. If a Content-Length header field (section 14.14) is\n present, its decimal value in OCTETs represents both the\n entity-length and the transfer-length.  The Content-Length\n header field MUST NOT be used if these two lengths are\n different (i.e., if a Transfer-Encoding header field is\n present).\n\nChange\n 4. If the message uses the media type \"multipart/byteranges\",\n which is self-delimiting, then that defines the length. This\n media type MUST NOT be used unless the sender knows that the\n recipient can parse it; the presence in a request of a Range\n header with multiple byte-range specifiers implies that the\n client can parse multipart/byteranges responses.\nTo\n 4. If the message uses the media type \"multipart/byteranges\"\n and the transfer-length is not otherwise specified, then this\n self-delimiting media type defines the transfer-length. This\n media type MUST NOT be used unless the sender knows that the\n recipient can parse it; the presence in a request of a Range\n header with multiple byte-range specifiers implies that the\n client can parse multipart/byteranges responses.\n\nChange\n       Messages MUST NOT include both a Content-Length header field and\n       the \"chunked\" transfer coding. If both are received, the\n       Content-Length MUST be ignored.\nTo\n       Messages MUST NOT include both a Content-Length header field and\n       a non-identity transfer coding.  If the message does include\n       a non-identity transfer code, the Content-Length header field\n       MUST be ignored.\n\n(3) Change section 7.2.2 from\n\n7.2.2 Length\n\nThe length of an entity-body is the length of the message-body\nafter any transfer codings have been removed. Section 4.4\ndefines how the length of a message-body is determined.\n\nTo\n7.2.2 Entity-Length\n\nThe entity-length of a message is the length of the\nmessage-body before any transfer codings have been applied.\nSection 4.4 defines how the transfer-length of a message-body\nis determined.\n\n(4) In section 13.5.2 (Non-modifiable Headers)\n\nChange [NORMATIVE CHANGE]:\nA cache or non-caching proxy MUST NOT modify any of the\nfollowing fields in a response:\n \n  .  Expires\n  .  Content-Length\n \nbut it may add any of these fields if not already present. If an\nExpires header is added, it MUST be given a field-value\nidentical to that of the Date header in that response. If a\nContent-Length header is added, it MUST correctly reflect the\nlength of the entity-body.\n \n  Note: a typical reason for adding the Content-Length header is\n  that the origin server sent the content chunked encoded.\n \nTo\nA cache or non-caching proxy MUST NOT modify any of the\nfollowing fields in a response:\n \n  .  Expires\n \nbut it may add any of these fields if not already present. If an\nExpires header is added, it MUST be given a field-value\nidentical to that of the Date header in that response.\n \nat the end of section 13.5.2, add this new paragraph:\n\nThe Content-Length field of a request or response is added or\ndeleted according to the rules in section 4.4.  A cache or\nnon-caching proxy MUST preserve the entity-length (section\n7.2.2) of the entity-body, although it MAY change the\ntransfer-length (section 4.4).\n\n(5) In section 14.14 (Content-Length)\n\nChange\nThe Content-Length entity-header field indicates the size of\nthe message-body, in decimal number of OCTETs, sent to the\nrecipient or, in the case of the HEAD method, the size of the\nentity-body that would have been sent had the request been a\nGET.\nTo\nThe Content-Length entity-header field indicates the size of\nthe entity-body, in decimal number of OCTETs, sent to the\nrecipient or, in the case of the HEAD method, the size of the\nentity-body that would have been sent had the request been a\nGET.\n\n[NB: there is exactly one word changed above.]\n\nChange\nApplications SHOULD use this field to indicate the size of the\nmessage-body to be transferred, regardless of the media type\nof the entity. It must be possible for the recipient to\nreliably determine the end of HTTP/1.1 requests containing an\nentity-body, e.g., because the request has a valid\nContent-Length field, uses Transfer-Encoding: chunked or a\nmultipart body.\nTo\nApplications SHOULD use this field to indicate the\ntransfer-length of the message-body, unless this is prohibited\nby the rules in section 4.4.\n\nChange\n Note: The meaning of this field is significantly different\n from the corresponding definition in MIME, where it is an\n optional field used within the \"message/external-body\"\n content-type. In HTTP, it SHOULD be sent whenever the\n message's length can be determined prior to being\n transferred.\nTo\n Note: The meaning of this field is significantly different\n from the corresponding definition in MIME, where it is an\n optional field used within the \"message/external-body\"\n content-type. In HTTP, it SHOULD be sent whenever the\n message's length can be determined prior to being\n transferred, unless this is prohibited by the rules in\n section 4.4.\n\n(6) In section 14.17 (Content-Range), change all occurrences\nof \"entity-length\" to \"instance-length\".  (The non-terminal\n\"entity-length\" appears nowhere else in the -rev-01 version\nof the HTTP/1.1 specification.)\n\n[end]\n\n\n\n"
        },
        {
            "subject": "new editorial (?) issue: IDENTITYT",
            "content": "This is implicit in the current (rev-01) draft, but I think it\ncould be made explicit:\n\nIn section 3.6 (Transfer Codings), after:\n       The Internet Assigned Numbers Authority (IANA) acts as a\n       registry for transfer-coding value tokens. Initially, the\n       registry contains the following tokens: \"chunked\" (section\n       3.6.1), \"identity\" (section 3.6.2), \"gzip\" (section 3.5),\n       \"compress\" (section 3.5), and \"deflate\" (section 3.5).\n\nAdd this paragraph:\n\n       The \"identity\" transfer-coding is used only in the TE\n       header, and SHOULD NOT be used in the Transfer-Encoding\n       header.\n\nAlso, in section 3.5, there's a minor grammatical error in the\nexisting analogous statement:\n\n       identity\n    The default (identity) encoding; the use of no\n    transformation whatsoever. This content-coding is used only\n    in the Accept-Encoding header, and SHOULD NOT be used in\n    Content-Encoding header.\n\nshould be:\n\n       identity\n    The default (identity) encoding; the use of no\n    transformation whatsoever. This content-coding is used only\n    in the Accept-Encoding header, and SHOULD NOT be used in\n    the Content-Encoding header.\n            ^^^\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Test Day 1998-021",
            "content": "  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-state-man-mec07.txt,.p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group \nof the IETF.\n\nTitle: HTTP State Management Mechanism\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-07.txt,.ps\nPages: 19\nDate: 10-Feb-98\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n\nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-07.txt\".\nA URL for the Internet-Draft is:\nftp://ftp.ietf.org/internet-drafts/draft-ietf-http-state-man-mec-07.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-07.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "state-man-mec0",
            "content": "The latest version of the cookie spec. is available via the\nInternet Drafts directory and from my usual repository:\nhttp://portal.research.bell-labs.com/~dmk/cookie-ver.html\n\nCopies are available with change bars from the previous I-D and from\nRFC 2109.\n\nstate-man-mec-07 is the culmination of the discussions concerning\nthe \"wire protocol\" aspects of RFC 2109+.  Further discussion will\nfocus on privacy/security aspects ONLY.  I hope to submit a new\nI-D soon that restores the words for those pieces.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "This may be a stupid question ..",
            "content": "We are developing a transparent web proxy, such that a browser does not\nnecessarily know that it is sending requests through a proxy. I have\ninherited the Web proxy code from a contractor who left. It turns out,\nthat our Proxy only supports HTTP/1.0 and will reject any requests for\nHTTP/1.1  Looking at the Netscape Proxy server, it seems to have some\nway of 'downgrading' the Browser to HTTP/1.0 as a stopgap measure. \n\nMy question: Is there a document / RFC somwhere which explains how we\nare supposed to do this ? \n\nI haven't been able to find anything so far.\n\nNick\n\n\"Hey, Cool. When you press the black button labelled in black on a black\nbackground, a black light lights up black to let you know you've done\nit!\"\n-- Zaphod Beeblebrox\n\n\n\n"
        },
        {
            "subject": "Re: This may be a stupid question ..",
            "content": "Nick Ambrose wrote:\n> \n> We are developing a transparent web proxy, such that a browser does not\n> necessarily know that it is sending requests through a proxy. I have\n> inherited the Web proxy code from a contractor who left. It turns out,\n> that our Proxy only supports HTTP/1.0 and will reject any requests for\n> HTTP/1.1  Looking at the Netscape Proxy server, it seems to have some\n> way of 'downgrading' the Browser to HTTP/1.0 as a stopgap measure.\n> \n> My question: Is there a document / RFC somwhere which explains how we\n> are supposed to do this ?\n> \n> I haven't been able to find anything so far.\n\nYou can't write a transparent HTTP/1.1 proxy.  HTTP/1.1 makes explicit\nprovisions for proxies which requires the client to be aware whether or\nnot a proxy is in between.\n\n-- \nAri Luotonen, Mail-Stop MV-068Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://people.netscape.com/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: This may be a stupid question ..",
            "content": "[snip]\n> It turns out,\n> that our Proxy only supports HTTP/1.0 and will reject any requests for\n> HTTP/1.1  Looking at the Netscape Proxy server, it seems to have some\n> way of 'downgrading' the Browser to HTTP/1.0 as a stopgap measure. \n\nIt is more than a stopgap measure - it is a must (for HTTP/1.0 proxies).\n\n> My question: Is there a document / RFC somwhere which explains how we\n> are supposed to do this ?\n\nYou might find\nftp://ds.internic.net/internet-drafts/draft-ietf-http-versions-01.txt\ninstructive. Basically you must down- or upgrade both the request and\nthe response to the highest version you're conditionally compliant with.\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "SECCACHING editorial issue..",
            "content": "Larry is worried we don't say enough explicitly in security considerations\non the threats that proxy caching represent...\n\nI just drafted and added this text to Rev-02 in preparation (expect by\nthe end of next week).\n- Jim\n\n\n15.7 Proxy Caching\n\nBy their very nature, HTTP proxies and proxy caches are men-in-the-middle, \nand open up clients to men-in-the-middle attacks. Compromise of the systems \non which the proxies run can result in both serious security and privacy \nproblems. Operators of HTTP proxy caches should treat the systems on which \nthe proxies run as very sensitive systems, since both personal information \nand security related information usually present in the proxies, and all \nsorts of potential attacks on clients are possible from such systems. \n\nLog information gathered at such proxies often contains highly sensitive \npersonal information, and should be carefully guarded and appropriate \nguidelines for use developed and followed. (Section 15.1.1). \n\nUsers of proxy caches need to be aware that they are no more trustworthy \nthan the people who run the proxy caches; HTTP itself cannot solve this \nproblem.\n\n--\nJim Gettys\nIndustry Standards and Consortia\nDigital Equipment Corporation\nVisting Scientist, World Wide Web Consortium, M.I.T.\nhttp://www.w3.org/People/Gettys/\njg@w3.org, jg@pa.dec.com\n\n\n\n"
        },
        {
            "subject": "Re: SECCACHING editorial issue..",
            "content": "Jim writes:\n\n    15.7 Proxy Caching\n    \n    By their very nature, HTTP proxies and proxy caches are men-in-the-middle, \n    and open up clients to men-in-the-middle attacks. Compromise of the systems \n    on which the proxies run can result in both serious security and privacy \n    problems. Operators of HTTP proxy caches should treat the systems on which \n    the proxies run as very sensitive systems, since both personal information \n    and security related information usually present in the proxies, and all \n    sorts of potential attacks on clients are possible from such systems. \n    \n    Log information gathered at such proxies often contains highly sensitive \n    personal information, and should be carefully guarded and appropriate \n    guidelines for use developed and followed. (Section 15.1.1). \n    \n    Users of proxy caches need to be aware that they are no more trustworthy \n    than the people who run the proxy caches; HTTP itself cannot solve this \n    problem.\n\nI'd suggest re-writing this to make it clear that the problem is\nrelated primarily to the use of proxies, and not just to the use of caching:\n\n    15.7 Proxies and proxy caches\n\n    By their very nature, HTTP proxies are men-in-the-middle, and may\n    represent an opportunity for man-in-the-middle attacks. Compromise\n    of the systems on which the proxies run can result in serious\n    security and privacy problems.  Proxies have access to\n    security-related information, personal information about individual\n    users and organizations, and proprietary information belonging to\n    users and content providers.  A compromised proxy, or a proxy\n    implemented or configured without regard to security and privacy\n    considerations, might be used in the commission of a wide range of\n    potential attacks.\n\n    Proxy operators should protect the systems on which proxies run as\n    they would protect any system that contains or transports sensitive\n    information.  In particular, log information gathered at proxies\n    often contains highly sensitive personal information, and/or\n    information about organizations.  Log information should be\n    carefully guarded, and appropriate guidelines for use developed and\n    followed. (Section 15.1.1).\n\n    Caching proxies provide additional potential vulnerabilities, since\n    the contents of the cache represent an attractive target for\n    malicious exploitation.  Because cache contents persist after an\n    HTTP request is complete, an attack on the cache may reveal\n    information long after a user believes that the information has\n    been removed from the network.  Therefore, cache contents should\n    be protected as sensitive information.\n\n    Proxy implementors should consider the privacy and security\n    implications of their design and coding decisions, and of the\n    configuration options they provide to proxy operators (especially\n    the default configuration).\n\n    Users of a proxy need to be aware that they are no more trustworthy\n    than the people who run the proxy; HTTP itself cannot solve this\n    problem.\n\nI would suggest adding:\n\n    The judicious use of cryptography, when appropriate, may suffice\n    to protect against a broad range of security and privacy attacks.\n    Such cryptography is beyond the scope of the HTTP/1.1 specification.\n\nif people don't think this is going too far out on a political limb.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "If CGI outgput is spooled, doesn't that defeat the purpose of chunked\nencoding for CGI output?\n\nI share your concern about errors that occur after the 200 response. I\ncan't think of any way to signal a problem except to close the connection\nbefore the final chunk length of 0 is transmitted. This would work, of\ncourse, but it would be nice if the server could give some indication of\nthe nature of the problem.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Security considerations from RE-AUTHENTICATIONREQUESTE",
            "content": "I've pulled Paul's proposal from Rev-02 for RE-AUTHENTICATION-REQUESTED\nper the discussion in Washington and the mailing list.  The lack\nof this facility does need discussion in the Security Considerations\nsection, however.  So I had an editorial task to generate such a section.\n\nHere's my crack at drafting such a section.  Comments welcome (for a short\nwhile, anyway...).\n- Jim\n\n15.6 15.6 Authentication Credentials and Idle Clients\n\nExisting HTTP clients typically retain authentication information \nindefinately. HTTP/1.1 lacks a facility to force reauthentication of clients, \nwhich may have been idle for extended periods, by an origin server or \na proxy. This is considered a significant defect that requires further \nadditions to HTTP, and is under separate study. There are a number of \nwork-arounds to parts of this problem, and we encourage the use of password \nprotected screen savers on idle clients to mitigate some of the resulting \nsecurity problems.\n\n\n\n"
        },
        {
            "subject": "CFP: HTTPNG Workshop at WWW",
            "content": "*** CALL FOR PARTICIPATION ***\n\n\"Towards a New Generation of HTTP\"\nA workshop on global hypermedia infrastructure\nHeld in Conjunction with the 7th Int'l WWW Conference\nApril 14th, 1998 - Brisbane, Australia\nhttp://www.ics.uci.edu/pub/httpng/\n\nABSTRACT\n\nCurrent Web infrastructure, based on HTML, URIs, and HTTP has created\na dynamic, vibrant global hypermedia information space.  As groups\nrush to add diverse facilities such as document management and\nprinting, both locally and globally the extensibility limitations of\nthe current infrastructure are exposed. The inertia of the installed\nbase makes the key technical question how to gracefully evolve the web\nto include these and other next-generation services.  This workshop\nprovides a timely opportunity to collect researchers and practitioners\nfrom the Web and Hypermedia communities to broadly consider the future\ninfrastructure of the global hypermedia information space.\n\n\nDESCRIPTION\n\nThe Web community has a broad agenda for the future evolution of Web\ninfrastructure.  Today's infrastructure, based on the triad of HTML,\nURIs, and HTTP, has created the main platform for a large and diverse\nset of applications providing mission critical services for a\ndramatically growing Web community.  But, the existing infrastructure\nis starting to show its limitations, a victim of its success.  The\ngrowing complexity of often undefined or even unintended (or even\nconflicting) interactions between extensions has become a threat to\nthe future evolution of the Web.\n\nOver the next year or so there will be significant, concrete progress\non assessing the limitations of HTTP/1.x and designing extensions to\naddress them. Some of these areas include performance engineering,\nsupport for extensibility, distributed authoring and versioning,\nasynchronous notification, distributed object interfaces, tuning for\nembedded Web devices, and realtime multimedia support.\n\nThis workshop offers a timely opportunity to collect researchers and\npractitioners from the Web and Hypermedia communities to broadly\nconsider the future infrastructure of the global hypermedia\ninformation space.  With over 1.8 million web servers fielded on the\nInternet, there is tremendous inertia in the existing Web\ninfrastructure.  Adoption of new Web infrastructure will be slow, and\nits effects long-lasting. New infrastructure must be significantly\nbetter in all critical aspects, meeting the needs of a broad spectrum\nof users, while resolving existing problems of the current\ninfrastructure and providing a solid, yet extensible foundation for\nfuture growth.\n\nThere are many visions for the future web infrastructure being\ndeveloped today:\n\n* The IETF WebDAV working group is an existing effort working to\n  define the future of the Web. WebDAV, World Wide Web Distributed\n  Authoring and Versioning, is working to extend the HTTP/1.x protocol\n  to support remote, asynchronous, collaborative authoring of Web\n  documents. WebDAV provides an infrastructure for creating locks,\n  recording properties on resources, collection and namespace\n  operations, and versioning. Lessons learned from this\n  effort will inform the development of HTTP-NG.\n\n* Simplifying HTTP implementations and multiplexing concurrent\n  connections were the twin goals of Simon Spero's 1994-5 era Session\n  Control Protocol (SCP) and ASN.1-encoded binary HTTP-NG. Both aspects\n  of his proposal have been quietly explored in the research community\n  while HTTP/1.1 standardization was in the foreground.\n\n* The Open Hypermedia community has developed the Open Hypermedia\n  Protocol (OHP) as a communications path between a constellation\n  of hypermedia aware applications on a user's machine, and a hypermedia\n  server.  OHP raises key issues, questioning why the browser is the\n  nexus for a user's hypertext interaction, and what services are necessary\n  to extend the Web's hypermedia capabilities.\n\n* The Web community and the TCP community are coming together to solve\n  the problem of how to prevent the often highly congested links on\n  the Internet.  Recent studies have shown the importance of\n  collaboration between all network layers, from IP, to the applications\n  layer, even extending to Web content.\n\n* W3C officially launched an HTTP-NG activity in July, 1997.  This\n  project will be investigating issues (focusing on simplicity and\n  extensibility) relevant to the next generation of HTTP, led by\n  Henrik Frystyk Nielsen and Jim Gettys.  The goal of this project\n  is to investigate whether a generic distributed object system can\n  form a reliable foundation for the Web.  Complementing this effort\n  to build the Web on top of a distributed object infrastructure, in\n  August 1997, the IETF held a BOF session for HTTP directions in\n  Munich.\n\nWe believe the HTTP-NG development process is beginning to coalesce\nand the WWW7 conference in April 1998 is an ideal point to discuss\ntechnical and political influences on the future of HTTP.  WWW7 offers\na unique, neutral forum to discuss these issues within the Web\ncommunity, a forum distinct from existing next generation development\nand standardization processes.\n\nPARTICIPATING IN THE WORKSHOP\n\nThis workshop solicits participation from individuals who wish to\ndiscuss the future infrastructure of the World Wide Web.\nSpecifically, Web and Hypertext practitioners and researchers are\ninvited to participate.\n\nParticipants must submit either a 2-4 page position paper or a\nfull-length paper on an issue directly related to the next\ngeneration hypermedia infrastructure.\n\nTopics of interest include, but are not restricted to:\n  - requirements for global hypermedia infrastructure\n  - prototypes and case studies of innovative hypermedia protocols\n  - requirements based on evolution of network-layer protocols\n  - architectural models and approaches\n  - scenarios of use of next generation hypermedia services\n  - adoption and deployment issues\n  - standards process issues\n\nSubmission Deadline: March 26, 1998\nAcceptance: Rolling acceptance, notification will be mailed within a week\nFinal Submissions: April 8, 1998\n\nPlease submit your paper via email to the workshop organizers at\n<http-future@xent.ics.uci.edu> in either HTML 3.2 or Adobe PDF format\nby March 26, 1998.  Acceptance notification will be made on a rolling\nbasis within a week of your submission.\n\n\nRELATED PAPERS\n\nWorkshop participants will find the following materials to be helpful\nbackground for the workshop.\n\nW3C's HyperText Transfer Protocol Overview \nhttp://www.w3.org/Protocols/\n\nW3C's HyperText Transfer Protocol - Next Generation Overview \nhttp://www.w3.org/Protocols/HTTP-NG/\n\nW3C's Protocol Extension Protocol for HTTP Overview \nhttp://www.w3.org/Protocols/PEP/\n\nIETF WebDAV Working Group \nhttp://www.ics.uci.edu/pub/ietf/webdav/\n\nOpen Hypermedia Protocol\nhttp://www.csdl.tamu.edu/ohs/\n\nW3C Note on Network Performance Effects of HTTP/1.1, CSS1, and PNG\nhttp://www.w3.org/Protocols/HTTP/Performance/Pipeline.html\n\n\nORGANIZING COMMITTEE\n\nWorkshop Chairs\n\n* Rohit Khare, U.C. Irvine, <rohit@uci.edu>\n* Jim Whitehead, U.C. Irvine, <ejw@ics.uci.edu>\n* Henrik Nielsen, World Wide Web Consortium, <frystyk@w3.org>\n\nWorkshop Committee\n\n* Roy Fielding, U.C. Irvine, <fielding@ics.uci.edu>\n* Henry Sanders, Microsoft, <henrysa@microsoft.com>\n\n\nWORKSHOP REGISTRATION\n\nInformation on the WWW7 conference can be found at:\n\nhttp://www7.conf.au/\n\nInformation on this workshop can be found at:\n\nhttp://www.ics.uci.edu/pub/httpng/\n\nInformation on registering for the workshop once you have been\naccepted can be found at:\n\nhttp://www.webventures.com.au/javelin/jssi/www7/welcome.jhtml\n\n\n\n"
        },
        {
            "subject": "MULTIPART issu",
            "content": "I think this is the most appropriate set of changes needed to resolve\nthe confusion regarding multiparts (and other MIMEness) in HTTP.\nIn some cases this is walking a tightrope, but that's better than\nfalling off the tightrope.  I don't think it changes the protocol at all,\njust the words used to describe it (and the addition of application/http).\n\n....Roy\n\np.s. I also fixed some minor typos (and my address) in the process\n===========================================================\n*** draft-ietf-http-v11-spec-rev-01.txtThu Feb 12 15:17:03 1998\n--- multidraft.txtThu Feb 12 18:00:38 1998\n***************\n*** 431,443 ****\n   19.2 Internet Media Type multipart/byteranges ..........152\n   19.3 Tolerant Applications .............................152\n   1.4  Differences Between HTTP Entities and RFC 2045 Entities   153\n!   1.4.1  Conversion to Canonical Form ...................153\n!   1.4.2  Conversion of Date Formats .....................154\n!   1.4.3  Introduction of Content-Encoding ...............154\n!   1.4.4  No Content-Transfer-Encoding ...................154\n!   1.4.5  HTTP Header Fields in Multipart Body-Parts .....155\n    1.4.6  Introduction of Transfer-Encoding ..............155\n-   1.4.7  MIME-Version ...................................155\n   1.5  Changes from HTTP/1.0 .............................156\n    1.5.1  Changes to Simplify Multi-homed Web Servers and Conserve IP\n    Addresses .............................................156\n--- 431,442 ----\n   19.2 Internet Media Type multipart/byteranges ..........152\n   19.3 Tolerant Applications .............................152\n   1.4  Differences Between HTTP Entities and RFC 2045 Entities   153\n!   1.4.1  MIME-Version ...................................155\n!   1.4.2  Conversion to Canonical Form ...................153\n!   1.4.3  Conversion of Date Formats .....................154\n!   1.4.4  Introduction of Content-Encoding ...............154\n!   1.4.5  No Content-Transfer-Encoding ...................154\n    1.4.6  Introduction of Transfer-Encoding ..............155\n   1.5  Changes from HTTP/1.0 .............................156\n    1.5.1  Changes to Simplify Multi-homed Web Servers and Conserve IP\n    Addresses .............................................156\n***************\n*** 1435,1445 ****\n  Parameter values may or may not be case-sensitive, depending on the\n  semantics of the parameter name. Linear white space (LWS) MUST NOT be\n  used between the type and subtype, nor between an attribute and its\n! value. User agents that recognize the media-type MUST process (or\n! arrange to be processed by any external applications used to process\n! that type/subtype by the user agent) the parameters for that MIME type\n! as described by that type/subtype definition to the and inform the user\n! of any problems discovered.\n  \n    Note: some older HTTP applications do not recognize media type\n    parameters. When sending data to older HTTP applications,\n--- 1434,1442 ----\n  Parameter values may or may not be case-sensitive, depending on the\n  semantics of the parameter name. Linear white space (LWS) MUST NOT be\n  used between the type and subtype, nor between an attribute and its\n! value. The presence or absence of a parameter may be significant to\n! the processing of a media-type, depending on its definition within the\n! media type registry.\n  \n    Note: some older HTTP applications do not recognize media type\n    parameters. When sending data to older HTTP applications,\n***************\n*** 1502,1525 ****\n  CRLF to represent line breaks between body-parts. Unlike in RFC 2046,\n  the epilogue of any multipart message MUST be empty; HTTP applications\n  MUST NOT transmit the epilogue (even if the original multipart contains\n! an epilogue).\n! \n! \n! Fielding, et al                                    [Page 26]\n! \n! \n! INTERNET-DRAFT            HTTP/1.1 Friday, November 21, 1997\n! \n! \n! In HTTP, multipart body-parts MAY contain header fields which are\n! significant to the meaning of that part. A Content-Location header field\n! (section 14.15) SHOULD be included in the body-part of each enclosed\n! entity that can be identified by a URL.\n! \n! In general, an HTTP user agent SHOULD follow the same or similar\n! behavior as a MIME user agent would upon receipt of a multipart type. If\n! an application receives an unrecognized multipart subtype, the\n! application MUST treat it as being equivalent to \"multipart/mixed\".\n  \n    Note: The \"multipart/form-data\" type has been specifically defined\n    for carrying form data suitable for processing via the POST request\n--- 1499,1520 ----\n  CRLF to represent line breaks between body-parts. Unlike in RFC 2046,\n  the epilogue of any multipart message MUST be empty; HTTP applications\n  MUST NOT transmit the epilogue (even if the original multipart contains\n! an epilogue).  These restrictions exist in order to preserve the\n! self-delimiting nature of a multipart message-body, wherein the \"end\"\n! of the message-body is indicated by the ending multipart boundary.\n! \n! In general, HTTP treats a multipart message-body no differently than any\n! other media type: as payload.  The one exception is the\n! \"multipart/byteranges\" type (appendix 19.2) when it appears in a 206\n! (Partial Content) response, which will be interpreted by some HTTP\n! caching mechanisms as described in sections 13.5.4 and 14.17.\n! In all other cases, an HTTP user agent SHOULD follow the same\n! or similar behavior as a MIME user agent would upon receipt of a\n! multipart type. If an application receives an unrecognized multipart\n! subtype, the application MUST treat it as being equivalent to\n! \"multipart/mixed\".  The MIME header fields within each body-part of a\n! multipart message-body do not have any significance to HTTP beyond that\n! defined by their MIME semantics.\n  \n    Note: The \"multipart/form-data\" type has been specifically defined\n    for carrying form data suitable for processing via the POST request\n***************\n*** 6572,6578 ****\n  The Content-Range entity-header is sent with a partial entity-body to\n  specify where in the full entity-body the partial body should be\n  inserted. It SHOULD indicate the total length of the full entity-body,\n! unless length this is unknown or difficult to determine.\n  \n         Content-Range = \"Content-Range\" \":\" content-range-spec\n         content-range-spec      = byte-content-range-spec\n--- 6567,6573 ----\n  The Content-Range entity-header is sent with a partial entity-body to\n  specify where in the full entity-body the partial body should be\n  inserted. It SHOULD indicate the total length of the full entity-body,\n! unless this length is unknown or difficult to determine.\n  \n         Content-Range = \"Content-Range\" \":\" content-range-spec\n         content-range-spec      = byte-content-range-spec\n***************\n*** 6634,6645 ****\n         Content-Type: image/gif\n  When an HTTP message includes the content of multiple ranges (for\n  example, a response to a request for multiple non-overlapping ranges),\n! these are transmitted as a multipart MIME message. The multipart MIME\n! content-type used for this purpose is defined in this specification to\n! be \"multipart/byteranges\". See appendix 19.2 for its definition. See\n! appendix 19.8.3 for a compatibility issue.\n  \n! A client that cannot decode a MIME multipart/byteranges message should\n  not ask for multiple byte-ranges in a single request.\n  \n  When a client requests multiple byte-ranges in one request, the server\n--- 6629,6639 ----\n         Content-Type: image/gif\n  When an HTTP message includes the content of multiple ranges (for\n  example, a response to a request for multiple non-overlapping ranges),\n! these are transmitted as a multipart message. The multipart media type\n! used for this purpose is \"multipart/byteranges\" as defined in \n! appendix 19.2. See appendix 19.8.3 for a compatibility issue.\n  \n! A client that cannot decode a \"multipart/byteranges\" media type should\n  not ask for multiple byte-ranges in a single request.\n  \n  When a client requests multiple byte-ranges in one request, the server\n***************\n*** 8364,8370 ****\n  will help reduce past confusion over the relationship between HTTP and\n  Internet mail message formats.\n  \n! The HTTP protocol has evolved considerably over the past four years. It\n  has benefited from a large and active developer community--the many\n  people who have participated on the www-talk mailing list--and it is\n  that community which has been most responsible for the success of HTTP\n--- 8358,8364 ----\n  will help reduce past confusion over the relationship between HTTP and\n  Internet mail message formats.\n  \n! The HTTP protocol has evolved considerably over the past seven years. It\n  has benefited from a large and active developer community--the many\n  people who have participated on the www-talk mailing list--and it is\n  that community which has been most responsible for the success of HTTP\n***************\n*** 8672,8679 ****\n  Roy T. Fielding\n  Department of Information and Computer Science\n  University of California\n! Irvine, CA 92717-3425, USA\n! Fax: +1 (714) 824-4056\n  Email: fielding@ics.uci.edu\n  \n  James Gettys\n--- 8666,8673 ----\n  Roy T. Fielding\n  Department of Information and Computer Science\n  University of California\n! Irvine, CA 92697-3425, USA\n! Fax: +1 (714) 824-1715\n  Email: fielding@ics.uci.edu\n  \n  James Gettys\n***************\n*** 8731,8741 ****\n  19 Appendices\n  \n  \n! 19.1 Internet Media Type message/http\n  \n  In addition to defining the HTTP/1.1 protocol, this document serves as\n! the specification for the Internet media type \"message/http\". The\n! following is to be registered with IANA [17].\n  \n         Media Type name:         message\n         Media subtype name:      http\n--- 8725,8741 ----\n  19 Appendices\n  \n  \n! 19.1 Internet Media Types message/http and application/http\n  \n  In addition to defining the HTTP/1.1 protocol, this document serves as\n! the specification for the Internet media types \"message/http\" and\n! \"application/http\".  The message/http type can be used to enclose a\n! single HTTP request or response message, provided that it obeys the\n! MIME restrictions for all \"message\" types regarding line length and\n! encodings.  The application/http type can be used to enclose a pipeline\n! of one or more HTTP request or response messages (not intermixed).\n! \n! The following is to be registered with IANA [17].\n  \n         Media Type name:         message\n         Media subtype name:      http\n***************\n*** 8752,8757 ****\n--- 8752,8772 ----\n         Security considerations: none\n  \n  \n+        Media Type name:         application\n+        Media subtype name:      http\n+        Required parameters:     none\n+        Optional parameters:     version, msgtype\n+         version: The HTTP-Version number of the enclosed messages\n+                  (e.g., \"1.1\"). If not present, the version can be\n+                  determined from the first line of the body.\n+         msgtype: The message type -- \"request\" or \"response\". If not\n+                  present, the type can be determined from the first\n+                  line of the body.\n+        Encoding considerations: HTTP messages enclosed by this type are\n+                                 in \"binary\" format; use of an appropriate\n+                                 Content-Transfer-Encoding is required when\n+                                 transmitted via E-mail.\n+        Security considerations: none\n  \n  \n  \n***************\n*** 8763,8776 ****\n  \n  19.2 Internet Media Type multipart/byteranges\n  \n! When an HTTP message includes the content of multiple ranges (for\n! example, a response to a request for multiple non-overlapping ranges),\n! these are transmitted as a multipart MIME message. The multipart media\n  type for this purpose is called \"multipart/byteranges\".\n  \n  The multipart/byteranges media type includes two or more parts, each\n! with its own Content-Type and Content-Range fields. The parts are\n! separated using a MIME boundary parameter.\n  \n         Media Type name:         multipart\n         Media subtype name:      byteranges\n--- 8778,8791 ----\n  \n  19.2 Internet Media Type multipart/byteranges\n  \n! When an HTTP 206 (Partial Content) response message includes the content\n! of multiple ranges (a response to a request for multiple non-overlapping\n! ranges), these are transmitted as a multipart message-body. The media\n  type for this purpose is called \"multipart/byteranges\".\n  \n  The multipart/byteranges media type includes two or more parts, each\n! with its own Content-Type and Content-Range fields. The required boundary\n! parameter specifies the boundary string used to separate each body-part.\n  \n         Media Type name:         multipart\n         Media subtype name:      byteranges\n***************\n*** 8779,8797 ****\n         Encoding considerations: only \"7bit\", \"8bit\", or \"binary\" are\n                                  permitted\n         Security considerations: none\n  For example:\n  \n!    HTTP/1.1 206 Partial content\n     Date: Wed, 15 Nov 1995 06:25:24 GMT\n     Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n     Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 500-999/8000\n     ...the first range...\n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 7000-7999/8000\n     ...the second range\n     --THIS_STRING_SEPARATES--\n  \n--- 8794,8816 ----\n         Encoding considerations: only \"7bit\", \"8bit\", or \"binary\" are\n                                  permitted\n         Security considerations: none\n+ \n  For example:\n  \n!    HTTP/1.1 206 Partial Content\n     Date: Wed, 15 Nov 1995 06:25:24 GMT\n     Last-modified: Wed, 15 Nov 1995 04:58:08 GMT\n     Content-type: multipart/byteranges; boundary=THIS_STRING_SEPARATES\n+ \n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 500-999/8000\n+ \n     ...the first range...\n     --THIS_STRING_SEPARATES\n     Content-type: application/pdf\n     Content-range: bytes 7000-7999/8000\n+ \n     ...the second range\n     --THIS_STRING_SEPARATES--\n  \n***************\n*** 8853,8868 ****\n  types, to make date comparisons easier, and to acknowledge the practice\n  of some early HTTP servers and clients.\n  \n- \n- \n  This appendix describes specific areas where HTTP differs from RFC 2045.\n  Proxies and gateways to strict MIME environments SHOULD be aware of\n  these differences and provide the appropriate conversions where\n  necessary. Proxies and gateways from MIME environments to HTTP also need\n  to be aware of the differences because some conversions may be required.\n  \n  \n! 19.4.1 Conversion to Canonical Form\n  \n  RFC 2045 requires that an Internet mail entity be converted to canonical\n  form prior to being transferred, as described in Appendix G of RFC 2045\n--- 8872,8901 ----\n  types, to make date comparisons easier, and to acknowledge the practice\n  of some early HTTP servers and clients.\n  \n  This appendix describes specific areas where HTTP differs from RFC 2045.\n  Proxies and gateways to strict MIME environments SHOULD be aware of\n  these differences and provide the appropriate conversions where\n  necessary. Proxies and gateways from MIME environments to HTTP also need\n  to be aware of the differences because some conversions may be required.\n  \n+ 19.4.1 MIME-Version\n  \n! HTTP is not a MIME-compliant protocol. However, HTTP is capable of\n! transmitting MIME-compliant messages. \n! HTTP/1.1 messages may include a single MIME-Version general-header field\n! to indicate what version of the MIME protocol was used to construct the\n! message. Use of the MIME-Version header field indicates that the message\n! is in full compliance with the MIME protocol (as defined in RFC\n! 2045[7]). Proxies/gateways are responsible for ensuring full compliance\n! (where possible) when exporting HTTP messages to strict MIME\n! environments.\n! \n!        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n! MIME version \"1.0\" is the default for use in HTTP/1.1. However, HTTP/1.1\n! message parsing and semantics are defined by this document and not the\n! MIME specification.\n! \n! 19.4.2 Conversion to Canonical Form\n  \n  RFC 2045 requires that an Internet mail entity be converted to canonical\n  form prior to being transferred, as described in Appendix G of RFC 2045\n***************\n*** 8888,8895 ****\n  of some character sets which do not use octets 13 and 10 to represent CR\n  and LF, as is the case for some multi-byte character sets.\n  \n  \n! 19.4.2 Conversion of Date Formats\n  \n  HTTP/1.1 uses a restricted set of date formats (section 3.3.1) to\n  simplify the process of date comparison. Proxies and gateways from other\n--- 8921,8932 ----\n  of some character sets which do not use octets 13 and 10 to represent CR\n  and LF, as is the case for some multi-byte character sets.\n  \n+ Implementers should note that conversion will break any cryptographic\n+ checksums applied to the original content unless the original content\n+ is already in canonical form.  Therefore, the canonical form is\n+ recommended for any content that uses such checksums in HTTP.\n  \n! 19.4.3 Conversion of Date Formats\n  \n  HTTP/1.1 uses a restricted set of date formats (section 3.3.1) to\n  simplify the process of date comparison. Proxies and gateways from other\n***************\n*** 8898,8904 ****\n  necessary.\n  \n  \n! 19.4.3 Introduction of Content-Encoding\n  \n  RFC 2045 does not include any concept equivalent to HTTP/1.1's Content-\n  Encoding header field. Since this acts as a modifier on the media type,\n--- 8935,8941 ----\n  necessary.\n  \n  \n! 19.4.4 Introduction of Content-Encoding\n  \n  RFC 2045 does not include any concept equivalent to HTTP/1.1's Content-\n  Encoding header field. Since this acts as a modifier on the media type,\n***************\n*** 8910,8916 ****\n  Content-Encoding. However, this parameter is not part of RFC 2045.)\n  \n  \n! 19.4.4 No Content-Transfer-Encoding\n  \n  HTTP does not use the Content-Transfer-Encoding (CTE) field of RFC 2045.\n  Proxies and gateways from MIME-compliant protocols to HTTP MUST remove\n--- 8947,8953 ----\n  Content-Encoding. However, this parameter is not part of RFC 2045.)\n  \n  \n! 19.4.5 No Content-Transfer-Encoding\n  \n  HTTP does not use the Content-Transfer-Encoding (CTE) field of RFC 2045.\n  Proxies and gateways from MIME-compliant protocols to HTTP MUST remove\n***************\n*** 8935,8948 ****\n  INTERNET-DRAFT            HTTP/1.1 Friday, November 21, 1997\n  \n  \n- 19.4.5 HTTP Header Fields in Multipart Body-Parts\n- \n- In RFC 2045, most header fields in multipart body-parts are generally\n- ignored unless the field name begins with \"Content-\". In HTTP/1.1,\n- multipart body-parts may contain any HTTP header fields which are\n- significant to the meaning of that part.\n- \n- \n  19.4.6 Introduction of Transfer-Encoding\n  \n  HTTP/1.1 introduces the Transfer-Encoding header field (section 14.40).\n--- 8972,8977 ----\n***************\n*** 8967,8989 ****\n         }\n         Content-Length := length\n         Remove \"chunked\" from Transfer-Encoding\n- \n- 19.4.7 MIME-Version\n- \n- HTTP is not a MIME-compliant protocol (see appendix 19.4). However,\n- HTTP/1.1 messages may include a single MIME-Version general-header field\n- to indicate what version of the MIME protocol was used to construct the\n- message. Use of the MIME-Version header field indicates that the message\n- is in full compliance with the MIME protocol (as defined in RFC\n- 2045[7]). Proxies/gateways are responsible for ensuring full compliance\n- (where possible) when exporting HTTP messages to strict MIME\n- environments.\n- \n-        MIME-Version   = \"MIME-Version\" \":\" 1*DIGIT \".\" 1*DIGIT\n- MIME version \"1.0\" is the default for use in HTTP/1.1. However, HTTP/1.1\n- message parsing and semantics are defined by this document and not the\n- MIME specification.\n- \n  \n  \n  \n--- 8996,9001 ----\n\n\n\n"
        },
        {
            "subject": "Re: SECCACHING editorial issue..",
            "content": "Jeff Mogul suggested adding:\n> The judicious use of cryptography, when appropriate, may suffice\n> to protect against a broad range of security and privacy attacks.\n> Such cryptography is beyond the scope of the HTTP/1.1 specification.\n\nwith the caveat\n>  if people don't think this is going too far out on a political limb.\n\nI don't think this is a 'political' ; it is just not very helpful. In a handbook\nof good\npractices for site security, 'use of cryptography' is just one of a large number\nof\nthings that need to be done judiciously. In fact, I could imagine for a proxy\ncache\nthat the main thing to do is to limit remote access.\n\n\n\n"
        },
        {
            "subject": "Re: This may be a stupid question ..",
            "content": "At 15:26 1998-02-11 -0800, Ari Luotonen wrote:\n>Nick Ambrose wrote:\n>> \n>> We are developing a transparent web proxy, such that a browser does not\n>> necessarily know that it is sending requests through a proxy. I have\n>> inherited the Web proxy code from a contractor who left. It turns out,\n>> that our Proxy only supports HTTP/1.0 and will reject any requests for\n>> HTTP/1.1  Looking at the Netscape Proxy server, it seems to have some\n>> way of 'downgrading' the Browser to HTTP/1.0 as a stopgap measure.\n>> \n>> My question: Is there a document / RFC somwhere which explains how we\n>> are supposed to do this ?\n>> \n>> I haven't been able to find anything so far.\n>\n>You can't write a transparent HTTP/1.1 proxy.  HTTP/1.1 makes explicit\n>provisions for proxies which requires the client to be aware whether or\n>not a proxy is in between.\n>\n\nHi!\n\nAs I see it, if only your transparent proxy returns a HTTP/1.0-response it\nwould \ndowngrade the client correctly. If it totally rejects a request just\nbecause it's HTTP/1.1,\nthere is probably just some stupid strcmp(lastpartofrequest,\"HTTP/1.0\")\nsomewhere.\nI've seen that before on some webservers.\n\nOn the other hand, I can't see why a transparent HTTP/1.1-proxy would be\nimpossible.\nI just wrote one myself (transparently proxies socks port 80) and it seems\nto work.\nIt will rather upgrade on both sides to 1.1 than downgrade to 1.0, and IMHO\nit works like a charm.\nSo, I nervously wonder, what paragraph of RFC2068 did I just break, who's\nthe victim,\nand what's the verdict? :)\n\nTIA for comments.\n\nRegards,\n\nRobban\n\nRobTex http://www.robtex.com/\nhome of JavaMachine, SuperSearch, VikingServer and Robban's Homepage.\n\nApologies for delayed answers\n\n\n\n"
        },
        {
            "subject": "Re: This may be a stupid question ..",
            "content": "On Fri, 13 Feb 1998, Robert Olsson wrote:\n\n> >You can't write a transparent HTTP/1.1 proxy.  HTTP/1.1 makes explicit\n> >provisions for proxies which requires the client to be aware whether or\n> >not a proxy is in between.\n\n> On the other hand, I can't see why a transparent HTTP/1.1-proxy would be\n> impossible.\n> I just wrote one myself (transparently proxies socks port 80) and it seems\n> to work.\n> It will rather upgrade on both sides to 1.1 than downgrade to 1.0, and IMHO\n> it works like a charm.\n> So, I nervously wonder, what paragraph of RFC2068 did I just break, who's\n> the victim, and what's the verdict? :)\n\n  You missed at least section 14.44 Via:\n\n   The Via general-header field MUST be used by gateways and proxies to\n   indicate the intermediate protocols and recipients between the user\n   agent and the server on requests, and between the origin server and\n   the client on responses.\n\n  I invite you to join the ongoing interoperability testing by putting\nyour proxy on an Internet-reachable system so that the rest of us can test\nthrough it.\n\n  This requirement is not just a matter of known problems in the present\nprotocol - it comes from long experience with protocol development that\nhas shown that having 'invisible' participants in the protocol makes\ndebugging the protocol and detecting what features may be expected to work\nfor a given set of participants difficult or impossible.\n\n  \n\n\n\n"
        },
        {
            "subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE",
            "content": "Jim Gettys:\n>\n>I've pulled Paul's proposal from Rev-02 for RE-AUTHENTICATION-REQUESTED\n>per the discussion in Washington and the mailing list.  The lack\n>of this facility does need discussion in the Security Considerations\n>section, however.  So I had an editorial task to generate such a section.\n>\n>Here's my crack at drafting such a section.  Comments welcome (for a short\n>while, anyway...).\n>- Jim\n>\n>15.6 15.6 Authentication Credentials and Idle Clients\n>\n>Existing HTTP clients typically retain authentication information \n>indefinately. HTTP/1.1 lacks a facility to force reauthentication of clients, \n>which may have been idle for extended periods, by an origin server or \n>a proxy. This is considered a significant defect that requires further \n>additions to HTTP, and is under separate study. There are a number of \n>work-arounds to parts of this problem, and we encourage the use of password \n>protected screen savers on idle clients to mitigate some of the resulting \n>security problems.\n\nHmm, I think you are using `clients' to mean `user agents' here.  A\nsuggested rewrite:\n\n15.6 Authentication Credentials and Idle Users\n\nExisting HTTP user agents typically retain user-supplied\nauthentication information indefinately. HTTP/1.1 lacks a facility to\nforce reauthentication of users, which may have been idle for extended\nperiods, by an origin server or a proxy. This is considered a\nsignificant defect that requires further additions to HTTP, and is\nunder separate study. There are a number of work-arounds to parts of\nthis problem, including terminating the user agent in order to clear\nall accumulated authentication credentials.  We encourage the use of\npassword protected screen savers on systems which run such user\nagents to mitigate some of the resulting security problems.\n\n\nThis still does not spell out the actual problem scenario involved,\nwhich is that a user walks away after which a malicious other user\ntakes control of his user agent, so I guess it can be improved still\nfurther.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP practice and Year2000: the bad(?) new",
            "content": "Phillip M Hallam-Baker writes:\n\n    I recall the change was made to the HTTP spec sometime in 1992.\n    Perhaps you could state which clients were producing the buggy\n    fields.\n    \nAs I stated elsewhere in that message, I don't intend to \"name names\".\n\nAt any rate, most of the \"buggy\" (2-digit-year) date fields came\nfrom servers, not clients.\n\nMy main concern is actually with the proxies that have been embedded\nin various firewall products.  I know that in at least some firewall\ndesigns, it's not possible to replace the existing HTTP proxy with\nan off-the-shelf upgrade from an arbitrary vendor; you need to get\nthe firewall vendor to do the upgrade.  Which might slow down the\nadoption of new proxies.\n\nIf any of these hard-to-upgrade proxies (1) do caching and (2)\nare not year-2000-aware, we could be in for some interesting\nresults.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE",
            "content": "Actually, it is clients, though user agents are the largest\npart of the threat (leaving sleeping browsers lie...).\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Security considerations from RE-AUTHENTICATIONREQUESTE",
            "content": "Here's my revision, given Ted and Koen's comments...\n- Jim\n\n15.6 Authentication Credentials and Idle Clients\n\nExisting HTTP clients and user agents typically retain authentication \ninformation indefinately. HTTP/1.1. does not provide a method for an origin \nserver or proxy to force reauthentication. Since clients may be idle for \nextended periods between use (and unauthorized users may have access to \nthe user agent during these idle periods), this is a significant defect \nthat requires further extensions to HTTP. This is currently under separate \nstudy. For user agents, there are a number of work-arounds to parts of \nthis problem, and we enourage the use of password protection in screen \nsavers, idle time-outs, and other methods which mitigate the security \nproblems inherent in this problem.\n\n\n\n"
        },
        {
            "subject": "Re: SSL tunneling statu",
            "content": "I beleive Ari submitted the draft, so it should be.\n\n(I know he did it, I suppose its possible that it didnt\nget submitted, Ill check with him )\n\njosh\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department               \"My opinions, not Netscape's\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Digest Authentication, Netscape, and Microsof",
            "content": "As I prepared my IETF trip report, I found the following remarks from the\nprevious (San Jose) meeting:\n\n(In a private conversation I had with Jim Gettys, W3C, I learned that\nNetscape's earlier objections to Digest have been resolved, although it\nappears to me that the current draft does not include the changes that\nwould make them happy.  In any case, apparently future Netscape browsers\nwill support Digest.)\n\nSupposedly Netscape was going to propose changes to address their concerns,\nbut I have seen no such proposals.\n\nWhat's happening?  What are Netscape's and Microsoft's plans vis a vis Digest?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Pipelinin",
            "content": "    When a client is talking to a proxy server and is pipelining requests,\n    should it use a single pipeline connection to issue requests to \n    different origin servers ?\n    ie should GET http://www.ups.com/ HTTP/1.1\n       and    GET http://www.fedex.com/ HTTP/1.1 \n    be sent over the same pipeline or should a new connection be established.\n    A common occurence of this is when advertisement gifs come from a \n    different server than the html file.\n    \n    I guess the question becomes :\n    Can requests in the same pipeline be to different origin servers?\n    \nIn the case where you have an HTML file coming from one server, and\nan imbedded GIF from another, I think the \"use two connections\"\nrule applies.  I.e., the client has two connections to the proxy,\nuses the first connection to fetch the HTML file, and the other\nfor the imbedded images.\n\nNote that this is actually not a particularly hard example; you\ncan't actually start fetching the image until you have at least\nsome of the HTML file (since you don't have the IMG SRC until then).\n\nA harder question is \"what if the HTML file has lots of IMG SRCs,\nwhich come from more than one origin server?\"  Then you could get\ninto a situation where you request several different images over\none connection, and some might be blocked waiting for earlier ones\nto complete.\n\nI think the best answer is that as long as the images are ultimately\nbeing used to construct the same layout, then you should stick\nto using as few connections as possible.  Ultimately, you can't\nfinish the layout until you have most or all of the images, and so\nwhile there might be some minor benefit in trying to get them in\n\"fastest first\" order, this probably isn't a big deal.\n\nOn the other hand, if one browser program has two or more independent\nWeb-browser windows open (I often do this, especially when following\nup on search-engine results), then it might be best to act as if\nthese are separate instances of the application.  I.e., each window\ngets its own pair of TCP connections, so that the two windows can\nbe operated without interfering with each other.\n\nAlthough the use of lots of parallel connections has taken some\ncriticism (including from me), the real problem is not having lots\nof TCP connections open per se.  The two bad things to do are\n(1) open and close connections frequently, resulting in lots of\noverhead at the server, and (2) run lots packets simultaneously\nover parallel connections, which defeats the TCP congestion\ncontrol algorthms.  If you get into a situation where you have\ntwo proxy connections open, but both of them appear stalled, it might\nmake sense to open another connection *if* you are going to use\nit to get responses from a different origin server.\n\nAnd if you have multiple origin servers to load from at once,\nit probably pays to distribute the URLs among your TCP connections\nso that all of the URLs from one server are fetched on one connection.\nI.e., if the servers are A and B, do this:\nconnection 1: A A A A A A\nconnection 2: B B B B B B\ninstead of this\nconnection 1: A B A B A B\nconnection 2: A B A B A B\nsince if server B is slow or dead, the latter approach will cause\na lot more trouble.  It might even cause more TCP opening/closing,\nif the user gets tired and hits \"stop\" ... since if a request is\nin progress when that happens, you pretty much have to close the\nTCP connection to recover.\n\nBottom line: minimize the number of (1) simultaneously active\nTCP connections and (2) the number of TCP opens and closes.\nBut don't make the user wait because you've done things in the\nwrong order.\n\n    With the advent of pipelined persistent connections ( and to a lesser\n    extend 1.0 keep alives ), the distinction of 'who the client is\n    talking to' is confusing to me.  Since while the client may be\n    pipelining to a proxy, and the proxy can go ahead and do an\n    old style connection to an origin server, how does the client deal\n    with old responses?\n    Assuming the answer to the previous question,  is yes...\n    \n    IE: client pipelines:\n      GET http://www.foo.com/ HTTP/1.1\n      GET http://www.bar.com/ HTTP/1.1\n    what happens if foo.com is a 1.1 server ( the proxy can do a\n    persistent conenction ) and bar.com is 1.0 (proxy cannot)?\n    \nIt's important to understand that persistent connections are\n\"hop-by-hop\", NOT \"end-to-end\".  I.e., what goes on between\nproxy and client (as far as TCP connections are concerned)\nis almost entirely unrelated to what goes on between proxy\nand origin server; at a higher level, the proxy is always just\nforwarding requests and responses.  If it has to use a separate\nTCP connection for each request/response, because its peer\nspeaks HTTP/1.0, so be it.\n\nThe \"almost entirely\" in that paragraph is because if the user\nhits \"stop\", the only way for the user agent to stop a transfer\nin progress is to close the TCP connection.  And since we want the\nproxy to pass this along (e.g., to stop wasting Internet bandwidth\nthat's being used to load a large response from the origin server),\nthen this causes the connection-close to propagate all the way through\nthe chain of proxies.  Which is sad, but we've always said that\nTCP connections are not immortal.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 Pipelinin",
            "content": "Josh Cohen <josh@birdcage.mcom.com> writes:\n    What do we do with sticky headers?  Should each origin server get 1\n    copy of the sticky headers? Should the client have to specify\n    sticky headers for the 1st request to a new origin server in a\n    pipeline?\n\nWe don't have sticky headers in HTTP/1.1.  (I think we should, but that's\nanother discussion.)\n\nIf we did have them, then the rules for multiplexing such requests\nover a proxy would have to be specified carefully.  In fact,\nearlier versions of the hit-metering draft did do something like\nthis, but people talked me out of using anything sticky in\nthe hit-metering design.\n\nI believe that the solution is not extremely complex, but it wouldn't\nbe a total no-brainer, either.  Let's worry about this in HTTP/1.2.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": ">>>>> \"DK\" == Dave Kristol <dmk@bell-labs.com> asks:\n\nDK> What's happening?  What are Netscape's and Microsoft's plans vis a\nDK> vis Digest?\n\n  For those who missed the IAB report on security issues, the first\n  thing on their list of 'things to be killed asap' was 'sending\n  passwords in clear'.\n\n  I believe that Basic authentication falls in this category.\n\n  I'd like to extend the question to other browser vendors.\n\n--\nScott Lawrence                                       <lawrence@agranat.com>\nAgranat Systems, Inc.                               http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: midcourse error",
            "content": "One could always invent a 1xx response which says \"This current response\njust went south.\"\nYaron\n\n> -----Original Message-----\n> From:Gregory J. Woodhouse [SMTP:gjw@wnetc.com]\n> Sent:Monday, April 14, 1997 9:46 AM\n> To:Bob Denny\n> Cc:Dave Kristol; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: mid-course errors\n> \n> If CGI outgput is spooled, doesn't that defeat the purpose of chunked\n> encoding for CGI output?\n> \n> I share your concern about errors that occur after the 200 response. I\n> can't think of any way to signal a problem except to close the\n> connection\n> before the final chunk length of 0 is transmitted. This would work, of\n> course, but it would be nice if the server could give some indication\n> of\n> the nature of the problem.\n> \n> ---\n> gjw@wnetc.com    /    http://www.wnetc.com/home.html\n> If you're going to reinvent the wheel, at least try to come\n> up with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "   Date: Mon, 14 Apr 1997 21:08:31 -0400\n   From: \"Scott Lawrence\" <lawrence@agranat.com>\n\n     For those who missed the IAB report on security issues, the first\n     thing on their list of 'things to be killed asap' was 'sending\n     passwords in clear'.\n\n     I believe that Basic authentication falls in this category.\n\n     I'd like to extend the question to other browser vendors.\n\nProject GNU doesn't exactly count as a `vendor'; nor am I really\nan official representive.\n\nHowever, we internally use very little security, and 99.9% of the\ntime that works fine.  The fact that my passwords get sent cleartext\nacross the net doesn't really bother me.\n\nIt's true that I wouldn't send credit card information cleartext; but\nmost information I have stored in my accounts isn't really that\nimportant to me.  I'm not paranoid about protecting it anyway.\n\nAs a practical matter, it's a huge inconvinience to me when I'm not\nroot.  Many other contributors to GNU feel that way, and I think\nthat has something to with our decisions to configure our machines\nin a less than paranoid way.\n\nGNU doesn't really have any competiors per se.  It's true that the\nNetBSD people tend to reimplement everything GNU does in order to\nremove restrictions related to proprietary derivatives; and it's\ntrue that those who write proprietary software are competitors\nin a way.  But I would be quite happy if Netscape or Microsoft\ndecided to use some of the code from E-scape, as long as they\nfollow the conditions of the GNU General Public License.\n\nAnother thing: I hate firewalls.  It's ridiculous spending hours\nto get a workstation to print, just because the printer is behind\na firewall, and the workstation is outside.\n\nEspecially when there aren't any other machines running any IP\nserver software behind that firewall.\n\nHowever, having said that, if someone adds additional capabilities\nto my browser, and it's clear that there are no legal problems, I'll\nbe happy to merge password encryption code.\n\n(And it will only take one competent user living in the right\ncountry to get those capabilities.)\n\nHowever, much as the basic authentication scheme has problems, doesn't\nSSL solve all those problems?\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "The reason for wanting the asynchronous behavior is that it allows for\nnotifications. As HTTP is being used in more powerful systems an\nasynchronous notification system is needed.\n\nThe only implementation concern was stated by Larry who was worried\nabout what this would do to client's object models. I have talked with\nJosh Cohen at Netscape's proxy group and he says he is fine with this\ninterpretation. I have talked with Microsoft's Proxy group and they\nconcur. I am the PM for Microsoft's client side HTTP implementation and\nI am happy with it. I am trying to talk to Netscape's client side people\nto hear their thoughts. I haven't had a chance to talk with the Apache\nproxy people or any of the other client developers. That is one of the\nreasons why I posted to the list, I wanted to hear their thoughts. Do\nyou have an issue with the proposed interpretation?\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From:Scott Lawrence [SMTP:lawrence@agranat.com]\n> Sent:Friday, April 11, 1997 7:01 AM\n> To:Josh Cohen\n> Cc:Scott Lawrence; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: 1xx Clarification\n> \n> \n> >> We would actually prefer to see this set of rules made more general\n> >> in that we'd like it to apply to any POST, not just one being\n> >> retried (which may or may not have been what was intended).\n> \n> >>>>> \"JC\" == Josh Cohen <josh@netscape.com> replies:\n> \n> JC> If the 100 is only supposed to happen on a retried request, then\n> JC> how does a server know if its a retried request or not ?\n> \n>   I was unclear - what I meant was that I would like to see the client\n>   always wait for a 100 response following the headers on a 1.1 POST,\n>   not just when it is retrying one that was interrupted.\n> \n>   Our current behaviour is that we will send the 100 Continue after\n>   reading the headers on any 1.1 POST which we think is ok (it may\n>   still be rejected if the authentication digest does not check out,\n>   but we can't know that until we get the body).\n> \n> JC> BTW: was 'Yoland' meant to be an abbreviation for Yaron Goland ?\n> :)\n> \n>   Yes, my apologies, Yaron - my fingers are faster than my brain\n>   sometimes.\n> \n> --\n> Scott Lawrence\n> <lawrence@agranat.com>\n> Agranat Systems, Inc.\n> http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "privacy platform",
            "content": "I'm not sure what W3C will do, but one idea I heard\nat the CFP conference was the possibility of using\nsomething like PICS to rate sites by \nhow well (or poorly) they treat the privacy of users\nwho visit their sites.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "That was the assumption.\nYaron\n\n> -----Original Message-----\n> From:Roy T. Fielding [SMTP:fielding@kiwi.ICS.UCI.EDU]\n> Sent:Sunday, April 13, 1997 6:46 PM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: 1xx Clarification \n> \n> >Section 8.2 of RFC 2068 has the following requirement:\n> >\n> >o  An HTTP/1.1 (or later) client MUST be prepared to accept a 100\n> >   (Continue) status followed by a regular response.\n> >\n> >I have gone around asking people if the following behavior is legal:\n> >\n> >User Establishes Connection\n> >\n> >Server sends first 1xx Response\n> >\n> >User sends GET\n> \n> Just a clarification: this would be assuming that the server knows\n> that the client supports HTTP/1.1 before it makes the first request\n> (as would be the case for some non-typical-web services, which is what\n> I think Yaron is envisioning).\n> \n> .....Roy\n\n\n\n"
        },
        {
            "subject": "RE: midcourse error",
            "content": "Yaron Goland <yarong@microsoft.com> wrote:\n  > [about the problem of server-side things going bad after the response\n  > headers had been sent]\n\n  > One could always invent a 1xx response which says \"This current response\n  > just went south.\"\n\nNo, then it's too late.  Because the server has already sent the\nresponse headers, the client would be looking for the entity body, not\nheaders.  My whole point was that, once the server began sending the\nentity body, there's no way for it to inform the client, \"Whoops,\nthings have gone bad.\"\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: FW: Proposed amendment to RFC210",
            "content": "Jonathan Stark wrote:\n> \n<text Deleted>\n> \n> As I understand it, the real problem with allowing cookies\n> from EVERYWHERE is that it allows the collection of data about individuals\n\nHere is one of the problems which seems to show up too much.  Issuers of\nindirect (I like this term better than unverifiable) transactions are\nnot collecting data about individuals.  We do not have any method other\nthan direct registration on our site or another site sharing the\ninformation, to understand \"who\" a viewer is.  We set cookies in\nuser-agents.  We do not know the individual!  Part of the fear of\nprivacy violation is that we can track someone down, get their phone\nnumber, etc. and this is not true.  So, we collect data about the\nbehavior of people who use and potentially share a web browser, but we\ndo NOT know who that person is. In no cases have we ever found a web\nsite who is willing to share information about registered users.  One of\nthe big issues is what do the sites who issue verifiable transaction\ncookies do with their information.  If they were to provide us with\ntheir registered users' data and the IDs of the issued cookies, we could\ncompletely skirt the issue of unverifiable transactions (with some\nwork).  So, there are ways around this. \n\n> without their knowledge or consent, and once collected, the user\n> has no idea what it's going to be used for, or by whom it will be used.\n> The proposal provides a mechanism of informing users of business and\n> data relationships other than just the ones dictated by a particular domain,\n> and a mechanism that allows them to weigh the benefits and hazards of\n> accepting cookies from entities.\n> \n <text deleted>\n\n> Conversely, tripleclick may actually collect names and phone numbers\n\nHow...tripleclick does not have access to names and phonenumbers unless\nsomeone gives it to them.  This\nkind of information cannot be extrapolated from IP address.\n\n> and sell that information as targeted mailing lists based upon\n> user's preferences on the adds they sent, or they may work with\n> \"Company B\" to compare logs with Copmany B and link the preferences\n> that tripleclick collected with user names that Company B collects.  In\n> that case, the third party CA may issue them a \"class 3\" certificate\n> that says that tripleclick actively trades personally identifiable\n> information with other groups.  Under certain circumstances, the\n> user may want to accept class 3 certificates for the added value\n> of getting more information about things they are interested in.  In\n> most cases, however, they are likely to not want to accept these cookies.\n> The point is that the user should have the choice of accepting the\n> cookies and the information policies that they want.  Informed consent\n> prior to divulging information is much better than arbitrarily limiting\n> the use of cookies by domains.  The use of cookies and the whole\n> issue of trust and privacy change from company to company, situation\n> to situation, and the user should be able to make informed decisions\n> about what they want to do in any particular set of situations.\n\nI honestly believe that once again the big issue is for people to\nunderstand exactly what are the potential breaches of privacy and how\nthey can occur. Let's face it, the Ad networks are interested in\ntracking user behavior, no secret there....however, there are limits to\nwhat we can and would do with this information.  \n\nIt is our policy to completely disclose how we use cookie data (and this\nwill show up on our web site in the near future). I also believe that\nother Ad networks disclose this information. \n\nI personally would accept the certification process if this is the only\nalternative.  One issue with this is how fast can sites be certified,\nwhat are the criteria, and how much will it cost?  \n\nI would prefer an easier solution which I assume has been discussed\n(please excuse my late entrance into this mailing list and my potential\nignorance of history) which is to make the default for user-agents to\naccept all cookies with the ability to turn off unverifiable\ntransactions if desired. \n\nCan anyone from the companies providing browsers give a company position\non what they intend to do with respect to RFC 2109 and the default of\nturning off unverifiable transactions. \n\n> \n> Jonathan\n> \n> ===============================================================\n> Jonathan Stark                             (415) 858 1930 x217\n> eTRUST Technical Director                  stark@eTRUST.org\n\n-- \nSteve Reiss\nFlyCast Communicationsemail sreiss@flycast.com\n123 Townsend St.Phone 415-975-5373\nSuite 226Fax   415-977-1009\nSan Francisco, CA 94107\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "If I understand correctly you are interested in 1xx responses appearing within the context of a tcp connection to a HTTP server but outside of  the context of a HTTP request.  If this is correct, I don't see how this will work through a proxy since proxies generally only maintain a connection to the end server for the duration of the request.  Even if the proxy maintains a persistent connection to the server, the connection may only be allocated to a specific client for the duration of a HTTP request.  So in your scenario, going through a proxy I think the sequence would be the following:\n\nUser Establishes Connection to Proxy\nUser sends GET to Proxy\nProxy Establishes connection to Server\nServer sends 1xx Response to Proxy\nProxy sends 1xx response to client\nProxy sends GET to Server\nServer sends second 1xx Response to Proxy\nProxy sends second 1xx response to client\nServer Sends 200 Response to Proxy\nProxy sends 200 response to client\nServer sends third 1xx Response to Proxy\nProxy sends third 1xx response to client\nProxy closes server connection or \"assigns\" it to another \"client\"\n(note that this could happen before the 3rd 1xx response is received)\nUser Closes Connection to Proxy\n\nso the first 1xx response makes it through, but not outside of the request context.  The second 1xx response works as expected, and the 3rd may or may not make it to the client based on a race condition of when the proxy closes or \"reuses\" the server connection.\n\n-Doug Crow\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "Dave Kristol writes:\n    Yaron Goland <yarong@microsoft.com> wrote:\n      > [about the problem of server-side things going bad after the response\n      > headers had been sent]\n    \n      > One could always invent a 1xx response which says\n      > \"This current response just went south.\"\n    \n    No, then it's too late.  Because the server has already sent the\n    response headers, the client would be looking for the entity body, not\n    headers.  My whole point was that, once the server began sending the\n    entity body, there's no way for it to inform the client, \"Whoops,\n    things have gone bad.\"\n\nFirst of all, note that RFC2068 requires that (section 14.14)\n   It must be possible for the recipient to reliably determine\n   the end of HTTP/1.1 requests containing an entity-body, e.g., because\n   the request has a valid Content-Length field, uses Transfer-Encoding:\n   chunked or a multipart body.\n\n(Perhaps that \"must\" should be a \"MUST\"?)\n\nSo if the response headers include a Content-length field, one\nwould assume that if the server runs into an error, it can terminate\nthe TCP connection before sending the specified number of bytes; the\nrecipient should pay attention to the discrepancy.  In fact, section \n13.8 says:\n   A cache that receives an incomplete response (for example, with fewer\n   bytes of data than specified in a Content-Length header) may store\n   the response. However, the cache MUST treat this as a partial\n   response.  Partial responses may be combined as described in section\n   13.5.4; the result might be a full response or might still be\n   partial. A cache MUST NOT return a partial response to a client\n   without explicitly marking it as such, using the 206 (Partial\n   Content) status code. A cache MUST NOT return a partial response\n   using a status code of 200 (OK).\n\nI.e., the current specification seems to cover the case where\na Content-Length doesn't match the number of bytes sent.\n\nI'm not sure that RFC2068 has similar language for non-caching\nclients, but it would seem reasonable to implement the same\nkind of sanity checking.  (In passing, I note that at least one\nof the existing browser caches that I have used does not do so!)\n\nIn the case of a server sending a chunked response intending to\nput the content-length in the footer (which is allowed by\nsection 3.6 of RFC2068 (Transfer Codings)), it seems reasonable\nthat the server could signal an error by sending a completely\nbogus Content-length.  E.g., if you have sent 4096 content bytes \nbefore recognizing the error, send a chunk footer with\nContent-length: 1000000000000\nand the recipient should know that something is wrong.  In some\ncases, the server might have to pad the chunk past the point\nwhere the error occurred.\n\nI would imagine that in some server implementations, it's hard\nto tell that a sub-process (e.g., CGI script) screwed up.  But\nin a UNIX-based server, if the CGI process exits with an error\nstatus, then the server should presumably find some way of\nsending a bogus length value before closing the connection.\nI doubt this is possible in all cases, but we can't specify\nsloppy programming out of existence.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "Jeffrey Mogul wrote:\n> [...]\n> In the case of a server sending a chunked response intending to\n> put the content-length in the footer (which is allowed by\n> section 3.6 of RFC2068 (Transfer Codings)), it seems reasonable\n> that the server could signal an error by sending a completely\n> bogus Content-length.  E.g., if you have sent 4096 content bytes\n> before recognizing the error, send a chunk footer with\n>         Content-length: 1000000000000\n> and the recipient should know that something is wrong.  In some\n> cases, the server might have to pad the chunk past the point\n> where the error occurred.\n\nI looked at the wording in 3.6, which, after translating double negatives,\nsays that applications can only send header fields in the footer if the\nheader field is explicitly allowed for that purpose.  I probably didn't look\nin the right place for it, but I saw no such words in 14.16 Content-MD5\n(mentioned in 3.6), though I'm sure the intent was there to use it that way.\nHowever, I seem to recall discussions about Content-Length as a chunked\nfooter, and I seem to recall it was considered a bad idea.  Presumably if it\nwere present, it should be ignored, since the chunking overrides it.  That's\na long-winded way of saying I don't think your idea will work to signal a\nfailure in the chunked case.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "\"Henry Sanders (Exchange)\" <henrysa@EXCHANGE.MICROSOFT.com> pointed\nthis out to me:\n    > In the case of a server sending a chunked response intending to\n    > put the content-length in the footer (which is allowed by\n    > section 3.6 of RFC2068 (Transfer Codings)), it seems reasonable\n\n    Umm, where do you see this in section 3.6? I read it as saying that\n    you're only allowed to send entity header fields in the footer\n    which are explicitly defined as appropriate, as I don't see any\n    wording anywhere explicitly defining Content-Length: as\n    appropriate. I know our server doesn't check for Content-Length: in\n    the footer, and I don't think our client will either.\n\nI may have misread this section.  \n\nBut, it says:\n\n   The chunked encoding is ended by a zero-sized chunk followed by the\n   footer, which is terminated by an empty line. The purpose of the\n   footer is to provide an efficient way to supply information about an\n   entity that is generated dynamically; applications MUST NOT send\n   header fields in the footer which are not explicitly defined as being\n   appropriate for the footer, such as Content-MD5 or future extensions\n   to HTTP for digital signatures or other facilities.\n\nContent-length is definitely an entity-header (see section 7.1), and I\nthink it is clearly \"information about an entity that is generated\ndynamically\" (although whether \"that is generated dynamically\" refers\nto the \"entity\" or the \"information\" is ambiguous).\n\nWhat's interesting is that there seem to be NO fields in RFC2068\nthat are \"explicitly defined as being appropriate for the footer\",\nincluding Content-MD5.  I searched the document for the word \"footer\",\nand (besides section 3.6) it appears in only one other section (8.2).\nThere is no mention at all in the section on Content-MD5 (section 14.16).\n\nDoes your server or client actually have a list of headers that it\n*does* allow in the footer?  Where did this list come from?\n\nI think we have some document-editing to do.  Either we have to\nremove the \"MUST NOT\" from section 3.6, or remove the example of\nContent-MD5, or add a list of \"explicitly allowed in the footer\"\nheaders.\n\nAnyway, it seems quite reasonable to allow Content-Length to appear\nin a footer.  And it seems to solve the \"midcourse correction\"\nproblem, in most cases.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Insisting on a message digest (rfc2069",
            "content": "   The Digest Authentication scheme specifies a mechanism (the 'digest'\n   attribute of the Authentication-Info and Authorization headers) by\n   which a protected digest message of the message body and selected\n   headers may be transmitted.  This provides a valuable means of\n   protecting the message body from modification or replay attacks\n   based on modifying either the message body or headers, while\n   preserving the authentication headers.\n\n   The mechanism is only valuable, however, if the message recipient\n   can require that the digest attribute is present; if the attribute\n   is optional (as currently specified), then an attacker can remove\n   the attribute, preserving the remainder of the authentication\n   information, and modify the parts of the message it was meant to\n   protect.\n\n   Our implementation of Digest Authentication includes an attribute\n   for each resource which may be set to require that a message digest\n   be supplied in order access the resource (used principally for\n   submission of a form), but at present there is no way for the\n   server to communicate this requirement to the browser so that it\n   knows to generate one.\n\n   I suggest that attributes be added to the WWW-Authenticate and\n   Authorization headers to indicate that a message digest is required\n   on the subsequent message:\n\n   ================================================================\n   in section 2.1.1:\n\n     WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n                              digest-challenge\n\n     digest-challenge    = 1#( realm | [ domain ] | nonce |\n                          [ opaque ] |[ stale ] | [ algorithm ] |\n                          [ digest-required ] )\n   ...\n     digest-required     = \"digest-required\"\n   ...\n\n   digest-required\n   A flag, indicating that any request for the resource to which this\n   response applies must include the 'digest' attribute in its\n   Authorization header.\n\n   ================================================================\n   in section 2.1.2:\n\n   Authorization       = \"Authorization\" \":\" \"Digest\" digest-response\n\n   digest-response     = 1#( username | realm | nonce | digest-uri |\n                            response | [ digest ] | [ algorithm ] |\n                            opaque | digest-required )\n\n   ...\n     digest-required     = \"digest-required\"\n   ...\n\n   digest-required\n   A flag, indicating that the response to this request must include\n   the 'digest' attribute in its Authentication-Info header.\n\n   ================================================================\n\n   There are minor editorial changes to the text which would be needed\n   to accompany this change; I'll be glad to help with those if we\n   agree to make this change or one like it.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "On Mon, 14 Apr 1997, nemo/Joel N. Weber II wrote:\n\n> However, much as the basic authentication scheme has problems, doesn't\n> SSL solve all those problems?\n\nExcept that SSL is rather heavy weight performance wise and hence may be\noverkill where the real objective is reasonably reliable identification of\na user w/o compromising their password data.\n\nAnd frankly, hearing your security philosophy raises concerns about hidden\nvirus being added to complex software many many folks use from the GNU\nproject. But that is clearly offtopic for this list.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "   Date: Tue, 15 Apr 1997 14:26:13 -0700 (PDT)\n   From: \"David W. Morris\" <dwm@xpasc.com>\n\n   Except that SSL is rather heavy weight performance wise and hence may be\n   overkill where the real objective is reasonably reliable identification of\n   a user w/o compromising their password data.\n\nI still don't quite see this.\n\nBecause if I can watch someone's packets fly across a network segment,\ncan't I take over their connection after it has been established?\nObviously, for me to read the password, I have to know what I'm doing.\nSo hijacking a connection would not be much harder.  (Especially\nconsidering I've seen proprietary software that makes taking over\na connection extremely easy.)\n\nAnd if I were an end user, I'd think that the network connection\nhad just died because of a glitch.\n\n   And frankly, hearing your security philosophy raises concerns about hidden\n   virus being added to complex software many many folks use from the GNU\n   project. But that is clearly offtopic for this list.\n\nIf some sort of virus or trojan horse were added to the sources, I'm\nsure we'd notice.  I don't want to document all the techniques that\nare likely to work for us to notice things, because that WOULD reduce security.\n\nIf it's a virus that works by modifying the executable, and source is\nnever distributed for that virus, then standard GNU packages\non prep.ai.mit.edu are immune; FSF generally doesn't distribute binaries.\n\nrom what I've seen, most UN*X trojan horses seem to be distributed\nas binaries.  So I generally get the source for the programs I use,\nand compile them myself.\n\nAdmittedly, that approach looks at the way human nature works, more than\ntrying to make sure I have a bulletproof solution.\n\nBut I should also add that I've seen about two weeks worth of changes\nto files destroyed, and that didn't change my view on security.  (But\nI think we started making better backups after that lossage.)\n\nAnyway, as I understand it, Microsoft Internet Explorer has some security\nproblem that allows deleting files; and Java did too at one point.\nSo imprefect security is not a unique problem.\n\n\n\n"
        },
        {
            "subject": "RE: midcourse error",
            "content": "Jeff Mogul writes:\n\n> Does your server or client actually have a list of headers that it\n> *does* allow in the footer?  Where did this list come from?\n> \nIt turns out that our server doesn't check for any headers in the\nfooter, with a comment in the code indicating that the only footer\ndefined for 1.1 is Content-MD5:, which we don't support. My recollection\nof why this was done this way was fuzzy, so I went back and checked the\nmail archives. Around the time of the LA IETF there was some desire to\nremove footers entirely from chunked T-E. Phill Hallam objected strongly\nto this, and a compromise was reached where Content-MD5 was allowed in\nthe footer for 1.1 and future versions of HTTP might allow other\nheaders. The issue was considered closed for draft 04 in\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/0058.html . The\nwording has changed a bit since then (I vaguely remember that happening\nas an editorial change) , but I believe the intent is still the same,\nwhich is that only Content-MD5 is valid in an HTTP 1.1 chunked  footer. \n\nI agree that the text could use some clarification. I think the simplest\nchange consistent with my understanding of the intent would be to add a\nsentence to 14.16 saying \"This header is allowed in the chunked\nTransfer-Encoding footer\". Regardless, our server supporting this hasn't\nshipped yet, and I'm willing to change it if there's consensus that\nother headers should be allowed in the footer. \n\nHenry\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "On Tue, 15 Apr 1997 17:47:03 -0400 (EDT), \"nemo/Joel N. Weber II\"\n<devnull@gnu.ai.mit.edu> wrote:\n>   Except that SSL is rather heavy weight performance wise and hence may be\n>   overkill where the real objective is reasonably reliable identification of\n>   a user w/o compromising their password data.\n>\n>I still don't quite see this.\n>Because if I can watch someone's packets fly across a network segment,\n>can't I take over their connection after it has been established?\n>Obviously, for me to read the password, I have to know what I'm doing.\n>So hijacking a connection would not be much harder.  (Especially\n\nWith Digest Authentication, hijacking a connection will not allow you to\nmake subsequent requests over that connection (of different URLs) without\nknowledge of the shared secret (aka password).  There's an MD5 hash of the\nURL, the password, and some other data.\n\n-----\nDaniel DuBois, Traveling Coderman        www.spyglass.com/~ddubois\n   \"The problem with political jokes is that they get elected.\"\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "Please, SSL has nothing to do with Digest Authentication. It is not\na replacement unless you believe that every password protected page\nshould also be encrypted.\n\nThe purpose of Digest is to allow people to stop using BASIC as soon\nas possible. Nothing else. SSL essentially defines a new protocol and\na pretty complex one at that.\n\n\nSSL unfortunately provides a relatively weak form of security. It\nis great if your definition of security is the use of cryptography.\nIt has no real model of how it should interact with firewalls for\nexample - nobody sends encrypted data through the firewalls I have\nexperience with, that is part of their purpose. Nor can data from \nan SSL transaction be cached by an intermediary.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "Henry Sanders writes:\n    Around the time of the LA IETF there was some desire to remove\n    footers entirely from chunked T-E. Phill Hallam objected strongly\n    to this, and a compromise was reached where Content-MD5 was allowed\n    in the footer for 1.1 and future versions of HTTP might allow other\n    headers. The issue was considered closed for draft 04 in\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/0058.html .\n    The wording has changed a bit since then (I vaguely remember that\n    happening as an editorial change) , but I believe the intent is\n    still the same, which is that only Content-MD5 is valid in an HTTP\n    1.1 chunked  footer.\n\nThanks for digging this up (although the message you cite doesn't\nspecifically address this issue).  I probably ignored most of that\ndiscussion when it took place.\n\n    I agree that the text could use some clarification. I think the\n    simplest change consistent with my understanding of the intent\n    would be to add a sentence to 14.16 saying \"This header is allowed\n    in the chunked Transfer-Encoding footer\". \n\nThat's a reasonable resolution to the confusion regarding Content-MD5.\nI'll bug Jim Gettys to add that.\n\n    Regardless, our server\n    supporting this hasn't shipped yet, and I'm willing to change it if\n    there's consensus that other headers should be allowed in the\n    footer.\n\nI'm still not sure if I agree with the sentiment conveyed by Dave Kristol,\nthat\nI seem to recall discussions about Content-Length as a chunked\nfooter, and I seem to recall it was considered a bad idea.\n\nBut if the consensus is that Content-Length doesn't belong in the footer,\nthen perhaps we need to define a new \"footer-eligible\" header that\nserves purpose that we are looking for: \"Stop this response, I didn't\nmean it when I said '200' back in the header\".  E.g., a new header\ncalled \"Failed:\" which can convey an updated status code and message.\n\nE.g.,\nHTTP/1.1 200 OK\n...\nTransfer-Encoding: Chunked\n\n18\nThis is a complete\n0\nFailed: 500 Internal Server Error\n\nJust a modest proposal, of course.\n\n-Jeff\n\n\n\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "On Tue, 15 Apr 1997, Jeffrey Mogul wrote:\n\n> But if the consensus is that Content-Length doesn't belong in the footer,\n> then perhaps we need to define a new \"footer-eligible\" header that\n> serves purpose that we are looking for: \"Stop this response, I didn't\n> mean it when I said '200' back in the header\".  E.g., a new header\n> called \"Failed:\" which can convey an updated status code and message.\n> \n> E.g.,\n> HTTP/1.1 200 OK\n> ...\n> Transfer-Encoding: Chunked\n> \n> 18\n> This is a complete\n> 0\n> Failed: 500 Internal Server Error\n> \n> Just a modest proposal, of course.\n\nWell, independant of whether Content-Length belongs in the footer, I was\nuncomfortable with the side-effect usage proposed earlier. What you\nproposed above is close to what I was thinking of suggesting.\n\nAn alternate less modest proposal ....\n\n       0\n       Reset-Response: xxx abcde asdl\n       CRLF\n\nWhere the xxx... is a status code and interpretation.  With appropriate\nstatus codes (appropriate to be defined), a replacement response would\nimmediately follow and might be a full error response with optional\ncontent.  This approach appeals to me because is allows for the maximum\namount of communication between the server and the user for problem\nunderstanding/ resolution.\n\nYeah .... I agree ... either of these may be out of scope for the\ntransition we are facting from propsed->draft.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": "> Please, SSL has nothing to do with Digest Authentication. It is not\n> a replacement unless you believe that every password protected page\n> should also be encrypted.\n\nSSL does allow a null-cipher -- in Netscape Servers it's enabled via\nchoice \"No encryption, only MD5 message authentication\".  This\nprovides certificate based authentication and message integrity on\nHTTP data, but the data is not encrypted, so there's minimal overhead.\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "Your argument has the underlying assumption that the notification\ninformation was \"meant\" for a particular user. There are many scenarios,\nsuch as the server saying things like \"I am going down\" or \"My resources\nare getting limited\" where the asynchronous notification is meant for\nwhomever has the connection open.\n\nYaron\n\n> -----Original Message-----\n> From:doug_crow@cacheflow.com [SMTP:doug_crow@cacheflow.com]\n> Sent:Tuesday, April 15, 1997 11:02 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:RE: 1xx Clarification\n> \n> If I understand correctly you are interested in 1xx responses\n> appearing within the context of a tcp connection to a HTTP server but\n> outside of  the context of a HTTP request.  If this is correct, I\n> don't see how this will work through a proxy since proxies generally\n> only maintain a connection to the end server for the duration of the\n> request.  Even if the proxy maintains a persistent connection to the\n> server, the connection may only be allocated to a specific client for\n> the duration of a HTTP request.  So in your scenario, going through a\n> proxy I think the sequence would be the following:\n> \n> User Establishes Connection to Proxy\n> User sends GET to Proxy\n> Proxy Establishes connection to Server\n> Server sends 1xx Response to Proxy\n> Proxy sends 1xx response to client\n> Proxy sends GET to Server\n> Server sends second 1xx Response to Proxy\n> Proxy sends second 1xx response to client\n> Server Sends 200 Response to Proxy\n> Proxy sends 200 response to client\n> Server sends third 1xx Response to Proxy\n> Proxy sends third 1xx response to client\n> Proxy closes server connection or \"assigns\" it to another\n> \"client\"\n> (note that this could happen before the 3rd 1xx response\n> is received)\n> User Closes Connection to Proxy\n> \n> so the first 1xx response makes it through, but not outside of the\n> request context.  The second 1xx response works as expected, and the\n> 3rd may or may not make it to the client based on a race condition of\n> when the proxy closes or \"reuses\" the server connection.\n> \n> -Doug Crow\n\n\n\n"
        },
        {
            "subject": "Issues-list item &quot;CACHINGCGI&quot",
            "content": "According to my notes from the Memphis meeting, I agreed to draft\na clarification for the spec, in response to this item.  The\nreference from the Issues List is to a message I wrote in December:\n\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q4/0363.html\n\nThe question here is \"when should a cache store and reuse a response\nfrom a CGI script?\".\n\nI'm not sure that the HTTP/1.1 specification needs to say much more\nabout this ... but since it apparently was not sufficiently clear\nto at least some readers, I'll propose an editorial change.\n\nCurrently, in section 13.9 (Side Effects of GET and HEAD) reads,\nin its entirety:\n\n   Unless the origin server explicitly prohibits the caching of their\n   responses, the application of GET and HEAD methods to any resources\n   SHOULD NOT have side effects that would lead to erroneous behavior if\n   these responses are taken from a cache. They may still have side\n   effects, but a cache is not required to consider such side effects in\n   its caching decisions. Caches are always expected to observe an\n   origin server's explicit restrictions on caching.\n\n   We note one exception to this rule: since some applications have\n   traditionally used GETs and HEADs with query URLs (those containing a\n   \"?\" in the rel_path part) to perform operations with significant side\n   effects, caches MUST NOT treat responses to such URLs as fresh unless\n   the server provides an explicit expiration time. This specifically\n   means that responses from HTTP/1.0 servers for such URIs should not\n   be taken from a cache. See section 9.1.1 for related information.\n\n[9.1.1 defines \"safe methods\".]\n\nI propose adding this to the end of section 13.9:\n\nNote that some HTTP/1.0 cache operators have found that it is\ndangerous to cache responses to requests for URLs including the\nstring \"cgi-bin\".  HTTP/1.1 caches should follow this practice\nfor responses that do not include an explicit expiration time.\nHTTP/1.1 origin servers that want to allow caching of responses\nfor URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\nexpiration time.  Explicit expiration times may be specified\nusing Expires, or the max-age directive of Cache-Control, or\nboth.\n\n-Jeff\n\nP.S.: I base the first sentence in the note on the sample configuration\nfile distributed with a recent version of the Squid cache software.\nIf this is actually contrary to normal practice, someone should say so.\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Jeffrey Mogul wrote:\n\n> I'm not sure that the HTTP/1.1 specification needs to say much more\n> about this ... but since it apparently was not sufficiently clear\n> to at least some readers, I'll propose an editorial change.\n\n> I propose adding this to the end of section 13.9:\n> \n> Note that some HTTP/1.0 cache operators have found that it is\n> dangerous to cache responses to requests for URLs including the\n> string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n> for responses that do not include an explicit expiration time.\n> HTTP/1.1 origin servers that want to allow caching of responses\n> for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n> expiration time.  Explicit expiration times may be specified\n> using Expires, or the max-age directive of Cache-Control, or\n> both.\n\nI think CERN server is usually configured with \"htbin\" as CGI directory.\nThere are still a lot of them around. And people who switched from CERN\nto something else probably kept htbin directory because all existing\npages pointed to it.\n\nI'm a bit confused with the proposed addition. I thought \n   \n   Cache-Control: public\n\nwould be enough, but that's not explicitly stated.\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@srce.hr\n     |        dave@fly.cc.fer.hr\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "> I propose adding this to the end of section 13.9:\n> \n> Note that some HTTP/1.0 cache operators have found that it is\n> dangerous to cache responses to requests for URLs including the\n> string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n> for responses that do not include an explicit expiration time.\n> HTTP/1.1 origin servers that want to allow caching of responses\n> for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n> expiration time.  Explicit expiration times may be specified\n> using Expires, or the max-age directive of Cache-Control, or\n> both.\n> \n> -Jeff\n> \n> P.S.: I base the first sentence in the note on the sample configuration\n> file distributed with a recent version of the Squid cache software.\n> If this is actually contrary to normal practice, someone should say so.\n\nActually, the simple #1 rule for caching/non-caching of all time is:\n\nNEVER cache an HTTP/1.0 response that does not carry a\nLast-modified: header.\n\nThis supercedes any heuristics based on URLs containing strings like\n\"cgi-bin\", \".cgi\" or \"htbin\".  It's also intuitive: if a response\ndoesn't content L-M it implies that it probably didn't exist in this\nform before I asked for it, and probably won't exist in this form\nafter so it's not worth caching.\n\nThis rule is strictly obeyed by both CERN and Netscape Proxies.\nSpecific proxy applications that know their data source intimately may\nbreak this rule.  This assumes that the freshness of documents is\nguaranteed by configuration, not by the protocol.\n\nI've always strongly encouraged that both Last-modified: and Expires:\nbe sent out from CGI's whose result is produced from data that has a\nspecific last modification time, and will have a certain specific\nmodification time in the future (e.g. a database that is synchronized\nat every midnight, but during the 24 hour period in between the\nresponse would be unchanged for the same request).\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": ">The question here is \"when should a cache store and reuse a response\n>from a CGI script?\".\n\nCGI is no different than any other part of the server.  I think it\nis a mistake to encode namespace assumptions into the protocol,\nparticularly when we have already provided a means for origin servers\nto explicitly mark something as non-cachable.\n\n>   We note one exception to this rule: since some applications have\n>   traditionally used GETs and HEADs with query URLs (those containing a\n>   \"?\" in the rel_path part) to perform operations with significant side\n>   effects, caches MUST NOT treat responses to such URLs as fresh unless\n>   the server provides an explicit expiration time. This specifically\n>   means that responses from HTTP/1.0 servers for such URIs should not\n>   be taken from a cache. See section 9.1.1 for related information.\n\nI would prefer to delete the above from the spec.\n\n>[9.1.1 defines \"safe methods\".]\n>\n>I propose adding this to the end of section 13.9:\n>\n>Note that some HTTP/1.0 cache operators have found that it is\n>dangerous to cache responses to requests for URLs including the\n>string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n>for responses that do not include an explicit expiration time.\n>HTTP/1.1 origin servers that want to allow caching of responses\n>for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n>expiration time.  Explicit expiration times may be specified\n>using Expires, or the max-age directive of Cache-Control, or\n>both.\n\nI think it is a bad idea -- whether or not a resource is based on\na script has nothing to do with its cachability.  If we need a backwards\nway to protect against old CGI scripts, then use the Last-Modified \ndistinction that Ari mentioned.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "It's probably true that most CGI output is not intended to be cached, but\nI don't see any a priori reason why CGI output should never be cached.\nOff-hand, I don't see any reason why the current time can't be sent as the\nlast modified time for the resource. As a practical example, I use CGI to\nproduce indexes to the archives for a mailing list. Generally, digests are\ngenerated once a day, and the archive are updated with each digest. Now,\nif digests were always generated once a day, then it seems natural enough\nto generate cacheable output with a freshness lifetime of 24 hours. In\nfact, I've had people read the list exclusively through the web, and not\nby mail, and so it's quite possible that the same person would visit the\nsite several times during the day.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Tue, 15 Apr 1997, Roy T. Fielding wrote:\n\n> CGI is no different than any other part of the server.  I think it\n> is a mistake to encode namespace assumptions into the protocol,\n> particularly when we have already provided a means for origin servers\n> to explicitly mark something as non-cachable.\n> \n\nI absolutely agree. There is nothing to stop a CGI programmer from\ngenerating appropriate Expires: and Cache-control: headers. In fact, I\ndon't know why a  server couldn't have an option feature to disable\ncaching of resources with URLs matching *.cgi, http://domain/cgi-bin/*,\netc. But by all means, make this optional.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Tue, 15 Apr 1997, Gregory J. Woodhouse wrote:\n\n> On Tue, 15 Apr 1997, Roy T. Fielding wrote:\n> \n> > CGI is no different than any other part of the server.  I think it\n> > is a mistake to encode namespace assumptions into the protocol,\n> > particularly when we have already provided a means for origin servers\n> > to explicitly mark something as non-cachable.\n> > \n> \n> I absolutely agree. There is nothing to stop a CGI programmer from\n> generating appropriate Expires: and Cache-control: headers. In fact, I\n> don't know why a  server couldn't have an option feature to disable\n> caching of resources with URLs matching *.cgi, http://domain/cgi-bin/*,\n> etc. But by all means, make this optional.\n\nI don't .... while there is nothing to stop a CGI programmer from doing\nX or anything a new version of the spec suggests/requires, a unilateral\nchange in the base assumptions can result in rather unsatisfactory\nresults for users.\n\nThis is a clear area where distinction between a history buffer and\nprivate client cache is important to achieve predictable results for the\nuser. It is OK in my experience and indeed expected that a BACK button\nwill show the last version of the page the user actually saw ... this is\nlike a user of an old TTY flipping back thru the paper to review old\noutput.  On the otherhand, without a signal that it is appropriate to\nserve a response from a cache where the URL includes the ? separator,\ncurrent behavior shouldn't be expected to change.\n\nSide-effects are not the only issue ... data base queries (I did not\nsay web crawler index search) often have no side effect and hence are\nsave to repeat BUT each response might be different as a result of other\nuser transactions against the data base.\n\nI know of older applications which were written with the understanding\nthat the ? meant that a fresh result would be acquired each time the\nuser submitted a form. It so happens that the application also did not\nsend Last-modified or Expires so I can't assert with assurance that that\napplication would break of the current wording is removed but my sense of\nwww CGI development is that some percentage will break and we really don't\naccomplish anything my helping that along since we have defined perfectly\ngood mechanisms so that a 'modern' application/server can suggest caching\nwhen appropriate.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "> Your argument has the underlying assumption that the notification\n> information was \"meant\" for a particular user. There are many scenarios,\n> such as the server saying things like \"I am going down\" or \"My resources\n> are getting limited\" where the asynchronous notification is meant for\n> whomever has the connection open.\n> \n\nWell, so 1xx messages sent between a request and a response\napply to that response, but 1xx messages sent without\nan outstanding request only apply \"generically\" to whomever\nhas the connection open. Given the possibility of pipelining,\nit might be hard to tell which case you have. If I pipeline\ntwo requests and I get a 1xx after the first response and\nbefore the second, was it generated AFTER the second request\nwas received or before?\n\n\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "I believe we should leave this issue open. I feel that Roy's attitude is\nbest, \"If you don't understand it, then dump it.\" If others need to\nsolve this problem they are free to add headers, new 1xx messages with\nbodies, etc.\nYaron\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Tuesday, April 15, 1997 11:21 PM\n> To:Yaron Goland\n> Cc:'doug_crow@cacheflow.com'; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: 1xx Clarification\n> \n> > Your argument has the underlying assumption that the notification\n> > information was \"meant\" for a particular user. There are many\n> scenarios,\n> > such as the server saying things like \"I am going down\" or \"My\n> resources\n> > are getting limited\" where the asynchronous notification is meant\n> for\n> > whomever has the connection open.\n> > \n> \n> Well, so 1xx messages sent between a request and a response\n> apply to that response, but 1xx messages sent without\n> an outstanding request only apply \"generically\" to whomever\n> has the connection open. Given the possibility of pipelining,\n> it might be hard to tell which case you have. If I pipeline\n> two requests and I get a 1xx after the first response and\n> before the second, was it generated AFTER the second request\n> was received or before?\n> \n> \n> --\n> http://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Jeffrey Mogul wrote:\n\n> I'm not sure that the HTTP/1.1 specification needs to say much more\n> about this ... but since it apparently was not sufficiently clear\n> to at least some readers, I'll propose an editorial change.\n\n> I propose adding this to the end of section 13.9:\n>\n>    Note that some HTTP/1.0 cache operators have found that it is\n>    dangerous to cache responses to requests for URLs including the\n>    string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n>    for responses that do not include an explicit expiration time.\n>    HTTP/1.1 origin servers that want to allow caching of responses\n>    for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n>    expiration time.  Explicit expiration times may be specified\n>    using Expires, or the max-age directive of Cache-Control, or\n>    both.\n\nI think CERN server is usually configured with \"htbin\" as CGI directory.\nThere are still a lot of them around. And people who switched from CERN\nto something else probably kept htbin directory because all existing\npages pointed to it.\n\nI'm a bit confused with the proposed addition. I thought\n\n   Cache-Control: public\n\nwould be enough, but that's not explicitly stated.\n\n--\n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@srce.hr\n     |        dave@fly.cc.fer.hr\n\n\n\n"
        },
        {
            "subject": "Issues-list item &quot;CACHINGCGI&quot",
            "content": "According to my notes from the Memphis meeting, I agreed to draft\na clarification for the spec, in response to this item.  The\nreference from the Issues List is to a message I wrote in December:\n\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q4/0363.html\n\nThe question here is \"when should a cache store and reuse a response\nfrom a CGI script?\".\n\nI'm not sure that the HTTP/1.1 specification needs to say much more\nabout this ... but since it apparently was not sufficiently clear\nto at least some readers, I'll propose an editorial change.\n\nCurrently, in section 13.9 (Side Effects of GET and HEAD) reads,\nin its entirety:\n\n   Unless the origin server explicitly prohibits the caching of their\n   responses, the application of GET and HEAD methods to any resources\n   SHOULD NOT have side effects that would lead to erroneous behavior if\n   these responses are taken from a cache. They may still have side\n   effects, but a cache is not required to consider such side effects in\n   its caching decisions. Caches are always expected to observe an\n   origin server's explicit restrictions on caching.\n\n   We note one exception to this rule: since some applications have\n   traditionally used GETs and HEADs with query URLs (those containing a\n   \"?\" in the rel_path part) to perform operations with significant side\n   effects, caches MUST NOT treat responses to such URLs as fresh unless\n   the server provides an explicit expiration time. This specifically\n   means that responses from HTTP/1.0 servers for such URIs should not\n   be taken from a cache. See section 9.1.1 for related information.\n\n[9.1.1 defines \"safe methods\".]\n\nI propose adding this to the end of section 13.9:\n\n Note that some HTTP/1.0 cache operators have found that it is\n dangerous to cache responses to requests for URLs including the\n string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n for responses that do not include an explicit expiration time.\n HTTP/1.1 origin servers that want to allow caching of responses\n for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n expiration time.  Explicit expiration times may be specified\n using Expires, or the max-age directive of Cache-Control, or\n both.\n\n-Jeff\n\nP.S.: I base the first sentence in the note on the sample configuration\nfile distributed with a recent version of the Squid cache software.\nIf this is actually contrary to normal practice, someone should say so.\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": ">The question here is \"when should a cache store and reuse a response\n>from a CGI script?\".\n\nCGI is no different than any other part of the server.  I think it\nis a mistake to encode namespace assumptions into the protocol,\nparticularly when we have already provided a means for origin servers\nto explicitly mark something as non-cachable.\n\n>   We note one exception to this rule: since some applications have\n>   traditionally used GETs and HEADs with query URLs (those containing a\n>   \"?\" in the rel_path part) to perform operations with significant side\n>   effects, caches MUST NOT treat responses to such URLs as fresh unless\n>   the server provides an explicit expiration time. This specifically\n>   means that responses from HTTP/1.0 servers for such URIs should not\n>   be taken from a cache. See section 9.1.1 for related information.\n\nI would prefer to delete the above from the spec.\n\n>[9.1.1 defines \"safe methods\".]\n>\n>I propose adding this to the end of section 13.9:\n>\n>    Note that some HTTP/1.0 cache operators have found that it is\n>    dangerous to cache responses to requests for URLs including the\n>    string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n>    for responses that do not include an explicit expiration time.\n>    HTTP/1.1 origin servers that want to allow caching of responses\n>    for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n>    expiration time.  Explicit expiration times may be specified\n>    using Expires, or the max-age directive of Cache-Control, or\n>    both.\n\nI think it is a bad idea -- whether or not a resource is based on\na script has nothing to do with its cachability.  If we need a backwards\nway to protect against old CGI scripts, then use the Last-Modified\ndistinction that Ari mentioned.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "It's probably true that most CGI output is not intended to be cached, but\nI don't see any a priori reason why CGI output should never be cached.\nOff-hand, I don't see any reason why the current time can't be sent as the\nlast modified time for the resource. As a practical example, I use CGI to\nproduce indexes to the archives for a mailing list. Generally, digests are\ngenerated once a day, and the archive are updated with each digest. Now,\nif digests were always generated once a day, then it seems natural enough\nto generate cacheable output with a freshness lifetime of 24 hours. In\nfact, I've had people read the list exclusively through the web, and not\nby mail, and so it's quite possible that the same person would visit the\nsite several times during the day.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": ">>>>> \"HS: == Henry Sanders writes:\n\nHS> Around the time of the LA IETF there was some desire to remove\nHS> footers entirely from chunked T-E. Phill Hallam objected strongly\nHS> to this, and a compromise was reached where Content-MD5 was allowed\nHS> in the footer for 1.1 and future versions of HTTP might allow other\nHS> headers. The issue was considered closed for draft 04 in\nHS> http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q2/0058.html .\nHS> The wording has changed a bit since then (I vaguely remember that\nHS> happening as an editorial change) , but I believe the intent is\nHS> still the same, which is that only Content-MD5 is valid in an HTTP\nHS> 1.1 chunked  footer.\n\n>>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com>:\n\nJM> Thanks for digging this up (although the message you cite doesn't\nJM> specifically address this issue).  I probably ignored most of that\nJM> discussion when it took place.\n\nHS> I agree that the text could use some clarification. I think the\nHS> simplest change consistent with my understanding of the intent\nHS> would be to add a sentence to 14.16 saying \"This header is allowed\nHS> in the chunked Transfer-Encoding footer\".\n\n  As I recall, there was a consensus in Memphis that the\n  Authentication-Info header should be allowed in the footer, and I've\n  proposed wording both to clarify the paragraph that started this\n  discussion and to add explicit statements allowing both\n  Authentication-Info and Content-MD5 in the footer.  See:\n\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0063.html\n\nHS> I seem to recall discussions about Content-Length as a chunked\nHS> footer, and I seem to recall it was considered a bad idea.\n\n  We did agree that Content-Length could be added by a Proxy, for what\n  that is worth; it doesn't seem so different.\n\nJM> But if the consensus is that Content-Length doesn't belong in the footer,\nJM> then perhaps we need to define a new \"footer-eligible\" header that\nJM> serves purpose that we are looking for: \"Stop this response, I didn't\nJM> mean it when I said '200' back in the header\".  E.g., a new header\nJM> called \"Failed:\" which can convey an updated status code and message.\n\n  Our present implementation just closes the connection without\n  sending the terminating (zero-length) chunk.  Browsers all seem to\n  display some sort of error when you cut off a response in the middle\n  if you've passed Content-Length in the header; we assume that they\n  would do the same for this case.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "Scott Lawrence wrote:\n\n> [...]\n> HS> I seem to recall discussions about Content-Length as a chunked\n> HS> footer, and I seem to recall it was considered a bad idea.\n\nActually, those words were mine.\n> \n>   We did agree that Content-Length could be added by a Proxy, for what\n>   that is worth; it doesn't seem so different.\n> [...]\n\nThat's different.  A proxy can collect the entire chunked response and, once\nsuccessful, can add a Content-Length header in the message headers (and eliminate\nchunking).  If there's a Content-Length as a footer, then we have to decide what\nto do if that length disagrees with the entity body length gathered through\nchunking.  Jeff Moguls's solution, \"consider the response to be broken\", is one\nchoice.  Another possibility is \"disregard the (obviously wrong) header\".\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Issues-list item &quot;CACHINGCGI&quot",
            "content": "http-wg@cuckoo.hpl.hp.com writes:\n >  Note that some HTTP/1.0 cache operators have found that it is\n >  dangerous to cache responses to requests for URLs including the\n >  string \"cgi-bin\".  HTTP/1.1 caches should follow this practice\n >  for responses that do not include an explicit expiration time.\n >  HTTP/1.1 origin servers that want to allow caching of responses\n >  for URLs including \"?\" or \"cgi-bin\" SHOULD include an explicit\n >  expiration time.  Explicit expiration times may be specified\n >  using Expires, or the max-age directive of Cache-Control, or\n >  both.\n\nWhat about Last-modified?  In my use of cgi-bin programs to generate\npages for HyperNews forums and messages, I don't know about the\nfuture, but I know about the past.  Caching is possible, but\nIf-Modified-Since requests should always be used by cache servers to\nfind out if there has been a change.  So neither Expires nor max-age\nwill do what I want.\n\ndan\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "The more I think about it, the more I think the right thing to do is not\nto return the current time as Last-Modified: but (if this makes sense)\nreturn the last modified date fore the data used to generate the response.\nThis would have to be the CGI programmer's call. In the mailing list\nexample, the natural thing to do would be to return the creation date of\nthe most recent digest included in the archive or (more simply) the last\nmodified date for the directory (under Unix, at least).\n\nFor this approach to be useful, the CGI program would have to be able to\nrespond intelligently to HEAD requests, and return last modified dates\nand perhaps entity tags. Perhaps what we need is separate draft on CGI and\ncaching, and perhaps a draft on CGI for HTTP/1.1. Someone (Jeffrey\nMogul?) suggested I write one at one point, but it was simply impossible\nat the time. If this would be useful, I'd be willing to give it a try.\n\nI do, however, see this as being essentially orthogonal to the process of\nmoving HTTP/1.1 to draft status, so this is kind of late in the game. \n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP practice and Year2000: the bad(?) new",
            "content": "On Sat, 12 Apr 1997, Phillip M Hallam-Baker wrote:\n\n> After all the problem will go away in 2000 for the\n> rest of our lifetime.\n\n.. how about 32-bit problems in 2038, if our hardware lasts that long ?\n(I hope that's within my lifetime, with a bit of luck!)\n\nAndrew\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Wed, 16 Apr 1997, Gregory J. Woodhouse wrote:\n\n> The more I think about it, the more I think the right thing to do is not\n> to return the current time as Last-Modified: but (if this makes sense)\n> return the last modified date fore the data used to generate the response.\n\nAt some point I wrote http://vancouver-webpages.com/proxy/log-tail.pl \n(Perl using LWP4) which does this kind of thing. It's a bit of a \nnuiscance compared to just doing\n\nprint<<EOT;\nContent-type: text/html\n\n<title>Here we go!</title>\nHey..\nEOT\n\nand I confess I haven't moved to LWP5 yet - there may be a package to do \nit now.\nIn my search engine I attempt to handle IF_MODIFIED_SINCE, set \nLast-Modified to the modification date of the last key to change, and set \nExpires to the expected database update time. I haven't yet added any \nHTTP/1.1 headers.\nSome notes on this are at \nhttp://vancouver-webpages.com/CacheNow/detail.html#CGI\n\nIn my copy of Squid; I have hierarchy_stoplist set to cgi-bin,? to match\nthe cache_stoplist of the parent, but cache_stoplist set to \"map? gif?\"\nA scan of the access log shows several hits on DejaNews queries.\n\nServer-side imagemap queries are clearly cacheable, but would have a very \nlow hit rate under normal conditions. Hmm; I think the output is usually \na redirect hence uncacheable; oh well ...  map? was from the xerox Parc \nmap server\n\nAndrew Daviel\nTRIUMF; Vancouver Webpages\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "> The more I think about it, the more I think the right thing to do is not\n> to return the current time as Last-Modified: but (if this makes sense)\n> return the last modified date fore the data used to generate the response.\n\nCouldn't have put it better myself.\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: HTTP practice and Year2000: the bad(?) new",
            "content": "> > After all the problem will go away in 2000 for the\n> > rest of our lifetime.\n> \n> .. how about 32-bit problems in 2038, if our hardware lasts that long ?\n> (I hope that's within my lifetime, with a bit of luck!)\n\nWell, this may be a slightly inappropriate remark, but I'll be pretty\ndamn pissed off if we're still stuck using HTTP/1.x in 2038. ;) ;) ;)\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Wow, things are confused.  Part of this is my fault, for writing\nsomething that wasn't as precise as it should have been.\n\nMost of the confusion is on the part of people who do not distinguish\nbetween \"what a CGI script should send\" and \"what a cache should do\nwhen a CGI script doesn't send what it should have sent\".\n\nOF COURSE, CGI-generated responses (and all other origin server\nresponses, too) should include either an explicit \"don't cache\nthis\" marking (e.g., \"Cache-control: max-age=0\") or an explicit\nexpiration time.  Of course, of course, of course.\n\nThat wasn't the question.  The question (which I stated too\ninformally) was \"what happens when the response is NOT clearly\nmarked as to cachability?\"  This is a *different* question.\n\nCurrent practice seems to be split.  Ari says that the CERN\nand Netscape proxies never cache a response without a Last-Modified\nheader. (With HTTP/1.1, this rule would presumably change to\n\"without either a Last-Modifed or Etag header.\")  However,\nthe practice in the Squid world seems to be different.  I'm\nnot sure I fully understand the Squid code, but the version\nI looked at seems to allow caching of a response without\na Last-Modified header.\n\nI was tasked by the working group meeting last week to address the\nspecific issue of CGI, not the larger issue of whether a response\nwithout Last-Modified should be cached.  based on my belief that the\nHTTP/1.1 spec should not discourage caching unnecessarily (reflecting\nwhat Roy wrote earlier, that the Web \"depends on accurate caching to\nreduce network costs\"), I constructed my proposed Note to reflect the\nlooser approach used by Squid.\n\nSo let's take these issues separately.  If someone wants to\npropose a specification change (or a new Note for the spec)\nthat says \"do not cache responses without a Last-Modified header\",\nthat's fine with me, although it would be a good idea to\ncombine this proposal with evidence (from a real-life proxy)\nthat this doesn't significantly reduce caching in today's Internet.\n\nBack to the CACHING-CGI issue.  My original proposal was sloppy\nin that it didn't make a distinction between \"cache and reuse\nwithout revalidation\" or \"cache but must revalidate\".  And I\nforgot to include \"htbin\" as being more or less equivalent\nto \"cgi-bin\" (and yes, there are still lots of htbin URLs in\nactive use).  Also, I violated the informal rule that Notes should\nnot use terms like \"SHOULD\".\n\nHere's a revised version, to replace the second paragraph\nin section 13.9:\n\nSome HTTP/1.0 cache operators have found that it is dangerous\nto cache and reuse without revalidation responses to requests\nfor URLs that include any of the strings \"cgi-bin\", \"htbin\", or\n\"?\", because applications have traditionally used these URLs in\nconjunction with operations with significant side effects for\nGET or HEAD methods.  However, if such a response includes an\nexplicit, future, expiration time, then this implies that the\nresponse may be cached and reused without revalidation until it\nexpires.  If such a response includes a Last-Modified or Etag\nheader, this implies that the response may be reused after\nrevalidation (or without revalidation if explicitly fresh).\n\nA cache MUST NOT assign a heuristic expiration time to a\nresponse for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n\"?\" in its rel_path part.  If such a response does not \ncarry an explicit expiration time, it must be treated as\nif it expires immediately.\n\nThis does two things: (1) it clarifies that a cache can indeed\nfollow its usual caching with \"?\" and \"cgi-bin\" responses, if\nthey are explicitly marked to allow caching, and (2) we tighten\nup the rules on assigning heuristic expiration times for such\nresponses, because of the known risks of this specific situation.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Sorry to randomize y'all a bit but,\n\nMicrosoft Proxy Server v1 caches objects that don't have LM.   \n\nOver the course of the billions of objects we've proxied, we've found\nthat all objects that generate dynamic output (as well as all authoring\ntools that generate dynamic HTML) have some other directive (e.g.\nimmediate expires; cache-control:private, etc.) that indicate the\nnon-cacheability of the object.\n\n\n-----Original Message-----\nFrom:Ari Luotonen [SMTP:luotonen@netscape.com]\nSent:Tuesday, April 15, 1997 6:41 PM\nTo:mogul@pa.dec.com\nCc:http-wg@cuckoo.hpl.hp.com\nSubject:Re: Issues-list item \"CACHING-CGI\"\n\n\n> I propose adding this to the end of section 13.9:\n> \n> Note that some HTTP/1.0 cache operators have found that\nit is\n> dangerous to cache responses to requests for URLs\nincluding the\n> string \"cgi-bin\".  HTTP/1.1 caches should follow this\npractice\n> for responses that do not include an explicit expiration\ntime.\n> HTTP/1.1 origin servers that want to allow caching of\nresponses\n> for URLs including \"?\" or \"cgi-bin\" SHOULD include an\nexplicit\n> expiration time.  Explicit expiration times may be\nspecified\n> using Expires, or the max-age directive of\nCache-Control, or\n> both.\n> \n> -Jeff\n> \n> P.S.: I base the first sentence in the note on the sample\nconfiguration\n> file distributed with a recent version of the Squid cache\nsoftware.\n> If this is actually contrary to normal practice, someone\nshould say so.\n\nActually, the simple #1 rule for caching/non-caching of all time\nis:\n\nNEVER cache an HTTP/1.0 response that does not carry a\nLast-modified: header.\n\nThis supercedes any heuristics based on URLs containing strings\nlike\n\"cgi-bin\", \".cgi\" or \"htbin\".  It's also intuitive: if a\nresponse\ndoesn't content L-M it implies that it probably didn't exist in\nthis\nform before I asked for it, and probably won't exist in this\nform\nafter so it's not worth caching.\n\nThis rule is strictly obeyed by both CERN and Netscape Proxies.\nSpecific proxy applications that know their data source\nintimately may\nbreak this rule.  This assumes that the freshness of documents\nis\nguaranteed by configuration, not by the protocol.\n\nI've always strongly encouraged that both Last-modified: and\nExpires:\nbe sent out from CGI's whose result is produced from data that\nhas a\nspecific last modification time, and will have a certain\nspecific\nmodification time in the future (e.g. a database that is\nsynchronized\nat every midnight, but during the 24 hour period in between the\nresponse would be unchanged for the same request).\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not\nNetscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Road\nhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server\nDevelopment\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Drazen Kacar <dave@public.srce.hr> writes:\n\n    I'm a bit confused with the proposed addition. I thought \n       \n       Cache-Control: public\n    \n    would be enough, but that's not explicitly stated.\n    \nThe \"public\" directive was intended for a different purpose.  From\nRFC2068:\n\n    public\n      Indicates that the response is cachable by any cache, even if it\n      would normally be non-cachable or cachable only within a\n      non-shared cache. (See also Authorization, section 14.8, for\n      additional details.)\n\nWhile simply adding \"Cache-control: public\" to a response does\nimply that it is cachable, this doesn't say enough.  I.e., how\nlong is the response \"fresh\"?  It seems more useful, in general,\nto use\nCache-control: max-age=3600\n(or whatever), since this also implies cachability, but it also\ngives more explicit information to the cache.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Jeffrey Mogul wrote:\n\n> The \"public\" directive was intended for a different purpose.  From\n> RFC2068:\n> \n>     public\n>       Indicates that the response is cachable by any cache, even if it\n>       would normally be non-cachable or cachable only within a\n>       non-shared cache. (See also Authorization, section 14.8, for\n>       additional details.)\n> \n> While simply adding \"Cache-control: public\" to a response does\n> imply that it is cachable, this doesn't say enough.  I.e., how\n> long is the response \"fresh\"?  It seems more useful, in general,\n> to use\n> Cache-control: max-age=3600\n> (or whatever), since this also implies cachability, but it also\n> gives more explicit information to the cache.\n\nWhat if freshness is infinite? I have a CGI which converts from X-Face mail\nheader format to XBM. If it receives If-Modified-Since it always returns\n304. Anything it sends is cacheable and always fresh since it's a converter.\nI wanted it to receive input via PATH_INFO (so it won't look like a CGI),\nbut I've found out that my servers have problems with that, since the input\ncan be quite large. So the URL has `?' in it and CGI returns\n\"Cache-Control: public\" and Last-Modified with some fixed date in the past.\nIt should be good enough to make output cacheable.\n\n-- \n .-.   .-.    Life is a sexually transmitted disease.\n(_  \\ /  _)\n     |        dave@srce.hr\n     |        dave@fly.cc.fer.hr\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "> I believe we should leave this issue open. I feel that Roy's attitude is\n> best, \"If you don't understand it, then dump it.\" If others need to\n> solve this problem they are free to add headers, new 1xx messages with\n> bodies, etc.\n\nI don't have a problem with adding something which has\na well-defined meaning with the caveat that some recipients\nmight not \"understand\" it. The problem is that allowing for\n1xx responses outside of the context of a request would\nmake them ambiguous where they are not now ambiguous.\n\nNow, if a client has sent two pipelined requests, gets one\nresponse, and then a 1xx response, it would be ambiguous as to\nwhether the server intended the 1xx response to be associated\nwith the second request or to be interpreted outside of\nthe context of that request.\n\nIf we (continue to) disallow 1xx responses outside of\nthe context of a specific request, then the context of\nthe response is not ambiguous.\n\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Vinod Valloppillil <vinodv@microsoft.com> wrote:\n\n  > Sorry to randomize y'all a bit but,\n  > \n  > Microsoft Proxy Server v1 caches objects that don't have LM.   \n  > \n  > Over the course of the billions of objects we've proxied, we've found\n  > that all objects that generate dynamic output (as well as all authoring\n  > tools that generate dynamic HTML) have some other directive (e.g.\n  > immediate expires; cache-control:private, etc.) that indicate the\n  > non-cacheability of the object.\n\nYour experience may depend on what origin servers your proxy acts as\nproxy for.  None of the (HTTP/1.0) CGIs I've written (on Unix systems)\nproduce any headers that would indicate anything about cachability.\nBut they also don't contain Last-Modified.  I think that's typical of\nUnix-based servers and their CGIs.\n\nHowever, I just poked an Apache (1.2b7) server with a CGI and found it\nproduces:\n\nCache-Control: no-cache\nExpires: Tue, 01 Jan 1980 00:00:00 GMT\nLast-Modified: Tue, 01 Jan 1980 00:00:00 GMT\n\nSo maybe that's why things largely seem to work.\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Wed, 16 Apr 1997, Jeffrey Mogul wrote:\n\n> the practice in the Squid world seems to be different.  I'm\n> not sure I fully understand the Squid code, but the version\n> I looked at seems to allow caching of a response without\n> a Last-Modified header.\n\nLooking at Squid 1.1 (1.0 was slightly different)\n\nDefault : cache_stoplist cgi-bin ?\n (don't cache queries or anything with cgi-bin in the path)\nDefault : refresh_pattern   . 0 20% 4320\n\nGiven :\n\n    AGE is how much the object has aged *since* it was retrieved:\n                \n        AGE = NOW - OBJECT_DATE\n\n    LM_AGE is how old the object was *when* it was retrieved:\n\n        LM_AGE = OBJECT_DATE - LAST_MODIFIED_TIME\n\n    LM_FACTOR is the ratio of AGE to LM_AGE:\n\n        LM_FACTOR = AGE / LM_AGE\n\n    CLIENT_MAX_AGE is the (optional) maximum object age the client will\n    accept as taken from the HTTP/1.1 Cache-Control request header.\n\n    EXPIRES is the (optional) expiry time from the server reply headers.\n\nand\n  refresh_pattern <URL regular expression> MIN_AGE PERCENT MAX_AGE\n\nthen\n\n    if (CLIENT_MAX_AGE)\n        if (AGE > CLIENT_MAX_AGE)\n            return STALE\n    if (AGE <= MIN_AGE)\n        return FRESH\n    if (EXPIRES) {\n        if (EXPIRES <= NOW)\n            return STALE\n        else\n            return FRESH\n    }\n    if (AGE > MAX_AGE)\n        return STALE\n    if (LM_FACTOR < PERCENT)\n        return FRESH\n    return STALE\n\n(from Release-Notes-1.1.txt)\n\nSo, if I understand this correctly, with a stock config file:\n\nObjects with a Last-Modified header are cached for 3 days, or when they \nexpire, or for 20% of their age when cached, whichever is shortest.\n\nObjects without a Last-Modified header are cached for 3 days, or until \nthey expire.\n\nQueries are not cached at all.\n\nOne can modify the config, as I have done, to cache queries, perhaps \npattern-matched to expire sooner than text or images.\n\nApart from CGI, server-side includes in Apache without XBitHack will\ntypically not have a Last-Modified date. \n\n(Incidentally, from a recent survey I did, I see \n  91% with Date\n  70% with Content-Length\n  64% with Last-Modified\n  12% with Etag \n   4% with Expires\n   1% with Set-Cookie\n   1% with Cache-Control \nthough I know we're discussing the future, not the present)\n\nAndrew Daviel\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Hi,\n> Actually, the simple #1 rule for caching/non-caching of all time is:\n> \n> NEVER cache an HTTP/1.0 response that does not carry a\n> Last-modified: header.\nI don't think that would be right. Ari: sorry for mentioning this, but\nthat seems to be a good example: the URL http://home.netscape.com/ does\nnot contain Last-Modified header, but at our campus couple of users often\nvisit this site. I don't want our backbone congested by this traffic since\nthis page doesn't change in every minute or so.\n\nCheers,\nBertold\n\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n Kolics, Bertold                             E-Mail: bertold@tohotom.vein.hu\n University of Veszprem, Hungary        W3: http://tohotom.vein.hu/~bertold/\n Information Engineering Course\n==-==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==--==\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication, Netscape, and Microsof",
            "content": ">>>>> \"AL\" == Ari Luotonen <luotonen@netscape.com> writes:\n\nAL> SSL does allow a null-cipher -- in Netscape Servers it's enabled via\nAL> choice \"No encryption, only MD5 message authentication\".  This\nAL> provides certificate based authentication and message integrity on\nAL> HTTP data, but the data is not encrypted, so there's minimal overhead.\n\n  It is not nearly as minimal as 2069 - in order use even a null\n  cipher, I must be able to process a certificate.  For a good many\n  systems, this is too costly (in code to do public key certificate\n  handling, and licensing of that technology) and not justified by the\n  product requirements.  I don't want to do RSA code in an ethernet\n  repeater or a web coffeepot (and only one of those is a frivolous\n  example).\n\n  Certificate based security is wonderfull, and I fully support its\n  wide use in the Internet and incorporation into all sorts of\n  standards, but it is _not_ a replacement for simpler schemes which\n  have different requirements.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: 1xx Clarificatio",
            "content": "I've been watching this discussion, and have several observations, and\na (so far personal) conclusion:\n\n1) HTTP lacks a notification mechanism.  The RTSP people, for example,\nhave a real requirement for this ability, and everyone doing cooperative\nwork has identified this as a basic need.  I agree.  Certainly the protocols\nI've worked on in the past found notification an essential part of the\nprotocol, and if HTTP 1.X is to be extended further, this is a necessity.\n\n2) Larry has identified a problem with using 1XX responses for this;\nin particular, distinguising one generated as a normal part of the\nprotocol transaction (i.e. the client is expecting it may get such\na response), vs. one generated completely asynchronously.  Getting\na client confused due to a race condition is a \"bad thing\" in my opinion.\n\n3) It is probably not nice to send such asynchronous notifications to\nclients not expecting them.  The way we handled this in the X protocol\nwas by having the client express interest in (a class of) an event type.\nIf the client doesn't know about the event type, he won't ask about it,\nand therefore won't recieve event types they don't expect/understand.\n\nThis is telling me that while subverting 1XX responses for notifications\nand other items is possible, it is probably wrong to shoehorn it into\n1XX responses and probably a real kludge to do so. Confusing\nresponses with events will generate no end of confusion, particularly\nsince HTTP/1.X lacks any sort of request # in the response to allow\nfor bookkeeping between which request generated what response.  (A fundamental\nlack in HTTP; sigh...).\n\nMy conclusion:\n\nHTTP needs an \"Event\" mechanism, separate from 1XX responses (after all,\nthey are \"responses\", and clients can expect them at particular points\nin the conversation).\n\nI think the cleanest way to do this is to define a new set of response\ncodes as \"event codes\".  Such events could be sent asynchronously to\nany client which has \"selected\" that event type (and not sent to clients\nthat don't select them.), for as long as that connection is established.\nClients that don't know about events won't see them.\n\nI think the way to make progress quickly here is for someone to write\nup a short ID on the topic, with the intent of going to proposed standard\nwith the HTTP/1.1 draft standard.  I expect this should only take\na couple pages, and solve a bunch of people's problems.\n\nAny volunteers?\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "RE: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Dave, you wrote:\n\n>Your experience may depend on what origin servers your proxy acts as\n>proxy for.  None of the (HTTP/1.0) CGIs I've written (on Unix systems)\n>produce any headers that would indicate anything about cachability.\n>But they also don't contain Last-Modified.  I think that's typical of\n>Unix-based servers and their CGIs.\n\nI am not sure what is typical, but I have two CGI applications on our\nIntranet (our Corporate Technical Memory electronic reference document\nrepository and our CAD Productivity Tools home-grown CAD tools\ninterface), both of which use caching control headers.  For CTM, cache\ncontrol is used to enforce data freshness as the repository database can\nbe updated at any time.  With the CAD Productivity Tools web, we must\nauthenticate users (using a home-brew variation on Digest\nAuthentication), so we need to keep the authentication cookie fairly\nfresh to discourage session hijacking.\n\nI would honestly expect to use cache control on any CGI that makes\ndatabase queries, unless the database was entirely and only controlled\nby the CGI program.\n================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                 Indianapolis, IN\n\"ViaCrypt?  Vhy not!\"\n\n\n\n"
        },
        {
            "subject": "RE: Digest Authentication, Netscape, and Microsof",
            "content": "Phill, you wrote:\n\n>SSL unfortunately provides a relatively weak form of security. It\n>is great if your definition of security is the use of cryptography.\n>It has no real model of how it should interact with firewalls for\n>example - nobody sends encrypted data through the firewalls I have\n>experience with, that is part of their purpose. Nor can data from \n>an SSL transaction be cached by an intermediary.\n\nSo are you saying that all firewalls you have encountered block SSL?\nFor companies like us, that would seem to block the easiest method of\nsending encrypted data to sites outside our firewall.  (Our firewall\ndoes permit SSL.)  Or did you mean something else?\n================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                 Indianapolis, IN\n\"ViaCrypt?  Vhy not!\"\n\n>\n\n\n\n"
        },
        {
            "subject": "Re: privacy platform",
            "content": "To: Larry Masinter<masinter@parc.xerox.com>\nCc:swick@w3.org,reagle@w3.org\n\nLarry Masinter wrote:\n> \n> I'm not sure what W3C will do, but one idea I heard\n> at the CFP conference was the possibility of using\n> something like PICS to rate sites by\n> how well (or poorly) they treat the privacy of users\n> who visit their sites.\n\nSomething like that.\n\nAs you noted at the recent IETF meeting in Memphis,\nthe privacy and demographics issue is stretching\nthe bounds of the charter of the HTTP working group.\n\nI mentioned W3C's work on this issue at that meeting.\nHere are the details I alluded to...\n\nSocial policy issues surrounding web technology are a core\ncompetency of the W3C Technology and Society Domain:\nhttp://www.w3.org/pub/WWW/TandS/\n\nW3C is collecting input on the following proposal:\n\n        Executive Summary of Platform for Privacy\n        Preferences (P3) Project\n        2 April, 1997\n        Ralph Swick swick@w3.org, Project Manager\n        Joseph Reagle reagle@w3.org, Policy Analyst\n        http://www.w3.org/pub/WWW/Privacy/exec-P3Brief.html\n\n\nFor further details, please contact Ralph Swick or Joseph Reagle.\n\n\n-- \nDan Connolly, W3C Architecture Domain Lead\n<connolly@w3.org> +1 512 310-2971\nhttp://www.w3.org/People/Connolly/\nPGP:EDF8 A8E4 F3BB 0F3C FD1B 7BE0 716C FF21\n\n\n\n"
        },
        {
            "subject": "Memphis followu",
            "content": "As promised in Memphis, here is the text of the clarifications I\nproposed.  \n\nSTATUS100 Message transmission requirements may need clarification (rlgray)\n-------------------------------------------------------------------------------\n\n10.1 Informational 1xx\n\nIs:\n---\nThis class of status code indicates a provisional response, consisting only\nof the Status-Line and optional headers, and is terminated by an empty line.\n\nProposed:\n---------\nThis class of status code indicates a provisional response, consisting only\nof the Status-Line and optional headers, and is terminated by an empty line.\n| There are no required headers for this class of status codes.\n\n-------------------------------------------------------------------------------\n\n14.19 Date\n\nIs:\n---\nHowever, since the date--as it is believed by the origin--is important for\nevaluating cached responses, origin servers MUST include a Date header field\nin all responses.\n\nProposed:\n---------\nHowever, since the date--as it is believed by the origin--is important for\nevaluating cached responses, origin servers MUST include a Date header field\n| in all responses, except those with 1xx status.\n\n-------------------------------------------------------------------------------\n \n\nRegards,\nRichard L. Gray\nInternet Connection Server Development\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "I wrote:\n\n> While simply adding \"Cache-control: public\" to a response does\n> imply that it is cachable, this doesn't say enough.  I.e., how\n> long is the response \"fresh\"?  It seems more useful, in general,\n> to use\n> Cache-control: max-age=3600\n> (or whatever), since this also implies cachability, but it also\n> gives more explicit information to the cache.\n\nDrazen Kacar <dave@public.srce.hr> writes:\n\n    What if freshness is infinite?\n\nThen send something like\nCache-control: max-age=99999999\nwhich isn't exactly \"infinite\", but (at about 3 years) is close\nenough for any realistic cache.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "no one supports Diges",
            "content": "In light of the silence from Netscape and Microsoft on my question of\nwhether their browsers will support Digest Authentication, I will state\nfor the record that neither vendor supports it and neither has said\nthey *would* support it.\n\n(I believe Spyglass does support Digest.  At least they used to.)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Wed, 16 Apr 1997, Andrew Daviel wrote:\n\n> Looking at Squid 1.1 (1.0 was slightly different)\n> \n>     if (AGE > MAX_AGE)\n>         return STALE\n>     if (LM_FACTOR < PERCENT)\n>         return FRESH\n>     return STALE\n> \n> (from Release-Notes-1.1.txt)\n> \n> So, if I understand this correctly, with a stock config file:\n> \n> Objects with a Last-Modified header are cached for 3 days, or when they \n> expire, or for 20% of their age when cached, whichever is shortest.\n> \n***> Objects without a Last-Modified header are cached for 3 days, or \n***> until > they expire.\n\nWRONG! My mistake. If LM_FACTOR is undefined, the algorithm returns \nSTALE. Squid 1.1.2 issues a GET/If-Modified-Since every time if neither \nLast-Modified nor Expires are defined.\n\nAndrew Daviel         mailto:advax@triumf.ca \nTRIUMF & Vancouver Webpages\n\n\n\n"
        },
        {
            "subject": "Re: no one supports Diges",
            "content": "On Thu, 17 Apr 1997, Dave Kristol wrote:\n\n> In light of the silence from Netscape and Microsoft on my question of\n> whether their browsers will support Digest Authentication, I will state\n> for the record that neither vendor supports it and neither has said\n> they *would* support it.\n> \n> (I believe Spyglass does support Digest.  At least they used to.)\n> \n> Dave Kristol\n> \n\nWell, the version that comes bundled with Website does.\n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "No Date in 1xx responses (was Re: Memphis followup",
            "content": "These changes look fine to me, and match how we implemented 100\nin Apache 1.2b.\n\n.....Roy\n\n>STATUS100 Message transmission requirements may need clarification (rlgray)\n>------------------------------------------------------------------------------\n>\n>10.1 Informational 1xx\n>\n>Is:\n>---\n>This class of status code indicates a provisional response, consisting only\n>of the Status-Line and optional headers, and is terminated by an empty line.\n>\n>Proposed:\n>---------\n>This class of status code indicates a provisional response, consisting only\n>of the Status-Line and optional headers, and is terminated by an empty line.\n>| There are no required headers for this class of status codes.\n>\n>------------------------------------------------------------------------------\n>\n>14.19 Date\n>\n>Is:\n>---\n>However, since the date--as it is believed by the origin--is important for\n>evaluating cached responses, origin servers MUST include a Date header field\n>in all responses.\n>\n>Proposed:\n>---------\n>However, since the date--as it is believed by the origin--is important for\n>evaluating cached responses, origin servers MUST include a Date header field\n>| in all responses, except those with 1xx status.\n>\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "\"Scott Lawrence\" <lawrence@agranat.com> writes:\n\n  Our present implementation just closes the connection without\n  sending the terminating (zero-length) chunk.  Browsers all seem to\n  display some sort of error when you cut off a response in the middle\n  if you've passed Content-Length in the header; we assume that they\n  would do the same for this case.\n\nThis makes sense as long as the content that *is* received is\nnon-corrupt.  I guess the worrisome case is if the failing\ncontent-generation mechanism generates bogus content, not just\ntruncated content.\n\nThe browsers I've used, when they see a premature close for a\nnon-chunked response, display the data received so far, and a \"Transfer\nInterrupted!\" warning.  This doesn't necessarily warn the user \"and by\nthe way, what you already have on your screen might be bogus\".\n\nBut this is probably just \"advice to client implementors\", not\nsomething we need to put into the spec.  (And I've seldom seen\nany content-generator realize when it has generated bogus data,\nalthough I have certainly seen enough cases where the output was\nclearly broken.)\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "Regardless of the possible need for a \"notification mechanism\",\nas Jim Gettys discusses, there is confusion over the use of\n100 (Continue) messages as specified in RFC2068 section 8.2\n(Message Transmission Requirements).\n\nThis section is indeed confusing.  Even though I wrote a lot\nof the words here, when I read it last week, I couldn't\nunderstand exactly what was intended.  Another week passed,\nand then I recalled why this was written the way that it is.\n(I tried arguing, last year, that the RFC needed to include some\nrationale statements, but the \"it's too long already\" camp won the\nargument.)\n\nHere's the historical background, as close as I can remember:\n\nRoy was concerned that some servers might not want to accept\ncertain requests with very large bodies (e.g., a POST or PUT).\n\nThe \"100 Continue\" response code was introduced for this purpose.  The\nidea was that when a client sends, e.g., a POST with a large body, it\nwould first send the headers, then it would wait for the server's 100\nresponse before sending the body.  This would allow the server to\nreject the request (e.g., with a 4xx response status) before reading\nthe whole thing.\n\nI believe we called this a \"two-phase\" method (although the term\ndoesn't appear in RFC2068).\n\nOther people objected to this simple design, since it effectively adds\nan entire round-trip to each POST.\n\nThe \"compromise\" that I proposed is this:\n\n    The client first tries to send its request without pausing\n    between the headers and the body.  This results in optimal\n    performance if the server is willing to receive the\n    request, except that the spec requires the server to stick\n    a \"100 Continue\" line just before the real response (so\n    we waste a few header bytes).\n    \n    If the server is happy, fine.  That's it.  No extra delay.\n    \n    If, however, the server is unhappy, it can immediately close\n    the connection.  I.e., the server can punt on reading any\n    of the request body, and let the client's TCP stack notify\n    the client application that something when wrong.  Since,\n    in the persistent-connection model, the server is *always*\n    free to close a connection with a request in progress, the\n    client is *always* allowed to retry the request (after opening\n    a new connection).\n    \n  However (and this is the tricky part):\n    If the client sees the connection close prematurely (i.e.,\n    before it sees a response status code), AND if the request it sent\n    has a body, AND if it knows (based on a previous response) that\n    the server is an HTTP/1.1 server, THEN it must wait for the \"100\n    Continue\" response after sending the request header.\n\n    This means that, after one failed attempt, the client switches\n    from using the one-phase approach to using the two-phase\n    approach.\n\nAs far as I can tell, this is a hop-by-hop mechanism, NOT an end-to-end\nmechanism.  The two-phase interaction is between the proxy and the\nserver, not the server and the proxy's own client.  The reason for this\nis that use of the two-phase approach is triggered by the \"client\"\n(which in this case is the proxy) seeing the TCP connection close\nbefore it receives a response status.  But just because the server\ncloses the server-to-proxy TCP connection does NOT mean that the proxy\nneeds to close the proxy-to-client connection.  From the client's point\nof view, there is a long delay, but nothing else is unusual.\n\nOf course, if the *proxy* wants its own client to use a two-phase\napproach, then it can trigger this by prematurely closing the\nproxy-to-client TCP connection.  But this is hop-by-hop, this\nis independent of the proxy-to-server behavior.\n\nAnd, because it is hop-by-hop, the \"server\" in question need\nnot be an origin server; it might be an intermediate proxy.\n\nAnd, if this isn't obvious: any HTTP/1.1 proxy, when communicating\nwith an HTTP/1.1 client, is a \"server\" from the point of view\nof this section, and so needs to send a \"100 Continue\" before\nit sends a response to a request with a body.  Since it might\nhave received the response from an HTTP/1.0 server, the proxy\nmight have to add the \"100 Continue\" on its own initiative;\nsimilarly, if it is forwarding a response from an HTTP/1.1\nserver to an HTTP/1.0 client, it needs to remove the \"100\nContinue\" response.  I.e., the \"100 Continue\" message is\nreally hop-by-hop, not end-to-end.\n\nSection 8.2 says:\n   Upon receiving a method subject to these requirements from an\n   HTTP/1.1 (or later) client, an HTTP/1.1 (or later) server MUST either\n   respond with 100 (Continue) status and continue to read from the\n   input stream, or respond with an error status. \nwhich implies that it MUST NOT simply send a \"200 OK\" response without\nsending a \"100 Continue\" response.  The reason for this is that,\nin the persistent-connection model, a server might close the TCP\nconnection without realizing that the client has already sent\na new request (because of network delays), and so the client might\nhave to retry a request for reasons other than the server wishing\nto switch to a two-phase approach.  But since section 8.2 requires\nthe client to wait, possibly forever, for a \"100 Continue\" before\nit sends the request body, and the server cannot send an actual\nresponse to the request before it receives the body, without the\nmandatory \"100 Continue\" we would have a deadlock.\n\nOn the other hand (to finally get to the question that started\nthis whole thread), I couldn't think of any harm that would\ncome from sending spurious \"100 Continue\" responses, as long\nas proxy implementors realize that these have to be treated\nas hop-by-hop messages.  Since the HTTP response stream over\na connection is fully synchronized with the request stream\n(i.e., the order is exactly the same in both directions), an\n\"extra\" 100 response can always be ignored by the recipient.\n\nThe key issue is that if the server sends just one \"100 Continue\"\nresponse for a request, and if it might sends it before the request\nactually arrives, then a client which reads ahead on a persistent\nconnection must \"remember\" that the most recent response was a\n\"100 Continue\", so that if its next request has a body, then it\nwon't wait forever for the next \"100\".  This seems like reasonably\nsound defensive coding, although it's only required if we allow\nservers to send the mandatory \"100\" before receiving the corresponding\nrequest.\n\nProbably we should either rewrite section 8.2 entirely, or we\nshould include some sort of note explaining the purpose of this\nsetup.\n\n-Jeff\n\nP.S.: Or we should eliminate \"100 Continue\" and the two-phase\nmechanism entirely.  We added it because Roy (and maybe a few\nother server implementors?) wanted to be able to reject long\nrequests without reading the whole message.  But it clearly\nhas led to a lot of complexity for all implementors (clients,\nservers, and proxies) and introduces some unavoidable overheads.\nMy own feeling is that it is probably not such a big deal for\na server to have to bit-bucket a large request body once in\na while.  Especially compared to the added complexity that\nthe two-phase model implies.\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "On Fri, 18 Apr 1997, Jeffrey Mogul wrote:\n\n> But this is probably just \"advice to client implementors\", not\n> something we need to put into the spec.  (And I've seldom seen\n> any content-generator realize when it has generated bogus data,\n> although I have certainly seen enough cases where the output was\n> clearly broken.)\n\nProbably because if the server/application cares enough to know the data\nmight be bogus, it has made provisions to purge the partial output and\nstart over. If you've been busy generating HTML and forwarding it to a\nclient and discover some kind of error condition, it is really tough in\nthe general case to generate additional HTML which one can be sure will\nbe seen by the user in a meaningful way. (For example, if one is in the\nmiddle of generating a table but the error routine doesn't close the\ntable, the browser may not display the error message, etc.)\n\nMy latest project takes the effort to queue all output until the response\nis complete. If an intermediate failure is detected. the queued output is\npurged and an error response generated.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "&quot;last call&quot;: draft-ietf-http-versions01.tx",
            "content": "At the HTTP working group meeting on April 7, there were no objections\nto moving\nforward with:\n\n   ftp://ietf.org/internet-drafts/draft-ietf-http-versions-01.txt\n\nas the best advice of the HTTP working group. Based on the wording\nin the abstract (\"not a modification of the intended meaning of the\nexisting ... documents\"), I propose that this be submitted to\nthe IESG to publish as an 'Informational' RFC as soon as possible.\n\nAs there has been no controversy on this issue in the working group,\nI don't believe a separate \"working group last call\" is necessary.\n(other issues will probably require one.)\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "> I believe the argument against a bit-bucket is that the server has to waste\n> resources to consume the incoming bits, and network bandwidth gets wasted at a\n> time when we're trying to reduce HTTP-induced network bandwidth.  It's hard to\n> know how much of a problem either of these *really* is.  Does anyone have\n> numbers for how often servers reject PUT/POST because they can't accept the\n> content?  My guess is it's not a big problem yet.  Can we afford to defer the\n> solution until it is? \n> \n> Dave Kristol\n\nThe most frequent case for our implementation at least will, I suspect, \nbe when the server has configured different realms for serving and \nsubmitting a form.  This turns out to be a common situation in an \nembedded system because it saves memory and simplifies interface design; \nthe same page is used to display current configuration information and to \nchange it, but the authentication required is different.\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "On Mon, 21 Apr 1997, Dave Kristol wrote:\n\n> Jeff Mogul wrote:\n> \n> > P.S.: Or we should eliminate \"100 Continue\" and the two-phase\n> > mechanism entirely.  We added it because Roy (and maybe a few\n> > other server implementors?) wanted to be able to reject long\n> > requests without reading the whole message.  But it clearly\n> > has led to a lot of complexity for all implementors (clients,\n> > servers, and proxies) and introduces some unavoidable overheads.\n> > My own feeling is that it is probably not such a big deal for\n> > a server to have to bit-bucket a large request body once in\n> > a while.  Especially compared to the added complexity that\n> > the two-phase model implies.\n> \n> I think the two-phase commit increases the odds that implementors of HTTP/1.1\n> clients and servers will \"get it wrong\", leading to poor interoperation.  The\n> reason is that time-dependent problems are much harder to test and find.\n> Jeff's suggestion of a bit-bucket is pretty easy to implement, and clients\n> (which don't require any special coding for this) and servers are both likely\n> to get it right.\n> \n> I believe the argument against a bit-bucket is that the server has to waste\n> resources to consume the incoming bits, and network bandwidth gets wasted at a\n> time when we're trying to reduce HTTP-induced network bandwidth.  It's hard to\n> know how much of a problem either of these *really* is.  Does anyone have\n> numbers for how often servers reject PUT/POST because they can't accept the\n> content?  My guess is it's not a big problem yet.  Can we afford to defer the\n> solution until it is? \n> \n> Dave Kristol\n\n\nI haven't been following the \"100 Continue\" discussion, but today I\nread all the relevant sections of the spec and I must say I am \nconfused.  Here are a few of my questions:\n\nAll questions apply to a transaction between a 1.1 origin server\nand a 1.1 client.\n\n1) Is it legal for the server to never send \"100 Continue\" and instead\njust close the connection for requests it does not wish to grant?\nSince the server can always close the connection whenever it wants,\nthe question is will the client hang indefinitely waiting for a \"100\ncontinue\" or error message?  An old version of the spec had a 5 second\npause, but now that is gone.\n\n\n2) Is it the intent that *every* client request with a body should\nsend headers and then wait for a \"100 coninue\" or error?  The spec\ndoes not explicitly say which transactions use this \"two-phase\"\nprocedure.  It only says, \n\n  \"Upon receiving a method subject to these requirements from an\n  HTTP/1.1 (or later) client, an HTTP/1.1 (or later) server MUST either\n  respond with 100 (Continue) status and continue to read from the input\n  stream, or respond with an error status.\"\n\nWhich transactions are \"subject to these requirements\"?  Is it any\nrequest with a body?  If so, the spec should say this.\n\n3)  Does a client only wait for a \"100 continue\" on a retry?  If so\ndoes the server always send the \"100 continue\" or just on retries?\nIf the latter how does the server know it is a retry?\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "John Franks <john@math.nwu.edu> writes:\n    I haven't been following the \"100 Continue\" discussion, but today I\n    read all the relevant sections of the spec and I must say I am \n    confused.\nYou might also want to read \nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0134.html\nwhich is my attempt to explain the motivation behind the design.\n    \n    Here are a few of my questions:\n    \n    All questions apply to a transaction between a 1.1 origin server\n    and a 1.1 client.\n    \n    1) Is it legal for the server to never send \"100 Continue\" and instead\n    just close the connection for requests it does not wish to grant?\n    Since the server can always close the connection whenever it wants,\n    the question is will the client hang indefinitely waiting for a \"100\n    continue\" or error message?  An old version of the spec had a 5 second\n    pause, but now that is gone.\n    \nIt's always legal for the server to close the connection at any\ntime (in the persistent-connection model).  The problem is that it\nis equally legal for the client, when it sees a premature close\nwithout any indication of error, to simply retry the connection\nand the request.\n\nThis can lead to an infinite loop, if the client really wants to\nmake the request, and the server really wants to reject it.  A\nhuman user would probably get bored and hit \"stop\" after a while,\nbut we have to consider the possibility of an automated client\nsucking down all of the server CPU cycles.\n\nWe got rid of the 5 second pause, as I recall, because it was clear\nthat an arbitrary (but fixed) timeout here would be wrong in at least\nsome cases.\n    \n    2) Is it the intent that *every* client request with a body should\n    send headers and then wait for a \"100 coninue\" or error?  The spec\n    does not explicitly say which transactions use this \"two-phase\"\n    procedure.  It only says, \n    \n      \"Upon receiving a method subject to these requirements from an\n      HTTP/1.1 (or later) client, an HTTP/1.1 (or later) server MUST either\n      respond with 100 (Continue) status and continue to read from the input\n      stream, or respond with an error status.\"\n    \n    Which transactions are \"subject to these requirements\"?  Is it any\n    request with a body?  If so, the spec should say this.\n\nI think it does say it, but badly.  Clearly this part needs a rewrite\n(if we don't get rid of it entirely).\n    \n    3)  Does a client only wait for a \"100 continue\" on a retry?  If so\n    does the server always send the \"100 continue\" or just on retries?\n    If the latter how does the server know it is a retry?\n    \nMy understanding is that the client initially (i.e., the first\ntime it sends a request-with-body to a given server) does not wait.\nBut once it has seen the server close the connection \"prematurely\"\nand it retries the request, it MUST wait, at least for that request.\n\nThe spec is silent on whether it is then mandatory for the client\nto use the two-phase approach for a subsequent request-with-body.\nI believe that this is not mandatory from a logical standpoint\n(i.e., the protocol won't deadlock) but it might be necessary\nfrom an efficiency standpoint (i.e., there could be a lot of\nretrying otherwise).  But, again, I think the whole two-phase\napproach is of dubious merit, given its complexity.\n\n-Jeff\n\nP.S.: One note on terminology: this is NOT a \"two-phase commit\"\n(a term that has a very specific meaning).  This is a \"two-phase\nmethod invocation\".  Maybe I should have used the term \"two-step\".\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "On Mon, 21 Apr 1997, Jeffrey Mogul wrote:\n\n> John Franks <john@math.nwu.edu> writes:\n>     I haven't been following the \"100 Continue\" discussion, but today I\n>     read all the relevant sections of the spec and I must say I am \n>     confused.\n> You might also want to read \n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0134.html\n> which is my attempt to explain the motivation behind the design.\n>     \n\nThanks Jeff.  This is indeed very helpful and explains much of what I\nwanted to know.  The specification needs to be at least as clear as\nthis.\n\nI now am of the opinion that the benefits of \"100 Continue\" are not\nworth the added complexity.  Or at least the burden of proof should be\nshifted to the proponents to explain why this is worth it.  Even\nthough the internet is crowded simplicity and robustness must take\nprecedence over efficiency.\n\nI am also troubled by the part of the specification which says,\n\n   \"If the client does retry the request to this HTTP/1.0 server, it\n   should use the following \"binary exponential backoff\" algorithm to be\n   assured of obtaining a reliable response:\"\n\nIs the \"should\" here different from SHOULD?  I don't think this level\nof implementaion detail exists elsewhere in the spec.  Is there a\nrationale for having it here (as opposed to putting it with other\nimplementation notes)?\n\n\n\n> My understanding is that the client initially (i.e., the first\n> time it sends a request-with-body to a given server) does not wait.\n> But once it has seen the server close the connection \"prematurely\"\n> and it retries the request, it MUST wait, at least for that request.\n> \n> The spec is silent on whether it is then mandatory for the client\n> to use the two-phase approach for a subsequent request-with-body.\n> I believe that this is not mandatory from a logical standpoint\n> (i.e., the protocol won't deadlock) but it might be necessary\n> from an efficiency standpoint (i.e., there could be a lot of\n> retrying otherwise).  But, again, I think the whole two-phase\n> approach is of dubious merit, given its complexity.\n> \n\nThis needs to be clarified and put in the specification unless we\ncan discard \"100 Continue\".\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "backoff (Re: 1xx Clarification",
            "content": "On Mon, 21 Apr 1997, John Franks wrote:\n\n> \n>    \"If the client does retry the request to this HTTP/1.0 server, it\n>    should use the following \"binary exponential backoff\" algorithm to be\n>    assured of obtaining a reliable response:\"\n> \n> Is the \"should\" here different from SHOULD?  I don't think this level\n> of implementation detail exists elsewhere in the spec.  Is there a\n> rationale for having it here (as opposed to putting it with other\n> implementation notes)?\n>\n\nI think this is a protocol issue and not an implementation issue. Think\nabout it this way: If a server is unable to respond to requests due to\nload, then it is not desirable to have all clients retry their requests at\n(approximately) the same time. The exponential backoff algorithm will tend\nto spread out client retries and reduce the average number of requests to\nthe server. Certainly, this approach has precedent in lower layer\nprotocols (from the MAC layer up). Okay, now if this issue were one of the\nserver managing its own load, then I would agree that it's an\nimplementation issue, but since this is a matter of how clients should\nhandle retries so as minimize server load (and, of course, the network\ncongestion that also results), it is a protocol issue.\n \n> \n> \n> John Franks Dept of Math. Northwestern University\n> john@math.nwu.edu\n> \n\n---\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": ">This needs to be clarified and put in the specification unless we\n>can discard \"100 Continue\".\n\nI am not interested in revisiting two years worth of discussions just\nbecause people have forgotten why it exists.  That is what the mailing\nlist archive is for.  We cannot discard \"100 Continue\" because the\nimplementations have already been deployed and it is known to be useful\nin situations where the client doesn't want (for whatever reason) to\nsend the data before it knows that the URL is correct, that it accepts\nthe method being used, and that it doesn't require some additional form\nof authentication.  It is trivial for the server to implement (see\nApache 1.2b*) and trivial for a client to implement (just ignore it) --\nthe only difficult implementation is one that makes use of the\nreliability feature, and for them the difficulty is worth it.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": ">P.S.: Or we should eliminate \"100 Continue\" and the two-phase\n>mechanism entirely.  We added it because Roy (and maybe a few\n>other server implementors?) wanted to be able to reject long\n>requests without reading the whole message.\n\nNo, we could do that already.  The reason for it is so that a client\nwhich knows that it is talking to an HTTP/1.1 server can at least\ndetermine that the action part of the request is OK before it sends\nall the data, since that is important for some applications.  After all,\nHTTP is used for more things than just GETs and filling-out HTML forms.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "Scott Lawrence <lawrence@agranat.com> writes:\n\n>RF> The 100 response must be sent by an HTTP/1.1 server upon receipt of\n>RF> any HTTP/1.1 request containing a message body, after it receives the\n>RF> header fields and determines that it wishes to receive that body.  The\n>RF> RFC 2068 says this in section 8.2, but in a rather confusing way.\n>\n>RF> The client should not care either, since it should be ignoring any\n>RF> 1xx class of response.  A client that is not looking for such a response\n>RF> will simply see it (and ignore it) upon its next request, or never see\n>RF> it at all if it just drops the connection.\n>\n>  Our server does send a 100 Continue under those conditions; the rule\n>  I would like to see stated more directly is that:\n>\n>    When sending a request with a body to a server it has reason to\n>    believe implements HTTP/1.1, a client SHOULD send the headers and\n>    then wait for a 100 Continue or an error status before\n>    transmitting the body of the request.\n>\n>  the current spec essentially says this only for the case where the\n>  client is retrying a request, not for the first attempt.\n\nI believe that this is dependent on the nature of the request, and thus\nshould be determined by the user agent and not by the protocol.  This is\nwhy the 100 response is required from an HTTP/1.1 server, instead of\nbeing optional, so that a user agent can be selective in deciding whether\nto wait for the round trip.\n\nBecause of interactions with HTTP/1.0 and proxies, I do not expect this\nto be useful except for specialized applications, or until all HTTP/1.0\nare outside the operating domain.  Nevertheless, 100 was added because\nit was something that some HTTP applications needed to express, but\ncouldn't be expressed in HTTP/1.0.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: midcourse error",
            "content": "In message <9704141331.AA04939@zp>, Dave Kristol writes:\n>Here's something that has puzzled me about HTTP server\n>implementations.  What happens if the server encounters an error after\n>it sends, say, a \"200 OK\" response and part of the entity body?\n>Example cases include an origin server that gets a timeout from a CGI\n>part way through relaying the CGI's output, or when a proxy acts as a\n>pure relay and, for example, its connection to the next hop server (is\n>that upstream or downstream? :-) breaks.  The server has no way to\n>signal to its client that the previously claimed success has now turned\n>into an error.\n\nThe only useful thing a server could say at that point is the equivalent\nof 500 Internal Server Error, which is no better than just cutting-off\ntransmission of the message and closing the connection.  If the message\nis properly delimited, the client should be capable of seeing that\nsomething is missing and act appropriately.  A multiplexing HTTP should\nbe capable of terminating a \"thread\" early without harming the rest of\nthe connection, so I think trying to get HTTP/1.x to handle it would\nbe a mistake.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Issues-list item &quot;LWSDELIMITER&quot",
            "content": "rom the Memphis minutes:\n\n-- After consulting the language in use in RFC822, Koen Holtman will\n   also draft an editorial correction on the use of LWS in headers\n   like VIA.\n\nHere is the editorial correction, taken from a slide I showed in\nMemphis.  New/changed text is marked with ** **.  This is a correction\nto section 2.1 of the 1.1 spec.\n\n implied *LWS\n\n     The grammar described by this specification is word-based. Except\n     where noted otherwise, linear whitespace (LWS) can be included\n     between any two adjacent words (token or quoted-string), and\n     between adjacent tokens and **tspecials**, without changing the\n     interpretation of a field. At least one delimiter (**LWS and/or**\n     tspecials) must exist between any two tokens, since they would\n     otherwise be interpreted as a single token.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Issueslist item &quot;QZERO&quot",
            "content": "rom the Memphis minutes:\n\n -- Koen Holtman\n    will draft a clarification that a qvalue of 0.0 means \"Don't send\n    me this.\"\n\nThe clarification (taken from the slide I showed in the Memphis\nevening session) is one new sentence in section 3.9 of the 1.1 spec.\nThe new sentence is between ** **.\n\n 3.9 Quality Values\n\n   HTTP content negotiation (section 12) uses short \"floating point\"\n   numbers to indicate the relative importance (\"weight\") of various\n   negotiable parameters. A weight is normalized to a real number in the\n   range 0 through 1, where 0 is the minimum and 1 the maximum value.\n   **If a parameter has a quality value of 0, then content with this\n   parameter is `not acceptable' for the client.**\n   HTTP/1.1 applications MUST NOT generate more than three digits after\n   the decimal point. User configuration of these values SHOULD also be\n   limited in this fashion.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Issueslist item &quot;DISPOSITION&quot",
            "content": "Issues-list item \"DISPOSITION\"\n\nrom the Memphis minutes:\n\n-- Content-Disposition will be added to the Appendix, as one\n   of a group of common MIME headers about which implementors should\n   be aware; Koen Holtman will draft the addition.\n\nHere is a draft of the addition.  This text is based on 1806 and on\nmessages in the mailing list archive.  I have not tested this myself.\n\n\n19.6.2.x Content-Disposition\n\n   The Content-Disposition response-header field has been proposed as\n   a means for the origin server to suggest a default filename if the\n   user requests that the content is saved to a file.  This usage is\n   derived from the definition of Content-Disposition in RFC 1806.\n\n        content-disposition = \"Content-Disposition\" \":\"\n                              disposition-type *(\";\" disposition-parm)\n\n        disposition-type = \"attachment\" | disp-extension-token\n\n        disposition-parm = filename-parm | disp-extension-parm\n\n        filename-parm = \"filename\" \"=\" quoted-string\n\n        disp-extension-token = token\n\n        disp-extension-parm = token \"=\" ( token | quoted-string )\n\n   An example is\n\n        Content-Disposition: attachment; filename=\"fname.ext\"\n\n   The receiving user agent should not respect any directory path\n   information that may seem to be present in the filename parameter.\n   The filename should be treated as a terminal component only.\n\n   If this header is used in a response with the\n   application/octet-stream content-type, the implied suggestion is\n   that the user agent should not display the response, but directly\n   enter a `save response as..'  dialog.\n\n\n1806 lists some security considerations:\n\n   Since this memo provides a way for the sender to suggest a filename,\n   a receiving MUA must take care that the sender's suggested filename\n   does not represent a hazard. Using UNIX as an example, some hazards\n   would be:\n\n          + Creating startup files (e.g., \".login\").\n\n          + Creating or overwriting system files (e.g.,\n            \"/etc/passwd\").\n\n          + Overwriting any existing file.\n\n          + Placing executable files into any command search path\n            (e.g., \"~/bin/more\").\n\n          + Sending the file to a pipe (e.g., \"| sh\").\n\n   In general, the receiving MUA should never name or place the file\n   such that it will get interpreted or executed without the user\n   explicitly initiating the action.\n\n   It is very important to note that this is not an exhaustive list; it\n   is intended as a small set of examples only.  Implementors must be\n   alert to the potential hazards on their target systems.\n\nI'm not sure what we should do about these.  Should we repeat them in\nthe security considerations section of 1.1, or is that not necessary?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "Hi,\n\nin a relatively old message Henrik said:\n\n> In the performance paper that Jim sent a reference to a couple of days ago\n> \n> http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Pipeline.html\n> \n> we state that pipelining is an essential part in making HTTP/1.1 outperform\n> HTTP/1.0 speedwise. What the paper does not state directly is the impact\n> pipelining has on proxies.\n...\n> Compression will also have a positive impact as it allows proxies to\n> maintain the same compressed representation of the object in their\n> persistent cache hence giving room for more objects on disk and in memory.\n\nI have done a quick test on the content of our proxy cache: for each\ndirectory, I have compared the output of\n\ncat * | wc\nand\ncat * | gzip | wc\n\nwhich is not a very rigorous test (since files in the cache contain the\nHTTP header as well, and merging files before compression changes\nthe results a little bit) but gives the idea.\n\nThe total byte count is as follows:\n\nUncompressed:316.407.346\nCompressed:274.892.797\n\nwith a saving, due to compression, of approximately 13% . I suspect the\nactual use of compression would result in lower performance since\nmost files are short and headers compress a lot, thus biasing my result\ntoward better performance. These results can be explained with the fact\nthat large matherial is generally in compressed form at the source\nhence the additional compression is ineffective.\n\nNow gaining 10% on the cache size is not that much, considering the\nextra work involved, and besides it looks more like a FS issue than a\ncache issue.\n\nJust a data point...\n\nCheers\nLuigi\n-----------------------------+--------------------------------------\nLuigi Rizzo                  |  Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it    |  Universita' di Pisa\ntel: +39-50-568533           |  via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522           |  http://www.iet.unipi.it/~luigi/\n_____________________________|______________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "> I have done a quick test on the content of our proxy cache: for each\n> directory, I have compared the output of\n> \n> cat * | wc\n> and\n> cat * | gzip | wc\n> \n> which is not a very rigorous test (since files in the cache contain the\n> HTTP header as well, and merging files before compression changes\n> the results a little bit) but gives the idea.\n> \n> The total byte count is as follows:\n> \n> Uncompressed:316.407.346\n> Compressed:274.892.797\n> \n> with a saving, due to compression, of approximately 13% . I suspect the\n> actual use of compression would result in lower performance since\n> most files are short and headers compress a lot, thus biasing my result\n> toward better performance. These results can be explained with the fact\n> that large matherial is generally in compressed form at the source\n> hence the additional compression is ineffective.\n\n\nAnother way to look at this is that not only is \"large\" textual data, such as \npostscript, often compressed, but images are inherently compressed.  Can you \ntell us what fraction of files in your cache are content-type image/* (and the \nlike) as opposed to text?  \n\nIn any case, I agree with your conclusion, in the sense that no matter what \nthe cause of the poor compression is, the end result is that compression will \nonly do so much.\n\nAn aside: does anyone know what the difference in compression will be between\ncat * | gzip\nand\nfor i in *; gzip $i   ?\n\nMy guess is that by glomming everything together you are getting better \ncompression than you would in practice, when each file is compressed \ndistinctly, due to the adaptive algorithms -- here you may use data from file \nX to do a better job compressing Y.\n\n\n-- \n\nFred Douglis     MIME accepted  douglis@research.att.com\nAT&T Labs - Research     908 582-3633 (office)\n600 Mountain Ave., Rm. 2B-105         908 582-3063 (fax)\nMurray Hill, NJ 07974                http://www.research.att.com/~douglis/\n\nAs of 6/1/97:\nAT&T Labs - Research\n180 Park Ave, Room A181\nFlorham Park, NJ 07932-0971\n973-360-8775 (office)\n973-360-8871 (fax)\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "> > I have done a quick test on the content of our proxy cache: for each\n...\n> > which is not a very rigorous test (since files in the cache contain the\n...\n> > with a saving, due to compression, of approximately 13% . I suspect the\n> > actual use of compression would result in lower performance since\n> > most files are short and headers compress a lot, thus biasing my result\n> > toward better performance. These results can be explained with the fact\n> > that large matherial is generally in compressed form at the source\n...\n> Another way to look at this is that not only is \"large\" textual data, such as \n> postscript, often compressed, but images are inherently compressed.  Can you \n\nof course. Potentially large matherial is most of the times\ncompressed (because of native format, or because the provider is trying\nto save bandwidth).\n\n> tell us what fraction of files in your cache are content-type image/*\n> (and the like) as opposed to text?  \n\ncouting them now, they are about 73% over 17000 files (2/3 of the cache,\nwhich is rather small)\n\n> An aside: does anyone know what the difference in compression will be between\n> cat * | gzip\n> and\n> for i in *; gzip $i   ?\n> \n> My guess is that by glomming everything together you are getting better \n> compression than you would in practice, when each file is compressed \n> distinctly, due to the adaptive algorithms -- here you may use data from file \n> X to do a better job compressing Y.\n\ngenerally speaking, this is correct.  In this specific case, however,\nI suspect that the advantages are only achieved on the http headers\n(which are stored with the body), since a large amount of data does\nnot really compress. And for compressing headers there are probably\nmore efficient ways (using tokens for the keywords, binary\nrepresentation of dates times and numbers, etc.\n\nCheers\nLuigi\n-----------------------------+--------------------------------------\nLuigi Rizzo                  |  Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it    |  Universita' di Pisa\ntel: +39-50-568533           |  via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522           |  http://www.iet.unipi.it/~luigi/\n_____________________________|______________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": "On Apr 22,  4:04am, Koen Holtman wrote:\n> Subject: Issues-list item \"DISPOSITION\"\n>\n> Issues-list item \"DISPOSITION\"\n>\n> From the Memphis minutes:\n>\n> -- Content-Disposition will be added to the Appendix, as one\n>    of a group of common MIME headers about which implementors should\n>    be aware; Koen Holtman will draft the addition.\n>\n> Here is a draft of the addition.  This text is based on 1806 and on\n> messages in the mailing list archive.  I have not tested this myself.\n>\n>\n> 19.6.2.x Content-Disposition\n>\n>    The Content-Disposition response-header field has been proposed as\n>    a means for the origin server to suggest a default filename if the\n>    user requests that the content is saved to a file.  This usage is\n>    derived from the definition of Content-Disposition in RFC 1806.\n>\n>         content-disposition = \"Content-Disposition\" \":\"\n>                               disposition-type *(\";\" disposition-parm)\n>\n>         disposition-type = \"attachment\" | disp-extension-token\n>\n>         disposition-parm = filename-parm | disp-extension-parm\n>\n>         filename-parm = \"filename\" \"=\" quoted-string\n>\n>         disp-extension-token = token\n>\n>         disp-extension-parm = token \"=\" ( token | quoted-string )\n>\n>    An example is\n>\n>         Content-Disposition: attachment; filename=\"fname.ext\"\n>\n>    The receiving user agent should not respect any directory path\n>    information that may seem to be present in the filename parameter.\n>    The filename should be treated as a terminal component only.\n>\n>    If this header is used in a response with the\n>    application/octet-stream content-type, the implied suggestion is\n>    that the user agent should not display the response, but directly\n>    enter a `save response as..'  dialog.\n>\n>\nWe still have a discrepancy on the delivered filename.  In general\nthe MUA tries to name the current stream based on the URI.  This leads to\nunknown file types on the client system.  Based on messages in the archives,\nmaintainers are concerned that this will be compounded by groupware users and\nothers who want to share common files but the files are generated or retrieved\nby an engine and not a straight URI.\n\n>\n[snip the security section]\n>\n> I'm not sure what we should do about these.  Should we repeat them in\n> the security considerations section of 1.1, or is that not necessary?\n>\nIMHO, we should put these concerns in 1.1.  Since the MUA is acting our behalf,\nshouldn't it obey a variant of the first law of robotics?\n\n> Koen.\n>\n>-- End of excerpt from Koen Holtman\n\n\nKevin\n--\n=============================================================================\nKevin J. DyerDraper Laboratory  MS 23.\nEmail: <kdyer@draper.com>        555 Tech. Sq.\nPhone: 617-258-4962Cambridge, MA 02139\nFAX: 617-258-2121\n-----------------------------------------------------------------------------\n   \"Beware Geeks bearing GIFs\"   Author Unknown\n=============================================================================\n\n\n\n"
        },
        {
            "subject": "WARNINGS issue (was Re: draft minutes, HTTPWG meetings April 7",
            "content": "The draft minutes say:\n    -- Jeff Mogul will respond to syntactic changes proposed by Koen\n       Holtman for the HTTP-warning draft; if appropriate, a new version\n       of the draft will appear.\n\nKoen and I discussed this briefly at WWW6, and then he was offline\nfor a week.  We've now agreed that no change is necessary, since the\noriginal proposal in draft-ietf-http-warning-00.txt actually already\naddressed his concern.\n\nThis means that as far as we're concerned, the proposal in\ndraft-ietf-http-warning-00.txt is ready for \"last call\" and,\nif nobody objects, to be applied as edits to the text in RFC2068.\n\n-Jeff\n\nP.S.: draft-ietf-http-warning-00.txt contains a Note (on page 10)\nto the effect that we could save some bytes by defining a modified\nversion of the HTTP-date format ... but Koen and I agree that this is\nprobably not worth doing for HTTP/1.1.  This explanatory Note is not\npart of the actual proposed edits, and so should not delay any WG action.\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "Luigi Rizzo:\n[...]\n>I have done a quick test on the content of our proxy cache: for each\n>directory, I have compared the output of\n[...]\n>with a saving, due to compression, of approximately 13% .\n\nThe 13% may be too pessimistic.\n\nAnother data point: about 2 years ago, I measured the amount of text/*\ndata in the HTTP traffic between our campus proxy cache and outside\nservers, this turned out to be 30%.  As text/* data generally\ncompresses with a factor of 75%, compression would lead to savings of\n30%*0.75 = 23% for total off-campus traffic.\n\nAlso, compressing HTML pages may increase the perceived speed by more\nthan this percentage because it would allow browsers to render the\nlayout and start loading inlines (or subframes) earlier.\n\nBy the way, we really need more datapoints on this, and on other\nproposed optimisations.\n\n>Cheers\n>Luigi\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "> Luigi Rizzo:\n> [...]\n> >I have done a quick test on the content of our proxy cache: for each\n> >directory, I have compared the output of\n> [...]\n> >with a saving, due to compression, of approximately 13% .\n> \n> The 13% may be too pessimistic.\n> \n> Another data point: about 2 years ago, I measured the amount of text/*\n> data in the HTTP traffic between our campus proxy cache and outside\n> servers, this turned out to be 30%.  As text/* data generally\n> compresses with a factor of 75%, compression would lead to savings of\n> 30%*0.75 = 23% for total off-campus traffic.\n\nunfortunately WEB statistics are a moving target. Web pages nowadays\nare much different than they were two years ago. We have moved from\n1-2 inline graphics to the dozen that you can find on many such\npages.\n\n> Also, compressing HTML pages may increase the perceived speed by more\n> than this percentage because it would allow browsers to render the\n> layout and start loading inlines (or subframes) earlier.\n\nwell, consider that modems also compress data, so the savings are not\nas large as one could think. And, going along these lines, one would\nthen move to a binary HTTP and binary tags for HTML...\n\npersonally my feeling is that compression is an orthogonal problem,\nwhich is already addressed at many layers (network; file system;\nsource encoding).  A content provider has all the interest to\nprovide compressed data.  If he chose not so, probably he'll pay\nthe consequence in the long term (because of user dissatisfaction),\nand it is not the proxy code which has to take care of this.\n\n> By the way, we really need more datapoints on this, and on other\n> proposed optimisations.\n\nAgreed.\n\nCheers\nLuigi\n-----------------------------+--------------------------------------\nLuigi Rizzo                  |  Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it    |  Universita' di Pisa\ntel: +39-50-568533           |  via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522           |  http://www.iet.unipi.it/~luigi/\n_____________________________|______________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "On Tue, 22 Apr 1997, Koen Holtman wrote:\n\n> Luigi Rizzo:\n> [...]\n> >I have done a quick test on the content of our proxy cache: for each\n> >directory, I have compared the output of\n> [...]\n> >with a saving, due to compression, of approximately 13% .\n> \n> The 13% may be too pessimistic.\n> \n> Another data point: about 2 years ago, I measured the amount of text/*\n> data in the HTTP traffic between our campus proxy cache and outside\n> servers, this turned out to be 30%.  As text/* data generally\n> compresses with a factor of 75%, compression would lead to savings of\n> 30%*0.75 = 23% for total off-campus traffic.\n\n75% is only for typical for large chunks of text/*. For small chunks (such\nas most web pages) compression is closer to 60%. I just compressed our\nhome page as a test: uncompressed: 5208 bytes. compressed: 2216 bytes for\na compression of 57.5%. 30% * 57.5% = 17%. And your 30% figure for the\nfraction of text/* is probably too high - the net has become much more\ngraphical in the last two years.\n\nMy figures on www.xmission.com (a large server with many different\ncommercial and non-commercial residents) from a sample of 27 gigabytes of\nrecent measured traffic indicates that only about 13% of the traffic is\ntext/*.  This slashes the potential savings to a mere 13% x 57.5% = 7.5%\nfrom compressing the text/* files. And this overlooks the fact that the\nmajority of people browsing are doing so over modem links that *already*\nperform pretty good on the fly compression of the data flowing through\nthem - thus reducing the potential savings to the end user from\npre-compressing text/* to negligible.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Origin Servers without Clock",
            "content": "  I've just submitted the following to the I-D repository as:\n\n    draft-lawrence-http-noclock-00.txt\n\n  At the Memphis meeting, I briefly discussed the fact that many\n  systems where our 1.1 server implementation will be used do not (and\n  will not) have clocks, so sending the Date header is problematic.\n\n  This draft attempts to address this issue.\n\n  Much thanks to Jeff and Richard for helping put this together; any\n  obvious mistakes are probably mine in preparing the final edits.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\nInternet Draft                                           Scott Lawrence\ndraft-lawrence-http-noclock-00.txt                Agranat Systems, Inc.\nExpires: October 1997                                     Jeffrey Mogul\n                                                Digital Equipment Corp.\n                                                           Richard Gray\n                                  International Business Machines Corp.\n                                                         April 22, 1997\n\n\n               HTTP/1.1 Operation without a Clock\n\nStatus of this Memo\n\n     This document is an Internet-Draft.  Internet-Drafts are working\n     documents of the Internet Engineering Task Force (IETF), its\n     areas, and its working groups.  Note that other groups may also\n     distribute working documents as Internet-Drafts.\n\n     Internet-Drafts are draft documents valid for a maximum of six\n     months and may be updated, replaced, or obsoleted by other\n     documents at any time.  It is inappropriate to use Internet-\n     Drafts as reference material or to cite them other than as\n     ``work in progress.''\n\n     To learn the current status of any Internet-Draft, please check\n     the ``1id-abstracts.txt'' listing contained in the Internet-\n     Drafts Shadow Directories on ftp.is.co.za (Africa),\n     nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n     ds.internic.net (US East Coast), or ftp.isi.edu (US West Coast).\n\n1. Abstract\n\n   This memo describes a problem with the current Proposed Standard\n   for HTTP/1.1 found as a result of implementation experience.  A web\n   server may be implemented in an embedded system as a network user\n   interface.  Often the embedded system is one which has no other use\n   for a real-time clock, and/or the web interface is being added to\n   an existing device which has no clock.  Without a clock, no\n   accurate HTTP Date header can be generated.\n\n   This memo examines the implications of this situation for the\n   operation of HTTP/1.1 origin servers, proxies, and clients, and\n   proposes changes to the HTTP/1.1 specification to permit compliant\n   operation in such systems.\n\ndraft-lawrence-http-noclock-00.txt                             Page 2/5\n\n2. Background\n\n   Web browsers provide a powerful set of user interface primitives,\n   which are rapidly being applied to a wide range of applications;\n   the browser has become a de-facto network user interface standard.\n   One area of such application is the embedded system: a computer\n   system built into a device that serves some purpose other than just\n   being a computer.  Including a clock in an embedded system design\n   both adds cost and requires that the clock be accurately set,\n   adding system complexity.  For many embedded systems, a clock is\n   not otherwise required, and many existing embedded systems that are\n   otherwise capable of providing a web interface do not have a clock.\n\n   The HTTP/1.1 Proposed Standard requires that an origin server\n   always include a Date header ([RFC 2068], section 14.19).  This\n   requirement was strengthened from a SHOULD in the HTTP/1.0\n   specification to a MUST in the HTTP/1.1 specification, apparently\n   in order to support the correct operation of caching both in\n   proxies and clients.\n\n   The HTTP/1.1 Proposed Standard specifies a number of headers for\n   mechanisms to affect the operation of caches, including:\n\n       - Date\n       - Last-Modified\n       - Expires\n       - Cache-Control\n       - Etag\n\n   it also documents usage of the 'Pragma: no-cache' header for\n   backward compatible cache control with some pre-HTTP/1.1\n   implementations.\n\n   An important characteristic of an embedded web server\n   implementation is that the content of such a server is well defined\n   at the time the system is built, and each potential response is\n   either:\n\n       - A Static response, which changes only when the firmware of\n         the system is changed.\n         Examples include: pages of help information, cosmetic\n         elements, and external links to the manufacturers web site.\n\n       - A Dynamic response, which may change on any access.\n         Examples include: pages which include information on the\n         current state of the device.\n\n   It is desirable that Static responses be cachable, and that Dynamic\n   responses never be cached.  The authors' contention here is that\n   this can be achieved by correct usage of the other headers already\n   specified by HTTP/1.1, without the requirement that the Date header\n   always be sent by origin servers.\n\ndraft-lawrence-http-noclock-00.txt                             Page 3/5\n\n3. Proposed Change for HTTP/1.1 Requirements\n\n   Section 14.19 of [RFC 2068] be replaced with (delimited by the '='\n   lines):\n\n   ================\n14.19 Date\n\n   The Date general-header field represents the date and time at which\n   the message was originated, having the same semantics as orig-date in\n   RFC 822. The field value is an HTTP-date, as described in section\n   3.3.1.\n\n          Date  = \"Date\" \":\" HTTP-date\n\n   An example is\n\n          Date: Tue, 15 Nov 1994 08:12:31 GMT\n\n   Origin servers MUST include a Date header field in all responses,\n   except in these cases:\n1. If the response status code is 100 (Continue) or\n   101 (Switching Protocols), the response SHOULD NOT\n   include a Date header field.\n2. If the response status code conveys a server error,\n   e.g. 500 (Internal Server Error) or 503 (Service Unavailable),\n   and it is inconvenient or impossible to generate a valid\n   Date.\n3. If the server does not have a clock that can provide a\n   reasonable approximation of the current time, its responses\n   MUST NOT include a Date header field.  In this case, the\n   rules in section 14.19.1 MUST be followed.\n\n   A received message that does not have a Date header field MUST be\n   assigned one by the recipient if the message will be cached by that\n   recipient or gatewayed via a protocol which requires a Date.   An\n   HTTP implementation without a clock MUST NOT cache responses without\n   revalidating them on every use.  An HTTP cache, especially a shared\n   cache, SHOULD use a mechanism, such as NTP[28], to synchronize its\n   clock with a reliable external standard.\n\n   Clients SHOULD only send a Date header field in messages that\n   include an entity-body, as in the case of the PUT and POST\n   requests, and even then it is optional.  A client without a\n   clock MUST NOT send a Date header field in a request.\n   ================\n\ndraft-lawrence-http-noclock-00.txt                             Page 4/5\n\n   The following subsection should be added:\n\n   ================\n14.19.1 Clockless Origin Server Operation\n\n   Some origin server implementations may not have a clock available.\n   An origin server without a clock MUST NOT assign Expires or\n   Last-Modified values to a response, unless these values were\n   associated with the resource by a system or user with a reliable\n   clock.  It MAY assign an Expires value that is known, at or before\n   server configuration time, to be in the past (this allows\n   \"pre-expiration\" of responses without storing separate Expires\n   values for each resource).\n   ================\n\n   Section 10.3.5 (\"304 Not Modified\"), after:\n\n     The response MUST include the following header fields:\n\n   Replace\n\n     o  Date\n\n   with\n\n     o  Date, unless its omission is required by section 14.19.1\n\n   If a clockless origin server obeys these rules, and proxies and\n   clients add their own Date to any response received without one (as\n   already specified by [RFC 2068], section 14.19), caches will\n   operate correctly.\n\ndraft-lawrence-http-noclock-00.txt                             Page 5/5\n\n4. Security Considerations\n\n   The Date header is not an important part of any security mechanism;\n   it is a component of the entity digest specified by [RFC 2069], but\n   that document already specifies the behavior for all parties when no\n   Date header is supplied.\n\n   The author believes that the proposed changes have no security\n   implications.\n\n5. Author's Addresses\n\n   Scott Lawrence\n      Agranat Systems, Inc.\n      1345 Main St.\n      Waltham, MA 02154\n   Phone:  +1-617-893-7868\n   Fax:    +1-617-893-5740\n   Email:  lawrence@agranat.com\n\n   Jeffrey Mogul\n      Western Research Laboratory\n      Digital Equipment Corporation\n      250 University Avenue\n      Palo Alto, California, 94305, USA\n   Email:  mogul@wrl.dec.com\n\n   Richard Gray\n      International Business Machines Corp.\n      4205 S. Miami Blvd.\n      RTP, NC 27709\n   Email:  rlgray@raleigh.ibm.com\n\n6. References\n\n   [RFC 2068]\n       R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee.\n       \"Hypertext Transfer Protocol -- HTTP/1.1.\"\n       RFC 2068,\n       U.C. Irvine, DEC, MIT/LCS,\n       January 1997.\n\n   [RFC 2069]\n       J. Franks, P. Hallam-Baker, J. Hostetler, P. Leach,\n       A. Luotonen, E. Sink, and L. Stewart.\n       \"An Extension to HTTP : Digest Access Authentication\"\n       RFC 2069,\n       Northwestern University, CERN, Spyglass Inc., Microsoft Corp.,\n       Netscape Communications Corp., Spyglass Inc., Open Market Inc.,\n       January 1997.\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": ">Issues-list item \"DISPOSITION\"\n>\nFrom the Memphis minutes:\n>\n>-- Content-Disposition will be added to the Appendix, as one\n>   of a group of common MIME headers about which implementors should\n>   be aware; Koen Holtman will draft the addition.\n>\n>Here is a draft of the addition.  This text is based on 1806 and on\n>messages in the mailing list archive.  I have not tested this myself.\n\nIs there some reason why\n\n    Entity-Header = ...\n                  | Content-Disposition    ; defined by RFC 1806 [nn]\n\nis not sufficient?\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;QZERO&quot",
            "content": "From the Memphis minutes:\n>\n> -- Koen Holtman\n>    will draft a clarification that a qvalue of 0.0 means \"Don't send\n>    me this.\"\n>\n>The clarification (taken from the slide I showed in the Memphis\n>evening session) is one new sentence in section 3.9 of the 1.1 spec.\n>The new sentence is between ** **.\n>\n> 3.9 Quality Values\n>\n>   HTTP content negotiation (section 12) uses short \"floating point\"\n>   numbers to indicate the relative importance (\"weight\") of various\n>   negotiable parameters. A weight is normalized to a real number in the\n>   range 0 through 1, where 0 is the minimum and 1 the maximum value.\n>   **If a parameter has a quality value of 0, then content with this\n>   parameter is `not acceptable' for the client.**\n>   HTTP/1.1 applications MUST NOT generate more than three digits after\n>   the decimal point. User configuration of these values SHOULD also be\n>   limited in this fashion.\n\nThat change does not belong in that section -- it belongs in the sections\non the Accept* header fields.  Servers send qvalues too.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "To:http-wg@cuckoo.hpl.hp.com\n\n\n\nOn Tue, 22 Apr 1997, Benjamin Franz wrote:\n\n> majority of people browsing are doing so over modem links that *already*\n> perform pretty good on the fly compression of the data flowing through\n> them - thus reducing the potential savings to the end user from\n> pre-compressing text/* to negligible.\n\nBut the user's modem link is only one of many hops between the user and\nthe server so I don't agree that the fact of modem compression on one hop\nnegates the importance of compression of the original material. \n\nAnother point to consider is that typical content travels over several\nhops before arriving at the client. Depending on content length, \nhop-link speeds, hop-queues, etc. a 7.5% reduction in effective length\ncan have a much greater effect on the transit time from the server to\nthe client.\n\nAnd as has already been pointed out, the content most impacted by end-end\ncompression is likely to be the HTML controlling the remainder of the\npresentation. My sense it that the next transition in web traffic is\nlikely to be to much larger and more complex 'html' files as providers add\ncomplex J(ava)Script et al to the pages. This would counter balance the\nmost recent shift we've seen toward more graphical content.\n\nrom my perspective, there is a very real payback here for very minimal\nimplementation effort.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "At 12:16 PM 4/22/97 -0700, Benjamin Franz wrote:\n\n>My figures on www.xmission.com (a large server with many different\n>commercial and non-commercial residents) from a sample of 27 gigabytes of\n>recent measured traffic indicates that only about 13% of the traffic is\n>text/*.  This slashes the potential savings to a mere 13% x 57.5% = 7.5%\n>from compressing the text/* files. And this overlooks the fact that the\n>majority of people browsing are doing so over modem links that *already*\n>perform pretty good on the fly compression of the data flowing through\n>them - thus reducing the potential savings to the end user from\n>pre-compressing text/* to negligible.\n\nFigures showing (potentially lack of) savings using compression compared to\nall other data formats are all very good, but is in fact not what our data\nresults are all about.\n\n1) In typical browsing mode, the very first packet on a connection contains\nan HTML page - the images are not requested until the HTML has arrived and\nstarted being parsed. TCPs behavior over time is a non-linear function\nwhere the first packet is much more expensive than the last. Therefore, it\nis likely to be a win to concentrate our efforts on the first packet. This\nis exactly what compressing HTML gives us.\n\n2) Modem compression has on several occasions been indicated to have\n\"pretty good\" performance. Our data show otherwise - but not explicitly. I\njust made some simple tests of modem compression with and without deflated\ndata and the figures are compelling - gaining about 2/3 in both time and\npackets when using deflate. Look at\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/PPP.html\n\nfor details. Compression also helps on a LAN - see the figures at\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/LAN.html\n\n3) On the more speculative side, I don't consider the current composition\nof data formats in caches being constant. The paper describes the potential\nbenefits of using style sheets and other data formats than the more\ntraditional gif and jpeg. Style sheets are just starting to be deployed and\nit may change the contents significantly over the next 6 months. CSS1 style\nstyle sheets compress just as well as HTML, so there is yet another point\ncounting for compression.\n\nSo, the _actual_data_ that we have now for the effect of compression seems\nto indicate with little doubt that it is worth doing!\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: 1xx Clarificatio",
            "content": "SDL> The most frequent case for our implementation at least will, I suspect,\nSDL> be when the server has configured different realms for serving and\nSDL> submitting a form.  This turns out to be a common situation in an\nSDL> embedded system because it saves memory and simplifies interface design;\nSDL> the same page is used to display current configuration information and to\nSDL> change it, but the authentication required is different.\n\nDK> Did you answer some other question?  It sounds like you're talking about\nDK> Basic Authentication, not PUT/POST and 1xx responses.\n\n  Sorry - brevity can be overdone.\n\n  I was addressing the normal circumstances under which it would be\n  good for a client to have waited before sending the message body:\n\n        Client                               Server\n          |                                    |\n       >1 |-> GET /form.html HTTP/1.1 -------->|\n          |                                    |\n          |<------------- HTTP/1.1 200 Ok <----|\n          |                                    |\n          |                                    |\n       >2 |-> POST /form.html HTTP/1.1 ------->|\n          |        (no authorization)          |\n          |                                    |\n          |<---- HTTP/1.1 401 Unauthorized <---|\n          |                                    |\n          |                                    |\n       >3 |-> POST /form.html HTTP/1.1 ------->|\n          |        (with authorization)        |\n          |                                    |\n          |<------- HTTP/1.1 100 Continue <----|\n          |                                    |\n          |-> (form data) -------------------->|\n          |                                    |\n          |<------------- HTTP/1.1 200 Ok <----|\n          |                                    |\n\n  Request 1 is sent for a resource which contains a form, but which is\n  not protected by any realm.  The resource is returned, and the\n  client has the opportunity to note that the server is 1.1.\n\n  Request 2 is sent to post the form, but submission of the form is\n  protected by some realm, so this request is rejected.  The server\n  can determine this before seeing the request body.\n\n  Request 3 is the retry of 2 with authorization information; after\n  the headers are received, the server returns the 100 Continue to\n  indicate to the client that it is ok to proceed with the request\n  body.\n\n  If the client had sent the form data immediately with Request 2, it\n  would have been just discarded by the server - a waste of time and\n  bandwidth.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Origin Servers without Clock",
            "content": "http-wg@cuckoo.hpl.hp.com writes:\n\n>  At the Memphis meeting, I briefly discussed the fact that many\n>  systems where our 1.1 server implementation will be used do not (and\n>  will not) have clocks, so sending the Date header is problematic.\n\nWhile I understand and sympathize with the issue here (I've already\ngot too many clocks in my home, I don't need more to reset twice a\nyear!), I find it suprising that TCP can be implemented on a system that\nhas no timing facilities.  For that matter, don't many (most?) embedded\nsystems have a real-time kernel, with a timer-based dispatcher?  Or am I\nmistaking a timer for a time-of-day-and-date clock?\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "On Tue, 22 Apr 1997, Henrik Frystyk Nielsen wrote:\n\n> At 12:16 PM 4/22/97 -0700, Benjamin Franz wrote:\n> \n> >My figures on www.xmission.com (a large server with many different\n> >commercial and non-commercial residents) from a sample of 27 gigabytes of\n> >recent measured traffic indicates that only about 13% of the traffic is\n> >text/*.  This slashes the potential savings to a mere 13% x 57.5% = 7.5%\n> >from compressing the text/* files. And this overlooks the fact that the\n> >majority of people browsing are doing so over modem links that *already*\n> >perform pretty good on the fly compression of the data flowing through\n> >them - thus reducing the potential savings to the end user from\n> >pre-compressing text/* to negligible.\n> \n> Figures showing (potentially lack of) savings using compression compared to\n> all other data formats are all very good, but is in fact not what our data\n> results are all about.\n> \n> 1) In typical browsing mode, the very first packet on a connection contains\n> an HTML page - the images are not requested until the HTML has arrived and\n> started being parsed. TCPs behavior over time is a non-linear function\n> where the first packet is much more expensive than the last. Therefore, it\n> is likely to be a win to concentrate our efforts on the first packet. This\n> is exactly what compressing HTML gives us.\n> \n> 2) Modem compression has on several occasions been indicated to have\n> \"pretty good\" performance. Our data show otherwise - but not explicitly. I\n> just made some simple tests of modem compression with and without deflated\n> data and the figures are compelling - gaining about 2/3 in both time and\n> packets when using deflate. Look at\n> \n>    http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/PPP.html\n> \n> for details.\n\n   \"Note that we only download  the HTML page and not any of the following\n    images. The size of the uncompressed HTML page is 42K HTML and the\n    compressed was 11K. This means that we decrease the overall payload\n    with about 31K or 73.8%. \"\n\nThat is an *exceptionally large* HTML document - about 10 times the size\nof the average HTML document based on the results from our webcrawling\nrobot here (N ~= 5,000 HTML documents found by webcrawling). Very few web\ndesigners would put that much on a single page because they are aiming for\na target of 30-50K TOTAL for a page - including graphics.\n\nAs noted: deflate and other compression schemes do much better on large\ntext/* documents than small ones. Using an overly large document gives a\nmisleading comparision against the short window compression that modems\nperform by basically allowing deflate a 'running start'. You should do the\ncomparision using 3-4K HTML documents: The whole test document should be\nonly 3-5K uncompressed and 1-2K compressed. \n\n> Compression also helps on a LAN - see the figures at\n> \n>    http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/LAN.html\n> \n> 3) On the more speculative side, I don't consider the current composition\n> of data formats in caches being constant. The paper describes the potential\n> benefits of using style sheets and other data formats than the more\n> traditional gif and jpeg. Style sheets are just starting to be deployed and\n> it may change the contents significantly over the next 6 months. CSS1 style\n> style sheets compress just as well as HTML, so there is yet another point\n> counting for compression.\n\nAgain, the document used was around 10 times the size of the typical HTML\ndocument. This should be re-done with more typical test documents. In\nfact, it would probably be a good idea to test multiple sizes of documents\nas well as realistic mixes of text/* and image/* to understand how\ndocument size and mix affect the results of compression and pipelining.\n\n> So, the _actual_data_ that we have now for the effect of compression seems\n> to indicate with little doubt that it is worth doing!\n\nNo - it only indicates that is may be worth doing. Or may not. Your PPP\nand LAN tests were done using atypical input data - their results may be\n(probably are) atypical as well. \n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "At 02:43 PM 4/22/97 -0700, David W. Morris wrote:\n>On Tue, 22 Apr 1997, Benjamin Franz wrote:\n>\n>> majority of people browsing are doing so over modem links that *already*\n>> perform pretty good on the fly compression of the data flowing through\n>> them - thus reducing the potential savings to the end user from\n>> pre-compressing text/* to negligible.\n>\n>But the user's modem link is only one of many hops between the user and\n>the server so I don't agree that the fact of modem compression on one hop\n>negates the importance of compression of the original material. \n\nMost modem dial-up links employ PPP to initiate their connection and PPP\ncompression, as implemented in Win95 and NT, negates most if not all of the\neffects of modem (v.42bis) compression.\n\nA bigger-picture issue for compressing HTML is the potential to reduce the\nnumber of IP packets traveling over those 'hops'. This is an effect that\nneither PPP nor modem compression can offer. The fewer packets handled at\neach hop, the more capacity available for other traffic at those hops and\nthus, more overall network improvement. That is, the raw 'local' efficiency\nof the compression is not the only benefit, even when that efficiency is\nrelatively low.\n\n[snip...]\n\nRegards,\n\nBob Monsour\nrmonsour@hifn.com\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "On Tue, 22 Apr 1997, Bob Monsour wrote:\n\n> Most modem dial-up links employ PPP to initiate their connection and PPP\n> compression, as implemented in Win95 and NT, negates most if not all of the\n> effects of modem (v.42bis) compression.\n\nDon't underestimate the modem compression. When moving highly compressible\n(textual) data it can easily double to triple throughput in my experience.\n\n> A bigger-picture issue for compressing HTML is the potential to reduce the\n> number of IP packets traveling over those 'hops'. This is an effect that\n> neither PPP nor modem compression can offer. The fewer packets handled at\n> each hop, the more capacity available for other traffic at those hops and\n> thus, more overall network improvement. That is, the raw 'local' efficiency\n> of the compression is not the only benefit, even when that efficiency is\n> relatively low.\n\nEven without the modem compression you are only talking on the order of a\n7.5% savings - or about 3/4 of a second difference on a ten second load -\nfor a typical web page with a mix of about 30K of text and graphics\ncombined. This seems to me to be chasing the point of diminishing returns.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Origin Servers without Clock",
            "content": "   Date: Tue, 22 Apr 97 18:17:14 EDT\n   From: \"Ross Patterson\" <Ross_Patterson@ns.reston.vmd.sterling.com>\n\n   While I understand and sympathize with the issue here (I've already\n   got too many clocks in my home, I don't need more to reset twice a\n   year!), I find it suprising that TCP can be implemented on a system that\n   has no timing facilities.  For that matter, don't many (most?) embedded\n   systems have a real-time kernel, with a timer-based dispatcher?  Or am I\n   mistaking a timer for a time-of-day-and-date clock?\n\nYes, there's a difference between a timer designed to help with mutlitasking\nand a timer which tells you the time of day.\n\nAssuming a completely standalone server which has no battery backup,\nwhich boots using DHCP and tftp or whatever (or has all its software\nin ROM--though then it still needs at least rarp), there's generally\nno need to have a clock to run a soda machine or a thermometer.\nAdmittedly, a soda machine which logs times people get sodas\nis quite interesting.  So is a thermomenter which generates\nnice graphs showing time.\n\nAdmittedly, if you're going to all this trouble, you could ask\nanother machine on the network for the time; but I bet there are\nother applications where you don't care about the time.\n\n(What about routers?  Some of them can be configured from the web;\nbut why would a router care about wall clock time?)\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": "Roy T. Fielding:\n>\n>>Issues-list item \"DISPOSITION\"\n>>\n>>From the Memphis minutes:\n>>\n>>-- Content-Disposition will be added to the Appendix, as one\n>>   of a group of common MIME headers about which implementors should\n>>   be aware; Koen Holtman will draft the addition.\n>>\n>>Here is a draft of the addition.  This text is based on 1806 and on\n>>messages in the mailing list archive.  I have not tested this myself.\n>\n>Is there some reason why\n>\n>    Entity-Header = ...\n>                  | Content-Disposition    ; defined by RFC 1806 [nn]\n>\n>is not sufficient?\n\nThe proposed addition would answer a FAQ, a semi-obscure reference\nwould not.\n\n>....Roy\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": "Kevin J. Dyer:\n>\n>On Apr 22,  4:04am, Koen Holtman wrote:\n>> Subject: Issues-list item \"DISPOSITION\"\n[...]\n>\n>We still have a discrepancy on the delivered filename.  In general\n>the MUA tries to name the current stream based on the URI.  This leads to\n>unknown file types on the client system.  Based on messages in the archives,\n>maintainers are concerned that this will be compounded by groupware users and\n>others who want to share common files but the files are generated or retrieved\n>by an engine and not a straight URI.\n\nHmm, this seems like an new problem to me, which is not solved by\ncontent-disposition current practice.  The idea of the DISPOSITION\nappendix is to document current practice only, not to solve new\nproblems.\n\nI think WEBDAV is chartered to solve new problems like this one.\n\n>Kevin\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "At 03:44 PM 4/22/97 -0700, Benjamin Franz wrote:\n\n>That is an *exceptionally large* HTML document - about 10 times the size\n>of the average HTML document based on the results from our webcrawling\n>robot here (N ~= 5,000 HTML documents found by webcrawling). Very few web\n>designers would put that much on a single page because they are aiming for\n>a target of 30-50K TOTAL for a page - including graphics.\n\nIt would be interesting to elaborate a bit on getting a better impression\non what the distribution of web pages is. A sample of 5000 is not big\nenough to put *'s around your conclusions. I know that there are many cache\nmaintainers and maybe even indexers on this mailing list. Benjamin, what if\nyou tried getting these people to take a snapshot of their caches and get\nthe sizes of the HTML pages? It would be very useful information to a lot\nof us!\n\n>As noted: deflate and other compression schemes do much better on large\n>text/* documents than small ones. Using an overly large document gives a\n>misleading comparision against the short window compression that modems\n>perform by basically allowing deflate a 'running start'. You should do the\n>comparision using 3-4K HTML documents: The whole test document should be\n>only 3-5K uncompressed and 1-2K compressed. \n\nI tried to do this with the page\n\n  http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/PPP.html\n\nwhich is 4312 uncompressed and 1759 compressed. It still gives a 30%\nincrease in speed and a 35% gain in packets. Below that size, the number of\nTCP packets begin to be the same and therefore little difference is to be\nexpected.\n\nNote, this is using default compression _including_ the dictionary.\nIntelligent tricks can be played by making a pre-defined HTML-aware\ndictionary in which case the win will be bigger.\n\n>> 3) On the more speculative side, I don't consider the current composition\n>> of data formats in caches being constant. The paper describes the potential\n>> benefits of using style sheets and other data formats than the more\n>> traditional gif and jpeg. Style sheets are just starting to be deployed and\n>> it may change the contents significantly over the next 6 months. CSS1 style\n>> style sheets compress just as well as HTML, so there is yet another point\n>> counting for compression.\n>\n>Again, the document used was around 10 times the size of the typical HTML\n>document. This should be re-done with more typical test documents. In\n>fact, it would probably be a good idea to test multiple sizes of documents\n>as well as realistic mixes of text/* and image/* to understand how\n>document size and mix affect the results of compression and pipelining.\n\nMy point here was that the size may not be that bad after all - considering\nthe effect of style sheets. As style sheets may be included in the HTML\ndocument this may cause the overall size of HTML documents to increase.\nLikewise, it will make a lot of graphics go away, as it gets replaced by\nstyle sheets.\n\n>> So, the _actual_data_ that we have now for the effect of compression seems\n>> to indicate with little doubt that it is worth doing!\n>\n>No - it only indicates that is may be worth doing. Or may not. Your PPP\n>and LAN tests were done using atypical input data - their results may be\n>(probably are) atypical as well. \n\nIf you look closely at the LAN data, the size is actually of little\nimportance.  The main point here is that adding information in the first\nTCP packet increases the possibility of avoiding delayed TCP\nacknowledgement. This alone accounts for up to 200ms delays and this may\nhappen even on a 2-3K document. On a 10MBit Ethernet, it takes about 50ms\nto transfer 40K, so this is not nearly as significant a delay.\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "On Wed, 23 Apr 1997, Henrik Frystyk Nielsen wrote:\n\n> At 03:44 PM 4/22/97 -0700, Benjamin Franz wrote:\n> \n> >That is an *exceptionally large* HTML document - about 10 times the size\n> >of the average HTML document based on the results from our webcrawling\n> >robot here (N ~= 5,000 HTML documents found by webcrawling). Very few web\n> >designers would put that much on a single page because they are aiming for\n> >a target of 30-50K TOTAL for a page - including graphics.\n> \n> It would be interesting to elaborate a bit on getting a better impression\n> on what the distribution of web pages is. A sample of 5000 is not big\n> enough to put *'s around your conclusions. I know that there are many cache\n> maintainers and maybe even indexers on this mailing list. Benjamin, what if\n> you tried getting these people to take a snapshot of their caches and get\n> the sizes of the HTML pages? It would be very useful information to a lot\n> of us!\n\nDammit, you are just begging for me to create a Squid store.log analysis\ntool. :) \n\n> >As noted: deflate and other compression schemes do much better on large\n> >text/* documents than small ones. Using an overly large document gives a\n> >misleading comparision against the short window compression that modems\n> >perform by basically allowing deflate a 'running start'. You should do the\n> >comparision using 3-4K HTML documents: The whole test document should be\n> >only 3-5K uncompressed and 1-2K compressed. \n> \n> I tried to do this with the page\n> \n>   http://www.w3.org/pub/WWW/Protocols/HTTP/Performance/Compression/PPP.html\n> \n> which is 4312 uncompressed and 1759 compressed. It still gives a 30%\n> increase in speed and a 35% gain in packets. Below that size, the number of\n> TCP packets begin to be the same and therefore little difference is to be\n> expected.\n\nBut remember this is _only_ on the text/* document portion of the traffic\n- which is itself only around 13% of the total traffic. So, basically you\nsave 30% in time on 13% of the traffic - or about a net 3.9% savings. By\nyour own figures this is even worse than the figures I gave - which\nestimated a net 7.5% savings (based on a much higher compression estimate\nof 57% for text/*). \n\n> Note, this is using default compression _including_ the dictionary.\n> Intelligent tricks can be played by making a pre-defined HTML-aware\n> dictionary in which case the win will be bigger.\n\nEven if it doubles the compression efficiency, you would not crack 8% net\nsavings. \n\n> >Again, the document used was around 10 times the size of the typical HTML\n> >document. This should be re-done with more typical test documents. In\n> >fact, it would probably be a good idea to test multiple sizes of documents\n> >as well as realistic mixes of text/* and image/* to understand how\n> >document size and mix affect the results of compression and pipelining.\n> \n> My point here was that the size may not be that bad after all - considering\n> the effect of style sheets. As style sheets may be included in the HTML\n> document this may cause the overall size of HTML documents to increase.\n> Likewise, it will make a lot of graphics go away, as it gets replaced by\n> style sheets.\n\nI doubt it. The graphics load is not determined by things that stylesheets\nwill affect ultimately: Designers would put more and higher quality\ngraphics in than they do today if it wouldn't slow the load to\nunacceptable levels. Byte hungry designers will implement *external*\nstylesheets and scripting to get the cache win. They will then use the\nfreed bytes will add *more* graphics and multi-media to do things\nstylesheets still can't.\n\nrom their perspective stylesheets are not an opportunity to reduce the\noverall byte count but rather to include more things they couldn't before\nbecause the byte count would be too high to be acceptable. If anything,\nyou may actually see the HTML docs *shrink* while becoming harder to\ncompress because of the ability to toss styling and scripting into\nseperate re-usable documents. This will reduce the win from compressing\ntext/* even more.\n\nThe process of web page design is usually a balance between the graphic\nartist who wants to cry when he is told that his machine has less than 128\nmeg of memory and 'only' 2 gig of hard drive space and the web designer\nwho must brutalize the artist's work until it will fit into less than 30K.\nMake no mistake - the usual design process is forcing the size *down* to\n30K - not *up* to it. \n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "Henrik Frystyk Nielsen:\n[...]\n>  I know that there are many cache\n>maintainers and maybe even indexers on this mailing list. Benjamin, what if\n>you tried getting these people to take a snapshot of their caches and get\n>the sizes of the HTML pages? It would be very useful information to a lot\n>of us!\n\nOne note here: the size distribution of pages stored in web caches\nwill not in general equal the size distribution of the pages\ntransported over the HTTP link.  Especially if you have a cache\nreplacement algorithm which takes the size of the page into account.\n\nAnd it is the properties of the traffic over the link (usually the\nupstream link these days, unless you have a slow modem) we are\ninterested in.\n\nFor those who have time to do statistics, I suggest using Digital's\nWeb Proxy Traces at\n\nftp://ftp.digital.com/pub/DEC/traces/proxy/webtraces.html\n\n>Henrik\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "> enough to put *'s around your conclusions. I know that there are many\n> cache\n> maintainers and maybe even indexers on this mailing list. Benjamin,\n> what if\n> you tried getting these people to take a snapshot of their caches and\n> get\n> the sizes of the HTML pages? It would be very useful information to a\n> lot\n> of us!\n> \nFor what it's worth, we recently did some analysis of the logs from\nMicrosoft's corporate proxies. For static HTML, there were approximately\n77,000 hits, and the average size was 6882 bytes. For HTML that we\nbelieve was dynamically generated there were approx. 74,500 hits with an\naverage size of 7419.\n\nHenry\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;QZERO&quot",
            "content": "Roy T. Fielding:\n>\n>>From the Memphis minutes:\n>>\n>> -- Koen Holtman\n>>    will draft a clarification that a qvalue of 0.0 means \"Don't send\n>>    me this.\"\n>>\n>>The clarification (taken from the slide I showed in the Memphis\n>>evening session) is one new sentence in section 3.9 of the 1.1 spec.\n>>The new sentence is between ** **.\n>>\n>> 3.9 Quality Values\n>>\n>>   HTTP content negotiation (section 12) uses short \"floating point\"\n>>   numbers to indicate the relative importance (\"weight\") of various\n>>   negotiable parameters. A weight is normalized to a real number in the\n>>   range 0 through 1, where 0 is the minimum and 1 the maximum value.\n>>   **If a parameter has a quality value of 0, then content with this\n>>   parameter is `not acceptable' for the client.**\n>>   HTTP/1.1 applications MUST NOT generate more than three digits after\n>>   the decimal point. User configuration of these values SHOULD also be\n>>   limited in this fashion.\n>\n>That change does not belong in that section -- it belongs in the sections\n>on the Accept* header fields.  Servers send qvalues too.\n\nThe interpretation of q=0 is a general thing, so it belongs in the\ngeneral section as far as I am concerned.  And servers do not send\nqvalues in HTTP/1.1.  They do send them under TCN, but such a qs=0 will\nstill mean that the content is not acceptable for the client.\n\nI'm willing to change my proposal from \n\n is `not acceptable' for the client\n\nto\n\n will be `not acceptable' for the client\n\nbut I think it would be a mistake to move this to the Accept* sections.\n\n>....Roy\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "Well it looks like we are discussing a bunch of different problems.\n\nI was looking at storage management at the proxy (and thought the\nsubject was clearly indicating this, although now I see it might\nbe a bit ambiguous). Probably, wrong mailing list...\n\nSome of the other postings and pointers were referring to minimizing\ntraffic over the link (or perhaps transfer time, which is yet another\nissue ?).\n\nApart from using caches, you can reduce the traffic by using:\n\n    - a more compact source encoding;\n    - link layer compression;\n    - shorter/compressed protocol headers;\n\nto me the former two approaches seem to be slightly out of scope:\nalthough in the short term it might be worthwile to try to compress\nredundant encodings such as HTML or JAVASCRIPT files, as others\nhave commented, this is just a temporary solution which will probably\nbecome less and less encessary as providers will realize that they have\nto cut data at the source.\n\nI'd also note that many of the notes on the web pages focus on TCP\nfeatures such as Nagle, slow start, delayed acks and try to defeat them\nin various ways. Again, once a problem is known, just solve it the\nright way (e.g. by using appropriate socket options).\n\nCheers\nLuigi\n-----------------------------+--------------------------------------\nLuigi Rizzo                  |  Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it    |  Universita' di Pisa\ntel: +39-50-568533           |  via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522           |  http://www.iet.unipi.it/~luigi/\n_____________________________|______________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "At 07:05 PM 23/04/97 +0200, Koen Holtman wrote:\n\n>One note here: the size distribution of pages stored in web caches\n>will not in general equal the size distribution of the pages\n>transported over the HTTP link.  Especially if you have a cache\n>replacement algorithm which takes the size of the page into account.\n\n  On the other hand, it may provide a reasonable approximation to the\ndistribution of pages passed between caches, which is of value in\ndetermining what improvement might be possible in cache to cache\ncommunication, even if not HTTP communication more generally.\n\n- Donald Neal\n\n\n\n"
        },
        {
            "subject": "Back from the grave: Unverifiable Transactions and Cookie",
            "content": "Oh boy. It looks to me like the 'but unverifiable transactions are GOOD'\ncrowd just went the guerilla PR route. Those of you who get ClariNet\nshould check out the article titled: \"****Online Professionals Support Web\nCookies 04/23/97\" <URL:news:Naf6_35U@clari.net>, clari.tw.top. If you were\nto take what the 'Association of Online Professionals (AOP)' says at face\nvalue you would think that the WG had just proposed turning off ALL\ncookies by default (naughty, naughty WG). \n\nFair use excerpt:\n\n  Among the negative impacts, according to McClure are a \"potential\n  loss of services from online services such as MSN, which rely on\n  cookies for passwords, preferences and other common tasks; loss of\n  all electronic commerce that relies on cookies, including those\n  based on the \"shopping cart\" models; and loss of one of the major\n  methods of advertising effectiveness analysis for Web sites that\n  rely on such revenues and sponsorships for their economic base.\"\n\nAm I crazy or of the items they list is the ONLY one *actually* affected\nthe \"loss of one of the major methods of advertising effectiveness\nanalysis for Web sites\"?\n\nThe Big Lie lives....\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Back from the grave: Unverifiable Transactions and Cookie",
            "content": "Benjamin Franz:\n>\n>\n>Oh boy. It looks to me like the 'but unverifiable transactions are GOOD'\n>crowd just went the guerilla PR route. Those of you who get ClariNet\n>should check out the article titled: \"****Online Professionals Support Web\n>Cookies 04/23/97\" <URL:news:Naf6_35U@clari.net>, clari.tw.top.\n\nI found the same article in biz.clarinet, which is a more common\nnewsgroup I believe.  There is also an article about this at\nhttp://www.news.com/News/Item/0,4,9962,00.html, though that one does\nnot have the quote below.\n\n> If you were\n>to take what the 'Association of Online Professionals (AOP)' says at face\n>value you would think that the WG had just proposed turning off ALL\n>cookies by default (naughty, naughty WG). \n>\n>Fair use excerpt:\n>\n>  Among the negative impacts, according to McClure are a \"potential\n>  loss of services from online services such as MSN, which rely on\n>  cookies for passwords, preferences and other common tasks; loss of\n>  all electronic commerce that relies on cookies, including those\n>  based on the \"shopping cart\" models; and loss of one of the major\n>  methods of advertising effectiveness analysis for Web sites that\n>  rely on such revenues and sponsorships for their economic base.\"\n>\n>Am I crazy or of the items they list is the ONLY one *actually* affected\n>the \"loss of one of the major methods of advertising effectiveness\n>analysis for Web sites\"?\n\nYes, only that one is affected somewhat.  And even `advertising\neffectiveness analysis' does not rely on cookies that much.\n\n>\n>The Big Lie lives....\n>\n>-- \n>Benjamin Franz\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "A linguistic note on unverifiable transaction",
            "content": "This reminds me: if you are a browser vendor implementing (some parts\nof) 2109, please do *not* use language like\n\n  [X] disable cookies in unverifiable transactions \n\nin your preference setting panels.  It is not only ugly, it is\nimprecise as well, because 2109 talks about the option\n\n  [X] disable cookies in unverifiable transactions on domains which\n      do not domain-match the domain of the origin transaction.\n\n`unverifiable transaction' is fine terminology for specs (disclaimer:\nI believe I invented it), but in end-user applications you should say\nsomething like\n\n  [X] disable third-party cookies\n\n\nThanks,\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: A linguistic note on unverifiable transaction",
            "content": "On Thu, 24 Apr 1997, Koen Holtman wrote:\n\n> \n> This reminds me: if you are a browser vendor implementing (some parts\n> of) 2109, please do *not* use language like\n> \n>   [X] disable cookies in unverifiable transactions \n> \n> in your preference setting panels.  It is not only ugly, it is\n> imprecise as well, because 2109 talks about the option\n> \n>   [X] disable cookies in unverifiable transactions on domains which\n>       do not domain-match the domain of the origin transaction.\n> \n> `unverifiable transaction' is fine terminology for specs (disclaimer:\n> I believe I invented it), but in end-user applications you should say\n> something like\n> \n>   [X] disable third-party cookies\n\nI would really hope that any browser vendor would do a much better job\nof providing a user interface than any of the above variants. There is\nNO WAY Joan Average-User has any way of understanding:\n     \"unverifiable transaction\"\n     \"domain\"\n     \"domain match\"\n     \"origin transaction\"\nEtc.  Therefore, informed consent would be impossible.\n\nActually 'unverifiable transaction' is bad spec termnology as it implies\nthere might be a verifiable transaction.  There was a recent suggestion\nthat the transaction be refered to as an 'indirect transaction'.\n\nFor informed consent to exist, users need:\n\na.  An explanation of the issues written in terms they can understand\nb.  An explanation they can *easily* view as to how an individual cookie \n    provider will use the cookies.\n\nBrowser vendors can nicely differentiate themselves by how well they\nintegrate the decision process for the user. The protocol doesn't make\n(b) possible and it should (e.g., the commenturl).  Secondly, some form \nof the general Cookie Certificate Approach proposed to the WG should be\nintegrated as an alternative which enables immediate acceptance of\nauthenticated level-1 cookies preconfigured in browsers. It really\nshouldn't be difficult to get the CCA infrastructure established by the\ntime UAs are available with the CCA support.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": "> I think WEBDAV is chartered to solve new problems like this one.\n\nIt's not.\n\n\n\n"
        },
        {
            "subject": "Re: A linguistic note on unverifiable transaction",
            "content": "David W. Morris:\n[...]\n>\n>Actually 'unverifiable transaction' is bad spec termnology as it implies\n>there might be a verifiable transaction.\n\n??\n\nVerifiable transactions do exist, and are defined by the spec.\n\n>Dave Morris\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: A linguistic note on unverifiable transaction",
            "content": "On Fri, 25 Apr 1997, Koen Holtman wrote:\n\n> David W. Morris:\n> [...]\n> >\n> >Actually 'unverifiable transaction' is bad spec termnology as it implies\n> >there might be a verifiable transaction.\n> \n> ??\n> \n> Verifiable transactions do exist, and are defined by the spec.\n\nThat I understand ... I guess my point wasn't clear. The whole notion of\nverifiable here is rather obscure as the english word means something to\nme much stronger than the usage in the spec.  The implication is along the\nlines of a certificate authority having done research to verify the\nidentity of the origin and providing a CA to that end.  When I initially\nstudied the various drafts, I found it confusing because I had to keep\nreminding myself exactly how the termnology was defined in the draft.\n\nAs you pointed out, the concept is completely unsuitable from a UI\nperspective.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: Issueslist item &quot;DISPOSITION&quot",
            "content": "On Apr 24,  6:29pm, Larry Masinter wrote:\n> Subject: Re: Issues-list item \"DISPOSITION\"\n> > I think WEBDAV is chartered to solve new problems like this one.\n>\n> It's not.\n>\n>\n>-- End of excerpt from Larry Masinter\nThanks for the backup, and I'm not going crazy.  I just spent the last day\nreviewing the archives of WEBDAV and this topic of a UA using the C-D as a\nsuggested filename on download is somewhere in the new black hole that was\nfound.\n\nI understand that the C-D section is a documentation of the current pratice.\nBut, what is the collective say of the WG about the future DISPOSITION\ndirection.  I've initiated a private discussion with members of WEBDAV, but\nwho should be driving the bus?  IMHO, the WG should since it is general\nissue.\n\nKevin Dyer\n\n\n--\n=============================================================================\nKevin J. DyerDraper Laboratory  MS 23.\nEmail: <kdyer@draper.com>        555 Tech. Sq.\nPhone: 617-258-4962Cambridge, MA 02139\nFAX: 617-258-2121\n-----------------------------------------------------------------------------\n   \"Beware Geeks bearing GIFs\"   Author Unknown\n=============================================================================\n\n\n\n"
        },
        {
            "subject": "Re: A linguistic note on unverifiable transaction",
            "content": "> \n> \n> \n> On Fri, 25 Apr 1997, Koen Holtman wrote:\n> \n> > David W. Morris:\n> > [...]\n> > >\n> > >Actually 'unverifiable transaction' is bad spec termnology as it implies\n> > >there might be a verifiable transaction.\n> > \n> > ??\n> > \n> > Verifiable transactions do exist, and are defined by the spec.\n> \n> That I understand ... I guess my point wasn't clear. The whole notion of\n> verifiable here is rather obscure as the english word means something to\n> me much stronger than the usage in the spec.  The implication is along the\n> lines of a certificate authority having done research to verify the\n> identity of the origin and providing a CA to that end.  When I initially\n> studied the various drafts, I found it confusing because I had to keep\n> reminding myself exactly how the termnology was defined in the draft.\n\nThere is actually a proposed addition (seperate RFC?) that explores\nthe idea of the CA type entity in this context.  \"Verifiable,\" in that\nexpanded context, seems to be very appropriate.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: Pipelining and compression effect on HTTP/1.1 proxie",
            "content": "There seems to be some question about whether end-to-end compression\nis useful in the Web.\n\nTogether with some people from AT&T Labs - Research, I wrote a\npaper for this year's SIGCOMM conference that includes some \nReal Data that sheds some light on this issue.  SIGCOMM recently\naccepted our paper, which means that we are now free to discuss\nit in public (the SIGCOMM review process is supposed to be blind).\n\nThe paper is:\n    Jeffrey C. Mogul, Fred Douglis, Anja Feldman, and Balachander\n    Krishnamurthy.  Potential benefits of delta-encoding and data\n    compression for HTTP.  In Proc. SIGCOMM '97 (to appear), Cannes,\n    France, September 1997.\n    \nand you can retrieve an UNREVISED DRAFT copy from\nhttp://ftp.digital.com:80/~mogul/DRAFTsigcomm97.ps\nPlease do not treat this as a final draft of the paper!!!\n\nThe paper is primarily about a bandwidth-saving technique called\ndelta-encoding, which is not mature enough for IETF standardization\n(so let's not discuss it here).  But we also looked at the potential\nfor improvement using simple data compression.\n\nOther messages in this thread have pointed out that when evaluating\nthe utility of compression, it's not necessarily a good idea to look\nat a static collection of URLs (since some URLs are referenced far\nmore often than others), and it's not a good idea to look at the\nresponses sitting in a cache (since this also mostly ignores the\nrelative reference rates, and completely ignores non-cachable\nresponses).\n\nWe instead looked at complete reference streams between a collection\nof clients and the Internet, capturing the entire contents of the\nrequests and responses.  At Digital, I did this by modifying a\nNON-CACHING proxy (wait - did he say \"NON-CACHING?\" Yes, I did)\nand captured over 500,000 responses over a 2-day period, from 8K\nclients and to 22K servers ... but (probably a mistake, in retrospect)\nI tried to save space and so did not capture most of the image\nURLs (.gif, .jpeg, etc.)\n\nAt AT&T, my co-authors used a packet-sniffing approach to capture\nover a million responses, including images, over a longer period\nof time, but from a much smaller set of clients.  (Again, no\nproxy cache was involved.)\n\nWe looked at a few different compression algorithms, but the\nwinner was usually \"gzip\" (although I understand that \"deflate\",\nwhich we did not try, is somewhat better than gzip). For the\nDigital trace (which includes very few images), the overall\nsavings in bytes transferred was about 39% (75% of the responses\nwere improve at least a little by compression).  For the AT&T\ntrace, which includes images, the overall savings was about 20%\nof the total bytes (but still we managed to improve 73% of the\nresponses).\n\nThe paper includes a table showing how compression improves\ndifferent HTTP content-types for the AT&T trace.  Overall,\n99.7% of the text/html responses were compressible, and this\nsaved almost 69% of the text/html bytes.  So compression could\nbe quite useful in practice, even if the bulk of the responses\n(as images) are not very compressible, for several reasons:\n(1) As Henrik and Jim have pointed out, compression\nof HTML files means that the IMG refs come sooner,\nwhich improves latency in retrieving them.\n(2) The increasing use of wireless or other slow\nnetworks, and of PDAs (or other small screens), means\nthat there will be users who mostly care about HTML\nperformance (because they are not going to load most\nimages anyway).\n\nOne of the things that we could do with our traces (but \nhaven't done yet) is to see if image responses have better\ncache behavior than HTML responses.  We do have some evidence\nthat images change less often than HTML pages (i.e., once\nan image is in a cache, it's highly likely that the next\nreference to that URL will result in the same image; this\nis not as true for HTML responses, since these often change\nmore rapidly over time.)  So it's possible that if we can\ndo various things to improve the caching of images, the\nnon-image (and therefore compressible) responses will become\nmore important as a fraction of total network load.\n\nRegarding modem compression, Benjamin Franz wrote:\n    Don't underestimate the modem compression. When moving highly\n    compressible (textual) data it can easily double to triple\n    throughput in my experience.\nTrue.  However, modem compression seems to be less effective than\ndocument-level compressors such as gzip, because (probably) it\nhas to operate more \"locally\".\n\nWe looked into this issue, and the paper presents results of\na fairly crude experiment, involving just 4 URLs (chosen\nto show HTML files of a range of sizes and complexities).\nWe transferred the files over a 28.8K modem (with modem\ncompression enabled), with and without file-level compression.\nFor short documents, gzip seems to beat plain modem compression\nby a moderate amount.  For longer documents, gzip does pretty\nwell, but another compressor (\"vdelta\") did even better ...\nbeating simple modem compression by as much as 69% of the\noverall transfer time.  (And, as Bob Monsour pointed out,\nhigh-level compression reduces the total number of packets\nsent, which is usually a win).\n\nThe paper also looks at some other issues, such as the\ncomputational cost of compressing and decompressing responses.\nEven on a fairly slow machine, existing compression programs\nrun fast enough to provide some benefit, except when using\na fast LAN.\n\nBottom line: based on our results, we think that end-to-end\ncompression is a win, even though it's hard to compress\nimages.  It would be a real shame if HTTP/1.1 had some minor\nflaws that make this impossible.  Henrik and I have an action\nitem, from the Memphis IETF meeting, to address the problems\nthat he has discovered when deploying \"deflate\" compression,\nand we should be making a proposal soon.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "I have reviewed the current proposal for State Management, and would like to \nsuggest one additional cookie attribute that would be extremely useful for \ninTRAnet environments.\n\nSpecifically, I would like to suggest a new attribute that will specify a URL \nto be submitted if the user closes the browser while the cookie is still \nvalid.\n\nThis new attribute would be used to make sure that server resources are freed \nif the user terminates the browser without logging off from the application.\n\nThe situation I am addressing is the case where an application is implemented \non a mainframe or very large Unix server.  The application requires a logon, \nand maintains some context between interactions with the browser.  The cookie \nallows the application to select the correct context.  The following issues \nwill exist in such an application.\n1. 10,000 or more users connected.\n2. The application *WILL* contain a logout button that users *should*\n   press when disconnecting.\n3. The application *WILL* have a timeout mechanism to expire sessions that\n   have not been active.\n4. All users are behind corporate firewall, and employees of the company.\n\nThe problem that occurs is that some users do not press the logout button. \n When this occurs, the mainframe must hold the resources associated with the \ncontext until the timeout occurs.  In some cases, this involves holding \ndatabase resources and memory resources that impact overall system \nperformance.  A malicious user might even mount a denial of service attack by \nstarting many sessions.\n\nIn order to notify the server that the browser has terminated, and \nconsequently will not be sending a logout anytime soon, I would like to \npropose a new cookie attribute that would be processed by the browser during \nshutdown.  The attribute would specify a URL to be submitted that would tell \nthe mainframe server to terminate the session and free associated resources.\n\nI recognize that such a mechanism will not be acceptable to users in general. \n However, for the corporate inTRAnet environment, it is important to be able \nto manage resources, and the availability of such an attribute would \nsignificantly improve availability and performance of a server.  As \ncorporations replace 3270 terminals with browsers, this type of mechanism \nwill become more important.\n\nThere are a number of very good reasons why one might not want the browser \nexecuting such a cookie.  Privacy issues, connection statistics, and many \nmore.  Performance is another factor.  If a browser collected several of \nthese cookies, it could take a while to shut down.  Recognizing these issues, \nI suggest that the user have full control over the proposed EndSession \nattribute.  The user should be able to configure a browser to act on this \nattribute in any number of optional ways:\na. ignore the attribute completely\nb. notify user when EndSession attribute is received\nc. notify user when EndSession is about to be executed during browser \nshutdown\nd. process EndSession only for specific \"trusted\" sites, thus allowing \ncorporate\n   applications to use EndSession, while rejecting the attribute from general \ninternet sites.\n\nThere may be other optional modes of processing as well.\n\nSummary:\nThe EndSession attribute would allow a site to *ask* the browser to execute a \nlogoff url if the browser is shut down.  This attribute attempts to solve a \nvery specific problem for large scale corporate inTRAnet applications.  I \nwould hope that this attribute is executed very rarely.  Normally, users will \nend a session explicitly by using an application LOGOFF button. The user \nshould have full control over how a browser processes such an attribute.\n\nFinally, the attribute name \"EndSession\" is only a suggestion.  The working \ngroup may have a much better name.  The proposed user controls at the browser \nlevel are also only suggestions.  The wg may have a better solution for user \nmanagement.\n\nMichael Giroux\n\n\n\n"
        },
        {
            "subject": "Re: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "Hi Mike,\n\nI understand the problme you are trying to solve, but I see\na large number of difficulties with the method which you are trying to\nsolve it.\n\n> The problem that occurs is that some users do not press the logout button. \n>  When this occurs, the mainframe must hold the resources associated with the \n> context until the timeout occurs.  In some cases, this involves holding \n> database resources and memory resources that impact overall system \n> performance.  A malicious user might even mount a denial of service attack\n> by  starting many sessions.\n> \n1 On the D.O.S. attack, I dont really see how this helps.  In mounting\nany serious attack, the attacker would be smart to write a small\nclient program to produce many sessions, assuming it could\ndefeat a duplicate IP addr check ( multi session same client ),\nit could simply choose never to honor your endsession url..\n\n2 What about people like me who leave their browsers running forever,\nI just lock my screen at night, etc..  The endession would never\nget executed.\n\n3 Dont cookies often persist longer than the 'browser session' ?\nie stored in the cookie file?  SHould the browser delete the cookie\non shutdown ?\n\n4. What is the method which the endsession URL should be submitted?\nPOST, GET?  What is the browser to do with the response?\n\n5. Security.\nGee, this sounds like a nice way for a site to induce a client\nto access an abritrary URL at shutdown.   What if the URL is a \nfile containing Java, ActiveX or the like ?\n\n\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department            \"Mighty Morphin' Proxy Ranger\"\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "New PEP draft available as ID",
            "content": "As a result of the discussions at the Memphis IETF meeting and WWW6, a new\nversion of the PEP draft has been submitted as an ID. I have attached the\ndraft to this mail.\n\nYou can also find the draft as an W3C working draft at\n\nhttp://www.w3.org/pub/WWW/TR/WD-http-pep-970428.html\n\nPlease read and comment!\n\nThanks,\n\nHenrik\n\n\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n\ntext/plain attachment: draft-ietf-http-pep-03.txt\n\n\n\n\n"
        },
        {
            "subject": "proxy autoconfiguratio",
            "content": "At work (this is my \"home\" address) we have a number of proxy servers\nwhich are geographically/topologically distributed, and people manually\nconfigure their browsers to use the appropriate proxy. Lately, I've been\nthinking about how one might go about configuring a network so that proxy\nconfiguration is automatic. I suppose the proxies could send 305 responses\nto requests originating on the wrong network, but if a client has no proxy\nconfigured it then becomes necessary for a router to send back a 305\nresponse in response to attempts to connect via TCP port 80, and I really\ndon't like this approach. Ideally, I'd like to see this done through DHCP,\nbut that requires OS level support.\n\nCan anyone point me in the direction of what has already been done here?\n\n---\nGregory Woodhouse\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "RE: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "On Monday, April 28, 1997 7:37 AM, Scott Lawrence [SMTP:lawrence@agranat.com] \nwrote:\n>\n> >>>>> \"MG\" == Michael Giroux <mgiroux@worldnet.att.net>:\n>\n> MG> I have reviewed the current proposal for State Management, and\n> MG> would like to suggest one additional cookie attribute that would\n> MG> be extremely useful for inTRAnet environments.\n>\n> MG> Specifically, I would like to suggest a new attribute that will\n> MG> specify a URL to be submitted if the user closes the browser while\n> MG> the cookie is still valid.\n>\n>   I'm afraid I must agree with Joshs' critique; especially the\n>   security implications.\n\nThis is precisely why the browser needs to have configuration\noptions for processing.\n\nI fail to see how adding an attribute to the specification causes the \nproblems\nthat you envision.  First, the browser is not required to implement support.\nSecond, this is for inTRAnet applications where sites will have considerable\ncontrol over user configurations.  Finally, if the specification suggests \nthat\nbrowsers should ignore such an attribute by default, then it seems tough to\nmake any assumptions about it.\n\n\n>   In addition, because you can't assume among other things that:\n>     - the users browser is up to date enough to support this\n>     - the user has not configured the browser to disable this\n>     - the client system will not crash\n\nThis feature is intended for inTRAnet applications.  In this case, the\nbrowsers *will* be up to date, and the site will dictate how to configure\nthe browsers. Again, inTRAnet.  These are corporate applications,\nbehind the firewall, inside the glass house, ...\nCorporate policy will mandate that the proper browser is installed,\nand that it is configured appropriately.\n\n\n>     ... your server application must still have all the support for\n>   timing out the session and cleaning it up anyway, and good system\n>   design would need to allow for the worst case (the power fails and\n>   comes back - all the clients die, then come up and try to create new\n>   sessions - and then the power fails...), so the attribute looks to\n>   me like an easy way to assume you've solved a problem you really\n>   have not.\n\nAs I said, the intent is for inTRAnet applications where the site \nadministration\nhas considerable control over the user environment.  True that the server \nwill\nhave to program for timeouts, but this should not preclude the workgroup from\nconsidering features that allow a site to improve on the situation.  It seems \na\nbit shortsighted to leave a feature out of the specification just because it\nmight cause a security problem.  If all specs took potential risks into \naccount,\nwe would never have been able to do file io or dereference pointers in C.\n\nMichael Giroux\n\n\n\n"
        },
        {
            "subject": "RE: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "On Tuesday, April 29, 1997 12:02 AM, Josh Cohen [SMTP:josh@netscape.com] \nwrote:\n> Hi Mike,\n>\n> I understand the problme you are trying to solve, but I see\n> a large number of difficulties with the method which you are trying to\n> solve it.\n>\n> > The problem that occurs is that some users do not press the logout \nbutton.\n> >\n> >  When this occurs, the mainframe must hold the resources associated with\n> >  the\n> > context until the timeout occurs.  In some cases, this involves holding\n> > database resources and memory resources that impact overall system\n> > performance.  A malicious user might even mount a denial of service\n> > attack\n> > by  starting many sessions.\n> >\n> 1 On the D.O.S. attack, I dont really see how this helps.  In mounting\n> any serious attack, the attacker would be smart to write a small\n> client program to produce many sessions, assuming it could\n> defeat a duplicate IP addr check ( multi session same client ),\n> it could simply choose never to honor your endsession url..\n\nI agree, a clever programmer will write a program.  This would only stop \nbeginners.\n\n> 2 What about people like me who leave their browsers running forever,\n> I just lock my screen at night, etc..  The endession would never\n> get executed.\n\nThe server will be required to have a timeout on the sessions.\nI tried to explain that in the initial post.\n\nHowever, always relying on the timeout will force servers to hold resources \nlonger\nthan necessary.  One sure event that could be trapped is browser shutdown. \n Any\nsessions that are terminated prior to the server defined timeout will help.\n\n> 3 Dont cookies often persist longer than the 'browser session' ?\n> ie stored in the cookie file?  SHould the browser delete the cookie\n> on shutdown ?\n\nSome cookies can persist longer than the browser session, but the State \nManagement spec\nstates that the DISCARD attribute, and Max-Age attributes will control this.\n\n> 4. What is the method which the endsession URL should be submitted?\n> POST, GET?  What is the browser to do with the response?\n\nThis could be specified in the EndSession attribute.  However, I suspect it \nshould be a POST.\n\n> 5. Security.\n> Gee, this sounds like a nice way for a site to induce a client\n> to access an abritrary URL at shutdown.   What if the URL is a\n> file containing Java, ActiveX or the like ?\n\nI agree this is a concern.  This concern is exactly why the browser needs to \nimplement\nuser control over whether this attribute is processed.  For the inTRAnet \ncase, the\nserver would be trusted, and this is not a concern.  I suggest that the \nbrowser be configurable\nto specify a set of trusted servers that this attribute will be accepted \nfrom.\n\nMichael Giroux\n\n\n\n"
        },
        {
            "subject": "RE: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "The suggestion that\n>     - the users browser is up to date enough to support this\n\nDoes not seem to apply.  This is a specification for a new type of cookie,\ndefined by Set-Cookie2, and there is no guarantee that the browser\nwill support the Set-Cookie2, least of all one of the attributes.  For those\nbrowsers that do support Set-Cookie2, the EndSession attribute would\nbe entirely optional.  The specification would only define how the attribute\nis processed *if* implemented.\n\nAs for\n>     - the client system will not crash\n\nAny brwoser that crashes is coded incorrectly.  Browsers are supposed to\nignore headers they do not recognize.  From the draft for\nHTTP State Management Mechanism (Rev 1) dated March 19, 1997\n\"The user agent should ignore attribute-values pairs whos attribute\nit does not recognize.\"\n\nOn the issue of security, here again, the draft is fairly clear\n\"This state management specification therefore requires that a user\nagent give the user control over such a possible intrusion, although\nthe interface through which the user is given this control is left \nunspecified.\"\n\n\n\nOn Monday, April 28, 1997 7:37 AM, Scott Lawrence [SMTP:lawrence@agranat.com] \nwrote:\n>\n> >>>>> \"MG\" == Michael Giroux <mgiroux@worldnet.att.net>:\n>\n> MG> I have reviewed the current proposal for State Management, and\n> MG> would like to suggest one additional cookie attribute that would\n> MG> be extremely useful for inTRAnet environments.\n>\n> MG> Specifically, I would like to suggest a new attribute that will\n> MG> specify a URL to be submitted if the user closes the browser while\n> MG> the cookie is still valid.\n>\n>   I'm afraid I must agree with Joshs' critique; especially the\n>   security implications.\n>\n>   In addition, because you can't assume among other things that:\n>     - the users browser is up to date enough to support this\n>     - the user has not configured the browser to disable this\n>     - the client system will not crash\n>\n>     ... your server application must still have all the support for\n>   timing out the session and cleaning it up anyway, and good system\n>   design would need to allow for the worst case (the power fails and\n>   comes back - all the clients die, then come up and try to create new\n>   sessions - and then the power fails...), so the attribute looks to\n>   me like an easy way to assume you've solved a problem you really\n>   have not.\n>\n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-pep03.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : PEP - an Extension Mechanism for HTTP                   \n       Author(s) : H. Frystyk\n       Filename  : draft-ietf-http-pep-03.txt\n       Pages     : 24\n       Date      : 04/28/1997\n\nHTTP is used increasingly in applications that need more facilities than \nthe standard version of the protocol provides, from distributed authoring, \ncollaboration and printing, to various remote procedure call mechanisms.   \n\nThe Protocol Extension Protocol (PEP) is an extension mechanism designed to\naddress the tension between private agreement and public specification and \nto accommodate extension of HTTP clients and servers by software \ncomponents.                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-pep-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-pep-03.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-pep-03.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "State managemen",
            "content": "At different points during the Memphis meetings\nwe talked about the need/possibility of splitting\nstate management into a seperate working group.\n\nAs a straw poll, what do people think the\nwork of such a group would be, and is that\nwork best dealt with in a different group?\n\n\nregards,\nTed Hardie\n\n\n\n"
        },
        {
            "subject": "Re: State managemen",
            "content": "On Tue, 29 Apr 1997, Ted Hardie wrote:\n\n> At different points during the Memphis meetings\n> we talked about the need/possibility of splitting\n> state management into a seperate working group.\n> \n> As a straw poll, what do people think the\n> work of such a group would be, and is that\n> work best dealt with in a different group?\n\nI don't think a new WG is needed with all the salient overhead ... but we\ndo need to take the task 'offline' to a reconsituted and more open\nsubgroup. What I think we primarily need is is an offer to host the\nmail list serving the group along with a list archive. (The largely\ndormant mail list used by the original state managment sub-group is\nactually run as a sendmail alias with heavy duty manual effort required \nas a result to add to the list.  No echo back to submitter, no archive,\netc.)\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: State managemen",
            "content": "David W. Morris wrote:\n> I don't think a new WG is needed with all the salient overhead ... but we\n> do need to take the task 'offline' to a reconsituted and more open\n> subgroup. What I think we primarily need is is an offer to host the\n> mail list serving the group along with a list archive. (The largely\n> dormant mail list used by the original state managment sub-group is\n> actually run as a sendmail alias with heavy duty manual effort required\n> as a result to add to the list.  No echo back to submitter, no archive,\n> etc.)\n\nI'm setting up a mailing list and will send a message to http-wg when\nit's available (within a day or two).\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Comments on PEP draf",
            "content": "Here are some brief comments on the latest PEP draft.\n\nMy understanding of this draft is that is only really usable to describe\nextensions to HTTP that involve the use of new headers to modify the\nsemantics of existing methods.  If you create an extension which defines\nnew methods, then PEP doesn't allow you to specify what those new methods\nare, nor any new response codes those methods may generate.\n\nIf my understanding is correct, then there should be some text in the draft\nexplaining the scope of PEP.  If my understanding is incorrect, then\nperhaps there should be an example showing how PEP can be used to describe\nan HTTP extension which uses a new method.\n\nSome nits:\n\nSection 5, first paragraph: BFN -> BNF\nAlso, \"field-name\" should be added to the list of productions used in the\nPEP draft and defined in RFC2068.\n\nSection 6, strength production: needs closing parenthesis\n\nSection 7, \"We call such a request for \"binding\".  Missing a noun after \"a\".\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "Michael,\n\nIt is putting it perhaps to strongly to say that the IETF is the\n\"Internet Engineering Task Force\" and not the \"InTRAnet Engineering Task\nForce\", but in general there's very little sympathy for the point of\nview that Internet Standards need not take into account the security\nconsiderations necessary for deployment across the Internet.\n\nThis is true in general, but seems even more applicable in this\nparticular case, where your proposal is motivated primarily by\na desire to close off a security problem in the first place.\n\n> I agree, a clever programmer will write a program.  This would only stop \n> beginners.\n\nWhat we've learned is that non-beginners share their hacks with\nbeginners so that everyone on alt.cool.hack.warez knows how\nto crack the system.\n\nIn short: \n\nthere's some sympathy that you've identified a 'problem', but\nyour 'proposal' to solve the problem doesn't do so effectively.\n\nIn lieu of a more effective proposal, I don't think we're going\nto make progress on this issue.\n\nRegards,\n\nLarry\n\n\n\n"
        },
        {
            "subject": "MIME multipart/* vs HTT",
            "content": "  On the Internet Printing Protocol list the following issues were\n  raised in the context of a discussion of whether or not IPP should\n  use HTTP/1.1 as a 'transport' protocol, defining IPP operations as\n  usage conventions and extentions.\n\n  I will attempt to respond based on my understanding; if anyone else\n  on http-wg disagrees, please correct or amplify.\n\nIPP> We discussed how MIME differs from the HTTP MIME-like\nIPP> protocol. There was a concern that the HTTP version of MIME\nIPP> doesn't support Content-Transfer-Encoding, though we think that\nIPP> we probably could add such an entity-header as an extension and\nIPP> support it through a CGI script if necessary (though we aren't\nIPP> sure about this).\n\n  Content-Transfer-Encoding is not supported by HTTP because it isn't\n  needed; the HTTP transport is 8 bit clean.\n\nIPP> There was also a question about how to send binary data in a\nIPP> multipart/mixed, especially in the chunked case because there is\nIPP> no way to know if a CRLF in the midst of binary data is really a\nIPP> CRLF. Thus it is hard to find the boundary string.\n\n  CRLF is what it is - if it is in the binary data and is followed by\n  the specified boundary string and another CRLF then you are at the\n  end of the body part.  What is the question?\n\n  As I understand it, the selection of a boundary string in MIME is\n  already 'probabilistic'; the sender is responsible for choosing a\n  string that 'probably' won't appear in the body (I do not claim to\n  be an authority on MIME).\n\nIPP> We believe that chunked applies to the entire multipart/mixed\nIPP> entity and cannot be used for one of the sub-entities alone.\nIPP> Thus there is no length to mark the boundary of a sub-entity.\n\n  Correct; the 'Transfer-Encoding: chunked' applies to all of the HTTP\n  message body.  For completeness, my companys' server implementation\n  does support chunked encoding of the entire multipart/* body part,\n  but we think it doesn't make much sense (because it is redundant) so\n  that support may be compiled out to save code.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "Scott Lawrence wrote:\n> [...]\n> IPP> There was also a question about how to send binary data in a\n> IPP> multipart/mixed, especially in the chunked case because there is\n> IPP> no way to know if a CRLF in the midst of binary data is really a\n> IPP> CRLF. Thus it is hard to find the boundary string.\n> \n>   CRLF is what it is - if it is in the binary data and is followed by\n>   the specified boundary string and another CRLF then you are at the\n>   end of the body part.  What is the question?\n> \n>   As I understand it, the selection of a boundary string in MIME is\n>   already 'probabilistic'; the sender is responsible for choosing a\n>   string that 'probably' won't appear in the body (I do not claim to\n>   be an authority on MIME).\n\nIn my ignorance of MIME, I've been puzzled about this boundary\nbusiness.  Assuming each multipart contains a Content-Length header,\ndoes it matter what the boundary is?  Can't the recipient just eat the\nnumber of content bytes before looking for the next boundary?  If so,\nthe boundary strings don't have to be particularly clever, do they?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": ">>>>> \"DK\" == Dave Kristol <dmk@bell-labs.com> writes:\n\nDK> In my ignorance of MIME, I've been puzzled about this boundary\nDK> business.  Assuming each multipart contains a Content-Length header,\nDK> does it matter what the boundary is?  Can't the recipient just eat the\nDK> number of content bytes before looking for the next boundary?  If so,\nDK> the boundary strings don't have to be particularly clever, do they?\n\n  I had also considered the use of Content-Length to address this; the\n  HTTP spec seems to allow it:\n\n2068> 19.4.5 HTTP Header Fields in Multipart Body-Parts\n\n2068>    In MIME, most header fields in multipart body-parts are generally\n2068>    ignored unless the field name begins with \"Content-\". In HTTP/1.1,\n2068>    multipart body-parts may contain any HTTP header fields which are\n2068>    significant to the meaning of that part.\n\n  So is it legal for me to put an HTTP/1.1 Content-Length header into\n  a multipart/* part to indicate its length?  It seems a workable (in\n  fact, preferable) solution.  As server vendors we can't assume that\n  though.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "> DK> In my ignorance of MIME, I've been puzzled about this boundary\n> DK> business.  Assuming each multipart contains a Content-Length header,\n> DK> does it matter what the boundary is?  Can't the recipient just eat the\n> DK> number of content bytes before looking for the next boundary?  If so,\n> DK> the boundary strings don't have to be particularly clever, do they?\n\n\nOK I'll admit it. Content-Length is not actually valid MIME. It was added to\nthe HTTP spec after I discovered a problem with the original POST and PUT\nmethod - the spec stated that closing the connection was used to indicate\nthe end of object which was kinda a lose on POST (think about it).\n\nSo I went off to talk it over with Tim and he agreed with me that we\nneeded a length specifier. We spent a while looking through the MIME specs\nlooking for it but didn't find it. Finally we punted and decided that it\nmust be in there and that the only logical name was \"Content-Length\".\nWe added it to the HTTP spec as a header incorporated from MIME.\n\nA while later we discovered that not only was there no MIME content length\nheader but its absence was an anathema to the MIME people. I had a long\nargument with Nat B. about this at WWW2. There was absolutely no way I\nwas going to corrupt the HTTP spec with stupid and unnecessary boundary\nmarkers. Introducing probabalistic fudges when there is an analytic\nsolution is something I dislike intensely. Besides searching for the\nboundary marker was very expensive computationally, every byte had to be\nexamnined.\n\nAgainst this the MIME argument was that you might want to gate HTTP to mail.\nThe idea that the gateway should handle the convbersion was not acceptable.\n\nI was keen to keep the HTTP spec clean because I wanted to support a \nhyperterminal like \"streaming mode\" where the server sent a continuously \nextending page at the client. This is what I believe the so called push\ntechnologies are doing although I don't regard that as \"true push\". For \nthat I would want to make the browser a client/server. To push content onto \nthe browser you would execute a PUT or POST to a URL specified by the \nbrowser.  There would have to be some hailing protocol but I don't think \nthat is too complex. If the job was done properly there would be no need\nfor all these quite bizare plugins to do chat.\n\nSo if people are wondering why the specs don't agree the answer is simple. \nI was right, they were persuing a theological point and I didn't really\nwant to call attention to the dispute at the time :-)\n\nMind you, those guys did win on Content-MD5 :-(\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "PH-B> OK I'll admit it. Content-Length is not actually valid MIME. It\nPH-B> was added to the HTTP spec after I discovered a problem with the\nPH-B> original POST and PUT method [...]\n\nPH-B> [...] There was absolutely no way I was going to corrupt the\nPH-B> HTTP spec with stupid and unnecessary boundary\nPH-B> markers. Introducing probabalistic fudges when there is an\nPH-B> analytic solution is something I dislike intensely. Besides\nPH-B> searching for the boundary marker was very expensive\nPH-B> computationally, every byte had to be examnined.\n\n  Hear hear.\n\n  If the intent is/was that Content-Length should be used in each part\n  of a multipart/* body part rather than boundary markers, then I\n  believe that the spec needs some clarification on this point.  While\n  I expect that others may disagree, I believe that such a change\n  would be a big improvement.\n\nPH-B> Against this the MIME argument was that you might want to gate\nPH-B> HTTP to mail.  The idea that the gateway should handle the\nPH-B> convbersion was not acceptable.\n\n  A gateway would also have to do work to change the HTTP 8-bit data\n  to some Content-Transfer-Encoding anyway, wouldn't it?  Adding a\n  boundary marker (or the Content-Length in the other direction) in\n  the process hardly seems an extraordinary requirement.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "I have copied this message from the http list to the mhtml list\n(HMTL in email, or more exactly documents taken from the web in\nemail) list, and suggest that you continue to cross-post to mhtml\nin this thread. They might be very interested, and have something\nto say from their viewpoint.\n\nRegards,Martin.\n\nOn Wed, 30 Apr 1997, Scott Lawrence wrote:\n\n> \n> PH-B> OK I'll admit it. Content-Length is not actually valid MIME. It\n> PH-B> was added to the HTTP spec after I discovered a problem with the\n> PH-B> original POST and PUT method [...]\n> \n> PH-B> [...] There was absolutely no way I was going to corrupt the\n> PH-B> HTTP spec with stupid and unnecessary boundary\n> PH-B> markers. Introducing probabalistic fudges when there is an\n> PH-B> analytic solution is something I dislike intensely. Besides\n> PH-B> searching for the boundary marker was very expensive\n> PH-B> computationally, every byte had to be examnined.\n> \n>   Hear hear.\n> \n>   If the intent is/was that Content-Length should be used in each part\n>   of a multipart/* body part rather than boundary markers, then I\n>   believe that the spec needs some clarification on this point.  While\n>   I expect that others may disagree, I believe that such a change\n>   would be a big improvement.\n> \n> PH-B> Against this the MIME argument was that you might want to gate\n> PH-B> HTTP to mail.  The idea that the gateway should handle the\n> PH-B> convbersion was not acceptable.\n> \n>   A gateway would also have to do work to change the HTTP 8-bit data\n>   to some Content-Transfer-Encoding anyway, wouldn't it?  Adding a\n>   boundary marker (or the Content-Length in the other direction) in\n>   the process hardly seems an extraordinary requirement.\n> \n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n> \n> \n\n\n----\nDr.sc.  Martin J. Du\"rst    ' , . p y f g c R l / =\nInstitut fu\"r Informatik     a o e U i D h T n S -\nder Universita\"t Zu\"rich      ; q j k x b m w v z\nWinterthurerstrasse  190     (the Dvorak keyboard)\nCH-8057   Zu\"rich-Irchel   Tel: +41 1 257 43 16\n S w i t z e r l a n d   Fax: +41 1 363 00 35   Email: mduerst@ifi.unizh.ch\n x \n----\n\n\n\n"
        },
        {
            "subject": "RE: MIME multipart/* vs HTT",
            "content": "-----Original Message-----\nFrom:Scott Lawrence [SMTP:lawrence@agranat.com]\nSent:Wednesday, April 30, 1997 11:35 AM\nTo:Hallam-Baker\nCc:http-wg@cuckoo.hpl.hp.com\nSubject:Re: MIME multipart/* vs HTTP\n\n\nPH-B> OK I'll admit it. Content-Length is not actually valid MIME. It\nPH-B> was added to the HTTP spec after I discovered a problem with the\nPH-B> original POST and PUT method [...]\n\nPH-B> [...] There was absolutely no way I was going to corrupt the\nPH-B> HTTP spec with stupid and unnecessary boundary\nPH-B> markers. Introducing probabalistic fudges when there is an\nPH-B> analytic solution is something I dislike intensely. Besides\nPH-B> searching for the boundary marker was very expensive\nPH-B> computationally, every byte had to be examnined.\n\n  Hear hear.\n\n  If the intent is/was that Content-Length should be used in each part\n  of a multipart/* body part rather than boundary markers, then I\n  believe that the spec needs some clarification on this point.  While\n  I expect that others may disagree, I believe that such a change\n  would be a big improvement.\n\nIt seems that this would be an improvement, but what about the case where you are not sure in advance what the Content-Length will be ? I also dislike the Multipart boundaries, but don't see another way around the problem if you don't know the content-length.\n\nPH-B> Against this the MIME argument was that you might want to gate\nPH-B> HTTP to mail.  The idea that the gateway should handle the\nPH-B> convbersion was not acceptable.\n\n  A gateway would also have to do work to change the HTTP 8-bit data\n  to some Content-Transfer-Encoding anyway, wouldn't it?  Adding a\n  boundary marker (or the Content-Length in the other direction) in\n  the process hardly seems an extraordinary requirement.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\nNick\n\n\n\n"
        },
        {
            "subject": "Re: IPP&gt;PRO  http comment",
            "content": "> From lawrence@agranat.com Wed Apr 30 11:51:49 1997\n \n>   My original reply seems to have started an interesting thread on the\n>   subject on the http-wg list; I'll let you know how it comes out, but\n>   the opinion that the HTTP Content-Length header may be used for each\n>   part of a multipart/* seems to be well supported.\n\n\nIt would be good to get a definitive answer as to whether\nContent-Length takes priority over the boundary string in a\nmultipart/*.  At a recent IETF meeting I asked such a question to some\nknowledgeable person who said that Content-Length was ignored in this\ncontext and that the boundary string was the only way to determine the\nend of a part in a multipart/*. I would prefer that Content-Length take\npriority if it is present.\n\n\n\n"
        },
        {
            "subject": "Re: IPP&gt; MIME multipart/* vs HTT",
            "content": "> From lawrence@agranat.com Wed Apr 30 12:05:41 1997\n> \n> IPP> We believe that chunked applies to the entire multipart/mixed\n> IPP> entity and cannot be used for one of the sub-entities alone.\n> IPP> Thus there is no length to mark the boundary of a sub-entity.\n> \n>   Correct; the 'Transfer-Encoding: chunked' applies to all of the HTTP\n>   message body.  For completeness, my companys' server implementation\n>   does support chunked encoding of the entire multipart/* body part,\n>   but we think it doesn't make much sense (because it is redundant) so\n>   that support may be compiled out to save code.\n> \n\nWhy is chunking redundant? It seems important for the case where the\nclient doesn't know the length of the data at the beginning of the\ntransmission.  Without the Transfer-Encoding of chunked, HTTP/1.1 seems\nto require a Content-Length for a client transmission (as a client, it\ncannot close the connection and multipart/byterange doesn't seem\nappropriate for clients to send).\n\n\n\n"
        },
        {
            "subject": "Re: IPP&gt; MIME multipart/* vs HTT",
            "content": "SDL> From lawrence@agranat.com Wed Apr 30 12:05:41 1997:\n\nSDL> Correct; the 'Transfer-Encoding: chunked' applies to all of the HTTP\nSDL> message body.  For completeness, my companys' server implementation\nSDL> does support chunked encoding of the entire multipart/* body part,\nSDL> but we think it doesn't make much sense (because it is redundant) so\nSDL> that support may be compiled out to save code.\n\n>>>>> \"BH\" == Robert Herriot <Robert.Herriot@Eng.Sun.COM> writes:\n\nBH> Why is chunking redundant? It seems important for the case where the\nBH> client doesn't know the length of the data at the beginning of the\nBH> transmission.  Without the Transfer-Encoding of chunked, HTTP/1.1 seems\nBH> to require a Content-Length for a client transmission (as a client, it\nBH> cannot close the connection and multipart/byterange doesn't seem\nBH> appropriate for clients to send).\n\n  Not in general!\n\n  Generating a multipart/* body (which has mechanisms already in it\n  for describing the length of each part), and then wrapping all that\n  in an HTTP 'Transfer-Encoding: chunked'.  It's not that we think it\n  won't work or even that it would never be the right thing, just that\n  in the application environments we are targetting it is too\n  expensive in code complexity to be supported by default (code size\n  is _very_ important to our customers; our 1.1 server can be under\n  20K).\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": ">   If the intent is/was that Content-Length should be used in each part\n>   of a multipart/* body part rather than boundary markers, then I\n>   believe that the spec needs some clarification on this point.  While\n>   I expect that others may disagree, I believe that such a change\n>   would be a big improvement.\n\nI think that the situation is now similar to that of Gopher where there\nwere also unhelpful demands. The Web is now the main engine for adoption\nof MIME and it is quite reasonable to expect changes in MIME to reflect\nHTTP usage at this point.\n\n\n> PH-B> Against this the MIME argument was that you might want to gate\n> PH-B> HTTP to mail.  The idea that the gateway should handle the\n> PH-B> convbersion was not acceptable.\n> \n>   A gateway would also have to do work to change the HTTP 8-bit data\n>   to some Content-Transfer-Encoding anyway, wouldn't it?  Adding a\n>   boundary marker (or the Content-Length in the other direction) in\n>   the process hardly seems an extraordinary requirement.\n\nI entirely agree. I never understood the boundary gateway argument at\nthe time. Conversations tended to go '\"but you have to encode the\ncharacter set because HTTP requires an 8 bit clean connection. In reply we\ngot a demand to cease using 8 bit clean connections and base 64 encode\nEVERY message.\n\nMIME should have a content-length header and it should specify a \nsubset for tunneling through backward compatible gateways.\n\nI would like to see a radical overhaul of SMTP that ignores the sendmail\nlegacy and explicitly considers the store and forward mail delivery\nparadigm. SMTP is designed to deliver from source to destination, the\nidea that mail gateways should gateway SMTP-SMTP is really not considered\nin the script.\n\nAs part of this overhaul I would like to see a specification for \"gold-class\"\nhandlers. These would not introduce arbitary crap like stripping to 7-bits,\ntruncating lines or any other stupid transformation. The only character \nsets permitted in this class would be ASCII and UNICODE. The protocol\nwould explicitly support transaction like semantics including giving a\nreliable notification of delivery. Mail delivered through the gold class \nnetwork would never be held in \"resend queues\" and never screw up mailing\nlists. It would support address books allowing them to acquire knowledge \nof the display abilities of mail users so the Word97 to Word95 user problem\nwould be avoided. Oh and all clients would support HTML.\n\nIn short it would work when a gold class object talked to another gold class\nobject. Gateways would be possible but the spec would not be corrupted to\nsupport a bunch of braindammaged bozos who use a mailler written by\nThomas Jefferson with help from Charles Babbage which can only cope with\nline lengths of 8 characters written in morse code.\n\n\nI'm sure that more effort goes into making the broken work than would be \nrequired to buiild somethin that really worked. All we need is some competent\nengineers, the big vendors and some people who are willing to insist on \nextreeme principle. \n\nPhill\n\n\n\n"
        },
        {
            "subject": "RE: New PEP draft available as ID",
            "content": "I was just sitting down to read through the 23 or so pages of the PEP\ndraft when I asked myself \"Why?\" My original interest in PEP was caused\nby my work in DAV where I needed to extend HTTP in all sorts of\ninteresting ways without breaking anything. In the end however, I\ndiscovered the whole effort was largely for not because PEP only works\nif everyone supports it and currently, no one supports it. This would be\nthe famous chicken and egg problem.\n\nHowever I did spend some time thinking about what the world would look\nlike if PEP did exist and the answer is \"expensive\". Having to send\nalong a bag with all those options, not to mention writing all the\nsupporting code, just seems like maximum overkill. The solution we are\nusing in DAV is to have clients look up a particular attribute to see if\na set of features are supported. Once you have attributes, PEP becomes\nlargely irrelevant.\n\nI realize that PEP is a lot more powerful than a simple \"check the\nswitch\" solution but in all the real world scenarios I am looking at,\nsuch as deploying DAV, I don't seem to need anything more than the\nattribute. Has someone else had a different experience?\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From:Henrik Frystyk Nielsen [SMTP:frystyk@w3.org]\n> Sent:Monday, April 28, 1997 8:24 AM\n> To:http-wg@cuckoo.hpl.hp.com\n> Subject:New PEP draft available as ID!\n> \n> \n> As a result of the discussions at the Memphis IETF meeting and WWW6, a\n> new\n> version of the PEP draft has been submitted as an ID. I have attached\n> the\n> draft to this mail.\n> \n> You can also find the draft as an W3C working draft at\n> \n> http://www.w3.org/pub/WWW/TR/WD-http-pep-970428.html\n> \n> Please read and comment!\n> \n> Thanks,\n> \n> Henrik\n>  << File: draft-ietf-http-pep-03.txt >>  << File: ATT46336.txt >> \n\n\n\n"
        },
        {
            "subject": "Re: New PEP draft available as ID",
            "content": "> I realize that PEP is a lot more powerful than a simple \"check the\n> switch\" solution but in all the real world scenarios I am looking at,\n> such as deploying DAV, I don't seem to need anything more than the\n> attribute. Has someone else had a different experience?\n\nWhile I agree with the particular points I don't agree with the conclusion.\n\nI believe that the problems with PEP could be solved by abandoning the\nidea of sending meta-protocol information along with the connection\ndata.\n\nInstead of doing this I would like to see a single tag to define the\nsemantics of any x-tended headers and how proxies should deal with them.\nThe tag would be of the form :-\n\nPEP-Extension: <version> <url>\n\nWhere version is the version of the PEP specification format and uri is\nthe location (yes it is a location in this case) where the specification\ncan be downloaded from.\n\nI understand that there are occasions in which this will not work such \nas intranets not connected to the Internet and intergalactic spacecraft.\nI think that attempting to address these corner cases by sending a\ncouple of hundred bytes of verbiage along with every message is a bit \nbroken and unlikely to fly in any case.\n\nPEP only specifies the grammar of the protocol, not the semantics. It\ncan only solve the problem of proxies and other intermediaries that need\nto know something about the tags being passed.\n\nI think the problem of extending the protocols has mainly been that until \nrecently there have not been clients that are extendable. NCSA Mosaic\nhas been a long time withering on the vine. I think that the new generation\nof clients may solve many of the percieved problems.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: New PEP draft available as ID",
            "content": "Phill writes:\n> I understand that there are occasions in which this will not work such\n> as intranets not connected to the Internet and intergalactic spacecraft.\n> I think that attempting to address these corner cases by sending a\n> couple of hundred bytes of verbiage along with every message is a bit\n> broken and unlikely to fly in any case.\n>\n\n\"Corner Cases\"?  Some of my best friends work with intergalactic\nspacecraft.  Not to mention intranets....\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "> IPP> There was also a question about how to send binary data in a\n> IPP> multipart/mixed, especially in the chunked case because there is\n> IPP> no way to know if a CRLF in the midst of binary data is really a\n> IPP> CRLF. Thus it is hard to find the boundary string.\n\nIt isn't hard to find the boundary string in binary data. Look\nfor CRLF followed by the boundary string followed by CRLF. It\ndoesn't matter whether a CRLF in the midst of binary data might\nactually be binary data, the robustness of multipart/* is based\non the fact that the boundary doesn't appear, not on the parsing\nof CRLFs.\n\n> Besides searching for the boundary marker was very expensive computationally, every byte had to be examnined.\n\nI think 'very' is pretty subjective, and using string matching\nalgorithms like boyer-moore mean that the number of comparisons\nis reduced for longer boundary strings.\n\n>   As I understand it, the selection of a boundary string in MIME is\n>   already 'probabilistic'; the sender is responsible for choosing a\n>   string that 'probably' won't appear in the body (I do not claim to\n>   be an authority on MIME).\n\nThe standard says only that the boundary string DOES NOT appear\nin the body. It happens that if you know nothing about the body\nat all, then you can implement this in a probabalistic way, e.g.,\nthe likelihood that a randomly generated boundary string would\nappear in arbitrary data could be made arbitrarily small (\"less\nthan the probability that the computer would spontaneously explode\").\n\nIn any case, I don't think we're going to change HTTP to suddenly\nrequire content-length on multipart boundaries; there may be some\nclarification needed to identify what's necessary for an interoperable\nimplementation.\n\n\n> I was keen to keep the HTTP spec clean ...\n\nWe've lost this particular battle a long time ago. I just want\nto keep the HTTP spec ambiguous and functional.\n\nI think we could do chunked multipart if we need to, but I don't think\nwe're going to be able to require senders to generate it, so it\nmay be that this whole discussion is just 'wishful thinking', or,\nto put it another way, part of the requirements setting for\nHTTP-NG.\n\n\n\n"
        },
        {
            "subject": "Re: IPP&gt;PRO  http comment",
            "content": "> It would be good to get a definitive answer as to whether\n> Content-Length takes priority over the boundary string in a\n> multipart/*.  At a recent IETF meeting I asked such a question to some\n> knowledgeable person who said that Content-Length was ignored in this\n> context and that the boundary string was the only way to determine the\n> end of a part in a multipart/*. I would prefer that Content-Length\n> take priority if it is present.\n\nMy take:\n\nIt is illegal to send content where the content-length and the boundary\nstring disagree. So one doesn't take priority over the other. A\nrecipient\nshould signal an error if it detects that they are different.\n\nSenders that are at all uncertain about the length of the data should\nomit content-length and rely on the boundary alone.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "RE: MIME multipart/* vs HTT",
            "content": "> In my ignorance of MIME, I've been puzzled about this boundary\n> business.  Assuming each multipart contains a Content-Length header,\n> does it matter what the boundary is?\n\nI agree, but let me say something about Content-Length.\n\nContent-Length is not a very efficient mechanism for high volume \n(tens of thousands of concurrent connections, at 200+ TPS) servers.\nIt requires that the server cache the entire message in order to \ndetermine the final length, then send the content.  This will generally\nrequire that the data is processed at least twice.  First to cache it,\nand again to send it.\n\nIt would be better if these protocols used a scheme that breaks the\nmessage into multiple messages.  Each message would start with\na length field.  In this way, it would be easy to add a boundry marker\nas defined by a zero length part at end.  Continuous streams of\nbytes are nice for some applications, but for high volume server\nactivity, it is much better to have a protocol that is more specific\nabout the length of the content, and does not require intermediate\ncache of content so you can calculate the length.\n\nI'm not that familiar with MIME, but I would expect that one could\ndefine a MIME type that does contain \"records\", each of which contains\na control header (length) and data (length bytes long).\n\nMichael Giroux\n\n\n\n"
        },
        {
            "subject": "Re: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "On Wed, 30 Apr 1997 21:35:06 -0700, Michael Giroux\n<mgiroux@worldnet.att.net> wrote:\n\n>offering alternatives.  As you indicated, I have identified a problem.\n>It would be nice to get some discussion on it.\n\nSeems to me the problem is a social one, not a technological one, so use a\nsocial solution.  Find a way to more strongly encourage end-users to\nlogout.  For instance, automatically send them warning emails if they\ntime-out instead of logout.  Flash big warning messages to them at next\nlogon.  Cut their pay.  Whatever.\n\n-----\nDaniel DuBois                                        ddubois@pobox.com\nSoftware Engineer                                       (415) 917-1126\n\n\n\n"
        },
        {
            "subject": "Re: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "On Thu, 1 May 1997, Daniel DuBois wrote:\n\n> On Wed, 30 Apr 1997 21:35:06 -0700, Michael Giroux\n> <mgiroux@worldnet.att.net> wrote:\n> \n> >offering alternatives.  As you indicated, I have identified a problem.\n> >It would be nice to get some discussion on it.\n> \n> Seems to me the problem is a social one, not a technological one, so use a\n> social solution.  Find a way to more strongly encourage end-users to\n> logout.  For instance, automatically send them warning emails if they\n> time-out instead of logout.  Flash big warning messages to them at next\n> logon.  Cut their pay.  Whatever.\n\nSurely you jest!  We have a technologically flawed protocol and you\npropose that a social solution be applied. The stateless nature of HTTP\nhas some positive implications and some negative implications. The\ncomplete lack of any potential for automatic detection of user disconnect\nis a major negative problem for folks building interactive applications on\ntop of HTTP.  I really don't see any security issue with a server\nregistering a disconnect URL with a user agent. I think a GET method\nshould be used to keep the protocol simple and stress the restriction on\nside-effects. If the URL can only be sent back to the server which\nregistered it and if the registration can only arrive with an explicit\nuser initiated transaction and not an indirect transaction. Furthermore,\ndefine the URL as having a 'don't care' response ... as in send on a best\neffort basis and close after the status is received or whatever but ignore\nthe response content, etc. Define some conditions which represent session\ntermination ... UA is exited, user backs up in the history list and starts\ndown a new path, perhaps the user types a new URL manually. Essentially,\na 'session' is over when the user can no longer click on links/buttons \nwithin the application context.\n\nMy perspective is that there is certainly a problem here. There are safe\ntechnological approaches for improving the situation but I'm not really\nmotivated to spend group time working on a solution given the large list\nof pressing issues. A good topic for HTTP-2-WG, in part because I would\nrather see a new method like DISCON to convey the logical disconnect and\nin part because this group is supposed to finish sometime soon.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: HTTP State Management Mechanism (Rev1): EndSession attribut",
            "content": "> I never tried to claim that I had the perfect solution.  My understanding\n> was that the http-wg would discuss issues and hopefully come up \n> with acceptable solutions. \n\nThe HTTP-WG is currently primarily discussing those issues that have\narisen\nin the interpretation of the HTTP/1.1 spec we delivered and the\nimplementation of HTTP/1.1, or those few issues which we decided to\nleave on our agenda.\n\nIt's an open mailing list, and of course people post interesting\nproblems\nand we discuss them a bit, but I feel some responsibility for cutting\noff discussion on \"new\" issues if we're not getting anywhere on them.\n\nWe're currently considering a new draft of PEP, a Protocol Extension\nProtocol.\nCould your session ending state be implemented with PEP?\n\n\n\n"
        },
        {
            "subject": "Re: MIME multipart/* vs HTT",
            "content": "> Content-Length is not a very efficient mechanism for high volume \n> (tens of thousands of concurrent connections, at 200+ TPS) servers.\n> It requires that the server cache the entire message in order to \n> determine the final length, then send the content.  This will generally\n> require that the data is processed at least twice.  First to cache it,\n> and again to send it.\n\nCorrect, and this is one of the main reasons why content-length\nmechanisms were not chosen for delimiting multipart boundaries\nin MIME messages.  The other reason was differences in the \nrepresentation of text messages on different platforms.\nHTTP doesn't have the latter problem to the same degree as MIME\nemail does, but it does have the former problem.  Content-Length\nis almost as undesirable for HTTP as it is for MIME email.\n\n(Not to mention that mixing the protocol-level framing in with \nthe description of the payload causes all sorts of confusion.)\n\n> It would be better if these protocols used a scheme that breaks the\n> message into multiple messages.  Each message would start with\n> a length field.  In this way, it would be easy to add a boundry marker\n> as defined by a zero length part at end. \n\nExactly the solution adopted for SMTP chunking.\n\nKeith\n\n\n\n"
        },
        {
            "subject": "Re: New feature tag registration drafts availabl",
            "content": "Graham Klyne:\n>\n> \n>At 10:43 PM 7/7/97 +0200, Koen Holtman wrote:\n>>\n>>I have made two new drafts about feature tag registration \n>>[...]\n>>\n>> draft-ietf-http-feature-scenarios-00.txt \n>>     `Feature Tag Scenarios' \n>>     Discusses why we need feature tag registration.\n> \n>Koen,\n> \n>Following a first read-through the feature secanarios draft, I have some\n>comments for you.\n\n\nGraham,\n\nThanks for your quick comments.  All the other ongoing HTTP issues\nprevent me from responding to these comments just as quickly, but here\nis a first batch of responses.\n\n>\n>I don't know how long you have been brewing your ideas in this area,\n\nActually, I thought up most of it over last weekend, though I have been\nthinking longer about some parts.  I think the scenarios draft in\nparticular is still a bit rough in some areas, and I intend to make\nanother pass over it before the Munich ID cutoff.\n\n> but\n>they are very close in concept to one aspect of the protocol-independent\n>negotiation framework that I have recently been suggesting.\n\nYes, that was the idea.  Note however that the drafts strictly limit\nthemselves to providing a namespace only, they do not define other\nmechanisms which could be commonly used, even though the existence of\nsuch mechanisms is conceivable.  \n\nTo make rapid progress on getting at least the namespace in place, I\nlimited the drafts to the namespace only.\n\n> \n> * Section 2.1:\n> \n>I like your characterization of the problem as a multidimensional search\n>process.  But, following a discussion I had the other day, I wonder if this\n>may be insufficient.\n> \n>The specific scenario was that of a device which contains a small LCD\n>display and a printer.  Such a device could reasonably be used for WWW\n>browsing, using the small display to display menu choices and the printer\n>to display content.\n> \n>One way to support this would be for content negotiation to say \"Accept:\n>(<low-resolution> AND <refreshable-display> AND <contains-hyperlinks>) OR\n>(<high-resolution> AND <hardcopy> AND <no-hyperlinks>)\".  This requires\n>that the various feature dimensions (as they have been portrayed in various\n>discussions to date) are not necessarily negotiated independently of each\n>other.\n\nYes, you often have negotiation in which dimensions are\ninterdependent, and the drafts do not exclude such cases (at least I\ndid not mean them to).\n\n>One approach I can conceive is to allow a feature-set to be used as a\n>feature for negotiation purposes, thus creating a recursively structured\n>feature-set space.  Allowing this allows the above scenario to fit within\n>the negotiation model you suggest if you allow compound 'dimensions'.\n\nI like to think of the namespace as unstructured: there is no\npredefined order in which negotiation on features has to be done.  I\nthink that any ordering in the namespace would exclude some legitimate\nuse cases in which the order would have to be just the other way\naround.  My idea is to leave ordering (and in general the question of\nhow to handle the presence or absence of interdependency between\ndimensions in any particular negotiation case) outside of the scope of\nthe registry.\n\nAnother thing which would be outside of the scope of the registry is\nthe distinction between the two cases `if the other party does not\nunderstand the meaning of this tag, it is safe to proceed with\nnegotiation' and `if the other party does not understand the meaning\nof this tag, it is *unsafe* to proceed with negotiation'.  Again, I\nthink that there are tags for which either one could be true depending\non the specific negotiation case.  So this distinction would have to\nbe conveyed with metadata, which is attached to the tag when it is\ntransmitted, rather than being encoded in its registration entry.  The\nmetadata format could either be general or bound to a specific\nnegotiation mechanism.\n\nI plan to make such in/out of scope distinctions more explicit in a\nrevision of the scenario draft.\n\n[...]\n>* Section 4.2:\n> \n>(a) I note that you are still adopting a very web-centric view in\n>notionally application-independent draft.\n\nI intended 4.2 be read as an example, not as an exhaustive list.  I\nsee I did not make that very clear in the title, though.\n\nMore later...\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Is 100-Continue hop-byhop",
            "content": "Yaron Goland:\n    >On 100 being hop by hop, I would also throw in the following scenario\n    >from DAV land:\n    >A client executes a COPY on a container with a large number of members.\n    >The user agent will want to be able to provide update information on how\n    >the copy is progressing rather than just sitting there for a few minutes\n    >while the procedure is underway. 100 continue responses are perfect for\n    >this scenario.\n    \nKoen:\n    Sorry, but 100 continue is _not_ perfect for this scenario.  There\n    is a message by Jeff in the archives which explains why.\n    Basically, a proxy which is multiplexing requests from multiple\n    clients over a single upstream connection would have no idea to\n    which client a 100 continue would have to be forwarded.\n    \nYou're presumably referring to my mail on \"head of line blocking\",\nwhich is a separate issue.  In particular, there I said \"proxies\nshould not multiplex requests from multiple clients\" because of\nthe H-O-L blocking problem, which can greatly limit performance.\n\nBut in any case, your concern is misplaced; a 100 response is\nno different from any other kind of response.  I.e., if a\nproxy *were* multiplexing requests from multiple clients (which\nit shouldn't, but suppose it did), then it would certainly\nhave to match up responses with requests.  This is just as\ntrue for 200 responses as it is for 100 responses.\n\nThe rule for HTTP persistent connections is that responses cannot be\nreordered.  That is, until the server sends the response for request N,\nit cannot send the response for request N+1 (or any request N+x, for x\n>= 1).  Another way of looking at this is that for a given transport\nconnection, although more than one request or response may be\npipelined, they cannot be parallelized.\n\nThe \"100\" status code complicates this, because it allows the\nsender to send more than one response for a request.  Fine; just\nchange the rule to say\nuntil the server sends the LAST response for request N, it\ncannot send ANY response for request N+1 (or any request N+x,\nfor x >= 1).\n\nSimple, foolproof, and not in any way a change from current practice\n(given that some people have apparently implement 100 already).\n\n    1.1 does not offer an end-to-end event notification service, nor\n    can 100 be easily `fixed' to produce such a service.  Adding such a\n    service is out of scope for this WG I think.  IF DAV needs\n    something, I suggest that you either document Netscape server push\n    and use that, or spec a mechanism in which the client makes\n    occasional status requests.\n\nHTTP/1.1 already includes \"100 (Continue)\", and (as our \"Summary of\nSTATUS100 issue\" makes clear) there is exactly one problem that this\nstatus code actually solves: allowing the client to defer transmission\nof the request body until the server has indicated a willingness to\naccept it.\n\nThe working group consensus seems to be that this is a useful\nfeature (and I'm on the \"reluctantly agree\" side of this myself).\nOn the other hand, I think everyone agrees that the current text\nin RFC2068 is misleading and perhaps wrong.  The scope of our\ncurrent effort is to \"fix the bugs in RFC2068\", not to re-argue\nthe basic design features.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>Yaron Goland:\n>>\n>[...]\n>>BTW I find it strange that we are pushing a draft to proposed standard\n>>when no one has implemented it and, in so far as I am aware, is even\n>>planning on implementing it.\n>\n>A proposed standard does not require existing or planned\n>implementations.\n>\n>I am not aware of anybody planning to implement hit metering either,\n>and that is going to be a proposed standard too.\n>\n>For a proposed standard, the only thing required is that the WG thinks\n>it is a good idea.  If you were to argue this WG as a whole has no\n>consensus on state-man-mec being a good idea, you would have a very\n>valid point.\n>\n>Though I have no objections to this thing being submitted as a\n>proposed standard, I think that submission as `an experimental RFC\n>which supersedes 2109' would better reflect the WG status.\n>\n>Not submitting it at all would mean not fixing the bugs in 2109, which\n>is not an acceptable option as far as I am concerned.\n\nThe revision has been implemented in Lynx, and is undergoing\nfield testing, with no problems so far.  Most of the changes relative\nto the current RFC where made to accomodate MSIE, and the draft does\nso, fully now, within the context of valid concerns about user privacy\nand not just those of market forces.  Just as no one should expect\nto be believed simply on the basis of where he is employed, rather\nthan on the basis of the credibility he establishes over time, no one\nshould assume to have a crystal ball concerning what will or will not\nbe implemented.\n\nI agree that it should be moved on to Experimental RFC which\nsupersedes RFC 2109 (and that most of the time most of the options\nto disable this or that aspect of cookie handling should be used,\njust as BLINK should be disabled :).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "That is not accurate, Microsoft is planning on implementing hit meeting.\nI also know that AOL and Netscape have expressed interest although, in\nso far as I am aware, neither party has committed to an implementation.\n\nAs for RFC 2109, what happens to a protocol that is broken and that no\none intends to implement? The thing should just be declared null and\nvoid and the whole situation forgotten about.\n\nYaron\n\n> -----Original Message-----\n> From:koen@win.tue.nl [SMTP:koen@win.tue.nl]\n> Sent:Thursday, July 10, 1997 1:08 PM\n> To:Yaron Goland\n> Cc:masinter@parc.xerox.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> to Propo\n> \n> Yaron Goland:\n> >\n> [...]\n> >BTW I find it strange that we are pushing a draft to proposed\n> standard\n> >when no one has implemented it and, in so far as I am aware, is even\n> >planning on implementing it.\n> \n> A proposed standard does not require existing or planned\n> implementations.\n> \n> I am not aware of anybody planning to implement hit metering either,\n> and that is going to be a proposed standard too.\n> \n> For a proposed standard, the only thing required is that the WG thinks\n> it is a good idea.  If you were to argue this WG as a whole has no\n> consensus on state-man-mec being a good idea, you would have a very\n> valid point.\n> \n> Though I have no objections to this thing being submitted as a\n> proposed standard, I think that submission as `an experimental RFC\n> which supersedes 2109' would better reflect the WG status.\n> \n> Not submitting it at all would mean not fixing the bugs in 2109, which\n> is not an acceptable option as far as I am concerned.\n> \n> >Yaron\n> \n> Koen.\n\n\n\n"
        },
        {
            "subject": "RE: Is 100-Continue hop-byhop",
            "content": "Are proxies not already prevented from safely multiplexing streams\nbecause of cookies, authentication, and OPS?\n\nIf client A sends up a cookie to http://foo and client B sends up a\ncookie to http://foo there is no way, just looking at the response, to\nknow which of the two responses the server generates is meant for which\nclient. \n\nA similar situation applies to authentication where client A and client\nB may both perform a GET on a resource with authentication information\nand one client makes it and the other doesn't, given that custom\nauthentication mechanisms can be used, there is no way for the proxy to\nfigure out which response was intended for which client.\n\nA similar situation also applies to OPS. Two clients may establish an\nOPS session with a server and then make separate requests for the same\nresource. The response may be different based upon the OPS information\nbut there is no way to examine the response and know which client that\nparticular response is meant for.\n\nOf course, its late and maybe I missed something. But it looks to me\nlike proxy multiplexing is already impossible.\n\nYaron\n\n\n> -----Original Message-----\n> From:koen@win.tue.nl [SMTP:koen@win.tue.nl]\n> Sent:Thursday, July 10, 1997 1:16 PM\n> To:Yaron Goland\n> Cc:mogul@pa.dec.com; dwm@xpasc.com; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Is 100-Continue hop-by-hop?\n> \n> Yaron Goland:\n> >\n> >On 100 being hop by hop, I would also throw in the following scenario\n> >from DAV land:\n> >A client executes a COPY on a container with a large number of\n> members.\n> >The user agent will want to be able to provide update information on\n> how\n> >the copy is progressing rather than just sitting there for a few\n> minutes\n> >while the procedure is underway. 100 continue responses are perfect\n> for\n> >this scenario.\n> \n> Sorry, but 100 continue is _not_ perfect for this scenario.  There is\n> a message by Jeff in the archives which explains why.  Basically, a\n> proxy which is multiplexing requests from multiple clients over a\n> single upstream connection would have no idea to which client a 100\n> continue would have to be forwarded.\n> \n> 1.1 does not offer an end-to-end event notification service, nor can\n> 100 be easily `fixed' to produce such a service.  Adding such a\n> service is out of scope for this WG I think.  IF DAV needs something,\n> I suggest that you either document Netscape server push and use that,\n> or spec a mechanism in which the client makes occasional status\n> requests.\n> \n> >Yaron\n> \n> Koen.\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Propo sed Standar",
            "content": "Declare it historic and forget about it. As for the new draft, no one\nseems interested in implementing it so why put an RFC on proposed track\nwhen it will never make it to draft status?\n\nYaron\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Thursday, July 10, 1997 9:25 AM\n> To:Yaron Goland\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> to Proposed Standard\n> \n> You have raised some (apparently new) objections to\n> moving \"HTTP State Management Mechanism (Rev1)\" to\n> Proposed Standard.\n> \n> On the other hand, the working group has previously\n> issued RFC 2109, a Proposed Standard, which has serious\n> interoperability problems with currently deployed\n> software. (I assume you're familiar with this software).\n> \n> So what do you recommend that we do? It seems intolerable\n> to have a Proposed Standard that we wouldn't actually\n> want to propose that people implement, and we should\n> move on this.\n> \n> Withdraw 2109 (mark it Historical?) Document current\n> practice for cookies?\n> \n> (If we proceed with this document, we should deal\n> with Yaron's comments on 4.2.2.)\n> \n> Larry\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Yaron Goland:\n>\n>That is not accurate, Microsoft is planning on implementing hit meeting.\n>I also know that AOL and Netscape have expressed interest although, in\n>so far as I am aware, neither party has committed to an implementation.\n\nAh, thanks for the information!  This is the first time I hear about\nany vendor planning to implement hit metering.\n\n>As for RFC 2109, what happens to a protocol that is broken and that no\n>one intends to implement?\n\nIt is not broken (at least less so than Netscape cookies, which are\nthe current state of the art), and we have at least one implementer\nwho intends to implement.\n\n>Yaron\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Is 100-Continue hop-byhop",
            "content": "Jeffrey Mogul:\n>\n>Yaron Goland:\n>    >On 100 being hop by hop, I would also throw in the following scenario\n>    >from DAV land:\n>    >A client executes a COPY on a container with a large number of members.\n>    >The user agent will want to be able to provide update information on how\n>    >the copy is progressing rather than just sitting there for a few minutes\n>    >while the procedure is underway. 100 continue responses are perfect for\n>    >this scenario.\n>    \n>Koen:\n>    Sorry, but 100 continue is _not_ perfect for this scenario.  There\n>    is a message by Jeff in the archives which explains why.\n>    Basically, a proxy which is multiplexing requests from multiple\n>    clients over a single upstream connection would have no idea to\n>    which client a 100 continue would have to be forwarded.\n>    \n>You're presumably referring to my mail on \"head of line blocking\",\n>which is a separate issue.\n\nNo, think I was referring to\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0134.html (Re:\n1xx Clarification, which was written a few weeks ago).\n\nAnyway, I think I interpreted Yaron's message wrong.  I thought he was\narguing the case\n\n  <copy request> <200 OK response to copy request> <100 response about\n  progress=50%> <100 response about progress=100%>\n\nin which the progress events are sent *after* the OK response which\nends the transaction.\n\nBut I guess that what he actually had in mind was \n\n  <copy request> <100 response about progress=50%> <100 response about\n  progress=100%> <200 OK response to copy request>\n\nAnd this latter case would indeed work through a multiplexing proxy\n(though the code would have to be 101 or something, with semantics\nslightly different from 100). \n\nSo I believe I was wrong, and that Yaron has a valid case for keeping\nthe 1xx codes.\n\nSorry for all the confusion.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: New feature tag registration drafts availabl",
            "content": "At 11:05 PM 7/10/97 +0200, Koen Holtman wrote:\n\n[...]\n>To make rapid progress on getting at least the namespace in place, I\n>limited the drafts to the namespace only.\n\nFine.  (And I like the hierarchical namespace approach.)\n\n>> * Section 2.1:\n>> \n>>I like your characterization of the problem as a multidimensional search\n>>process.  But, following a discussion I had the other day, I wonder if this\n>>may be insufficient.\n>\n>Yes, you often have negotiation in which dimensions are\n>interdependent, and the drafts do not exclude such cases (at least I\n>did not mean them to).\n\nOK.  (I think it's the term 'dimension' -- it tends to imply a scalar\nassociated value.)\n\n>I like to think of the namespace as unstructured: there is no\n>predefined order in which negotiation on features has to be done.\n\nAnother draft to cover a generic negotiation framework?  (Not to specify an\norder, but partly to make it clear that there is no predefined order.)\n\n>Another thing which would be outside of the scope of the registry is\n>the distinction between the two cases `if the other party does not\n>understand the meaning of this tag, it is safe to proceed with\n>negotiation' and `if the other party does not understand the meaning\n>of this tag, it is *unsafe* to proceed with negotiation'.  Again, I\n>think that there are tags for which either one could be true depending\n>on the specific negotiation case.  So this distinction would have to\n>be conveyed with metadata, which is attached to the tag when it is\n>transmitted, rather than being encoded in its registration entry.  The\n>metadata format could either be general or bound to a specific\n>negotiation mechanism.\n\nHmmm... I'm not sure one could construct a sufficiently general metadata\nfraework either.  I had assumed that such detail would be bound up in the\nsemantics of a particular tag:  e.g. any component which new about a tag\nwould know what kind of response (if any) would be needed to conclude\nnegotiation w.r.t. that tag.\n\n>I plan to make such in/out of scope distinctions more explicit in a\n>revision of the scenario draft.\n\nI think it was fairly clear that details of values associated with the tags\nwere not part of the feature tag.  But I felt there were implicit\nassumptions about the range and complexity of values which might be allowed.\n\n>[...]\n>>* Section 4.2:\n>> \n>>(a) I note that you are still adopting a very web-centric view in\n>>notionally application-independent draft.\n>\n>I intended 4.2 be read as an example, not as an exhaustive list.  I\n>see I did not make that very clear in the title, though.\n\nThat comment of mine was slightly tongue-in-cheek.  And I just wanted to\nraise a flag concerning the possibility of Web-centric thinking allowing\nassumptions to creep in.\n\nIt was clear from what you wrote that it was just an example.  \n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Proposed Standar",
            "content": "At 06:27 PM 7/10/97 -0700, Yaron Goland wrote:\n>Declare it historic and forget about it. As for the new draft, no one\n>seems interested in implementing it so why put an RFC on proposed track\n>when it will never make it to draft status?\n\nWe need a consistent spec for interoperable Cookies. The UAs and servers\nimplement them, and many applications rely on them. OPS is (will be) good\nfor profiles, but the majority of our state info is application\ninfrastructure, equally applicable to anonymous and ID'd users. Don't throw\nthe interop baby out with the bathwater.\n\n\n>Yaron\n>\n>> -----Original Message-----\n>> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n>> Sent:Thursday, July 10, 1997 9:25 AM\n>> To:Yaron Goland\n>> Cc:http-wg@cuckoo.hpl.hp.com\n>> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n>> to Proposed Standard\n>> \n>> You have raised some (apparently new) objections to\n>> moving \"HTTP State Management Mechanism (Rev1)\" to\n>> Proposed Standard.\n>> \n>> On the other hand, the working group has previously\n>> issued RFC 2109, a Proposed Standard, which has serious\n>> interoperability problems with currently deployed\n>> software. (I assume you're familiar with this software).\n>> \n>> So what do you recommend that we do? It seems intolerable\n>> to have a Proposed Standard that we wouldn't actually\n>> want to propose that people implement, and we should\n>> move on this.\n>> \n>> Withdraw 2109 (mark it Historical?) Document current\n>> practice for cookies?\n>> \n>> (If we proceed with this document, we should deal\n>> with Yaron's comments on 4.2.2.)\n>> \n>> Larry\n>\n>\n>\n--\nMatthew Rubenstein                    North American Media Engines\nToronto, Ontario   *finger matt for public key*      (416)943-1010\n\n    Chess is for computers.\n\n\n\n"
        },
        {
            "subject": "cc:Mail Link to SMTP Undeliverable Messag",
            "content": "Message is undeliverable.\nReason: A platform specific error occurred while writing to a file.\nLikely disk full.\nOriginal text follows:\n---------------------\n\n\nReceived: from sb2inet2.dowjones.com by ccmail.dowjones.com (ccMail Link to SMTP R8.00.00)\n; Fri, 11 Jul 97 09:26:19 -0500\nReturn-Path: <http-wg-request@cuckoo.hpl.hp.com>\nReceived: by sb2inet2.dowjones.com; id JAA02958; Fri, 11 Jul 1997 09:20:43 -0400\nReceived: from palrel3.hp.com(156.153.255.219) by sb2inet2.sb2inet2.dowjones.com via smap (3.2)\nid xma002944; Fri, 11 Jul 97 09:20:41 -0400\nReceived: from cuckoo.hpl.hp.com (cuckoo.hpl.hp.com [15.144.62.116])\nby palrel3.hp.com (8.8.5/8.8.5) with ESMTP id GAA28155;\nFri, 11 Jul 1997 06:24:15 -0700 (PDT)\nReceived: (from procmail@localhost) by cuckoo.hpl.hp.com (8.7.1/8.7.1) id JAA22052; Fri, 11 Jul 1997 09:23:30 -0400 (EDT)\nResent-Date: Fri, 11 Jul 1997 09:23:30 -0400 (EDT)\nMessage-Id: <3.0.1.32.19970711091753.00948460@name.net>\nX-Sender: matt@name.net\nX-Mailer: Windows Eudora Pro Version 3.0.1 (32)\nDate: Fri, 11 Jul 1997 09:17:53 -0400\nTo: Yaron Goland <yarong@microsoft.com>\nFrom: Matthew Rubenstein <ruby@name.net>\nSubject: RE: LAST CALL, \"HTTP State Management Mechanism (Rev1) \" to\n  Proposed Standard\nCc: \"'Larry Masinter'\" <masinter@parc.xerox.com>, http-wg@cuckoo.hpl.hp.com\nIn-Reply-To: <11352BDEEB92CF119F3F00805F14F48503187B2E@RED-44-MSG.dns.mi\n crosoft.com>\nMime-Version: 1.0\nContent-Type: text/plain; charset=\"us-ascii\"\nResent-Message-ID: <\"FD5MC.0.PO5.AFZnp\"@cuckoo>\nResent-From: http-wg@cuckoo.hpl.hp.com\nX-Mailing-List: <http-wg@cuckoo.hpl.hp.com> archive/latest/3737\nX-Loop: http-wg@cuckoo.hpl.hp.com\nPrecedence: list\nResent-Sender: http-wg-request@cuckoo.hpl.hp.com\n\nAt 06:27 PM 7/10/97 -0700, Yaron Goland wrote:\n>Declare it historic and forget about it. As for the new draft, no one\n>seems interested in implementing it so why put an RFC on proposed track\n>when it will never make it to draft status?\n\nWe need a consistent spec for interoperable Cookies. The UAs and servers\nimplement them, and many applications rely on them. OPS is (will be) good\nfor profiles, but the majority of our state info is application\ninfrastructure, equally applicable to anonymous and ID'd users. Don't throw\nthe interop baby out with the bathwater.\n\n\n>Yaron\n>\n>> -----Original Message-----\n>> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n>> Sent:Thursday, July 10, 1997 9:25 AM\n>> To:Yaron Goland\n>> Cc:http-wg@cuckoo.hpl.hp.com\n>> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n>> to Proposed Standard\n>> \n>> You have raised some (apparently new) objections to\n>> moving \"HTTP State Management Mechanism (Rev1)\" to\n>> Proposed Standard.\n>> \n>> On the other hand, the working group has previously\n>> issued RFC 2109, a Proposed Standard, which has serious\n>> interoperability problems with currently deployed\n>> software. (I assume you're familiar with this software).\n>> \n>> So what do you recommend that we do? It seems intolerable\n>> to have a Proposed Standard that we wouldn't actually\n>> want to propose that people implement, and we should\n>> move on this.\n>> \n>> Withdraw 2109 (mark it Historical?) Document current\n>> practice for cookies?\n>> \n>> (If we proceed with this document, we should deal\n>> with Yaron's comments on 4.2.2.)\n>> \n>> Larry\n>\n>\n>\n--\nMatthew Rubenstein                    North American Media Engines\nToronto, Ontario   *finger matt for public key*      (416)943-1010\n\n    Chess is for computers.\n\n\n\n"
        },
        {
            "subject": "TSPECIALS: another ni",
            "content": "Sorry to raise this while you're trying to close down issues for the Munich\nIETF meeting, BUT I notice that RFC 2045 and RFC 2068 contain conflicting\ndefinitions of syntax production 'tspecials'.\n\nRFC 2045, section 5.1, p12\nRFC 2068, section 2.2, p16\n\nSections 3.7.2 and 16 of RFC 2068 incorporate RFC 2045 syntax productions\nby reference, so not only is this a potential source of confusion but I\nthink it is technically ambiguous.\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Propo sed Standar",
            "content": "So write up a \"current practices\" RFC and use it obsolete RFC 2109.\nYaron\n\n> -----Original Message-----\n> From:Matthew Rubenstein [SMTP:ruby@name.net]\n> Sent:Friday, July 11, 1997 6:18 AM\n> To:Yaron Goland\n> Cc:'Larry Masinter'; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> to Proposed Standard\n> \n> At 06:27 PM 7/10/97 -0700, Yaron Goland wrote:\n> >Declare it historic and forget about it. As for the new draft, no one\n> >seems interested in implementing it so why put an RFC on proposed\n> track\n> >when it will never make it to draft status?\n> \n> We need a consistent spec for interoperable Cookies. The UAs and\n> servers\n> implement them, and many applications rely on them. OPS is (will be)\n> good\n> for profiles, but the majority of our state info is application\n> infrastructure, equally applicable to anonymous and ID'd users. Don't\n> throw\n> the interop baby out with the bathwater.\n> \n> \n> >Yaron\n> >\n> >> -----Original Message-----\n> >> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> >> Sent:Thursday, July 10, 1997 9:25 AM\n> >> To:Yaron Goland\n> >> Cc:http-wg@cuckoo.hpl.hp.com\n> >> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> >> to Proposed Standard\n> >> \n> >> You have raised some (apparently new) objections to\n> >> moving \"HTTP State Management Mechanism (Rev1)\" to\n> >> Proposed Standard.\n> >> \n> >> On the other hand, the working group has previously\n> >> issued RFC 2109, a Proposed Standard, which has serious\n> >> interoperability problems with currently deployed\n> >> software. (I assume you're familiar with this software).\n> >> \n> >> So what do you recommend that we do? It seems intolerable\n> >> to have a Proposed Standard that we wouldn't actually\n> >> want to propose that people implement, and we should\n> >> move on this.\n> >> \n> >> Withdraw 2109 (mark it Historical?) Document current\n> >> practice for cookies?\n> >> \n> >> (If we proceed with this document, we should deal\n> >> with Yaron's comments on 4.2.2.)\n> >> \n> >> Larry\n> >\n> >\n> >\n> --\n> Matthew Rubenstein                    North American Media Engines\n> Toronto, Ontario   *finger matt for public key*      (416)943-1010\n> \n>     Chess is for computers.\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Who intends to implement it?\nYaron\n\n> -----Original Message-----\n> From:koen@win.tue.nl [SMTP:koen@win.tue.nl]\n> Sent:Friday, July 11, 1997 12:17 AM\n> To:Yaron Goland\n> Cc:koen@win.tue.nl; masinter@parc.xerox.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> to Propo\n> \n> Yaron Goland:\n> >\n> >That is not accurate, Microsoft is planning on implementing hit\n> meeting.\n> >I also know that AOL and Netscape have expressed interest although,\n> in\n> >so far as I am aware, neither party has committed to an\n> implementation.\n> \n> Ah, thanks for the information!  This is the first time I hear about\n> any vendor planning to implement hit metering.\n> \n> >As for RFC 2109, what happens to a protocol that is broken and that\n> no\n> >one intends to implement?\n> \n> It is not broken (at least less so than Netscape cookies, which are\n> the current state of the art), and we have at least one implementer\n> who intends to implement.\n> \n> >Yaron\n> \n> Koen.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Yaron Goland:\n>\n>Who intends to implement it?\n\nThe lynx people.  See the recent message by Foteos Macrides in this\nthread.\n\n>Yaron\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Yaron Goland asked:\n> >\n> >Who intends to implement it?\n\nKoen Holtman replied:\n> The lynx people.  See the recent message by Foteos Macrides in this\n> thread.\n>\n\nI am also aware of several efforts to use this mechanism\nin the context of lightweight no-human user agents.  One\nof these, being done by a different group within NASA, will\nbe using the lightweight state information available through\ncookies to maintain information about what image data sets\nhave been received from and passed to different reporting\nstations.  Having cookies available lets the interacting\nservers know that which sets are complete without having\nto query on each image in each sets; it makes for a nice\ncheap short cut.\n\nI believe that cookies are in widespread use in passing\njust this kind of shortcut information. Repairing the existing\nState Management Mechanism is important for interoperability;\nwe want to make sure everyone is using the same standard\nto create this kind of application. Even if those applications\nwould work fine with the original Netscape docs, having a standards\ntrack doc is important.\n\nIt is being Proposed; if it does not receive adequate implementation\nsupport, it will not move on.  That's okay.  But we shouldn't short\ncircuit the process by assuming that it's dead because two vendors\nwon't be updating to it.  The browser market is not the same as the\nuseful field of play for internet protocols.\n\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "I am all in favor of an information RFC documenting how cookies are\nimplemented today, which covers your concerns Ted.\n\nYaron\n\n> -----Original Message-----\n> From:hardie@thornhill.arc.nasa.gov\n> [SMTP:hardie@thornhill.arc.nasa.gov]\n> Sent:Friday, July 11, 1997 1:25 PM\n> To:koen@win.tue.nl; Yaron Goland; http-wg@cuckoo.hpl.hp.com\n> Cc:masinter@parc.xerox.com\n> Subject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \"\n> to Propo\n> \n> Yaron Goland asked:\n> > >\n> > >Who intends to implement it?\n> \n> Koen Holtman replied:\n> > The lynx people.  See the recent message by Foteos Macrides in this\n> > thread.\n> >\n> \n> I am also aware of several efforts to use this mechanism\n> in the context of lightweight no-human user agents.  One\n> of these, being done by a different group within NASA, will\n> be using the lightweight state information available through\n> cookies to maintain information about what image data sets\n> have been received from and passed to different reporting\n> stations.  Having cookies available lets the interacting\n> servers know that which sets are complete without having\n> to query on each image in each sets; it makes for a nice\n> cheap short cut.\n> \n> I believe that cookies are in widespread use in passing\n> just this kind of shortcut information. Repairing the existing\n> State Management Mechanism is important for interoperability;\n> we want to make sure everyone is using the same standard\n> to create this kind of application. Even if those applications\n> would work fine with the original Netscape docs, having a standards\n> track doc is important.\n> \n> It is being Proposed; if it does not receive adequate implementation\n> support, it will not move on.  That's okay.  But we shouldn't short\n> circuit the process by assuming that it's dead because two vendors\n> won't be updating to it.  The browser market is not the same as the\n> useful field of play for internet protocols.\n> \n> regards,\n> Ted Hardie\n> NASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: Is 100-Continue hop-byhop",
            "content": ">But I guess that what he actually had in mind was \n>\n>  <copy request> <100 response about progress=50%> <100 response about\n>  progress=100%> <200 OK response to copy request>\n>\n>And this latter case would indeed work through a multiplexing proxy\n>(though the code would have to be 101 or something, with semantics\n>slightly different from 100). \n\n101 is already being used, but yes it would definitely have to be\na new 1xx code.  Extensibility is meant to be used.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Propo sed Standar",
            "content": "Yaron Goland wrote:\n> \n> Oops sorry, things do tend to fall between the cracks.\n\n> Comments:\n> 4.2.2 - \"If an attribute appears more than once in a cookie, the\n> behavior is undefined.\" Undefined things have a habit of defining\n> themselves, let us not repeat the mistakes which caused so much trouble\n> with cookies in the first place. If an attribute appears more than once\n> then the first appearance defines the value and subsequent attributes\n> are ignored.\n\nMy apologies.  I did not willfully fail to incorporate these comments,\nbut did so through oversight.\n\nOthers have addressed Yaron's other remarks and follow-ups.\n\nI will be on vacation for another week, so I clearly won't have a new\ndraft ready before the 7/15 deadline, but I will submit one after I\nreturn home.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1)&quot; to Prop",
            "content": "Dave Kristol <dmk@bell-labs.com> wrote:\n>Yaron Goland wrote:\n>> \n>> Oops sorry, things do tend to fall between the cracks.\n>\n>> Comments:\n>> 4.2.2 - \"If an attribute appears more than once in a cookie, the\n>> behavior is undefined.\" Undefined things have a habit of defining\n>> themselves, let us not repeat the mistakes which caused so much trouble\n>> with cookies in the first place. If an attribute appears more than once\n>> then the first appearance defines the value and subsequent attributes\n>> are ignored.\n>\n>My apologies.  I did not willfully fail to incorporate these comments,\n>but did so through oversight.\n>\n>Others have addressed Yaron's other remarks and follow-ups.\n>\n>I will be on vacation for another week, so I clearly won't have a new\n>draft ready before the 7/15 deadline, but I will submit one after I\n>return home.\n\nIf you're going to be submitting another draft, I suggest that\nin the section explaining the port attribute you include an explicit\nstatement that its value should be double-quoted if it's a comma\nseparated list of ports.  It was not difficult to do \"sanity checks\"\nfor whether what follows a comma is another port number versus the\nstart of another cookie if the value is not double-quoted, but it would\nbe better to promote the double-quoting explicitly, particularly because\nin \"historical\" Set-Cookie headers the expires attribute value, which\nincludes commas, is not double-quoted (for backwardness compatibility).\n\nYaron's other criticism of 4.2.2, regarding the mushiness of\nthe \"at least as secure\" phrase, is the same one I raised some time\nago, and you answered at length, so Yaron (and Larry as well if he's\nforgotten) can find that in the archives.  However, now that the\nblanket port restriction has been lifted, and cookies can be shared\nbetween https and http servers unless the secure attribute was\nspecified, it might be worth indicating that UAs can offer human users\nthe option to set cookies as secure in addition to taking any \"advice\"\nabout that from the server's Set-Cookie/Set-Cookie2 header(s).  That's\njust an \"implementation issue\", so it's OK if you'd rather not, but we\ndid make that both a configuration and run-time option in Lynx.\n\nThese are just comment/explanation suggestions.  The actual\nspecs are fine.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "I have a number of remaining concerns with the HTTP State Management\nMechanism draft-02. They range from editorial and clarity to\nfunctional. I'll sumarize the functional concerns and then step\nthru the draft with detailed comments and/or suggestions.\n\nFunctional Concerns:\n\n  1.  The cookie/cookie2 interoperational requirement that clients\n      clients merge the values from two cookie/cookie2 and servers\n      split the values present unnecessary implementation complexity\n      and opportunity for errors to conver a transition interval\n      and to save small amount of network traffic. More below.\n\n  2.  I see no advantage to requiring the use of a FQHN in all\n      cases. It is sufficient to exactly match the host name\n      between URL's response which set the cookie and the URL\n      candidate for inclusion of the cookie. Intranet applications\n      work quite nicely with host names which are not FQHNs.\n      Furthermore, in the intranet context this has the potential\n      of requiring a user to type a FQHN to access an application\n      which uses Cookies. An unnecessary restriction. We\n      have already concluded in discussions relationg to 2068\n      and the Host: header that it is impossible to require a UA\n      to expand a simple host name into a FQHN.\n\n  3.  Requiring the leading period in the domain= value can be\n      avoided by simply requiring that the client add the period\n      if it isn't provided prior to performing the domain matching.\n\n  4.  There was a significant amount of interest/support for the\n      CommentURL as a better mechanism for communication between\n      the server and the user.\n\nDave Morris ---------------------->\n\n\nProposed changes and additional comments:\n\n:: 2. Terminology\n[...]\n:: The terms request-host and request-URI refer to the values the client\n:: would send to the server as, respectively, the host (but not port) and\n:: abs_path portions of the absoluteURI (http_URL) of the HTTP request\n:: line.  Note that request-host must be a FQHN.\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete this phrase\n[...]\n\n:: Host names can be specified either as an IP address or a FQHN string.\n\nReplace the above line with:\n   Host names can be specified in any form acceptable to the base\n   HTTP protocol. That may be an IP address, an incomplete host name\n   string, or a FQHN string which is the prefered form.\n\n:: Sometimes we compare one host name with another.  Host A's name domain-\n:: matches host B's if\n\nReplace the following two points:\n\n::    * both host names are IP addresses and their host name strings\n::      match exactly; or\n\n::    * both host names are FQDN strings and their host name strings\n::      match exactly; or\n\nwith:\n    * both host name strings match exactly (this applies to IP\n      addresses and host name strings which are not FQHNs).\n\n\n::    * A is a FQDN string and has the form NB, where N is a non-empty\n::      name string, B has the form .B', and B' is a FQDN string.\n::      (So, x.y.com  domain-matches .y.com but not y.com.)\n\n[...]\n:: 3. STATE AND SESSIONS\n[...]\n\n:: There are, of course, many different potential contexts and thus many\n:: different potential types of session.  The designers' paradigm for\n:: sessions created by the exchange of cookies has these key attributes:\n::\n::   1.  Each session has a beginning and an end.\n::\n::   2.  Each session is relatively short-lived.\n::\n::   3.  Either the user agent or the origin server may terminate a\n::       session.\n::\n::   4.  The session is implicit in the exchange of state information.\n\nI believe the above paragraphis is an unnecessary elaboration and\nfurthermore points 1 and 2 are incorrect. Point 2 is in\nconflict with examples later in the document when storage of\npreferences is discussed. If something like this paragraph remains,\npoints 1 and two should be more like:\n     1.  Each session as an explicit beginning and an end which\n         can be approximated based on time and/or user termination\n         of user agent execution.\n     2.  A cookie based session last over the interval appropriate\n         to the context established by the session.\n\n[...]\n\n:: 4.2.2 Set-Cookie2 Syntax\n\nAn earlier comment was made about quotes in the \"Port\" attribute,\nbut I think there are additional problems with the syntax as\nspecified and suggest that:\n\n\n::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n:: port-list       |       1#DIGIT\n\nbe replaced with:\n\n                   |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n   portnum         =       1*DIGIT\n\nIf I correctly understand RFC2068 syntax, 1#X means 1 or more\noccurances of X delimited by commas. My changes fix the \"=\"\nin port-list, the 1#DIGIT in port list and make the quotes\noptional for the single port case.\n\nTo support the CommentURL, add:\n\n                    |    \"CommentURL\" \"=\" <\"> http_URL <\">\n[...]\n\n[...]\n\n\nI think the issue is not so much that a cookie can contain private\ninformation as it can be used to derive private information. Thus\nI think the following:\n\n:: Comment=comment\n::   Optional.  Because cookies can contain private information about a\n::   user, the Comment attribute allows an origin server to document how\n::   it intends to use the cookie.  The user can inspect the information\n\nshould be changed to read something like:\n     Optional.  Because cookies can be used to derive or store\n     private information about a user, the Comment attribute allows an\n     origin server to document how it intends to use the cookie. ...\n\nand also to support the CommentURL in the description section, add:\n\nCommentURL\n      Optional.  Because cookies can be used to derive or store\n      private information about a user, the CommentURL\n      attribute alows an origin server to document how it intends\n      to use the cookie.  The user can inspect the information\n      identified by the URL to decide whether to initiate or\n      continue a session with this cookie.\n[...]\n\n:: Domain=domain\n::    Optional.  The Domain attribute specifies the domain for which the\n::    cookie is valid.  An explicitly specified domain must always start\n::    with a dot.\n\nChange the 'must' in the last sentence to 'should' and add:\n  If the domain does not start with a dot, the client MUST prepend\n  the dot logically prior to use of the domain value.\n[...]\n\n:: 4.2.3 Controlling Caching\n[...]\n\n:: HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n:: long in the past) on responses containing Set-Cookie2 response headers\n:: unless they know for certain (by out of band means) that there are no\n:: upstream HTTP/1.0 proxies.  HTTP/1.1 servers may send other Cache-\n                            ^^^^ insert:\n         or that any upstream proxies will not cache the response.\n\n:: Control directives that permit caching by HTTP/1.1 proxies in addition\n:: to the Expires: old-date directive; the Cache-Control directive will\n:: override the Expires: old-date for HTTP/1.1 proxies.\n\nFor example, is it has been fairly well accepted that HTTP/1.0\nproxies will not cache responses to POST requests or requests which\ninclude a '?' part.\n\n[...]\n:: 4.3 User Agent Role\n\n\n:: Port   The default behavior is that a cookie may be returned only to\n::        its request-port.\n\nI think the correct default for Port is:\n\n   Port   If the Port attribute is not specified, there are no\n          port restrictions for subsequent use of the cookie.  If the\n          Port attribute is specified without a value, the cookie\n          may only be sent in a request to the same port that it\n          was received on.\n[...]\n\n:: 4.3.2 Rejecting Cookies\n[...]\n::    * The value for the Domain attribute contains no embedded dots\n::      or does not start with a dot.\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete the phrase\n\n::    * A Set-Cookie2 with Domain=ajax.com will be rejected because the\n::      value for Domain does not begin with a dot.\n\nThe above example should be deleted or replace with something like:\n      * A Set-Cookie2 with Domain=ajax.com will have a dot prepended\n        to value for Domain because it does not begin with a dot.\n\n[...]\n:: 4.3.3  Cookie Management  If a user agent receives a Set-Cookie2\n:: response header whose NAME is the same as a pre-existing cookie, and\n                  ^^ insert 'value with a cookie'\n:: whose Domain and Path attribute values exactly (string) match those\n\nA single set-cookie2 response header can contain multiple cookies.\n\n[...]\n:: Because user agents have finite space in which to store cookies, they\n:: may also discard older cookies to make space for newer ones, using, for\n:: example, a least-recently-used algorithm, along with constraints on the\n:: maximum number of cookies that each origin server may set.\n                 ^^ insert 'or size'\n\n:: If a Set-Cookie2 response header includes a Comment attribute, the user\n:: agent should store that information in a human-readable form with the\n:: cookie and should display the comment text as part of a cookie\n          ^^^ or\n:: inspection user interface.\n\nThe critical issue is that the Comment be made available to the user.\nWith the 'and' included, the specification makes an unnecessary\nimplementation restriction. How it is stored is not important unless\npoking around in the storage area is the only mechanism the user\nhas for examination of the cookie.\n\nTo support CommentURL, the following should be added:\n  If a Set-Cookie2 response header includes a CommentURL attribute,\n  the user agent should either store the value in a human-readable\n  form with the cookie or preferably allow the user to link to the\n  URL as part of a cookie inspection interface.\n\n:: 4.3.4 Sending Cookies to the Origin Server\n[...]\n\n:: The value of the cookie-version attribute must be the value from the\n:: Version attribute of the corresponding Set-Cookie2 response header.\n:: Otherwise the value for cookie-version is 0.  The value for the path\n:: attribute must be the value from the Path attribute, if any, of the\n:: corresponding Set-Cookie2 response header.  Otherwise the attribute\n:: should be omitted from the Cookie request header.  The value for the\n:: domain attribute must be the value from the Domain attribute, if any,\n:: of the corresponding Set-Cookie2 response header.  Otherwise the\n:: attribute should be omitted from the Cookie request header.\n\nI read the above to make sending of the $path and $domain values\non a cookie: response header as optional.  To me it says, if the\n$path and $domain values are sent, then be the values in the\nset-cookie2 response header. Is this what is intended? Based on\nlater examples, I would have expected it be required that they\nbe returned if set.\n\n[...]\n\n::     2.  If the attribute is present but has no value (e.g., Port),\n::         the cookie must only be sent to its request-port.\n                                           ^^^^^^^^^^^^^^^^^\n       would be clearer I think if replaced with:\n           \"the request-port it was received from.\n\n4.3.5 Sending Cookies in Unverifiable Transactions\n[...]\n:: However, even this would not make all links verifiable; for example,\n:: links to automatically loaded images would not normally be subject to\n                                       ^^^ insert \"or other objects\n                                                   such as frame\n                                                   content\"\n:: ``mouse pointer'' verification.\n\n\n:: Many user agents also provide the option for a user to view the HTML\n:: source of a document, or to save the source to an external file where it\n:: can be viewed by another application.  While such an option does provide\n:: a crude review mechanism, some users might not consider it acceptable\n                             ^^^^^^^^^^^^^^^^\n:: for this purpose.\n\nThat is the understatement of the year .... I would prefer words like\n                             \"many users would\"\n\n\n\n:: 4.5  Caching Proxy Role\n[...]\n:: Proxies must not introduce Set-Cookie2 (Cookie) headers of their own\n           ^^^^ I would prefer 'should'\n:: in proxy responses (requests).\n\n:: 5. Examples\n\nIt looks to me like the exmaples are WRONG ... the syntax requires\nthe version prior to the cookie name/value. (This also implies\nthe historical rules for merging cookie and cookie2 are wrong).\n\n\n\n:: 6. IMPLEMENTATION CONSIDERATIONS\n[...]\n:: 6.1 Set-Cookies Content\n[...]\n\n:: The session information can obviously be clear or encoded text that\n:: describes state.  However, if it grows too large, it can become\n:: unwieldy.  Therefore, an implementor might choose for the session\n:: information to be a key to a server-side resource.\n\nThe above is fine and sufficient.  The following comment asserts\na motivations for this specification which has not been established\nand which I would disagree with. Delete the following ....\n\n::                                                    Of course, using\n:: a database creates some problems that this state management\n:: specification was meant to avoid, namely:\n::\n::   1.  keeping real state on the server side;\n::\n::   2.  how and when to garbage-collect the database entry, in case the\n::       user agent terminates the session by, for example, exiting.\n\n[...]\n:: 6.3 Implementation Limits\n[...]\n\nEither delete the following completely or replace it. Dictating\nthat such a device dedicate 80K+ of storage to cookies is not\nreasonable.\n\n:: User agents created for specific purposes or for limited-capacity\n:: devices should provide at least 20 cookies of 4096 bytes, to ensure\n:: that the user can interact with a session-based origin server.\n\nCalling something a limited capacity device, we have acknowledged\nthat we don't know its capabilities or its intended audience. I\ndon't advocate the following but could tolerate it if we are\ncompelled to say anything:\n\n   User agents created for specific purposes or for limited-capacity\n   devices should provide sufficient cookie storage capacity to\n   address the requirements of the applications they are intended\n   to serve.\n\n[...]\n:: 7.1  User Agent Control\n[...]\n\nThe following is implementation advice which can be dropped. Surely\nwe need not instruct UA authors about data integrity.\n\n:: NOTE: User agents should probably be cautious about using files to store\n:: cookies long-term.  If a user runs more than one instance of the user\n::agent, the cookies could be commingled or otherwise corrupted.\n\n[...]\n:: 10.  HISTORICAL\n:: 10.1  Compatibility with Existing Implementations\n\nAs I stated in my introduction, I feel quite strongly that the\nproposed handling of old and new cookies is wrong. I shall attempt\nto proposed edits which will make this section fit my\nrecommendations.\n[...]\nRewrite:\n:: An origin server that supports user agents that are compatible both\n:: with Netscape's original proposal and this one must, for a transition\n:: period, send two response headers.  Set-Cookie contains the ``old''\n:: cookie information.\n::                     Set-Cookie2 contains the cookie information that\n:: is new to this specification.  The rules below ensure that the two\n:: pieces get combined correctly.\n::                                Eventually, when the majority of user\n:: agents follow this specification, the Set-Cookie response header can\n:: be phased out, and all cookie information can be carried in the\n:: Set-Cookie2 response header.\n\nby replacing the broken out middle section with\n                       Set-Cookie2 contains the the same cookie\n          information expressed according to this specification.\n\nThen replace:\n:: Once a server receives a new cookie from a client, it may continue a\n:: session by sending only Set-Cookie2 response headers.\n\nwith:\n   New version of user agents which had previously stored cookies\n   based on Netscape's original specifications should convert\n   existing cookies by adding the $Version attribute with a value\n   of 0 to signal to the server the ability to handle Set-cookie2\n   format cookies.\n\n   Once a session is initiated during the transition period using\n   both set cookie headers in the first response to a user-agent\n   the server will know from the cookie header received in the\n   next request which form of cookies the user agent supports.\n   The server should only send either the Set-cookie2 if it is\n   supported or the set-cookie if Set-cookie2 is not supported on\n   subsequent responses which require an updated cookie value.\n\n\nThen delete sections \"10.1.1 Combining Set-Cookie and Set-cookie2\"\nand \"10.1.2 An Example\".\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-negotiation02.tx",
            "content": "Graham Klyne:\n>\n>Koen,\n>\n>I am taking a second bite at understanding your negotiation draft, and have\n>a few more questions and comments for you:\n>\n>At 08:23 PM 6/7/97 +0200, Koen Holtman wrote:\n>>>\n>>>* Sections 4.9, 4.10\n>>>\n>>>I find I am not clear how the negotiation scheme described might interact\n>>>with or be distinguished from other schemes, in view of the exhortation in\n>>>4.10.\n>>>\n>>>I am particularly unnsure of the point number 2 in section 4.10:  does the\n>>>encouragement to re-use the \"transport and caching protocol\" extend to the\n>>>transparent negotiation header names? \n>>\n>>Yes.\n>>\n>>> If so, how might invocation of an \"other negotiation scheme\" be\n>>>distinguished from your transparent negotiation?\n>>\n>>The other scheme would presumably use different keywords in the\n>>Negotiate and TCN headers for the invocation of things with different\n>>semantics.  But the spec does not require this, some schemes could use\n>>other methods to distinguish themselves.\n>\n>I don't understand this response, which probably means I still don't\n>understand the full implications of TCN.\n\nThe kind of re-use which is encouraged at the end of 4.10 is not\nreally particular to TCN, it is a general thing in the HTTP protocol\nsuite.  So I guess what you are missing is the general picture of how\nprotocol elements are re-used in HTTP.\n\nRe-use in HTTP is often creative abuse.  For example, HTTP/1.1 abuses\nthe HTTP/1.0 semantics of Expires: <yesterday> to tunnel various\nthings safely through HTTP/1.0 proxies.  1.1 will add Expires to\nresponses which contain fresh information (from a 1.1 viewpoint), even\neven though this means violating the letter of the 1.0 protocol, in\nwhich Expires is supposed to tell when the content gets stale.\n\nSo, though 1.1 reuses the 1.0 expires header, it pays no attention\nwhatsoever to the `meaning' which is attributed to this header by 1.0.\nIt only pays attention to the plain semantics defined by 1.0.\n\nAs another example of creative abuse, TCN defines `structured entity\ntags', and goes on to creatively abuse the semantics of the 1.1 entity\ntag related headers, even though 1.1 declares that entity tags are\nopaque.\n\nIn HTTP practice, a protocol extension is allowed to ignore\neverything but the plain semantics of the protocol elements which are\nreused.  This means that TCN cannot be very specific about the ways in\nwhich its protocol elements may be reused.  Basically, everything\ngoes.\n\nBack to your question:\n\n   how might invocation of an \"other negotiation scheme\" be\n   distinguished from your transparent negotiation?\n\nThe answer is that an other negotiation scheme, say the X scheme, can\ndistinguish itself in any way it wants to.  It would presumably do so\nwith new keywords.  But it could also use PEP, for example.\n\nAlso, there is no requirement that clients which do not understand the\nX scheme, for example plain 1.1 clients or 1.1+TCN clients, are able\nto detect that the X scheme is being used.  The X scheme may even\ntrick a TCN-capable client into thinking it is dealing with a\nTCN-negotiated resource, so that the client, following the TCN spec\nsemantics, takes some action which happens to suit the purposes of the\nX scheme.\n\n>But on reflection and partial re-reading of the draft I have formed the\n>idea that the features used by TCN are identified by virtue of appearing in\n>an 'Alternates' header.  But the description of 'Alternates' suggests that\n>this understanding is, at best, incomplete.\n\nI think you are confusing the features _of_ TCN (i.e. the TCN protocol\nelements) with the feature tags used by TCN here, but I am not sure.\n\n[...]\n>I've constructed myself a little graph showing the relationships between\n>the various headers and feature-related syntax productions.  Have I missed\n>anything vital here?\n\nNo, this looks about right, though I would add\n\n  feature-set --> ftag\n\n>  'Accept-features:' --> feature-expr --> ftag\n>\n>  'Alternates:' --> variant-description -->\n>                    variant-attribute-list -->  )  feature-list\n>  'Content-features:'                      -->  )\n>   \n>  feature-list --> fpred --> ftag\n>\n>I conclude from this that 'feature-expr' and 'fpred' are counterparts, in\n>that 'feature-expr' describes the features of a message recipient (more\n>strictly: an HTTP client), and 'fpred' describes features of a message\n>value which may be supplied by a server (a 'variant'; a particular\n>representation of a resource).\n\nNo, I would not put it this way.  You are trying to construct a\nsymmetry between sender and receiver which is not present.\nfeature-expr and fpred are not counterparts: fpreds are part of the\ncore semantics of the protocol and feature-exprs are only part of an\noptimization device.\n\n[...]\n>My own thinking about the issues of content negotiation (posted to the\n>HTTP-WG list) leads me to believe that the process should be performed\n>within a symmetric framework (at least insofar as the identification of\n>negotiable features is concerned).  Therefore I find myself questioning the\n>asymmetry in your proposal.\n\nSee my response to your message `Content negotiation requirements'.\nDifferences between feature-expr and fpred are not a flaw in the\nsymmetry of TCN, they are symptoms of its fundamental asymmetry.\n\n[...]\n>\n>And I have some more editorial comments regarding your draft:\n>\n\n>* Section 5.7:\n>\n>I think the reference to \"new dimensions\" of negotiation contradicts\n>section 4.7.\n\nI'm not sure what you mean here, I see no contradiction.  The `future\nspecifications' do not need to be specifications of TCN.\n\n>* Section 6.3, 1st para:\n>\n>This implies that a feature predicate can exist *only* in the context of a\n>specific request.\n\n?? I don't read it as implying that, but I'll change it to:\n\n   Feature predicates are predicates on the contents of feature sets.\n\n>* Section 6.4:\n>\n>I assume that true-improvement < 1 or false-degradation > 1 are permitted?\n\nYes.  This will make life easier for some automatic predicate\ngenerators.\n\n>* Section 8.4:\n>\n>Are there any circumstances in which a response from a transparently\n>negotiable resource is not required to include an 'Alternates:' header?\n\nYes.  If the response is an error, list or adhoc response, Alternates\nneed not be included.\n\n>GK.\n\nThanks for your comments,\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation requirement",
            "content": "Graham Klyne:\n>\n>\n>At 08:15 PM 7/4/97 +0200, Koen Holtman wrote:\n[...]\n>>In other types of negotiation, where two machines are involved, a\n>>symmetric metadata format may be a nice win, but not so in TCN.  In\n>>TCN, it would just add to the complexity of defining the choice\n>>process.\n>\n>I'm not convinced.  But I can see I must produce some more concrete\n>evidence for my view -- I'll think about it (it may take a while).\n(See\n>also later comments.)\n\n[...]\n\n>Question:  if I could offer (a) a generic framework and metadata\nformat,\n>and (b) indicate how these might be subsetted to current HTTP/TCN\n>capabilities, would you consider that this would address your\n>concerns?\n\nOne of my concerns is that symmetric mechanisms would add complexity\nto HTTP negotiation, so if your (b) is simple enough, this concern\nwould be addressed.\n\nHowever, another concern is that any new HTTP negotiation framework\nmust not shift paradigms more than is needed.  Especially for the\nsimple cases, the metadata format must stay as close as possible to\nthe way the target metadata authors think about negotiation.  And\nthese target authors are not programmers who write server-side\nrendering engines, but content authors who may not have any knowledge\nof programming or mathematical logic.  This puts some limitations to\nthe amount of complexity the protocol can shift to the metadata\nauthor.\n\n>(NOTE: I am NOT proposing to re-design TCN or any part of HTTP!)\n>\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: New feature tag registration drafts availabl",
            "content": "Graham Klyne:\n>\n>At 10:43 PM 7/7/97 +0200, Koen Holtman wrote:\n>>\n>> draft-ietf-http-feature-scenarios-00.txt \n>>     `Feature Tag Scenarios' \n>>     Discusses why we need feature tag registration.\n>\n>Koen,\n>\n>Following a first read-through the feature secanarios draft, I have some\n>comments for you.\n>\n\nMy second batch of responses is below.  I left comments about simple\nthings I will resolve in editing unanswered.\n\n[...]\n>* Section 3.1:\n>\n>A nit-pick...\n>\n>I agree with the point you are making but I question the example you choose.\n>\n>I understand HTTP negotiation to be extensible, in that server-side\n>negotiation can depend upon *any* of the supplied request headers,\n\nThere are a number of things in HTTP you could call `negotiation\nmechanisms', and I was talking about the `4 Accept header mechanism'\nonly.  I am not claiming that HTTP as a whole is inextensible.  I'll\ntry to rephrase.\n\n>* Section 4.3, item t+2.5:\n>\n>This implies another requirement:  it must be possible for content authors\n>to specify feature tags and associated values in authored content (this has\n>possible implications for representability within HTML, for example).\n\nYes.  This is why < and > are not allowed in feature tags, and why\nfeature predicates have the syntax paper!=A4, not paper<>A4.\n\n>* Section 4.4, para 2:\n>\n>I think there is a potential problem more pernicious than 'dead' tags,\n>etc., which is use of different tags by different vendors to mean the same\n>thing.  This could cause unecessary negotiation failures -- maybe some\n>alias mecchanism could be defined within the registration procedures?\n\nThere is some `comments on tags' and `declare OBSOLETE' stuff in the\nregistration procedure which could be used for alias cases, among\nother things.  But I don't think that a crisp alias registry would\naddress all alias problems.\n\nThe trouble with aliases is: who is allowed to declare that A and B\nare really the same thing?  If Bob registers a tag B which is the same\nas the A tag registered before by Alice, then Bob will generally not\nknow about the A tag, else Bob would have just used A.  So there would\nhave to be a third person Chris to register the alias relationship\nafter the fact.  But then Alice or Bob might disagree with Chris and\nsay that their implementations are not entirely identical, so Chris is\nmaking things less interoperable by declaring A and B equal, etc.\n\nTCN takes the viewpoint that, in the end, it is up to the content\nauthors do decide whether A and B are interchangeable in a particular\ncase at hand.  See section 19.1 in draft-ietf-http-negotiation-02.txt.\nLike with dead tags, I do not assume that content authors will directly\ngo to the registry, but that there will be third parties who make\nlists of useful aliases, describing for each alias the necessary\nboundary conditions under which the alias relationship holds.\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-feature-reg01.tx",
            "content": "Graham Klyne:\n>\n>Koen,\n>\n>A couple of comments regarding your feature registration draft...\n>\n>\n>* Section 3.6, item (3):\n>\n>In conflict with section 2.2 final paragraph, this item does seem to place\n>prior restrictions on the things can be identified by a feature tag.\n>\n>Specifically, the wording suggests the range of a feature value must be a\n>scalar type (enumeration, number range, etc.).  Compound values seem to be\n>excluded; e.g. sets (multiple choices), cartesian products\n>(multidimensional values).\n\nI did not intend it to suggest that dimensions must be scalar, quite\nthe contrary.  I plan to add some examples which make this more clear.\n\n>* Section 3.8:\n>\n>(A) In \"Summary of the indicated dimension of negotiation\", I am\n>uncomfortable with your use of the term 'dimension' in this context.  I\n>think what you are describing is the range of values associated with a\n>dimension.\n\nThe trouble is that some dimensions can be described nicely in terms\nof their values, while for other dimensions, where the values are yes\nand no, a description in terms of the `meaning' of these values must\nbe given.\n\nI plan to change \"Summary of the indicated dimension of negotiation\"\nto \"Summary of negotiation with this feature tag\", or something.  I am\nnot sure I want to use the word `dimension' in the form.\n\n>Underlying this comment is my feeling that I don't think sufficient\n>distinction is drawn between identification of a dimension of negotiation\n>(the feature tag) and the values which are associated with that dimension\n>(feature values?).\n\nDo you mean in the form or in the text around it?\n\n\n>(B) I feel your list of \"Result in the indicated dimension of negotiation\"\n>is rather arbitrary.\n\nIt is.  It is supposed to reflect increasing complexity, both from a\nhuman viewpoint and from the viewpoint of an extensible negotiation\nmechanism.  For example, not all extensible mechanisms may support\n\n     [ ] 4. A choice for a non-integer value among several\n\n>  Why separate items (1) and (2)?\n\nI think (1) is a very frequent, and simple case, so separating it out\nwould make it easier to fill in and read the forms.\n\n>  Similarly (3) and\n>(4) in that they both represent a choice from an enumeration of\n>values.\n\nNo, (4) is also supposed to represent choice values without a natural\nenumeration.\n\n>I would suggest:\n>\n>(1) A yes/no choice\n>(2) A choice of one value from a finite enumeration of (possibly numeric)\n>values\n>(3) A choice of multiple values from a finite enumeration of values (powerset)\n>(4) A value selected from a finite or infinite range of some scalar type\n>(integer, real)\n>(5) A compound value (e.g. MIME Content-type, range of numeric values)\n>\n>[I also note that these values relate to a specific message which is\n>transferred: the feature negotiation mechanism would have to deal with\n>multiple values for any of these value range types.]\n\n?? If there were not multiple values to choose from, there would be no\nnegotiation.\n\n\nHere is a rewrite of the first part of the form, please say if you\nlike this better.  Note that I'm trying to avoid the `dimension'\nterminology.\n\n3.8 Registration template\n\n   To: ietf-feature-tags@iana.org (Feature tags mailing list)\n        (or directly to iana@iana.org)\n   Subject: Registration of feature tag XXXX\n\n    | Instructions are preceded by `|'.  Some fields are optional.\n\n   Feature tag name:\n\n   Summary of negotiation with this feature tag\n\n    | Include a short (no longer than 4 lines) description or summary\n    | Examples:\n    |   `Negotiation on whether to use the xyzzy feature of ...'\n    |   `Negotiation on the MIME media type of the data which\n    |    is transmitted for ...'\n    |   `Negotiation on whether to use colors in displaying ...'\n    |   `Negotiation on the number of colors to use in displaying ...'\n\n   Number of alternatives in (sub)negotiation with this tag:\n\n     [ ] 1. Two alternatives\n     [ ] 2. More than two alternatives\n\n   For case 1: nature of the two alternatives:\n\n     [ ] 1a. A particular feature is used/invoked/enabled, or not\n     [ ] 1b. Other\n\n   For case 2: Is there a natural way to identify every possible\n               alternative?\n\n     [ ] 2a. Yes\n     [ ] 2b. No\n\n   For case 2a: How is a single alternative naturally identified?\n\n     [ ] 2a.1 With a name, keyword, label, or tag (e.g. a language tag)\n     [ ] 2a.2 With an integer value\n     [ ] 2a.3 With a numeric value of a non-integer type (e.g. float)\n     [ ] 2a.4 With a non-numeric (non-scalar) value (e.g. a piece of code)\n     [ ] 2a.5 With a record or tuple of values\n     [ ] 2a.6 With a set of values\n     [ ] 2a.7 Other\n\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "David W. Morris wrote:\n> \n> I have a number of remaining concerns with the HTTP State Management\n> Mechanism draft-02. They range from editorial and clarity to\n> functional. I'll sumarize the functional concerns and then step\n> thru the draft with detailed comments and/or suggestions.\n\n[I'm still on vacation, so I'll do my best to respond.  Please attribute\nstupid remarks from me to my brain's not being fully engaged. :-) ]\n\n> \n> Functional Concerns:\n> \n>   1.  The cookie/cookie2 interoperational requirement that clients\n>       clients merge the values from two cookie/cookie2 and servers\n>       split the values present unnecessary implementation complexity\n>       and opportunity for errors to conver a transition interval\n>       and to save small amount of network traffic. More below.\n\nI think we've been through this discussion before, and we've rejected\nthe alternatives.\n> \n>   2.  I see no advantage to requiring the use of a FQHN in all\n>       cases. It is sufficient to exactly match the host name\n>       between URL's response which set the cookie and the URL\n>       candidate for inclusion of the cookie. Intranet applications\n\nFWIW, didn't Netscape's original spec. require FQHN?\n\nWould you accept the case where the host name was a single unqualified\nword (no dots)?  If so, I'll see if I can work that through the\nwording.  But the assumption that would follow is that the cookie could\nreturn *only* to that host, not to related ones.\n\n>       work quite nicely with host names which are not FQHNs.\n>       Furthermore, in the intranet context this has the potential\n>       of requiring a user to type a FQHN to access an application\n>       which uses Cookies. An unnecessary restriction. We\n\nI think the reality is that most cookie-bearing applications get entered\nby following a link, and I don't think it's unreasonable for the link to\nhave a FQHN.\n\n>       have already concluded in discussions relationg to 2068\n>       and the Host: header that it is impossible to require a UA\n>       to expand a simple host name into a FQHN.\n\nAnd I wouldn't expect a UA to expand one here, either.\n> \n>   3.  Requiring the leading period in the domain= value can be\n>       avoided by simply requiring that the client add the period\n>       if it isn't provided prior to performing the domain matching.\n\nNot so.  I've discussed this several times before (although possibly\nprivately).  The no-dot form of the value (which is specifically not\npermitted by the spec., but which is the default if Domain= is omitted)\nrestricts a cookie to return to just the origin server.  The with-dot\nform permits returning the cookie to more than one server.  (All the\nones that domain-match the value.) \n> \n>   4.  There was a significant amount of interest/support for the\n>       CommentURL as a better mechanism for communication between\n>       the server and the user.\n\nMy take on the discussion was that the opinions were mixed.  Granted\nthere is merit in the proposal, there are also concerns.  You would have\nto establish rules about cookies in CommentURL, etc.\n\nI would like to table CommentURL to a followup to (the followup to) RFC\n2109.  (I made no further comments about your CommentURL remarks here.)\n\n> \n> Dave Morris ---------------------->\n> \n> Proposed changes and additional comments:\n> \n> :: 2. Terminology\n> [...]\n> :: The terms request-host and request-URI refer to the values the client\n> :: would send to the server as, respectively, the host (but not port) and\n> :: abs_path portions of the absoluteURI (http_URL) of the HTTP request\n> :: line.  Note that request-host must be a FQHN.\n>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete this phrase\n\nNo.  The domain-matching rules and everything that flows from them\ndepends on FQHN's.  But I'll consider what I mentioned above, namely the\ncase where request-host is a single word (no dots), and I would try to\nchange the words accordingly.\n\n> [...]\n> \n> :: Host names can be specified either as an IP address or a FQHN string.\n> \n> Replace the above line with:\n>    Host names can be specified in any form acceptable to the base\n>    HTTP protocol. That may be an IP address, an incomplete host name\n>    string, or a FQHN string which is the prefered form.\n\nNo again.  See above.\n\n> \n> :: Sometimes we compare one host name with another.  Host A's name domain-\n> :: matches host B's if\n> \n> Replace the following two points:\n> \n> ::    * both host names are IP addresses and their host name strings\n> ::      match exactly; or\n> \n> ::    * both host names are FQDN strings and their host name strings\n> ::      match exactly; or\n> \n> with:\n>     * both host name strings match exactly (this applies to IP\n>       addresses and host name strings which are not FQHNs).\n\nMaybe, depending on your response to my question above.\n\n> \n> ::    * A is a FQDN string and has the form NB, where N is a non-empty\n> ::      name string, B has the form .B', and B' is a FQDN string.\n> ::      (So, x.y.com  domain-matches .y.com but not y.com.)\n> \n> [...]\n> :: 3. STATE AND SESSIONS\n> [...]\n> \n> :: There are, of course, many different potential contexts and thus many\n> :: different potential types of session.  The designers' paradigm for\n> :: sessions created by the exchange of cookies has these key attributes:\n> ::\n> ::   1.  Each session has a beginning and an end.\n> ::\n> ::   2.  Each session is relatively short-lived.\n> ::\n> ::   3.  Either the user agent or the origin server may terminate a\n> ::       session.\n> ::\n> ::   4.  The session is implicit in the exchange of state information.\n> \n> I believe the above paragraphis is an unnecessary elaboration and\n> furthermore points 1 and 2 are incorrect. Point 2 is in\n> conflict with examples later in the document when storage of\n> preferences is discussed. If something like this paragraph remains,\n> points 1 and two should be more like:\n>      1.  Each session as an explicit beginning and an end which\n>          can be approximated based on time and/or user termination\n>          of user agent execution.\n>      2.  A cookie based session last over the interval appropriate\n>          to the context established by the session.\n\nI see your point.  Let me think about it.\n\n> \n> [...]\n> \n> :: 4.2.2 Set-Cookie2 Syntax\n> \n> An earlier comment was made about quotes in the \"Port\" attribute,\n> but I think there are additional problems with the syntax as\n> specified and suggest that:\n> \n> ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n> :: port-list       |       1#DIGIT\n> \n> be replaced with:\n> \n>                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n>    portnum         =       1*DIGIT\n> \n> If I correctly understand RFC2068 syntax, 1#X means 1 or more\n> occurances of X delimited by commas. My changes fix the \"=\"\n> in port-list, the 1#DIGIT in port list and make the quotes\n> optional for the single port case.\n\nGiven that Port is new, I see no value, and potential risk, in making\nthe quotes optional.  The risk is that someone will forget them when\nthere's a list.\n\nAnd I guess I don't understand what you want to \"fix\" about \"=\".  Except\nfor your allowing a single portnum to omit the quotes, your syntax and\nmine appear equivalent.\n\n> \n> To support the CommentURL, add:\n> \n>                     |    \"CommentURL\" \"=\" <\"> http_URL <\">\n> [...]\n> \n> [...]\n> \n> I think the issue is not so much that a cookie can contain private\n> information as it can be used to derive private information. Thus\n\nSurely both are true, depending on your definition of \"private\".  For\nexample, your username and password at some service (e.g., New York\nTimes) is supposed to be private, but the service will, with your\npermission, put them in a cookie.\n\n> I think the following:\n> \n> :: Comment=comment\n> ::   Optional.  Because cookies can contain private information about a\n> ::   user, the Comment attribute allows an origin server to document how\n> ::   it intends to use the cookie.  The user can inspect the information\n> \n> should be changed to read something like:\n>      Optional.  Because cookies can be used to derive or store\n>      private information about a user, the Comment attribute allows an\n>      origin server to document how it intends to use the cookie. ...\n> \n> and also to support the CommentURL in the description section, add:\n> \n> CommentURL\n>       Optional.  Because cookies can be used to derive or store\n>       private information about a user, the CommentURL\n>       attribute alows an origin server to document how it intends\n>       to use the cookie.  The user can inspect the information\n>       identified by the URL to decide whether to initiate or\n>       continue a session with this cookie.\n\nThat's only true, of course, if the UA allows you to do so.\n\n> [...]\n> \n> :: Domain=domain\n> ::    Optional.  The Domain attribute specifies the domain for which the\n> ::    cookie is valid.  An explicitly specified domain must always start\n> ::    with a dot.\n> \n> Change the 'must' in the last sentence to 'should' and add:\n>   If the domain does not start with a dot, the client MUST prepend\n>   the dot logically prior to use of the domain value.\n\nNo.  (See above.)\n\n> [...]\n> \n> :: 4.2.3 Controlling Caching\n> [...]\n> \n> :: HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n> :: long in the past) on responses containing Set-Cookie2 response headers\n> :: unless they know for certain (by out of band means) that there are no\n> :: upstream HTTP/1.0 proxies.  HTTP/1.1 servers may send other Cache-\n>                             ^^^^ insert:\n>          or that any upstream proxies will not cache the response.\n> \n> :: Control directives that permit caching by HTTP/1.1 proxies in addition\n> :: to the Expires: old-date directive; the Cache-Control directive will\n> :: override the Expires: old-date for HTTP/1.1 proxies.\n> \n> For example, is it has been fairly well accepted that HTTP/1.0\n> proxies will not cache responses to POST requests or requests which\n> include a '?' part.\n\nThe problem with that is those rules are heuristics that have been\nimplemented by some proxies.  They are not mandated by any (HTTP/1.0)\nspec., so you can't depend on them.  All you really (might) know is\ntheir HTTP version number.  As a practical matter I think it's unlikely\nfor an originserver to know there are no upstream proxies, and it will\nhave to send Expires anyway.\n\n> \n> [...]\n> :: 4.3 User Agent Role\n> \n> :: Port   The default behavior is that a cookie may be returned only to\n> ::        its request-port.\n> \n> I think the correct default for Port is:\n> \n>    Port   If the Port attribute is not specified, there are no\n>           port restrictions for subsequent use of the cookie.  If the\n>           Port attribute is specified without a value, the cookie\n>           may only be sent in a request to the same port that it\n>           was received on.\n\nNo.  I believe I wrote what was the consensus of the comments about Port\nwhen it was first discussed.  Furthermore, I asked for concurrence on\nthe words when I first issued an I-D that describe Port, and I got it\n(through an absence of objections).\n\n> [...]\n> \n> :: 4.3.2 Rejecting Cookies\n> [...]\n> ::    * The value for the Domain attribute contains no embedded dots\n> ::      or does not start with a dot.\n>         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete the phrase\n> \n> ::    * A Set-Cookie2 with Domain=ajax.com will be rejected because the\n> ::      value for Domain does not begin with a dot.\n> \n> The above example should be deleted or replace with something like:\n>       * A Set-Cookie2 with Domain=ajax.com will have a dot prepended\n>         to value for Domain because it does not begin with a dot.\n\nNo.  That would mean something different.  See above.\n> \n> [...]\n> :: 4.3.3  Cookie Management  If a user agent receives a Set-Cookie2\n> :: response header whose NAME is the same as a pre-existing cookie, and\n>                   ^^ insert 'value with a cookie'\n> :: whose Domain and Path attribute values exactly (string) match those\n\nI sort of see what you're trying to say (especially given your next\ncomment), but I don't think the words are necessary.  A Set-Cookie2\nlogically takes a single NAME, even though multiple Set-Cookie2's could\nbe folded together.\n> \n> A single set-cookie2 response header can contain multiple cookies.\n\nRedundant.  It's implied by RFC 822 header folding.\n\n> \n> [...]\n> :: Because user agents have finite space in which to store cookies, they\n> :: may also discard older cookies to make space for newer ones, using, for\n> :: example, a least-recently-used algorithm, along with constraints on the\n> :: maximum number of cookies that each origin server may set.\n>                  ^^ insert 'or size'\n> \n> :: If a Set-Cookie2 response header includes a Comment attribute, the user\n> :: agent should store that information in a human-readable form with the\n> :: cookie and should display the comment text as part of a cookie\n>           ^^^ or\n> :: inspection user interface.\n> \n> The critical issue is that the Comment be made available to the user.\n> With the 'and' included, the specification makes an unnecessary\n> implementation restriction. How it is stored is not important unless\n> poking around in the storage area is the only mechanism the user\n> has for examination of the cookie.\n\nI take your point.  However, the second part is a \"should\", and what\nconstitutes a \"cookie inspection user interface\" is intentionally\nunspecified so as not to \"[make] an unnecessary implementation\nrestriction\".  If we make this an \"or\", a UA could (as some already do)\nbury the cookie in some obscure place where it's hard for a user to\nfind.\n \n> \n> To support CommentURL, the following should be added:\n>   If a Set-Cookie2 response header includes a CommentURL attribute,\n>   the user agent should either store the value in a human-readable\n>   form with the cookie or preferably allow the user to link to the\n>   URL as part of a cookie inspection interface.\n> \n> :: 4.3.4 Sending Cookies to the Origin Server\n> [...]\n> \n> :: The value of the cookie-version attribute must be the value from the\n> :: Version attribute of the corresponding Set-Cookie2 response header.\n> :: Otherwise the value for cookie-version is 0.  The value for the path\n> :: attribute must be the value from the Path attribute, if any, of the\n> :: corresponding Set-Cookie2 response header.  Otherwise the attribute\n> :: should be omitted from the Cookie request header.  The value for the\n> :: domain attribute must be the value from the Domain attribute, if any,\n> :: of the corresponding Set-Cookie2 response header.  Otherwise the\n> :: attribute should be omitted from the Cookie request header.\n> \n> I read the above to make sending of the $path and $domain values\n> on a cookie: response header as optional.  To me it says, if the\n> $path and $domain values are sent, then be the values in the\n> set-cookie2 response header. Is this what is intended? Based on\n> later examples, I would have expected it be required that they\n> be returned if set.\n\nI'll have to clarify that.  What's meant is that if the UA receives\nthose attributes, it must return them.  If it uses the defaults because\nthey were omitted in Set-Cookie[2], it should not return them. The idea\nis for the server to get reflected back to it what it sent to the UA.\n> \n> [...]\n> \n> ::     2.  If the attribute is present but has no value (e.g., Port),\n> ::         the cookie must only be sent to its request-port.\n>                                            ^^^^^^^^^^^^^^^^^\n>        would be clearer I think if replaced with:\n>            \"the request-port it was received from.\n\nOkay.\n\n> \n> 4.3.5 Sending Cookies in Unverifiable Transactions\n> [...]\n> :: However, even this would not make all links verifiable; for example,\n> :: links to automatically loaded images would not normally be subject to\n>                                        ^^^ insert \"or other objects\n>                                                    such as frame\n>                                                    content\"\n\nUnfortunately, content magic is outrunning the speed with which we can\nget out the spec. :-)  I think (hope) that just mentioning images gets\nthe point across.  Otherwise it begins to look like we're trying to give\nan exhaustive list of unverifiable transactions, which is impossible.\n\n> :: ``mouse pointer'' verification.\n> \n> :: Many user agents also provide the option for a user to view the HTML\n> :: source of a document, or to save the source to an external file where it\n> :: can be viewed by another application.  While such an option does provide\n> :: a crude review mechanism, some users might not consider it acceptable\n>                              ^^^^^^^^^^^^^^^^\n> :: for this purpose.\n> \n> That is the understatement of the year .... I would prefer words like\n>                              \"many users would\"\n> \n> :: 4.5  Caching Proxy Role\n> [...]\n> :: Proxies must not introduce Set-Cookie2 (Cookie) headers of their own\n>            ^^^^ I would prefer 'should'\n> :: in proxy responses (requests).\n\nGiven that cookies are an end-to-end mechanism and controlled by an\norigin server (supposedly), it's hard for me to imagine a valid case\nwhere a proxy could introduce a Set-Cookie2.  Now proxy cookies are\nanother matter....\n> \n> :: 5. Examples\n> \n> It looks to me like the exmaples are WRONG ... the syntax requires\n> the version prior to the cookie name/value. (This also implies\n> the historical rules for merging cookie and cookie2 are wrong).\n\nI don't think so.  (Granted I only made a quick scan of the examples.) \nThe Version attribute in Set-Cookie[2] can be in any order after\nNAME=VALUE.  $Version must be first in the Cookie header.  I think the\nexamples consistently do that.  And I think the cookie merging rules\nwork.\n\n> \n> :: 6. IMPLEMENTATION CONSIDERATIONS\n> [...]\n> :: 6.1 Set-Cookies Content\n> [...]\n> \n> :: The session information can obviously be clear or encoded text that\n> :: describes state.  However, if it grows too large, it can become\n> :: unwieldy.  Therefore, an implementor might choose for the session\n> :: information to be a key to a server-side resource.\n> \n> The above is fine and sufficient.  The following comment asserts\n> a motivations for this specification which has not been established\n> and which I would disagree with. Delete the following ....\n> \n> ::                                                    Of course, using\n> :: a database creates some problems that this state management\n> :: specification was meant to avoid, namely:\n> ::\n> ::   1.  keeping real state on the server side;\n> ::\n> ::   2.  how and when to garbage-collect the database entry, in case the\n> ::       user agent terminates the session by, for example, exiting.\n\nThey're only implementation *considerations*.  Would you claim that 1\nand 2 are false?  We're pointing out the trade-off:  you can either have\ncomplete state sent back and forth between origin server and UA, or you\ncan pass a handle back and forth.  But if you do the latter, there are\nother considerations that come into play. Doing the former, where\npossible, seems easier to me.\n> \n> [...]\n> :: 6.3 Implementation Limits\n> [...]\n> \n> Either delete the following completely or replace it. Dictating\n> that such a device dedicate 80K+ of storage to cookies is not\n> reasonable.\n> \n> :: User agents created for specific purposes or for limited-capacity\n> :: devices should provide at least 20 cookies of 4096 bytes, to ensure\n> :: that the user can interact with a session-based origin server.\n> \n> Calling something a limited capacity device, we have acknowledged\n> that we don't know its capabilities or its intended audience. I\n> don't advocate the following but could tolerate it if we are\n> compelled to say anything:\n> \n>    User agents created for specific purposes or for limited-capacity\n>    devices should provide sufficient cookie storage capacity to\n>    address the requirements of the applications they are intended\n>    to serve.\n\nI see your point, and I guess that's okay.  The purpose of the section\nwas to set out some bounds so (unrelated) origin servers and UAs would\nbe able to depend on certain minima.  I grant that special-purpose\nagents tailored to a single application can cut things closer to the\nbone.  But of course the \"should\" already lets them do so.\n\n> \n> [...]\n> :: 7.1  User Agent Control\n> [...]\n> \n> The following is implementation advice which can be dropped. Surely\n> we need not instruct UA authors about data integrity.\n\nI suppose, but it doesn't hurt anything.\n\n> \n> :: NOTE: User agents should probably be cautious about using files to store\n> :: cookies long-term.  If a user runs more than one instance of the user\n> ::agent, the cookies could be commingled or otherwise corrupted.\n> \n> [...]\n> :: 10.  HISTORICAL\n> :: 10.1  Compatibility with Existing Implementations\n> \n> As I stated in my introduction, I feel quite strongly that the\n> proposed handling of old and new cookies is wrong. I shall attempt\n> to proposed edits which will make this section fit my\n> recommendations.\n\nYou lost me.  Which recommendations?  Oh, you mean to send complete\nSet-Cookie and Set-Cookie2.  No, we've chosen not to do that.\n\n> [...]\n> Rewrite:\n> :: An origin server that supports user agents that are compatible both\n> :: with Netscape's original proposal and this one must, for a transition\n> :: period, send two response headers.  Set-Cookie contains the ``old''\n> :: cookie information.\n> ::                     Set-Cookie2 contains the cookie information that\n> :: is new to this specification.  The rules below ensure that the two\n> :: pieces get combined correctly.\n> ::                                Eventually, when the majority of user\n> :: agents follow this specification, the Set-Cookie response header can\n> :: be phased out, and all cookie information can be carried in the\n> :: Set-Cookie2 response header.\n> \n> by replacing the broken out middle section with\n>                        Set-Cookie2 contains the the same cookie\n>           information expressed according to this specification.\n\nSorry, but that proposal was considered and rejected many months ago.\n> \n> Then replace:\n> :: Once a server receives a new cookie from a client, it may continue a\n> :: session by sending only Set-Cookie2 response headers.\n> \n> with:\n>    New version of user agents which had previously stored cookies\n>    based on Netscape's original specifications should convert\n>    existing cookies by adding the $Version attribute with a value\n>    of 0 to signal to the server the ability to handle Set-cookie2\n>    format cookies.\n> \n>    Once a session is initiated during the transition period using\n>    both set cookie headers in the first response to a user-agent\n>    the server will know from the cookie header received in the\n>    next request which form of cookies the user agent supports.\n>    The server should only send either the Set-cookie2 if it is\n>    supported or the set-cookie if Set-cookie2 is not supported on\n>    subsequent responses which require an updated cookie value.\n> \n> Then delete sections \"10.1.1 Combining Set-Cookie and Set-cookie2\"\n> and \"10.1.2 An Example\".\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Sun, 13 Jul 1997, Dave Kristol wrote:\n\n> David W. Morris wrote:\n> > \n> > I have a number of remaining concerns with the HTTP State Management\n> > Mechanism draft-02. They range from editorial and clarity to\n> > functional. I'll sumarize the functional concerns and then step\n> > thru the draft with detailed comments and/or suggestions.\n> \n> [I'm still on vacation, so I'll do my best to respond.  Please attribute\n> stupid remarks from me to my brain's not being fully engaged. :-) ]\n> \n> > \n> > Functional Concerns:\n> > \n> >   1.  The cookie/cookie2 interoperational requirement that clients\n> >       clients merge the values from two cookie/cookie2 and servers\n> >       split the values present unnecessary implementation complexity\n> >       and opportunity for errors to conver a transition interval\n> >       and to save small amount of network traffic. More below.\n> \n> I think we've been through this discussion before, and we've rejected\n> the alternatives\n\nWell, there merge rules must be new since 2109 since the requirement stems\nfrom the change to cookie2.  When I reviewed the earlier draft and\nobjected to the rules, I recall two responses, one which supported my\nposition and one which indirectly blamed the requirement on a large\nnetwork service which felt the overhead would be too great. The parent\ncompany has since made it quite clear that they have no intent of\nsupporting cookie2 proposal so their objection would seem moot.  In any\ncase I have a real hard time accepting the overhead argument since the\nduplicate is only required on the first encounter with a client and\nperhaps not even then if the server site cares enough about overhead\nto check the UserAgent string as has become tradition and in fact is \nwell supported by Microsoft's ASP support for sites which use IIS or PWS.\n\n\n\n\n> > \n> >   2.  I see no advantage to requiring the use of a FQHN in all\n> >       cases. It is sufficient to exactly match the host name\n> >       between URL's response which set the cookie and the URL\n> >       candidate for inclusion of the cookie. Intranet applications\n> \n> FWIW, didn't Netscape's original spec. require FQHN?\n\nDon't recall, but in practice the implementations don't require FQHN.\nZooWorks uses cookies quite nicely with NSNav 2, 3, & 4 and IE 2, 3 and\nusually uses just 'zooworks' as the host name.\n\n> \n> Would you accept the case where the host name was a single unqualified\n> word (no dots)?  If so, I'll see if I can work that through the\n> wording.  But the assumption that would follow is that the cookie could\n> return *only* to that host, not to related ones.\n\nMy argument, and my re-wording of the rules below, is that the FQHN\nis not required and will only return to the origin host. There is no\npoint in asserting the no dot rule.\n\nI really don't like unnecessary restrictions.\n\n> \n> >       work quite nicely with host names which are not FQHNs.\n> >       Furthermore, in the intranet context this has the potential\n> >       of requiring a user to type a FQHN to access an application\n> >       which uses Cookies. An unnecessary restriction. We\n> \n> I think the reality is that most cookie-bearing applications get entered\n> by following a link, and I don't think it's unreasonable for the link to\n> have a FQHN.\n\nNot if the application is entered from a bookmark for an Intranet\napplication which never used a FQHN.  If all web pages in the site use\nrelative links and the user starts with something like:\n   http://mint/myap.asp\nthe FQHN will never exist.\n\n> \n> >       have already concluded in discussions relationg to 2068\n> >       and the Host: header that it is impossible to require a UA\n> >       to expand a simple host name into a FQHN.\n> \n> And I wouldn't expect a UA to expand one here, either.\n\nNot true, you demand it.  Otherwise, the UA is in violation of this\nspecification or can't support cookies.\n\n> > \n> >   3.  Requiring the leading period in the domain= value can be\n> >       avoided by simply requiring that the client add the period\n> >       if it isn't provided prior to performing the domain matching.\n> \n> Not so.  I've discussed this several times before (although possibly\n> privately).  The no-dot form of the value (which is specifically not\n> permitted by the spec., but which is the default if Domain= is omitted)\n> restricts a cookie to return to just the origin server.  The with-dot\n> form permits returning the cookie to more than one server.  (All the\n> ones that domain-match the value.) \n\nIf the domain= value is specified and doesn't include the dot, the simple\nfix is for the UA to add the dot ... this is a different situation from\nthe domain= attribute defaulting to the source host. Hence the domain\nmatching rules still hold.\n\n\n> > \n> >   4.  There was a significant amount of interest/support for the\n> >       CommentURL as a better mechanism for communication between\n> >       the server and the user.\n> \n> My take on the discussion was that the opinions were mixed.  Granted\n> there is merit in the proposal, there are also concerns.  You would have\n> to establish rules about cookies in CommentURL, etc.\n> \n> I would like to table CommentURL to a followup to (the followup to) RFC\n> 2109.  (I made no further comments about your CommentURL remarks here.)\n> \n> > \n> > Dave Morris ---------------------->\n> > \n> > Proposed changes and additional comments:\n> > \n> > :: 2. Terminology\n> > [...]\n> > :: The terms request-host and request-URI refer to the values the client\n> > :: would send to the server as, respectively, the host (but not port) and\n> > :: abs_path portions of the absoluteURI (http_URL) of the HTTP request\n> > :: line.  Note that request-host must be a FQHN.\n> >           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete this phrase\n> \n> No.  The domain-matching rules and everything that flows from them\n> depends on FQHN's.  But I'll consider what I mentioned above, namely the\n> case where request-host is a single word (no dots), and I would try to\n> change the words accordingly.\n\nNot true ... with the change I made below, the domain matching rules don't\ndepend on FQHNs and I can't see any possible breach of the intended\nrestrictions in terms of cookie sharing.  In any case, there is absolutely\nno way for a UA to know if a host name which includes a dot is a FQHN or\nnot. The domain matching and reach definition work just fine with\nany three level name or an absolute match for a single host.\n\n\n> \n> > [...]\n> > \n> > :: Host names can be specified either as an IP address or a FQHN string.\n> > \n> > Replace the above line with:\n> >    Host names can be specified in any form acceptable to the base\n> >    HTTP protocol. That may be an IP address, an incomplete host name\n> >    string, or a FQHN string which is the prefered form.\n> \n> No again.  See above.\n\nMy statement stands. The above fits the existing and expected RFC 2068/+.\nDomain matching for support of multiple sites requires FQHNs and I\ndon't propose changing that requirement.  Nor do I propose changing the\ndefinition of FQHNs.  My objection is for basic single host support, this\nspecification makes an unnecessary restriction.\n\n> \n> > \n> > :: Sometimes we compare one host name with another.  Host A's name domain-\n> > :: matches host B's if\n> > \n> > Replace the following two points:\n> > \n> > ::    * both host names are IP addresses and their host name strings\n> > ::      match exactly; or\n> > \n> > ::    * both host names are FQDN strings and their host name strings\n> > ::      match exactly; or\n> > \n> > with:\n> >     * both host name strings match exactly (this applies to IP\n> >       addresses and host name strings which are not FQHNs).\n> \n> Maybe, depending on your response to my question above.\n\nWhile my immediate concern is resolved by a simple name, I can see no\nreason to restrict this comparison to simple dot-less names. Hence, \nsimplicty should be the objective.\n\n> \n> > \n> > ::    * A is a FQDN string and has the form NB, where N is a non-empty\n> > ::      name string, B has the form .B', and B' is a FQDN string.\n> > ::      (So, x.y.com  domain-matches .y.com but not y.com.)\n> > \n> > [...]\n> > :: 3. STATE AND SESSIONS\n> > [...]\n> > \n> > :: There are, of course, many different potential contexts and thus many\n> > :: different potential types of session.  The designers' paradigm for\n> > :: sessions created by the exchange of cookies has these key attributes:\n> > ::\n> > ::   1.  Each session has a beginning and an end.\n> > ::\n> > ::   2.  Each session is relatively short-lived.\n> > ::\n> > ::   3.  Either the user agent or the origin server may terminate a\n> > ::       session.\n> > ::\n> > ::   4.  The session is implicit in the exchange of state information.\n> > \n> > I believe the above paragraphis is an unnecessary elaboration and\n> > furthermore points 1 and 2 are incorrect. Point 2 is in\n> > conflict with examples later in the document when storage of\n> > preferences is discussed. If something like this paragraph remains,\n> > points 1 and two should be more like:\n> >      1.  Each session as an explicit beginning and an end which\n> >          can be approximated based on time and/or user termination\n> >          of user agent execution.\n> >      2.  A cookie based session last over the interval appropriate\n> >          to the context established by the session.\n> \n> I see your point.  Let me think about it.\n> \n> > \n> > [...]\n> > \n> > :: 4.2.2 Set-Cookie2 Syntax\n> > \n> > An earlier comment was made about quotes in the \"Port\" attribute,\n> > but I think there are additional problems with the syntax as\n> > specified and suggest that:\n> > \n> > ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n> > :: port-list       |       1#DIGIT\n> > \n> > be replaced with:\n> > \n> >                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n> >    portnum         =       1*DIGIT\n> > \n> > If I correctly understand RFC2068 syntax, 1#X means 1 or more\n> > occurances of X delimited by commas. My changes fix the \"=\"\n> > in port-list, the 1#DIGIT in port list and make the quotes\n> > optional for the single port case.\n> \n> Given that Port is new, I see no value, and potential risk, in making\n> the quotes optional.  The risk is that someone will forget them when\n> there's a list.\n> \n> And I guess I don't understand what you want to \"fix\" about \"=\".  Except\n> for your allowing a single portnum to omit the quotes, your syntax and\n> mine appear equivalent.\n\nNo, you have a \"|\" where there should be an equal sign and 1#DIGIT means\n1,2,3 not 123 does it not?\n\n> \n> > \n> > To support the CommentURL, add:\n> > \n> >                     |    \"CommentURL\" \"=\" <\"> http_URL <\">\n> > [...]\n> > \n> > [...]\n> > \n> > I think the issue is not so much that a cookie can contain private\n> > information as it can be used to derive private information. Thus\n> \n> Surely both are true, depending on your definition of \"private\".  For\n> example, your username and password at some service (e.g., New York\n> Times) is supposed to be private, but the service will, with your\n> permission, put them in a cookie.\n\nMy point which in the tome might have been missed is that cookies will\noften NOT contain any private information directly.  The major discussion\nhas centered around what a site can learn by using cookies to track\nuser behavior, etc.  I don't deny that they can contain explictly \nprivate data (but so can hidden fields in a form, visable fields, etc.)\nbut think that it is important to mention the information gathering\nexposure as at least an important motivation for our restrictions as\nthe actual possiblity of private content.\n\n> \n> > I think the following:\n> > \n> > :: Comment=comment\n> > ::   Optional.  Because cookies can contain private information about a\n> > ::   user, the Comment attribute allows an origin server to document how\n> > ::   it intends to use the cookie.  The user can inspect the information\n> > \n> > should be changed to read something like:\n> >      Optional.  Because cookies can be used to derive or store\n> >      private information about a user, the Comment attribute allows an\n> >      origin server to document how it intends to use the cookie. ...\n> > \n> > and also to support the CommentURL in the description section, add:\n> > \n> > CommentURL\n> >       Optional.  Because cookies can be used to derive or store\n> >       private information about a user, the CommentURL\n> >       attribute alows an origin server to document how it intends\n> >       to use the cookie.  The user can inspect the information\n> >       identified by the URL to decide whether to initiate or\n> >       continue a session with this cookie.\n> \n> That's only true, of course, if the UA allows you to do so.\n\nThat's true for both Comment and CommentURL.\n\n> \n> > [...]\n> > \n> > :: Domain=domain\n> > ::    Optional.  The Domain attribute specifies the domain for which the\n> > ::    cookie is valid.  An explicitly specified domain must always start\n> > ::    with a dot.\n> > \n> > Change the 'must' in the last sentence to 'should' and add:\n> >   If the domain does not start with a dot, the client MUST prepend\n> >   the dot logically prior to use of the domain value.\n> \n> No.  (See above.)\nYes, ... this is one of those be tolerant about what you receive issues.\nIf the attribute is provided, which is what is discussed in the origina\ntext (\"explictily specified ...\") can easily be fixed by the client.\n> \n> > [...]\n> > \n> > :: 4.2.3 Controlling Caching\n> > [...]\n> > \n> > :: HTTP/1.1 servers must send Expires: old-date (where old-date is a date\n> > :: long in the past) on responses containing Set-Cookie2 response headers\n> > :: unless they know for certain (by out of band means) that there are no\n> > :: upstream HTTP/1.0 proxies.  HTTP/1.1 servers may send other Cache-\n> >                             ^^^^ insert:\n> >          or that any upstream proxies will not cache the response.\n> > \n> > :: Control directives that permit caching by HTTP/1.1 proxies in addition\n> > :: to the Expires: old-date directive; the Cache-Control directive will\n> > :: override the Expires: old-date for HTTP/1.1 proxies.\n> > \n> > For example, is it has been fairly well accepted that HTTP/1.0\n> > proxies will not cache responses to POST requests or requests which\n> > include a '?' part.\n> \n> The problem with that is those rules are heuristics that have been\n> implemented by some proxies.  They are not mandated by any (HTTP/1.0)\n> spec., so you can't depend on them.  All you really (might) know is\n> their HTTP version number.  As a practical matter I think it's unlikely\n> for an originserver to know there are no upstream proxies, and it will\n> have to send Expires anyway.\n\nAs a practical matter, the key is 'out of band' and it applies not to\nthe existance of a flavor of proxy but to whether caching will occur.\nBrowsers screw up their history lists bad enough w/o expires being\nadded to what they see. With my words the implementor who bothers to \nread has been alterted to the controlling caching issues and must make\na judgement call.  For example, if the cookie only carries a session-id\nwhich must also be synchronized with an IP address to be valid, there is\nlittle risk if the wrong user receives the cookie.\n\n\n> \n> > \n> > [...]\n> > :: 4.3 User Agent Role\n> > \n> > :: Port   The default behavior is that a cookie may be returned only to\n> > ::        its request-port.\n> > \n> > I think the correct default for Port is:\n> > \n> >    Port   If the Port attribute is not specified, there are no\n> >           port restrictions for subsequent use of the cookie.  If the\n> >           Port attribute is specified without a value, the cookie\n> >           may only be sent in a request to the same port that it\n> >           was received on.\n> \n> No.  I believe I wrote what was the consensus of the comments about Port\n> when it was first discussed.  Furthermore, I asked for concurrence on\n> the words when I first issued an I-D that describe Port, and I got it\n> (through an absence of objections).\n\nPlease re-read my words ... basically I just made this statement match\nthe later words in the document about Port.  There are three states:\n    No attribute; Port w/o a number; and port with numbers.\nThe existing draft words here and later don't agree.  See starting at\nline 606 in the draft discussing port selection.\n\n> \n> > [...]\n> > \n> > :: 4.3.2 Rejecting Cookies\n> > [...]\n> > ::    * The value for the Domain attribute contains no embedded dots\n> > ::      or does not start with a dot.\n> >         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  delete the phrase\n> > \n> > ::    * A Set-Cookie2 with Domain=ajax.com will be rejected because the\n> > ::      value for Domain does not begin with a dot.\n> > \n> > The above example should be deleted or replace with something like:\n> >       * A Set-Cookie2 with Domain=ajax.com will have a dot prepended\n> >         to value for Domain because it does not begin with a dot.\n> \n> No.  That would mean something different.  See above.\n\nWell, in the end we either agree that my proposal that the UA fix up\na bad domain= value or not. I tried to cover all required edits to make\nthe document match up. If after my latest comments no-one else cares and\nyou still disagree, I don't consider this a very big deal just a nit where\nwe could tolerate poor coding to no harm done.\n\n> > \n> > [...]\n> > :: 4.3.3  Cookie Management  If a user agent receives a Set-Cookie2\n> > :: response header whose NAME is the same as a pre-existing cookie, and\n> >                   ^^ insert 'value with a cookie'\n> > :: whose Domain and Path attribute values exactly (string) match those\n> \n> I sort of see what you're trying to say (especially given your next\n> comment), but I don't think the words are necessary.  A Set-Cookie2\n> logically takes a single NAME, even though multiple Set-Cookie2's could\n> be folded together.\n\nI suppose.\n\n> > \n> > A single set-cookie2 response header can contain multiple cookies.\n> \n> Redundant.  It's implied by RFC 822 header folding.\n> \n> > \n> > [...]\n> > :: Because user agents have finite space in which to store cookies, they\n> > :: may also discard older cookies to make space for newer ones, using, for\n> > :: example, a least-recently-used algorithm, along with constraints on the\n> > :: maximum number of cookies that each origin server may set.\n> >                  ^^ insert 'or size'\n> > \n> > :: If a Set-Cookie2 response header includes a Comment attribute, the user\n> > :: agent should store that information in a human-readable form with the\n> > :: cookie and should display the comment text as part of a cookie\n> >           ^^^ or\n> > :: inspection user interface.\n> > \n> > The critical issue is that the Comment be made available to the user.\n> > With the 'and' included, the specification makes an unnecessary\n> > implementation restriction. How it is stored is not important unless\n> > poking around in the storage area is the only mechanism the user\n> > has for examination of the cookie.\n> \n> I take your point.  However, the second part is a \"should\", and what\n> constitutes a \"cookie inspection user interface\" is intentionally\n> unspecified so as not to \"[make] an unnecessary implementation\n> restriction\".  If we make this an \"or\", a UA could (as some already do)\n> bury the cookie in some obscure place where it's hard for a user to\n> find.\n\nMy intent was that they either provide a cookie inspection interface OR\nmake it human readible.  As written the draft requires it be human\nreadible, not human findable. \n\n>  \n> > \n> > To support CommentURL, the following should be added:\n> >   If a Set-Cookie2 response header includes a CommentURL attribute,\n> >   the user agent should either store the value in a human-readable\n> >   form with the cookie or preferably allow the user to link to the\n> >   URL as part of a cookie inspection interface.\n> > \n> > :: 4.3.4 Sending Cookies to the Origin Server\n> > [...]\n> > \n> > :: The value of the cookie-version attribute must be the value from the\n> > :: Version attribute of the corresponding Set-Cookie2 response header.\n> > :: Otherwise the value for cookie-version is 0.  The value for the path\n> > :: attribute must be the value from the Path attribute, if any, of the\n> > :: corresponding Set-Cookie2 response header.  Otherwise the attribute\n> > :: should be omitted from the Cookie request header.  The value for the\n> > :: domain attribute must be the value from the Domain attribute, if any,\n> > :: of the corresponding Set-Cookie2 response header.  Otherwise the\n> > :: attribute should be omitted from the Cookie request header.\n> > \n> > I read the above to make sending of the $path and $domain values\n> > on a cookie: response header as optional.  To me it says, if the\n> > $path and $domain values are sent, then be the values in the\n> > set-cookie2 response header. Is this what is intended? Based on\n> > later examples, I would have expected it be required that they\n> > be returned if set.\n> \n> I'll have to clarify that.  What's meant is that if the UA receives\n> those attributes, it must return them.  If it uses the defaults because\n> they were omitted in Set-Cookie[2], it should not return them. The idea\n> is for the server to get reflected back to it what it sent to the UA.\n\nThat is what I figured you meant.  I think that is the better alternative.\n\n> > \n> > [...]\n> > \n> > ::     2.  If the attribute is present but has no value (e.g., Port),\n> > ::         the cookie must only be sent to its request-port.\n> >                                            ^^^^^^^^^^^^^^^^^\n> >        would be clearer I think if replaced with:\n> >            \"the request-port it was received from.\n> \n> Okay.\n> \n> > \n> > 4.3.5 Sending Cookies in Unverifiable Transactions\n> > [...]\n> > :: However, even this would not make all links verifiable; for example,\n> > :: links to automatically loaded images would not normally be subject to\n> >                                        ^^^ insert \"or other objects\n> >                                                    such as frame\n> >                                                    content\"\n> \n> Unfortunately, content magic is outrunning the speed with which we can\n> get out the spec. :-)  I think (hope) that just mentioning images gets\n> the point across.  Otherwise it begins to look like we're trying to give\n> an exhaustive list of unverifiable transactions, which is impossible.\n\nI agree about the exhaustive concern, but frames are such an obvious \nplace for abuse I thougt it might be worth mentioning them but then I'm\na lot less concerned overall about potential abuse so I don't care.\n\n> \n> > :: ``mouse pointer'' verification.\n> > \n> > :: Many user agents also provide the option for a user to view the HTML\n> > :: source of a document, or to save the source to an external file where it\n> > :: can be viewed by another application.  While such an option does provide\n> > :: a crude review mechanism, some users might not consider it acceptable\n> >                              ^^^^^^^^^^^^^^^^\n> > :: for this purpose.\n> > \n> > That is the understatement of the year .... I would prefer words like\n> >                              \"many users would\"\n> > \n> > :: 4.5  Caching Proxy Role\n> > [...]\n> > :: Proxies must not introduce Set-Cookie2 (Cookie) headers of their own\n> >            ^^^^ I would prefer 'should'\n> > :: in proxy responses (requests).\n> \n> Given that cookies are an end-to-end mechanism and controlled by an\n> origin server (supposedly), it's hard for me to imagine a valid case\n> where a proxy could introduce a Set-Cookie2.  Now proxy cookies are\n> another matter....\n\nWell I once wrote a long explanation about how a very clever proxy \nmight be able to combine cookies with an extended authentication scheme\nby tacking an ID set-cookie on every response. Don't know how good \nan idea it was of if it would work but on the principal that I \ncould see no harm in having a proxy do something with cookies, I figured\nthat should was a strong enough restriction.\n\n> > \n> > :: 5. Examples\n> > \n> > It looks to me like the exmaples are WRONG ... the syntax requires\n> > the version prior to the cookie name/value. (This also implies\n> > the historical rules for merging cookie and cookie2 are wrong).\n> \n> I don't think so.  (Granted I only made a quick scan of the examples.) \n> The Version attribute in Set-Cookie[2] can be in any order after\n> NAME=VALUE.  $Version must be first in the Cookie header.  I think the\n> examples consistently do that.  And I think the cookie merging rules\n> work.\n\nHmmm... perhaps it was my bleary eyes and reading set-cookieX as cookie.\nThe merging examples seem to assume version= as the last av pair and\nperhaps that is the precise intent of the syntax but I didn't see it \nstated anywhere and it is generally traditional when av kinds of pairs\nhave names that order may not be significant.  I certainly don't read\nan order requirement into the diagrams ... but I could be wrong on that\none.  Anyway now I don't see a problem with the $version positioning.\nSorry.\n\n> \n> > \n> > :: 6. IMPLEMENTATION CONSIDERATIONS\n> > [...]\n> > :: 6.1 Set-Cookies Content\n> > [...]\n> > \n> > :: The session information can obviously be clear or encoded text that\n> > :: describes state.  However, if it grows too large, it can become\n> > :: unwieldy.  Therefore, an implementor might choose for the session\n> > :: information to be a key to a server-side resource.\n> > \n> > The above is fine and sufficient.  The following comment asserts\n> > a motivations for this specification which has not been established\n> > and which I would disagree with. Delete the following ....\n> > \n> > ::                                                    Of course, using\n> > :: a database creates some problems that this state management\n> > :: specification was meant to avoid, namely:\n> > ::\n> > ::   1.  keeping real state on the server side;\n> > ::\n> > ::   2.  how and when to garbage-collect the database entry, in case the\n> > ::       user agent terminates the session by, for example, exiting.\n> \n> They're only implementation *considerations*.  Would you claim that 1\n> and 2 are false?  We're pointing out the trade-off:  you can either have\n> complete state sent back and forth between origin server and UA, or you\n> can pass a handle back and forth.  But if you do the latter, there are\n> other considerations that come into play. Doing the former, where\n> possible, seems easier to me.\n\nWell, its hard for me to know everyone's motivations for this\nspecification but I consider it bad design to pass bunches of stuff\nback and forth ... performance, security implications, etc.\n\nIf you want to get into implemenation considerations, there are is a log\nmore that could be said.\n\nMy objection isn't really to statements 1 and 2 but rather the\ndeclaration of intent for the specification. If you feel a need to say\nmore than something like:\n    Of course, using a database introduces the problem of how to manage\n    the persistence of server side state information.\nHaving written or designed multiple applications before and since cookies\nwhich used the handle approach, I don't see it as a very big deal.\n\nI see the motivation as being elimination of munged URLs and hidden form\ndata as being the primary motivation for this specification.\n\nFor example, with ASP on Microsoft IIS and PWS, complete provisions\nare builtin for managing server side state, timing it out, etc. \n\n> > \n> > [...]\n> > :: 6.3 Implementation Limits\n> > [...]\n> > \n> > Either delete the following completely or replace it. Dictating\n> > that such a device dedicate 80K+ of storage to cookies is not\n> > reasonable.\n> > \n> > :: User agents created for specific purposes or for limited-capacity\n> > :: devices should provide at least 20 cookies of 4096 bytes, to ensure\n> > :: that the user can interact with a session-based origin server.\n> > \n> > Calling something a limited capacity device, we have acknowledged\n> > that we don't know its capabilities or its intended audience. I\n> > don't advocate the following but could tolerate it if we are\n> > compelled to say anything:\n> > \n> >    User agents created for specific purposes or for limited-capacity\n> >    devices should provide sufficient cookie storage capacity to\n> >    address the requirements of the applications they are intended\n> >    to serve.\n> \n> I see your point, and I guess that's okay.  The purpose of the section\n> was to set out some bounds so (unrelated) origin servers and UAs would\n> be able to depend on certain minima.  I grant that special-purpose\n> agents tailored to a single application can cut things closer to the\n> bone.  But of course the \"should\" already lets them do so.\n> \n> > \n> > [...]\n> > :: 7.1  User Agent Control\n> > [...]\n> > \n> > The following is implementation advice which can be dropped. Surely\n> > we need not instruct UA authors about data integrity.\n> \n> I suppose, but it doesn't hurt anything.\n> \n> > \n> > :: NOTE: User agents should probably be cautious about using files to store\n> > :: cookies long-term.  If a user runs more than one instance of the user\n> > ::agent, the cookies could be commingled or otherwise corrupted.\n> > \n> > [...]\n> > :: 10.  HISTORICAL\n> > :: 10.1  Compatibility with Existing Implementations\n> > \n> > As I stated in my introduction, I feel quite strongly that the\n> > proposed handling of old and new cookies is wrong. I shall attempt\n> > to proposed edits which will make this section fit my\n> > recommendations.\n> \n> You lost me.  Which recommendations?  Oh, you mean to send complete\n> Set-Cookie and Set-Cookie2.  No, we've chosen not to do that.\n\nWhich choice I don't accept.\n\n> \n> > [...]\n> > Rewrite:\n> > :: An origin server that supports user agents that are compatible both\n> > :: with Netscape's original proposal and this one must, for a transition\n> > :: period, send two response headers.  Set-Cookie contains the ``old''\n> > :: cookie information.\n> > ::                     Set-Cookie2 contains the cookie information that\n> > :: is new to this specification.  The rules below ensure that the two\n> > :: pieces get combined correctly.\n> > ::                                Eventually, when the majority of user\n> > :: agents follow this specification, the Set-Cookie response header can\n> > :: be phased out, and all cookie information can be carried in the\n> > :: Set-Cookie2 response header.\n> > \n> > by replacing the broken out middle section with\n> >                        Set-Cookie2 contains the the same cookie\n> >           information expressed according to this specification.\n> \n> Sorry, but that proposal was considered and rejected many months ago.\n\nIn the only discussion I saw on this list it was two for and 1 negative \nexpressed represting a third party I never saw speak on the issue. \n\n> > \n> > Then replace:\n> > :: Once a server receives a new cookie from a client, it may continue a\n> > :: session by sending only Set-Cookie2 response headers.\n> > \n> > with:\n> >    New version of user agents which had previously stored cookies\n> >    based on Netscape's original specifications should convert\n> >    existing cookies by adding the $Version attribute with a value\n> >    of 0 to signal to the server the ability to handle Set-cookie2\n> >    format cookies.\n> > \n> >    Once a session is initiated during the transition period using\n> >    both set cookie headers in the first response to a user-agent\n> >    the server will know from the cookie header received in the\n> >    next request which form of cookies the user agent supports.\n> >    The server should only send either the Set-cookie2 if it is\n> >    supported or the set-cookie if Set-cookie2 is not supported on\n> >    subsequent responses which require an updated cookie value.\n> > \n> > Then delete sections \"10.1.1 Combining Set-Cookie and Set-cookie2\"\n> > and \"10.1.2 An Example\".\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-negotiation02.tx",
            "content": "At 07:59 PM 7/14/97 +0200, Koen Holtman wrote:\n\n>>But on reflection and partial re-reading of the draft I have formed the\n>>idea that the features used by TCN are identified by virtue of appearing in\n>>an 'Alternates' header.  But the description of 'Alternates' suggests that\n>>this understanding is, at best, incomplete.\n>\n>I think you are confusing the features _of_ TCN (i.e. the TCN protocol\n>elements) with the feature tags used by TCN here, but I am not sure.\n\nI think I'm getting the idea.  If I understand correctly, it would not, in\ngeneral, be possible for some non-TCN negotiation scheme to provide\ninformation in the 'Alternates:' header also used for a TCN resource response.\n\n>>I've constructed myself a little graph showing the relationships between\n>>the various headers and feature-related syntax productions.  Have I missed\n>>anything vital here?\n>\n>No, this looks about right, though I would add\n>\n>  feature-set --> ftag\n\nI cannot find a syntax production for feature-set in your draft.  I would\njudge that this concept is \"inlined\" in the 'Accept-features:' header\ndescription.\n\n>>  'Accept-features:' --> feature-expr --> ftag\n>>\n>>  'Alternates:' --> variant-description -->\n>>                    variant-attribute-list -->  )  feature-list\n>>  'Content-features:'                      -->  )\n>>   \n>>  feature-list --> fpred --> ftag\n>>\n\n>[...]\n>>My own thinking about the issues of content negotiation (posted to the\n>>HTTP-WG list) leads me to believe that the process should be performed\n>>within a symmetric framework (at least insofar as the identification of\n>>negotiable features is concerned).  Therefore I find myself questioning the\n>>asymmetry in your proposal.\n>\n>See my response to your message `Content negotiation requirements'.\n>Differences between feature-expr and fpred are not a flaw in the\n>symmetry of TCN, they are symptoms of its fundamental asymmetry.\n\nOK, I'm still not convinced, but that leaves the ball in my court.\n\nI think it would be possible to construct a symmetric framework which can\nbe subsetted (asymmetrically) down to your proposed framework for\ndeployment within HTTP, and hence I believe should overcome your concern\nregarding complexity for HTTP content authors.\n\nWhen time permits, I shall attempt to construct a proposal to address this\nissue.  That will be fairly close to the top of my priority list, but I've\na vacation coming up so it could be a while.\n\n>>* Section 5.7:\n>>\n>>I think the reference to \"new dimensions\" of negotiation contradicts\n>>section 4.7.\n>\n>I'm not sure what you mean here, I see no contradiction.  The `future\n>specifications' do not need to be specifications of TCN.\n\nSorry!  I should have said 4.8, not 4.7, in which you say \"really a\nsub-dimension of the feature dimension\".  I understand the\n'extension-token' of 5.7 to introduce such a *sub*-dimension.\n\nThis is a really trivial matter.  I think it's the 'accident of history'\n(ref some previous message) which tends to muddy the waters here.\n\n>>* Section 6.3, 1st para:\n>>\n>>This implies that a feature predicate can exist *only* in the context of a\n>>specific request.\n>\n>?? I don't read it as implying that, but I'll change it to:\n>\n>   Feature predicates are predicates on the contents of feature sets.\n\nFine!  That certainly removes the implication which I felt was created by\nbinding the statement to 'the current request'.\n\n>>* Section 6.4:\n>>\n>>I assume that true-improvement < 1 or false-degradation > 1 are permitted?\n>\n>Yes.  This will make life easier for some automatic predicate\n>generators.\n\nOr simply cases where thr truth of an 'fpred' actually represents a\ndegradation in quality, or vice versa.\n\n>>* Section 8.4:\n>>\n>>Are there any circumstances in which a response from a transparently\n>>negotiable resource is not required to include an 'Alternates:' header?\n>\n>Yes.  If the response is an error, list or adhoc response, Alternates\n>need not be included.\n\nAha!  So Normal and Choice responses containing a transparently negotiated\nresource are required to carry an 'Alternates' header?\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Re: New feature tag registration drafts availabl",
            "content": "At 08:00 PM 7/14/97 +0200, Koen Holtman wrote:\n>>> draft-ietf-http-feature-scenarios-00.txt \n>>>     `Feature Tag Scenarios' \n>>>     Discusses why we need feature tag registration.\n\n>>* Section 4.3, item t+2.5:\n>>\n>>This implies another requirement:  it must be possible for content authors\n>>to specify feature tags and associated values in authored content (this has\n>>possible implications for representability within HTML, for example).\n>\n>Yes.  This is why < and > are not allowed in feature tags, and why\n>feature predicates have the syntax paper!=A4, not paper<>A4.\n\nIsn't there a conflict here with <draft-mutz-http-attributes-02.txt>?\n\n>>* Section 4.4, para 2:\n>>\n>>I think there is a potential problem more pernicious than 'dead' tags,\n>>etc., which is use of different tags by different vendors to mean the same\n>>thing.  This could cause unecessary negotiation failures -- maybe some\n>>alias mecchanism could be defined within the registration procedures?\n>\n>TCN takes the viewpoint that, in the end, it is up to the content\n>authors do decide whether A and B are interchangeable in a particular\n>case at hand.  See section 19.1 in draft-ietf-http-negotiation-02.txt.\n>Like with dead tags, I do not assume that content authors will directly\n>go to the registry, but that there will be third parties who make\n>lists of useful aliases, describing for each alias the necessary\n>boundary conditions under which the alias relationship holds.\n\nOK -- I accept that an alias mechanism is probably not a Good Idea.\n\nI think your comment suggests a possible requirement on a generic\nnegotiation framework is the ability to treat some set of features as\ninterchangeable in the context of some specific negotiation exchange, as\nyour 'fpred-bag' does for  indicating the quality of a variant.\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-feature-reg01.tx",
            "content": "At 08:01 PM 7/14/97 +0200, Koen Holtman wrote:\n\n>>* Section 3.8:\n>>\n>>(A) In \"Summary of the indicated dimension of negotiation\", I am\n>>uncomfortable with your use of the term 'dimension' in this context.  I\n>>think what you are describing is the range of values associated with a\n>>dimension.\n>\n>The trouble is that some dimensions can be described nicely in terms\n>of their values, while for other dimensions, where the values are yes\n>and no, a description in terms of the `meaning' of these values must\n>be given.\n>\n>I plan to change \"Summary of the indicated dimension of negotiation\"\n>to \"Summary of negotiation with this feature tag\", or something.  I am\n>not sure I want to use the word `dimension' in the form.\n\nI think that approach would be consistent with the motivations indicated in\nyour response to my point (B).\n\n>>Underlying this comment is my feeling that I don't think sufficient\n>>distinction is drawn between identification of a dimension of negotiation\n>>(the feature tag) and the values which are associated with that dimension\n>>(feature values?).\n>\n>Do you mean in the form or in the text around it?\n\nA bit of both.\n\nI think the word 'dimension' gets a bit overloaded in the draft.\n\n>>I would suggest:\n>>\n>>(1) A yes/no choice\n>>(2) A choice of one value from a finite enumeration of (possibly numeric)\n>>values\n>>(3) A choice of multiple values from a finite enumeration of values\n(powerset)\n>>(4) A value selected from a finite or infinite range of some scalar type\n>>(integer, real)\n>>(5) A compound value (e.g. MIME Content-type, range of numeric values)\n>>\n>>[I also note that these values relate to a specific message which is\n>>transferred: the feature negotiation mechanism would have to deal with\n>>multiple values for any of these value range types.]\n>\n>?? If there were not multiple values to choose from, there would be no\n>negotiation.\n\nI phrased that badly.  I was trying to indicate the ability to specify\nmultiple instances of each value.  E.g.\n(1)  {Yes,No}\n(2)  {1,2,3,5,7,11} or {Red,Green,Blue}\n(3)  {{1},{1,5},{1,5,11}}\n(4)  {1.1,3.3,7.7,100}\n(5)  {text/plain,text/html,application/word} or {[1.5-2.5],[2.0-20.0]}\n\nAnother thought: for item (4) should there be a distinction between\nopen/closed intervals?\n\n\n\n\n>Here is a rewrite of the first part of the form, please say if you\n>like this better.  Note that I'm trying to avoid the `dimension'\n>terminology.\n>\n>3.8 Registration template\n>\n>   To: ietf-feature-tags@iana.org (Feature tags mailing list)\n>        (or directly to iana@iana.org)\n>   Subject: Registration of feature tag XXXX\n>\n>    | Instructions are preceded by `|'.  Some fields are optional.\n>\n>   Feature tag name:\n>\n>   Summary of negotiation with this feature tag\n>\n>    | Include a short (no longer than 4 lines) description or summary\n>    | Examples:\n>    |   `Negotiation on whether to use the xyzzy feature of ...'\n>    |   `Negotiation on the MIME media type of the data which\n>    |    is transmitted for ...'\n>    |   `Negotiation on whether to use colors in displaying ...'\n>    |   `Negotiation on the number of colors to use in displaying ...'\n>\n>   Number of alternatives in (sub)negotiation with this tag:\n\n?? (sub)negotiation -- I don't know what you mean by that.\n\nHow about:\n\"Number of alternative values for the feature identified by this tag\"?\n\n>\n>     [ ] 1. Two alternatives\n>     [ ] 2. More than two alternatives\n>\n>   For case 1: nature of the two alternatives:\n>\n>     [ ] 1a. A particular feature is used/invoked/enabled, or not\n>     [ ] 1b. Other\n>\n>   For case 2: Is there a natural way to identify every possible\n>               alternative?\n>\n>     [ ] 2a. Yes\n>     [ ] 2b. No\n>\n>   For case 2a: How is a single alternative naturally identified?\n>\n>     [ ] 2a.1 With a name, keyword, label, or tag (e.g. a language tag)\n>     [ ] 2a.2 With an integer value\n>     [ ] 2a.3 With a numeric value of a non-integer type (e.g. float)\n>     [ ] 2a.4 With a non-numeric (non-scalar) value (e.g. a piece of code)\n>     [ ] 2a.5 With a record or tuple of values\n>     [ ] 2a.6 With a set of values\n>     [ ] 2a.7 Other\n>\n\n2a.2: bounded/unbounded alternatives?  I am thinking that there might be\nknown limits on the value of an integer used as an enumerated value.  I am\nnot saying this is a Good Idea, just asking the question.\n\n2a.3: bounded/unbounded alternatives (e.g. real numbers <= 1.0)?.  Again,\nI'm just asking the question.\n\n2a.4, 2a.7: don't these amount to the same thing?\n\n2a.5, 2a.6: to provide complete coverage of structured types, union should\nbe included.\n\nGiven that this is not intended to be used by programmers, I would suggest\ncollapsing 2a.4..2a.7 into a single 'other' category (possibly singling out\n2a.6 as a 'common' case).\n\n\n\nTo answer your question, I like the earlier paragraphs better than the\noriginal, but I am uneasy about the technical depth implied by the\nalternatives for 2a.  If you follow this approach I do not think there is a\nsingle correct answer -- just a judgement as to what is appropriate.\n\nMaybe the summary of negotiable features I posted a while back could\nprovide a yardstick for what should be explicitly offered?  I don't think\nthere is a reason why the form should not be extended later if new feature\nvalue types start occurring frequently in the future (as this would simply\nrepresent a refining of the 'other' category, not the addition of new\ninformation).\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "New version of PEP specification availabl",
            "content": "A new PEP draft is ready - I have submitted it as an ID today and also as\nan W3C WD. You can find it in both txt, html, and ps as\n\nhttp://www.w3.org/TR/WD-http-pep-970714.txt\nhttp://www.w3.org/TR/WD-http-pep-970714.html\nhttp://www.w3.org/TR/WD-http-pep-970714.ps\n\nYou can also find information about PEP at\n\nhttp://www.w3.org/Protocols/PEP/\n\nThanks for the input on the previous PEP - you know who you are!\n\nThe main new feature is that the mode of \"long lived\" mappings no longer is\nthere as it was too hard to get to work through a proxy. A PEP extension\ndeclaration is now for a single message, which makes all extended messages\nunderstandable by any other PEP agent.\n\nAnother new feature is that the mapping of PEP into HTTP/1.1 has been made\nexplicit so that it is clear what is part of the PEP model and what is part\nof the mapping.\n\nEric Prud'hommeaux will be able to give out some demo code shortly which\nwill demonstrate the functionality of PEP.\n\nPlease read and comment! I will try and get input incorporated into the\ndraft before the July 30 dead line for submissions for the Munich IETF\nmeeting.\n\nhttp://www.ietf.org/meetings/Munich.html\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "input on HTTP for the year 2000 working grou",
            "content": "At the April IETF meeting I volunteered to coordinate input for the\nyear 2000 working group on the various \"information services and file\nmanagement\" standards.\n\nThanks to Jeffrey Mogul <mogul@pa.dec.com> who started this\ndiscussion off.  See\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0050.html\nand the subsequent discussion.  Sorry it has taken me so long to respond.\n\n\nUsing local data I have confirmed his results that about 20% of the\nserver responses sent thru a proxy server currently use RFC850 dates\nwith two-digit year fields.\n\nHere is some preliminary text for the draft that the 2000 working\ngroup is doing.  If nothing else I would like people to think a bit\nmore about some of the scenarios for what might go wrong based on\nthese ambiguous dates.  I suggest some possible minor additions to the\nadvice in RFC2068 to avoid unnecessary delay and bandwidth caused by\nunnecessary requests.\n\n\nCheers,\n\nNeal McBurnett <nealmcb@bell-labs.com>  503-331-5795  Portland/Denver\nBell Labs / Lucent Technologies\nhttp://bcn.boulder.co.us/~neal/      (with PGP key)\n\n----------\nInformation Services\n\nHTTP\n\nThe main IETF standards-track document on the HTTP protocol is RFC2068\non HTTP 1.1.  It notes that historically three different date formats\nhave been used, and that one of them uses a two-digit year field.  In\nsection 3.3.1 it requires HTTP 1.1 implementations to generate this RFC1123\nformat:\n\n     Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123\n\ninstead of this RFC850 format:\n     Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036\n\nUnfortunately, many existing servers, serving on the order of\none fifth of the current HTTP traffic, send dates in the ambiguous\nRFC850 format.\n\nSection 19.3 of the RFC2068 says this:\n\n  o  HTTP/1.1 clients and caches should assume that an RFC-850 date\n     which appears to be more than 50 years in the future is in fact\n     in the past (this helps solve the \"year 2000\" problem).\n\nThis avoids a \"stale cache\" problem, which would cause the\nuser to see out-of-date data.\n\nBut to avoid unnecessary delays and bandwidth indicated in Scenario 2\nbelow, this should be extended to say that a date which appears to be\nmore than 50 years in the past may be assumed to be in the future, if\na future date is legal for that field.\n\nScenario 3 indicates that servers may also want to follow these rules.\n\n\nHere is some more background and justification for these arguments.\n\nThe following headers use full dates:\n\nHTTP/1.0:\nDate:\nExpires:# can be in the future\nIf-Modified-Since:# required to be in the past\nLast-Modified:# required to be in the past\nRetry-After:# can be in the future, also takes\n# relative time - number of seconds\n\nHTTP/1.1:\nIf-Range:\nIf-Unmodified-Since:# required to be in the past\n\nNote that clock skew between hosts can lead to confusion here - see\nthe RFC for details.\n\nHere are some scenarios of the implications of RFC850 dates, which\ninclude stale caches, unnecessary requests for things which are\nvalidly cached, delays for the user, extra bandwidth, and presenting\nincorrect information to the user.\n\nSome cases involve comparisons with the current time, and others may\ninvolve comparisons between dates from different sources.  The\nabbreviation \"/99\" is used to imply an RFC850 date with the value \"99\"\nfor the year.\n\n\nRFC850 date from server\n\nScenario 1:\nIf a client gets an Expires /99 date after the year 2000, it\nshould interpret it as 1999, to avoid ending up with a stale\ncache entry.\n\nThis is as already specified in RFC2068.\n\nScenario 2:\nIf a client gets an Expires /00 date before the year 2000, and\nsubsequently is faced with a choice to either retrieve the\ndocument from its cache or look for an updated copy, it may\ninterpret it as the year 2000, to avoid the unnecessary delay\nand bandwidth of an extra request.\n\n\nRFC850 date from client\n\nScenario 3:\nIf a server gets an If-Modified-Since /99 date from a client\nafter the year 2000, it should interpret it as 1999 when\ncomparing with the local modification date, in order to\npossibly avoid sending a full GET response rather than a HEAD\nresponse.\n\nNote that an If-Modified-Since header must never be in the\nfuture.\n\n\n(Note that there has been a lot of discussion about whether\nIf-Modified-Since should really be compared with \"less than\" semantics\nor whether the date should be considered a 'token' which is required\nto exactly match the Last-Modified date in order to avoid a full GET\nresponse.  There have been bugs and problems with implementations that\ndeal badly with dailight savings time, etc.  I'm not sure how this\ndebate dealt with the question of how to convert the dates, or whether\nservers can actually use \"less-than\" semantics.  Input is\nrequested....)\n\n\n\nDates in HTML\n\nIn RFC1866, on HTML 2.0,the <META> tag allows the embedding of\nrecommended values for some HTTP headers, including Expires.  E.g.\n\n    <META HTTP-EQUIV=\"Expires\"\n          CONTENT=\"Tue, 04 Dec 1993 21:29:02 GMT\">\n\nServers should rewrite these dates into RFC1123 format if necessary.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "\"David W. Morris\" <dwm@aimnet.com> wrote:\n>On Sun, 13 Jul 1997, Dave Kristol wrote:\n>\n>> David W. Morris wrote:\n>> > \n>> > I have a number of remaining concerns with the HTTP State Management\n>> > Mechanism draft-02. They range from editorial and clarity to\n>> > functional. I'll sumarize the functional concerns and then step\n>> > thru the draft with detailed comments and/or suggestions.\n>> \n>> [I'm still on vacation, so I'll do my best to respond.  Please attribute\n>> stupid remarks from me to my brain's not being fully engaged. :-) ]\n>> \n>> > \n>> > Functional Concerns:\n>> > \n>> >   1.  The cookie/cookie2 interoperational requirement that clients\n>> >       clients merge the values from two cookie/cookie2 and servers\n>> >       split the values present unnecessary implementation complexity\n>> >       and opportunity for errors to conver a transition interval\n>> >       and to save small amount of network traffic. More below.\n>> \n>> I think we've been through this discussion before, and we've rejected\n>> the alternatives\n>\n>Well, there merge rules must be new since 2109 since the requirement stems\n>from the change to cookie2.  When I reviewed the earlier draft and\n>objected to the rules, I recall two responses, one which supported my\n>position and one which indirectly blamed the requirement on a large\n>network service which felt the overhead would be too great. The parent\n>company has since made it quite clear that they have no intent of\n>supporting cookie2 proposal so their objection would seem moot.  In any\n>case I have a real hard time accepting the overhead argument since the\n>duplicate is only required on the first encounter with a client and\n>perhaps not even then if the server site cares enough about overhead\n>to check the UserAgent string as has become tradition and in fact is \n>well supported by Microsoft's ASP support for sites which use IIS or PWS.\n\nFor what it's worth, here's some \"implementation experience\" on\nthat.  The draft has a parenthetical statement:\n\n\"(Obviously the Set-Cookie and Set-Cookie2 headers must\n  both contain the same number of cookies.)\"\n\nIt might be interpreted as a requirement that they MUST be the same\nnumber, but not necessarily.  The draft's examples all have Set-Cookie2\nand Set-Cookie cookies in the same order, but there is no clear MUST\nabout that either.  The Lynx implementation does not depend on either\n(not because of anything in or not in the draft, but because I personally\nwouldn't assume they'll always be the same number and in the same order,\neven if they MUST be).\n\n\n>> >   4.  There was a significant amount of interest/support for the\n>> >       CommentURL as a better mechanism for communication between\n>> >       the server and the user.\n>> \n>> My take on the discussion was that the opinions were mixed.  Granted\n>> there is merit in the proposal, there are also concerns.  You would have\n>> to establish rules about cookies in CommentURL, etc.\n>> \n>> I would like to table CommentURL to a followup to (the followup to) RFC\n>> 2109.  (I made no further comments about your CommentURL remarks here.)\n\nThe Lynx implementation includes handling of a commentURL\nattribute.  It addresses the criticism, otherwise not addressed, that\nthe comment attribute, by itself, does not accommodate i18n concerns.\nIf the comment also can be sought via a URL, charset and language\nnegotiation can be brought into play.  The Canadian freenets probably\nare still the largest Lynx user base (well into the six digits) and\nwhether the comment is in English or French is an important factor\nthere.  It's also an important matter for our CJK user base.  Could\nthe RFC at least designate commentURL as a reserved name (not to be used\nas a cookie name)?  It also might be helpful to state explicitly in the\ncomments/explanations that use of expires as a Version 1 cookie name,\nor max-age, version, port, and discard as Version 0 cookie names, is\nill-advised (I know, \"That goes without saying!\", and those to whom it\nshould be said probably won't ever read the RFC. :)  While tabled, I\nguess sites which want to use it can do a User-Agent check for Lynx\n(and any other browsers which implement it).\n\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "cachebusting doc as RFC ",
            "content": "-----BEGIN PGP SIGNED MESSAGE-----\n\n\nWhat's the feeling here about putting out (some future version of...)\nthat document about cache-busting as an RFC ?\n\nIt could definitely benefit from some peer review, which is one \nthing.  Would it be a useful community resource, though ?\n\nCheerio,\n\nMartin \n\nPS Whether or not it should be an http-wg work item is another \nmatter :-)\n\n\n\n-----BEGIN PGP SIGNATURE-----\nVersion: 2.6.3i\nCharset: noconv\n\niQCVAwUBM8zMtNZdpXZXTSjhAQEMEAP+LmGDq/dWUa9tTTpTc2RE/VEF7pOGMZhZ\nmQTdzYbu8PporPjFBTMBTHPkIDoGuxIJl4iZ6p8wUW+0x6V3Hicj9mMDO2IB96FD\nH2pF7uWp/GHIvJemK9Gs2wvKR+lQ63KuHlAvfWxLkjS/6qcT3NdMDa9QtZjiXoia\nBGosKrgKnc0=\n=nvWN\n-----END PGP SIGNATURE-----\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiate-scenario01.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Scenarios for the Delivery of \n                   Negotiated Content using HTTP                                                    \n       Author(s) : Ted Hardie\n       Filename  : draft-ietf-http-negotiate-scenario-01.txt\n       Pages     : 5\n       Date      : 07/15/1997\n\nThis document describes various problems which have arisen in attempts to \ndeliver negotiated content using HTTP and examines several scenarios by \nwhich improvements in delivery might be accomplished.                      \nThis document does not discuss how HTTP might be used to negotiate the use \nof other protocols to deliver content.                                     \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-negotiate-scenario-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiate-scenario-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-negotiate-scenario-01.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-pep04.txt, .p",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : PEP - an Extension Mechanism for HTTP                   \n       Author(s) : D. Connolly, R. Khare, H. Frystyk\n       Filename  : draft-ietf-http-pep-04.txt, .ps\n       Pages     : 27\n       Date      : 07/15/1997\n\nHTTP is used increasingly in applications that need more facilities than \nthe standard version of the protocol provides, ranging from distributed \nauthoring, collaboration, and printing, to various remote procedure call \nmechanisms. The Protocol Extension Protocol (PEP) is an extension mechanism\ndesigned to address the tension between private agreement and public \nspecification and to accommodate extension of applications such as HTTP \nclients, servers, and proxies. The PEP mechanism is designed to associate \neach extension with a URI[2], and use a few new RFC 822[1] derived header \nfields to carry the extension identifier and related information between \nthe parties involved in an extended transaction.                           \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-pep-04.txt\".\n Or \n     \"get draft-ietf-http-pep-04.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-pep-04.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-pep-04.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-pep-04.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Request IESG action for draft-ietf-http-hit-metering03.tx",
            "content": "The HTTP working group wishes for the IESG to consider\n\ndraft-ietf-http-hit-metering-03.txt\n\nas a Proposed Standard.\n\n\n\n"
        },
        {
            "subject": "Proposed resolution for the STATUS100 issu",
            "content": "Warning: long message.\n\nThis message contains proposed changes and additions to several\nsections of RFC2068, as a resolution of the STATUS100 issue.\nPlease refer to RFC2068 for the original language (if any).\n\nSee\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#STATUS100\nalthough a lot of the discussion is not easily found except by\nlooking at a lot of the HTTP-WG mailing list archive.\n\nComments should be reported as soon as possible, since the HTTP/1.1\neditorial group intends to issue a last-call on this issue within\nthe next week or so.\n\n-Jeff\n\n=== \n=== Major revisions to 8.2\n=== \n======================\n\n8.2 Message Transmission Requirements\n\n8.2.1 Persistent connections and flow control\n\n   HTTP/1.1 servers SHOULD maintain persistent connections and use\n   TCP's flow control mechanisms to resolve temporary overloads,\n   rather than terminating connections with the expectation that\n   clients will retry. The latter technique can exacerbate network\n   congestion.\n\n8.2.2 Monitoring connections for error status messages\n\n   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor\n   the network connection for an error status while it is transmitting\n   the request. If the client sees an error status, it SHOULD\n   immediately cease transmitting the body. If the body is being sent\n   using a \"chunked\" encoding (section 3.6), a zero length chunk and\n   empty footer MAY be used to prematurely mark the end of the\n   message. If the body was preceded by a Content-Length header, the\n   client MUST close the connection.\n\n8.2.3 Automatic retrying of requests\n\n   If a client sees the transport connection close before it receives a\n   final response to its request, if the request method is idempotent\n   (see section 9.1.2), the client SHOULD retry the request without\n   user interaction.  If the request method is not idempotent, the\n   client SHOULD NOT retry the request without user confirmation.\n   (Confirmation by user agent software with semantic understanding of\n   the application MAY substitute for user confirmation.)\n\n8.2.4 Use of the 100 (Continue) status\n\n   The purpose of the 100 (Continue) status (see section 10.1.1) is to\n   allow an end-client that is sending a request message with a request\n   body to determine if the origin server is willing to accept the\n   request (based on the request headers) before the client sends the\n   request body.  In some cases, it may either be inappropriate or\n   highly inefficient for the client to send the body if the server\n   will reject the message without looking at the body.\n\n   Requirements for HTTP/1.1 or later clients:\n   o  If a client will wait for a 100 (Continue) response before sending\n      the request body, it MUST send an \"Expect\" request-header field\n      (section 14.XX) with the \"100-continue\" expectation.\n\n   o  A client MUST be prepared to accept a 100 (Continue) status\n      message followed by a regular response, even if the client does\n      not expect a 100 (Continue) status message.\n\n   o  A client MUST NOT send an \"Expect\" request-header field\n      (section 14.XX) with the \"100-continue\" expectation if it\n      does not intend to send a request body.\n\n      Note: Because of the presence of older implementations, the\n      protocol allows ambiguous situations in which a client may send\n      \"Expect: 100-continue\" without receiving either a 419\n      (Expectation Failed) status or a 100 (Continue) status.\n      Therefore, when a client sends this header field to an origin\n      server (possibly via a proxy) from which it has never seen a 100\n      (Continue) status, the client should not wait for an indefinite\n      or lengthy period before sending the request body.\n\n   Requirements for HTTP/1.1 or later origin servers:\n   o  Upon receiving a request which includes an \"Expect\" request-header\n      field with the \"100-continue\" expectation, an origin server must\n      either respond with 100 (Continue) status and continue to read\n      from the input stream, or respond with an error status. If it\n      responds with an error status, it MAY close the transport (TCP)\n      connection or it MAY continue to read and discard the rest of the\n      request. It MUST NOT perform the requested method if it returns\n      an error status.\n   \n   o  An origin server SHOULD NOT send a 100 (Continue) response if\n      the request message does not include an \"Expect\" request-header\n      field with the \"100-continue\" expectation, and MUST NOT send a\n      100 (Continue) response if such a request comes from an HTTP/1.0\n      (or earlier) client.\n   \n   o  An origin server SHOULD NOT send a 100 (Continue) response if\n      has already received some or all of the request body for the\n      corresponding request.\n   \n   o  An origin server that sends a 100 (Continue) response MUST\n      ultimately send a final status code, once the request body\n      is received and processed, unless it terminates the transport\n      connection prematurely.\n   \n   o  If an origin server receives a request that does not include an\n      \"Expect\" request-header field with the \"100-continue\"\n      expectation, and the request includes a request body, and the\n      server responds with an error status before reading the entire\n      request body from the transport connection, then the server\n      SHOULD NOT close the transport connection until it has read the\n      entire request, or until the client closes the connection.\n      Otherwise, the client may not reliably receive the response\n      message.\n\n   For compatibility with RFC 2068, a server MAY send a 100 (Continue)\n   status in response to an HTTP/1.1 PUT or POST request that does not\n   include an \"Expect\" request-header field with the \"100-continue\"\n   expectation.  This exception, the purpose of which is to minimize\n   any client processing delays associated with an undeclared wait for\n   100 (Continue) status, applies only to HTTP/1.1 requests, and not to\n   requests with any other HTTP-version value.\n\n   Requirements for HTTP/1.1 or later proxies:\n   o  If a proxy receives a request that includes an \"Expect\"\n      request-header field with the \"100-continue\" expectation, and the\n      proxy either knows that the next-hop server complies with\n      HTTP/1.1 or higher, or does not know the HTTP version of the\n      next-hop server, it MUST forward the request, including the\n      Expect header field.\n\n   o  If the proxy knows that the version of the next-hop server is\n      HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\n      respond with a 419 (Expectation Failed) status.\n\n   o  Proxies SHOULD maintain a cache recording the HTTP version\n      numbers received from recently-referenced next-hop servers.\n\n   o  A Proxy MUST NOT forward a 100 (Continue) response if the request\n      message was received from an HTTP/1.0 (or earlier) client and did\n      not include an \"Expect\" request-header field with the\n      \"100-continue\" expectation.  Otherwise, proxies MUST forward\n      response messages with status code 100 (Continue), unless the\n      proxy itself added the \"Expected:  100-continue\" field to the\n      request, or unless the connection between the proxy and its\n      client has been closed.\n\n8.2.5 Client behavior if server prematurely closes connection\n\n   If an HTTP/1.1 (or later) client sends a request which includes a\n   request body, but which does not include an \"Expect\" request-header\n   field with the \"100-continue\" expectation, and if the client is not\n   directly connected to an HTTP/1.1 (or later) origin server, and if\n   the the client sees the connection close before receiving any status\n   from the server, the client SHOULD retry the request, subject to the\n   restrictions in section 8.2.3. If the client does retry this\n   request, it MAY use the following \"binary exponential backoff\"\n   algorithm to be assured of obtaining a reliable response:\n\n  1. Initiate a new connection to the server\n\n  2. Transmit the request-headers\n\n  3. Initialize a variable R to the estimated round-trip time to the\n     server (e.g., based on the time it took to establish the\n     connection), or to a constant value of 5 seconds if the round-trip\n     time is not available.\n\n  4. Compute T = R * (2**N), where N is the number of previous retries\n     of this request.\n\n  5. Wait either for an error response from the server, or for T seconds\n     (whichever comes first)\n\n  6. If no error response is received, after T seconds transmit the body\n     of the request.\n\n  7. If client sees that the connection is closed prematurely, repeat\n     from step 1 until the request is accepted, an error response is\n     received, or the user terminates the retry process.\n\n   If at any point an error status is received, the client\n\n  o  SHOULD NOT continue and\n\n  o  SHOULD close the connection if it has not completed sending the\n     request message.\n\n=============================\n=== \n=== 10.4.1 100 Continue:\n=== \n=== One new sentence added at the end, as a cross-reference:\n=== \n=============================\n\n10.4.1 100 Continue\n\n   The client may continue with its request. This interim response is\n   used to inform the client that the initial part of the request has\n   been received and has not yet been rejected by the server. The client\n   SHOULD continue by sending the remainder of the request or, if the\n   request has already been completed, ignore this response. The server\n   MUST send a final response after the request has been completed.\n   See section 8.2.4 for detailed discussion of the use and handling\n   of this status code.\n=============================\n===\n=== What follows is basically what I sent on Wed, 02 Jul 97, in\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0027.html\n=== but with a few changes:\n=== \n=== (1) I've changed the header name from \"Expected\" to \"Expect\",\n=== just to save a couple of bytes.\n=== \n=== (2) Following Scott Lawrence's suggestion in\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0032.html\n=== I've changed the status code from 412 (Precondition Failed) to a new\n=== 419 (Expectation failed) code, and included additional language\n=== for specifying that new code.\n=== \n=== (3) I've added some clarifications based on my message on \"Is 100-Continue\n=== hop-by-hop?\",\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0078.html\n=== \n=== (4) I did NOT add an \"Expect: 100-hopbyhop\" because nobody has\n=== spoken up in its favor.\n=== \n=== (5) I reorganized the paragraphs slightly, and introduced a new\n=== subhead.\n=== \n=============================\n\n10.4.20 419 Expectation Failed\n\n   The expectation given in an \"Expect\" request-header field (see\n   section 14.XX) could not be met by this server, or, if the server is\n   a proxy, the server has unambiguous evidence that the request could\n   not be met by the next-hop server.\n\n=============================\n\n14.XX Expect\n\n    The Expect request-header field is used to indicate that\n    particular server behaviors are required by the client.  A\n    server that does not understand or is unable to comply with any of\n    the expectation values in the Expect field of a request MUST\n    respond with appropriate error status.\n\n      Expect              =  \"Expect\" \":\" 1#expectation\n\n      expectation  =  \"100-continue\" | expectation-extension\n      expectation-extension =  token [ \"=\" ( token | quoted-string )\n                                       *expect-params ]\n      expect-params           =  \";\" token [ = ( token | quoted-string ) ]\n\n    The server SHOULD respond with a 419 (Expectation Failed) status\n    if any of the expectations cannot be met.\n\n    This header field is defined with extensible syntax to allow for\n    future extensions.  If a server receives a request containing\n    an Expect field that includes an expectation-extension that\n    it does not support, it MUST respond with a 419 (Expectation\n    Failed) status.\n\n14.XX.1 Expect 100-continue\n\n    When the \"100-continue\" expectation is present on a request that\n    includes a body, the requesting client will wait after sending the\n    request headers before sending the content-body.  In this case, the\n    server MUST conform to the requirements of section 8.2.4: it MUST\n    either send a 100 (Continue) status, or an error status, after\n    receiving the \"Expect: 100-continue\" request header.\n\n    If a proxy receives a request with the \"100-continue\" expectation,\n    and the proxy either knows that the next-hop server complies with\n    HTTP/1.1 or higher, or does not know the HTTP version of the\n    next-hop server, it MUST forward the request, including the Expect\n    header field.  If the proxy knows that the version of the next-hop\n    server is HTTP/1.0 or lower, it MUST NOT forward the request, and\n    it MUST respond with a 419 (Expectation Failed) status.  Proxies\n    SHOULD maintain a cache recording the HTTP version numbers received\n    from recently-referenced next-hop servers.\n    \nNote: Because of the presence of older implementations, the\nprotocol allows ambiguous situations in which a client may send\n\"Expect: 100-continue\" without receiving either a 419\n(Expectation Failed) status or a 100 (Continue) status.\nTherefore, when a client sends this header field to an origin\nserver (possibly via a proxy) from which it has never seen a\n100 (Continue) status, the client should not wait for an\nindefinite or lengthy period before sending the request body.\n\n=============================\n=== \n=== 13.11 in RFC 2068 incorrectly allows a proxy to inject\n=== its own 100 response into the reply stream.  The change\n=== below modifies *only* the last sentence of the first\n=== paragraph.\n=== \n=============================\n\n13.11 Write-Through Mandatory\n\n   All methods that may be expected to cause modifications to the origin\n   server's resources MUST be written through to the origin server. This\n   currently includes all methods except for GET and HEAD. A cache MUST\n   NOT reply to such a request from a client before having transmitted\n   the request to the inbound server, and having received a\n   corresponding response from the inbound server. This does not\n   prevent a proxy cache from forwarding a 100 (Continue) response\n   before the inbound server has sent its final reply.\n\n   The alternative (known as \"write-back\" or \"copy-back\" caching) is not\n   allowed in HTTP/1.1, due to the difficulty of providing consistent\n   updates and the problems arising from server, cache, or network\n   failure prior to write-back.\n\n=============================\n=== \n=== Add this to the end of 8.1.2.2 (Pipelining)\n=== \n=============================\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2).  Otherwise,\n   a premature termination of the transport connection may lead to\n   indeterminate results.  A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\n=============================\n\n[End of changes for STATUS100]\n\n\n\n"
        },
        {
            "subject": "LAST CALL (and  other status changes) on HTTP/1.1 issue",
            "content": "The HTTP/1.1 editing group voice conference reviewed the issues list\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP/Issues\n\nand wish to make a \"LAST CALL\" on the resolution of various technical\nissues.\n\nPLEASE if you wish to respond on a particular issue, send a\nseparate message with ISSUE <NAME OF ISSUE> in the subject line.\nNote that Jim Gettys will start editing the actual document to account\nfor closed technical issues and editorial issues, starting tomorrow,\n7/17.\n\nThe following issues are now in LAST CALL; please review and send\nany comments AS SOON AS POSSIBLE:\n100DATE\nQUOTED-BACK\nQZERO\nCACHE-CONTRA\nCACHE-DIRECTIVE\nBYTE-RANGE\nLWS-DELIMITER\nCRLF\nMAX-AGE\n\nThe following issues are now closed:\nWARNINGS\nPADDING\nCONNECTION\nRANGES\n\nPlease do not attempt to reopen closed issues without contacting me\nprivately first.\n\nThe following issues will be in LAST CALL after some messages \nwith the proposed resolution are sent:\n\nPROXY-AUTHORIZATION (after Henrik's message)\nCONTENT-LOCATION (after Henrik's message)\nPROXY-LENGTH (after Paul Leach's message)\n\nFor your information, the following action items were accepted:\n\nLarry Masinter:\nCOMMENT: summarize & call for consensus\nCACHING-CGI: draft wording\nLANGUAGE-TAG: open discussion in working group\nHIT-METERING: Forward to area directors & ask for Proposed Standard\n\nPaul Leach:\nPUT-RANGE: Draft wording\nAUTH-CHUNKED: prod Henry Sanders \nRE-AUTHENTICATION-REQUESTED: will follow up\nJeff Mogul:\nCONTENT-ENCODING: work out with Henrik\nRANGE-ERROR: Draft proposed resolution\nAGE-CALCULATION: Draft proposed resolution\nCLARIFY-NO-CACHE: Draft proposed resolution\nENCODING-NOT-CONNECT: Draft proposed resolution (after content encoding\nissues)\nSTATUS100: followup on mailing list\nJim Gettys:\nHOST: Draft proposed resolution\nRETRY-AFTER: Draft proposed resolution\n\nHenrik Frystyk:\nCONTENT-LOCATION: Draft editorial change\nVARY: Henrik will summarize & draft proposed resolution\nCONTENT-ENCODING: work with Jeff\nsend email for PROXY-AUTHORIZATION, CONTENT-LENGTH\n\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "At 11:27 PM -0700 7/14/97, David W. Morris wrote:\n[hisresponses to my responses to his comments to the above I-D, with\nadditional comments by me on comments by Foteos Macrides]\n\n\n>> > An earlier comment was made about quotes in the \"Port\" attribute,\n>> > but I think there are additional problems with the syntax as\n>> > specified and suggest that:\n>> >\n>> > ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n>> > :: port-list       |       1#DIGIT\n>> >\n>> > be replaced with:\n>> >\n>> >                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n>> >    portnum         =       1*DIGIT\n>> >\n>> > If I correctly understand RFC2068 syntax, 1#X means 1 or more\n>> > occurances of X delimited by commas. My changes fix the \"=\"\n>> > in port-list, the 1#DIGIT in port list and make the quotes\n>> > optional for the single port case.\n\nYou're absolutely right about my having botched the syntax.  I'll have to\nfix this up.  Thanks!\n\nConcerning making the quotes optional for a single port number:  I accept\nFoteos's argument that it's easy to handle.  I'll allow that in the spirit\nof \"be liberal in what you accept\", an implementation may want to handle it\nthat way, but I still think it's a really bad idea for the syntax to be\n*specified* that way.  Apart from making the syntax (marginally) more\ncomplex, the (proposed) syntax above invites errors of omission, where a\nserver goes from sending a single port number to sending more than one and\nthe implementor has to remember to add quotes to get it right.\n\nConcerning remarks about requiring FQHN:  I'll have to think this through\nmore carefully when I get back from vacation.\n\nConcerning Foteos's suggestion about reserving the attribute name\n\"CommentURL\", sure.  Concerning CommentURL itself, I'll think about that\nsome more.  The risk in adding it is that supporting it has implications\nfor browsers and browser vendors, and they haven't seemed too keen about\nRFC 2109 (and successors) as it is.\n\nConcerning Foteos's suggestion that all the attribute names be reserved\nfrom use as cookie NAMEs, it's unnecessary.  The cookie NAME=VALUE always\ncomes first in the Set-Cookie2 header, so you can always distinguish it\nfrom any attributes.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "draft-ietf-http-hit-metering03.tx",
            "content": "Regarding draft-ietf-http-hit-metering-03.txt ...\n\n>1.1 Goals, non-goals, and limitations\n>...\n>   This design has certain potential limitations:\n>\n>      - If it is not deployed widely in both proxies and servers,\n>        it will provide little benefit.\n>\n>      - It may, by partially solving the hit-counting problem,\n>        reduce the pressure to adopt more complete solutions, if\n>        any become available.\n>\n>      - Even if widely deployed, it might not be widely used, and\n>        so might not significantly improve performance.\n>\n>   These potential limitations might not be problems in actual practice.\n\nIt may also be that, by placing this proposal on the standards track,\nproxy implementers will be forced, for the sake of efficiency or lack\nof network resources, to add the Meter field to every request regardless\nof their intention to pass along metering information.  The result would\nbe that every proxied HTTP request would include an extra 19 bytes serving\nno useful purpose.\n\n>   The Meter header carries zero or more directives, similar to the way\n\nIt is NOT the \"Meter header\".  The header of a message is the sum of all\nof the header fields.  It should be referred to as the \"Meter field\" or\n\"Meter header field\" or \"Meter request-header\" (the latter is a BNF rule).\n\nThe proposal places restrictions on proxies in order to restrict the\nactions of shared caches.  The presence of a non-caching proxy in the\nchain will break the metering tree, even though a non-caching proxy\nhas no impact on metering itself.  Section 5.5 (which should be referred\nto by earlier sections) suggests that such proxies always add the Meter\nfield, thus increasing network overhead merely to sustain the possibility\nof the metering tree.\n\n>3 Design concepts\n>...\n>   No proxy is required to implement hit-metering or usage-limiting.\n>   However, any proxy that transmits the Meter header in a request MUST\n>   implement every unconditional requirement of this specification,\n>   without exception or amendment.\n>\n>   This is a conservative design, which may sometimes fail to take\n>   advantage of hit-metering support in proxies outside the metering\n>   subtree.  However, it is likely that without the reliability offered\n>   by a conservative design, managers of origin servers with\n>   requirements for accurate approximations will not take advantage of\n>   any hit-metering proposal.\n\nIn other words, it is a design that forces the proxy (with or without\ncache) to advertize metering support, rather than using the existing\nextensibility of the Cache-Control field to allow the origin server\nto state its requirements for metering explicitly.  The alternative\ndesign that is not mentioned is to introduce a metering cache-directive\nthat modifies the semantics of the \"private\" or \"s-maxage\" directives, e.g.\n\n    Cache-control: s-maxage=0, meter=\"...\"\n\nin a response, wherein a cache that recognized the meter directive could\nignore the s-maxage directive.  The advantage of the alternative is that\nit does not add \"Connection: Meter\" to every proxied request, it does not\nrequire the proxy to modify the response, it does not break when a non-caching\nproxy is in the chain, and it has zero impact when \"cache-busting for the\nsake of metering\" is not an issue.  The disadvantage is that it reveals\nthe origin server's intentions for busting the shared cache.\n\nThe proposal assumes that the impact of a MUST requirement in a proposed\nstandard will be sufficient to establish trust, since it allows the\norigin server to hide behind a veil of otherwise important cache-control\ndirectives.  I believe that network applications exist to perform a\nservice, and that the requirements of that service will not be altered\nby a MUST requirement in a specification that has nothing to do with\ninteroperability.  The applications will find a way around those\nrequirements, or simply ignore them, if doing so results in better service.\n\n>5.2 Abbreviations for Meter directives\n>   To allow for the most efficient possible encoding of Meter headers,\n>   we define abbreviated forms of all Meter directives.  These are\n>   exactly semantically equivalent to their non-abbreviated\n>   counterparts.  All systems implementing the Meter header MUST\n>   implement both the abbreviated and non-abbreviated forms.\n>   Implementations SHOULD use the abbreviated forms in normal use.\n\nThen don't define any non-abbreviated form!  This is a network protocol,\nnot a user-readable treatise.  Long header field names are often necessary\nto avoid conflicts with other names (and protocols); long field values\nare never useful unless they contain data intended to be read by a\nhuman being, which is clearly not the case here.\n\nThere is also a great deal of unnecessary duplication in the specification.\nSome of the meter directives are actually defined in three different\nlocations, four if you include the abbreviated forms.\n\nWhen the proposal is ready for publication as an RFC, it should be\nassigned Experimental status until such time as the overhead placed on\nproxies is *demonstrated* to be offset by the benefit gained by caching\nresponses that would otherwise have been busted for the sole purpose of\ngathering hit counts.  In particular, no Internet proxy should activate an\nimplementation of this proposal until there exist some deployed origin\nserver implementations.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-1715\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Wed, 16 Jul 1997, Dave Kristol wrote:\n\n> At 11:27 PM -0700 7/14/97, David W. Morris wrote:\n> [hisresponses to my responses to his comments to the above I-D, with\n> additional comments by me on comments by Foteos Macrides]\n> \n> \n> >> > An earlier comment was made about quotes in the \"Port\" attribute,\n> >> > but I think there are additional problems with the syntax as\n> >> > specified and suggest that:\n> >> >\n> >> > ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n> >> > :: port-list       |       1#DIGIT\n> >> >\n> >> > be replaced with:\n> >> >\n> >> >                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n> >> >    portnum         =       1*DIGIT\n> >> >\n> >> > If I correctly understand RFC2068 syntax, 1#X means 1 or more\n> >> > occurances of X delimited by commas. My changes fix the \"=\"\n> >> > in port-list, the 1#DIGIT in port list and make the quotes\n> >> > optional for the single port case.\n> \n> You're absolutely right about my having botched the syntax.  I'll have to\n> fix this up.  Thanks!\n> \n> Concerning making the quotes optional for a single port number:  I accept\n> Foteos's argument that it's easy to handle.  I'll allow that in the spirit\n> of \"be liberal in what you accept\", an implementation may want to handle it\n> that way, but I still think it's a really bad idea for the syntax to be\n> *specified* that way.  Apart from making the syntax (marginally) more\n> complex, the (proposed) syntax above invites errors of omission, where a\n> server goes from sending a single port number to sending more than one and\n> the implementor has to remember to add quotes to get it right.\n\nI actually believed you had intended the quotes to be optional ... my\nmain concern was getting the syntax right so on the quotes I'm satisfied\nwith whatever you choose because I think the port=xxxx case would not be\nused since it is exactly the same case as 'port' with no value.\n\n> Concerning remarks about requiring FQHN:  I'll have to think this through\n> more carefully when I get back from vacation.\n\nSounds fair to me ... you've been working hard enough from vacation.\n\n> Concerning Foteos's suggestion about reserving the attribute name\n> \"CommentURL\", sure.  Concerning CommentURL itself, I'll think about that\n> some more.  The risk in adding it is that supporting it has implications\n> for browsers and browser vendors, and they haven't seemed too keen about\n> RFC 2109 (and successors) as it is.\n\nBut I don't recall a vendor objection to the suggestion. If I were the\nvendor, I'd like this one because it would be a real value add to handle\nit well and I believe it's a real win for users.\n\n> \n> Concerning Foteos's suggestion that all the attribute names be reserved\n> from use as cookie NAMEs, it's unnecessary.  The cookie NAME=VALUE always\n> comes first in the Set-Cookie2 header, so you can always distinguish it\n> from any attributes.\n\nYeah, I scratched my head on that one for a long time before I concluded \nI could see no reason for breakage if the attribute names weren't \nreserved.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "305/30",
            "content": "Here is a simplified 305/306 draft..\n\nChanges:\n* Scope is an 'advisory' scope, a client may follow it\nor may use its own, potentially at the cost of inefficiency..\n\n* scope syntaxt, no more 'reverse domain name', now its \n  just a regexp..\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                     \n#include<disclaimer.h>\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n\n\n\n\nHTTP Working Group                                            Josh Cohen\nInternet-Draft                             Netscape Communications Corp.\n                                                         5 December 1996\n\n                  HTTP/1.1 305 and 306 Response Codes\n\n                    <draft-cohen-http305036-00.txt>\n\nStatus of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups.  Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time.  It is inappropriate to use Internet- Drafts as reference\n   material or to cite them other than as ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please check the\n   ``1id-abstracts.txt'' listing contained in the Internet- Drafts\n   Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\nAbstract\n\n   The HTTP/1.1 RFC specifies a response code '305 Use Proxy' which is\n   intended to cause a client to retry the request using a specified\n   proxy server.  This functionality is important, but underspecified in\n   the current spec.  The spec does not specify for how long or which\n   URLs the redirect applies to, or how proxies can deal with or\n   generate similar responses.  This draft proposes a specification for\n   both the 305 response and a new response, \"306 Switch Proxy\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 1]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\nSummary\n\n 1.0 Response Codes\n\n  1.1 305 Use Proxy\n  1.2 306 Switch Proxy\n  1.3 506 Redirection Failed\n\n 2.0 Headers\n\n  2.1 Set-proxy:\n  2.2 Location:\n\n 3.0 Methods\n\n  3.1 OPTIONS\n\n 4.0 Operational Constraints\n\n 5.0 Notes\n\n\n1.0 Response Codes\n\n 1.1 305 Use Proxy\n\n   The 305 is generated by an origin server to indicate that the client,\n   or proxy, should use a proxy to access the requested resource.\n\n   The request SHOULD be accompanied by a 'Set-proxy' response header\n   indicating what proxy is to be used. The client will parse the 'Set-\n   proxy' header as defined below to decide how long, for what URLs it\n   should use the specified proxy.\n\n   If the 305 response is not accompanied by a 'Set-proxy' header, it\n   MUST be accompanied by a 'Location' header.  The 'Location' header\n   will specify a URL to the proxy.\n\n   If both headers are present in the response, the client SHOULD use\n   the 'Set-proxy' header only.\n\n 1.2 306 Switch Proxy\n\n   The 306 response is generated by a proxy server to indicate that the\n   client or proxy should use the information in the accompanying 'Set-\n   proxy' header to choose a proxy for subsequent requests.\n\n   The 306 response code MUST be accompanied by the 'Set-proxy' response\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 2]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n   header.  The client or proxy will parse the 'Set-proxy' header to\n   determine which proxy to use, how long to use it, and for which URLs\n   to use it.\n\n   The scope in the set-proxy header is considered an optional advisory.\n   The client or proxy may choose to ignore it, and use it for just this\n   request, for all requests, or for a scope previously or implicitly\n   defined by another configuration method or autoconfiguration system.\n\n 1.3 506 Redirection Failed\n\n   The 506 response is returned when a redirection fails or is refused\n   by a proxy or client.  If the redirection response included a body,\n   then it SHOULD be included in the 506 response.\n\n2.0 Headers\n\n 2.1 'Set-proxy' Response Header\n\n           The 'Set-proxy' header is defined as:\n\n           Set-proxy: \"Set-proxy\" \":\" 1(\n                   action #(parameters)\n                   )\n\n           parameters = #( ( \"scope\" \"=\" scopePattern ) |\n                   ( proxyURI \"=\" URI ) |\n                   lifetime )\n\n           lifetime = ( \"seconds\"  \"=\" integer )\n                   | ( \"hits\"      \"=\" integer )\n\n           action =  ( \"DIRECT\"\n                   | \"IPL\"\n                   | \"SET\" )\n                   ) \";\"\n\n           scopePattern = \"*\" | \"-\" | URIpattern\n\n   An example header:\n       Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\",\n           scope=\"http://\", seconds=5\n\n action\n\n   The first item, \"action\" specifies the type or mode of the change.\n   Possible modes are:\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 3]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n   DIRECT\n    Attempt to connect directly, with no proxy\n\n\n   IPL\n    Initial Program Load, the client or proxy should attempt to revert\n    back to its default or initial proxy setting.  This is meant to\n    instruct a client to re-fetch its proxy configuration, or PAC file.\n    When set, the accompanying scope field MUST be \"*\" A client receiv-\n    ing this response SHOULD prompt the user for confirmation.\n\n\n    If accompanied by a 'proxyURI' parameter, a proxy or client MAY use\n    the value as a URL containing a configuration to retrieve.  If a\n    client  does so, it MUST prompt the user for confirmation.\n\n\n   SET\n    Set to parameter \"proxyURI\".  The client should use the URL speci-\n    fied for \"proxyURI\" as the proxy.  If the SET mode is specified, the\n    parameter, \"proxyURI\", MUST be present.\n\n Scope\n\n    Scope refers to a regular expression pattern that specifies which\n    URIs are subject to this header setting.  URIs should be matched\n    against the scope with this rule :\n\n     The scope \"*\" means all requests\n     The scope \"-\" means this EXACT URL ONLY\n\n    Otherwise, the URL is compared with the regular expression scope.\n\n\n    The lifetime parameter specifies how long the specified proxy should\n    be used.  If lifetime is specified as \"seconds\" then the proxy set-\n    ting remains in effect for 'integer' seconds.  If lifetime is speci-\n    fied in 'hits' then the proxy setting remains in effect for\n    'integer' transactions.\n\n 2.2 Location Header\n\n\n    In the original HTTP/1.1 spec, the 'Location' header was used to\n    indicate the proxy setting.  Its use is DEPRECATED by the 'Set-\n    proxy' header in the context of a 305 response. All new implementa-\n    tions MUST send the Set-proxy header.  Implementations MAY send the\n    'Location' header so as to allow backward compatibility.\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 4]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n    If the 'Location' header is specified, it should contain a URI of\n    the proxy.  If the Set-proxy header is not specified, the client\n    should use this proxy for just one request, and only for the origi-\n    nally requested exact URL.\n\n 3.0 Methods\n\n\n    A client or proxy receiving a 305 or 306, should use the OPTIONS\n    method to determine if the server or proxy it is talking to actually\n    is an HTTP/1.1 server supporting 305 and 306 responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 5]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n4.0 Operational Constraints\n\n\n   * Both the 305 and 306 response codes are HOP by HOP.  A proxy server\n     MUST not forward a 305 or 306 respose code (unless it generated the\n     306).\n\n\n   * A webserver MUST NOT send a 306 response under any circumstances\n\n\n   * A proxy server MUST NOT generate a 305 response.\n\n\n   * A client or proxy SHOULD NOT accept a 306 from a proxy that it\n     learned of via a 305 response code.\n\n\n   * A client or proxy MAY maintain state and allow a lifetime to extend\n     beyond a session or restart.\n\n\n   * A 'Set-proxy: IPL' SHOULD override any previous 'Set-proxy' header.\n\n\n   * A 305 or 306 response MAY contain a body containing an explanation\n     of the redirect for clients which do not understand the redirect\n\n\n   * In the absence of any parameter, the following defaults should be\n     used:\n\n       lifetime = this transaction only\n       scope = this exact URL only\n\n\n   * When receiving a 305 response, the client or proxy will enforce the\n     following rule with respect to the scope.\n\n     The scope specified must be more restrictive than the transformed\n     URL in question based on the rightmost slash in the URI.\n\n     Example: (in order of restrictiveness)\n       for URI = http://www.ups.com/services/index.html\n\n       http://www.ups.com/services/.*  (allowed)\n       http://www.ups.com/services/express/.* ( allowed )\n       http://www.ups.com/.* (NOT allowed)\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 6]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n     If the scope returned with a 305 response is less restrictive than\n     the requested URL, the client may reject the redirection and return\n     506 Redirection Failed.  If the client wished to honor the\n     redirect, it client MUST prompt the user for confirmation before\n     accepting the new proxy setting.\n\n\n   * Since HTTP/1.0 proxies may unknowingly forward a 305 or 306\n     response code that was generated maliciously or in good faith, the\n     client must attempt to ascertain if the proxy with which it is\n     directly communicating is HTTP/1.1 and if it supports the 'Set-\n     proxy' header.  To determine this, the client or proxy should use\n     the OPTIONS method to make a request check for this feature.  The\n     extension string should be 'set-proxy' in the OPTIONS request.\n\nSecurity Considerations\n\n     Great care should be taken when implementing client side actions\n     based on the 305 or 306.  Since older proxies may unknowingly for-\n     ward either of these reponses, clients should be prepared to check\n     the validity.\n\n\n   * Please read the section 'Operational Constraints'\n\n\n   * A client or proxy MUST NOT accept a 305 response from a proxy.\n\n\n   * A client or proxy MUST NOT accept a 306 response from an origin\n     server.\n\n\n   * When receiving a 306 response from a proxy, the client MUST verify\n     that the proxy supports the 306 response with an OPTIONS request.\n\n5.0 Notes\n\n     Further specification is needed to define exactly how to use\n     OPTIONSs, or another mechanism to determin if set-proxy is sup-\n     ported.\n\nAuthor's Address\n\n     Josh Cohen\n     Netscape Communications Corporation\n     501 E. Middlefield Rd\n     Mountain View, CA 94043\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 7]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n     Phone (415) 937-4157\n     EMail: josh@netscape.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 8]\n\n\n\n"
        },
        {
            "subject": "exi",
            "content": "Dear Sir:\nI am very sorry that i want to exit this discussion mailing list. First\nof all,I really want to thanks all the people here to supply such good\narticles to me.they gave me the deeply\nunderstand the HTTP and its application.\nBut i will go for my holiday for two mouth,and i cannot receive my email\nfor those day.\nI am afraid that i can not bear for so many letter to me.my email box\nmax size is up to\n1M byte.So I have to leave this good discussion mailing list.\n\nThanks for all!\n\nyours Dongfan Chen \n\n\n\n"
        },
        {
            "subject": "STATUS100 Re: Proposed resolutio",
            "content": "  An observation: I find it interesting that the set of rules to limit\n  use of 100 Continue seems to require such a long specification,\n  given that the original mechanism was so simple...\n\n>>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> 8.2 Message Transmission Requirements\nJM>...\nJM>    Requirements for HTTP/1.1 or later clients:\nJM>...\nJM>    o  A client MUST be prepared to accept a 100 (Continue) status\nJM>       message followed by a regular response, even if the client does\nJM>       not expect a 100 (Continue) status message.\nJM>...\nJM>    Requirements for HTTP/1.1 or later origin servers:\nJM>    o  Upon receiving a request which includes an \"Expect\" request-header\nJM>       field with the \"100-continue\" expectation, an origin server must\nJM>       either respond with 100 (Continue) status and continue to read\nJM>       from the input stream, or respond with an error status. If it\nJM>       responds with an error status, it MAY close the transport (TCP)\nJM>       connection or it MAY continue to read and discard the rest of the\nJM>       request. It MUST NOT perform the requested method if it returns\nJM>       an error status.\n\n  I would reword this to reflect that the 100 Continue response MUST\n  be sent after the request _headers_ have been recieved, since the\n  'request' includes the body.\n\nJM>    o  An origin server SHOULD NOT send a 100 (Continue) response if\nJM>       the request message does not include an \"Expect\" request-header\nJM>       field with the \"100-continue\" expectation, and MUST NOT send a\nJM>       100 (Continue) response if such a request comes from an HTTP/1.0\nJM>       (or earlier) client.\n\nJM>    o  An origin server SHOULD NOT send a 100 (Continue) response if\nJM>       has already received some or all of the request body for the\nJM>       corresponding request.\n\n  I don't see the point of these two SHOULD NOTs, since the client\n  MUST be prepared to accept an unexpected 100 response anyway.\n  Arguing against these rules:\n\n    - As noted elsewhere, existing 1.1 servers (yes, there are some)\n      won't have been coded to include these restrictions (since the\n      Expect header was only suggested a couple of weeks ago).\n\n    - I think that it is poor design to encourage look-ahead in the\n      data stream to determine whether or not body has been received.\n\nJM>...\nJM>    o  If an origin server receives a request that does not include an\nJM>       \"Expect\" request-header field with the \"100-continue\"\nJM>       expectation, and the request includes a request body, and the\nJM>       server responds with an error status before reading the entire\nJM>       request body from the transport connection, then the server\nJM>       SHOULD NOT close the transport connection until it has read the\nJM>       entire request, or until the client closes the connection.\nJM>       Otherwise, the client may not reliably receive the response\nJM>       message.\n\n  Does this amount to a rule for the purpose of avoiding bugs in some\n  TCP implementations?  I can live with this rule since it is not a\n  MUST.\n\nJM>    For compatibility with RFC 2068, a server MAY send a 100 (Continue)\nJM>    status in response to an HTTP/1.1 PUT or POST request that does not\nJM>    include an \"Expect\" request-header field with the \"100-continue\"\nJM>    expectation.  This exception, the purpose of which is to minimize\nJM>    any client processing delays associated with an undeclared wait for\nJM>    100 (Continue) status, applies only to HTTP/1.1 requests, and not to\nJM>    requests with any other HTTP-version value.\n\n  I think that making this a special case allowed _only_ for HTTP/1.1\n  is going too far.  We already have the requirement that 100 Continue\n  must be accepted by the client anyway.  We're talking about as few\n  as 'HTTP/1.1 100CRLF\" - 14 bytes - 23 if you send the ' Continue'.\n  What justification is there for the complexity of this restriction?\n\n  Furthermore, I think that the above rule, which is stated using\n  'MAY', is really a 'MUST NOT':\n\n    A server using any revision of HTTP later than HTTP/1.1 MUST NOT\n    send a 100 (Continue) status in response to an HTTP/1.1 PUT or\n    POST request that does not include an \"Expect\" request-header\n    field with the \"100-continue\" expectation.\n\n  Referring to the use of the imperitive 'key word's, RFC 2119 says\n  that:\n\n    Imperatives of the type defined in this memo must be used with\n    care and sparingly.  In particular, they MUST only be used where\n    it is actually required for interoperation or to limit behavior\n    which has potential for causing harm (e.g., limiting\n    retransmisssions)\n\n  I don't believe that this rule on future versions of the protocol\n  meets that test.\n\nJM>    Requirements for HTTP/1.1 or later proxies:\nJM> ...\nJM>    o  If the proxy knows that the version of the next-hop server is\nJM>       HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\nJM>       respond with a 419 (Expectation Failed) status.\n\n  How is the client to know that repeating the request using HTTP/1.0 could\n  resolve this situation?  Should clients always back off to 1.0 if\n  they receive a 419?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Q: About transmission schemes ",
            "content": "Andreas Brusinsky wrote:\n\n> >From what does it depend how HTTP transmits special MIME typed data?\n> I mean are there different transmission schemes used (e.g. connection\n> oriented, connection free, error tolerant ..)\n> If there are different schemes used is then the transmission\n> automaticaly\n> be delegated to a different protocoll or are there really different\n> transmission schemes implemented within HTTP.\n\nAlthough HTTP can nominally be used over different protocols, currently\nHTTP as we know it is always over TCP.\n\n> Is the QUALITY specifier in an ACCEPT Method really implemented?\n> I mean if HTTP is build on top of tcp then transmission should be\n> save in any case. So if I would specify a q<1 it would be of no use as\n>\n> long the transmission is done by HTTP.\n\nYou misunderstand the purpose of quality - it defines your preference\nfor particular content types, not the QoS (which is what I suspect you\nare getting at).\n\nCheers,\n\nBen.\n\n--\nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Wed, 16 Jul 1997, Dave Kristol wrote:\n>\n>> At 11:27 PM -0700 7/14/97, David W. Morris wrote:\n>> [hisresponses to my responses to his comments to the above I-D, with\n>> additional comments by me on comments by Foteos Macrides]\n>> \n>> \n>> >> > An earlier comment was made about quotes in the \"Port\" attribute,\n>> >> > but I think there are additional problems with the syntax as\n>> >> > specified and suggest that:\n>> >> >\n>> >> > ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n>> >> > :: port-list       |       1#DIGIT\n>> >> >\n>> >> > be replaced with:\n>> >> >\n>> >> >                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum <\"> ]\n>> >> >    portnum         =       1*DIGIT\n>> >> >\n>> >> > If I correctly understand RFC2068 syntax, 1#X means 1 or more\n>> >> > occurances of X delimited by commas. My changes fix the \"=\"\n>> >> > in port-list, the 1#DIGIT in port list and make the quotes\n>> >> > optional for the single port case.\n>> \n>> You're absolutely right about my having botched the syntax.  I'll have to\n>> fix this up.  Thanks!\n>> \n>> Concerning making the quotes optional for a single port number:  I accept\n>> Foteos's argument that it's easy to handle.  I'll allow that in the spirit\n>> of \"be liberal in what you accept\", an implementation may want to handle it\n>> that way, but I still think it's a really bad idea for the syntax to be\n>> *specified* that way.  Apart from making the syntax (marginally) more\n>> complex, the (proposed) syntax above invites errors of omission, where a\n>> server goes from sending a single port number to sending more than one and\n>> the implementor has to remember to add quotes to get it right.\n>\n>I actually believed you had intended the quotes to be optional ... my\n>main concern was getting the syntax right so on the quotes I'm satisfied\n>with whatever you choose because I think the port=xxxx case would not be\n>used since it is exactly the same case as 'port' with no value.\n\nMy statements about that concerned the comments/explanation section\nof the draft, which I think could be improved in the interest of promoting\nsuccessful implementation of the specs.  I had interpreted the specs,\nthemselves, to be making quoting of the port value mandatory in all cases\nwhere a value is present.  My suggestion was to state that explicitly, with\nwords, in the comments/explanations sections.  Dave Morris' statements, above,\nin effect are confirmation that an explicit statement about this in the\ncomments/explanation section would help avoid confusion about what the\nspecs, themselves, actually intend.\n\nI was not suggesting that the quoting be made optional.  I was\ncautioning that implementors include \"sanity checks\" to cope with the\npossibility that the value is not quoted, despite that requirement.\nThe absence of quoting is highly likely, IMHO, as a carryover from the\n\"historical\" implementations.\n\nThe principle that a browser should be liberal in what it accepts\nis unavoidable on today's Web.  But that principle should not apply to IETF\nspecs, themselves.  The comments/explanations sections should include\n\"Note that some UAs incorrectly ...\" caveats to assist implementors in\ncoping with today's Web, but the specs themselves should not waffle on\nwhat is correct versus incorrect.\n\n\n>> Concerning Foteos's suggestion about reserving the attribute name\n>> \"CommentURL\", sure.  Concerning CommentURL itself, I'll think about that\n>> some more.  The risk in adding it is that supporting it has implications\n>> for browsers and browser vendors, and they haven't seemed too keen about\n>> RFC 2109 (and successors) as it is.\n>\n>But I don't recall a vendor objection to the suggestion. If I were the\n>vendor, I'd like this one because it would be a real value add to handle\n>it well and I believe it's a real win for users.\n\nI similarly did not perceive vendor objections to the suggestion,\njust a word of caution about not getting caught in a loop of cookie\nexchanges when retrieving the comments.  That's simple to deal with in\nany decent implementation.  The discussion turned to one about a PEP\nor PEP-like extension, which is a good idea, and hopefully will be\nforthcoming, but is not an alternative to the commentURL.\n\nIf you do not include commentURL, with the ability to bring charset\nand language negotiation into play, then the criticism still stands that\nthe comment attribute, whose value must conform to header constraints, does\nnot adequately accomodate i18n concerns.  Please address that criticism.\nThe comment, in effect, is trying to stick a body in a header.  That\ncan't be done adequately, in contrast to a commentURL.\n\n\n>> Concerning Foteos's suggestion that all the attribute names be reserved\n>> from use as cookie NAMEs, it's unnecessary.  The cookie NAME=VALUE always\n>> comes first in the Set-Cookie2 header, so you can always distinguish it\n>> from any attributes.\n>\n>Yeah, I scratched my head on that one for a long time before I concluded \n>I could see no reason for breakage if the attribute names weren't \n>reserved.\n\nI was not suggesting that Versions 1 cookie attributes be\ndesignated as reserved.  Of course that's nonsense.  I was suggesting\nthat the comments/explanations sections state explicitly, as an aid\nto successful and reliable implementation of this spec, that use of\nthe new, Version 1 cookie attributes as names for \"historical\" cookies\nin \"historical\" Set-Cookie headers (as opposed to \"modern\" Set-Cookie2\nheaders) is \"ill-advised\".  Has anyone besides the Lynx folks actually\ntried to implement this based on the draft as it presently stands?  No\noffense intended, honest, but it's as muddled as this discussion about\nit, particularly when trying to cope with that ill-advised combinatorial\nrequirement.  Frankly, that will be a bigger impediment to it's successful\ndeployment than any media spin being generated by some vendors.  The\nconcerns about invasion of privacy on the Web are not going to just\ngo away due to media spin.  Whatever might be the present plans of some\nvendors, no one can foresee what market forces, or perhaps legislative\nforces, might change those plans.  I think it is very important for the\nIETF to get out a *stable* State Management RFC that adequately addresses\nthe concerns about privacy which have been discussed at length in this WG,\nand about which adequate, albeit not total, consensus has been reached.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "-----Original Message-----\nFrom:David W. Morris [SMTP:dwm@xpasc.com]\nSent:Thursday, July 17, 1997 2:20 AM\nTo:Dave Kristol\nCc:http-wg@cuckoo.hpl.hp.com; Foteos Macrides\nSubject:Re: LAST CALL, \"HTTP State Management Mechanism (Rev1) \" to \nPropo\n\n\n\nOn Wed, 16 Jul 1997, Dave Kristol wrote:\n\n> At 11:27 PM -0700 7/14/97, David W. Morris wrote:\n> [hisresponses to my responses to his comments to the above I-D, \nwith\n> additional comments by me on comments by Foteos Macrides]\n>\n>\n> >> > An earlier comment was made about quotes in the \"Port\" \nattribute,\n> >> > but I think there are additional problems with the syntax as\n> >> > specified and suggest that:\n> >> >\n> >> > ::                 |       \"Port\" [ \"=\" <\"> 1#port-list <\"> ]\n> >> > :: port-list       |       1#DIGIT\n> >> >\n> >> > be replaced with:\n> >> >\n> >> >                    |       \"Port\" [ \"=\" portnum | <\"> 1#portnum \n<\"> ]\n> >> >    portnum         =       1*DIGIT\n> >> >\n> >> > If I correctly understand RFC2068 syntax, 1#X means 1 or more\n> >> > occurances of X delimited by commas. My changes fix the \"=\"\n> >> > in port-list, the 1#DIGIT in port list and make the quotes\n> >> > optional for the single port case.\n>\n> You're absolutely right about my having botched the syntax.  I'll \nhave to\n> fix this up.  Thanks!\n>\n> Concerning making the quotes optional for a single port number:  I \naccept\n> Foteos's argument that it's easy to handle.  I'll allow that in the \nspirit\n> of \"be liberal in what you accept\", an implementation may want to \nhandle it\n> that way, but I still think it's a really bad idea for the syntax to \nbe\n> *specified* that way.  Apart from making the syntax (marginally) \nmore\n> complex, the (proposed) syntax above invites errors of omission, \nwhere a\n> server goes from sending a single port number to sending more than \none and\n> the implementor has to remember to add quotes to get it right.\n\nI actually believed you had intended the quotes to be optional ... my\nmain concern was getting the syntax right so on the quotes I'm \nsatisfied\nwith whatever you choose because I think the port=xxxx case would not \nbe\nused since it is exactly the same case as 'port' with no value.\n\n> Concerning remarks about requiring FQHN:  I'll have to think this \nthrough\n> more carefully when I get back from vacation.\n\nSounds fair to me ... you've been working hard enough from vacation.\n\n> Concerning Foteos's suggestion about reserving the attribute name\n> \"CommentURL\", sure.  Concerning CommentURL itself, I'll think about \nthat\n> some more.  The risk in adding it is that supporting it has \nimplications\n> for browsers and browser vendors, and they haven't seemed too keen \nabout\n> RFC 2109 (and successors) as it is.\n\nBut I don't recall a vendor objection to the suggestion. If I were \nthe\nvendor, I'd like this one because it would be a real value add to \nhandle\nit well and I believe it's a real win for users.\n\nAll of the web site and software developers I have spoken to on this \nwould love the opportunity to provide a link to explain what and why \nthey are doing what they are doing to help address the spread of \nmisinformation (particularly by the media)  This would be a big win \nfor everyone...\n\n>\n> Concerning Foteos's suggestion that all the attribute names be \nreserved\n> from use as cookie NAMEs, it's unnecessary.  The cookie NAME=VALUE \nalways\n> comes first in the Set-Cookie2 header, so you can always distinguish \nit\n> from any attributes.\n\nYeah, I scratched my head on that one for a long time before I \nconcluded\nI could see no reason for breakage if the attribute names weren't\nreserved.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "STATUS100 Re: Proposed resolutio",
            "content": "** Reply to note from \"Scott Lawrence\" <lawrence@agranat.com> Thu, 17 Jul 1997 10:33:32 -0400\n>  An observation: I find it interesting that the set of rules to limit \n>  use of 100 Continue seems to require such a long specification, \n>  given that the original mechanism was so simple...\nPerhaps, as Koen suggested, we should use Occam's razor to cut this\nfrom the spec.  \n\n[...]\n\nJM>    o  An origin server SHOULD NOT send a 100 (Continue) response if\nJM>       has already received some or all of the request body for the\nJM>       corresponding request.\n>   \n>     - I think that it is poor design to encourage look-ahead in the\n>       data stream to determine whether or not body has been received.\nI agree completely.\n\n[...]\nJM>...\nJM>    o  If an origin server receives a request that does not include an\nJM>       \"Expect\" request-header field with the \"100-continue\"\nJM>       expectation, and the request includes a request body, and the\nJM>       server responds with an error status before reading the entire\nJM>       request body from the transport connection, then the server\nJM>       SHOULD NOT close the transport connection until it has read the\nJM>       entire request, or until the client closes the connection.\nJM>       Otherwise, the client may not reliably receive the response\nJM>       message.\nThis is excellent advice, but it does expose the server implementer to\nattacks where the amount of data is *very* large or the datastream is\nself-defining (e.g. chunked).\n\n[...]\n>   \n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: Summary of STATUS100 issu",
            "content": "** Reply to note from koen@win.tue.nl (Koen Holtman) Thu, 10 Jul 1997 21:45:24 +0200 (MET DST)\n>   \n[...]\n>   \n> Urgl.  I get the feeling that 100 is taking far too much of our scarce\n> time, given that it is in no way central to basic HTTP operation.  I\n> also have the feeling that is taking so much time because it had loads\n> of *unnecessary complexity* from the start.\n>   \n> I think we are trying to untangle a knot which should be cut instead.\n>   \n> I move that we either simplify the thing, or remove it from the spec\n> entirely.  We already know that 100 processing is a pain to implement\n> in servers and clients, and I think it is time to question whether we\n> cannot achieve the same goals in a much simpler way.\n>   \nI agree. The KISS principle applies.\n\n[...]\n>   \n> To meet (1) above, we could define a new method by which the client\n> can ask the server if an X request with the following headers would be\n> allowed.  Such a request could look like:\n>   \n> ASK-IF-ALLOWED /a/page HTTP/1.1\n> Is-method-allowed: PUT\n> Authorization: ......\n>  etc. \n>   \n>   \n> I don't think things need to be more complicated than that.\nWhy not OPTIONS?\n\n>   \n> Koen.\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE VARY: Proposed wordin",
            "content": "This is what I believe to be the consensus between Jeff Mogul, Dave Morris,\nKoen, and myself regarding rewording the description of the Vary header\nfield in section 12.1, 12.3, 13.6 and 14.43. This text is to completely\nreplace the existing wording in section 13.6 and 14.43 and to edit section\n12.1 and 12.3\n\n**********\n\n12.1 Server-driven Negotiation\n\nChange the last two paragraphs from\n\n   HTTP/1.1 origin servers MUST include an appropriate Vary header\n   field (section 14.43) in any cachable response based on server-\n   driven negotiation. The Vary header field describes the \n   dimensions over which the response might vary (i.e. the \n   dimensions over which the origin server picks its \"best guess\" \n   response from multiple representations).\n\n   HTTP/1.1 public caches MUST recognize the Vary header field\n   when it is included in a response and obey the requirements \n   described in section 13.6 that describes the interactions\n   between caching and content negotiation.\n\nto\n\n   The Vary header field can be used to express the parameters used\n   to select a representation subject to server-driven negotiation.\n   See section 13.6 for use of the Vary header field by caches and\n   section 14.43 for use of the Vary header field by servers.\n\n12.3 Transparent Negotiation\n\nChange the last paragraph from\n\n   This specification does not define any mechanism for\n   transparent negotiation, though it also does not prevent any \n   such mechanism from being developed as an extension and used \n   within HTTP/1.1. An HTTP/1.1 cache performing transparent \n   negotiation MUST include a Vary header field in the response \n   (defining the dimensions of its variance) if it is cachable to \n   ensure correct interoperation with all HTTP/1.1 clients. The \n   agent-driven negotiation information supplied by the origin \n   server SHOULD be included with the transparently negotiated \n   response.\n\nto\n\n   This specification does not define any mechanism for\n   transparent negotiation, though it also does not prevent any \n   such mechanism from being developed as an extension and used \n   within HTTP/1.1.\n\n13.6 Caching Negotiated Responses\n\nUse of server-driven content negotiation (section 12), as indicated by the\npresence of a Vary header field in a response, alters the conditions and\nprocedure by which a cache can use the response for subsequent requests.\nSee section 14.43 for use of the Vary header field by servers.\n\nA server SHOULD use the Vary header field to inform a cache of what\nrequest-header fields were used to select among multiple representations of\na cachable response subject to server-driven negotiation. The set of header\nfields named by the Vary field value is known as the \"selecting\"\nrequest-headers.\n\nWhen the cache receives a subsequent request whose Request-URI specifies\none or more cache entries including a Vary header field, the cache MUST NOT\nuse such a cache entry to construct a response to the new request unless\nall of the field-names in the cached Vary header field are present in the\nnew request, and all of the stored selecting request-headers from the\nprevious request match the corresponding request-headers in the new request. \n\nThe selecting request-headers from two requests are defined to match if and\nonly if the selecting request-headers in the first request can be\ntransformed to the selecting request-headers in the second request by\nadding or removing linear whitespace (LWS) at places where this is allowed\nby the corresponding BNF, and/or combining multiple message-header fields\nwith the same field name following the rules about message headers in\nsection 4.2.\n\nIf the selecting request-headers do not match, then the cache MUST forward\nthe request to the origin server; it MAY forward the request as a\nconditional request.\n\nIf an entity tag was assigned to the representation, the forwarded request\nSHOULD be conditional and include the entity tags in an If-None-Match\nheader field from all its cache entries for the Request-URI. This conveys\nto the server the set of entities currently held by the cache, so that if\nany one of these entities matches the requested entity, the server can use\nthe ETag header field in its 304 (Not Modified) response to tell the cache\nwhich entry is appropriate. If the entity-tag of the new response matches\nthat of an existing entry, the new response SHOULD be used to update the\nheader fields of the existing entry, and the result MUST be returned to the\nclient.\n\nIf the representation was selected using criteria not limited to the\nrequest-headers, then the server SHOULD include a \"Vary: *\" header field in\nthe response if it is cachable. In this case, a cache MUST NOT use the\nresponse in a reply to a subsequent request unless the cache relays the new\nrequest to the origin server in a conditional request and the server\nresponds with 304 (Not Modified), including an entity tag or\nContent-Location that indicates which entity should be used.\n\nIf any of the existing cache entries contains only partial content for the\nassociated entity, its entity-tag SHOULD NOT be included in the\nIf-None-Match header field unless the request is for a range that would be\nfully satisfied by that entry.\n \nIf a cache receives a successful response whose Content-Location field\nmatches that of an existing cache entry for the same Request-URI, whose\nentity-tag differs from that of the existing entry, and whose Date is more\nrecent than that of the existing entry, the existing entry SHOULD NOT be\nreturned in response to future requests, and should be deleted from the cache.\n\n14.43 Vary\n\nThe Vary field value indicates the set of request-header fields that fully\ndetermines, while the response is fresh, whether a cache may use the\nresponse to reply to a subsequent request without revalidation. For\nuncachable or stale responses, the Vary field value advises the user agent\nabout the criteria that were used to select the representation. A Vary\nfield value of \"*\" implies that a cache cannot determine from the request\nheaders of a subsequent request whether this response is the appropriate\nrepresentation. See section 13.6 for use of the Vary header field by caches.\n\n       Vary  = \"Vary\" \":\" ( \"*\" | 1#field-name )\n\nAn HTTP/1.1 server SHOULD include a Vary header field with any cachable\nresponse that is subject to server-driven negotiation. Doing so allows a\ncache to properly interpret future requests on that resource and informs\nthe user agent about the presence of negotiation on that resource. A server\nMAY include a Vary header field with a non-cachable response that is\nsubject to server-driven negotiation, since this might provide the user\nagent with useful information about the dimensions over which the response\nvaries at the time of the response.\n\nA Vary field value consisting of a list of field-names signals that the\nrepresentation selected for the response is based on a selection algorithm\nwhich considers ONLY the listed request-header field values in selecting\nthe most appropriate representation. A cache MAY assume that the same\nselection will be made for future requests with the same values for the\nlisted field names, for the duration of time in which the response is fresh.\n\nThe field-names given are not limited to the set of standard request-header\nfields defined by this specification. Field names are case-insensitive.\n\nA Vary field value of \"*\" signals that unspecified parameters not limited\nto the request-headers (e.g., the network address of the client), play a\nrole in the selection of the response representation. Subsequent requests\non that resource can only be properly interpreted by the origin server, and\nthus a cache MUST forward a (possibly conditional) request even when it has\na fresh response cached for the resource. The \"*\" value MUST NOT be\ngenerated by a proxy server; it may only be generated by an origin server.\n\n**********\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium\nhttp://www.w3.org/People/Frystyk\n\n\n\n"
        },
        {
            "subject": "ISSUE PROXYAUTHORIZATION: Proposal wordin",
            "content": "http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0040.html\n\nThis text is to completely replace the existing wording in chapter 11, and\nto add a one-liner in section 13.5.1.\n\nThe remaining MUSTs could potentially be downgraded to SHOULDs as it would\nnot cause any interoperability problems, but I have left them as in the\noriginal text.\n\n**********\n\n11 Access Authentication \n\nHTTP provides a simple challenge-response authentication mechanism which\nMAY be used by a server to challenge a client request and by a client to\nprovide authentication information. It uses an extensible, case-insensitive\ntoken to identify the authentication scheme, followed by a comma-separated\nlist of attribute-value pairs which carry the parameters necessary for\nachieving authentication via that scheme.\n\n       auth-scheme    = token\n       auth-param     = token \"=\" quoted-string\n\nThe 401 (Unauthorized) response message is used by an origin server to\nchallenge the authorization of a user agent. This response MUST include a\nWWW-Authenticate header field containing at least one challenge applicable\nto the requested resource. The 407 (Proxy Authentication Required) response\nmessage is used by a proxy to challenge the authorization of a client and\nMUST include a Proxy-Authenticate header field containing a challenge\napplicable to the proxy for the requested resource.\n\n       challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\n       realm          = \"realm\" \"=\" realm-value\n       realm-value    = quoted-string\n\nThe realm attribute (case-insensitive) is required for all authentication\nschemes which issue a challenge. The realm value (case-sensitive), in\ncombination with the canonical root URL (see section 5.1.2) of the server\nbeing accessed, defines the protection space. These realms allow the\nprotected resources on a server to be partitioned into a set of protection\nspaces, each with its own authentication scheme and/or authorization\ndatabase. The realm value is a string, generally assigned by the origin\nserver, which may have additional semantics specific to the authentication\nscheme.\n\nA user agent that wishes to authenticate itself with an origin\nserver--usually, but not necessarily, after receiving a 401\n(Unauthorized)--MAY do so by including an Authorization header field with\nthe request. A client that wishes to authenticate itself with a\nproxy--usually, but not necessarily, after receiving a 407 (Proxy\nAuthentication Required)--MAY do so by including a Proxy-Authorization\nheader field with the request. Both the Authorization field value and the\nProxy-Authorization field value consists of credentials containing the\nauthentication information of the client for the realm of the resource\nbeing requested.\n\n       credentials    = basic-credentials\n                      | auth-scheme #auth-param\n\nThe domain over which credentials can be automatically applied by a client\nis determined by the protection space. If a prior request has been\nauthorized, the same credentials MAY be reused for all other requests\nwithin that protection space for a period of time determined by the\nauthentication scheme, parameters, and/or user preference. Unless otherwise\ndefined by the authentication scheme, a single protection space cannot\nextend outside the scope of its server.\n\nIf the origin server does not wish to accept the credentials sent with a\nrequest, it SHOULD return a 401 (Unauthorized) response. The response MUST\ninclude a WWW-Authenticate header field containing at least one (possibly\nnew) challenge applicable to the requested resource. If a proxy does not\naccept the credentials sent with a request, it SHOULD return a 407 (Proxy\nAuthentication Required). The response MUST include a Proxy-Authenticate\nheader field containing a (possibly new) challenge applicable to the proxy\nfor the requested resource.\n\nThe HTTP protocol does not restrict applications to this simple\nchallenge-response mechanism for access authentication. Additional\nmechanisms MAY be used, such as encryption at the transport level or via\nmessage encapsulation, and with additional header fields specifying\nauthentication information. However, these additional mechanisms are not\ndefined by this specification.\n\nThe WWW-Authenticate and Authorization header fields are end-to-end headers\nfollowing the rules found in section 14.8 and 14.46. Both the\nProxy-Authenticate and the Proxy-Authorization header fields are hop-by-hop\nheaders (see section 13.5.1).\n\n11.1 Basic Authentication Scheme \n\nThe \"basic\" authentication scheme is based on the model that the client\nmust authenticate itself with a user-ID and a password for each realm. The\nrealm value should be considered an opaque string which can only be\ncompared for equality with other realms on that server. The server will\nservice the request only if it can validate the user-ID and password for\nthe protection space of the Request-URI. There are no optional\nauthentication parameters.\n\nUpon receipt of an unauthorized request for a URI within the protection\nspace, the origin server may respond with a challenge like the following:\n\n       WWW-Authenticate: Basic realm=\"WallyWorld\"\n\nwhere \"WallyWorld\" is the string assigned by the server to identify the\nprotection space of the Request-URI. A proxy may respond with the same\nchallenge using the Proxy-Authenticate header field.\n\nTo receive authorization, the client sends the userid and password,\nseparated by a single colon (\":\") character, within a base64 [7] encoded\nstring in the credentials.\n\n       basic-credentials = \"Basic\" SP basic-cookie\n       basic-cookie   = <base64 [7] encoding of user-pass,\n                        except not limited to 76 char/line>\n       user-pass   = userid \":\" password\n       userid      = *<TEXT excluding \":\">\n       password    = *TEXT\n\nUserids might be case sensitive.\n\nIf the user agent wishes to send the userid \"Aladdin\" and password \"open\nsesame\" to an origin server, it would use the following header field:\n\n       Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==\n\nIf a client wishes to send the same userid and password to a proxy, it\nwould use the Proxy-Authorization header field. See section 15 for security\nconsiderations associated with Basic authentication.\n\n11.2 Digest Authentication Scheme \n\nNote for the RFC editor: This section is reserved for including the Digest\nAuthentication specification, or if the RFC editor chooses to issue a\nsingle RFC rather than two RFC's, this section should be deleted.\n\n13.5.1 End-to-end and Hop-by-hop Headers\n\nChange\n\n   The following HTTP/1.1 headers are hop-by-hop headers:\n      ? Connection\n      ? Keep-Alive\n      ? Public\n      ? Proxy-Authenticate \n      ? Transfer-Encoding\n      ? Upgrade\n\nto\n\n   The following HTTP/1.1 headers are hop-by-hop headers:\n      ? Connection\n      ? Keep-Alive\n      ? Public\n      ? Proxy-Authenticate \n      ? Proxy-Authorization\n      ? Transfer-Encoding\n      ? Upgrade\n\nThanks,\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "Responses to comments from Scott Lawrence:\n\n      An observation: I find it interesting that the set of rules to limit\n      use of 100 Continue seems to require such a long specification,\n      given that the original mechanism was so simple...\n\nH. L. Mencken once said \"For every complex problem, there is a solution\nthat is simple, neat, and wrong.\"\n\nAnyway, a lot of the lengthening that I added was to address the\ncomplaints about ambiguity in the previous draft.\n    \n        Requirements for HTTP/1.1 or later origin servers:\n        o  Upon receiving a request which includes an \"Expect\" request-header\n           field with the \"100-continue\" expectation, an origin server must\n           either respond with 100 (Continue) status and continue to read\n           from the input stream, or respond with an error status. If it\n           responds with an error status, it MAY close the transport (TCP)\n           connection or it MAY continue to read and discard the rest of the\n           request. It MUST NOT perform the requested method if it returns\n           an error status.\n    \n  I would reword this to reflect that the 100 Continue response MUST\n  be sent after the request _headers_ have been recieved, since the\n  'request' includes the body.\n\nYou're right that it needs a re-wording (and one \"must\" should be\na \"MUST\"), but I don't think that the requirement is exactly that\nthe server MUST wait for the entire set of request headers.  It\ncould, for example, decide to send the 100 Continue as soon as it\nsees the Expect header, if it has no intention of rejecting the\nrequest based on other headers.\n\nHow about:\n   o  Upon receiving a request which includes an \"Expect\" request-header\n      field with the \"100-continue\" expectation, an origin server MUST\n      either respond with 100 (Continue) status and continue to read\n      from the input stream, or respond with an error status.  The\n      origin server MUST NOT wait for the request body before sending\n      the 100 (Continue) response.  If it\n      responds with an error status, it MAY close the transport (TCP)\n      connection or it MAY continue to read and discard the rest of the\n      request. It MUST NOT perform the requested method if it returns\n      an error status.\n\nNote that since the decision between sending 100 (Continue) or \nsome error status could take the server an arbitrary amount of\ntime, so we can't actually say how soon after receiving the request\nheaders the server must send a response.\n    \n        o  An origin server SHOULD NOT send a 100 (Continue) response if\n           the request message does not include an \"Expect\" request-header\n           field with the \"100-continue\" expectation, and MUST NOT send a\n           100 (Continue) response if such a request comes from an HTTP/1.0\n           (or earlier) client.\n    \n        o  An origin server SHOULD NOT send a 100 (Continue) response if\n           has already received some or all of the request body for the\n           corresponding request.\n    \n   I don't see the point of these two SHOULD NOTs, since the client\n   MUST be prepared to accept an unexpected 100 response anyway.\n\nWe do have an obligation to limit the number of unnecessary bits that\ntravel over the Internet.  One might argue that adding\nHTTP/1.1 100 Continue<CR><LF><CR><LF>\nto every response to a PUT or POST is insignificant, or one\nmight argue the contrary.  Anyway, the second rule seems innocuous\n(since if the server is receiving the request body, manifestly\nthe client isn't waiting for the 100 Continue).  The first rule\nmight cause some delays for existing clients that wait for\n100 Continue, if there are any.\n\n      Arguing against these rules:\n    \n- As noted elsewhere, existing 1.1 servers (yes, there are some)\n  won't have been coded to include these restrictions (since the\n  Expect header was only suggested a couple of weeks ago).\n    \nWe shouldn't unnecessarily break any deployed systems.  On the other\nhand, the use of words such as SHOULD NOT will ultimately be used to\ndetermine whether an implementation complies with the HTTP/1.1\nstandard, and since that standard does not yet exist in its final\nform, it's unlikely that any existing \"1.1\" server could actually\nmeet the final-standard rules for conditional compliance, anyway.\n\n- I think that it is poor design to encourage look-ahead in the\n  data stream to determine whether or not body has been received.\n    \nThe language does not specifically require the server to look ahead\ninto the data stream.  However, if the server has actually read some\nof the request body, I see no harm in encouraging efficient use of\nthe Internet.\n\n    ...\n        o  If an origin server receives a request that does not include an\n           \"Expect\" request-header field with the \"100-continue\"\n           expectation, and the request includes a request body, and the\n           server responds with an error status before reading the entire\n           request body from the transport connection, then the server\n           SHOULD NOT close the transport connection until it has read the\n           entire request, or until the client closes the connection.\n           Otherwise, the client may not reliably receive the response\n           message.\n    \n   Does this amount to a rule for the purpose of avoiding bugs in some\n   TCP implementations?  I can live with this rule since it is not a\n   MUST.\n\nYes.  I'd appreciate it if Henrik Frystyk or Jim Gettys could review\nthis rule, since I included it based on my understanding of their\nimplementation experience and suggestions.\n    \n        For compatibility with RFC 2068, a server MAY send a 100 (Continue)\n        status in response to an HTTP/1.1 PUT or POST request that does not\n        include an \"Expect\" request-header field with the \"100-continue\"\n        expectation.  This exception, the purpose of which is to minimize\n        any client processing delays associated with an undeclared wait for\n        100 (Continue) status, applies only to HTTP/1.1 requests, and not to\n        requests with any other HTTP-version value.\n    \n   I think that making this a special case allowed _only_ for HTTP/1.1\n   is going too far.  We already have the requirement that 100 Continue\n   must be accepted by the client anyway.  We're talking about as few\n   as 'HTTP/1.1 100CRLF\" - 14 bytes - 23 if you send the ' Continue'.\n   What justification is there for the complexity of this restriction?\n\nSee above.  But I agree with you that this is a judgement call.\nI'm just worried that we may not need to send these extra bytes,\nand we have the chance now to discourage 23*N extra bytes over\nthe Internet for many years to come (where N is a large number!)\n\nWhat do other people think?\n    \n   Furthermore, I think that the above rule, which is stated using\n   'MAY', is really a 'MUST NOT':\n \n     A server using any revision of HTTP later than HTTP/1.1 MUST NOT\n     send a 100 (Continue) status in response to an HTTP/1.1 PUT or\n     POST request that does not include an \"Expect\" request-header\n     field with the \"100-continue\" expectation.\n\nI don't think the HTTP/1.1 spec can actually define the behavior\nof an implementation of a later version of HTTP, without also\nat least defining that behavior for an implementation of HTTP/1.1.\nThis is basically what you said, I think:\n   I don't believe that this rule on future versions of the protocol\n   meets that test.\nbut part of my proposal which you are paraphrasing here doesn't\nhave the same problem.  (Perhaps, though, I need to change\n\"Requirements for HTTP/1.1 or later XXX\" to \"Requirements for\nHTTP/1.1 XXX\".)\n\nAt any rate, your proposed change doesn't address the RFC2068\ncompatibility issue, which is that *existing* clients might\nbe waiting for 100 (Continue) without having sent the Expect\nheader.  (Unless you are assuming that all such deployed clients\nare gone by the time that HTTP/1.2 arrives.)\n \n     Requirements for HTTP/1.1 or later proxies:\n  ...\n     o  If the proxy knows that the version of the next-hop server is\nHTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\nrespond with a 419 (Expectation Failed) status.\n \n   How is the client to know that repeating the request using HTTP/1.0 could\n   resolve this situation?  Should clients always back off to 1.0 if\n   they receive a 419?\n\nIf you send \"Expect: 100-continue\" and you get a 419 (Expectation\nfailed) response, then you know that there is no guarantee that\nyou will get a 100 (Continue) response before you send the request\nbody, no longer how long you wait.\n\nAt this point, the user-agent has two choices:\n(1) fall back to the HTTP/1.0-like behavior (this is NOT\nthe same thing as falling back to HTTP/1.0), i.e., send\nthe request body without waiting.\n(2) tell the user that the request cannot be accomplished.\n\nMy guess is that, in reality, relatively few (if any) general-purpose\napplications are actually going to require \"Expect: 100-continue\" in\nthe first place, and so this is really only an issue for applications\nin which the need for server pre-confirmation is actually significant.\nWhen we write the HTTP/1.1 spec, we can't presume to decide for these\napplications whether it is semantically necessary to get the server's\npre-confirmation (i.e., 100 response).  All we can do is to give them\na relatively unambiguous signal about whether it will be available\nor not.  The application implementor has to make the final decision.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "On the general topic of \"is the whole status 100 thing worth having?\":\n\nI've always been neutral on the topic.  However, several members\nof the HTTP/1.1 editorial group are firmly in favor of keeping\nit, as are at least some other members of the full working group.\nAs far as I can tell, the consensus of the editorial group is\n\"keep it, but fix it.\"\n\nI got stuck with the responsibility to fix it.  My interest is\nto try to see that, if we have it, then it is done right.\n\nKoen writes:\n    > To meet (1) above, we could define a new method by which the client\n    > can ask the server if an X request with the following headers would be\n    > allowed.  Such a request could look like:\n    >   \n    > ASK-IF-ALLOWED /a/page HTTP/1.1\n    > Is-method-allowed: PUT\n    > Authorization: ......\n    >  etc. \n    >   \n    >   \n    > I don't think things need to be more complicated than that.\n\nI don't believe that it's feasible to add a new method at this point,\nespecially since it would not interoperate with existing proxies\nand servers.  Koen's ASK-IF-ALLOWED method might have been a good\ndesign for the original HTTP protocol, but I don't see how to deploy\nit today, without a lot of complexity.\n\nRichard Gray added:\n\n   Why not OPTIONS?\n\nUsing OPTIONS is a possibility, but we are still waiting for\na formal specification for OPTIONS (everyone seems to agree\nthat RFC2068 doesn't explain enough how to use it).  Also,\ncan you suggest a way of using OPTIONS that doesn't add round\ntrips to every PUT/POST interaction?\n\nRichard continues:\n\n    o  If an origin server receives a request that does not include an\n       \"Expect\" request-header field with the \"100-continue\"\n       expectation, and the request includes a request body, and the\n       server responds with an error status before reading the entire\n       request body from the transport connection, then the server\n       SHOULD NOT close the transport connection until it has read the\n       entire request, or until the client closes the connection.\n       Otherwise, the client may not reliably receive the response\n       message.\n\n    This is excellent advice, but it does expose the server implementer\n    to attacks where the amount of data is *very* large or the\n    datastream is self-defining (e.g. chunked).\n\nThere are, of course, may other kinds of denial-of-service attack.\nBut perhaps it would help to add one more sentence to that rule:\n\n    o  If an origin server receives a request that does not include an\n       \"Expect\" request-header field with the \"100-continue\"\n       expectation, and the request includes a request body, and the\n       server responds with an error status before reading the entire\n       request body from the transport connection, then the server\n       SHOULD NOT close the transport connection until it has read the\n       entire request, or until the client closes the connection.\n       Otherwise, the client may not reliably receive the response\n|      message.  However, this requirement should not be construed\n|      as preventing a server from defending itself against\n|      denial-of-service attacks, or from badly broken client\n|      implementations.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "Maurizio Codogno <mau@beatles.cselt.it> writes:\n\n     8.2.2 Monitoring connections for error status messages\n     \n       An HTTP/1.1 (or later) client sending a message-body SHOULD\n       monitor the network connection for an error status while it is\n       transmitting the request. If the client sees an error status, it\n       SHOULD immediately cease transmitting the body. If the body is\n       being sent using a \"chunked\" encoding (section 3.6), a zero\n       length chunk and empty footer MAY be used to prematurely mark\n       the end of the message. If the body was preceded by a\n       Content-Length header, the client MUST close the connection.\n    \n    This is present in RFC 2068 too, but it puzzles me. Why just the client\n    and not the server? could someone explain it to me?\n\nMaybe this is a little ambiguous.  It basically says that the\nclient should be watching to see if the server says something\nlike \"HTTP/1.1 401 Unauthorized\" while the client is sending\nthe request body.  This prevents the server from having to\nconsume a lot of unnecessary bytes, and prevents the Internet\nfrom having to carry those bytes.  At the same time (if the\nclient is using \"chunked\") it avoids having to shut down\n(and probably reopen) a persistent TCP connection, which is\nan expensive operation.\n\nThe situation in the other direction is different.  Once the\nserver has started sending a response body, the client has no\nway to send an \"error status\", except to close the TCP connection.\nI guess we thought it was obvious that, if the client closes the\nTCP connection, then the server should stop sending its response.\n    \nAt some point in the past, we discussed defining a new HTTP\nmechanism, perhaps together with TCP Urgent Pointer mechanism,\nfor the client to abort a request without closing the connection.\nBut this proposal did not meet with sufficient approval.\n\nIf the request method is not idempotent, the\nclient SHOULD NOT retry the request without user confirmation.\n    \n    I'd prefer a MUST NOT, especially in light of the following\n    sentence.  As far as I can parse English language, it would mean\n    that either the user agent knows for sure that it is safe to retry\n    the request or it warns the user.\n\nI've been given encouragement, by the working group, to avoid\nusing MUST when SHOULD is sufficient.  In this case, it seems\nplausible that there may be some applications in which user\nconfirmation is hard to obtain, and the risks associated with\nnot finishing the request are less important than the risks\nassociated with non-idempotency.  At any rate, it might be hard\nto rigorously define when a request is truly non-idempotent\n(this is another issue that has recently come up).  So we should\nnot place mandatory restrictions on the implementors that might\nbe too strict.\n    \n    As for the exception for HTTP/1.1 only, am I correcting in\n    inferring that the special case is just for interoperability with\n    existing implementations, since in future versions no 100 status\n    will be sent only if the client sends an Expect: header too?\n    \nYes.  That's why it starts with \"For compatibility with RFC 2068.\"\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "** Reply to note from Jeffrey Mogul <mogul@pa.dec.com> Thu, 17 Jul 97 14:40:59 MDT\n>   \n> On the general topic of \"is the whole status 100 thing worth having?\":\n>   \n> I've always been neutral on the topic.  However, several members\n> of the HTTP/1.1 editorial group are firmly in favor of keeping\n> it, as are at least some other members of the full working group.\n> As far as I can tell, the consensus of the editorial group is\n> \"keep it, but fix it.\"\n>   \n> I got stuck with the responsibility to fix it.  My interest is\n> to try to see that, if we have it, then it is done right.\nUnderstood, just wanted to weigh in on the \"I don't really like it\" but\nif we have to keep it let's fix it right\" side.\n\n>   \n> Koen writes:\n>     > To meet (1) above, we could define a new method by which the client\n>     > can ask the server if an X request with the following headers would be\n>     > allowed.  Such a request could look like:\n>     >   \n>     > ASK-IF-ALLOWED /a/page HTTP/1.1\n>     > Is-method-allowed: PUT\n>     > Authorization: ......\n>     >  etc. \n>     >   \n>     >   \n>     > I don't think things need to be more complicated than that.\n>   \n> I don't believe that it's feasible to add a new method at this point,\n> especially since it would not interoperate with existing proxies\n> and servers.  Koen's ASK-IF-ALLOWED method might have been a good\n> design for the original HTTP protocol, but I don't see how to deploy\n> it today, without a lot of complexity.\n>   \n> Richard Gray added:\n>   \n>    Why not OPTIONS?\n>   \n> Using OPTIONS is a possibility, but we are still waiting for\n> a formal specification for OPTIONS (everyone seems to agree\n> that RFC2068 doesn't explain enough how to use it).  Also,\n> can you suggest a way of using OPTIONS that doesn't add round\n> trips to every PUT/POST interaction?\nI thought Roy explained fairly clearly that OPTIONS is meant to be\nextensible.  So, you invent a MIME type for the message you want to\nsend, and a format for the body:\n  http/query-authorization\n  METHOD=PUT\n\nTrue it would add round trips, but you have the same problem with the\n\"Expect\" header, right?\n\nIt occurs to me that another workaround would be to issue HEAD against\nthe resource...\n\n(as an aside, it occured to me while looking at this, that TRACE and\nOPTIONS are not listed as idempotent, but I don't see why they are not;\ndid I miss discussion of this somewhere?)\n\n>   \n> Richard continues:\n>   \n>     o  If an origin server receives a request that does not include an\n>        \"Expect\" request-header field with the \"100-continue\"\n>        expectation, and the request includes a request body, and the\n>        server responds with an error status before reading the entire\n>        request body from the transport connection, then the server\n>        SHOULD NOT close the transport connection until it has read the\n>        entire request, or until the client closes the connection.\n>        Otherwise, the client may not reliably receive the response\n>        message.\n>   \n>     This is excellent advice, but it does expose the server implementer\n>     to attacks where the amount of data is *very* large or the\n>     datastream is self-defining (e.g. chunked).\n>   \n> There are, of course, may other kinds of denial-of-service attack.\n> But perhaps it would help to add one more sentence to that rule:\n>   \n>     o  If an origin server receives a request that does not include an\n>        \"Expect\" request-header field with the \"100-continue\"\n>        expectation, and the request includes a request body, and the\n>        server responds with an error status before reading the entire\n>        request body from the transport connection, then the server\n>        SHOULD NOT close the transport connection until it has read the\n>        entire request, or until the client closes the connection.\n>        Otherwise, the client may not reliably receive the response\n> |      message.  However, this requirement should not be construed\n> |      as preventing a server from defending itself against\n> |      denial-of-service attacks, or from badly broken client\n> |      implementations.\nI agree, that is an improvement.\n\n>   \n> -Jeff\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: [URN] HTTP resolution protoco",
            "content": "At 3:04 PM -0700 1996-11-04, Ron Daniel wrote:\n\n>>IMHO, HTTP URN resolvers should follow HTTP 1.1 on use of \"Accept:\".\n>\n>OK, I will add words to the effect that the resolver SHOULD\n>restrict the resources returned to those in the Accept: header.\n\nNow that we have your attention, can we come up with an HTTP 1.1 extension\nmethod to do resolution rather than the magic directory hack?\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "Richard Gray writes:\n    >    Why not OPTIONS?\n    >   \n    > Using OPTIONS is a possibility, but we are still waiting for\n    > a formal specification for OPTIONS (everyone seems to agree\n    > that RFC2068 doesn't explain enough how to use it).  Also,\n    > can you suggest a way of using OPTIONS that doesn't add round\n    > trips to every PUT/POST interaction?\n    I thought Roy explained fairly clearly that OPTIONS is meant to be\n    extensible.  So, you invent a MIME type for the message you want to\n    send, and a format for the body:\n      http/query-authorization\n      METHOD=PUT\n    \nIt's great that OPTIONS is extensible, but if it is going to be\nused to solve a specific problem (such as the one in question),\nwe need to have general agreement on the syntax and semantics\nfor the specific extension.  Your suggestion may be fine, but it's\nthe first time I've seen it.\n\n    True it would add round trips, but you have the same problem with the\n    \"Expect\" header, right?\n    \nNo, the nice thing about Expect is that as long as the expectation\nis met, there is no extra round-trip.  (And if the expectation is\nnot met, then the client might not even want to retry the request).\n\n    It occurs to me that another workaround would be to issue HEAD against\n    the resource...\n    \nBut, at the moment, we have no generic way of asking, with a HEAD,\nwhether a specific PUT (i.e., one with a specific set of request\nheaders) on the same resource would be accepted by the server.\n\n    (as an aside, it occured to me while looking at this, that TRACE and\n    OPTIONS are not listed as idempotent, but I don't see why they are not;\n    did I miss discussion of this somewhere?)\n    \nActually, the whole discussion about \"idempotent methods\" in RFC2068\nis probably wrong, and I am planning to create a new \"issue\" for\nthis (as well as a proposed solution).  Mea culpa, I think I wrote\nthe current text.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "Hmm...It seems to me that the only scenario where an extra round trip is\nunavoidable is a client which is unwilling to wait for a 100 response but\nwhich wishes to send a chunked PUT or POST. Since both of these features\nare new in HTTP/1.1 this seems like it would be a very unusual situation,\nand in this case I'm not sure an extra round trip would be such a bad\nthing.\n\nTo me, it seems like the real problem is that the server has no way of\nknowing how much data to expect. Accepting a chunked PUT or POST is an all\nor nothing type of commitment. I doubt it's possible in HTTP/1.1, but it\nseems to me that the server need to be able to indicate how much data it\nis willing to accept and then allow the client to decide whether or not to\nattempt to send the request. (A client may not know how much data it has\nto send, but it may know that it will not exceed a certain threshold.)\n\n---\nGregory Woodhouse\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: [URN] HTTP resolution protoco",
            "content": "At 06:34 PM 7/17/97 -0400, John C. Mallery wrote:\n>can we come up with an HTTP 1.1 extension\n>method to do resolution rather than the magic directory hack?\n\n[The magic directory hack is Experimental RFC 2169]\nI think that would be the right thing to do. I'd be happy to see\nthe magic directory stuff go away, although for backward\ncompatibility with old HTTP servers there doesn't seem to be\nmuch of an alternative.\n\nMichael Mealling and I have done some more work on a resolution\nservices draft. It will define services such as\nI2C (Given a URI, return a description of the resource identified by the URI.\n     The Accept: header should be honored to allow the client some ability\n     to tell the resolver what sort of description is desired. This is\n     pretty much the LINK method, if memory serves. There may be some\n     interaction with the work of the WEB-DAV group as well.)\nI2R (Given a URI, return the resource. This would, one assumes, be implemented\n     in HTTP as GET.)\nI2L (Given a URI, return a URL for it.)\nI2N (Given a URI, return a URN for it. (This comes from a working group\n     that distinguishes between URNs and URLs. Not everyone in the world\n     does so. This point seems likely to get some discussion :-)  )\n\nThere are some other methods I can't recall off the top of my head,\nI'll check with Michael on the current state of the draft. Anyway,\ndoing it with proper methods seems achievable.  More later...\n\n\n\nRon Daniel Jr.              voice:+1 505 665 0597\nAdvanced Computing Lab        fax:+1 505 665 4939\nMS B287                     email:rdaniel@lanl.gov\nLos Alamos National Lab      http://www.acl.lanl.gov/~rdaniel\nLos Alamos, NM, USA, 87545  \n\n\n\n"
        },
        {
            "subject": "RE: Is 100-Continue hop-byhop",
            "content": "Either way it seems clear that we need to clean up on the language in\nthe spec so proxy implementers are clear on the need to pass 1xx\nresponses through to the client.\nYaron\n\n> -----Original Message-----\n> From:Roy T. Fielding [SMTP:fielding@kiwi.ics.uci.edu]\n> Sent:Friday, July 11, 1997 5:03 PM\n> To:Koen Holtman\n> Cc:http-wg@cuckoo.hpl.hp.com\n> Subject:Re: Is 100-Continue hop-by-hop? \n> \n> >But I guess that what he actually had in mind was \n> >\n> >  <copy request> <100 response about progress=50%> <100 response\n> about\n> >  progress=100%> <200 OK response to copy request>\n> >\n> >And this latter case would indeed work through a multiplexing proxy\n> >(though the code would have to be 101 or something, with semantics\n> >slightly different from 100). \n> \n> 101 is already being used, but yes it would definitely have to be\n> a new 1xx code.  Extensibility is meant to be used.\n> \n> .....Roy\n\n\n\n"
        },
        {
            "subject": "RE: Proposed resolution for the STATUS100 issu",
            "content": "In order to allow maximum flexibility shouldn't:\n   o  A client MUST be prepared to accept a 100 (Continue)\nstatus\n      message followed by a regular response, even if the client\ndoes\n      not expect a 100 (Continue) status message.\n\nInstead read:\nA client MUST be prepared to accept 100 (Continue) status messages...\n\n?\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From:Jeffrey Mogul [SMTP:mogul@pa.dec.com]\n> Sent:Wednesday, July 16, 1997 2:16 PM\n> To:http-wg@cuckoo.hpl.hp.com\n> Subject:Proposed resolution for the STATUS100 issue\n> \n> Warning: long message.\n> \n> This message contains proposed changes and additions to several\n> sections of RFC2068, as a resolution of the STATUS100 issue.\n> Please refer to RFC2068 for the original language (if any).\n> \n> See\n> http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#STATUS100\n> although a lot of the discussion is not easily found except by\n> looking at a lot of the HTTP-WG mailing list archive.\n> \n> Comments should be reported as soon as possible, since the HTTP/1.1\n> editorial group intends to issue a last-call on this issue within\n> the next week or so.\n> \n> -Jeff\n> \n> === \n> === Major revisions to 8.2\n> === \n> ======================\n> \n> 8.2 Message Transmission Requirements\n> \n> 8.2.1 Persistent connections and flow control\n> \n>    HTTP/1.1 servers SHOULD maintain persistent connections and use\n>    TCP's flow control mechanisms to resolve temporary overloads,\n>    rather than terminating connections with the expectation that\n>    clients will retry. The latter technique can exacerbate network\n>    congestion.\n> \n> 8.2.2 Monitoring connections for error status messages\n> \n>    An HTTP/1.1 (or later) client sending a message-body SHOULD monitor\n>    the network connection for an error status while it is transmitting\n>    the request. If the client sees an error status, it SHOULD\n>    immediately cease transmitting the body. If the body is being sent\n>    using a \"chunked\" encoding (section 3.6), a zero length chunk and\n>    empty footer MAY be used to prematurely mark the end of the\n>    message. If the body was preceded by a Content-Length header, the\n>    client MUST close the connection.\n> \n> 8.2.3 Automatic retrying of requests\n> \n>    If a client sees the transport connection close before it receives\n> a\n>    final response to its request, if the request method is idempotent\n>    (see section 9.1.2), the client SHOULD retry the request without\n>    user interaction.  If the request method is not idempotent, the\n>    client SHOULD NOT retry the request without user confirmation.\n>    (Confirmation by user agent software with semantic understanding of\n>    the application MAY substitute for user confirmation.)\n> \n> 8.2.4 Use of the 100 (Continue) status\n> \n>    The purpose of the 100 (Continue) status (see section 10.1.1) is to\n>    allow an end-client that is sending a request message with a\n> request\n>    body to determine if the origin server is willing to accept the\n>    request (based on the request headers) before the client sends the\n>    request body.  In some cases, it may either be inappropriate or\n>    highly inefficient for the client to send the body if the server\n>    will reject the message without looking at the body.\n> \n>    Requirements for HTTP/1.1 or later clients:\n>    o  If a client will wait for a 100 (Continue) response before\n> sending\n>       the request body, it MUST send an \"Expect\" request-header field\n>       (section 14.XX) with the \"100-continue\" expectation.\n> \n>    o  A client MUST be prepared to accept a 100 (Continue) status\n>       message followed by a regular response, even if the client does\n>       not expect a 100 (Continue) status message.\n> \n>    o  A client MUST NOT send an \"Expect\" request-header field\n>       (section 14.XX) with the \"100-continue\" expectation if it\n>       does not intend to send a request body.\n> \n>       Note: Because of the presence of older implementations, the\n>       protocol allows ambiguous situations in which a client may send\n>       \"Expect: 100-continue\" without receiving either a 419\n>       (Expectation Failed) status or a 100 (Continue) status.\n>       Therefore, when a client sends this header field to an origin\n>       server (possibly via a proxy) from which it has never seen a 100\n>       (Continue) status, the client should not wait for an indefinite\n>       or lengthy period before sending the request body.\n> \n>    Requirements for HTTP/1.1 or later origin servers:\n>    o  Upon receiving a request which includes an \"Expect\"\n> request-header\n>       field with the \"100-continue\" expectation, an origin server must\n>       either respond with 100 (Continue) status and continue to read\n>       from the input stream, or respond with an error status. If it\n>       responds with an error status, it MAY close the transport (TCP)\n>       connection or it MAY continue to read and discard the rest of\n> the\n>       request. It MUST NOT perform the requested method if it returns\n>       an error status.\n>    \n>    o  An origin server SHOULD NOT send a 100 (Continue) response if\n>       the request message does not include an \"Expect\" request-header\n>       field with the \"100-continue\" expectation, and MUST NOT send a\n>       100 (Continue) response if such a request comes from an HTTP/1.0\n>       (or earlier) client.\n>    \n>    o  An origin server SHOULD NOT send a 100 (Continue) response if\n>       has already received some or all of the request body for the\n>       corresponding request.\n>    \n>    o  An origin server that sends a 100 (Continue) response MUST\n>       ultimately send a final status code, once the request body\n>       is received and processed, unless it terminates the transport\n>       connection prematurely.\n>    \n>    o  If an origin server receives a request that does not include an\n>       \"Expect\" request-header field with the \"100-continue\"\n>       expectation, and the request includes a request body, and the\n>       server responds with an error status before reading the entire\n>       request body from the transport connection, then the server\n>       SHOULD NOT close the transport connection until it has read the\n>       entire request, or until the client closes the connection.\n>       Otherwise, the client may not reliably receive the response\n>       message.\n> \n>    For compatibility with RFC 2068, a server MAY send a 100 (Continue)\n>    status in response to an HTTP/1.1 PUT or POST request that does not\n>    include an \"Expect\" request-header field with the \"100-continue\"\n>    expectation.  This exception, the purpose of which is to minimize\n>    any client processing delays associated with an undeclared wait for\n>    100 (Continue) status, applies only to HTTP/1.1 requests, and not\n> to\n>    requests with any other HTTP-version value.\n> \n>    Requirements for HTTP/1.1 or later proxies:\n>    o  If a proxy receives a request that includes an \"Expect\"\n>       request-header field with the \"100-continue\" expectation, and\n> the\n>       proxy either knows that the next-hop server complies with\n>       HTTP/1.1 or higher, or does not know the HTTP version of the\n>       next-hop server, it MUST forward the request, including the\n>       Expect header field.\n> \n>    o  If the proxy knows that the version of the next-hop server is\n>       HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\n>       respond with a 419 (Expectation Failed) status.\n> \n>    o  Proxies SHOULD maintain a cache recording the HTTP version\n>       numbers received from recently-referenced next-hop servers.\n> \n>    o  A Proxy MUST NOT forward a 100 (Continue) response if the\n> request\n>       message was received from an HTTP/1.0 (or earlier) client and\n> did\n>       not include an \"Expect\" request-header field with the\n>       \"100-continue\" expectation.  Otherwise, proxies MUST forward\n>       response messages with status code 100 (Continue), unless the\n>       proxy itself added the \"Expected:  100-continue\" field to the\n>       request, or unless the connection between the proxy and its\n>       client has been closed.\n> \n> 8.2.5 Client behavior if server prematurely closes connection\n> \n>    If an HTTP/1.1 (or later) client sends a request which includes a\n>    request body, but which does not include an \"Expect\" request-header\n>    field with the \"100-continue\" expectation, and if the client is not\n>    directly connected to an HTTP/1.1 (or later) origin server, and if\n>    the the client sees the connection close before receiving any\n> status\n>    from the server, the client SHOULD retry the request, subject to\n> the\n>    restrictions in section 8.2.3. If the client does retry this\n>    request, it MAY use the following \"binary exponential backoff\"\n>    algorithm to be assured of obtaining a reliable response:\n> \n>   1. Initiate a new connection to the server\n> \n>   2. Transmit the request-headers\n> \n>   3. Initialize a variable R to the estimated round-trip time to the\n>      server (e.g., based on the time it took to establish the\n>      connection), or to a constant value of 5 seconds if the\n> round-trip\n>      time is not available.\n> \n>   4. Compute T = R * (2**N), where N is the number of previous retries\n>      of this request.\n> \n>   5. Wait either for an error response from the server, or for T\n> seconds\n>      (whichever comes first)\n> \n>   6. If no error response is received, after T seconds transmit the\n> body\n>      of the request.\n> \n>   7. If client sees that the connection is closed prematurely, repeat\n>      from step 1 until the request is accepted, an error response is\n>      received, or the user terminates the retry process.\n> \n>    If at any point an error status is received, the client\n> \n>   o  SHOULD NOT continue and\n> \n>   o  SHOULD close the connection if it has not completed sending the\n>      request message.\n> \n> =============================\n> === \n> === 10.4.1 100 Continue:\n> === \n> === One new sentence added at the end, as a cross-reference:\n> === \n> =============================\n> \n> 10.4.1 100 Continue\n> \n>    The client may continue with its request. This interim response is\n>    used to inform the client that the initial part of the request has\n>    been received and has not yet been rejected by the server. The\n> client\n>    SHOULD continue by sending the remainder of the request or, if the\n>    request has already been completed, ignore this response. The\n> server\n>    MUST send a final response after the request has been completed.\n>    See section 8.2.4 for detailed discussion of the use and handling\n>    of this status code.\n> =============================\n> ===\n> === What follows is basically what I sent on Wed, 02 Jul 97, in\n> === http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0027.html\n> === but with a few changes:\n> === \n> === (1) I've changed the header name from \"Expected\" to \"Expect\",\n> === just to save a couple of bytes.\n> === \n> === (2) Following Scott Lawrence's suggestion in\n> === http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0032.html\n> === I've changed the status code from 412 (Precondition Failed) to a\n> new\n> === 419 (Expectation failed) code, and included additional language\n> === for specifying that new code.\n> === \n> === (3) I've added some clarifications based on my message on \"Is\n> 100-Continue\n> === hop-by-hop?\",\n> === http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0078.html\n> === \n> === (4) I did NOT add an \"Expect: 100-hopbyhop\" because nobody has\n> === spoken up in its favor.\n> === \n> === (5) I reorganized the paragraphs slightly, and introduced a new\n> === subhead.\n> === \n> =============================\n> \n> 10.4.20 419 Expectation Failed\n> \n>    The expectation given in an \"Expect\" request-header field (see\n>    section 14.XX) could not be met by this server, or, if the server\n> is\n>    a proxy, the server has unambiguous evidence that the request could\n>    not be met by the next-hop server.\n> \n> =============================\n> \n> 14.XX Expect\n> \n>     The Expect request-header field is used to indicate that\n>     particular server behaviors are required by the client.  A\n>     server that does not understand or is unable to comply with any of\n>     the expectation values in the Expect field of a request MUST\n>     respond with appropriate error status.\n> \n>       Expect              =  \"Expect\" \":\" 1#expectation\n> \n>       expectation  =  \"100-continue\" | expectation-extension\n>       expectation-extension =  token [ \"=\" ( token | quoted-string )\n>                                        *expect-params ]\n>       expect-params           =  \";\" token [ = ( token | quoted-string\n> ) ]\n> \n>     The server SHOULD respond with a 419 (Expectation Failed) status\n>     if any of the expectations cannot be met.\n> \n>     This header field is defined with extensible syntax to allow for\n>     future extensions.  If a server receives a request containing\n>     an Expect field that includes an expectation-extension that\n>     it does not support, it MUST respond with a 419 (Expectation\n>     Failed) status.\n> \n> 14.XX.1 Expect 100-continue\n> \n>     When the \"100-continue\" expectation is present on a request that\n>     includes a body, the requesting client will wait after sending the\n>     request headers before sending the content-body.  In this case,\n> the\n>     server MUST conform to the requirements of section 8.2.4: it MUST\n>     either send a 100 (Continue) status, or an error status, after\n>     receiving the \"Expect: 100-continue\" request header.\n> \n>     If a proxy receives a request with the \"100-continue\" expectation,\n>     and the proxy either knows that the next-hop server complies with\n>     HTTP/1.1 or higher, or does not know the HTTP version of the\n>     next-hop server, it MUST forward the request, including the Expect\n>     header field.  If the proxy knows that the version of the next-hop\n>     server is HTTP/1.0 or lower, it MUST NOT forward the request, and\n>     it MUST respond with a 419 (Expectation Failed) status.  Proxies\n>     SHOULD maintain a cache recording the HTTP version numbers\n> received\n>     from recently-referenced next-hop servers.\n>     \n> Note: Because of the presence of older implementations, the\n> protocol allows ambiguous situations in which a client may send\n> \"Expect: 100-continue\" without receiving either a 419\n> (Expectation Failed) status or a 100 (Continue) status.\n> Therefore, when a client sends this header field to an origin\n> server (possibly via a proxy) from which it has never seen a\n> 100 (Continue) status, the client should not wait for an\n> indefinite or lengthy period before sending the request body.\n> \n> =============================\n> === \n> === 13.11 in RFC 2068 incorrectly allows a proxy to inject\n> === its own 100 response into the reply stream.  The change\n> === below modifies *only* the last sentence of the first\n> === paragraph.\n> === \n> =============================\n> \n> 13.11 Write-Through Mandatory\n> \n>    All methods that may be expected to cause modifications to the\n> origin\n>    server's resources MUST be written through to the origin server.\n> This\n>    currently includes all methods except for GET and HEAD. A cache\n> MUST\n>    NOT reply to such a request from a client before having transmitted\n>    the request to the inbound server, and having received a\n>    corresponding response from the inbound server. This does not\n>    prevent a proxy cache from forwarding a 100 (Continue) response\n>    before the inbound server has sent its final reply.\n> \n>    The alternative (known as \"write-back\" or \"copy-back\" caching) is\n> not\n>    allowed in HTTP/1.1, due to the difficulty of providing consistent\n>    updates and the problems arising from server, cache, or network\n>    failure prior to write-back.\n> \n> =============================\n> === \n> === Add this to the end of 8.1.2.2 (Pipelining)\n> === \n> =============================\n> \n>    Clients SHOULD NOT pipeline requests using non-idempotent methods\n> or\n>    non-idempotent sequences of methods (see section 9.1.2).\n> Otherwise,\n>    a premature termination of the transport connection may lead to\n>    indeterminate results.  A client wishing to send a non-idempotent\n>    request SHOULD wait to send that request until it has received the\n>    response status for the previous request.\n> \n> =============================\n> \n> [End of changes for STATUS100]\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "Yaron Goland <yarong@microsoft.com> writes:\n\n    In order to allow maximum flexibility shouldn't:\n\no  A client MUST be prepared to accept a 100 (Continue) status\nmessage followed by a regular response, even if the client does\nnot expect a 100 (Continue) status message.\n\n    Instead read:\n\nA client MUST be prepared to accept 100 (Continue) status\nmessages...\n    \nHow about this?\n\n   o  A client MUST be prepared to accept one or more 100 (Continue)\n      status messages prior to a regular response, even if the client\n      does not expect a 100 (Continue) status message.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "On Thu, 17 Jul 1997, Jeffrey Mogul wrote:\n\n> How about this?\n> \n>    o  A client MUST be prepared to accept one or more 100 (Continue)\n>       status messages prior to a regular response, even if the client\n>       does not expect a 100 (Continue) status message.\n> \n> -Jeff\n>\n\nHow about\n\nEven if a client has not indicated that it is prepared to accept initial\n100 responses, it MUST still be prepared to accept one or more 100\nresponses prior to a final response header (e.g. a 200 response) and the\nbody of the message.\n\n---\nGregory Woodhouse\ngjw@wnetc.com    /    http://www.wnetc.com/home.html\nIf you're going to reinvent the wheel, at least try to come\nup with a better one.\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "On Thu, 17 Jul 1997, Jeffrey Mogul wrote:\n\n> Responses to comments from Scott Lawrence:\n>    I think that making this a special case allowed _only_ for HTTP/1.1\n>    is going too far.  We already have the requirement that 100 Continue\n>    must be accepted by the client anyway.  We're talking about as few\n>    as 'HTTP/1.1 100CRLF\" - 14 bytes - 23 if you send the ' Continue'.\n>    What justification is there for the complexity of this restriction?\n> \n> See above.  But I agree with you that this is a judgement call.\n> I'm just worried that we may not need to send these extra bytes,\n> and we have the chance now to discourage 23*N extra bytes over\n> the Internet for many years to come (where N is a large number!)\n\nI think we have lost sight of the fact that it isn't 14 (or 23) bytes\nbut rather those bytes encapsulated in a packet.  The implication\nto network routers and TCP/IP stacks is not linear with length.  The\nrouting function is more costly then the simple buffering associated\nwith length.  Secondly, this 100 Continue packet may require\na separate acknowledgement packet.  The assumption of at most 2\npackets is based on the TCP/IP application sending the data with a\nsingle API call, etc. I learned the hard way that sending a line\nof data in one write followed by CRLF in a second is bad news as is\nsending each line of the HTTP headers one per write request.\n\nAll in all the implications to the network are much greater than the\nsimple 23 bytes we are discussing.\n\nDave Morris\n\n(FWIW ... the minimum would be:  HTTP/1.1 100CRLFCRLF or 16 bytes, \nwouldn't it?)\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "accept one or more 100\nresponses\"? (help)\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": ">How about this?\n>\n>   o  A client MUST be prepared to accept one or more 100 (Continue)\n>      status messages prior to a regular response, even if the client\n>      does not expect a 100 (Continue) status message.\n\nThis should be the case for 1xx responses in general, so the\nrequirement should also be in general.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": ">        o  An origin server SHOULD NOT send a 100 (Continue) response if\n>           the request message does not include an \"Expect\" request-header\n>           field with the \"100-continue\" expectation, and MUST NOT send a\n>           100 (Continue) response if such a request comes from an HTTP/1.0\n>           (or earlier) client.\n\nNo, this is not an appropriate use of SHOULD NOT.  If we reference the\nBradner RFC, then we follow its rules, and one of them is that we MUST NOT\nuse the capitalized forms for things which are not interoperability\nrequirements.\n\n>        o  An origin server SHOULD NOT send a 100 (Continue) response if\n>           has already received some or all of the request body for the\n>           corresponding request.\n\nLikewise, inappropriate.  Recommendations should be presented as\nrecommendations, not as requirements.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE PROXYAUTHORIZATION: Proposal wordin",
            "content": ">This text is to completely replace the existing wording in chapter 11, and\n>to add a one-liner in section 13.5.1.\n>\n>The remaining MUSTs could potentially be downgraded to SHOULDs as it would\n>not cause any interoperability problems, but I have left them as in the\n>original text.\n\nIt looks fine.  Those MUSTs are necessary because some clients will\npuke if they receive a 401 or 407 without the corresponding challenge.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "ISSUE: LANGUAGETA",
            "content": "I have been one of those originally discussing this issue,\nand I think I should take it up again after it has appeared\non Larry's list.\n\nThe question is how the Accept-Language field is matched with\nthe language information of the documents on the server. In\nparticular, the question is what should be done in terms of\nprefix matching.\n\nLanguage tags, as to RFC 1766, are strings separated into\ncomponents with \"-\". RFC 1766 does not look at the individual\ncomponents, but it is in general very convenient to consider\nthe components and to do prefix matching.\n\nTo give an examlpe, the current situation is:\n\n% Accept-Language      Document        Match?\n% \n% en                   en              YES\n% en-us                en-us           YES\n% en                   en-us           YES\n% en-us                en              NO????\n% en-us                en-uk           NO!!!!\n\nThe NO!!!! in the last case is there because even though it\nwould make sense to return an en-uk (GB English) document\nin request for en-us (US English) [of course if en-uk as\nsuch is not available], this never applies in general.\nFor example, there could be x-klingon and x-martian, without\nany mutual intellegibility. There is zh-cn and zh-tw (mainland\nChinese Chinese and Taiwanese Chinese, usually standing for\nsimplified and traditional style Chinese), where some people\nwouldn't care getting either of these, but others would have\ndifficulties understanding one or the other. Even for \"en\",\nwe could have something like en-ebonics, not easily intellegible\nfor an average \"en\" reader.\n\nOnce it is understood that we cannot match aa-bb with aa-cc,\nthe question is whether we can do anything to improve the\nsituation for getting a match between en-us and en, as marked\nwith NO???? above. In an earlier mail, I proposed to solve\nthis by just changing the spec to saying that this case\nmatches. I found out in the meantime that this has some\nsubtle undesired consequences. The story goes as follows:\n\nWe want to have a way to get en-uk back to an en-us reader\n(for this and similar cases, not for the general case).\nWith the current spec, we know that it's the user side's\nresponsibility to do something about it, i.e. to have\nAccept-Language: en-us, en\nmaybe with the necessary q values.\n\nNow if we add to the spec that en-us matches an en document,\nit could as well be the responsibility of the server side\nto tag the document both as en-uk and en, in order to get\nit send back on an en-us request. We don't know anymore\nwho is responsible, which means that either both will do\nit (to make sure it works) or nobody will do it (because\nthey hope for the other side). This is rather undesired :-(.\n\nThe current solution is better in that even if it doesn't\ndo as much as possible automatically, it assigns clear\nresponsibilities.\n\nThe question is whether the responsibilities are on the\nright side. In one respect, they are, because it's the\nreader who ultimately knows what she can (or wants to)\nread and what not. However, in another respect, it's on\nthe wrong side, because the end user is not aware that\nshe is responsible, or how she should take up her\nresponsibility. As a consequence, she might have set up\nlanguage preferences as en-us only, and then be very\nannoyed when she gets back a variant list saying:\n\nThe document is not available in US English\nas you requested, but it is available in the\nfollowing languages:\n\nEnglish\n\nPlease click on the language in which you prefer\nto receive the document.\n\nTo change the spec and put the responsibility on the server\nside might have some advantages (server side generally has\na little more expertise than user side), but also has\ndisadvantages (because it assumes the server side knows what\nusers with various prefeneces do understand and what they\ndon't, which is difficult e.g. in the zh (Chinese) case).\nIts biggest disadvantage is of course that we would have\nto turn the spec upside down.\n\nTo avoid the annoying case above, the best thing that can\nbe done is that browser implementors help users to get\ntheir choices right. For example, after a user selects\nen-us and fr-ca and hits OK, a little dialog could come\nup and ask:\n\nYou selected US English, but not general English.\nIn order for the browser to obtain all documents\nreadable to you, we suggest to add general English.\nShould we do that for you?[YES] [NO] [CANCEL]\n\nAnother question that remains to me is that currently, the\nspec assumes that each document has assigned exactly one\nlanguage-tag, and that there either is a match or there\nis none. On sophisticated servers with database background\nand so on, this could look vastly different. How is the\nspec supposed to be applied in such a case?\n\n\nI therefore propose the following:\n\n- Leave the matching mechanism in the spec as is.\n- Add some comments to help avoid situations that are\nreally annoying to end users.\n\n\nThe current text is:\n\n> 14.4 Accept-Language\n> \n>    The Accept-Language request-header field is similar to Accept, but\n>    restricts the set of natural languages that are preferred as a\n>    response to the request.\n> \n>           Accept-Language = \"Accept-Language\" \":\"\n>                             1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n> \n>           language-range  = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) ) | \"*\" )\n> \n>    Each language-range MAY be given an associated quality value which\n>    represents an estimate of the user's preference for the languages\n>    specified by that range. The quality value defaults to \"q=1\". For\n>    example,\n> \n>           Accept-Language: da, en-gb;q=0.8, en;q=0.7\n> \n>    would mean: \"I prefer Danish, but will accept British English and\n>    other types of English.\" A language-range matches a language-tag if\n>    it exactly equals the tag, or if it exactly equals a prefix of the\n>    tag such that the first tag character following the prefix is \"-\".\n>    The special range \"*\", if present in the Accept-Language field,\n>    matches every tag not matched by any other range present in the\n>    Accept-Language field.\n> \n>      Note: This use of a prefix matching rule does not imply that\n>      language tags are assigned to languages in such a way that it is\n>      always true that if a user understands a language with a certain\n>      tag, then this user will also understand all languages with tags\n>      for which this tag is a prefix. The prefix rule simply allows the\n>      use of prefix tags if this is the case.\n> \n>    The language quality factor assigned to a language-tag by the\n>    Accept-Language field is the quality value of the longest language-\n>    range in the field that matches the language-tag. If no language-\n>    range in the field matches the tag, the language quality factor\n>    assigned is 0. If no Accept-Language header is present in the\n>    request, the server SHOULD assume that all languages are equally\n>    acceptable. If an Accept-Language header is present, then all\n>    languages which are assigned a quality factor greater than 0 are\n>    acceptable.\n> \n>    It may be contrary to the privacy expectations of the user to send an\n>    Accept-Language header with the complete linguistic preferences of\n>    the user in every request. For a discussion of this issue, see\n>    section 15.7.\n> \n>      Note: As intelligibility is highly dependent on the individual\n>      user, it is recommended that client applications make the choice of\n>      linguistic preference available to the user. If the choice is not\n>      made available, then the Accept-Language header field must not be\n>      given in the request.\n\nI propose to add another note at the end of Section 14.4:\n\n>>>> START OF PROPOSED ADDITION\nNote: When making the choice of linguistic preference available to\nthe user, implementors should take into account the fact that users\nare not familliar with the details of language matching as described\nabove, and should provide appropriate guidance. As an examlpe, users\nmay assume that on selecting \"en-gb\", they will be served any kind\nof English document if British English is not available. A user\nagent may suggest in such a case to add \"en\" to get the best\nmatching behaviour.\n<<<< END OF PROPOSED ADDITION\n\n\nI hope that giving the proposed addition in the form above is\nsufficient. Please inform me of whatever other action that\nmight be necessary to move this ISSUE forward.\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE VARY: Proposed wordin",
            "content": "The first part is fine.\n\n>13.6 Caching Negotiated Responses\n>\n>Use of server-driven content negotiation (section 12), as indicated by the\n>presence of a Vary header field in a response, alters the conditions and\n>procedure by which a cache can use the response for subsequent requests.\n>See section 14.43 for use of the Vary header field by servers.\n>\n>A server SHOULD use the Vary header field to inform a cache of what\n>request-header fields were used to select among multiple representations of\n>a cachable response subject to server-driven negotiation. The set of header\n>fields named by the Vary field value is known as the \"selecting\"\n>request-headers.\n>\n>When the cache receives a subsequent request whose Request-URI specifies\n>one or more cache entries including a Vary header field, the cache MUST NOT\n>use such a cache entry to construct a response to the new request unless\n>all of the field-names in the cached Vary header field are present in the\n>new request, and all of the stored selecting request-headers from the\n>previous request match the corresponding request-headers in the new request. \n\nThat's not quite right -- if the field-name is missing from both the\nold request and the new request, then they still match.  We should also\nbe referring to the requested resource rather than the Request-URI.\n\n>The selecting request-headers from two requests are defined to match if and\n>only if the selecting request-headers in the first request can be\n>transformed to the selecting request-headers in the second request by\n>adding or removing linear whitespace (LWS) at places where this is allowed\n>by the corresponding BNF, and/or combining multiple message-header fields\n>with the same field name following the rules about message headers in\n>section 4.2.\n>\n>If the selecting request-headers do not match, then the cache MUST forward\n>the request to the origin server; it MAY forward the request as a\n>conditional request.\n\nThis is wrong as well -- we cannot require the cache to make a request.\nThe only thing we can require is that it not use the existing response\nto satisfy the new request unless it is confirmed by a 304 response\nfrom the next server in the request chain.\n\n>If an entity tag was assigned to the representation, the forwarded request\n>SHOULD be conditional and include the entity tags in an If-None-Match\n>header field from all its cache entries for the Request-URI. This conveys\n>to the server the set of entities currently held by the cache, so that if\n>any one of these entities matches the requested entity, the server can use\n>the ETag header field in its 304 (Not Modified) response to tell the cache\n>which entry is appropriate. If the entity-tag of the new response matches\n>that of an existing entry, the new response SHOULD be used to update the\n>header fields of the existing entry, and the result MUST be returned to the\n>client.\n>\n>If the representation was selected using criteria not limited to the\n>request-headers, then the server SHOULD include a \"Vary: *\" header field in\n>the response if it is cachable. In this case, a cache MUST NOT use the\n>response in a reply to a subsequent request unless the cache relays the new\n>request to the origin server in a conditional request and the server\n>responds with 304 (Not Modified), including an entity tag or\n>Content-Location that indicates which entity should be used.\n\nThis is getting convoluted and also a bit repetitive.  We should only\ntalk about what it means to \"match\" first, and then just explain what\nmust be done if it doesn't match.  In other words\n\n   If the response includes a Vary header field with a field-value of \"*\",\n   indicating that the representation was selected using criteria not\n   limited to the original request header fields, then subsequent\n   requests always fail to match the selecting request header fields.\n\n   If the selecting request header fields for the cached entry do not\n   match the selecting request header fields of the new request, then\n   the cache MUST NOT use the cached entry to satisfy the request unless\n   it first relays the new request to the origin server in a conditional\n   request and the server responds with a 304 (Not Modified) response\n   indicating that the cached entry can be reused.\n\nThat way, we don't have to repeat it for every single condition.\nStill a bit of a tongue-twister -- why don't we just call them\n\"selecting fields\"?\n\n>If any of the existing cache entries contains only partial content for the\n>associated entity, its entity-tag SHOULD NOT be included in the\n>If-None-Match header field unless the request is for a range that would be\n>fully satisfied by that entry.\n> \n>If a cache receives a successful response whose Content-Location field\n>matches that of an existing cache entry for the same Request-URI, whose\n>entity-tag differs from that of the existing entry, and whose Date is more\n>recent than that of the existing entry, the existing entry SHOULD NOT be\n>returned in response to future requests, and should be deleted from the cache.\n\n>14.43 Vary\n>\n>The Vary field value indicates the set of request-header fields that fully\n>determines, while the response is fresh, whether a cache may use the\n>response to reply to a subsequent request without revalidation. For\n>uncachable or stale responses, the Vary field value advises the user agent\n>about the criteria that were used to select the representation. A Vary\n>field value of \"*\" implies that a cache cannot determine from the request\n>headers of a subsequent request whether this response is the appropriate\n>representation. See section 13.6 for use of the Vary header field by caches.\n>\n>       Vary  = \"Vary\" \":\" ( \"*\" | 1#field-name )\n>\n>An HTTP/1.1 server SHOULD include a Vary header field with any cachable\n>response that is subject to server-driven negotiation. Doing so allows a\n>cache to properly interpret future requests on that resource and informs\n>the user agent about the presence of negotiation on that resource. A server\n>MAY include a Vary header field with a non-cachable response that is\n>subject to server-driven negotiation, since this might provide the user\n>agent with useful information about the dimensions over which the response\n>varies at the time of the response.\n>\n>A Vary field value consisting of a list of field-names signals that the\n>representation selected for the response is based on a selection algorithm\n>which considers ONLY the listed request-header field values in selecting\n>the most appropriate representation. A cache MAY assume that the same\n>selection will be made for future requests with the same values for the\n>listed field names, for the duration of time in which the response is fresh.\n>\n>The field-names given are not limited to the set of standard request-header\n>fields defined by this specification. Field names are case-insensitive.\n>\n>A Vary field value of \"*\" signals that unspecified parameters not limited\n>to the request-headers (e.g., the network address of the client), play a\n>role in the selection of the response representation. Subsequent requests\n>on that resource can only be properly interpreted by the origin server, and\n>thus a cache MUST forward a (possibly conditional) request even when it has\n>a fresh response cached for the resource. The \"*\" value MUST NOT be\n>generated by a proxy server; it may only be generated by an origin server.\n\nThis section is great up until this last paragraph.  Since it just repeats\nwhat is already said in the first paragraph, we should delete it.  As\nmentioned above, we cannot place a MUST requirement on the cache making\na new request (i.e., the cache must have the right to deny service).\nAlso, the requirement in the last sentence is bogus, unless we mean\nto apply it to all values of Vary (i.e., The Vary field MUST NOT be\nset or modified by any application other than the origin server.).\nThe existing wording is always a contradiction for the CERN server.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "RE: Proposed resolution for the STATUS100 issu",
            "content": "Agreed. I really think we need to add language to 10.1 that makes it\nclear that\n1) Proxies should be passing through all 1xx messages\n2) That multiple 1xx messages may be sent between the time that a\nrequest is made and non-1xx response is sent.\n3) That clients, at minimum, MUST be able to ignore any incoming 1xx\nresponses.\n\nYaron\n\n> -----Original Message-----\n> From:Roy T. Fielding [SMTP:fielding@kiwi.ics.uci.edu]\n> Sent:Friday, July 18, 1997 4:37 AM\n> To:Jeffrey Mogul\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: Proposed resolution for the STATUS100 issue \n> \n> >How about this?\n> >\n> >   o  A client MUST be prepared to accept one or more 100 (Continue)\n> >      status messages prior to a regular response, even if the client\n> >      does not expect a 100 (Continue) status message.\n> \n> This should be the case for 1xx responses in general, so the\n> requirement should also be in general.\n> \n> ....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "Jeffrey Mogul:\n>\n>Maurizio Codogno <mau@beatles.cselt.it> writes:\n>\n     [Proposed text by Jeffrey Mogul:]\n>If the request method is not idempotent, the\n>client SHOULD NOT retry the request without user confirmation.\n>    \n>    I'd prefer a MUST NOT, especially in light of the following\n>    sentence.  As far as I can parse English language, it would mean\n>    that either the user agent knows for sure that it is safe to retry\n>    the request or it warns the user.\n>\n>I've been given encouragement, by the working group, to avoid\n>using MUST when SHOULD is sufficient. \n\nThe original text in 2068 had a MUST here:\n\n     the client SHOULD retry\n   the request without user interaction so long as the request method is\n   idempotent (see section 9.1.2); other methods MUST NOT be\n                                                 ^^^^\n   automatically retried, although user agents MAY offer a human\n   operator the choice of retrying the request..\n\nand I would view changing this MUST to a SHOULD as a completely\nunacceptable change in the protocol.\n\nI *never* want proxies to take the initiative in retrying an\nidempotent operation.  It would be OK to dilute the MUST to a SHOULD\nfor user agents, but not for proxies.  This would make the web\nunsafe for ordering pizzas.\n\n> In this case, it seems\n>plausible that there may be some applications in which user\n>confirmation is hard to obtain, and the risks associated with\n>not finishing the request are less important than the risks\n>associated with non-idempotency.\n\nApplications in which the risks associated with not finishing the\nrequest are less important than the risks associated with\nnon-idempotency should never use HTTP/1.1, because HTTP/1.1 allows\nrandom parties to abort the transaction at any time.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "Jeffrey Mogul:\n>\n>On the general topic of \"is the whole status 100 thing worth having?\":\n>\n>I've always been neutral on the topic.  However, several members\n>of the HTTP/1.1 editorial group are firmly in favor of keeping\n>it, as are at least some other members of the full working group.\n>As far as I can tell, the consensus of the editorial group is\n>\"keep it, but fix it.\"\n\nReading your proposal, I think that overall you have done a good job\nat fixing the language and at cutting away some of the unnecessary\ncomplexity.\n\nIt it now simple enough as far as I am concerned, though I would not\nmind making it still more simple.\n\nAs far as I can see, a user agent which sends an Expect 100 would have\nto have time-out code so that it can handle the case that the request\nis done on an unknown server which turns out to be a 1.0 server, so\nthat there never is a 100 response or an error response, because the\nserver keeps waiting for a request body.  As this time-out code has to\nbe present anyway, I think we can get away with dropping the\nrequirement that proxies remember the versions of upstream servers,\nand return error responses to an expect 100 if they know that the\nupstream server is a 1.0 server.  We could simply add a note that an\nexpect 100 can only be used successfully over a pure 1.1 (or higher)\nchain, and that agents can inspect the Via header field and status\nline of an earlier request to the same server to see if sending an\nexpect 100 could be useful.  The note should also say that the\npossibility of having a mesh of proxies including a 1.0 proxy, and of\nservers simply switching to a lower protocol version if they like to,\nmeans that a prediction based on Via cannot be 100% reliable.\n\n\nAs for my ASK-IF-ALLOWED proposal: I did not use OPTIONS because I did\nnot remember that it could be used for this at the time, but I see no\nreason why this could not be done with OPTIONS.\n\nJeff writes:\n>I don't believe that it's feasible to add a new method at this point,\n>especially since it would not interoperate with existing proxies\n>and servers.\n\nWhy would it not interoperate?  Proxies would simply relay it, like\nthey do with other unknown methods, and origin servers which do not\nknow the method would simply return an error.  Nothing breaks as far\nas I can see, and no extra complexity is needed.  It would even work\nover a 1.0 chain.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "On Fri, 18 Jul 1997, Roy T. Fielding wrote:\n\n> \n> >        o  An origin server SHOULD NOT send a 100 (Continue) response if\n> >           has already received some or all of the request body for the\n> >           corresponding request.\n> \n> Likewise, inappropriate.  Recommendations should be presented as\n> recommendations, not as requirements.\n\nIn my proposal, this point was only intended as an optimization permission\nor recomentation. I didn't want to preclude a server which had the\ninformation from bypassing the 100 (Continue) so I would be happy with\nany wording which expressed the exception case allowing the omission.\n\nEither:\n\n     o An origin server need not send a 100 (continue) response if it ...\n                                                        missing?   ^^\n(which I agree feels like awkward wording) or:\n\n     o It is not necessary for an origin server (or proxy) to send a\n       100 (Continue) response if it has already received some or all\n       of the request body for the corresponding request.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE: LANGUAGETA",
            "content": "The editing group accepts your proposed addition as a reasonable\nbit of implementation advice to add to the HTTP/1.1 spec\n\n> >>> START OF PROPOSED ADDITION\n> Note: When making the choice of linguistic preference available to\n> the user, implementors should take into account the fact that users\n> are not familliar with the details of language matching as described\n> above, and should provide appropriate guidance. As an example, users\n> may assume that on selecting \"en-gb\", they will be served any kind\n> of English document if British English is not available. A user\n> agent may suggest in such a case to add \"en\" to get the best\n> matching behavior.\n> <<<< END OF PROPOSED ADDITION\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "Roy Fielding writes:\n        o  An origin server SHOULD NOT send a 100 (Continue) response if\n           the request message does not include an \"Expect\" request-header\n           field with the \"100-continue\" expectation, and MUST NOT send a\n           100 (Continue) response if such a request comes from an HTTP/1.0\n           (or earlier) client.\n\n    No, this is not an appropriate use of SHOULD NOT.  If we reference\n    the Bradner RFC, then we follow its rules, and one of them is that\n    we MUST NOT use the capitalized forms for things which are not\n    interoperability requirements.\n    \nIf you are referring to RFC2119, it says:\n\n  6. Guidance in the use of these Imperatives\n  \n     Imperatives of the type defined in this memo must be used with care\n     and sparingly.  In particular, they MUST only be used where it is\n     actually required for interoperation or to limit behavior which has\n     potential for causing harm (e.g., limiting retransmisssions)  For\n     example, they must not be used to try to impose a particular method\n     on implementors where the method is not required for\n     interoperability.\n    \nI suppose one could start a pointless argument about whether the\nintention behind \"potential for causing harm (e.g., limiting\nretransmissions)\" applies to \"limiting the transmission of\nunnecessary bytes over the network.\"  I'll leave this decision\nto the working group chair.  Larry, if you ask me to remove this\nSHOULD NOT, please say so.\n\n        o  An origin server SHOULD NOT send a 100 (Continue) response if\n           has already received some or all of the request body for the\n           corresponding request.\n\n    Likewise, inappropriate.  Recommendations should be presented as\n    recommendations, not as requirements.\n    \nI've rephrased this as:\n\n   o  An origin server MAY omit a 100 (Continue) response if\n      has already received some or all of the request body for the\n      corresponding request.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposed resolution for the STATUS100 issu",
            "content": "Koen Holtman writes:\n\n    I *never* want proxies to take the initiative in retrying an\n    idempotent operation.  It would be OK to dilute the MUST to a SHOULD\n    for user agents, but not for proxies.  This would make the web\n    unsafe for ordering pizzas.\n    \nI've rewritten the subsection to make it clear that it applies\nto user-agent clients, not to proxies:\n\n   8.2.3 Automatic retrying of requests\n\n   If a user agent sees the transport connection close before it\n   receives a final response to its request, if the request method is\n   idempotent (see section 9.1.2), the user agent SHOULD retry the\n   request without user interaction.  If the request method is not\n   idempotent, the user agent SHOULD NOT retry the request without user\n   confirmation.  (Confirmation by user-agent software with semantic\n   understanding of the application MAY substitute for user\n   confirmation.)\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Revised resolution for the STATUS100 issu",
            "content": "I'd like to thank everyone who commented on the previous draft\nof this proposal.  I've incorporated most of the specific comments\nin the revised draft, below.\n\nChanges include:\n(1) Changing \"HTTP/1.1 or later\" to \"HTTP/1.1\", in\ncontexts where this was incorrectly placing a requirement\non the behavior of an implementation of a future version\nof HTTP/1.x\n\n(2) Made it clear that user-agents should retry requests,\nnot \"clients\" in general\n\n(3) Converted requirements for clients to ignore unexpected\n100 (Continue) responses, and for proxies to forward 100\nresponses, into a general requirement for 1xx responses.\n\n(4) Modified some TCP-specific language, to make it clearer\nthat non-TCP transports are supported for HTTP.\n\n(5) Require that the origin server MUST NOT wait for\nthe request body before it sends a required 100 (Continue)\nresponse.\n\n(6) Allow, rather than require, a server to omit\n100 (Continue) if it has already seen some of the\nrequest body.\n\n(7) Allow servers to defend against denial-of-service\nattacks and broken clients.\n\nDuring our editorial group discussion today, we agreed that the\nproposal (with these changes) was pretty close to the right\nsolution, so we are likely to issue a last-call on this soon.\nI.e., speak up now if you have complaints.\n\n-Jeff\nP.S.: Note that there is a new issue,\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#IDEMPOTENT\nopen; the section on Idempotent Methods will be revised.   Those\nchanges will NOT be included in the resolution for STATUS100.\n\n\n=== \n=== Major revisions to 8.2\n=== \n======================\n\n8.2 Message Transmission Requirements\n\n8.2.1 Persistent connections and flow control\n\n   HTTP/1.1 servers SHOULD maintain persistent connections and use\n   transport-level flow control mechanisms, if available, to resolve\n   temporary overloads, rather than terminating connections with the\n   expectation that clients will retry. The latter technique can\n   exacerbate network congestion.\n\n8.2.2 Monitoring connections for error status messages\n\n   An HTTP/1.1 client sending a message-body SHOULD monitor\n   the network connection for an error status while it is transmitting\n   the request. If the client sees an error status, it SHOULD\n   immediately cease transmitting the body. If the body is being sent\n   using a \"chunked\" encoding (section 3.6), a zero length chunk and\n   empty footer MAY be used to prematurely mark the end of the\n   message. If the body was preceded by a Content-Length header, the\n   client MUST close the connection.\n\n8.2.3 Automatic retrying of requests\n\n   If a user agent sees the transport connection close before it\n   receives a final response to its request, if the request method is\n   idempotent (see section 9.1.2), the user agent SHOULD retry the\n   request without user interaction.  If the request method is not\n   idempotent, the user agent SHOULD NOT retry the request without user\n   confirmation.  (Confirmation by user-agent software with semantic\n   understanding of the application MAY substitute for user\n   confirmation.)\n\n8.2.4 Use of the 100 (Continue) status\n\n   The purpose of the 100 (Continue) status (see section 10.1.1) is to\n   allow an end-client that is sending a request message with a request\n   body to determine if the origin server is willing to accept the\n   request (based on the request headers) before the client sends the\n   request body.  In some cases, it may either be inappropriate or\n   highly inefficient for the client to send the body if the server\n   will reject the message without looking at the body.\n\n   Requirements for HTTP/1.1 clients:\n   o  If a client will wait for a 100 (Continue) response before sending\n      the request body, it MUST send an \"Expect\" request-header field\n      (section 14.XX) with the \"100-continue\" expectation.\n\n   o  A client MUST NOT send an \"Expect\" request-header field\n      (section 14.XX) with the \"100-continue\" expectation if it\n      does not intend to send a request body.\n\n      Note: Because of the presence of older implementations, the\n      protocol allows ambiguous situations in which a client may send\n      \"Expect: 100-continue\" without receiving either a 419\n      (Expectation Failed) status or a 100 (Continue) status.\n      Therefore, when a client sends this header field to an origin\n      server (possibly via a proxy) from which it has never seen a 100\n      (Continue) status, the client should not wait for an indefinite\n      or lengthy period before sending the request body.\n\n   Requirements for HTTP/1.1 origin servers:\n   o  Upon receiving a request which includes an \"Expect\" request-header\n      field with the \"100-continue\" expectation, an origin server MUST\n      either respond with 100 (Continue) status and continue to read\n      from the input stream, or respond with an error status.  The\n      origin server MUST NOT wait for the request body before sending\n      the 100 (Continue) response.  If it responds with an error\n      status, it MAY close the transport connection or it MAY\n      continue to read and discard the rest of the request. It MUST NOT\n      perform the requested method if it returns an error status.\n   \n   o  An origin server SHOULD NOT send a 100 (Continue) response if\n      the request message does not include an \"Expect\" request-header\n      field with the \"100-continue\" expectation, and MUST NOT send a\n      100 (Continue) response if such a request comes from an HTTP/1.0\n      (or earlier) client.\n   \n   o  An origin server MAY omit a 100 (Continue) response if\n      has already received some or all of the request body for the\n      corresponding request.\n   \n   o  An origin server that sends a 100 (Continue) response MUST\n      ultimately send a final status code, once the request body\n      is received and processed, unless it terminates the transport\n      connection prematurely.\n   \n   o  If an origin server receives a request that does not include an\n      \"Expect\" request-header field with the \"100-continue\"\n      expectation, and the request includes a request body, and the\n      server responds with an error status before reading the entire\n      request body from the transport connection, then the server\n      SHOULD NOT close the transport connection until it has read the\n      entire request, or until the client closes the connection.\n      Otherwise, the client may not reliably receive the response\n      message.  However, this requirement should not be construed\n      as preventing a server from defending itself against\n      denial-of-service attacks, or from badly broken client\n      implementations.\n\n   For compatibility with RFC 2068, a server MAY send a 100 (Continue)\n   status in response to an HTTP/1.1 PUT or POST request that does not\n   include an \"Expect\" request-header field with the \"100-continue\"\n   expectation.  This exception, the purpose of which is to minimize\n   any client processing delays associated with an undeclared wait for\n   100 (Continue) status, applies only to HTTP/1.1 requests, and not to\n   requests with any other HTTP-version value.\n\n   Requirements for HTTP/1.1 proxies:\n   o  If a proxy receives a request that includes an \"Expect\"\n      request-header field with the \"100-continue\" expectation, and the\n      proxy either knows that the next-hop server complies with\n      HTTP/1.1 or higher, or does not know the HTTP version of the\n      next-hop server, it MUST forward the request, including the\n      Expect header field.\n\n   o  If the proxy knows that the version of the next-hop server is\n      HTTP/1.0 or lower, it MUST NOT forward the request, and it MUST\n      respond with a 419 (Expectation Failed) status.\n\n   o  Proxies SHOULD maintain a cache recording the HTTP version\n      numbers received from recently-referenced next-hop servers.\n\n   o  A Proxy MUST NOT forward a 100 (Continue) response if the request\n      message was received from an HTTP/1.0 (or earlier) client and did\n      not include an \"Expect\" request-header field with the\n      \"100-continue\" expectation.  This requirement overrides the\n      general rule for forwarding of 1xx responses (see section 10.1).\n\n8.2.5 Client behavior if server prematurely closes connection\n\n   If an HTTP/1.1 client sends a request which includes a\n   request body, but which does not include an \"Expect\" request-header\n   field with the \"100-continue\" expectation, and if the client is not\n   directly connected to an HTTP/1.1 origin server, and if\n   the the client sees the connection close before receiving any status\n   from the server, the client SHOULD retry the request, subject to the\n   restrictions in section 8.2.3. If the client does retry this\n   request, it MAY use the following \"binary exponential backoff\"\n   algorithm to be assured of obtaining a reliable response:\n\n  1. Initiate a new connection to the server\n\n  2. Transmit the request-headers\n\n  3. Initialize a variable R to the estimated round-trip time to the\n     server (e.g., based on the time it took to establish the\n     connection), or to a constant value of 5 seconds if the round-trip\n     time is not available.\n\n  4. Compute T = R * (2**N), where N is the number of previous retries\n     of this request.\n\n  5. Wait either for an error response from the server, or for T seconds\n     (whichever comes first)\n\n  6. If no error response is received, after T seconds transmit the body\n     of the request.\n\n  7. If client sees that the connection is closed prematurely, repeat\n     from step 1 until the request is accepted, an error response is\n     received, or the user terminates the retry process.\n\n   If at any point an error status is received, the client\n\n  o  SHOULD NOT continue and\n\n  o  SHOULD close the connection if it has not completed sending the\n     request message.\n\n=============================\n===\n=== What follows is basically what I sent on Wed, 02 Jul 97, in\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0027.html\n=== but with a few changes:\n=== \n=== (1) I've changed the header name from \"Expected\" to \"Expect\",\n=== just to save a couple of bytes.\n=== \n=== (2) Following Scott Lawrence's suggestion in\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0032.html\n=== I've changed the status code from 412 (Precondition Failed) to a new\n=== 419 (Expectation failed) code, and included additional language\n=== for specifying that new code.\n=== \n=== (3) I've added some clarifications based on my message on \"Is 100-Continue\n=== hop-by-hop?\",\n=== http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0078.html\n=== \n=== (4) I did NOT add an \"Expect: 100-hopbyhop\" because nobody has\n=== spoken up in its favor.\n=== \n=== (5) I reorganized the paragraphs slightly, and introduced a new\n=== subhead.\n=== \n=============================\n\n10.1 Informational 1xx\n\n   This class of status code indicates a provisional response,\n   consisting only of the Status-Line and optional headers, and is\n   terminated by an empty line. Since HTTP/1.0 did not define any 1xx\n   status codes, servers MUST NOT send a 1xx response to an HTTP/1.0\n   client except under experimental conditions.\n\n   A client MUST be prepared to accept one or more 1xx status responses\n   prior to a regular response, even if the client does not expect a\n   100 (Continue) status message.  Unexpected 1xx status responses\n   MAY be ignored by a user agent.\n\n   Proxies MUST forward 1xx responses, unless the connection between\n   the proxy and its client has been closed, or unless the proxy itself\n   requested the generation of the 1xx response.  (For example, if a\n   proxy adds a \"Expected:  100-continue\" field when it forwards a\n   request, then it need not forward the corresponding 100 (Continue)\n   response(s).)\n\n10.4.1 100 Continue\n\n   The client may continue with its request. This interim response is\n   used to inform the client that the initial part of the request has\n   been received and has not yet been rejected by the server. The client\n   SHOULD continue by sending the remainder of the request or, if the\n   request has already been completed, ignore this response. The server\n   MUST send a final response after the request has been completed.\n   See section 8.2.4 for detailed discussion of the use and handling\n   of this status code.\n\n=============================\n\n10.4.20 419 Expectation Failed\n\n   The expectation given in an \"Expect\" request-header field (see\n   section 14.XX) could not be met by this server, or, if the server is\n   a proxy, the server has unambiguous evidence that the request could\n   not be met by the next-hop server.\n\n=============================\n\n14.XX Expect\n\n    The Expect request-header field is used to indicate that\n    particular server behaviors are required by the client.  A\n    server that does not understand or is unable to comply with any of\n    the expectation values in the Expect field of a request MUST\n    respond with appropriate error status.\n\n      Expect              =  \"Expect\" \":\" 1#expectation\n\n      expectation  =  \"100-continue\" | expectation-extension\n      expectation-extension =  token [ \"=\" ( token | quoted-string )\n                                       *expect-params ]\n      expect-params           =  \";\" token [ = ( token | quoted-string ) ]\n\n    The server SHOULD respond with a 419 (Expectation Failed) status\n    if any of the expectations cannot be met.\n\n    This header field is defined with extensible syntax to allow for\n    future extensions.  If a server receives a request containing\n    an Expect field that includes an expectation-extension that\n    it does not support, it MUST respond with a 419 (Expectation\n    Failed) status.\n\n14.XX.1 Expect 100-continue\n\n    When the \"100-continue\" expectation is present on a request that\n    includes a body, the requesting client will wait after sending the\n    request headers before sending the content-body.  In this case, the\n    server MUST conform to the requirements of section 8.2.4: it MUST\n    either send a 100 (Continue) status, or an error status, after\n    receiving the \"Expect: 100-continue\" request header.\n\n    If a proxy receives a request with the \"100-continue\" expectation,\n    and the proxy either knows that the next-hop server complies with\n    HTTP/1.1 or higher, or does not know the HTTP version of the\n    next-hop server, it MUST forward the request, including the Expect\n    header field.  If the proxy knows that the version of the next-hop\n    server is HTTP/1.0 or lower, it MUST NOT forward the request, and\n    it MUST respond with a 419 (Expectation Failed) status.  Proxies\n    SHOULD maintain a cache recording the HTTP version numbers received\n    from recently-referenced next-hop servers.\n    \nNote: Because of the presence of older implementations, the\nprotocol allows ambiguous situations in which a client may send\n\"Expect: 100-continue\" without receiving either a 419\n(Expectation Failed) status or a 100 (Continue) status.\nTherefore, when a client sends this header field to an origin\nserver (possibly via a proxy) from which it has never seen a\n100 (Continue) status, the client should not wait for an\nindefinite or lengthy period before sending the request body.\n\n=============================\n=== \n=== 13.11 in RFC 2068 incorrectly allows a proxy to inject\n=== its own 100 response into the reply stream.  The change\n=== below modifies *only* the last sentence of the first\n=== paragraph.\n=== \n=============================\n\n13.11 Write-Through Mandatory\n\n   All methods that may be expected to cause modifications to the origin\n   server's resources MUST be written through to the origin server. This\n   currently includes all methods except for GET and HEAD. A cache MUST\n   NOT reply to such a request from a client before having transmitted\n   the request to the inbound server, and having received a\n   corresponding response from the inbound server. This does not\n   prevent a proxy cache from forwarding a 100 (Continue) response\n   before the inbound server has sent its final reply.\n\n   The alternative (known as \"write-back\" or \"copy-back\" caching) is not\n   allowed in HTTP/1.1, due to the difficulty of providing consistent\n   updates and the problems arising from server, cache, or network\n   failure prior to write-back.\n\n=============================\n=== \n=== Add this to the end of 8.1.2.2 (Pipelining)\n=== \n=============================\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2).  Otherwise,\n   a premature termination of the transport connection may lead to\n   indeterminate results.  A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\n=============================\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "   Date: Thu, 17 Jul 1997 15:47:13 -0700 (PDT)\n   From: \"Gregory J. Woodhouse\" <gjw@wnetc.com>\n   X-Url: http://www.wnetc.com/\n\n   To me, it seems like the real problem is that the server has no way of\n   knowing how much data to expect. Accepting a chunked PUT or POST is an all\n   or nothing type of commitment. I doubt it's possible in HTTP/1.1, but it\n   seems to me that the server need to be able to indicate how much data it\n   is willing to accept and then allow the client to decide whether or not to\n   attempt to send the request. (A client may not know how much data it has\n   to send, but it may know that it will not exceed a certain threshold.)\n\nIn general, I've been taught to write programs that don't have arbitrary\nlimits, so I think I would hate to write a server which places a limit\non the size of a POST request.\n\nPerhaps for a search engine, it might make sense to create some restriction\non the size of such data; but I think no matter what you do you're\nopen to denial-of-service attacks to some extent.\n\n\n\n"
        },
        {
            "subject": "LAST CALL on MAXAGE issu",
            "content": "Unfortunately, the issues list at\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#MAX-AGE\n\nreferences the wrong email message from Roy as the Proposed Resolution.\nThe reference to\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0650.html\n\nshould be replaced by a reference to\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0684.html\n\nwhere Roy said \"I am changing my proposed change.\"\n\nAnyway, I would like to suggest a few minor, non-normative changes\n(to Roy's updated proposal):\n\n(1) I think this note (in Roy's proposal)\n\n     Note: An origin server wishing to use a relatively new HTTP cache\n     control feature, such as the \"private\" directive, on a network\n     that includes older caches which do not understand that feature,\n     will need to combine the new feature with an old Expires value\n     in order to prevent the older caches from caching the response.\n\ncould be made slightly clearer:\n\n     Note: An origin server wishing to use a relatively new HTTP cache\n     control feature, such as the \"private\" directive, on a network\n     including older caches that do not understand that feature, will\n     need to combine the new feature with an Expires field whose value\n     is less than or equal to the Date value.  This will prevent older\n     caches from improperly caching the response.\n\n(2) I think it would be a good idea to include after the last\nparagraph of this section (14.9.3):\n\n   If a cache returns a stale response, either because of a max-stale\n   directive on a request, or because the cache is configured to\n   override the expiration time of a response, the cache MUST attach a\n   Warning header to the stale response, using Warning 10 (Response is\n   stale).\n\nthe following note:\n\n       Note: A cache may be configured to return stale responses\n       without validation, but only if this does not conflict with any\n       MUST-level requirements concerning cache validation (e.g., a\n       \"must-revalidate\" Cache-control directive).\n    \nIn a private email discussion during March, Roy pointed out that\nthis is said elsewhere in the specification.  However, I'm concerned\nthat some implementors may misconstrue the discussion in 14.9.3\nwithout such a reminder.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE CONTENTENCODING: Proposed wordin",
            "content": "Did we include specification of which headers are allowed\nin the footer? The 3.6 section indicates \"only those which are\nspecified explicitly\" but none are explicitly specified.\n\nIn the draft, there are only like 4 occurrences of the word\n\"footer\".  And all but 1 are in 3.6..\n\n\n-----------------------------------------------------------------------------\nJosh Cohen        Netscape Communications Corp.\nNetscape Fire Department                      #include<disclaimer.h>\nServer Engineering\njosh@netscape.com                       http://home.netscape.com/people/josh/\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE: LANGUAGETA",
            "content": "Martin J. Duerst:\n>\n[....]\n>Another question that remains to me is that currently, the\n>spec assumes that each document has assigned exactly one\n>language-tag, \n\n???  The spec does not assume this, see the section on the\nContent-Language header.  Is there some text in the spec which\ncontradicts the Content-Language section?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL on MAXAGE issu",
            "content": "Looks good to me.\n\n....Roy\n\nIn message <9707190116.AA23026@acetes.pa.dec.com>, Jeffrey Mogul writes:\n>(1) I think this note (in Roy's proposal)\n>\n>     Note: An origin server wishing to use a relatively new HTTP cache\n>     control feature, such as the \"private\" directive, on a network\n>     that includes older caches which do not understand that feature,\n>     will need to combine the new feature with an old Expires value\n>     in order to prevent the older caches from caching the response.\n>\n>could be made slightly clearer:\n>\n>     Note: An origin server wishing to use a relatively new HTTP cache\n>     control feature, such as the \"private\" directive, on a network\n>     including older caches that do not understand that feature, will\n>     need to combine the new feature with an Expires field whose value\n>     is less than or equal to the Date value.  This will prevent older\n>     caches from improperly caching the response.\n>\n>(2) I think it would be a good idea to include after the last\n>paragraph of this section (14.9.3):\n>\n>   If a cache returns a stale response, either because of a max-stale\n>   directive on a request, or because the cache is configured to\n>   override the expiration time of a response, the cache MUST attach a\n>   Warning header to the stale response, using Warning 10 (Response is\n>   stale).\n>\n>the following note:\n>\n>       Note: A cache may be configured to return stale responses\n>       without validation, but only if this does not conflict with any\n>       MUST-level requirements concerning cache validation (e.g., a\n>       \"must-revalidate\" Cache-control directive).\n>    \n>In a private email discussion during March, Roy pointed out that\n>this is said elsewhere in the specification.  However, I'm concerned\n>that some implementors may misconstrue the discussion in 14.9.3\n>without such a reminder.\n>\n>-Jeff\n>\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-negotiation02.tx",
            "content": "Graham Klyne:\n>\n>At 07:59 PM 7/14/97 +0200, Koen Holtman wrote:\n[...]\n>>No, this looks about right, though I would add\n>>\n>>  feature-set --> ftag\n>\n>I cannot find a syntax production for feature-set in your draft.\n\nThere is none, I drew the arrow to mean `uses'.  Feature sets are an\nimportant concept in the draft, but as they never appear on the wire,\nthere are no syntax rules for them.\n\n>>>* Section 8.4:\n>>>\n>>>Are there any circumstances in which a response from a transparently\n>>>negotiable resource is not required to include an 'Alternates:' header?\n>>\n>>Yes.  If the response is an error, list or adhoc response, Alternates\n>>need not be included.\n>\n>Aha!  So Normal and Choice responses containing a transparently negotiated\n>resource are required to carry an 'Alternates' header?\n\nA transparently negotiated resource may never send a normal response\n(see the table in section 12.1), but you are right about  the choice\nresponse.\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: New feature tag registration drafts availabl",
            "content": "Graham Klyne:\n>\n>\n>Isn't there a conflict here with <draft-mutz-http-attributes-02.txt>?\n\nThere probably is, the draft-mutz-http-attributes drafts do not\nreflect the syntax rules in the latest TCN draft, but this will be\nfixed eventually.\n\n[...]\n>OK -- I accept that an alias mechanism is probably not a Good Idea.\n>\n>I think your comment suggests a possible requirement on a generic\n>negotiation framework is the ability to treat some set of features as\n>interchangeable in the context of some specific negotiation exchange, as\n>your 'fpred-bag' does for  indicating the quality of a variant.\n\nYes, any sufficiently generic framework would have to be able to treat\nfeatures as interchangeable.  This would even have to be done if the\nalias problem were absent.  For example, for a document which will use\nanimated gifs if java is not present, the `java' and `animated gif'\nfeatures are interchangeable.\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "ISSUE QUOTEDBACK: What is the proposal",
            "content": "Reading the issue list, I am not sure what the proposed change for the\nQUOTED-BACK issue is.\n\nCould someone post the complete text which would implement the change?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "ISSUE CACHEDIRECTIVE: What is the proposal",
            "content": "Reading the issue list, I am not sure what the proposed change for the\nCACHE-DIRECTIVE issue is.  Is the proposal to delete the\n\n   [ \"=\" <\"> 1#field-name <\"> ] \n\nfrom the grammar?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-feature-reg01.tx",
            "content": "Graham Klyne:\n>\n>\n>I think the word 'dimension' gets a bit overloaded in the draft.\n\nYou may be right.  When writing the draft, I fluctuated between using\nthe terms `area of negotiation' and `dimension', and chose for\n`dimension' eventually.  But maybe I should have chosen for `area of\nnegotiation'.\n\n[...]\n>I phrased that badly.  I was trying to indicate the ability to specify\n>multiple instances of each value.  E.g.\n>(1)  {Yes,No}\n>(2)  {1,2,3,5,7,11} or {Red,Green,Blue}\n>(3)  {{1},{1,5},{1,5,11}}\n>(4)  {1.1,3.3,7.7,100}\n>(5)  {text/plain,text/html,application/word} or {[1.5-2.5],[2.0-20.0]}\n>\n>Another thought: for item (4) should there be a distinction between\n>open/closed intervals?\n\nI guess the problem is to find the right level of detail.  I'm trying\nto stay away from things like open/closed intervals.  I don't think\nthat the _result_ of negotiation in a dimension will often be an\ninterval, though intervals may be used in the exchange which\nestablishes the result.\n\n>>   Number of alternatives in (sub)negotiation with this tag:\n>\n>?? (sub)negotiation -- I don't know what you mean by that.\n\nOops, some more terminology I did not introduce.  Chop off the (sub).\n\n>How about:\n>\"Number of alternative values for the feature identified by this tag\"?\n\nI think that would not work: values are not always `for' a feature.\n\n[...]\n>>     [ ] 2a.2 With an integer value\n[...]\n>2a.2: bounded/unbounded alternatives?  I am thinking that there might be\n>known limits on the value of an integer used as an enumerated value.  I am\n>not saying this is a Good Idea, just asking the question.\n\nI don't know how often there would be known limits at registration\ntime.  I think that, when negotiating, the added value of knowing the\noverall min. and max. values beforehand would be small: parties in a\nnegotiation process would presumably have to exchange their own\nspecific min. and max. values anyway.\n\nAlso, the trouble is that `always big enough' often turns out to be\n`too small' 5 years later.  The 640K barrier in PCs is a good example.\nIf there are lots of deployed negotiation mechanisms which `know' that\nthe max. value is 640, these may end up blocking progress beyond 640.\n\n[...]\n>To answer your question, I like the earlier paragraphs better than the\n>original, but I am uneasy about the technical depth implied by the\n>alternatives for 2a.  If you follow this approach I do not think there is a\n>single correct answer -- just a judgement as to what is appropriate.\n\nI think you are right, I'll reduce the categories in the next version.\nI think I'll just delete 2a.4 - 2a.6.\n\n>Maybe the summary of negotiable features I posted a while back could\n>provide a yardstick for what should be explicitly offered?  \n\nIf I remember correctly, this summary has *lots* of different types of\nvalues.  Offering them all explicitly would make the list very long,\nI think.\n\n>I don't think\n>there is a reason why the form should not be extended later if new feature\n>value types start occurring frequently in the future (as this would simply\n>represent a refining of the 'other' category, not the addition of new\n>information).\n\nYes, so it makes sense to keep the initial list limited.\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: STATUS100 Re: Proposed resolutio",
            "content": "> I suppose one could start a pointless argument about whether the\n> intention behind \"potential for causing harm (e.g., limiting\n> retransmissions)\" applies to \"limiting the transmission of\n> unnecessary bytes over the network.\"  I'll leave this decision\n> to the working group chair.  Larry, if you ask me to remove this\n> SHOULD NOT, please say so.\n\nMy personal (not wg-chair) opinion is that we should avoid placing\nany requirements we don't need to place. Limiting the transmission\nof unnecessary bytes over the network is grounds for good implementation\nadvice, but not for a SHOULD NOT. There are a few cases where we've\nplaced requirements for reasons other than interoperability, so I don't\nthink it's a hard rule.\n\nOfficially, we can't make requirements that don't match experience;\nin going from Proposed to Draft, we cannot have a \"SHOULD NOT\ndo X\" if we can't document two independent interoperable implementations\nthat don't do X; but I don't think that's an issue in this case.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Last Call: Simple Hit-Metering and UsageLimiting for HTTP to Proposed Standar",
            "content": " The IESG has received a request from the HyperText Transfer Protocol\n Working Group to consider \"Simple Hit-Metering and Usage-Limiting for\n HTTP\" <draft-ietf-http-hit-metering-03.txt> for the status of Proposed\n Standard.\n\n The IESG plans to make a decision in the next few weeks, and solicits\n final comments on this action.  Please send any comments to the\n iesg@ietf.org or ietf@ietf.org mailing lists by August 4, 1997.\n\n\nFiles can be obtained via ftp://ds.internic.net/internet-drafts/<filename>\n\n\n\n"
        },
        {
            "subject": "Q: HOWTO use different transmission schemes",
            "content": "So is there/will there be an automatic adaption to  data/content dependent \ntransmission schemes within HTTP, i.e. certain streams of data are fault \ntolerant by nature or need more throughput or somthing else. So an adaption\nto the best protocoll for certain data types would possibly quite usefull.\n\nIs there a plan to put it into HTTP or is this delegated over an \ninterface (probably the MIME-type handling) to another protocoll or the \nactual http server itself?\n\nThanks for help or references\n\n        Bye   Brusi\n\n            by           E-Mail: ab2@inf.tu-dresden.de\n                         Tel.-priv: 0351-8499347 (Germany/Dresden)\n          \\____\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE: LANGUAGETA",
            "content": "Koen - I'm glad you came back on this one.\n\nOn Sat, 19 Jul 1997, Koen Holtman wrote:\n\n> Martin J. Duerst:\n> >\n> [....]\n> >Another question that remains to me is that currently, the\n> >spec assumes that each document has assigned exactly one\n> >language-tag, \n> \n> ???  The spec does not assume this, see the section on the\n> Content-Language header.  Is there some text in the spec which\n> contradicts the Content-Language section?\n\n\"exactly one language tag\" was probably wrong. What I mean is\nsomething on a somewhat higher level, which I at the moment\nhave difficulties to express.\n\nAssume we have a very sophisticated server, which knows a lot\nabout the documents it keeps or produces, and the languages\nthey are kept in. Assume some of this knowledge cannot be\nexpressed in one or more language tags attached to the\ndocuments, including q values.\n\nThe question then is whether such a server is allowed to\ncheat, i.e. whether it is e.g. allowed to peek at the\nAccept-Language header field sent in before it calculates\nthe language(s) and q values it assigns to a certain\ndocument.\n\nAs an example, assume that a server has an English document,\nas Japanese document, and an English-Japanese bilingual\ndocument. The bilingual document is the source and contains\nall text in the original, and therefore is the preferred\ndocument. However, if a reader has only English in her\nAccept-language header field, the English document should\nbe served, and so on. Currently, such behaviour could only\nbe modelled by \"cheating\" as explained above, i.e. by\nanalysing the Accept-language header field and e.g.\npretending\n\nEnglish document:en; q=1.0\nJapanese document:ja; q=1.0\nBilingual document:en; q=0.5, ja; q=0.5\n\nin case the Accept-language header field contains\nonly English or only Japanese, but\n\nEnglish document:en; q=0.5\nJapanese document:ja; q=0.5\nBilingual document:en; q=1.0, ja; q=1.0\n\nin case the Accept-language header field contains both\nEnglish and Japanese (with a reasonably high q value each).\nNote that many other q values in the above example will\nhave the desired effect, but there is no single combination\nof q values that will have the desired effect in both cases.\n\nI'm not proposing here to make the algorithm more complicated,\nas this is probably a rare case. But I wonder whether such\n\"cheating\" is allowed or disallowed according to the spec,\nor whether it is just an implementation issue.\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Editorial ISSUE REMOVE_19.",
            "content": "We plan to remove section 19.6, \"19.6 Additional Features \"\nwhich is part of the appendix (i.e. this is non-normative material currently\nin the specification).\n\nThis documents proposals for methods PATCH, LINK, UNLINK, and header\nfields Alternates, Content-Version, Derived-From, Link, and URI.\n\nThe grounds for removing this section include:\n1) there are no known implementations\n2) the WEBDAV group is working in this area, and documenting\ndifferent, and incomplete functions, is a bad idea.\n3) there are known problems with some of the proposals: e.g. Link\nsytax does not quite seem to be what the HTML group needs for\nenabling server setting of sytle sheets; its grammar is suspect.  I've\nsuggested to Dan Connolly that Link progress as a separate (very short)\nInternet Draft to proposed standard with whatever they need, and hopefully \nquickly.\n4) there has been no known discussion of these features on the\nmailing list, and given we don't know of any implementations, going to\ndraft standard with these in the document is a bad idea.\n\nIETF process does not allow documents to become draft standards with\nunimplemented features, and while this section is not part of the normative\ndocument, we don't need the confusion or errors that might result.\n\nNote that RFC 2068 is now an RFC, so anyone interested in seeing such\ndesign work can always refer to it, even when obsoleted by the draft\nstandard.\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE CACHEDIRECTIVE: What is the proposal",
            "content": "   Reading the issue list, I am not sure what the proposed change for the\n   CACHE-DIRECTIVE issue is.\n\nThe link, under Proposed Solution, is to\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/cachedirective.txt\n\nUnfortunately, nobody has gotten around to editing that message to\nreflect what the editorial group decided:\n\n    change the grammar for\n        cache-request-directive =\n                         \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n    to\n        cache-request-directive =\n                         \"no-cache\"\n\nThis same email message is being used as the problem statement for\nCLARIFY-NOCACHE:\n\n    Also, the meaning of no-cache=\"field-name\" in a response probably\n    needs to be made more explicit.\n\nI'm the responsible party for drafting the clarification, but this\nbeing postponed while we are working on proposals for substantive changes.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE: LANGUAGETA",
            "content": "Martin J. Duerst:\n>\n[...]\n>Assume we have a very sophisticated server, which knows a lot\n>about the documents it keeps or produces, and the languages\n>they are kept in. Assume some of this knowledge cannot be\n>expressed in one or more language tags attached to the\n>documents, including q values.\n>\n>The question then is whether such a server is allowed to\n>cheat, i.e. whether it is e.g. allowed to peek at the\n>Accept-Language header field sent in before it calculates\n>the language(s) and q values it assigns to a certain\n>document.\n\nYes, it is allowed to cheat, but I would not call it cheating.\n\nThe 1.1 spec prescribes how to assign a q value to a language tag\nusing an Accept-Language header, i.e. there os only one legal way to\ncompute q-of-en and q-of-ja for English and Japanese.  However, the\nspec does not prescribe what you should do with these q-of-* values\nonce you have computed them.\n\nThere is no rule which prescribes that, whenever you have an\nEnglish-Japanese bilingual document, jou must compute the overall\nquality of this document with the formulae\n\n  max(q-of-en, q-of-ja)\n\nor \n\n  (q-of-en + q-of-ja)/2\n\nor whatever.  HTTP leaves the selection of appropriate formulae up to\nthe service author.  In terms of your example below:\n\n>As an example, assume that a server has an English document,\n>as Japanese document, and an English-Japanese bilingual\n>document. The bilingual document is the source and contains\n>all text in the original, and therefore is the preferred\n>document. However, if a reader has only English in her\n>Accept-language header field, the English document should\n>be served, and so on.\n\nthe correct formulae for the overall qualities of the documents would,\nI think, be something like\n\n       English document:   q-of-en * 0.8\n       Japanese document:  q-of-ja * 0.8\n       Bilingual document: min(q-of-en,q-of-ja)\n\n(These are correct assuming that you always want to return the\ndocument with the highest overall quality.)\n\n>Regards,Martin.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RANGEERROR: proposed solutio",
            "content": "Reference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#RANGE-ERROR\n\nSummary of proposed solution:\nAdd 416 error with Content-Range showing the total content length.\nSlight modifications to specification of Content-Range.\n\nThe editorial group seems to agree that there should be a new \nstatus code for this purpose.  There is somewhat less consensus\nover whether it's important to tell the client the current\nlength of the requested entity, and (if so) how to do so.\nThis proposal incorporates the simplest way I could think\nof to provide that information.\n\nProposed solution:\n\n(1) New status code:\n\n10.4.17 416 Requested range not valid\n\nA server SHOULD return a response with this status code if a\nrequest included a Range request-header field (section 14.36),\nand none of the range-specifier values in this field overlap\nthe current extent of the selected resource, and the request\ndid not include an If-Range request-header field.  (For\nbyte-ranges, this means that the first-byte-pos of all of the\nbyte-range-spec values was greater than the current length of\nthe selected resource.)\n\nWhen this status code is returned for a byte-range request, the\nresponse MUST include a Content-Range entity-header field\nspecifying the current length of the selected resource (see\nsection 14.17).  This response MUST NOT use the\nmultipart/byteranges content-type.\n\n[Note to editor: verify that status code 416 is not multiply assigned.]\n\n(2) Changes to 14.17 Content-Range:\n\n(a) Replace:\n\n          content-range-spec      = byte-content-range-spec\n\nwith\n\n          content-range-spec      = byte-content-range-spec | \"*\"\n\n(b) After this paragraph:\n\n   A byte-content-range-spec whose last-byte-pos value is less than its\n   first-byte-pos value, or whose entity-length value is less than or\n   equal to its last-byte-pos value, is invalid. The recipient of an\n   invalid byte-content-range-spec MUST ignore it and any content\n   transferred along with it.\n\nAdd this new paragraph:\n\n   A server sending a response with status code 416 (Requested range\n   not valid) SHOULD include a Content-range field with a\n   content-range-spec of \"*\".  The entity-length specifies the current\n   length of the selected resource.  A response with status code 206\n   (Partial Content) MUST NOT include a Content-range field with a\n   content-range-spec of \"*\".\n\n(c) Replace\n\n   If the server ignores a byte-range-spec because it is invalid, the\n   server should treat the request as if the invalid Range header field\n   did not exist. (Normally, this means return a 200 response containing\n   the full entity). The reason is that the only time a client will make\n   such an invalid request is when the entity is smaller than the entity\n   retrieved by a prior request.\n\nwith\n\n   If the server ignores a byte-range-spec because it is syntactically\n   invalid, the server should treat the request as if the invalid Range\n   header field did not exist.  (Normally, this means returning a 200\n   response containing the full entity.)\n   \n   If the server receives a request (other than one including an\n   If-Range request-header field) with an unsatisfiable Range\n   request-header field (that is, all of whose byte-range-spec values\n   have a first-byte-pos value greater than the current length of the\n   selected resource), it SHOULD return a response code of 416\n   (Requested range not valid) (section 10.4.17).\n\n      Note: clients cannot depend on servers to send a 416 (Requested\n      range not valid) response instead of a 200 (OK) response for\n      an unsatisfiable Range request-header, since not all servers\n      implement this request-header.\n\n[End of changes]\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 305/30",
            "content": "Josh Cohen <josh@netscape.com> writes:\n\n>Here is a simplified 305/306 draft..\n>\n>Changes:\n>* Scope is an 'advisory' scope, a client may follow it\n>or may use its own, potentially at the cost of inefficiency..\n>\n>* scope syntaxt, no more 'reverse domain name', now its\n>  just a regexp..\n\nRFC 2068 doesn't contain a regular expression grammar, so of course one\nmust be defined before regular expressions can be used in HTTP.  There\nis one in RFC 2168 \"Resolution of Uniform Resource Identifiers using the\nDomain Name System\", but the authors seem not to like it very much -\nthere's a note to the effect that regular expressions are complex and\nerror-prone, as well as rarely useful.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "[Announce] PEP demo at w3c sit",
            "content": "The PEP demo consists of a simple HTTP client, proxy, and server, a PEP \nlibrary, and a couple extensions. It demonstrates policy announcement, \nextension declaration, and dynamic extensions.\n\nThe source and documentation are available at\nhttp://www.w3.org/Protocols/PEP/PEPmodel and a complete description of \nPEP is available at http://www.w3.org/Protocols/PEP. I hope to get a \nchance to strap the PEP library to the RTSP code, too.\n\n-eric\n\nEric Prud'hommeaux\neric@w3.org\n\n\n\n"
        },
        {
            "subject": "Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "The CONTENT-ENCODING issue:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CONTENT-ENCODING\n\nhas been assigned to myself and Henrik for resolution.  We're pretty\nclose to solving most of it, except for a seemingly minor concern:\nHow does a client say \"don't send me the 'identity' encoding\"?\nI.e., \"please send me a compressed form or send me nothing.\"\n\n(Why would a client want to say \"don't send 'identity'\"?  Well,\nperhaps the bandwidth costs or latency costs are too high.)\n\nA month or two ago, I proposed adding an \"identity\" content-coding\nso that we could define a way for the client to say that\nit does NOT want the identity encoding.  I had originally\nthought \"if the client sends a list of explicitly acceptable\nencodings, but 'identity' isn't on this list, then this\nmeans that it doesn't want 'identity'\".  But that is obviously\nnot going to work.  For example, existing clients (apparently\nincluding Lynx) send\nAccept-encodings: gzip, compress\neven though they are presumably willing to take \"identity.\"  If\nwe were to adopt the rule that I had originally proposed, Lynx\nclients would suddenly get error returns for most resources!\n\nSince we certainly do not want break existing clients, the\nother alternative that I thought of seems simpler, if somewhat\nodd at first glance: define an explicit way for the client to say\n\"I would rather have 406 than the identity-encoding\".  Here are\nsome ways to do this:\n\n  (a)Accept-Encoding: gzip, compress, no-identity\n/* an explicit \"no identity-encoding wanted\" token */\n\n  (b)Accept-Encoding: gzip, compress, strict\n/* \"strict\" means \"this set, or nothing\" */\n\n  (c)Accept-Encoding: gzip, compress, identity;q=0.0\n/* allow qvalues here */\n\n  (d)Accept-Encoding-Strict: gzip, compress\n/* define new header to avoid compatibility questions */\n\nWhich one to choose?  (d) is least likely to cause trouble, but\nit also means that the client has to guess the origin-server version.\nWe have no reliable way to do this, so in practice (d) would not\nreally encourage the use of compression.\n\n(c) seems closest to existing practice, although it's possible\nthat some existing servers might choke on the qvalue.\n\n(a) and (b) mean roughly the same thing, and are certainly not\ngoing to cause compatibility problems, but they are a little kludgey.\n\nWe're already likely to propose that Accept-Encoding allow\nthe use of \"*\" to mean \"whatever you want to send.\"  So a slight\nvariation on (c) would be\n\n  (e)Accept-Encoding: gzip, compress, *;q=0\n\nI.e., the \"coding\"\n*;q=0\nis semantically equivalent to the \"strict\" token in (b) above,\nbut somewhat closer to the form of the other Accept-* headers.\n\nMy own preference is for one of these two approaches:\n\n  (c)Accept-Encoding: gzip, compress, identity;q=0.0\n  (e)Accept-Encoding: gzip, compress, *;q=0\n\nHowever, neither of these will work if any existing servers\nor proxies would choke on a qvalue in an Accept-Encoding header.\nAlso, while (e) is cleaner in some ways, it also is infeasible\nif any existing servers or proxies would choke on a \"*\" here.\n\nComments?  As soon as possible, please!\n\n-Jeff\n\nP.S.: We may have to include a Note to client implementors that\nsending a qvalue for a content-coding from the set {\"compress\",\n\"gzip\", \"x-compress\", \"x-gzip\"} might be misinterpreted by older\nservers, and so is not recommended.  At least, the one older server\nsource that I looked at (Apache_1.1b3) looks like it will treat\nAccept-encoding: gzip;q=1.0\nas a request for a content-coding that it doesn't know about.\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": " >  (a) Accept-Encoding: gzip, compress, no-identity\n> /* an explicit \"no identity-encoding wanted\" token */\n>\n>  (b) Accept-Encoding: gzip, compress, strict\n> /* \"strict\" means \"this set, or nothing\" */\n>\n>  (c) Accept-Encoding: gzip, compress, identity;q=0.0\n> /* allow qvalues here */\n\nI think this is the best. it allows the case to be handled where the\nsource form is accepted but not ideal. Consider the case where the rank\nof preferences is gzip / source / compress where gzip is prefered, and\nthe source form prefered over unix compress.\n\nI think this is a more realistic case than \"don't want the source at\nall\".\n\n>  (d) Accept-Encoding-Strict: gzip, compress\n> /* define new header to avoid compatibility questions */\n\nUgh! Please, no more headers unless there is a serious problem with one\nalready defined!\n\nI dislike the * proposal intensely. Best leave punctuation for use in\nregular expressions and separators. It is always a mistake to use a\npunctuation mark to stand for a symbol IMHO. identity is OK by my\nreconning although NULL is shorter and may be more descriptive in this\ncontext.\n\n\n    Phill\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "Phillip M. Hallam-Baker writes:\n    I dislike the * proposal intensely. Best leave punctuation for use\n    in regular expressions and separators. It is always a mistake to\n    use a punctuation mark to stand for a symbol IMHO. identity is OK\n    by my reconning although NULL is shorter and may be more\n    descriptive in this context.\n    \nThere are already numerous places in RFC2068 where the \"*\" syntax\nis used as a \"wildcard\", including\nOPTIONS method\nAccept:\nAccept-Language:\nIf-Match:\nIf-None-Match:\nVary:\nAnd its use has also been last-called in the Content-Range header.\n\nTo me, it seems odd that Accept-Charset and Accept-Encoding don't\nalready allow a \"*\" value, given that Accept and Accept-Language do.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "   Date: Mon, 21 Jul 97 15:59:25 MDT\n   From: Jeffrey Mogul <mogul@pa.dec.com>\n\n     (a)Accept-Encoding: gzip, compress, no-identity\n   /* an explicit \"no identity-encoding wanted\" token */\n\n     (b)Accept-Encoding: gzip, compress, strict\n   /* \"strict\" means \"this set, or nothing\" */\n\nNote that any older server will ignore the `strict' or the `no-identity',\nso you still might get uncompressed content.\n\n     (c)Accept-Encoding: gzip, compress, identity;q=0.0\n   /* allow qvalues here */\n\n     (d)Accept-Encoding-Strict: gzip, compress\n   /* define new header to avoid compatibility questions */\n\nThis breaks existing servers to some extent, and I really think it's\nugly and really don't like it.\n\n     (e)Accept-Encoding: gzip, compress, *;q=0\n\n   I.e., the \"coding\"\n   *;q=0\n   is semantically equivalent to the \"strict\" token in (b) above,\n   but somewhat closer to the form of the other Accept-* headers.\n\n   My own preference is for one of these two approaches:\n\n     (c)Accept-Encoding: gzip, compress, identity;q=0.0\n     (e)Accept-Encoding: gzip, compress, *;q=0\n\nI like (c) and (e), though I wouldn't be able to agree that they are\ngood if any server will choke on them.\n\nOf course, I don't quite see why I would need to reject the identity\nencoding.\n\n   P.S.: We may have to include a Note to client implementors that\n   sending a qvalue for a content-coding from the set {\"compress\",\n   \"gzip\", \"x-compress\", \"x-gzip\"} might be misinterpreted by older\n   servers, and so is not recommended.  At least, the one older server\n   source that I looked at (Apache_1.1b3) looks like it will treat\n   Accept-encoding: gzip;q=1.0\n   as a request for a content-coding that it doesn't know about.\n\nSad.  I'd like E-scape to advertise that it can handle compress, but\nprefers gzip (because patents mean that a compress implementation\ncan't be free from fear of lawsuits unless royalties are paid;\nwhereas gzip doesn't seem to have this problem).\n\n\n\n"
        },
        {
            "subject": "Re: Editorial ISSUE REMOVE_19.",
            "content": ">We plan to remove section 19.6, \"19.6 Additional Features \"\n>which is part of the appendix (i.e. this is non-normative material currently\n>in the specification).\n\nI support this.  The WEBDAV working group has requirements for providing\npartial resource updates and for the creation, deletion, modification and\naccess of typed links between resources.  The latest protocol draft, which\nwas submitted to the Internet-drafts editor on Friday, details a PATCH\nmethod, and a mechanism for creating links using a general-purpose facility\nfor defining properties on resources.  The WEBDAV proposals differ from the\ndefinitions in Section 19.6 of RFC 2068.  Since WEBDAV is currently working\non these items, it makes sense to me to remove Section 19.6.\n\nSome brief comments:\n\n>The grounds for removing this section include:\n>        1) there are no known implementations\n\nThe NTT, Palo Alto group has produced an implementation of LINK, UNLINK,\nand the Link header.  However, to the best of my knowledge, this is not a\nproduction server, but rather a research testbed.  Since they have been\nactively tracking the WEBDAV working group, I suspect they would be\ncomfortable moving to the WEBDAV interface and semantics for links.\n\n>        2) the WEBDAV group is working in this area, and documenting\n>different, and incomplete functions, is a bad idea.\n\nI agree.\n\n>        3) there are known problems with some of the proposals: e.g. Link\n>sytax does not quite seem to be what the HTML group needs for\n>enabling server setting of sytle sheets; its grammar is suspect.  I've\n>suggested to Dan Connolly that Link progress as a separate (very short)\n>Internet Draft to proposed standard with whatever they need, and hopefully\n>quickly.\n\nSince WEBDAV is developing a mechanism for links, I'm not sure what the\nvalue of a separate I-D would be.\n\n- Jim Whitehead\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "Joel N. Weber II writes:\n\n (a)Accept-Encoding: gzip, compress, no-identity\n       /* an explicit \"no identity-encoding wanted\" token */\n    \n (b)Accept-Encoding: gzip, compress, strict\n       /* \"strict\" means \"this set, or nothing\" */\n    \n    Note that any older server will ignore the `strict' or the\n    `no-identity', so you still might get uncompressed content.\n    \nOf course.  I probably should have stated that the ability to\nsay \"don't send me 'identity'\" is an optimization, and not always\navailable (because of the situation you suggest).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Proposed solution for issue CLARIFY-NOCACH",
            "content": "The CLARIFY-NO-CACHE issue:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CLARIFY-NO-CACHE\n\nboils down to:\nthe meaning of no-cache=\"field-name\" in a response probably\nneeds to be made more explicit.\n\nTo state the issue in somewhat more detail, RFC2068 says this about\nthe \"no-cache\" Cache-control directive in a response:\n\n    14.9 Cache-Control\n    \n    [...]\n      cache-response-directive =\n    [...]\n      | \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n    [...]\n    \n    14.9.1 What is Cachable\n    [...]\n\n    no-cache\n      Indicates that all or part of the response message MUST NOT be cached\n      anywhere. This allows an origin server to prevent caching even by\n      caches that have been configured to return stale responses to client\n      requests.\n    \n14.9 also says:\n\n   When a directive appears without any 1#field-name parameter, the\n   directive applies to the entire request or response. When such a\n   directive appears with a 1#field-name parameter, it applies only to\n   the named field or fields, and not to the rest of the request or\n   response.  This mechanism supports extensibility; implementations of\n   future versions of the HTTP protocol may apply these directives to\n   header fields not defined in HTTP/1.1.\n\nHowever, I think it takes a fair amount of inference to combine these\ntwo paragraphs into a precise definition of what\nCache-control: no-cache=foo\nreally means.\n\nPROPOSED SOLUTION:\nI replacing (in 14.9.1) the paragraph:\n\n      Indicates that all or part of the response message MUST NOT be cached\n      anywhere. This allows an origin server to prevent caching even by\n      caches that have been configured to return stale responses to client\n      requests.\n    \nwith paragraph:\n\n      If the no-cache directive does not specify a field-name, then a\n      cache MUST NOT use the response to satisfy a subsequent request\n      without successful revalidation with the origin server.  This\n      allows an origin server to prevent caching even by caches that\n      have been configured to return stale responses to client\n      requests.\n\n      If the no-cache directive does specify one or more field-names,\n      then a cache MAY use the response to satisfy a subsequent\n      request, subject to any other restrictions on caching; however,\n      the specified field-name(s) MUST NOT be sent in the response to a\n      subsequent request without successful revalidation with the\n      origin server.  This allows an origin server to prevent\n      the re-use of certain header fields in a response, while\n      still allowing caching of the rest of the response.\n\n-Jeff\n\nP.S.: As I wrote in\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q1/0040.html\n\n    I'm not sure that this convoluted definition of no-cache really\n    makes life easier for people.  I remember arguing that we should\n    be using a wider range of names for cache-control directives, and\n    being accused of trying to make the specification \"too complex.\"\n    But it's probably too late to change the actual specification\n    of \"no-cache\", although I think it's clear that we need to clarify it.\n\nHowever, my preference would be to replace the BNF\n\n          cache-response-directive =\n                     | \"no-cache\" [ \"=\" <\"> 1#field-name <\"> ]\n\nwith\n          cache-response-directive =\n                     | \"no-cache\"\n                     | \"no-cache-field\" \"=\" <\"> 1#field-name <\"> \n\nand then do the obvious simplification of the description proposed\nabove.\n\n\n\n"
        },
        {
            "subject": "OPTIONS Spe",
            "content": "Here is the draft changes for the specification of the OPTIONS\nmethod.\n\nAs stated in the attachment, there is some overlap with PEP.\nOverall, PEP is much more flexible and efficient.\nOPTIONS is simpler at the possible expense of efficiency.\n \nHowever, OPTIONS is simple and easy to implement, and thus\nmore likely to be accepted in the HTTP/1.1 spec.  I think\nthat since many things which were once supposed to be\npart of the HTTP/1.1 spec are now on their own track,\nIts essential to have a simple mechanism of detecting\nsupport for those extensions as a part of the core\nHTTP/1.1 protocol.\n\n\n-----------------------------------------------------------------------------\nJosh Cohen      Netscape Communications Corp.\nNetscape Fire Department                #include<disclaimer.h>\nServer Engineering\njosh@netscape.com                          http://people.netscape.com/josh/\n-----------------------------------------------------------------------------\n\nOptions Method specification:\n\nOverall Changes:\n the URI '*' refers to the server, independent of any specific URI.\n\n The host: header may be used to specify a named proxy or origin \n server in a chained environment\n\n The max-forwards: header may be used to specify a maximum number\n of hops the request may be forwarded in a chained environment\n\n The actual OPTIONS message is contained in the request/response\n body.\n\nEditing changes:\nIn section 5.1.2:\n-   If a proxy receives a request without any path in the Request-URI and\n-   the method specified is capable of supporting the asterisk form of\n-   request, then the last proxy on the request chain MUST forward the\n-   request with \"*\" as the final Request-URI. For example, the request\n-\n-          OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n-\n-   would be forwarded by the proxy as\n-\n-          OPTIONS * HTTP/1.1\n-          Host: www.ics.uci.edu:8001\n-\n-   after connecting to port 8001 of host \"www.ics.uci.edu\".\n\n+   Proxy Servers should use the Host: or max-forwards: header\n+   to determine whether or not to forward the reqeust as \n+   specified in section 9.2\n\nIn section 9.2:\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server as a whole. A 200 response SHOULD\n   include any header fields which indicate optional features\n   implemented by the server (e.g., Public), including any extensions\n   not defined by this specification, in addition to any applicable\n-   general or response-header fields. As described in section 5.1.2, an\n-   \"OPTIONS *\" request can be applied through a proxy by specifying the\n-   destination server in the Request-URI without any path information.\n\n+   general or response-header fields.\n\n+   OPTIONS messages may be passed through a proxy by using\n+   either the host: header to indicate the intended destination\n+   of the OPTIONS message.  The max-forwards: header may also\n+   be used to indicate that the destination is the Nth hop,\n+   as specified by the max-forwards: header.\n\n+   Origin servers supporting 'virtual interfaces' or \n+   'virtual servers' may use the Host: header or the max-forwards:\n+   header to determine the intended destination as well.\n\nNew definitions:\n\n9.2.1 OPTIONS Messages\n\n9.2.1.1 The REQUEST:\n\n The body of the OPTIONS request will containt the specific\n information requested.\n\n The body will be of mime type TBD (text/ietf-http-options)\n\n9.2.1.2 The REPLY:\n\n The server will respond with 200 if it supports the OPTIONS\n method.\n\n The OPTIONS reply will contain a content of mime type\n TBD ( text/ietf-http-options )\n\n9.2.2.1 Body definition:\n\n9.2.2.2 REQUEST:\n\n The body will be a text formatted content, where:\n\nbody = 1(header-token) #(request)\n\nheader-token = \"OPTIONS\" options-version new-line\n\noptions-version = \"1.0\"\n\nrequest = command-token (feature-token | \"*\" ) new-line\n\nnew-line = CRLF\n\ncommand-parameters = token\n(defined per command )\n\n\n9.2.2.3 REPLY:\nbody = 1(header-token) #(response)\n\nheader-token = \"OPTIONS\" options-version new-line\n\noptions-version = \"1.0\"\n\nresponse = command-token feature-token response-parameters new-line\n\nnew-line = CRLF\n\nfeature-token = token\n\nresponse-parameters = token\n(defined per command )\n\n9.2.3 OPTIONS commands\n\n9.2.3.1 COMPLIANCE\n\n command-token =  \"COMPLIANCE\"\nREQUEST\ncommand-parameters = feature-token | \"*\"\n\nREPLY\nfeature-token = opaque string representing a feature/extenstion\nresponse-parameters = token representing feature/extension\nspecific parameters\n\n9.2.4 Usage:\n\n The first line contains:\n OPTIONS <options spec version>\n\n Followed by a series of answers, as defined according to the \n command issued:\n\n COMPLIANCE:\n  The COMPLIANCE response is a list of COMPLIANCE indications,\n  one per line:\n  COMPLIANCE <feature-name> <parameters>\n\n  where feature-name was the argument to the COMPLIANCE request.\n  parameters is an optional opaque string indicating parameters\n    for that feature.\n\n9.2.5 Examples\n\n9.2.5.1 To list all extensions supported by proxy \"proxy4.mcom.com\"\n\n OPTIONS * HTTP/1.1\n Host: proxy4.mcom.com\n Content-type: text/ietf-http-options\n Content-length: 44 (or whatever)\n\n OPTIONS 1.0\n COMPLIANCE * \n\n HTTP/1.1 200 OK\n Server: SuperServer/1.0\n Content-type: text/ietf-http-options\n Content-length: 44 ( feh )\n\n OPTIONS 1.0\n COMPLIANCE http://foobar.pep.org/pepmeister/\n COMPLIANCE set-proxy\n COMPLIANCE wonder-bar-http-widget-set\n COMPLIANCE rfc1543\n\n9.2.5.2 Probing for a feature which is NOT supported:\n\n OPTIONS * HTTP/1.1\n Host: proxy4.mcom.com\n Content-type: text/ietf-http-options\n Content-length: 44 (or whatever)\n\n OPTIONS 1.0\n COMPLIANCE http://foobar.pep.org/evil-not-implemented\n\n HTTP/1.1 200 OK\n Server: SuperServer/1.0\n Content-type: text/ietf-http-options\n Content-length: 44 ( feh )\n\n OPTIONS 1.0\n\n\n9.2.5.3 Probing a 1.0 server/proxy ( response will vary )\n\n OPTIONS * HTTP/1.1\n Content-Type: text/ietf-http-options\n Content-length: 40\n  \n HTTP/1.0 400 Malformed URL\n Proxy-agent: Netscape-Proxy/2.52\n Date: Wed, 16 Jul 1997 02:35:54 GMT\n Content-type: text/html\n Content-length: 157\n \n <HTML><HEAD><TITLE>Malformed URL</TITLE></HEAD>\n <BODY><H1>Malformed URL</H1>\n Your browser sent a request that this proxy could not understand.\n </BODY></HTML>Connection closed by foreign host.\n\n9.2.6 Applicability\n\n  The OPTIONS method is intended to provide a mechanism by which\n a downstream client or proxy may query an upstream proxy or\n origin server about what HTTP/1.1 extensions it supports.\n\n  While this has some overlap with the PEP[ref] specification,\n it only provides functionality for the lower spectrum of PEP.\n It is simple to implement and should be a MUST for implementations.\n Along with the simplicity comes a cost. OPTIONS request need a\n network round trip, while PEP does not.  Therefore, should\n PEP become a standard, or be rolled into this specification,\n implementations are encouraged to use PEP where applicable\n to gain extra flexibility and performance, however, they MUST\n support OPTIONS as well.\n\n\n\n"
        },
        {
            "subject": "SPEC 305/30",
            "content": "Here is my latest changes, removing the regexp stuff.\n\nNow its just a simple '*' for dns names..\n\nie scope;\n scope=\"http://*.foo.com/\"\n  matches:\n  http://www.foo.com/foo/index.html\n but not\n  http://www.bar.com/index.html.\n\n\n-----------------------------------------------------------------------------\nJosh Cohen      Netscape Communications Corp.\nNetscape Fire Department                #include<disclaimer.h>\nServer Engineering\njosh@netscape.com                          http://people.netscape.com/josh/\n-----------------------------------------------------------------------------\n\n\n\n\n\n\n\nHTTP Working Group                                            Josh Cohen\nInternet-Draft                             Netscape Communications Corp.\n                                                         5 December 1996\n\n                  HTTP/1.1 305 and 306 Response Codes\n\n                    <draft-cohen-http305036-00.txt>\n\nStatus of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups.  Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time.  It is inappropriate to use Internet- Drafts as reference\n   material or to cite them other than as ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please check the\n   ``1id-abstracts.txt'' listing contained in the Internet- Drafts\n   Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\nAbstract\n\n   The HTTP/1.1 RFC specifies a response code '305 Use Proxy' which is\n   intended to cause a client to retry the request using a specified\n   proxy server.  This functionality is important, but underspecified in\n   the current spec.  The spec does not specify for how long or which\n   URLs the redirect applies to, or how proxies can deal with or\n   generate similar responses.  This draft proposes a specification for\n   both the 305 response and a new response, \"306 Switch Proxy\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 1]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\nSummary\n\n 1.0 Response Codes\n\n  1.1 305 Use Proxy\n  1.2 306 Switch Proxy\n  1.3 506 Redirection Failed\n\n 2.0 Headers\n\n  2.1 Set-proxy:\n  2.2 Location:\n\n 3.0 Methods\n\n  3.1 OPTIONS\n\n 4.0 Operational Constraints\n\n 5.0 Notes\n\n\n1.0 Response Codes\n\n 1.1 305 Use Proxy\n\n   The 305 is generated by an origin server to indicate that the client,\n   or proxy, should use a proxy to access the requested resource.\n\n   The request SHOULD be accompanied by a 'Set-proxy' response header\n   indicating what proxy is to be used. The client will parse the 'Set-\n   proxy' header as defined below to decide how long, for what URLs it\n   should use the specified proxy.\n\n   If the 305 response is not accompanied by a 'Set-proxy' header, it\n   MUST be accompanied by a 'Location' header.  The 'Location' header\n   will specify a URL to the proxy.\n\n   If both headers are present in the response, the client SHOULD use\n   the 'Set-proxy' header only.\n\n 1.2 306 Switch Proxy\n\n   The 306 response is generated by a proxy server to indicate that the\n   client or proxy should use the information in the accompanying 'Set-\n   proxy' header to choose a proxy for subsequent requests.\n\n   The 306 response code MUST be accompanied by the 'Set-proxy' response\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 2]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n   header.  The client or proxy will parse the 'Set-proxy' header to\n   determine which proxy to use, how long to use it, and for which URLs\n   to use it.\n\n   The scope in the set-proxy header is considered an optional advisory.\n   The client or proxy may choose to ignore it, and use it for just this\n   request, for all requests, or for a scope previously or implicitly\n   defined by another configuration method or autoconfiguration system.\n\n 1.3 506 Redirection Failed\n\n   The 506 response is returned when a redirection fails or is refused\n   by a proxy or client.  If the redirection response included a body,\n   then it SHOULD be included in the 506 response.\n\n2.0 Headers\n\n 2.1 'Set-proxy' Response Header\n\n           The 'Set-proxy' header is defined as:\n\n           Set-proxy: \"Set-proxy\" \":\" 1(\n                   action #(parameters)\n                   )\n\n           parameters = #( ( \"scope\" \"=\" scopePattern ) |\n                   ( proxyURI \"=\" URI ) |\n                   lifetime )\n\n           lifetime = ( \"seconds\"  \"=\" integer )\n                   | ( \"hits\"      \"=\" integer )\n\n           action =  ( \"DIRECT\"\n                   | \"IPL\"\n                   | \"SET\" )\n                   ) \";\"\n\n           scopePattern = \"*\" | \"-\" | URIpattern\n\n           URIpattern = #( character | \"*\" )\n\n           character = Any character legal in the definition\n                       of a URL/URI in the context of RFC2068\n\n   An example header:\n       Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\",\n           scope=\"http://\", seconds=5\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 3]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n action\n\n   The first item, \"action\" specifies the type or mode of the change.\n   Possible modes are:\n\n\n   DIRECT\n    Attempt to connect directly, with no proxy\n\n\n   IPL\n    Initial Program Load, the client or proxy should attempt to revert\n    back to its default or initial proxy setting.  This is meant to\n    instruct a client to re-fetch its proxy configuration, or PAC file.\n    When set, the accompanying scope field MUST be \"*\" A client receiv-\n    ing this response SHOULD prompt the user for confirmation.\n\n\n    If accompanied by a 'proxyURI' parameter, a proxy or client MAY use\n    the value as a URL containing a configuration to retrieve.  If a\n    client  does so, it MUST prompt the user for confirmation.\n\n\n   SET\n    Set to parameter \"proxyURI\".  The client should use the URL speci-\n    fied for \"proxyURI\" as the proxy.  If the SET mode is specified, the\n    parameter, \"proxyURI\", MUST be present.\n\n Scope\n\n    Scope refers to an expression pattern that specifies which URIs are\n    subject to this header setting.  URIs should be matched against the\n    scope with this rule :\n\n     The scope \"*\" means all requests\n     The scope \"-\" means this EXACT URL ONLY\n\n    Otherwise, the URL is compared with the scope in the following\n    manner.\n\n    The Scope is a prefix of matching URLs.\n\n    The character \"*\" is allowed in the dns name portion of a URL, or in\n    the path portion of the URL, but ONLY when used with a 306, not a\n    305.\n\n    It matches any sequence of characters except '/'.\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 4]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n    This is intended to be a simple matching scheme to allow a prefix\n    match to take place.\n\n    See the examples section in \"Operational Constraints\"\n\n    The lifetime parameter specifies how long the specified proxy should\n    be used.  If lifetime is specified as \"seconds\" then the proxy set-\n    ting remains in effect for 'integer' seconds.  If lifetime is speci-\n    fied in 'hits' then the proxy setting remains in effect for\n    'integer' transactions.\n\n 2.2 Location Header\n\n\n    In the original HTTP/1.1 spec, the 'Location' header was used to\n    indicate the proxy setting.  Its use is DEPRECATED by the 'Set-\n    proxy' header in the context of a 305 response. All new implementa-\n    tions MUST send the Set-proxy header.  Implementations MAY send the\n    'Location' header so as to allow backward compatibility.\n\n\n    If the 'Location' header is specified, it should contain a URI of\n    the proxy.  If the Set-proxy header is not specified, the client\n    should use this proxy for just one request, and only for the origi-\n    nally requested exact URL.\n\n 3.0 Methods\n\n\n    A client or proxy receiving a 305 or 306, should use the OPTIONS\n    method to determine if the server or proxy it is talking to actually\n    is an HTTP/1.1 server supporting 305 and 306 responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 5]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n4.0 Operational Constraints\n\n\n   * Both the 305 and 306 response codes are HOP by HOP.  A proxy server\n     MUST not forward a 305 or 306 respose code (unless it generated the\n     306).\n\n\n   * A webserver MUST NOT send a 306 response under any circumstances\n\n\n   * A proxy server MUST NOT generate a 305 response.\n\n\n   * A client or proxy SHOULD NOT accept a 306 from a proxy that it\n     learned of via a 305 response code.\n\n\n   * A client or proxy MAY maintain state and allow a lifetime to extend\n     beyond a session or restart.\n\n\n   * A 'Set-proxy: IPL' SHOULD override any previous 'Set-proxy' header.\n\n\n   * A 305 or 306 response MAY contain a body containing an explanation\n     of the redirect for clients which do not understand the redirect\n\n\n   * In the absence of any parameter, the following defaults should be\n     used:\n\n       lifetime = this transaction only\n       scope = this exact URL only\n\n\n   * When receiving a 305 response, the client or proxy will enforce the\n     following rule with respect to the scope.\n\n     The scope specified must be more restrictive than the transformed\n     URL in question based on the rightmost slash in the URI.\n\n     Example: (in order of restrictiveness)\n       for URI = http://www.ups.com/services/index.html\n\n       http://www.ups.com/services/  (allowed)\n       http://www.ups.com/services/express/ ( allowed )\n       http://www.ups.com/ (NOT allowed)\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 6]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n     Using \"*\" in a 306 response set-proxy: header:\n\n     The scope may be set to:\n         http://*.foo.com/\n         which would apply to all URLs in to domain foo.com\n\n\n\n     If the scope returned with a 305 response is less restrictive than\n     the requested URL, the client may reject the redirection and return\n     506 Redirection Failed.  If the client wished to honor the\n     redirect, it client MUST prompt the user for confirmation before\n     accepting the new proxy setting.\n\n\n   * Since HTTP/1.0 proxies may unknowingly forward a 305 or 306\n     response code that was generated maliciously or in good faith, the\n     client must attempt to ascertain if the proxy with which it is\n     directly communicating is HTTP/1.1 and if it supports the 'Set-\n     proxy' header.  To determine this, the client or proxy should use\n     the OPTIONS method to make a request check for this feature.  The\n     extension string should be 'set-proxy' in the OPTIONS request.\n\nSecurity Considerations\n\n     Great care should be taken when implementing client side actions\n     based on the 305 or 306.  Since older proxies may unknowingly for-\n     ward either of these reponses, clients should be prepared to check\n     the validity.\n\n\n   * Please read the section 'Operational Constraints'\n\n\n   * A client or proxy MUST NOT accept a 305 response from a proxy.\n\n\n   * A client or proxy MUST NOT accept a 306 response from an origin\n     server.\n\n\n   * When receiving a 306 response from a proxy, the client MUST verify\n     that the proxy supports the 306 response with an OPTIONS request.\n\n5.0 Notes\n\n     Further specification is needed to define exactly how to use\n     OPTIONSs, or another mechanism to determin if set-proxy is\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 7]\n\n\n\n\n\nINTERNET-DRAFT                                           5 December 1996\n\n\n     supported.\n\nAuthor's Address\n\n     Josh Cohen\n     Netscape Communications Corporation\n     501 E. Middlefield Rd\n     Mountain View, CA 94043\n\n     Phone (415) 937-4157\n     EMail: josh@netscape.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 8]\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "Jeffrey Mogul:\n>\n[...]\n>  (a)Accept-Encoding: gzip, compress, no-identity\n>/* an explicit \"no identity-encoding wanted\" token */\n\nI like (a) best.  The trouble with adding q values to this header is\nthat it makes selecting the `best' encoding much more complicated\n(decoding short floats and finding the highest one is too complicated\nto do in a simple shell script, for example), and this would\ndiscourage the deployment of servers which know about encodings.\n\nI think that the knowledge that `gzip is better than compress is\nbetter than identity' can just as well be implemented at the server\nside, and implementing it there will be much cheaper.\n\nAnother reason I like (a) is that there won't be any compatibility\nproblems with existing servers.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": ">>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> The CONTENT-ENCODING issue:\nJM> http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CONTENT-ENCODING\n\nJM> has been assigned to myself and Henrik for resolution.  We're pretty\nJM> close to solving most of it, except for a seemingly minor concern:\nJM> How does a client say \"don't send me the 'identity' encoding\"?\n\n  I don't think this is a serious enough concern to merit doing\n  anything at all.  If this is not possible that's just fine.\n\n  That having been said, the q-value solution would seem the best to\n  me.  I prefer not to use '*' for this, so my choice from your\n  alternatives would be:\n\nJM>   (c)Accept-Encoding: gzip, compress, identity;q=0.0\n\n  Our server doesn't currently do anything with Accept-Encoding (our\n  customers generally don't have the storage to devote to storing\n  multiple encodings, and don't want the CPU spending time creating\n  them) so we have no backward compatibility issue.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "In a previous episode Koen Holtman said...\n:: \n:: Jeffrey Mogul:\n:: >\n:: [...]\n:: >  (a)Accept-Encoding: gzip, compress, no-identity\n:: >/* an explicit \"no identity-encoding wanted\" token */\n:: \n:: I like (a) best.  The trouble with adding q values to this header is\n:: that it makes selecting the `best' encoding much more complicated\n:: (decoding short floats and finding the highest one is too complicated\n:: to do in a simple shell script, for example), and this would\n:: discourage the deployment of servers which know about encodings.\n\n\nI find this a very persuasive argument.. Content Encodings are\nsomething we are intending to deal with aggressively in current and\nupcoming server extension projects.. (CGI, fast CGI, nsapi,\nwhatever..) If this is the type of functionality that's going to be\nhandled by a lot of different applications (as opposed to a handful of\nbase web servers) simplicity is paramount... that unfortunately means\nno q values.\n\n-P\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "On Tue, 22 Jul 1997, Patrick McManus wrote:\n\n> In a previous episode Koen Holtman said...\n> :: \n> :: Jeffrey Mogul:\n> :: >\n> :: [...]\n> :: >  (a)Accept-Encoding: gzip, compress, no-identity\n> :: >/* an explicit \"no identity-encoding wanted\" token */\n> :: \n> :: I like (a) best.  The trouble with adding q values to this header is\n> :: that it makes selecting the `best' encoding much more complicated\n> :: (decoding short floats and finding the highest one is too complicated\n> :: to do in a simple shell script, for example), and this would\n> :: discourage the deployment of servers which know about encodings.\n> \n> \n> I find this a very persuasive argument.. Content Encodings are\n> something we are intending to deal with aggressively in current and\n> upcoming server extension projects.. (CGI, fast CGI, nsapi,\n> whatever..) If this is the type of functionality that's going to be\n> handled by a lot of different applications (as opposed to a handful of\n> base web servers) simplicity is paramount... that unfortunately means\n> no q values.\n> \n\nI also vote for no q-values.  Perhaps identity/no-identity could be\nshortened to id/no-id.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "JM> The CONTENT-ENCODING issue:\nJM> http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CONTENT-ENCODING\n\nJM> has been assigned to myself and Henrik for resolution.  We're pretty\nJM> close to solving most of it, except for a seemingly minor concern:\nJM> How does a client say \"don't send me the 'identity' encoding\"?\n\n  On further reflection, I really think that we should just not\n  provide this capability at all.  The server should always be free to\n  just send the resource as is in response to the request.  If the\n  server has or can create a version of the resource in one of the\n  acceptable encodings, it should send that, but I think that the\n  implicit 'identity' coding (sent with no Content-Encoding header)\n  should always be an acceptable response.\n\n  If there is an Accept-Encoding header in the request, it should be\n  interpreted as meaning that the listed encodings are acceptable in\n  addition to the implicit 'identity'.\n\n  I can't quite puzzle out from the reference on the issues list what\n  the problem with this approach is.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "I'm in favor of keeping it simple and just having no way\nof saying \"don't send me the identity encoding\".\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Recently there was discussion between Dave Morris and me about whether\nthe request-host needed to be a FQHN, about changes to the definition\nof domain-match, and about the treatment of the Domain= attribute in\nSet-Cookie2.  The gist of the discussion was:  \"Host names can be\nspecified in any form acceptable to the base HTTP protocol. That may be\nan IP address, an incomplete host name string, or a FQHN string which\nis the prefe[r]red form.\"  I've done some more thinking about it and have\nthe following observations.\n\n1) (Sect 2) Relax requirement that request-host be a FQHN.\n\nOkay.  request-host could be any name string.\n\n2) (Sect 2) Change domain-match algorithm.\n\nOkay.  A domain-matches B if the strings for A and B exactly match,\nwhether they are IP addresses, partial name strings, or FQHNs.\n\n3) (Sect 4.3.2) Allow a Domain= value not to start with '.' and force a\n'.' in front of any such value.\n\nNo.  Suppose your local network has a host \"com\".  This transaction ensues:\n\nC -> S\nGET /cgi-bin/foo HTTP/1.1\nHost: com\n[other stuff]\n\nS -> C\nHTTP/1.1 200 OK\nSet-Cookie: foo=bar; Domain=com\nSet-Cookie2: Version=\"1\"\n\nThe proposed rule would have two undesirable effects, because the domain for\nthe cookie would become \".com\":\n\na) You could not return cookie \"foo\" to the server you got it from,\nbecause \"com\" does not domain-match \".com\".\n\nb) Cookie \"foo\" would get sent to every server of the form \"*.com\".\n\n\nThe effect of doing 1 and 2 (and not 3) above would be:\n\n1) You could use incomplete domain names (need not be FQHN) in Domain=.\n2) The rule about the implicit value for Domain= would remain, namely that\na cookie sent in a Set-Cookie[2] that had no Domain= attribute could only\nbe returned to the server from which it came.\n\nQuestion to Dave Morris:\nWould those changes meet your needs?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "ID on RFC 2069 issue: digest reques",
            "content": "  A New Internet-Draft is available from the on-line Internet-Drafts\n  directories.\n\n         Title     : HTTP/1.1 Message Digest Attribute Request\n         Author(s) : S. Lawrence\n         Filename  : draft-lawrence-digest-request-00.txt\n         Pages     : 3\n         Date      : 07/14/1997\n\n  This memo describes a security weakness in the current Proposed Standard\n  for HTTP Digest Access Authentication, and proposes a change to that scheme\n  to correct the deficiency.  The problem is that there is no mechanism for\n  either party to indicate a requirement that messages be sent with an\n  authentication digest.\n\n  ================\n\n  Since as I understand it, the plan is to issue a new document for\n  HTTP Authentication which would supersede RFC 2069, I would like to\n  get this considered as an improvement to Digest Access\n  Authentication as a part of that.  This is a problem with\n  interoperability found as a result of implementation experience, and\n  as such, I believe, proper for consideration at this point in the\n  process.\n\n  Since it isn't very long, I've attached it below.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\nInternet Draft                                           Scott Lawrence\ndraft-lawrence-digest-request-00.txt              Agranat Systems, Inc.\nExpires: December 1997                                    July 14, 1997\n\n\n             HTTP/1.1 Message Digest Attribute Request\n\nStatus of this Memo\n\n     This document is an Internet-Draft.  Internet-Drafts are working\n     documents of the Internet Engineering Task Force (IETF), its\n     areas, and its working groups.  Note that other groups may also\n     distribute working documents as Internet-Drafts.\n\n     Internet-Drafts are draft documents valid for a maximum of six\n     months and may be updated, replaced, or obsoleted by other\n     documents at any time.  It is inappropriate to use Internet-\n     Drafts as reference material or to cite them other than as\n     ``work in progress.''\n\n     To learn the current status of any Internet-Draft, please check\n     the ``1id-abstracts.txt'' listing contained in the Internet-\n     Drafts Shadow Directories on ftp.is.co.za (Africa),\n     nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n     ds.internic.net (US East Coast), or ftp.isi.edu (US West Coast).\n\n1. Abstract\n\n   This memo describes a security weakness in the current Proposed\n   Standard for HTTP Digest Access Authentication, and proposes a\n   change to that scheme to correct the deficiency.  The problem is\n   that there is no mechanism for either party to indicate a\n   requirement that messages be sent with an authentication digest.\n\n\ndraft-lawrence-digest-request-00.txt                            Page 2/4\n\n2. The Problem\n\n   The Digest Authentication scheme specifies a mechanism (the\n   'digest' attribute of the Authentication-Info and Authorization\n   header fields) by which a digest of the message body and selected\n   headers may be transmitted.  This provides a valuable means of\n   protecting the message body from modification or replay attacks\n   based on modifying either the message body or the protected\n   headers, while preserving the authentication headers.\n\n   The mechanism is only valuable, however, if the message recipient\n   can require that the digest attribute is present, but at present\n   the authentication scheme does not provide a means to indicate that\n   the digest is required.  There are two difficulties created by this\n   lack, an interoperability problem and a security problem:\n\n   - The interoperability problem is that if one party (either client\n     or server) wishes to require a digest for a particular HTTP\n     request or response, the other party cannot be told of the\n     requirement.  This leaves us with the situation that in order for\n     such a requirement to work that all messages using the Digest\n     Authentication scheme would need to use the message digest, even\n     those for which it would not be required.  Because computing the\n     digest is relatively expensive, this is undesirable.\n\n   - The security problem is that an attacker can remove the\n     attribute, preserving the remainder of the authentication\n     information, and modify the parts of the message the digest was\n     meant to protect.\n\n   For example, a server implementation might provide for a resource\n   attribute to require that Digest Authentication be used and a\n   message digest supplied in order to submit a form.  This would be\n   used to ensure that forms could be defined for which the flimsy\n   protections of Basic authentication are not appropriate.  The\n   requirement that the Digest Authentication scheme be used can be\n   communicated to the client by sending a WWW-Authenticate header\n   with 'digest' as the only acceptable scheme, but no mechanism is\n   provided to require the message digest.\n\n\ndraft-lawrence-digest-request-00.txt                            Page 3/4\n\n3. Solution\n\n   Attributes should be added to the WWW-Authenticate and\n   Authorization headers to indicate that a message digest is required\n   on the subsequent message.  The section numbers below refer to [RFC\n   2069].\n\n   Note that this draft does not propose that support for Digest\n   Access Authentication become a requirement for HTTP/1.1\n   conformance.  It does add a requirement to the definition of the\n   scheme if it is supported at all.\n\n   ================================================================\n   in section 2.1.1:\n\n     WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n                              digest-challenge\n\n     digest-challenge    = 1#( realm | [ domain ] | nonce |\n                          [ opaque ] |[ stale ] | [ algorithm ] |\n                          [ digest-required ] )\n   ...\n     digest-required     = \"digest-required\"\n   ...\n\n   digest-required\n   A flag, indicating that any request for the resource to which this\n   response applies must include the 'digest' attribute in its\n   Authorization header.\n\n   ================================================================\n   in section 2.1.2:\n\n   Authorization       = \"Authorization\" \":\" \"Digest\" digest-response\n\n   digest-response     = 1#( username | realm | nonce | digest-uri |\n                            response | [ digest ] | [ algorithm ] |\n                            opaque | digest-required )\n\n   ...\n     digest-required     = \"digest-required\"\n   ...\n\n   digest-required\n   A flag, indicating that the response to this request must include\n   the 'digest' attribute in its Authentication-Info header.\n\n   ================================================================\n\n   in section 3.3, paragraph 4:\n\n   The discussion of attacks based on removing the \"digest\" field of\n   the Authentication-Info header can be removed; the remainder is\n   still correct.\n\n\ndraft-lawrence-digest-request-00.txt                             Page 4/4\n\n4. Security Considerations\n\n   This entire draft is about security considerations.\n\n5. Author's Addresses\n\n   Scott Lawrence\n      Agranat Systems, Inc.\n      1345 Main St.\n      Waltham, MA 02154\n   Phone:  +1-617-893-7868\n   Fax:    +1-617-893-5740\n   Email:  lawrence@agranat.com\n\n6. References\n\n   [RFC 2068]\n       R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee.\n       \"Hypertext Transfer Protocol -- HTTP/1.1.\"\n       RFC 2068,\n       U.C. Irvine, DEC, MIT/LCS,\n       January 1997.\n\n   [RFC 2069]\n       J. Franks, P. Hallam-Baker, J. Hostetler, P. Leach,\n       A. Luotonen, E. Sink, and L. Stewart.\n       \"An Extension to HTTP : Digest Access Authentication\"\n       RFC 2069,\n       Northwestern University, CERN, Spyglass Inc., Microsoft Corp.,\n       Netscape Communications Corp., Spyglass Inc., Open Market Inc.,\n       January 1997.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Dave Morris and others have pretty consistently supported the inclusion\nof a CommentURL attribute in Set-Cookie2.  I was in the process of\nediting that capability in for the next draft when I ran into the\nfollowing puzzle:  how to express the general idea that no cookies\nshould be sent or received during the inspection process.\n\nHere's an illustration of the problem.  I send a request to foo.com and\nget back a cookie that contains\nCommentURL=\"http://foo.com/cookie-policy.html\".  I'm given the option\nto inspect that CommentURL, so I do so.  The HTML could potentially\nhave images in it, even links to images on advertising networks.  It\ncould also have links to other pages on foo.com.  If I follow those\nlinks (all while supposedly inspecting the cookie policy), I get deeper\nand deeper into the site.  All the while cookie handling should be\ndisabled, right?  How does it get re-enabled?\n\nDoes this wording express it adequately?:\n\nIf the user agent allows the user to follow the [CommentURL] link [as\npart of a cookie inspection user interface], it should neither send nor\naccept a cookie until the user has completed the inspection.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n>Dave Morris and others have pretty consistently supported the inclusion\n>of a CommentURL attribute in Set-Cookie2.  I was in the process of\n>editing that capability in for the next draft when I ran into the\n>following puzzle:  how to express the general idea that no cookies\n>should be sent or received during the inspection process.\n>\n>Here's an illustration of the problem.  I send a request to foo.com and\n>get back a cookie that contains\n>CommentURL=\"http://foo.com/cookie-policy.html\".  I'm given the option\n>to inspect that CommentURL, so I do so.  The HTML could potentially\n>have images in it, even links to images on advertising networks.  It\n>could also have links to other pages on foo.com.  If I follow those\n>links (all while supposedly inspecting the cookie policy), I get deeper\n>and deeper into the site.  All the while cookie handling should be\n>disabled, right?  How does it get re-enabled?\n>\n>Does this wording express it adequately?:\n>\n>If the user agent allows the user to follow the [CommentURL] link [as\n>part of a cookie inspection user interface], it should neither send nor\n>accept a cookie until the user has completed the inspection.\n\nI don't see why you want to exclude sending of cookies when\na UA is acting on the commentURL, nor why you are conceptualizing it\nas only for a policy statement.  That's possible, but the PEP or\nPEP-like extension is a better way to assess a site's \"policy\".  One\nof the main reasons for wanting cookie support in UAs is that it's a\nsimple, \"here now\" way to maintain user preferences across documents\nat a site, and is complementary to the TCN/features mechanisms.  The\nhope is that the reply will include a statement about the purpose of\nthat particular cookie, and \"preference cookies\" should still be able\nto affect how the reply is structured.\n\nYou simply want to guard against the server trying to set new\ncookies via the reply to the commentURL request, and there too, you\nneed not exclude things like it expiring or modifying old cookies\nwhich the user accepted.  How about something like this:\n\nIf the user agent allows the user to follow the CommentURL\nlink as part of a cookie inspection user interface, the server\nshould not include any new cookies in the reply, and the UA\nshould allow the use to inspect the body of the reply before\nacting on any other commentURL links.\n\nIf this is a \"first contact\", or is consequent to the user having\nenabled formerly disabled cookie support, that degrades to the\nreply having no more than the cookie about which a comment is being\nsought.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": " \n\nOn Tue, 22 Jul 1997, Dave Kristol wrote:\n\n> Dave Morris and others have pretty consistently supported the inclusion\n> of a CommentURL attribute in Set-Cookie2.  I was in the process of\n> editing that capability in for the next draft when I ran into the\n> following puzzle:  how to express the general idea that no cookies\n> should be sent or received during the inspection process.\n> \n> Here's an illustration of the problem.  I send a request to foo.com and\n> get back a cookie that contains\n> CommentURL=\"http://foo.com/cookie-policy.html\".  I'm given the option\n> to inspect that CommentURL, so I do so.  The HTML could potentially\n> have images in it, even links to images on advertising networks.  It\n> could also have links to other pages on foo.com.  If I follow those\n> links (all while supposedly inspecting the cookie policy), I get deeper\n> and deeper into the site.  All the while cookie handling should be\n> disabled, right?  How does it get re-enabled?\n> \n> Does this wording express it adequately?:\n> \n> If the user agent allows the user to follow the [CommentURL] link [as\n> part of a cookie inspection user interface], it should neither send nor\n> accept a cookie until the user has completed the inspection.\n\nI believe that wording is safe but perhaps too conservative. I think the\nonly ambiguous case is if the\nUA provides access to the CommentURL while the user is being asked whether\nor not to accept a cookie. Once a cookie has been stored and the user\nis simply reviewing cookies already acquired I can't see any problem \nwith treating the CommentURL normally. I also don't see any conflict\nwith sending or receiving already approved cookies with the CommentURL\nrequest. With those arguments in mind, how about the alternative:\n   A potentially confusing situation exists if a user agent's cookie\n   inspection interface allows a user to follow a CommentURL link\n   within a dialog which is prompting the user to decide if the cookie\n   containing the CommentURL is acceptable AND following the CommentURL\n   link results in receipt of a new, not previously approved cookie.\n   The useragent MAY discard any cookie received in this context in order\n   to avoid the complexities of interacting with the user regarding nested\n   set-cookie requests.  Servers which depend on cookies MUST allow for\n   the possibility that URLs used in their cookie's CommentURL value\n   will be ignored by user agents.\n\nWhile much longer, this version is more specific to the case of a cookie\nreceived with the CommentURL response. It 'requires' the client to \ninclude appropriate cookies with the CommentURL request and allows the \nclient designer to provide a nest inspection interface if desired. It \nalso allows the client designer to safely avoid the issue by dropping\ncookies received or to accept previously approved cookies.\n\nOn the otherhand, the IETF definition of 'should' allows the same design\nchoices under the compelling reason argument so I can live with Dave's\nproposed wording.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "   From: koen@win.tue.nl (Koen Holtman)\n   Date: Tue, 22 Jul 1997 09:39:19 +0200 (MET DST)\n\n   Jeffrey Mogul:\n   >\n   [...]\n   >  (a)Accept-Encoding: gzip, compress, no-identity\n   >/* an explicit \"no identity-encoding wanted\" token */\n\n   I like (a) best.  The trouble with adding q values to this header is\n   that it makes selecting the `best' encoding much more complicated\n   (decoding short floats and finding the highest one is too complicated\n   to do in a simple shell script, for example), and this would\n   discourage the deployment of servers which know about encodings.\n\nI think it can be done in a shell script.  Noah Friedman is a sysadmin\nI know who is very good at writing shell scripts.  I'm sure he could\nwrite an adaquate script within a few hours.\n\nFurthurmore, I think there are a lot of deployed servers which know\nabout q values for content-types.  If the code can't be recycled,\nthen the people who wrote it don't know what they're doing.\n\n   I think that the knowledge that `gzip is better than compress is\n   better than identity' can just as well be implemented at the server\n   side, and implementing it there will be much cheaper.\n\nProbably true.\n\nHowever, there will be cases where identity is better than any\ncompressed setting, if you are on a very fast link, and the server\nor client has a relatively slow processor.  These are very few;\nfor a modem connection, even with a 386 I'd want compression.\n\nSo I think that encoding that knowlege in the server is reasonable.\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "> >If the user agent allows the user to follow the [CommentURL] link [as\n> >part of a cookie inspection user interface], it should neither send nor\n> >accept a cookie until the user has completed the inspection.\n\nI think that's absolutely correct.  It's the only way not to confuse\neither a script on the server side, or the state on the UA side.\n\n> I don't see why you want to exclude sending of cookies when\n> a UA is acting on the commentURL, nor why you are conceptualizing it\n> as only for a policy statement.  That's possible, but the PEP or\n> PEP-like extension is a better way to assess a site's \"policy\".  One\n> of the main reasons for wanting cookie support in UAs is that it's a\n> simple, \"here now\" way to maintain user preferences across documents\n> at a site, and is complementary to the TCN/features mechanisms.  The\n> hope is that the reply will include a statement about the purpose of\n> that particular cookie, and \"preference cookies\" should still be able\n> to affect how the reply is structured.\n\nThe reason for not sending or accepting cookies is that the whole\npurpose of the CommentURL is to allow the user to evaluate the\npros and cons of accepting a cookie.  If you offer another cookie\nwhile they're trying to decide if they should accept the first cookie,\nnow they potentially have to evaluate whether they want to accept the\nsecond cookie, presumably before they can even look at the information\nexplaining the first cookie.  Obviously, this could be a very annoying \nendless loop to nowhere.\n\nWe also don't want to appear to be advertising to the Server that there's\nan opportunity to change the state on this particular user while he\ngrabs the CommentURL.  Sorry for the wording here, but if the server\nexpects the client to accept the change on a cookie that IS already being\nused, and the user agent doesn't accept the change because it's retrieving \na CommentURL, the applications on the server side may become confused.\n\n> You simply want to guard against the server trying to set new\n> cookies via the reply to the commentURL request, and there too, you\n> need not exclude things like it expiring or modifying old cookies\n> which the user accepted.  How about something like this:\n> \n> If the user agent allows the user to follow the CommentURL\n> link as part of a cookie inspection user interface, the server\n> should not include any new cookies in the reply, and the UA\n> should allow the use to inspect the body of the reply before\n> acting on any other commentURL links.\n\nYou can't put the responsibility for this on the server.  It must be\nin the client.  At the very least, the client should ignore any \"new\"\ncookies.  I think, however, it best to not accept any cookie actions\nwhile getting the commentURL document.  Some scripts may not react \nwell to having a cookie expire, and having another cookie that was \nissued in the same request not get set.  The \"correct\" action would be \nto feign ignorance of cookies and do nothing to the state of the UA \nuntil a decision is made on accepting or refusing the original cookie \nin question.  As stated in the original message, the UA should also\nnot send any cookie information when retrieving the CommentURL.\n\n> If this is a \"first contact\", or is consequent to the user having\n> enabled formerly disabled cookie support, that degrades to the\n> reply having no more than the cookie about which a comment is being\n> sought.\n> \n> Fote\n\nouch... parse error line 3...  sorry... The reply to what?  To the request for\nthe CommentURL?  The UA just needs to know if it should or\nshould not accept a cookie that's been sent to it.  The CommentURL allows\nthe user to make an informed decision about whether they should or\nshould not accept the cookie.  The process of attaining this \"decision\nmaking information\" should be \"sacred\"...  without cookies or sessions\nor anything... it should be anonymous, as though the browser does not\nhave support for cookies, and it should not be something that will result\nin any cookies being accepted, rejected, or changed in any way, except\nfor the one cookie that is in question, and the point of the whole process.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "David Morris wrote:\n\n> > If the user agent allows the user to follow the [CommentURL] link [as\n> > part of a cookie inspection user interface], it should neither send nor\n> > accept a cookie until the user has completed the inspection.\n> \n> I believe that wording is safe but perhaps too conservative. I think the\n> only ambiguous case is if the\n> UA provides access to the CommentURL while the user is being asked whether\n> or not to accept a cookie. Once a cookie has been stored and the user\n> is simply reviewing cookies already acquired I can't see any problem \n> with treating the CommentURL normally. I also don't see any conflict\n> with sending or receiving already approved cookies with the CommentURL\n> request. With those arguments in mind, how about the alternative:\n\nI think there are potential problems with scripts trying to change\nexisting, already \"accepted\" cookies, or expiring them, but I think\nyou very gracefully address these issues in your wording below.\nLooks good.\n\n>    A potentially confusing situation exists if a user agent's cookie\n>    inspection interface allows a user to follow a CommentURL link\n>    within a dialog which is prompting the user to decide if the cookie\n>    containing the CommentURL is acceptable AND following the CommentURL\n>    link results in receipt of a new, not previously approved cookie.\n>    The useragent MAY discard any cookie received in this context in order\n>    to avoid the complexities of interacting with the user regarding nested\n>    set-cookie requests.  Servers which depend on cookies MUST allow for\n>    the possibility that URLs used in their cookie's CommentURL value\n>    will be ignored by user agents.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Tue, 22 Jul 1997, Foteos Macrides wrote:\n\n> I don't see why you want to exclude sending of cookies when\n> a UA is acting on the commentURL, nor why you are conceptualizing it\n> as only for a policy statement.  That's possible, but the PEP or\n> PEP-like extension is a better way to assess a site's \"policy\".  One\n> of the main reasons for wanting cookie support in UAs is that it's a\n> simple, \"here now\" way to maintain user preferences across documents\n> at a site, and is complementary to the TCN/features mechanisms.  The\n\nI think a prior post of mine would indicate agreement with your concern\nbut I'm not sure I would accept as a 'main reason' for cookie support\nbeing the storage of preferences. Preferences are, IMHO, a minor \nconcern and a particular type of 'session' which cookies can be used\nto maintain. But all this really says is that cookies have tremendous\nappeal because they are a general facility which can be used in many \nways, each of which would probably be more effectively solved some \nother way.  They also enhance cachability of the web by reducing \nthe need for munged URLs, etc.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Roy Fielding's comments on draft-ietf-http-hit-metering0",
            "content": "Although draft-ietf-http-hit-metering-03 has moved into IESG\n\"last call\", it is still useful to ask, once more, if there\nare any additional working group comments on the draft in support\nof the points raised by Roy Fielding in\n<mid:9707162253.aa22535@paris.ics.uci.edu>.\n\nWhile there's been some concern that those comments were \"late\",\nI don't think that's the issue: if there is working group support\nfor considering them, we should consider them, no matter when they\nare offered. In any case, it is certainly better to raise issues\nbefore the RFC gets issued than afterward.\n\nHere is my analysis of the points raised, as working group chair\nI'd like to ask Roy to withhold continuing\nto argue the points just so that we can see if there is any\nother support for any of them from anyone else in the mailing list;\nif not, we can go on, without additional discussion:\n\nIssue 1:\n\n> It may also be that, by placing this proposal on the standards track,\n> proxy implementers will be forced, for the sake of efficiency or lack\n> of network resources, to add the Meter field to every request regardless\n> of their intention to pass along metering information. \n\nThis point is hypothetical, expressed as an \"It may also be that ...\".\nCreating a \"Proposed Standard\" which has the approval of the\nimplementors\nof proxies doesn't seem to force them to do anything they didn't\nwant to do by dint of supporting the protocol. So I don't think\nthis particular objection carries any weight, at least in the \nform it is expressed. It is our belief that proxy implementors\nDO intend to pass along metering information, and if they do\nnot, then this proposed standard will not progress to draft\nstandard.\n\n> It is NOT the \"Meter header\".  The header of a message is the sum of all\n> of the header fields.  It should be referred to as the \"Meter field\" or\n> \"Meter header field\" or \"Meter request-header\" (the latter is a BNF rule).\n\nThis objection is one of terminology and seems like a quibble;\ncertainly I don't think the draft is misleading even without\nfixing this particular nit.\n\n> The proposal places restrictions on proxies in order to restrict the\n> actions of shared caches.  The presence of a non-caching proxy in the\n> chain will break the metering tree, even though a non-caching proxy\n> has no impact on metering itself.  Section 5.5 (which should be referred\n> to by earlier sections) suggests that such proxies always add the Meter\n> field, thus increasing network overhead merely to sustain the possibility\n> of the metering tree.\n\nThis objection indicates that the document could use an additional\ncross reference;  the main point, though, seems to be based on a\nproblem which occurs in fairly rare circumstances (with a non-caching\nproxy in a meter tree). In any case, it is my reading that the\nweight of the working group does not believe this is an issue.\n\n> In other words, it is a design that forces the proxy (with or without\n> cache) to advertize metering support, rather than using the existing\n> extensibility of the Cache-Control field to allow the origin server\n> to state its requirements for metering explicitly.  The alternative\n> design that is not mentioned is to introduce a metering cache-directive\n> that modifies the semantics of the \"private\" or \"s-maxage\" directives, e.g.\n\nThis comment seems to indicate a preference for an alternative design,\nbut does not, by itself, render the proposal's design inoperable.\nAs was recounted, the alternative design has its own drawbacks.\n\n> Then don't define any non-abbreviated form!  This is a network protocol,\n> not a user-readable treatise.  Long header field names are often necessary\n> to avoid conflicts with other names (and protocols); long field values\n> are never useful unless they contain data intended to be read by a\n> human being, which is clearly not the case here.\n\nThis expresses a desire for a simplification of the design. However,\nthe design can be simplified if implementation experience warrents it.\nAs is, there doesn't seem to be a strong opinion that we should change\nthe proposal just because we can.\n\n> When the proposal is ready for publication as an RFC, it should be\n> assigned Experimental status until such time as the overhead placed on\n> proxies is *demonstrated* to be offset by the benefit gained by caching\n> responses that would otherwise have been busted for the sole purpose of\n> gathering hit counts.  In particular, no Internet proxy should activate an\n> implementation of this proposal until there exist some deployed origin\n> server implementations.\n\nIt is believed that the proposal _is_ ready for publication as an RFC,\nand that it is ready now, and that it is ready for Proposed Standard.\n\nAdvice about deployment strategy seems to be consistent with a Proposed\nStandard as well as an Experimental RFC. It may well be the case that\ndeployment of origin servers should be staged before proxy servers will\nprovide value, but given how fast Internet software and web browsers and\nproxies seem to be deployed, trying to control this by changing the\nstatus\nof the publications of RFCs doesn't seem to be appropriate.\n\nIf anyone on the mailing list disagrees and wishes to defend any of the\npoints raised in mid:9707162253.aa22535@paris.ics.uci.edu, will you\nplease\nspeak up now?\n\nIf you would prefer to contact me privately, I will make sure that your\nvoice gets heard.\n\nThanks,\n\nLarry\n(as HTTP working group chair)\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "I realize that some people were opposed to the introduction of qvalues\non Accept-Encoding.  However, during today's editorial-group discussion\nof the CONTENT-ENCODING issue, we realized that the syntax for the\nvarious Accept-* request headers in RFC2068 is almost, but not quite,\nuniform, and we reached a tentative agreement that it might be a good\nidea to have all of the Accept-* request headers (Accept-Range is a\nresponse header) have similar syntaxes.\n\nAnyway, here are the BNFs in question, from RFC2068:\n\n    14.1 Accept\n\n          Accept         = \"Accept\" \":\"\n                           #( media-range [ accept-params ] )\n\n          media-range    = ( \"*/*\"\n                           | ( type \"/\" \"*\" )\n                           | ( type \"/\" subtype )\n                           ) *( \";\" parameter )\n\n          accept-params  = \";\" \"q\" \"=\" qvalue *( accept-extension )\n\n          accept-extension = \";\" token [ \"=\" ( token | quoted-string ) ]\n\n    14.2 Accept-Charset\n\n          Accept-Charset = \"Accept-Charset\" \":\"\n                    1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\n    14.3 Accept-Encoding\n\n          Accept-Encoding  = \"Accept-Encoding\" \":\"\n                                    #( content-coding )\n\n    14.4 Accept-Language\n\n          Accept-Language = \"Accept-Language\" \":\"\n                            1#( language-range [ \";\" \"q\" \"=\" qvalue ] )\n\n          language-range  = ( ( 1*8ALPHA *( \"-\" 1*8ALPHA ) ) | \"*\" )\n\nThe CHARSET-WILDCARD issue, already declared \"closed\", changes the BNF\nfor 14.2 (Accept-Charset) to include \"*\", based on\n    ftp://ds.internic.net/internet-drafts/draft-holtman-http-wildcards-00.txt\nso that the BNF there is now:\n\n          Accept-Charset = \"Accept-Charset\" \":\"\n                    1#( charset [ \";\" \"q\" \"=\" qvalue ] )\n\nHenrik's proposed solution for CONTENT-ENCODING, at\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0038.html\nwould modify the BNF for 14.3 (Accept-Encoding) to be\n\n       Accept-Encoding  = \"Accept-Encoding\" \":\" \n                                 #( codings )\n        codings          = ( content-codings | \"*\" )\n\nAs you can see, there are only two non-uniformities remanining\namong these four sections:\n(1) The Accept header allows extension parameters, not\njust qvalues.\n(2) The Accept-Encoding header does not allow qvalues.\n\nI'll also note that RFC1945, while not defining qvalues, does say:\n\n    D.2.3 Accept-Encoding\n    \n       The Accept-Encoding request-header field is similar to Accept, but\n       restricts the content-coding values which are acceptable in the\n       response.\n\nI.e., there seems to have been an intent that Accept-Encoding\nbe similar in form to Accept, and (perhaps) that the omission\nof a qvalue here was simply an oversight.  (Note that RFC1945\nexplicitly allows \"*/*\" in the Accept header, so it implicitly\nallows \"*\" in the Accept-Encoding header, and so I think we\nneed to include \"*\" in the Accept-Encoding header simply for\ncompatibility with RFC1945.)\n\nAs I said in my previous message, introducing qvalues for\nAccept-Encoding won't work \"if any existing servers or proxies would\nchoke on a qvalue in an Accept-Encoding header.\"\nBut (so far) nobody has asserted than this is an actual problem.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: OPTIONS Spe",
            "content": ">>>>> \"JC\" == Josh Cohen <josh@netscape.com> writes:\n\nJC>  The actual OPTIONS message is contained in the request/response\nJC>  body.\n\n  What about backward compatibility with RFC 2068?  Our current\n  implementation doesn't expect or send any message body with the\n  OPTIONS method:\n\n   Request:                 Response:\n\n    OPTIONS * HTTP/1.1       HTTP/1.1 200 OK\n    Host: www.agranat.com    Date: Tue, 22 Jul 1997 20:14:06 GMT\n    Connection: close        Server: Agranat-EmWeb/R3_0alpha4\n    User-Agent: wwwreq/1.6   Connection: close\n                             Allow: HEAD, GET, POST, TRACE, OPTIONS\n\n  Re-reading the spec, it is possible that we should have used\n  'Public' rather than 'Allow' in that response.  I believe that our\n  thinking was that we wanted re result to get back through a proxy,\n  but I'm not sure.\n\n  Our response is consistent with Apache:\n\n    OPTIONS * HTTP/1.1       HTTP/1.1 200 OK\n    Host: www.apache.org     Date: Tue, 22 Jul 1997 20:21:51 GMT\n    Connection: close        Server: Apache/1.3-dev\n    User-Agent: wwwreq/1.6   Content-Length: 0\n                             Allow: GET, HEAD, OPTIONS, TRACE\n                             Connection: close\n\n  (hmm... apache doesn't list POST... perhaps it is interpeting that\n  as 'OPTIONS /')\n\n  ... but not quite what John Mallery's Common Lisp server does (it\n  uses the Public header) [I folded the 'Public:' for this mail]:\n\n    OPTIONS * HTTP/1.1       HTTP/1.1 200 OK\n    Host: wilson.ai.mit.edu  Date: Tue, 22 Jul 1997 20:25:37 GMT\n    Connection: close        Server: CL-HTTP/63.61 (Symbolics Common Lisp)\n    User-Agent: wwwreq/1.6   Connection: CLOSE\n                             Public: post, PUT, TRACE, OPTIONS,\n                                     DELETE, GET, HEAD\n                             Cache-Control: NO-CACHE\n\n  ... any others out there?\n\n  A section defining some variation on the above as the expected\n  behaviour if no body is sent (no Content-Length header) would do the\n  trick.  I just checked, and if there is a body, we just ignore it\n  and send the response above, so I guess that a client could detect\n  that a server like our current version just didn't know about this\n  when it got back no body.  I can't easily run that test on those\n  others...\n\n  On another note, is it your intention that the feature-token values\n  be specified as a part of whatever document defines the feature?  If\n  so, we should add feature-token values to the next HTTP/1.1 for the\n  optional features it specifies.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Another try at OPTION",
            "content": "This is the result of discussions during the editorial-group\nteleconference this morning.  As it happens, Scott Lawrence\nraised an overlapping set of issues in his message \"Re: OPTIONS Spec\"\ntoday (which has not yet shown up in the archive, so I can't\ncite the URL).  Josh and I redrafted his proposal to use HTTP\nheaders, rather than the entity body, to convey OPTIONS information.\nThis should be more compatible with RFC2068, although it's not\nclear than any deployed implementation is actually using OPTIONS\nto discover anything except the HTTP version number.\n\nWe also addressed the issue of managing the name space for\ndescribing options.  We believe that it is especially valuable\nto be able to use RFC numbers, rather than protocol version\nnumbers, to define compliance with a set of features, because\nof the many different interpretations of \"HTTP/1.1\" already\nin use.  That is, if an implementation says \"HTTP/1.1 200 OK\",\nthen you don't really know if this means:\n(1) compliance with RFC2068\n(2) compliance with the RFC defining the next edition of\nthe HTTP/1.1 spec\n(3) something else\nbut if the implementation says \"Compliance: rfc=2068;uncond\", then\nyou know what this means, because we have an immutable document\nthat is controlled by the IETF.\n\nOf course, RFC numbers are not the only way to define options,\nso there are other namespaces allowed, and we intend to register\nthe set of namespaces with IANA.\n\n-Jeff\n\nReference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#OPTIONS\n\nProblem statement:\nRFC2068 doesn't really say how to use OPTIONS to discover\nwhat an implementation supports.\n\nIt's essential to have a simple and reliable mechanism for\ndetecting support for extensions as a part of the core HTTP/1.1\nprotocol.\n\nOutline of proposed solution:\n \n    the URI '*' refers to the server, independent of any specific URI.\n   \n    The Host: header may be used to specify a named proxy or origin \n    server in a chained environment\n   \n    The Max-forwards: header may be used to specify a maximum number\n    of hops the request may be forwarded in a chained environment\n   \n    The Compliance: header may be used on requests to ask about\n    compliance, and on responses to assert compliance.  (This\n    is a change from Josh's previous proposal, in which the\n    message bodies are used to convey COMPLIANCE information.)\n\n    We define a new IANA-registered namespace for compliance\n    assertions.\n\nProposed Solution:\n\n(1) In section 5.1.2, remove this:\n\n   If a proxy receives a request without any path in the Request-URI and\n   the method specified is capable of supporting the asterisk form of\n   request, then the last proxy on the request chain MUST forward the\n   request with \"*\" as the final Request-URI. For example, the request\n\n          OPTIONS http://www.ics.uci.edu:8001 HTTP/1.1\n\n   would be forwarded by the proxy as\n\n          OPTIONS * HTTP/1.1\n          Host: www.ics.uci.edu:8001\n\n   after connecting to port 8001 of host \"www.ics.uci.edu\".\n\n(2) In section 9.2 (OPTIONS), replace:\n\n   Unless the server's response is an error, the response MUST NOT\n   include entity information other than what can be considered as\n   communication options (e.g., Allow is appropriate, but Content-Type\n   is not). Responses to this method are not cachable.\n\nwith\n\n   An OPTIONS request MAY include Compliance headers (see section 14.ZZZ)\n   that indicate the set of options the sender wants information\n   about.\n\n   Responses to OPTIONS are not cachable, unless caching is explicitly\n   allowed by the originating sender (see section 13.4).\n\n(3) In section 9.2 (OPTIONS), replace:\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server as a whole. A 200 response SHOULD\n   include any header fields which indicate optional features\n   implemented by the server (e.g., Public), including any extensions\n   not defined by this specification, in addition to any applicable\n   general or response-header fields. As described in section 5.1.2, an\n   \"OPTIONS *\" request can be applied through a proxy by specifying the\n   destination server in the Request-URI without any path information.\n\nwith\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server as a whole.  A 200 response SHOULD\n   include a Public header field (see section 14.35).  If the request\n   includes a Compliance header field, a 200 response SHOULD include a\n   Compliance header field, indicating the subset of the requested\n   Compliance options supported by the server as a whole.  The response\n   SHOULD include any other applicable general or response-header\n   fields.\n\n   If an OPTIONS request includes a Host header (see section 14.23),\n   this is the intended destination of the OPTIONS method.\n   Proxy servers MUST forward such a message until it reaches\n   the specified host.  If the specified host has more than\n   one `virtual server', the OPTIONS request applies to the\n   specified virtual server.\n   \n       Note: An OPTIONS request may also include a Max-Forwards header,\n       as described in section 14.31.  This allows the sender to select\n       the Nth proxy on a path, without knowing its hostname.\n\n(4) In section 9.2 (OPTIONS), replace:\n\n   If the Request-URI is not an asterisk, the OPTIONS request applies\n   only to the options that are available when communicating with that\n   resource.  A 200 response SHOULD include any header fields which\n   indicate optional features implemented by the server and applicable\n   to that resource (e.g., Allow), including any extensions not defined\n   by this specification, in addition to any applicable general or\n   response-header fields. If the OPTIONS request passes through a\n   proxy, the proxy MUST edit the response to exclude those options\n   which apply to a proxy's capabilities and which are known to be\n   unavailable through that proxy.\n\nwith\n\n   If the Request-URI is not an asterisk, the OPTIONS request applies\n   only to the options that are available when communicating with that\n   resource.  A 200 response SHOULD include an Allow header field (see\n   section 14.7).  If the request includes a Compliance header field, a\n   200 response SHOULD include a Compliance header field, indicating\n   the subset of the requested Compliance options supported by the\n   server as a whole.  If the subset is empty, the response SHOULD\n   include a Compliance header with an empty field-value.  The response\n   SHOULD include any other applicable general or response-header\n   fields.\n\n      Note: if an OPTION request contains a Compliance header, and the\n      response does not, the response may have been generated by\n      RFC2068-compliant implementation, which would not support\n      Compliance.  In this case, it is not possible to infer that the\n      sender fails to support all of the options listed in the\n      Compliance header of the request.\n\n   If an OPTIONS response passes through a proxy, the proxy MUST edit\n   the response to exclude those options that apply to a proxy's\n   capabilities and that are known to be unavailable through that\n   proxy.\n\n(5) unresolved issue: that last paragraph, essentially the same thing\nthat RFC2068 says about OPTIONS, directly contradicts this statement\nin 14.7 of RFC2068:\n\n   A proxy MUST NOT modify the Allow header field even if it does not\n   understand all the methods specified, since the user agent MAY have\n   other means of communicating with the origin server.\n\nHowever, the restriction does not apply to Public.  We need\nto resolve this inconsistency.\n\n(6) New section\n\n14.ZZZ Compliance\n\n    The Compliance general header field lists a set of options\n    that may or may not be supported by a server.  In a request\n    message, this header lists the set of options that a client\n    wishes to know about.  In a response message, this header\n    lists the set of options that the server complies with.\n    \n    A compliance header MAY appear on any message, but is\n    normally used with the OPTIONS request (see section 9.2).\n    \n    Compliance = \"Compliance\" \":\" (\"*\" | *(compliance-option))\n\ncompliance-option = compliance-namespace \"=\"\noption-item [ option-params ]\n\ncompliance-namespace = token\n\noption-item = token | quoted-string\n\noption-params = 1#( \";\" option-param)\n\noption-param = \"cond\" | \"uncond\" | token | quoted-string\n\n    A Compliance header field with the field-value of \"*\" MAY\n    be used in a request, to ask about all options complied\n    with by the recipient.  This field-value MUST NOT be used\n    in a response.\n\n    The compliance-namespace is used to select from one of several\n    namespaces for compliance options.  The option-item is used\n    to specify one or more options within a namespace.  \n\n    The Internet Assigned Numbers Authority (IANA) acts as a registry\n    for compliance-namespace tokens. Initially, the registry contains\n    the following tokens:\n\n\"RFC\"Compliance is with an RFC, specified by an RFC number.\nFor example, \"rfc=1945\".\n\n\"HDR\"Compliance is with a named HTTP header.  For example,\n\"HDR=Authorization\".  There is no IANA registry for\nHTTP header names, but to avoid potential namespace\nconfusion, only those HTTP headers listed in an\nIETF standards-track document should be used in\nthis namespace.\n\n\"PEP\"Compliance is with a PEP-specified extension, identified\nusing a quoted-string containing the PEP extension\ndeclaration.\n\n    The option-param is used to provide additional parameters.\n    Unconditional compliance with a compliance-option is indicated\n    using the \"uncond\" option-param; for example, \"rfc=1945;uncond\".\n    Conditional compliance is indicated using the \"cond\" option-param;\n    for example, \"HDR=Authorization;uncond\".  Additional option-param\n    values might be defined as part of another specification.\n\n    Examples:\n\nCompliance: rfc=2068;uncond\nCompliance: rfc=1945;uncond, rfc=2068;cond\nCompliance: rfc=2068, hdr=PEP, hdr=SetCookie2\nCompliance: rfc=9999999;uncond;\"onlyOn=Tuesdays\"\n\n\n(7) Examples (put this in 9.2.1?):\n\n  To list all extensions supported by proxy \"proxy4.microscape.com\"\n\n    Client sends:\nOPTIONS * HTTP/1.1\nHost: proxy4.microscape.com\nCompliance: *\n\n    proxy4.microscape.com responds:\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:51 GMT\nServer: SuperProxy/1.0\nPublic: OPTIONS, GET, HEAD, PUT, POST, TRACE\nCompliance: rfc=1543, rfc=2068, hdr=set-proxy\nCompliance: hdr=wonder-bar-http-widget-set\nCompliance: PEP=\"http://foobar.pep.org/pepmeister/\"\nContent-Length: 0\n\n[Editorial note: check syntax of PEP extensions]\n       \n  Probing for a feature which is not supported by \"proxy4.microscape.com\"\n\n    Client sends:\nOPTIONS * HTTP/1.1\nHost: proxy4.microscape.com\nCompliance: PEP=\"http://foobar.pep.org/evil-not-implemented\"\n\n    proxy4.microscape.com responds:\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:52 GMT\nServer: SuperProxy/1.0\nPublic: OPTIONS, GET, HEAD, PUT, POST, TRACE\nCompliance:\nContent-Length: 0\n\n  Piggybacking an options probe on a regular method:\n\n    Client sends:\nGET / HTTP/1.1\nHost: www.w3.org\nCompliance: rfc=9999,rfc=2068\n\n    www.w3.org replies\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:53 GMT\nLast-Modified: Wed, 22 Jul 1997 18:44:58 GMT\nServer: SuperServer/1.0\nAllow: OPTIONS, GET, HEAD, TRACE\nCompliance: rfc=2068;cond\nContent-Length: 7365\n\n<HTML> ... </HTML>\n\n[End of proposed changes]\n\n\n\n"
        },
        {
            "subject": "Re: 305/30",
            "content": "> RFC 2068 doesn't contain a regular expression grammar, so of course one\n> must be defined before regular expressions can be used in HTTP.  There\n> is one in RFC 2168 \"Resolution of Uniform Resource Identifiers using the\n> Domain Name System\", but the authors seem not to like it very much -\n> there's a note to the effect that regular expressions are complex and\n> error-prone, as well as rarely useful.\n\nWhile I may agree that regular expressions are more complex than some\nother types of wildcard pattern languages, regular expressions in\ngeneral are most certainly very useful for some Web applications.  As\nan example, Netscape Proxy moved to using regular expressions instead\nof shell expressions due to high number of customer requests.  Many\nURL patterns simply could not be expressed with shell expressions,\ne.g. a pattern that matches URLs without an FQHN.\n\nIt may well be that regular expressions are not necessary for the\nparticular application we're discussing here, but that decision should\nbe based purely on the application's requirements, not subjective\nstatements in other RFCs.\n\nThe question to pose is, can the 305/306 scope rules be expressed\nusing the single wildcard '*' as opposed to full regular expressions.\nThe purist answer is 'no'; you may want to specify a proxy redirection\nfor non-FQHN URLs.  However, if this seems like an unlikely\napplication, and a viable trade against simplicity, then yes, maybe\n'*' is sufficient.\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://home.netscape.com/people/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Jonathan Stark <stark@commerce.net> wrote:\n>[...]\n>The reason for not sending or accepting cookies is that the whole\n>purpose of the CommentURL is to allow the user to evaluate the\n>pros and cons of accepting a cookie.  If you offer another cookie\n>while they're trying to decide if they should accept the first cookie,\n>now they potentially have to evaluate whether they want to accept the\n>second cookie, presumably before they can even look at the information\n>explaining the first cookie.  Obviously, this could be a very annoying \n>endless loop to nowhere.\n>\n>We also don't want to appear to be advertising to the Server that there's\n>an opportunity to change the state on this particular user while he\n>grabs the CommentURL.  Sorry for the wording here, but if the server\n>expects the client to accept the change on a cookie that IS already being\n>used, and the user agent doesn't accept the change because it's retrieving \n>a CommentURL, the applications on the server side may become confused.\n>\n>> You simply want to guard against the server trying to set new\n  |||\n>> cookies via the reply to the commentURL request, and there too, you\n   |||||||\n>> need not exclude things like it expiring or modifying old cookies\n>> which the user accepted.  How about something like this:\n>> \n>> If the user agent allows the user to follow the CommentURL\n>> link as part of a cookie inspection user interface, the server\n>> should not include any new cookies in the reply, and the UA\n>> should allow the use to inspect the body of the reply before\n>> acting on any other commentURL links.\n>\n>You can't put the responsibility for this on the server.  It must be\n>in the client.  At the very least, the client should ignore any \"new\"\n>cookies.  I think, however, it best to not accept any cookie actions\n>while getting the commentURL document.  Some scripts may not react \n>well to having a cookie expire, and having another cookie that was \n>issued in the same request not get set.  The \"correct\" action would be \n>to feign ignorance of cookies and do nothing to the state of the UA \n>until a decision is made on accepting or refusing the original cookie \n>in question.  As stated in the original message, the UA should also\n>not send any cookie information when retrieving the CommentURL.\n>\n>> If this is a \"first contact\", or is consequent to the user having\n>> enabled formerly disabled cookie support, that degrades to the\n>> reply having no more than the cookie about which a comment is being\n>> sought.\n>> \n>> Fote\n>\n>ouch... parse error line 3...  sorry... The reply to what?  To the request for\n>the CommentURL?  The UA just needs to know if it should or\n>should not accept a cookie that's been sent to it.  The CommentURL allows\n>the user to make an informed decision about whether they should or\n>should not accept the cookie.  The process of attaining this \"decision\n>making information\" should be \"sacred\"...  without cookies or sessions\n>or anything... it should be anonymous, as though the browser does not\n>have support for cookies, and it should not be something that will result\n>in any cookies being accepted, rejected, or changed in any way, except\n>for the one cookie that is in question, and the point of the whole process.\n\nParsing error, indeed.  The server should not send any \"new\" cookies,\nmeaning none that are not included in the UA's Cookie: header, and the UA\nshould deal with the situation of a server doing so.  However, if you get\nso heavy handed as to bar any cookie exchanges, that's like the blanket\nport restriction in the current RFC, with it's side effect of blocking\nall cookie sharing between http and https servers, including unencrypted\ncookies from an http server going encrypted to an https server.  If you\nmake it that heavy handed, even UAs which do want to implement the IETF\nspecs are likely to respond with \"Sorry, we're not going off the deep end\nwith you.\"\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "CONTENTENCODING: revised proposed wordin",
            "content": "Reference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CONTENT-ENCODING\n\nThis is a rewrite of Henrik's proposed solution, from\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0038.html\n\nI have tried to make it somewhat more precise (especially about\nwhat it means for a response to be \"acceptable\" according to the\nAccept-Encoding header).\n\nAlso, Henrik has already pointed out that Content-Encoding is\nlegal on requests, not just on responses, and so we need to\nspecify what happens in certain corner cases there, as well.\n\n-Jeff\n\n(1) in section 3.5 (Content Codings), add this after the item\nfor \"deflate\"\n\n        identity        The default (identity) encoding; the use\n                        of no transformation whatsoever.  This\n                        content-coding is used only in the\n                        Accept-encoding header, and SHOULD NOT\n                        be used in Content-coding header.\n\n\n(2) Replace section 14.3 (Accept-Encoding) entirely with\n\n    14.3 Accept-Encoding \n\n    The Accept-Encoding request-header field restricts the set of\n    content-codings (defined in section 3.5) that are acceptable for\n    the response.\n    \n   Accept-Encoding  = \"Accept-Encoding\" \":\" \n     1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n    codings          = ( content-codings | \"*\" )\n    \n    Examples of its use are:\n    \n   Accept-Encoding: compress, gzip\n   Accept-Encoding:\n   Accept-Encoding: *\n   Accept-Encoding: compress;q=0.5, gzip;q=1.0\n   Accept-Encoding: gzip=1.0; identity=0.5; *;q=0\n    \n    A server tests whether a content-coding is acceptable, according\n    to an Accept-Encoding field, using these rules:\n(1) If the content-coding is one of the content-codings listed\nin the Accept-Encoding field, then it is acceptable. (Note that,\nas defined in section 3.9, a qvalue of 0 means \"not acceptable\".)\n(2) The special \"*\" symbol in an Accept-Encoding field matches\nany available content-coding.\n(3) If multiple content-codings are acceptable, then the\nacceptable content-coding with the highest non-zero qvalue is\npreferred.  \n(4) If the Accept-Encoding field value is empty, then only the\n\"identity\" encoding is acceptable.\n\n    If an Accept-Encoding field is present in a request, and if the\n    server cannot send a response which is acceptable according to the\n    Accept-Encoding header, then the server SHOULD send an error\n    response with the 406 (Not Acceptable) status code.\n\n    If no Accept-Encoding field is present in a request, the server MAY\n    assume that the client will accept any content coding.  In this\n    case, if \"identity\" is one of the available content-codings, then\n    the server SHOULD use the \"identity\" content-coding.\n\nNote: If the request does not include an Accept-Encoding field,\nand if the \"identity\" content-coding is unavailable, then\npreference should be given to content-codings commonly\nunderstood by HTTP/1.0 clients (i.e., \"gzip\" and \"compress\");\nsome older clients improperly display messages sent with other\ncontent-encodings.  The server may also make this decision\nbased on information about the particular user-agent or\nclient.\n\n(3) In section 14.12 (Content-Encoding), replace\n\n   The Content-Encoding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n\nwith\n\n   The content-coding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n   However, a proxy MAY modify the content-coding if the new coding\n   is known to be acceptable to the recipient, unless the \"no-transform\"\n   Cache-control directive is present in the message.\n\n   If the content-coding of an entity is not \"identity\", then the\n   response MUST including a Content-Encoding entity-header (section\n   14.12) that lists the non-identity content-coding(s) used.\n\n   If the content-coding of an entity in a request message is\n   not acceptable to the origin server, the server SHOULD respond\n   with a status code of 415 (Unsupported Media Type).\n\n(4) In section 14.9 (Cache-Control), add\n                          | \"no-transform\"\nto the BNF for cache-request-directive.\n\n(5) In section 14.9.5 (No-Transform Directive), replace\n\n   Therefore, if a response includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\nwith\n\n   Therefore, if a message includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\n[End of proposed changes]\n\n\n\n"
        },
        {
            "subject": "CONTENTENCODING: FIXED revised proposed wordin",
            "content": "[OOPS.  My previous message was missing one very important sentence,\nregarding when \"identity\" content-codings are implicitly allowed.  -Jeff]\n\nReference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#CONTENT-ENCODING\n\nThis is a rewrite of Henrik's proposed solution, from\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0038.html\n\nI have tried to make it somewhat more precise (especially about\nwhat it means for a response to be \"acceptable\" according to the\nAccept-Encoding header).\n\nAlso, Henrik has already pointed out that Content-Encoding is\nlegal on requests, not just on responses, and so we need to\nspecify what happens in certain corner cases there, as well.\n\n-Jeff\n\n(1) in section 3.5 (Content Codings), add this after the item\nfor \"deflate\"\n\n        identity        The default (identity) encoding; the use\n                        of no transformation whatsoever.  This\n                        content-coding is used only in the\n                        Accept-encoding header, and SHOULD NOT\n                        be used in Content-coding header.\n\n\n(2) Replace section 14.3 (Accept-Encoding) entirely with\n\n    14.3 Accept-Encoding \n\n    The Accept-Encoding request-header field restricts the set of\n    content-codings (defined in section 3.5) that are acceptable for\n    the response.\n    \n   Accept-Encoding  = \"Accept-Encoding\" \":\" \n     1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n    codings          = ( content-codings | \"*\" )\n    \n    Examples of its use are:\n    \n   Accept-Encoding: compress, gzip\n   Accept-Encoding:\n   Accept-Encoding: *\n   Accept-Encoding: compress;q=0.5, gzip;q=1.0\n   Accept-Encoding: gzip=1.0; identity=0.5; *;q=0\n    \n    A server tests whether a content-coding is acceptable, according\n    to an Accept-Encoding field, using these rules:\n(1) If the content-coding is one of the content-codings listed\nin the Accept-Encoding field, then it is acceptable. (Note that,\nas defined in section 3.9, a qvalue of 0 means \"not acceptable\".)\n(2) The special \"*\" symbol in an Accept-Encoding field matches\nany available content-coding.\n(3) If multiple content-codings are acceptable, then the\nacceptable content-coding with the highest non-zero qvalue is\npreferred.  \n(4) The \"identity\" content-coding is always acceptable, unless\nspecifically refused because the Accept-Encoding field includes\n\"identity;q=0\", or because the field includes \"*;q=0\" and does\nnot explictly include the \"identity\" content-coding.  If the\nAccept-Encoding field-value is empty, then only the \"identity\"\nencoding is acceptable.\n\n    If an Accept-Encoding field is present in a request, and if the\n    server cannot send a response which is acceptable according to the\n    Accept-Encoding header, then the server SHOULD send an error\n    response with the 406 (Not Acceptable) status code.\n\n    If no Accept-Encoding field is present in a request, the server MAY\n    assume that the client will accept any content coding.  In this\n    case, if \"identity\" is one of the available content-codings, then\n    the server SHOULD use the \"identity\" content-coding.\n\nNote: If the request does not include an Accept-Encoding field,\nand if the \"identity\" content-coding is unavailable, then\npreference should be given to content-codings commonly\nunderstood by HTTP/1.0 clients (i.e., \"gzip\" and \"compress\");\nsome older clients improperly display messages sent with other\ncontent-encodings.  The server may also make this decision\nbased on information about the particular user-agent or\nclient.\n\n(3) In section 14.12 (Content-Encoding), replace\n\n   The Content-Encoding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n\nwith\n\n   The content-coding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n   However, a proxy MAY modify the content-coding if the new coding\n   is known to be acceptable to the recipient, unless the \"no-transform\"\n   Cache-control directive is present in the message.\n\n   If the content-coding of an entity is not \"identity\", then the\n   response MUST including a Content-Encoding entity-header (section\n   14.12) that lists the non-identity content-coding(s) used.\n\n   If the content-coding of an entity in a request message is\n   not acceptable to the origin server, the server SHOULD respond\n   with a status code of 415 (Unsupported Media Type).\n\n(4) In section 14.9 (Cache-Control), add\n                          | \"no-transform\"\nto the BNF for cache-request-directive.\n\n(5) In section 14.9.5 (No-Transform Directive), replace\n\n   Therefore, if a response includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\nwith\n\n   Therefore, if a message includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\n[End of proposed changes]\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Tue, 22 Jul 1997, Dave Kristol wrote:\n\n> Recently there was discussion between Dave Morris and me about whether\n> the request-host needed to be a FQHN, about changes to the definition\n> of domain-match, and about the treatment of the Domain= attribute in\n> Set-Cookie2.  The gist of the discussion was:  \"Host names can be\n> specified in any form acceptable to the base HTTP protocol. That may be\n> an IP address, an incomplete host name string, or a FQHN string which\n> is the prefe[r]red form.\"  I've done some more thinking about it and have\n> the following observations.\n> \n> 1) (Sect 2) Relax requirement that request-host be a FQHN.\n> \n> Okay.  request-host could be any name string.\n> \n> 2) (Sect 2) Change domain-match algorithm.\n> \n> Okay.  A domain-matches B if the strings for A and B exactly match,\n> whether they are IP addresses, partial name strings, or FQHNs.\n> \n> 3) (Sect 4.3.2) Allow a Domain= value not to start with '.' and force a\n> '.' in front of any such value.\n> \n> No.  Suppose your local network has a host \"com\".  This transaction ensues:\n> \n> C -> S\n> GET /cgi-bin/foo HTTP/1.1\n> Host: com\n> [other stuff]\n> \n> S -> C\n> HTTP/1.1 200 OK\n> Set-Cookie: foo=bar; Domain=com\n> Set-Cookie2: Version=\"1\"\n> \n> The proposed rule would have two undesirable effects, because the domain for\n> the cookie would become \".com\":\n> \n> a) You could not return cookie \"foo\" to the server you got it from,\n> because \"com\" does not domain-match \".com\".\n> \n> b) Cookie \"foo\" would get sent to every server of the form \"*.com\".\n> \n> \n> The effect of doing 1 and 2 (and not 3) above would be:\n> \n> 1) You could use incomplete domain names (need not be FQHN) in Domain=.\n> 2) The rule about the implicit value for Domain= would remain, namely that\n> a cookie sent in a Set-Cookie[2] that had no Domain= attribute could only\n> be returned to the server from which it came.\n> \n> Question to Dave Morris:\n> Would those changes meet your needs?\n> \nDave Kristol and I have had some extensive discussions over the details\nand I believe we are in complete violent agreement now so let me\nsumarize our discussion and my response to the above based on that \nsummary.\n\nDave now agrees that my proposal to relax the handling of the domain=\nattribute such that the user-agent may accept a value w/o a leading\ndot but it must prepend a dot in that case before using the value.\nAll the other rules about embedded dots remain so the domain=com\nexample above would not apply.\n\nThe various requirements re. FQDN and FQHN are relaxed in favor of\nthe already existing rules about embedded dots which make the \nhost names look like a FQHN. The bottom line is that a server\nand/or client really can't tell if a three level name is a true FQHN\nor an arbitrary sub-part such as x.y.z of x.y.z.foo.com since \nthe number of TLDs is going to greatly expand and alternatives like\nAlterNic make even the expanded TLDs moot for comparison purposes.\n\nFor practical purposes, for a user-agent to make a request of a non-local\nserver, it will have the FQHN of the server so the 'relaxed' but\npreviously unenforceable rules will not enable cookie sharing outside\nof a local private network.\n\nThe current domain matching rules are: \n\n  Sometimes we compare one host name with another.  Host A's name domain-\n  matches host B's if\n\n    * both host names are IP addresses and their host name strings match\n      exactly; or\n\n    * both host names are FQDN strings and their host name strings match\n      exactly; or\n\n    * A is a FQDN string and has the form NB, where N is a non-empty name\n      string, B has the form .B', and B' is a FQDN string.  (So, x.y.com\n      domain-matches .y.com but not y.com.)\n\nAnd these would become:\n\n  Sometimes we compare one host name with another.  Host A's name domain-\n  matches host B's if\n\n    * both host names are IP addresses and their host name strings match\n      exactly; or\n\n    * both host names are strings and their host name strings match\n      exactly; or\n\n    * A is a string and has the form NB, where N is a non-empty name\n      string, B has the form .B', and B' is a string containing at least\n      one (1) embedded dot. (So, x.y.com domain-matches .y.com but not\n      y.com.)\n\nSo at this point my answer is yes, I'm completely satisfied with the \nresult.\n\nThanks to all for your patience!\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "On Tue, 22 Jul 1997, Jeffrey Mogul wrote:\n\n> I realize that some people were opposed to the introduction of qvalues\n> on Accept-Encoding.  However, during today's editorial-group discussion\n> of the CONTENT-ENCODING issue, we realized that the syntax for the\n> various Accept-* request headers in RFC2068 is almost, but not quite,\n> uniform, and we reached a tentative agreement that it might be a good\n> idea to have all of the Accept-* request headers (Accept-Range is a\n> response header) have similar syntaxes.\n\nGreat! I guess this might go along well with my suggestions to\nstreamline and concentrate the explanations, too.\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Re: CONTENTENCODING: FIXED revised proposed wordin",
            "content": "On Tue, 22 Jul 1997, Jeffrey Mogul wrote:\n\n> [OOPS.  My previous message was missing one very important sentence,\n> regarding when \"identity\" content-codings are implicitly allowed.  -Jeff]\n\n> (2) Replace section 14.3 (Accept-Encoding) entirely with\n> \n>     14.3 Accept-Encoding \n\n\n>     A server tests whether a content-coding is acceptable, according\n>     to an Accept-Encoding field, using these rules:\n> (1) If the content-coding is one of the content-codings listed\n> in the Accept-Encoding field, then it is acceptable. (Note that,\n> as defined in section 3.9, a qvalue of 0 means \"not acceptable\".)\n> (2) The special \"*\" symbol in an Accept-Encoding field matches\n> any available content-coding.\n> (3) If multiple content-codings are acceptable, then the\n> acceptable content-coding with the highest non-zero qvalue is\n> preferred.  \n> (4) The \"identity\" content-coding is always acceptable, unless\n> specifically refused because the Accept-Encoding field includes\n> \"identity;q=0\", or because the field includes \"*;q=0\" and does\n> not explictly include the \"identity\" content-coding.  If the\n> Accept-Encoding field-value is empty, then only the \"identity\"\n> encoding is acceptable.\n\nA lot of what is written above turns up exactly the same in\nAccept-Charset, Accept-Language, and so on. In many cases, there\nis something special, but not too much. Is there a way to organize\nthe text different? This would hopefully a) save space, b) help\ngetting an overview, and c) make things more consistent.\n\nFor example, with respect to \"*\", the general description could\nsay that:\n\n\"*\" in an accept field matches any value possible in that accept\nfield, including a default. A q-value given to \"*\" applies to all\nvalues, including the default, but except those that are explicitly\npresent. An accept field may not allow \"*\", or may use a more\nelaborate syntax for this functionality. Such cases are explicitly\nmentionned in the description of the field itself.\n\nSimilar texts could explain the use of a default, the meaning of\nq=0 (this seems already done to a certain extent), and so on.\n\nFor \"Accept-Encoding\", we would then just have to list the\npossible values and identify the default.\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "1.1 proxies",
            "content": "  I'd like to do some testing of our server through 1.1 proxies; I'm\n  mostly interested in how it affects the user experience.  If any of\n  you know of a 1.1 proxy on the Net that I can make light use of, I'd\n  love to hear about it (off list, if you prefer).  I will of course\n  pass on anything interesting I learn, and will keep any and all\n  information about the proxy confidential.\n\n  Our 1.1 servers are available for testing any time:\n\n    www.agranat.com\n\n    digest-test.agranat.com (1.1 server with a set of\n                             authentication tests)\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": "  I like the revised proposal better, and I think that it does as well\n  as can be done to preserve compatibility with 2068 implementations,\n  and I very much like the idea of using RFC numbers as feature\n  identifiers.\n\n  On the question of proxies modifying the responses, perhaps it would\n  be better for a proxy that does not implement a particular feature\n  to add a non-compliance indication without removing the original\n  assertion; this might enable a client to detect the true situation.\n\n  Non-Compliance =\n        \"Non-Compliance\" \":\" proxy-host 1#non-compliance-option\n\n    proxy-host = host [ \":\" port ]\n\nnon-compliance-option =\n        compliance-namespace \"=\" option-item [ \";\" option-disposition ]\n\noption-disposition = \"passed\" | \"dropped\"\n\n  If the proxy recognizes the option-item as one that it will pass\n  through but not react to, it will send the 'passed'\n  option-disposition.  If it recognizes the option-item as one that\n  will not operate correctly through itself (either due to an\n  implementation restriction or as a matter of policy), then it will\n  send the 'dropped' option-disposition.  If the proxy does not\n  recognize the option-item, it will send no option-disposition, in\n  which case the client must rely on its own knowlege of that item to\n  make any guess as to whether or not it might work through that\n  proxy.\n\n  A Non-Compliance header field MUST NOT be sent by any origin\n  server.\n\n  What do the client and proxy authors out there think?  Would this\n  enable you to do something useful?  Is there a way we could or\n  should modify the rules for the Public header like this?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "[Discussing language that would state under what circumstances a UA,\nexamining a page fetched via CommentURL, would accept a cookie...]\n\nScott Lawrence (responding to Foteos Macrides):\n> >ouch... parse error line 3...  sorry... The reply to what?  To the request for\n> >the CommentURL?  The UA just needs to know if it should or\n> >should not accept a cookie that's been sent to it.  The CommentURL allows\n> >the user to make an informed decision about whether they should or\n> >should not accept the cookie.  The process of attaining this \"decision\n> >making information\" should be \"sacred\"...  without cookies or sessions\n> >or anything... it should be anonymous, as though the browser does not\n> >have support for cookies, and it should not be something that will result\n> >in any cookies being accepted, rejected, or changed in any way, except\n> >for the one cookie that is in question, and the point of the whole process.\n> \n\nFoteos Macrides:\n>         Parsing error, indeed.  The server should not send any \"new\" cookies,\n> meaning none that are not included in the UA's Cookie: header, and the UA\n> should deal with the situation of a server doing so.  However, if you get\n> so heavy handed as to bar any cookie exchanges, that's like the blanket\n> port restriction in the current RFC, with it's side effect of blocking\n> all cookie sharing between http and https servers, including unencrypted\n> cookies from an http server going encrypted to an https server.  If you\n> make it that heavy handed, even UAs which do want to implement the IETF\n> specs are likely to respond with \"Sorry, we're not going off the deep end\n> with you.\"\n\nThe folks arguing *for* CommentURL say it's a valuable feature and (I\nthink they would claim) easy to implement.  I agree that, at the\nbits-on-the-wire protocol level, it's simple to specify, but I've always\nbeen skeptical about the UA implications.  I think Foteos's wording for\nhow a UA should handle a cookie that it receives while examining a\nCommentURL page (not reproduced here) help to make my point.  I think\nthey're complex.\n\nSome of the disagreement results from differing visions of when/how a\nuser could inspect a cookie.  (And we must allow for that flexibility.) \nI imagined that the inspection would take place at the point where a\ncookie arrived, subsequent to a user's clicking on a link, and before\nsome or all of the content has been received.  In that circumstance, if\nthe UA offers the user the opportunity to follow the CommentURL link for\nthe cookie, I think it's inappropriate for the UA to send a cookie to\nthe very server whose cookie policy the user is (presumably) about to\nexamine, or to accept cookies from that server or others.\n\nOn the other hand, if the cookie inspection mechanism is \"off-line\", in\nthe sense that it functions when the UA is otherwise quiescent (no page\nretrieval taking place), then different rules may apply.  The\ninterrogation of the CommentURL could be treated similarly to retrieval\nof any other resource.\n\nI think a UA could (perhaps *should*) provide both mechanisms.  The\nquestion is (assuming you agree with the two paragraphs above) how to\nword the spec. to restrict the first case and not the second.  Dave\nMorris proposed:\n\n>    A potentially confusing situation exists if a user agent's cookie\n>    inspection interface allows a user to follow a CommentURL link\n>    within a dialog which is prompting the user to decide if the cookie\n>    containing the CommentURL is acceptable AND following the CommentURL\n>    link results in receipt of a new, not previously approved cookie.\n>    The useragent MAY discard any cookie received in this context in order\n>    to avoid the complexities of interacting with the user regarding nested\n>    set-cookie requests.  Servers which depend on cookies MUST allow for\n>    the possibility that URLs used in their cookie's CommentURL value\n>    will be ignored by user agents.\n\nThat's not bad.  In his related commentary Dave (Morris) said that\npreviously accepted cookies could be returned with CommentURL.  I don't\nthink I agree, but in any case I don't see that addressed in these words\n(though it's not ruled out, either). \n\nHere's my rewording of Dave Morris's words:\n\nA potentially confusing situation exists if this sequence occurs:\n- the user agent receives a cookie that contains a CommentURL\nattribute;\n- the user agent's cookie inspection interface presents a dialog that\nallows the user to follow the CommentURL link; and,\n- when the user follows the CommentURL link, the origin server (or\nanother server, via other links in the returned content) returns another\ncookie.\n\n[My addition:]  The user agent should not send any cookies in this\ncontext.  The user agent may discard any cookie received in this context\nthat the user has not, through some user agent mechanism, deemed\nacceptable.\n\n[I left out the remark about allowing for ignored cookies, because\nthat's always true.]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n[ appended ]\n\nThe commentURL points to a resource which can be requested by\na UA under a variety of circummstances, not just in conjunction with\nan assessment of whether to accept a new (not previously accepted or\nexpired) cookie, or an assessment of whether to delete a previously\n(e.g., tentatively) accepted cookie.  A link for that URL might have\nbeen bookmarked, or added to some document.  The server thus must be\nprepared to have Cookie headers in requests for those resources, and\nshould act on them as it would for other requests sent by UAs.  If\nyou impose restrictions for such URLs, you are creating complications,\nnot alleviating them.\n\nIn those cases where the UA is sending the request in conjunction\nwith an assessment of whether to accept a new cookie or delete an existing\none, I still see no compelling reason to omit a Cookie header from the\nrequest, if there are cookies which the UA would normally send to that\nsite.  I also think it is desireable to send at least the cookie for which\na comment is being requested, to ensure that at least one cookie is\nincluded in the request (That's a MAY, not a MUST. :)  Such a cookie would\nhave the modern format for UAs which have adopted the modern IETF specs.\nConsider, for example, the case for a host which does need to impose a port\nrestriction.  It's needs cannot be met by a blanket port restriction, as in\nthe current RFC.  The inclusion of a cookie with the modern format provides\nassurance to the server that the request comes from a modern UA, and that a\ndocument which is suitable for modern UAs can be returned.  Otherwise, the\nserver may elect to return a comment such as:\n\nWARNING: Your browser may have a historical implementation of\ncookie handling which inadequately protects your privacy and\nmight result is transmissions of your cookies to inappropriate\nsites.  See IETF RFC??? <URL: ...> for more information.  A\nlist of browsers with modern cookie support is available from\n<URL: ...>.  Such browsers will enable you to derived the full\nbenefit of services offered by our site.\n\nOf course, the WebMaster for the server might consider that particular\nmessage impolite, or risks cache busting.  The point is that there are\nreasons why it would be good to allow exchanges of cookies in conjunction\nwith the requests for commentURLs, and how they are handled is an\n\"implementation issue\" dependent on the context in which the UA issues\nthe request.  All that's needed is a word of caution about getting caught\nin a loop, and if you go too far beyond that at this time, it may turn out\nthat you're added a bug rather than alleviated a problem.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n>[Discussing language that would state under what circumstances a UA,\n>examining a page fetched via CommentURL, would accept a cookie...]\n>\n>Scott Lawrence (responding to Foteos Macrides):\n>> >ouch... parse error line 3...  sorry... The reply to what?  To the request for\n>> >the CommentURL?  The UA just needs to know if it should or\n>> >should not accept a cookie that's been sent to it.  The CommentURL allows\n>> >the user to make an informed decision about whether they should or\n>> >should not accept the cookie.  The process of attaining this \"decision\n>> >making information\" should be \"sacred\"...  without cookies or sessions\n>> >or anything... it should be anonymous, as though the browser does not\n>> >have support for cookies, and it should not be something that will result\n>> >in any cookies being accepted, rejected, or changed in any way, except\n>> >for the one cookie that is in question, and the point of the whole process.\n>> \n>\n>Foteos Macrides:\n>>         Parsing error, indeed.  The server should not send any \"new\" cookies,\n>> meaning none that are not included in the UA's Cookie: header, and the UA\n>> should deal with the situation of a server doing so.  However, if you get\n>> so heavy handed as to bar any cookie exchanges, that's like the blanket\n>> port restriction in the current RFC, with it's side effect of blocking\n>> all cookie sharing between http and https servers, including unencrypted\n>> cookies from an http server going encrypted to an https server.  If you\n>> make it that heavy handed, even UAs which do want to implement the IETF\n>> specs are likely to respond with \"Sorry, we're not going off the deep end\n>> with you.\"\n>\n>The folks arguing *for* CommentURL say it's a valuable feature and (I\n>think they would claim) easy to implement.  I agree that, at the\n>bits-on-the-wire protocol level, it's simple to specify, but I've always\n>been skeptical about the UA implications.  I think Foteos's wording for\n>how a UA should handle a cookie that it receives while examining a\n>CommentURL page (not reproduced here) help to make my point.  I think\n>they're complex.\n>\n>Some of the disagreement results from differing visions of when/how a\n>user could inspect a cookie.  (And we must allow for that flexibility.) \n>I imagined that the inspection would take place at the point where a\n>cookie arrived, subsequent to a user's clicking on a link, and before\n>some or all of the content has been received.  In that circumstance, if\n>the UA offers the user the opportunity to follow the CommentURL link for\n>the cookie, I think it's inappropriate for the UA to send a cookie to\n>the very server whose cookie policy the user is (presumably) about to\n>examine, or to accept cookies from that server or others.\n>\n>On the other hand, if the cookie inspection mechanism is \"off-line\", in\n>the sense that it functions when the UA is otherwise quiescent (no page\n>retrieval taking place), then different rules may apply.  The\n>interrogation of the CommentURL could be treated similarly to retrieval\n>of any other resource.\n>\n>I think a UA could (perhaps *should*) provide both mechanisms.  The\n>question is (assuming you agree with the two paragraphs above) how to\n>word the spec. to restrict the first case and not the second.  Dave\n>Morris proposed:\n>\n>>    A potentially confusing situation exists if a user agent's cookie\n>>    inspection interface allows a user to follow a CommentURL link\n>>    within a dialog which is prompting the user to decide if the cookie\n>>    containing the CommentURL is acceptable AND following the CommentURL\n>>    link results in receipt of a new, not previously approved cookie.\n>>    The useragent MAY discard any cookie received in this context in order\n>>    to avoid the complexities of interacting with the user regarding nested\n>>    set-cookie requests.  Servers which depend on cookies MUST allow for\n>>    the possibility that URLs used in their cookie's CommentURL value\n>>    will be ignored by user agents.\n>\n>That's not bad.  In his related commentary Dave (Morris) said that\n>previously accepted cookies could be returned with CommentURL.  I don't\n>think I agree, but in any case I don't see that addressed in these words\n>(though it's not ruled out, either). \n>\n>Here's my rewording of Dave Morris's words:\n>\n>A potentially confusing situation exists if this sequence occurs:\n>- the user agent receives a cookie that contains a CommentURL\n>attribute;\n>- the user agent's cookie inspection interface presents a dialog that\n>allows the user to follow the CommentURL link; and,\n>- when the user follows the CommentURL link, the origin server (or\n>another server, via other links in the returned content) returns another\n>cookie.\n>\n>[My addition:]  The user agent should not send any cookies in this\n>context.  The user agent may discard any cookie received in this context\n>that the user has not, through some user agent mechanism, deemed\n>acceptable.\n>\n>[I left out the remark about allowing for ignored cookies, because\n>that's always true.]\n\n\n\n"
        },
        {
            "subject": "can't really be LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "We can't quite LAST CALL a document which has a technical\nissue unresolved. The technical issue that's unresolved\nis the limitation on accepting cookies while interacting\nwith the resource identified by the CommentURL annotating\na cookie.\n\nI will state my personal opinion, again, just in case there\nis some additional support for it:\n\nI think that the complexity inherent in \"CommentURL\" makes\nit suspect, and that the simplest thing to do is to remove\nit. If there is no CommentURL, then you don't need a policy\nfor accepting cookies while interacting with it.\n\nToo much icing on the cookie, just say no.\n\n\nLarry\n\n\n\n"
        },
        {
            "subject": "HTTP status code",
            "content": "I brought this up at the San Jose meeting, but there never was a\nsatisfactory (official) answer, as far as I remember.\n\nSIP and RTSP re-use a number of HTTP status codes, among other\nproperties. It is likely that they may need to or want to adopt other\nHTTP status codes that emerge in the future. Thus, it is desirable that\nthe SIP and RTSP-specific status codes do not conflict with HTTP codes.\n\nOne possible solution: Declare officially that HTTP will only use status\ncodes up to x49 (say) and leave others for private extensions, including\nSIP and RTSP.\n\n(Note to HTTP-WG: SIP and RTSP are described, inter alia, at  and\nhttp://www.cs.columbia.edu/~hgs/rtsp and\nhttp://www.cs.columbia.edu/~hgs/sip)\n\nComments and decisions are appreciated.\n\nHenning\n-- \nHenning Schulzrinne         email: schulzrinne@cs.columbia.edu\nDept. of Computer Science   phone: +1 908 949 8344 (at Bell Labs)\nColumbia University         fax:   +1 212 666-0140\nNew York, NY 10027          URL:   http://www.cs.columbia.edu/~hgs\n\n\n\n"
        },
        {
            "subject": "Re: HTTP status code",
            "content": "   Sender: hgs@dnrc.bell-labs.com\n   Date: Wed, 23 Jul 1997 16:17:22 -0400\n   From: \"Henning Schulzrinne (BL)\" <hgs@cs.columbia.edu>\n   Organization: Columbia University\n\n   SIP and RTSP re-use a number of HTTP status codes, among other\n   properties. It is likely that they may need to or want to adopt other\n   HTTP status codes that emerge in the future. Thus, it is desirable that\n   the SIP and RTSP-specific status codes do not conflict with HTTP codes.\n\n   One possible solution: Declare officially that HTTP will only use status\n   codes up to x49 (say) and leave others for private extensions, including\n   SIP and RTSP.\n\nThere may be things other than SIP and RTSP, and it may be the case that\nthose other protocols will define status codes which are useful in\nSIP and RTSP and HTTP, so it might be better to do something like this:\n\nx00 through x49 is for HTTP\nx50 through x59 is for SIP\nx60 through x69 is for RTSP\nand then we can assign blocks of ten to a few more protocols later.\n\nanother possibilty is to state that x00 through x49 are for things defined\nby the HTTP spec authors, and x50 through x99 are for others, and have\none central authority to control allocation of those numbers in the\nx50 through x99 range.  (It is probably desireable to keep the status\ncodes defined by HTTP consecutive.)\n\nBut whatever we decide to do, I don't think SIP and RTSP should be\nconsidered `private extensions'.  Perhaps `defined by people other\nthan the authors of the HTTP spec' is an OK description, though.\n\n\n\n"
        },
        {
            "subject": "The significance of Cookie",
            "content": "The latest Netscape Communicator Release seems to have removed the option provided\nin previous releases to disable the reception of cookies.  Does anyone know what Netscape's\nstance on this is? \n\n\n\n"
        },
        {
            "subject": "Re: CONTENTENCODING: FIXED revised proposed wordin",
            "content": "Martin J. Duerst writes:\n\n    A lot of what is written above turns up exactly the same in\n    Accept-Charset, Accept-Language, and so on. In many cases, there\n    is something special, but not too much. Is there a way to organize\n    the text different? This would hopefully a) save space, b) help\n    getting an overview, and c) make things more consistent.\n    \nI thought about doing that, but decided against it for several\nreasons:\n\n(1) I wanted to minimize the risk that any changes I proposed\nwould create new problems for the other Accept-* headers.\nBy not changing any of those sections, I hope I didn't break\nthem!\n\n(2) I'm don't really know the history behind the current\nwordings for Accept, Accept-Charset, and Accept-Language;\nI do know that each one is subtly different, and so trying\nto unify the descriptions might either require a lot of\nspecial cases, or it may break things that people depend\non.\n\n(3) Time is short, and the most important thing at this\npoint is to get the actual protocol specification frozen\nASAP.  I.e., the BNF and semantics for these headers.\nIf at some (not TOO distant) point in the future, someone\nwants to try to rewrite the description here, without\nchanging the meaning, I'm sure that would be a much easier\nprocess than actually changing the BNF or semantics.\n\nBottom line: if anyone thinks you know how to reorganize the text\nof the Accept-* headers, to make it simpler and more uniform,\nbut without changing the BNF or semantics, by all means go ahead\nand do so.  But probably this will have to wait until after the\nMunich IETF, since we already have too much work to do before\nthe July 30 I-D submission deadline.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: can't really be LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Wed, 23 Jul 1997, Larry Masinter wrote:\n\n> We can't quite LAST CALL a document which has a technical\n> issue unresolved. The technical issue that's unresolved\n> is the limitation on accepting cookies while interacting\n> with the resource identified by the CommentURL annotating\n> a cookie.\n\nWell the technical issue has only received intense discussion for a day\nand there would seem to be convergence.\n\n> \n> I will state my personal opinion, again, just in case there\n> is some additional support for it:\n> \n> I think that the complexity inherent in \"CommentURL\" makes\n> it suspect, and that the simplest thing to do is to remove\n> it. If there is no CommentURL, then you don't need a policy\n> for accepting cookies while interacting with it.\n> \n> Too much icing on the cookie, just say no.\n\nThe simple problem is that as a protocol design we are demanding that\nUI/UA designers provide a meaningful dialog with end-users for control\nof cookie usage in the interest of protecting user privacy. Without\nincluding the CommentURL we are not providing any way for the enduser\nand origin server to have a mutual understanding of how the cookie \nwill be used. There is no way to require the server to provide the\nCommentURL or to provide a meaningful message but one might expect that\nusers who bother to police their cookie transactions would be less\ninclined to accept cookies which don't have the CommentURL. As far as\nthe accuracy of the CommentURL description is usage is concerned, I \nwould speculate from a limited legal background that publishing a false\nstatement could be the basis of legal action for false advertising,\nmisrepresentation, etc.\n\nThe difference between the Comment attribute and the CommentURL is the\ndifference between the Windows application which provides a message \nbox with a message like:\n             \"Unable to write bookmark file\"\nand one which presents the message:\n             \"Unable to write bookmark file: \n              C:\\home\\user\\internet\\bkmrks.fil\n              because the file already exists and is owned\n              by another user\"\nIn the first case, only a user familiar with the application internals\ncould guess where to start looking.  In the second case, the average\nreasonably knowledgable user of the operating system usage would have \na good chance at successful problem resolution.\n\nIf user privacy is important to our protocol effort, we must make it\npossible for the user to receive sufficient information for informed\nconsent. If we don't, the user community will throw their hands up\nand take the course of least resistance and all of our concern about\ncookie sharing will be moot.\n\nIn other words, I don't consider CommentURL as icing on the cookie,\nit is central to any possibility of achieving user control over\nprivacy.\n\nDave Morris \n\n\n\n"
        },
        {
            "subject": "Re: The significance of Cookie",
            "content": "On Wed, 23 Jul 1997, Steve Colby wrote:\n\n> The latest Netscape Communicator Release seems to have removed the option provided\n> in previous releases to disable the reception of cookies.  Does anyone know what Netscape's\n> stance on this is? \n\nWell the NS Communicator I'm running still has the options ... the whole\noptions stuff has been moved and reorganized.  Under VIEW select\npreferences and then click the 'advanced' topic. There are 4\noptions to control cookies and others for java, javascript, style\nsheets, etc.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: HTTP status code",
            "content": "One way or the other, I believe there is going to need to be an IANA registry.\nPrivate codes don't work too well, as two different people might\nstill happen to choose the same one.\n\nTo get one, you need a standards track document that defines the\nprocess to deal with the registry.  (I.e. IANA needs directions on how\nit should be run).\n\nSo you need to generate an Internet draft outlining the process, and\ncirculate it to the appropriate working groups.  It doesn't have to be\nlong, but it does have to exist, and go through the normal mill.\n\nI think you'll find people here receptive to such a document (but don't\nexpect any volunteers from here either to write the first draft; we have\nour hands full getting 1.1 to draft standard right now).\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n  > The commentURL points to a resource which can be requested by\n  > a UA under a variety of circummstances, not just in conjunction with\n  > an assessment of whether to accept a new (not previously accepted or\n  > expired) cookie, or an assessment of whether to delete a previously\n  > (e.g., tentatively) accepted cookie.  A link for that URL might have\n  > been bookmarked, or added to some document.  The server thus must be\n  > prepared to have Cookie headers in requests for those resources, and\n  > should act on them as it would for other requests sent by UAs.  If\n  > you impose restrictions for such URLs, you are creating complications,\n  > not alleviating them.\n\nNot really.  The server has to be prepared not to receive the cookie\nback in any event.  Also, the complications I was trying to alleviate\nwere on the UA side, not the server side.\n\n  > \n  > In those cases where the UA is sending the request in conjunction\n  > with an assessment of whether to accept a new cookie or delete an existing\n  > one, I still see no compelling reason to omit a Cookie header from the\n  > request, if there are cookies which the UA would normally send to that\n  > site.  I also think it is desireable to send at least the cookie for which\n\nThink again about that.  Suppose site a.com sent cookie X=1 previously.\nI go to a.com again, send cookie X=1, get a new cookie, X=2, and pop up\na cookie inspection dialog.  CommentURL directs me to a URL on a.com.\nI certainly don't want to send X=2:  that's the cookie I just got, and\nI'm trying to decide whether to accept it.  But I just sent X=1.  Does\nit help to send it again in this context?\n\n  > a comment is being requested, to ensure that at least one cookie is\n\nOn the contrary.  I haven't decided whether I want to accept that cookie.\nWhy should I send it back before accepting it?\n\n  > included in the request (That's a MAY, not a MUST. :)  Such a cookie would\n  > have the modern format for UAs which have adopted the modern IETF specs.\n  > Consider, for example, the case for a host which does need to impose a port\n  > restriction.  It's needs cannot be met by a blanket port restriction, as in\n  > the current RFC.  The inclusion of a cookie with the modern format provides\n  > assurance to the server that the request comes from a modern UA, and that a\n  > document which is suitable for modern UAs can be returned.  Otherwise, the\n  > server may elect to return a comment such as:\n  > \n  > WARNING: Your browser may have a historical implementation of\n  > cookie handling which inadequately protects your privacy and\n  > might result is transmissions of your cookies to inappropriate\n  > sites.  See IETF RFC??? <URL: ...> for more information.  A\n  > list of browsers with modern cookie support is available from\n  > <URL: ...>.  Such browsers will enable you to derived the full\n  > benefit of services offered by our site.\n\nI think the server will be able to recognize good cookies whenever it\nreceives them.  I don't see the relevance of receiving a cookie during\nthe inspection process.  For example, a user might decide never to\ninspect cookies.\n\n  > \n  > Of course, the WebMaster for the server might consider that particular\n  > message impolite, or risks cache busting.  The point is that there are\n  > reasons why it would be good to allow exchanges of cookies in conjunction\n  > with the requests for commentURLs, and how they are handled is an\n  > \"implementation issue\" dependent on the context in which the UA issues\n  > the request.  All that's needed is a word of caution about getting caught\n  > in a loop, and if you go too far beyond that at this time, it may turn out\n  > that you're added a bug rather than alleviated a problem.\n\nI remain unpersuaded.  It would help me if you would comment on my\nproposed words and/or, especially, if you would propose words you\nconsider suitable in their stead.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: can't really be LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Wed, 23 Jul 1997, Larry Masinter wrote:\n>>> Too much icing on the cookie, just say no.\n>\n>[...]\n>\n>The difference between the Comment attribute and the CommentURL is the\n>difference between the Windows application which provides a message \n>box with a message like:\n>             \"Unable to write bookmark file\"\n>and one which presents the message:\n>             \"Unable to write bookmark file: \n>              C:\\home\\user\\internet\\bkmrks.fil\n>              because the file already exists and is owned\n>              by another user\"\n>In the first case, only a user familiar with the application internals\n>could guess where to start looking.  In the second case, the average\n>reasonably knowledgable user of the operating system usage would have \n>a good chance at successful problem resolution.\n\nNote that even in that example, you are restricting yourself\nto ASCII characters. :)\n\n\n>If user privacy is important to our protocol effort, we must make it\n>>possible for the user to receive sufficient information for informed\n>>consent. If we don't, the user community will throw their hands up\n>>and take the course of least resistance and all of our concern about\n>>cookie sharing will be moot.\n>>\n>>In other words, I don't consider CommentURL as icing on the cookie,\n>>it is central to any possibility of achieving user control over\n>>privacy.\n\nI doubt it will be considered icing by users whose language\nis not adequately accomodated by the device of try to stuff a body\ninto the value of a Set-Cookie2 header's comment attribute. :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP status code",
            "content": "I think the best approach would be to establish an IANA registry\nfor HTTP status codes, \"preload\" it with all of the status\ncodes defined in the next draft of the HTTP/1.1 spec, and then\ninsist that people defining new status codes use the IANA\nregistry to avoid conflicts.\n\nIf people are going to propose additional methods or headers,\noutside the context of the HTTP working group, then it might\nbe a good idea to establish IANA registries for these namespaces,\ntoo.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP status code",
            "content": "> SIP and RTSP re-use a number of HTTP status codes, among other\n> properties. It is likely that they may need to or want to adopt other\n> HTTP status codes that emerge in the future. Thus, it is desirable that\n> the SIP and RTSP-specific status codes do not conflict with HTTP codes.\n> \n> One possible solution: Declare officially that HTTP will only use status\n> codes up to x49 (say) and leave others for private extensions, including\n> SIP and RTSP.\n\nA brief inspection of the specs makes me think that what's described\naren't what I'd think of as HTTP extensions, but rather protocols that bear\nthe same kind of relation to HTTP that HTTP had to MIME: they\nare borrowing a lot of details but doing something different.\n\n(You might be able to make a server that talked both on one port,\nbut it would be a bit of a hack.)\n\nHaving multiple net-ascii-style protocols share a space of response\ncodes could get sticky. Even if the numbers match, you might find\nthat the interpretation differed. (Issues like caching are\nmaking the meaning of HTTP/1.1 responses a bit more complex.)\n\n--\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: The significance of Cookie",
            "content": "> From http-wg-request@cuckoo.hpl.hp.com Wed Jul 23 14:08:36 1997\n> From: Steve Colby <scolby@arrowpoint.com>\n> To: \"http-wg@cuckoo.hpl.hp.com\" <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n> Subject: The significance of Cookies\n> Date: Wed, 23 Jul 1997 17:08:12 -0400\n> \n> The latest Netscape Communicator Release seems to have removed the option provided\n> in previous releases to disable the reception of cookies.  Does anyone know what Netscape's\n> stance on this is? \n\nI found it (after much looking). It's in the 'general' security preferences.\n\nJoe\n----------------------------------------------------------------------\nJoe Touch - touch@isi.edu    http://www.isi.edu/~touch/\nISI / Project Leader, ATOMIC-2, LSAM       http://www.isi.edu/atomic2/\nUSC / Research Assistant Prof.                http://www.isi.edu/lsam/\n\n\n\n"
        },
        {
            "subject": "Re: can't really be LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>We can't quite LAST CALL a document which has a technical\n>issue unresolved. The technical issue that's unresolved\n>is the limitation on accepting cookies while interacting\n>with the resource identified by the CommentURL annotating\n>a cookie.\n>\n>I will state my personal opinion, again, just in case there\n>is some additional support for it:\n>\n>I think that the complexity inherent in \"CommentURL\" makes\n>it suspect, and that the simplest thing to do is to remove\n>it. If there is no CommentURL, then you don't need a policy\n>for accepting cookies while interacting with it.\n>\n>Too much icing on the cookie, just say no.\n\nThe criteria by which a UA might accept particular cookies in\nSet-Cookie or Set-Cookie2 headers, and might include particular cookies\nin Cookie headers, are in the specification.  There is no MUST for\nacceptance of cookies, nor for sending them.  For a URL obtained via a\ncommentURL, the UA could apply those criteria, and perhaps include cookies\nin a Cookie header when requesting that URL.  It could apply those criteria,\nand perhaps accept cookies in any Set-Cookie or Set-Cookie2 header that\naccompanies the server's reply, all dependent on the context, the manner\nin which the UA has implemented cookie support, and any configuration or\nrun-time options set by the user.  There is nothing about the commentURL\nwhich requires holding up LAST CALL, unless you want to change the critera,\nrather than just making suggestions or adding words of caution in the\ncomments/explanations.  Just say yes.\n\nThe FQDN issue is another matter.  That's a change in the critera.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n>Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n>  > The commentURL points to a resource which can be requested by\n>  > a UA under a variety of circummstances, not just in conjunction with\n>  > an assessment of whether to accept a new (not previously accepted or\n>  > expired) cookie, or an assessment of whether to delete a previously\n>  > (e.g., tentatively) accepted cookie.  A link for that URL might have\n>  > been bookmarked, or added to some document.  The server thus must be\n>  > prepared to have Cookie headers in requests for those resources, and\n>  > should act on them as it would for other requests sent by UAs.  If\n>  > you impose restrictions for such URLs, you are creating complications,\n>  > not alleviating them.\n>\n>Not really.  The server has to be prepared not to receive the cookie\n>back in any event.  Also, the complications I was trying to alleviate\n>were on the UA side, not the server side.\n\nThat sounds as though we agree but haven't both realized it yet.\nThis is almost entirely a UA issue.  The server may or may not act\non any cookies the UA sends, and the UA may or may not send any, which\nis already encompassed by the specs as they presently stand.  As far as\nmy UA is concerned, if it has a cookie that qualifies for inclusion in\na Cookie header for the commentURL based on the specs, and isn't\ndisqualified based on any additional configuration or run-time options\nset by the user, and for example is being used to maintain a preference\nfor documents that are suitable for a braille interface or speech\nsynthesizer, I don't see why it shouldn't be included, with the user,\nof course, crossing his/her fingers that the server respects it and\ndoesn't return glitz and blitz for GUIs.\n\n\n>  > In those cases where the UA is sending the request in conjunction\n>  > with an assessment of whether to accept a new cookie or delete an existing\n>  > one, I still see no compelling reason to omit a Cookie header from the\n>  > request, if there are cookies which the UA would normally send to that\n>  > site.  I also think it is desireable to send at least the cookie for which\n>\n>Think again about that.  Suppose site a.com sent cookie X=1 previously.\n>I go to a.com again, send cookie X=1, get a new cookie, X=2, and pop up\n>a cookie inspection dialog.  CommentURL directs me to a URL on a.com.\n>I certainly don't want to send X=2:  that's the cookie I just got, and\n>I'm trying to decide whether to accept it.  But I just sent X=1.  Does\n>it help to send it again in this context?\n\nIf the filters imposed by the basic specs are not adequate in\nthis case, then they are inadequate, period.  I think it is desireable\nto send at least one cookie, which in the case of a commentURL being\nrequested in an \"assessment\" context must have been in a Set-Cookie2\nheader, and thus will be sent with a $VERSION leader, signaling a\nmodern implementation.  If no cookie qualifies after applying the\nspec's filters and any additional contraints configured or imposed\nat run-time by the user, then none will be sent.  Life always has\nits hard knocks. :)\n\n\n>  > a comment is being requested, to ensure that at least one cookie is\n>\n>On the contrary.  I haven't decided whether I want to accept that cookie.\n>Why should I send it back before accepting it?\n\nThis is mere semantics.  You've tentatively accepted it, or you\nwouldn't be seeking the body of a commentURL, and will decide whether to\ndiscard it or not after receiving that body, hopefully with a charset\nand language that allows you to read it and make an informed decision.\n\n\n>[...]\n>I remain unpersuaded.  It would help me if you would comment on my\n>proposed words and/or, especially, if you would propose words you\n>consider suitable in their stead.\n\nI did send alternate wording, as did Dave Morris.\n\nHere' my effort at a synthesis that might yield consensus:\n\nA potentially confusing situation exists if this sequence occurs:\n  - the user agent receives a cookie that contains a CommentURL\n   attribute;\n - the user agent's cookie inspection interface presents a dialog\n    that allows the user to follow the CommentURL link; and,\n  - when the user follows the CommentURL link, the origin server (or\n   another server, via other links in the returned content) returns\n   another cookie.\nThe user agent MAY discard any cookie received in this context\nthat the user has not, through some user agent mechanism, deemed\nacceptable.\n\n\nNote that the user agent MAY discard *any* cookie it receives in *any*\ncontext, so this comment/explanation doesn't change anything that is in\nthe -02 draft, and could not validly be construed as a reason for postponing\nLAST CALL.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": ">>>>> \"Henry Sanders (Exchange)\" <henrysa@EXCHANGE.MICROSOFT.com>:\n\nHS> This proposal sounds good, except for the bit about having a\nHS> Compliance:  header on any request. I really dislike that part -\nHS> it's just one more thing for the server to have to check for and\nHS> deal with on every request. What's the rationale behind that? I'd\nHS> much prefer to see it specifed as only applicable to OPTIONS.\n\n  I agree; I'm not convinced that piggybacking the Compliance on other\n  methods would be usefull.  In particular, would such a header\n  provide information on what features are available for _any_ method,\n  or only the one to which the Compliance header was attached?\n\n  For the time being (that is, the next version of HTTP/1.1) I think\n  that introducing Compliance (and possibly Non-Compliance) as a\n  header to be used with OPTIONS is sufficient.  If we do not define\n  any behaviour for it with other methods, future versions of HTTP can\n  attempt to do so based on experience with the many new aspects we\n  are defining.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: can't really be LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "   Date: Wed, 23 Jul 1997 12:39:04 PDT\n   From: Larry Masinter <masinter@parc.xerox.com>\n   Organization: Xerox PARC\n\n   We can't quite LAST CALL a document which has a technical\n   issue unresolved. The technical issue that's unresolved\n   is the limitation on accepting cookies while interacting\n   with the resource identified by the CommentURL annotating\n   a cookie.\n\n   I will state my personal opinion, again, just in case there\n   is some additional support for it:\n\n   I think that the complexity inherent in \"CommentURL\" makes\n   it suspect, and that the simplest thing to do is to remove\n   it. If there is no CommentURL, then you don't need a policy\n   for accepting cookies while interacting with it.\n\n   Too much icing on the cookie, just say no.\n\nI disagree.\n\nEffectively, showing the CommentURL merely involves going to a different\npage, probably with a different set of cookies.  I don't see what's\nreally hard about it.  Is there something I'm missing?\n\nIt is likely that at some point in the future I will implement a UA\nwhich will support the CommentURL, and I would like the CommentURL\nto be available.\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": "Scott Lawrence wrote:\n> \n> >>>>> \"Henry Sanders (Exchange)\" <henrysa@EXCHANGE.MICROSOFT.com>:\n> \n> HS> This proposal sounds good, except for the bit about having a\n> HS> Compliance:  header on any request. I really dislike that part -\n> HS> it's just one more thing for the server to have to check for and\n> HS> deal with on every request. What's the rationale behind that? I'd\n> HS> much prefer to see it specifed as only applicable to OPTIONS.\n> \n>   For the time being (that is, the next version of HTTP/1.1) I think\n>   that introducing Compliance (and possibly Non-Compliance) as a\n>   header to be used with OPTIONS is sufficient.  If we do not define\n>   any behaviour for it with other methods, future versions of HTTP can\n>   attempt to do so based on experience with the many new aspects we\n>   are defining.\n> \nI agree, it opens too many cans of worms.  As a clarification to scott's\nprevious remark about Jeff's changes to my OPTIONS spec, I agreee with \nthose changes, to make the OPTIONS use a header instead of the request\nbody.\n\nBy using the header, we have made it possible for this issue to\ncome up, using the compliance with a non options request.\n\nThe original motivation I had to write the OPTIONS clarification/spec\nwas to allow a simple, required mechanism to detect extensions in\nHTTP/1.1\nwhich would be in the spec.  I specifically avoided making it very \ncomplicated to avoid the difficulties that PEP has faced.\nPEP is very flexible and powerful, and I beleive, a \"good thing\".\nHowever that complexity is something that has kept it from \nbeing included in HTTP/1.1. So, rather than rush PEP or cobble\ntogether a subset of it, it seems that a baseline OPTIONS is\nneeded in the protocol, as a MUST.\n\nSine PEP can do what the compliance header w/ non options method can\ndo, I beleive we should leave that alone, and not define \ncompliance: header behavior with non OPTIONS messages.\n\nAdditionally, excluding the goals of PEP, the reasons I can think\nof for using compliance: with a non OPTIONS method can be addressed\nby existing headers such as upgrade:...\n\n\n-- \n-----------------------------------------------------------------------------\nJosh Cohen      Netscape Communications Corp.\nNetscape Fire Department                #include<disclaimer.h>\nServer Engineering\njosh@netscape.com                         \nhttp://people.netscape.com/josh/\n-----------------------------------------------------------------------------\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": "Scott Lawrence writes:\n\n  On the question of proxies modifying the responses, perhaps it would\n  be better for a proxy that does not implement a particular feature\n  to add a non-compliance indication without removing the original\n  assertion; this might enable a client to detect the true situation.\n\nOn the face of it, this is a reasonable proposal.  It certainly seems\npreferable to have the proxy add information about its non-compliance\n(using a header) than to have the proxy try to edit a response,\nespecially if it might not understand the semantics of the response.\n\nBut I started to think about this, and after struggling with it for a\nwhile (this is actually my third attempt to compose a response to your\nmessage), I'm convinced that we should not be giving proxies too much\nroom to be non-compliant.  I.e., the proper thing for a proxy to do is\nnot say \"I don't comply\", but (if it doesn't support some feature) to\neither forward the request and the response verbatim, as a tunnel\nwould, or to reject the request as being unacceptable (e.g., because it\nviolates a security policy).\n\nWe've tried fairly hard so far to define the protocol in such a way\nthat a naive proxy won't screw things up.  E.g., we have a \"Connection\"\nheader to prevent the forwarding of a header that should only be\nforwarded by a proxy that understands it.  And we have a variety of\ncache-control mechanisms that disable caching in circumstances where a\nnaive proxy cache cannot be expected to do the right thing.\n\nHere's a list of the things that a proxy implementation could be\nnon-compliant with:\n\nMethods\nIf a proxy doesn't support a method, it should\nreject a request with that method, using\n405 (Method Not Allowed).\n\nRequest headers, not listed in Connection\nNormally, a non-compliant proxy would simply forward;\nit could also reject the request.\n\nRequest headers, listed in Connection\nNormally, a non-compliant proxy would simply delete;\nit could also reject the request.\n\n    Note that one could use the proposed OPTIONS redesign, plus the\n    Max-Forwards + Allow/Public headers, to discover which server on a\n    chain is rejecting a request with a 405 (Method Not Allowed).\n    Also, the Via header should provide the same information, no\n    matter what reason causes a proxy to reject a request.\n\nStatus codes\nNormally, a non-compliant proxy would simply forward\n(and would not cache the response; see 13.4)\n\nResponse headers, not listed in Connection\nNormally, a non-compliant proxy would simply forward\n\nResponse headers, listed in Connection\nNormally, a non-compliant proxy would simply delete\n\nBehavioral rules\nFor example, \"if you see both X and Y, then you\nmust do Z\"\n\nAs far as I can tell, it's only the last category (behavioral rules)\nwhich could cause problems, given the basic rule that a proxy should\nbecome transparent when it doesn't know what it's supposed to do.  In\nthis case, I think the right approach is to design the protocol so that\nerrors of omission (i.e., the proxy sees both X and Y, but fails to do\nZ) are not fatal.\n\nI could still see supporting the inclusion of Non-Compliance header,\nbut if so, I think it should be significantly simplified, and should\nbasically serve an advisory function (kind of like Warning, but with a\nmore specifically defined syntax).  So I would suggest omitting the\n\"passed\"/\"dropped\" distinction; if a proxy doesn't comply, it should\nalways forward messages, except that it can always reject a message\noutright (e.g., for security reasons).  [If a proxy rejects a response\nmessage, I guess it has to construct some sort of error status to send\nto the client in place of the original response.]\n\nAlso, your BNF\n\n  Non-Compliance =\n        \"Non-Compliance\" \":\" proxy-host 1#non-compliance-option\n\nhas a small problem, because section 4.2 of RFC2068 (Message Headers)\nsays:\n\n   Multiple message-header fields with the same field-name may be\n   present in a message if and only if the entire field-value for that\n   header field is defined as a comma-separated list [i.e., #(values)].\n   It MUST be possible to combine the multiple header fields into one\n   \"field-name: field-value\" pair, without changing the semantics of the\n   message, by appending each subsequent field-value to the first, each\n   separated by a comma.\n\nThis means that there is no legal way to use your proposed BNF\nif two proxies need to add a Non-Compliance header.\n\nHow about this syntax:\n\n  Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\n\n        proxy-host = host [ \":\" port ]\n\nnon-compliance-option = compliance-option \"@\" proxy-host\n\nThis just adds the hostname to each unsupported option listed in the\nreceived Compliance header, so we can use a simple comma-separated list\nof items.  The downside is that the hostname may be repeated several\ntimes (if it fails to comply with several options), but my hope is that\nthis is likely to be a short list!\n\nAnyway, my full revision of your proposal is:\n\n(1) add this new section:\n\n14.QQQ Non-Compliance\n\n   A proxy server SHOULD add this response-header to a response\n   containing a Compliance header if the proxy does not implement one\n   or more of the options described in the Compliance header.\n\n  Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\n\n        proxy-host = host [ \":\" port ]\n\nnon-compliance-option = compliance-option \"@\" proxy-host\n\n   A non-compliance-option listed in a Non-Compliance response-header\n   field indicates that the proxy server named by the proxy-host value\n   does not support the listed compliance-option.  The set of\n   non-compliance options SHOULD be a subset of the compliance-options\n   listed in a Compliance header field of the forwarded message.\n\n      Note: because the proxy-host value is not authenticated,\n      this is only for advisory purposes (e.g., for debugging).\n\n   A proxy MUST NOT delete a Non-Compliance header that it has\n   received from another server.\n\n(2) In section 9.2 (OPTIONS), replace this:\n\nIf the OPTIONS request passes through a\n   proxy, the proxy MUST edit the response to exclude those options\n   which apply to a proxy's capabilities and which are known to be\n   unavailable through that proxy.\n\nwith\nIf the OPTIONS request passes through a\n   proxy, the proxy SHOULD add a Non-Compliance header field (see\n   section 14.QQQ) to the response, to list those options that apply to\n   a proxy's capabilities and that are known to be unavailable through\n   that proxy.\n\n(3a) section 14.7 (Allow) says \n\n   A proxy MUST NOT modify the Allow header field even if it does not\n   understand all the methods specified, since the user agent MAY have\n   other means of communicating with the origin server.\n\n[Note: that MAY should be a \"might\"; it's not normative.]\n\nHowever, section 14.35 (Public), which is otherwise analogous to Allow, says:\n\n   This header field applies only to the server directly connected to\n   the client (i.e., the nearest neighbor in a chain of connections). If\n   the response passes through a proxy, the proxy MUST either remove the\n   Public header field or replace it with one applicable to its own\n   capabilities.\n\nThis discrepancy seems unnecessary; the text also conflicts with\nthe new proposal for OPTIONS,\n    http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0256.html\nwhich says \"If an OPTIONS request includes a Host header, this is the\nintended destination of the OPTIONS method\" ... so I don't think\nwe need to restrict Public to the directly-connected server, especially\nif we aren't so restricting Allow.\n\nI suggest replacing that paragraph with:\n\n   A proxy MUST NOT modify the Public header field even if it does not\n   understand all the methods specified, since the user agent might have\n   other means of communicating with the origin server.\n\n(3b) If you don't like the proposal in 3a, try the reverse:\n\nChange this paragraph in 14.7 from\n\n   A proxy MUST NOT modify the Allow header field even if it does not\n   understand all the methods specified, since the user agent MAY have\n   other means of communicating with the origin server.\n\nto\n\n   A proxy MUST remove methods from an Allow header field if it\n   does not support the use of those methods for the resource\n   identified by the Request-URI.\n\nChange this paragraph in 14.35 from\n\n   This header field applies only to the server directly connected to\n   the client (i.e., the nearest neighbor in a chain of connections). If\n   the response passes through a proxy, the proxy MUST either remove the\n   Public header field or replace it with one applicable to its own\n   capabilities.\n\nTo\n\n   A proxy MUST remove methods from a Public header field if it\n   does not support the use of those methods.\n\nComments?  Anyone have strong feelings about 3a vs. 3b?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Additional LAST CALL issue",
            "content": "As I have been reminded, the following issues are now\nREADY FOR LAST CALL:\n \n        RANGE-ERROR\n        CLARIFY-NO-CACHE\n        STATUS100\n\nb and are so noted on the Issues list\n   http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n\nwith links to the Proposed Resolutions. If you have\nany comments on these issues, please speak up as soon\nas possible, so that we might close them in time to\nhave the resolutions appear in the 7/30 Internet Draft.\n\nThanks,\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: RETRYAFTER HTTP working group issue..",
            "content": "Jim,\n\nI would propose the insertion of one sentence after the first sentence in\n14.38.\nBTW, this does highlight the fact that it hasn't been clarified whether\nRetry-After with a 503 is a MUST, SHOULD or MAY. Given it currently says\n\"can\", I'd say MAY, which I've also reflected in the wording below:\n\n\n14.38 Retry-After\n\n   The Retry-After response-header field MAY be used with a 503 (Service\n   Unavailable) response to indicate how long the service is expected to\n   be unavailable to the requesting client. This field MAY also be used\n   with any 3xx (Redirection) response to indicate the minimum time the\n   user-agent should wait before issuing the redirected request. The\n   value...\n\nBob\n\nAt 01:47 PM 16/07/97 -0700, Jim Gettys wrote:\n>Could you please draft exact wording changes to the HTTP/1.1 document \n>to resolve this issue as soon as possible, or at least could you\n>let me know that you can't spend the time?  We're working had on\n>the HTTP/1.1 specification for Munich.\n>\n>It needs to say exactly for which status codes this should apply.\n>Thanks,\n>Jim Gettys\n>\n________________________________________________________\nNotice: This contribution is the personal view of the \nauthor and does not necessarily reflect the technical nor \ncommercial direction of British Telecommunications plc.\n________________________________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": ">>>>> \"JM\" == Jeffrey Mogul <mogul@pa.dec.com> writes:\n\nJM> How about this syntax:\n\nJM>   Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\nJM>         proxy-host = host [ \":\" port ]\nJM> non-compliance-option = compliance-option \"@\" proxy-host\n\nJM> This just adds the hostname to each unsupported option listed in the\nJM> received Compliance header, so we can use a simple comma-separated list\nJM> of items.\n\n  Looks fine.\n\nJM> Anyone have strong feelings about 3a vs. 3b?\n\n  Where\n    (3a) ~= Proxies MUST NOT modify Allow or Public headers\n    (3b) ~= Proxies MUST modify both Allow and Public headers to\n            reflect what is possible through this proxy.\n\n  Either is better than treating the two headers differently, but I\n  strongly prefer 3a; I believe that alternative, combined with the\n  information in Compliance and Non-Compliance headers, will provide a\n  useful set of information to someone trying to debug why a given\n  request is not working, and use fewer requests to get what they\n  need.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: ID on RFC 2069 issue: digest reques",
            "content": "Some comments on the draft below:\n\n1.  It is factually inaccurate to say that this proposed change\naddresses either an interoperability problem or a security problem.\nThe change may have virtues (like saving bandwidth) but it has no\neffect on security.  This has been re-iterated here many times, but I\nwill do it once more: Under the current draft, if the recipient\nrequires the message body be accompanied by a digest, it must reject\nany message without the digest and send an appropriate error message.\nSome bandwidth has been wasted in sending a message that was rejected,\nbut security has not been comprimised.\n\n2. Nevertheless, the suggested change is probably a good one, as it\ndoes does simplify the issue of sender/receiver negotiation on whether\na digest is required.  Rejecting a message is not a very efficient method\nof negotiation.  \n\n3. Any change along the lines proposed should make clear (as the ID\nbelow does not) that for a client or server to be compliant with\nthe specification it is not necessary to be capable of providing\ndigests for message bodies.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\nOn Tue, 22 Jul 1997, Scott Lawrence wrote:\n\n> \n>   A New Internet-Draft is available from the on-line Internet-Drafts\n>   directories.\n> \n>          Title     : HTTP/1.1 Message Digest Attribute Request\n>          Author(s) : S. Lawrence\n>          Filename  : draft-lawrence-digest-request-00.txt\n>          Pages     : 3\n>          Date      : 07/14/1997\n> \n>   This memo describes a security weakness in the current Proposed Standard\n>   for HTTP Digest Access Authentication, and proposes a change to that scheme\n>   to correct the deficiency.  The problem is that there is no mechanism for\n>   either party to indicate a requirement that messages be sent with an\n>   authentication digest.\n> \n>   ================\n> \n>   Since as I understand it, the plan is to issue a new document for\n>   HTTP Authentication which would supersede RFC 2069, I would like to\n>   get this considered as an improvement to Digest Access\n>   Authentication as a part of that.  This is a problem with\n>   interoperability found as a result of implementation experience, and\n>   as such, I believe, proper for consideration at this point in the\n>   process.\n> \n>   Since it isn't very long, I've attached it below.\n> \n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n> \n> \n> Internet Draft                                           Scott Lawrence\n> draft-lawrence-digest-request-00.txt              Agranat Systems, Inc.\n> Expires: December 1997                                    July 14, 1997\n> \n> \n>              HTTP/1.1 Message Digest Attribute Request\n> \n> Status of this Memo\n> \n>      This document is an Internet-Draft.  Internet-Drafts are working\n>      documents of the Internet Engineering Task Force (IETF), its\n>      areas, and its working groups.  Note that other groups may also\n>      distribute working documents as Internet-Drafts.\n> \n>      Internet-Drafts are draft documents valid for a maximum of six\n>      months and may be updated, replaced, or obsoleted by other\n>      documents at any time.  It is inappropriate to use Internet-\n>      Drafts as reference material or to cite them other than as\n>      ``work in progress.''\n> \n>      To learn the current status of any Internet-Draft, please check\n>      the ``1id-abstracts.txt'' listing contained in the Internet-\n>      Drafts Shadow Directories on ftp.is.co.za (Africa),\n>      nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n>      ds.internic.net (US East Coast), or ftp.isi.edu (US West Coast).\n> \n> 1. Abstract\n> \n>    This memo describes a security weakness in the current Proposed\n>    Standard for HTTP Digest Access Authentication, and proposes a\n>    change to that scheme to correct the deficiency.  The problem is\n>    that there is no mechanism for either party to indicate a\n>    requirement that messages be sent with an authentication digest.\n> \n> \n> draft-lawrence-digest-request-00.txt                            Page 2/4\n> \n> 2. The Problem\n> \n>    The Digest Authentication scheme specifies a mechanism (the\n>    'digest' attribute of the Authentication-Info and Authorization\n>    header fields) by which a digest of the message body and selected\n>    headers may be transmitted.  This provides a valuable means of\n>    protecting the message body from modification or replay attacks\n>    based on modifying either the message body or the protected\n>    headers, while preserving the authentication headers.\n> \n>    The mechanism is only valuable, however, if the message recipient\n>    can require that the digest attribute is present, but at present\n>    the authentication scheme does not provide a means to indicate that\n>    the digest is required.  There are two difficulties created by this\n>    lack, an interoperability problem and a security problem:\n> \n>    - The interoperability problem is that if one party (either client\n>      or server) wishes to require a digest for a particular HTTP\n>      request or response, the other party cannot be told of the\n>      requirement.  This leaves us with the situation that in order for\n>      such a requirement to work that all messages using the Digest\n>      Authentication scheme would need to use the message digest, even\n>      those for which it would not be required.  Because computing the\n>      digest is relatively expensive, this is undesirable.\n> \n>    - The security problem is that an attacker can remove the\n>      attribute, preserving the remainder of the authentication\n>      information, and modify the parts of the message the digest was\n>      meant to protect.\n> \n>    For example, a server implementation might provide for a resource\n>    attribute to require that Digest Authentication be used and a\n>    message digest supplied in order to submit a form.  This would be\n>    used to ensure that forms could be defined for which the flimsy\n>    protections of Basic authentication are not appropriate.  The\n>    requirement that the Digest Authentication scheme be used can be\n>    communicated to the client by sending a WWW-Authenticate header\n>    with 'digest' as the only acceptable scheme, but no mechanism is\n>    provided to require the message digest.\n> \n> \n> draft-lawrence-digest-request-00.txt                            Page 3/4\n> \n> 3. Solution\n> \n>    Attributes should be added to the WWW-Authenticate and\n>    Authorization headers to indicate that a message digest is required\n>    on the subsequent message.  The section numbers below refer to [RFC\n>    2069].\n> \n>    Note that this draft does not propose that support for Digest\n>    Access Authentication become a requirement for HTTP/1.1\n>    conformance.  It does add a requirement to the definition of the\n>    scheme if it is supported at all.\n> \n>    ================================================================\n>    in section 2.1.1:\n> \n>      WWW-Authenticate    = \"WWW-Authenticate\" \":\" \"Digest\"\n>                               digest-challenge\n> \n>      digest-challenge    = 1#( realm | [ domain ] | nonce |\n>                           [ opaque ] |[ stale ] | [ algorithm ] |\n>                           [ digest-required ] )\n>    ...\n>      digest-required     = \"digest-required\"\n>    ...\n> \n>    digest-required\n>    A flag, indicating that any request for the resource to which this\n>    response applies must include the 'digest' attribute in its\n>    Authorization header.\n> \n>    ================================================================\n>    in section 2.1.2:\n> \n>    Authorization       = \"Authorization\" \":\" \"Digest\" digest-response\n> \n>    digest-response     = 1#( username | realm | nonce | digest-uri |\n>                             response | [ digest ] | [ algorithm ] |\n>                             opaque | digest-required )\n> \n>    ...\n>      digest-required     = \"digest-required\"\n>    ...\n> \n>    digest-required\n>    A flag, indicating that the response to this request must include\n>    the 'digest' attribute in its Authentication-Info header.\n> \n>    ================================================================\n> \n>    in section 3.3, paragraph 4:\n> \n>    The discussion of attacks based on removing the \"digest\" field of\n>    the Authentication-Info header can be removed; the remainder is\n>    still correct.\n> \n> \n> draft-lawrence-digest-request-00.txt                             Page 4/4\n> \n> 4. Security Considerations\n> \n>    This entire draft is about security considerations.\n> \n> 5. Author's Addresses\n> \n>    Scott Lawrence\n>       Agranat Systems, Inc.\n>       1345 Main St.\n>       Waltham, MA 02154\n>    Phone:  +1-617-893-7868\n>    Fax:    +1-617-893-5740\n>    Email:  lawrence@agranat.com\n> \n> 6. References\n> \n>    [RFC 2068]\n>        R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee.\n>        \"Hypertext Transfer Protocol -- HTTP/1.1.\"\n>        RFC 2068,\n>        U.C. Irvine, DEC, MIT/LCS,\n>        January 1997.\n> \n>    [RFC 2069]\n>        J. Franks, P. Hallam-Baker, J. Hostetler, P. Leach,\n>        A. Luotonen, E. Sink, and L. Stewart.\n>        \"An Extension to HTTP : Digest Access Authentication\"\n>        RFC 2069,\n>        Northwestern University, CERN, Spyglass Inc., Microsoft Corp.,\n>        Netscape Communications Corp., Spyglass Inc., Open Market Inc.,\n>        January 1997.\n> \n> \n> \n> \n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "Jeffrey Mogul:\n>\n>I realize that some people were opposed to the introduction of qvalues\n>on Accept-Encoding.  However, during today's editorial-group discussion\n>of the CONTENT-ENCODING issue, we realized that the syntax for the\n>various Accept-* request headers in RFC2068 is almost, but not quite,\n>uniform, and we reached a tentative agreement that it might be a good\n>idea to have all of the Accept-* request headers (Accept-Range is a\n>response header) have similar syntaxes.\n\nWell, I guess I disagree with your tentative agreement about what\nmight be a good idea.\n\n>As I said in my previous message, introducing qvalues for\n>Accept-Encoding won't work \"if any existing servers or proxies would\n>choke on a qvalue in an Accept-Encoding header.\"\n>But (so far) nobody has asserted than this is an actual problem.\n\nI feel that the burden of proof that adding q values does\n*not* introduce new problems is entirely on your side.  You need to\nshow that this does not break or disable existing implementations.\n\nAt this stage in the standards process, we are supposed to be fixing\nproblems based on implementation experience.  For me, this means that\ncompatibility considerations *always* outweigh considerations of\ncosmetic uniformity.  So you'd better show that the compatibility\nconsiderations are absent.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: CONTENTENCODING: FIXED revised proposed wordin",
            "content": "Jeffrey Mogul:\n>\n\nSome nitpicks:\n\n>(1) If the content-coding is one of the content-codings listed\n>in the Accept-Encoding field, then it is acceptable. (Note that,\n>as defined in section 3.9, a qvalue of 0 means \"not acceptable\".)\n\nThis is slightly self-contradictory.\n\n>(2) The special \"*\" symbol in an Accept-Encoding field matches\n>any available content-coding.\n\n..except those listed explicitly in the header field.\n\n\n>    If no Accept-Encoding field is present in a request, the server MAY\n>    assume that the client will accept any content coding.  In this\n>    case, if \"identity\" is one of the available content-codings, then\n>    the server SHOULD use the \"identity\" content-coding.\n\nThis SHOULD was not present in 2068, and I don't think adding it is a\ngood idea.  A server which knows that a legacy client accepts an\nencoding (e.g. by looking at the user-agent field) should be\nencouraged to send content in this encoding.\n\n\n>(4) In section 14.9 (Cache-Control), add\n>                          | \"no-transform\"\n>to the BNF for cache-request-directive.\n\nAre we allowed to make such an addition at this point in the standards\nprocess?\n\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": "Scott Lawrence wrote:\n>   For the time being (that is, the next version of HTTP/1.1) I think\n>   that introducing Compliance (and possibly Non-Compliance) as a\n>   header to be used with OPTIONS is sufficient.  If we do not define\n>   any behaviour for it with other methods, future versions of HTTP can\n>   attempt to do so based on experience with the many new aspects we\n>   are defining.\n> \nJosh Cohen wrote:\n    Sine PEP can do what the compliance header w/ non options method\n    can do, I beleive we should leave that alone, and not define\n    compliance: header behavior with non OPTIONS messages.\n\nI believe that the text I proposed in\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0256.html\nwhich says:\n    A compliance header MAY appear on any message, but is\n    normally used with the OPTIONS request (see section 9.2).\nand which never says that a server needs to reply to a Compliance\nheader, except in an OPTIONS method, is the right approach.\n\nThis wording makes it clearly optional on non-OPTIONS methods;\ni.e., a server doesn't even have to look for it.\n\nOn the other hand, I think it is generally a mistake to define\ndifferent behavior for a header (especially an advisory one,\nsuch as Compliance) based on which method it is used with.\nVery few of the other headers are method-specific.  (In fact,\nI just noticed that Max-Forwards is currently defined so\nthat it cannot be used with OPTIONS; here's an example of\nhow such a prohibition causes trouble, since if we adopt\nJosh's proposal, we need to change this.)\n\nSo I think we should at least consider how Compliance could\nbe used with non-OPTIONS methods, and then write the spec\nfor Compliance so that it's behavior is method-neutral.\n\nThis is slightly different from saying that we need to\nencourage its use with non-OPTIONS methods, and I guess\nI should simply remove the example of using Compliance\nwith a GET method, to avoid confusion.\n\n    Additionally, excluding the goals of PEP, the reasons I can think\n    of for using compliance: with a non OPTIONS method can be addressed\n    by existing headers such as upgrade:...\n\nUpgrade signals a switch between protocols.  I don't think it\noverlaps much with Compliance, which is asking about what is\nsupported in the current protocol.  I.e., if you do OPTIONS,\nthen Upgrade, then OPTIONS, you would presumably get\na different Compliance response for the second OPTIONS request.\nSimilarly, if you do GET+Compliance, then Upgrade, then\nanother GET+Compliance, then you might also get a different\nCompliance response on the second GET.\n\nAlso, note that \"The Upgrade header field only applies to the\nimmediate connection.  Therefore, the upgrade keyword MUST be\nsupplied within a Connection header field ...\"  I.e., Upgrade\nis hop-by-hop (as is the HTTP/1.1 version number in the request\nmethod line and the response status line).  But Josh has come\nup with a simple and compelling design for making OPTIONS\nend-to-end, and so I think Compliance also needs to be end-to-end.\nThat is, I don't think that Upgrade can tell a client whether\na particular option is actually supported end-to-end, but\nCompliance can.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "Dave Kristol:\n>\n[...]\n>Does this wording express it adequately?:\n>\n>If the user agent allows the user to follow the [CommentURL] link [as\n>part of a cookie inspection user interface], it should neither send nor\n>accept a cookie until the user has completed the inspection.\n\nI think the approach to solving this problem is wrong: the burden of\nensuring that the commentURL mechanism does not lead to\nuser-unfriendly or recursive situations should be on the server side.\n\nI propose something like this:\n\n Servers SHOULD ensure that the user can visit the information pointed\n to by the commentURL without causing the user agent to receive\n additional Set-Cookie2 headers.  User agents SHOULD guard against the\n entering of infinite loops due to the commentURL mechanism, and MAY do\n this by disabling cookie processing when the commentURL is visited.\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "If you merely look at the specifications, there is enough\nevidence that HTTP/1.0 had intended the \"accept*\" headers\nto be consistent and that the distinctions in HTTP/1.1 were\nmerely editorial oversight because different people wrote\ndifferent parts. From that point to not require a burden\nof proof in order to reinstate consistency.\n\nIf there's evidence that this would cause some interoperability\ndifficulty, then we should certainly consider this carefully,\nbut we're otherwise on good grounds procedurally for\nproceeding.\n\nLarry\n(as wg chair)\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": "    >As I said in my previous message, introducing qvalues for\n    >Accept-Encoding won't work \"if any existing servers or proxies would\n    >choke on a qvalue in an Accept-Encoding header.\"\n    >But (so far) nobody has asserted than this is an actual problem.\n    \n    I feel that the burden of proof that adding q values does *not*\n    introduce new problems is entirely on your side.  You need to show\n    that this does not break or disable existing implementations.\n\nAs stated, this would be a clearly infeasible burden of proof for ANY\nchange to the protocol, since nobody has an exhaustive list of\nexisting implementations.  One could just as easily argue that\nthe same requirement should apply to your proposed (and accepted)\nchange to Accept-Charset, draft-holtman-http-wildcards-00.txt,\nwhich changes the RFC2068 syntax to allow wildcards.  You do make\na plausible argument that implementations of RFC2068 would ignore\n\"*\" in Accept-Charset, but I don't recall seeing the results\nof exhaustive testing.\n\nI repeat my request: if anyone has specific information that this\nis an actual problem, I would certainly withdraw the proposal.\n\nAt any rate, I have looked at the source code of a number of\nservers (including Apache), and tried a few experiments\n(including via an existing commercial proxy implementation).\nIt looks like if Accept-Encoding includes a content-coding\nwith a qvalue, it's simply ignored.  (The fact of the matter\nis that almost NO clients actually send Accept-Encoding today;\nHenrik thought that there were none in actual use, but I found\na few uses in our proxy traces, all from Lynx users.)\n\nBy the same logic as you used in draft-holtman-http-wildcards-00.txt,\nwhere you said it was OK to define \"*\" in Accept-Charset because\nRFC2068 servers would simply ignore it, it should be OK to introduce\nqvalues in Accept-Encoding if the servers also would simply ignore\nthe associated content-coding.\n\nHowever, it might be a good idea to include this note for\nclient implementors:\n\nNote: use of a qvalue with a content-coding defined\nin RFC2068 (\"compress\" or \"gzip\") may cause an\nRFC2068-compliant implementation to ignore the\ncontent-coding value.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP status code",
            "content": "Henning Schulzrinne:\n>\n>I brought this up at the San Jose meeting, but there never was a\n>satisfactory (official) answer, as far as I remember.\n>\n>SIP and RTSP re-use a number of HTTP status codes, among other\n>properties. It is likely that they may need to or want to adopt other\n>HTTP status codes that emerge in the future. Thus, it is desirable that\n>the SIP and RTSP-specific status codes do not conflict with HTTP codes.\n>\n>One possible solution: Declare officially that HTTP will only use status\n>codes up to x49 (say) and leave others for private extensions, including\n>SIP and RTSP.\n\nI don't know if there are any deployed implementations of SIP and RTPS\nalready.  If there are not, I would suggest using 4 digits for all SIP\nand RTPS codes not inherited from HTTP, e.g. 1xxx for SIP and 2xxx for\nRTPS.  HTTP-inherited codes could be 0xxx or simply xxx.\n\n>Henning\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Proposed resolution for IDEMPOTENT issu",
            "content": "Reference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#IDEMPOTENT\n\nThis section:\n\n   9.1.2 Idempotent Methods\n\n   Methods may also have the property of \"idempotence\" in that (aside\n   from error or expiration issues) the side-effects of  N > 0 identical\n   requests is the same as for a single request. The methods GET, HEAD,\n   PUT and DELETE share this property.\n\nis misleading.  The problem is that, in some context, what matters\nis not whether a given method is idempotent, but whether a sequence\nof operations is.\n\nHere's an example: assume that we have resources RA, RB, and RC.  Assume\nthat the sequence of operations the client wants to do is to copy\nthe value of RA into RC, then copy the value of RB into RA.  I.e.,\nat the end of this sequence, the client wants the final value of\nRC to be the initial value of RA.\n\nI'll use a simplified \"object oriented\" notation to describe this\nsequence of operations:\n\n(1)Step 1:x := RA.GET();\nStep 2:RC.PUT(x);\nStep 3: y := RB.GET();\nStep 4: RA.PUT(y);\n\nNow, if we simply carry out this sequence (1) exactly as written,\nit does what we want.  Moreover, if we repeat any step(s) in\nthe sequence, it still works.  E.g., this:\n\n(2)Step 1:x := RA.GET();\nStep 2:RC.PUT(x);\nStep 3:RC.PUT(x);\nStep 4:RC.PUT(x);\nStep 5:RC.PUT(x);\nStep 6:RC.PUT(x);\nStep 7: y := RB.GET();\nStep 8: RA.PUT(y);\n\ndoes the same thing as sequence 1.  This is because, as stated\nin RFC2068, PUT is idempotent ... but only when viewed in isolation.\n\nHowever, if we repeat the entire sequence 1:\n\n(3)Step 1:x := RA.GET();\nStep 2:RC.PUT(x);\nStep 3: y := RB.GET();\nStep 4: RA.PUT(y);\n\nStep 5:x := RA.GET();\nStep 6:RC.PUT(x);\nStep 7: y := RB.GET();\nStep 8: RA.PUT(y);\n\nwe now end up with RC containing the initial value of RB, not\nthe initial value of RA.  I.e., the sequence is not idempotent,\neven if the individual methods are.\n\nWhy does this matter to HTTP?  First, note that we cannot prevent a\nsituation where the client does not know for sure if the server has\ncarried out a certain request.  E.g., the server may have sent the\nresponse, but it might have been lost due to a communication failure.\nIn particular, when the server closes a persistent connection before a\nclient has received a response to a request in progress, it's\nimpossible for the client to be sure that the request wasn't actually\ncompleted.  (This is \"impossible\" in a provable, theoretical sense.)\n\nThis is why section 8.1.4 says that clients SHOULD automatically\nretry idempotent requests.  If you try it enough times, sooner\nor later you should get a definite response.\n\nSince we allow pipelining of requests, however, it's crucially\nimportant that a client not automatically retry an non-idempotent\nsequence of requests, since this could lead to the execution\nof something like sequence 3.\n\nProposed solution:\n\n(1) Replace section 9.1.2 with:\n\n   9.1.2 Idempotent Methods and Sequences\n\n   Methods may also have the property of \"idempotence\" in that (aside\n   from error or expiration issues) the side effects of  N > 0\n   identical requests is the same as for a single request. The methods\n   GET, HEAD, PUT, and DELETE share this property.  Also, the methods\n   OPTIONS and TRACE should have no side effects, and so are inherently\n   idempotent.\n\n   However, it is possible that a sequence of several requests is\n   non-idempotent, even if all of the methods executed in that sequence\n   are idempotent.  (A sequence is idempotent if a single execution of\n   the entire sequence always yields a result that is not changed by a\n   reexecution of all, or part, of that sequence.)  For example, a\n   sequence is non-idempotent if its result depends on a value that is\n   later modified in the same sequence.\n   \n   A sequence that never has side effects is idempotent, by definition\n   (provided that no concurrent operations are being executed on the\n   same set of resources).\n\n(2) In section 8.1.4 (Practical Considerations), replace\n\n   This means that clients, servers, and proxies MUST be able to recover\n   from asynchronous close events. Client software SHOULD reopen the\n   transport connection and retransmit the aborted request without user\n   interaction so long as the request method is idempotent (see section\n   9.1.2); other methods MUST NOT be automatically retried, although\n   user agents MAY offer a human operator the choice of retrying the\n   request.\n\nwith\n\n   This means that clients, servers, and proxies MUST be able to\n   recover from asynchronous close events. Client software SHOULD\n   reopen the transport connection and retransmit the aborted sequence\n   of requests without user interaction, so long as the request\n   sequence is idempotent (see section 9.1.2); non-idempotent methods\n   or sequences MUST NOT be automatically retried, although user agents\n   MAY offer a human operator the choice of retrying the request.\n\n(3) In the proposed resolution for the STATUS100 issue, at\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0208.html\n\nreplace\n\n   8.2.3 Automatic retrying of requests\n\n   If a user agent sees the transport connection close before it\n   receives a final response to its request, if the request method is\n   idempotent (see section 9.1.2), the user agent SHOULD retry the\n   request without user interaction.  If the request method is not\n   idempotent, the user agent SHOULD NOT retry the request without user\n   confirmation.  (Confirmation by user-agent software with semantic\n   understanding of the application MAY substitute for user\n   confirmation.)\n\nwith\n\n   8.2.3 Automatic retrying of requests\n\n   If a user agent sees the transport connection close before it\n   receives a final response to its request, if the request sequence is\n   idempotent (see section 9.1.2), the user agent SHOULD retry the\n   request sequence without user interaction.  If the request sequence\n   is not idempotent, the user agent MUST NOT retry the request\n   sequence without user confirmation.  (Confirmation by user-agent\n   software with semantic understanding of the application MAY\n   substitute for user confirmation.)\n\n(4) Note that the proposed resolution for the STATUS100 issue, at\n        http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0208.html\nincludes this, to be added at the end of 8.1.2.2 (Pipelining):\n\n   Clients SHOULD NOT pipeline requests using non-idempotent methods or\n   non-idempotent sequences of methods (see section 9.1.2).  Otherwise,\n   a premature termination of the transport connection may lead to\n   indeterminate results.  A client wishing to send a non-idempotent\n   request SHOULD wait to send that request until it has received the\n   response status for the previous request.\n\nIf clients obey this restriction, then they won't find themselves\nin a situation where they would be tempted to retry a non-idempotent\nsequence.  However, I think it is best to leave this as \"SHOULD NOT\npipeline a non-idempotent sequence, MUST NOT auto-retry if that fails\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Any objections to &quot;Acceptencoding: gzip, *;q=0&quot;",
            "content": ">If you merely look at the specifications, there is enough\n>evidence that HTTP/1.0 had intended the \"accept*\" headers\n>to be consistent and that the distinctions in HTTP/1.1 were\n>merely editorial oversight because different people wrote\n>different parts. From that point to not require a burden\n>of proof in order to reinstate consistency.\n\nSorry, that's nonsense.  The specifications are different because\nthe implementations are different.  Henrik and I spent well over a\nyear looking at different implementations in order to derive the\nsyntax for each header field -- had they been the same, we would have\nbeen overjoyed to use a single syntax.  The most painful thing about\nthe HTTP spec work was dealing with shortsighted designs and then\nhaving to explain them to others.\n\nThere is no q-value for Accept-Encoding because the following two\nfields are not equivalent\n\n   Accept-Encoding: x-gzip;q=1, x-compress;q=1\n\n   Accept-Encoding: x-gzip, x-compress\n\nfor any of the existing server implementations of HTTP/1.x.\nIt is therefore wrong for the specification to suggest that it would be.\n\nIf such a change is made, then there must also be a requirement (not a note)\nthat prevents the use of a q-value with any encoding other than identity.\nOtherwise, you will have changed the protocol.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: CONTENTENCODING: FIXED revised proposed wordin",
            "content": "Koen Holtman writes:\n    \n    >(1) If the content-coding is one of the content-codings listed\n    >in the Accept-Encoding field, then it is acceptable. (Note that,\n    >as defined in section 3.9, a qvalue of 0 means \"not acceptable\".)\n    \n    This is slightly self-contradictory.\n    \nI know, I realized when I wrote it that it's a little awkward.  I was\ntrying too hard to be brief.  How about:\n\n    (1) If the content-coding is one of the content-codings listed\nin the Accept-Encoding field, then it is acceptable, unless it\nis accompanied by a qvalue of 0.  (As defined in section 3.9, a\nqvalue of 0 means \"not acceptable\".)\n    \nNote that the section on Accept-Charset leaves this implicit, by\nusing the term \"acceptable\" without specifically defining it.\n\n    >(2) The special \"*\" symbol in an Accept-Encoding field matches\n    >any available content-coding.\n    \n    ..except those listed explicitly in the header field.\n\nSorry, my mistake.  This should be\n\n    (2) The special \"*\" symbol in an Accept-Encoding field matches\n    any available content-coding not explicitly listed in the header\nfield.\n    \nPerhaps someone should check to see if the specification for Accept\n(which allows \"*/*\" wildcards) needs the same correction.  It looks\nlike Accept-Charset (via draft-holtman-http-wildcards-00.txt) and\nAccept-Language say the right thing.\n    \n    >    If no Accept-Encoding field is present in a request, the server MAY\n    >    assume that the client will accept any content coding.  In this\n    >    case, if \"identity\" is one of the available content-codings, then\n    >    the server SHOULD use the \"identity\" content-coding.\n    \n    This SHOULD was not present in 2068, and I don't think adding it is a\n    good idea.  A server which knows that a legacy client accepts an\n    encoding (e.g. by looking at the user-agent field) should be\n    encouraged to send content in this encoding.\n\nThe point was to fix the mistake in RFC2068, which explicitly\nencourages a server to send \"any content encoding\" in this case.\nAnd Henrik has demonstrated that this causes garbage-on-the-screen.\n\nBut it makes sense to modify it slightly:\n\n    If no Accept-Encoding field is present in a request, the server MAY\n    assume that the client will accept any content coding.  In this\n    case, if \"identity\" is one of the available content-codings, then\n    the server SHOULD use the \"identity\" content-coding, unless\n    it has additional information that a different content-coding\n    is meaningful to the client.\n\n\"Additional information\" could be User-Agent, or the belief\nthat all relevant clients handle gzip, or whatever.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "HTTP and friends status code",
            "content": "A first drafty draft is at\n\nhttp://www.cs.columbia.edu/~hgs/sip/draft-ietf-http-status-00.txt\n\nComments are appreciated; this is a first attempt. Please let me know if\nI should submit this (a) at all, (b) as a WG submission.\n\nThanks.\n\nHenning\n-- \nHenning Schulzrinne         email: schulzrinne@cs.columbia.edu\nDept. of Computer Science   phone: +1 212 939-7042\nColumbia University         fax:   +1 212 666-0140\nNew York, NY 10027          URL:   http://www.cs.columbia.edu/~hgs\n\n\n\n"
        },
        {
            "subject": "Updated proposal for CONTENTENCODING issu",
            "content": "[incorporates the edits I made in response to Koen's comments.]\n\n(1) in section 3.5 (Content Codings), add this after the item\nfor \"deflate\"\n\n        identity        The default (identity) encoding; the use\n                        of no transformation whatsoever.  This\n                        content-coding is used only in the\n                        Accept-encoding header, and SHOULD NOT\n                        be used in Content-coding header.\n\n\n(2) Replace section 14.3 (Accept-Encoding) entirely with\n\n    14.3 Accept-Encoding \n\n    The Accept-Encoding request-header field restricts the set of\n    content-codings (defined in section 3.5) that are acceptable for\n    the response.\n    \n   Accept-Encoding  = \"Accept-Encoding\" \":\" \n     1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n    codings          = ( content-codings | \"*\" )\n    \n    Examples of its use are:\n    \n   Accept-Encoding: compress, gzip\n   Accept-Encoding:\n   Accept-Encoding: *\n   Accept-Encoding: compress;q=0.5, gzip;q=1.0\n   Accept-Encoding: gzip=1.0; identity=0.5; *;q=0\n    \n    A server tests whether a content-coding is acceptable, according\n    to an Accept-Encoding field, using these rules:\n(1) If the content-coding is one of the content-codings listed\nin the Accept-Encoding field, then it is acceptable, unless it\nis accompanied by a qvalue of 0.  (As defined in section 3.9, a\n        qvalue of 0 means \"not acceptable\".)\n(2) The special \"*\" symbol in an Accept-Encoding field matches\nany available content-coding not explicitly listed in the header\n        field.\n(3) If multiple content-codings are acceptable, then the\nacceptable content-coding with the highest non-zero qvalue is\npreferred.  \n(4) The \"identity\" content-coding is always acceptable, unless\nspecifically refused because the Accept-Encoding field includes\n\"identity;q=0\", or because the field includes \"*;q=0\" and does\nnot explictly include the \"identity\" content-coding.  If the\nAccept-Encoding field-value is empty, then only the \"identity\"\nencoding is acceptable.\n\n    If an Accept-Encoding field is present in a request, and if the\n    server cannot send a response which is acceptable according to the\n    Accept-Encoding header, then the server SHOULD send an error\n    response with the 406 (Not Acceptable) status code.\n\n    If no Accept-Encoding field is present in a request, the server MAY\n    assume that the client will accept any content coding.  In this\n    case, if \"identity\" is one of the available content-codings, then\n    the server SHOULD use the \"identity\" content-coding, unless\n    it has additional information that a different content-coding\n    is meaningful to the client.\n\nNote: If the request does not include an Accept-Encoding field,\nand if the \"identity\" content-coding is unavailable, then\npreference should be given to content-codings commonly\nunderstood by HTTP/1.0 clients (i.e., \"gzip\" and \"compress\");\nsome older clients improperly display messages sent with other\ncontent-encodings.  The server may also make this decision\nbased on information about the particular user-agent or\nclient.\n\n(3) In section 14.12 (Content-Encoding), replace\n\n   The Content-Encoding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n\nwith\n\n   The content-coding is a characteristic of the entity identified by\n   the Request-URI. Typically, the entity-body is stored with this\n   encoding and is only decoded before rendering or analogous usage.\n   However, a proxy MAY modify the content-coding if the new coding\n   is known to be acceptable to the recipient, unless the \"no-transform\"\n   Cache-control directive is present in the message.\n\n   If the content-coding of an entity is not \"identity\", then the\n   response MUST including a Content-Encoding entity-header (section\n   14.12) that lists the non-identity content-coding(s) used.\n\n   If the content-coding of an entity in a request message is\n   not acceptable to the origin server, the server SHOULD respond\n   with a status code of 415 (Unsupported Media Type).\n\n(4) In section 14.9 (Cache-Control), add\n                          | \"no-transform\"\nto the BNF for cache-request-directive.\n\n(5) In section 14.9.5 (No-Transform Directive), replace\n\n   Therefore, if a response includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\nwith\n\n   Therefore, if a message includes the no-transform directive, an\n   intermediate cache or proxy MUST NOT change those headers that are\n   listed in section 13.5.2 as being subject to the no-transform\n   directive.  This implies that the cache or proxy must not change any\n   aspect of the entity-body that is specified by these headers.\n\n[End of proposed changes]\n\n\n\n"
        },
        {
            "subject": "Updated proposal for OPTIONS issu",
            "content": "[incorporates various changes since the last message, including\n the Non-Compliance header, removal of the GET example, changes\n to Allow and Public, and allows Max-Forwards to be used with\n OPTIONS.]\n\n****** Note: two possible options for item (8) ******\n\nReference:\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/#OPTIONS</a>\n\nProblem statement:\nRFC2068 doesn't really say how to use OPTIONS to discover\nwhat an implementation supports.\n\nIt's essential to have a simple and reliable mechanism for\ndetecting support for extensions as a part of the core HTTP/1.1\nprotocol.\n\nOutline of proposed solution:\n \n    the URI '*' refers to the server, independent of any specific URI.\n   \n    The Host: header may be used to specify a named proxy or origin \n    server in a chained environment\n   \n    The Max-forwards: header may be used to specify a maximum number\n    of hops the request may be forwarded in a chained environment\n   \n    The Compliance: header may be used on requests to ask about\n    compliance, and on responses to assert compliance.\n\n    The Non-Compliance header allows proxies to indicate when\n    they fail to comply with something that the origin server\n    complies with.\n\n    We define a new IANA-registered namespace for compliance\n    assertions.\n\nProposed Solution:\n\n(1) In section 5.1.2, remove this:\n\n   If a proxy receives a request without any path in the Request-URI and\n   the method specified is capable of supporting the asterisk form of\n   request, then the last proxy on the request chain MUST forward the\n   request with \"*\" as the final Request-URI. For example, the request\n\n          OPTIONS <a href=\"http://www.ics.uci.edu:8001\">http://www.ics.uci.edu:8001</a> HTTP/1.1\n\n   would be forwarded by the proxy as\n\n          OPTIONS * HTTP/1.1\n          Host: www.ics.uci.edu:8001\n\n   after connecting to port 8001 of host \"www.ics.uci.edu\".\n\n(2) In section 9.2 (OPTIONS), replace:\n\n   Unless the server's response is an error, the response MUST NOT\n   include entity information other than what can be considered as\n   communication options (e.g., Allow is appropriate, but Content-Type\n   is not). Responses to this method are not cachable.\n\nwith\n\n   An OPTIONS request MAY include Compliance headers (see section 14.ZZZ)\n   that indicate the set of options the sender wants information\n   about.\n\n   Responses to OPTIONS are not cachable, unless caching is explicitly\n   allowed by the originating sender (see section 13.4).\n\n(3) In section 9.2 (OPTIONS), replace:\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server as a whole. A 200 response SHOULD\n   include any header fields which indicate optional features\n   implemented by the server (e.g., Public), including any extensions\n   not defined by this specification, in addition to any applicable\n   general or response-header fields. As described in section 5.1.2, an\n   \"OPTIONS *\" request can be applied through a proxy by specifying the\n   destination server in the Request-URI without any path information.\n\nwith\n\n   If the Request-URI is an asterisk (\"*\"), the OPTIONS request is\n   intended to apply to the server as a whole.  A 200 response SHOULD\n   include a Public header field (see section 14.35).  If the request\n   includes a Compliance header field, a 200 response SHOULD include a\n   Compliance header field, indicating the subset of the requested\n   Compliance options supported by the server as a whole.  The response\n   SHOULD include any other applicable general or response-header\n   fields.\n\n   If an OPTIONS request includes a Host header (see section 14.23),\n   this is the intended destination of the OPTIONS method.\n   Proxy servers MUST forward such a message until it reaches\n   the specified host.  If the specified host has more than\n   one `virtual server', the OPTIONS request applies to the\n   specified virtual server.\n   \n       Note: An OPTIONS request may also include a Max-Forwards header,\n       as described in section 14.31.  This allows the sender to select\n       the Nth proxy on a path, without knowing its hostname.\n\n(4) In section 9.2 (OPTIONS), replace:\n\n   If the Request-URI is not an asterisk, the OPTIONS request applies\n   only to the options that are available when communicating with that\n   resource.  A 200 response SHOULD include any header fields which\n   indicate optional features implemented by the server and applicable\n   to that resource (e.g., Allow), including any extensions not defined\n   by this specification, in addition to any applicable general or\n   response-header fields. If the OPTIONS request passes through a\n   proxy, the proxy MUST edit the response to exclude those options\n   which apply to a proxy's capabilities and which are known to be\n   unavailable through that proxy.\n\nwith\n\n   If the Request-URI is not an asterisk, the OPTIONS request applies\n   only to the options that are available when communicating with that\n   resource.  A 200 response SHOULD include an Allow header field (see\n   section 14.7).  If the request includes a Compliance header field, a\n   200 response SHOULD include a Compliance header field, indicating\n   the subset of the requested Compliance options supported by the\n   server as a whole.  If the subset is empty, the response SHOULD\n   include a Compliance header with an empty field-value.  The response\n   SHOULD include any other applicable general or response-header\n   fields.\n\n      Note: if an OPTION request contains a Compliance header, and the\n      response does not, the response may have been generated by\n      RFC2068-compliant implementation, which would not support\n      Compliance.  In this case, it is not possible to infer that the\n      sender fails to support all of the options listed in the\n      Compliance header of the request.\n\n   If the OPTIONS request passes through a\n   proxy, the proxy SHOULD add a Non-Compliance header field (see\n   section 14.QQQ) to the response, to list those options that apply to\n   a proxy's capabilities and that are known to be unavailable through\n   that proxy.\n\n(5) In 14.31 (Max-Forwards), replace:\n\n   Each proxy or gateway recipient of a TRACE request containing a Max-\n   Forwards header field SHOULD check and update its value prior to\n   forwarding the request.\n\nwith\n\n   Each proxy or gateway recipient of a TRACE or OPTIONS request\n   containing a Max-Forwards header field SHOULD check and update its\n   value prior to forwarding the request.\n\n(6) New section\n\n14.ZZZ Compliance\n\n    The Compliance general header field lists a set of options\n    that may or may not be supported by a server.  In a request\n    message, this header lists the set of options that a client\n    wishes to know about.  In a response message, this header\n    lists the set of options that the server complies with.\n    \n    A compliance header MAY appear on any message, but is\n    normally used with the OPTIONS request (see section 9.2).\n    \n    Compliance = \"Compliance\" \":\" (\"*\" | *(compliance-option))\n\ncompliance-option = compliance-namespace \"=\"\noption-item [ option-params ]\n\ncompliance-namespace = token\n\noption-item = token | quoted-string\n\noption-params = 1#( \";\" option-param)\n\noption-param = \"cond\" | \"uncond\" | token | quoted-string\n\n    A Compliance header field with the field-value of \"*\" MAY\n    be used in a request, to ask about all options complied\n    with by the recipient.  This field-value MUST NOT be used\n    in a response.\n\n    The compliance-namespace is used to select from one of several\n    namespaces for compliance options.  The option-item is used\n    to specify one or more options within a namespace.  \n\n    The Internet Assigned Numbers Authority (IANA) acts as a registry\n    for compliance-namespace tokens. Initially, the registry contains\n    the following tokens:\n\n\"RFC\"Compliance is with an RFC, specified by an RFC number.\nFor example, \"rfc=1945\".\n\n\"HDR\"Compliance is with a named HTTP header.  For example,\n\"HDR=Authorization\".  There is no IANA registry for\nHTTP header names, but to avoid potential namespace\nconfusion, only those HTTP headers listed in an\nIETF standards-track document should be used in\nthis namespace.\n\n\"PEP\"Compliance is with a PEP-specified extension, identified\nusing a quoted-string containing the PEP extension\ndeclaration.\n\n    The option-param is used to provide additional parameters.\n    Unconditional compliance with a compliance-option is indicated\n    using the \"uncond\" option-param; for example, \"rfc=1945;uncond\".\n    Conditional compliance is indicated using the \"cond\" option-param;\n    for example, \"HDR=Authorization;uncond\".  Additional option-param\n    values might be defined as part of another specification.\n\n    Examples:\n\nCompliance: rfc=2068;uncond\nCompliance: rfc=1945;uncond, rfc=2068;cond\nCompliance: rfc=2068, hdr=PEP, hdr=SetCookie2\nCompliance: rfc=9999999;uncond;\"onlyOn=Tuesdays\"\n\n\n(7) New section:\n\n14.QQQ Non-Compliance\n\n   A proxy server SHOULD add this response-header to a response\n   containing a Compliance header if the proxy does not implement one\n   or more of the options described in the Compliance header.\n\n        Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\n\n        proxy-host = host [ \":\" port ]\n\n        non-compliance-option = compliance-option \"@\" proxy-host\n\n   A non-compliance-option listed in a Non-Compliance response-header\n   field indicates that the proxy server named by the proxy-host value\n   does not support the listed compliance-option.  The set of\n   non-compliance options SHOULD be a subset of the compliance-options\n   listed in a Compliance header field of the forwarded message.\n\n      Note: because the proxy-host value is not authenticated,\n      this is only for advisory purposes (e.g., for debugging).\n\n   A proxy MUST NOT delete a Non-Compliance header that it has\n   received from another server.\n\n(8) [For the moment, two possible alternatives here!]\n\n(8a) In section 14.35 (Public), replace\n\n   This header field applies only to the server directly connected to\n   the client (i.e., the nearest neighbor in a chain of connections). If\n   the response passes through a proxy, the proxy MUST either remove the\n   Public header field or replace it with one applicable to its own\n   capabilities.\n\nwith\n\n   A proxy MUST NOT modify the Public header field even if it does not\n   understand all the methods specified, since the user agent might have\n   other means of communicating with the origin server.\n\n(8b) In section 14.7 (Allow) replace:\n\n   A proxy MUST NOT modify the Allow header field even if it does not\n   understand all the methods specified, since the user agent MAY have\n   other means of communicating with the origin server.\n\nwith\n\n   A proxy MUST remove methods from an Allow header field if it\n   does not support the use of those methods for the resource\n   identified by the Request-URI.\n\nand in section 14.35 (Public), replace this paragraph:\n\n   This header field applies only to the server directly connected to\n   the client (i.e., the nearest neighbor in a chain of connections). If\n   the response passes through a proxy, the proxy MUST either remove the\n   Public header field or replace it with one applicable to its own\n   capabilities.\n\nwith\n\n   A proxy MUST remove methods from a Public header field if it\n   does not support the use of those methods.\n\n(9) Examples (put this in 9.2.1?):\n\n  To list all extensions supported by proxy \"proxy4.microscape.com\"\n\n    Client sends:\nOPTIONS * HTTP/1.1\nHost: proxy4.microscape.com\nCompliance: *\n\n    proxy4.microscape.com responds:\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:51 GMT\nServer: SuperProxy/1.0\nPublic: OPTIONS, GET, HEAD, PUT, POST, TRACE\nCompliance: rfc=1543, rfc=2068, hdr=set-proxy\nCompliance: hdr=wonder-bar-http-widget-set\nCompliance: PEP=\"<a href=\"http://foobar.pep.org/pepmeister/\">http://foobar.pep.org/pepmeister/</a>\"\nContent-Length: 0\n\n[Editorial note: check syntax of PEP extensions]\n       \n  Probing for a feature which is not supported by \"proxy4.microscape.com\"\n\n    Client sends:\nOPTIONS * HTTP/1.1\nHost: proxy4.microscape.com\nCompliance: PEP=\"<a href=\"http://foobar.pep.org/evil-not-implemented\">http://foobar.pep.org/evil-not-implemented</a>\"\n\n    proxy4.microscape.com responds:\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:52 GMT\nServer: SuperProxy/1.0\nPublic: OPTIONS, GET, HEAD, PUT, POST, TRACE\nCompliance:\nContent-Length: 0\n\n[End of proposed changes]\n\n\n\n"
        },
        {
            "subject": "305/306 spe",
            "content": "I havent gotten a whole lot of feedback on this latest revision,\n(ie without the regexps), although what I have gotten looks good.\n\nFor your viewing pleasure, here it is once more...:)\n\nI'd really like to see this in the spec, so if you have \ncomments, please share them.\n\nthanks\njosh\n\n-----------------------------------------------------------------------------\nJosh Cohen      Netscape Communications Corp.\nNetscape Fire Department                #include<disclaimer.h>\nServer Engineering\njosh@netscape.com                          http://people.netscape.com/josh/\n-----------------------------------------------------------------------------\n\n\n\n\n\n\n\nHTTP Working Group                                            Josh Cohen\nInternet-Draft                             Netscape Communications Corp.\nExpires: Six Months                                         24 July 1997\n\n                  HTTP/1.1 305 and 306 Response Codes\n\n                    <draft-cohen-http305036-00.txt>\n\nStatus of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups.  Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six months\n   and may be updated, replaced, or obsoleted by other documents at any\n   time.  It is inappropriate to use Internet- Drafts as reference\n   material or to cite them other than as ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please check the\n   ``1id-abstracts.txt'' listing contained in the Internet- Drafts\n   Shadow Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n   munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n   ftp.isi.edu (US West Coast).\n\nAbstract\n\n   The HTTP/1.1 RFC specifies a response code '305 Use Proxy' which is\n   intended to cause a client to retry the request using a specified\n   proxy server.  This functionality is important, but underspecified in\n   the current spec.  The spec does not specify for how long or which\n   URLs the redirect applies to, or how proxies can deal with or\n   generate similar responses.  This draft proposes a specification for\n   both the 305 response and a new response, \"306 Switch Proxy\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 1]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\nSummary\n\n 1.0 Response Codes\n\n  1.1 305 Use Proxy\n  1.2 306 Switch Proxy\n  1.3 506 Redirection Failed\n\n 2.0 Headers\n\n  2.1 Set-proxy:\n  2.2 Location:\n\n 3.0 Methods\n\n  3.1 OPTIONS\n\n 4.0 Operational Constraints\n\n 5.0 Notes\n\n\n1.0 Response Codes\n\n 1.1 305 Use Proxy\n\n   The 305 is generated by an origin server to indicate that the client,\n   or proxy, should use a proxy to access the requested resource.\n\n   The request SHOULD be accompanied by a 'Set-proxy' response header\n   indicating what proxy is to be used. The client will parse the 'Set-\n   proxy' header as defined below to decide how long, for what URLs it\n   should use the specified proxy.\n\n   If the 305 response is not accompanied by a 'Set-proxy' header, it\n   MUST be accompanied by a 'Location' header.  The 'Location' header\n   will specify a URL to the proxy.\n\n   If both headers are present in the response, the client SHOULD use\n   the 'Set-proxy' header only.\n\n 1.2 306 Switch Proxy\n\n   The 306 response is generated by a proxy server to indicate that the\n   client or proxy should use the information in the accompanying 'Set-\n   proxy' header to choose a proxy for subsequent requests.\n\n   The 306 response code MUST be accompanied by the 'Set-proxy' response\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 2]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n   header.  The client or proxy will parse the 'Set-proxy' header to\n   determine which proxy to use, how long to use it, and for which URLs\n   to use it.\n\n   The scope in the set-proxy header is considered an optional advisory.\n   The client or proxy may choose to ignore it, and use it for just this\n   request, for all requests, or for a scope previously or implicitly\n   defined by another configuration method or autoconfiguration system.\n\n 1.3 506 Redirection Failed\n\n   The 506 response is returned when a redirection fails or is refused\n   by a proxy or client.  If the redirection response included a body,\n   then it SHOULD be included in the 506 response.\n\n2.0 Headers\n\n 2.1 'Set-proxy' Response Header\n\n           The 'Set-proxy' header is defined as:\n\n           Set-proxy: \"Set-proxy\" \":\" action [ \";\" parameters ]\n\n           parameters = ( \"scope\" \"=\" scopePattern ) |\n                   ( proxyURI \"=\" URI ) |\n                   lifetime\n\n           lifetime = ( \"seconds\"  \"=\" integer )\n                   | ( \"hits\"      \"=\" integer )\n\n           action =  ( \"DIRECT\"\n                   | \"IPL\"\n                   | \"SET\" )\n\n\n           scopePattern = \"*\" | \"-\" | URIpattern\n\n           URIpattern = character | \"*\"\n\n           character = Any character legal in the definition\n                       of a URL/URI in the context of RFC2068\n\n   An example header:\n       Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\",\n           scope=\"http://\", seconds=5\n\n action\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 3]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n   The first item, \"action\" specifies the type or mode of the change.\n   Possible modes are:\n\n\n   DIRECT\n    Attempt to connect directly, with no proxy\n\n\n   IPL\n    Initial Program Load, the client or proxy should attempt to revert\n    back to its default or initial proxy setting.  This is meant to\n    instruct a client to re-fetch its proxy configuration, or PAC file.\n    When set, the accompanying scope field MUST be \"*\" A client receiv-\n    ing this response SHOULD prompt the user for confirmation.\n\n\n    If accompanied by a 'proxyURI' parameter, a proxy or client MAY use\n    the value as a URL containing a configuration to retrieve.  If a\n    client  does so, it MUST prompt the user for confirmation.\n\n\n   SET\n    Set to parameter \"proxyURI\".  The client should use the URL speci-\n    fied for \"proxyURI\" as the proxy.  If the SET mode is specified, the\n    parameter, \"proxyURI\", MUST be present.\n\n Scope\n\n    Scope refers to an expression pattern that specifies which URIs are\n    subject to this header setting.  URIs should be matched against the\n    scope with this rule :\n\n     The scope \"*\" means all requests\n     The scope \"-\" means this EXACT URL ONLY\n\n    Otherwise, the URL is compared with the scope in the following\n    manner.\n\n    The Scope is a prefix of matching URLs.\n\n    The character \"*\" is allowed in the dns name portion of a URL, or in\n    the path portion of the URL, but ONLY when used with a 306, not a\n    305.\n\n    It matches any sequence of characters except '/'.\n\n    This is intended to be a simple matching scheme to allow a prefix\n    match to take place.\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 4]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n    See the examples section in \"Operational Constraints\"\n\n    The lifetime parameter specifies how long the specified proxy should\n    be used.  If lifetime is specified as \"seconds\" then the proxy set-\n    ting remains in effect for 'integer' seconds.  If lifetime is speci-\n    fied in 'hits' then the proxy setting remains in effect for\n    'integer' transactions.\n\n 2.2 Location Header\n\n\n    In the original HTTP/1.1 spec, the 'Location' header was used to\n    indicate the proxy setting.  Its use is DEPRECATED by the 'Set-\n    proxy' header in the context of a 305 response. All new implementa-\n    tions MUST send the Set-proxy header.  Implementations MAY send the\n    'Location' header so as to allow backward compatibility.\n\n\n    If the 'Location' header is specified, it should contain a URI of\n    the proxy.  If the Set-proxy header is not specified, the client\n    should use this proxy for just one request, and only for the origi-\n    nally requested exact URL.\n\n 3.0 Methods\n\n\n    A client or proxy receiving a 305 or 306, should use the OPTIONS\n    method to determine if the server or proxy it is talking to actually\n    is an HTTP/1.1 server supporting 305 and 306 responses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 5]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n4.0 Operational Constraints\n\n\n   * Both the 305 and 306 response codes are HOP by HOP.  A proxy server\n     MUST not forward a 305 or 306 respose code (unless it generated the\n     306).\n\n\n   * A webserver MUST NOT send a 306 response under any circumstances\n\n\n   * A proxy server MUST NOT generate a 305 response.\n\n\n   * A client or proxy SHOULD NOT accept a 306 from a proxy that it\n     learned of via a 305 response code.\n\n\n   * A client or proxy MAY maintain state and allow a lifetime to extend\n     beyond a session or restart.\n\n\n   * A 'Set-proxy: IPL' SHOULD override any previous 'Set-proxy' header.\n\n\n   * A 305 or 306 response MAY contain a body containing an explanation\n     of the redirect for clients which do not understand the redirect\n\n\n   * In the absence of any parameter, the following defaults should be\n     used:\n\n       lifetime = this transaction only\n       scope = this exact URL only\n\n\n   * When receiving a 305 response, the client or proxy will enforce the\n     following rule with respect to the scope.\n\n     The scope specified must be more restrictive than the transformed\n     URL in question based on the rightmost slash in the URI.\n\n     Example: (in order of restrictiveness)\n       for URI = http://www.ups.com/services/index.html\n\n       http://www.ups.com/services/  (allowed)\n       http://www.ups.com/services/express/ ( allowed )\n       http://www.ups.com/ (NOT allowed)\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 6]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n     Using \"*\" in a 306 response set-proxy: header:\n\n     The scope may be set to:\n         http://*.foo.com/\n         which would apply to all URLs in to domain foo.com\n\n\n\n     If the scope returned with a 305 response is less restrictive than\n     the requested URL, the client may reject the redirection and return\n     506 Redirection Failed.  If the client wished to honor the\n     redirect, it client MUST prompt the user for confirmation before\n     accepting the new proxy setting.\n\n\n   * Since HTTP/1.0 proxies may unknowingly forward a 305 or 306\n     response code that was generated maliciously or in good faith, the\n     client must attempt to ascertain if the proxy with which it is\n     directly communicating is HTTP/1.1 and if it supports the 'Set-\n     proxy' header.  To determine this, the client or proxy should use\n     the OPTIONS method to make a request check for this feature.  The\n     extension string should be HDR='set-proxy', or, should this be\n     defined in the Standard RFC for HTTP/1.1, then the string should be\n     RFC='rfcXXXX'  in the OPTIONS request.\n\nSecurity Considerations\n\n     Great care should be taken when implementing client side actions\n     based on the 305 or 306.  Since older proxies may unknowingly for-\n     ward either of these reponses, clients should be prepared to check\n     the validity.\n\n\n   * Please read the section 'Operational Constraints'\n\n\n   * A client or proxy MUST NOT accept a 305 response from a proxy.\n\n\n   * A client or proxy MUST NOT accept a 306 response from an origin\n     server.\n\n\n   * When receiving a 306 response from a proxy, the client MUST verify\n     that the proxy supports the 306 response with an OPTIONS request.\n\n5.0 Notes\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 7]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n     Further specification is needed to define exactly how to use\n     OPTIONSs, or another mechanism to determin if set-proxy is sup-\n     ported.\n\nAuthor's Address\n\n     Josh Cohen\n     Netscape Communications Corporation\n     501 E. Middlefield Rd\n     Mountain View, CA 94043\n\n     Phone (415) 937-4157\n     EMail: josh@netscape.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 8]\n\n\n\n"
        },
        {
            "subject": "Re: LAST CALL, &quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "On Thu, 24 Jul 1997, Koen Holtman wrote:\n\n> Dave Kristol:\n> >\n> [...]\n> >Does this wording express it adequately?:\n> >\n> >If the user agent allows the user to follow the [CommentURL] link [as\n> >part of a cookie inspection user interface], it should neither send nor\n> >accept a cookie until the user has completed the inspection.\n> \n> I think the approach to solving this problem is wrong: the burden of\n> ensuring that the commentURL mechanism does not lead to\n> user-unfriendly or recursive situations should be on the server side.\n> \n> I propose something like this:\n> \n>  Servers SHOULD ensure that the user can visit the information pointed\n>  to by the commentURL without causing the user agent to receive\n>  additional Set-Cookie2 headers.  User agents SHOULD guard against the\n>  entering of infinite loops due to the commentURL mechanism, and MAY do\n>  this by disabling cookie processing when the commentURL is visited.\n> \n\nThat would be needless complexity. Prior to this suggestion, the server\nhas had no special responsiblity other than to provide a URL which can\nbe retrieved from some server.  Secondly, the UA has many alternatives\nfor dealing with the possible looping issue including if its designer\nso chooses doing nested interactions with the user, or flattening the\nprocess and pending the nested cookie handling until the first cookie\nis fully dealt with. Making a requirement on the server changes this\nsection from implementation advice into a protocol requirement.\n\nThe only appropriate solution falls to the UA's UI design. It is only\nuser unfriendly if the UI design makes it unfriendly.  And since UAs\nwould under this suggestion be expected to guard against the problem,\nthey might as well just take care of it.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Feedback on: draft-cohen-http30530602.tx",
            "content": "My comments are interspersed with **DWM: prefixes below. Mostly minor\neditorial except for the scope pattern which as I note I don't understand\nwhat is being specified.\n\nDave\n\n\n1.0 Response Codes\n\n 1.1 305 Use Proxy\n\n   The 305 is generated by an origin server to indicate that the client,\n   or proxy, should use a proxy to access the requested resource.\n\n   The request SHOULD be accompanied by a 'Set-proxy' response header\n   indicating what proxy is to be used. The client will parse the 'Set-\n   proxy' header as defined below to decide how long, for what URLs it\n**DWM:                                              ^^^ insert 'and' \n**DWM:    and I believe delete the comma.\n\n   should use the specified proxy.\n\n   If the 305 response is not accompanied by a 'Set-proxy' header, it\n   MUST be accompanied by a 'Location' header.  The 'Location' header\n   will specify a URL to the proxy.\n\n   If both headers are present in the response, the client SHOULD use\n   the 'Set-proxy' header only.\n\n**DWM: change the last phrase to read:  \"the client SHOULD only use\n**DWM: the 'Set-proxy' header.\"\n**DWM: OR \"the client SHOULD ignore the 'Location' header.\"\n\n 1.2 306 Switch Proxy\n\n   The 306 response is generated by a proxy server to indicate that the\n   client or proxy should use the information in the accompanying 'Set-\n   proxy' header to choose a proxy for subsequent requests.\n\n   The 306 response code MUST be accompanied by the 'Set-proxy' response\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 2]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n   header.  The client or proxy will parse the 'Set-proxy' header to\n   determine which proxy to use, how long to use it, and for which URLs\n   to use it.\n\n   The scope in the set-proxy header is considered an optional advisory.\n   The client or proxy may choose to ignore it, and use it for just this\n   request, for all requests, or for a scope previously or implicitly\n   defined by another configuration method or autoconfiguration system.\n\n 1.3 506 Redirection Failed\n\n   The 506 response is returned when a redirection fails or is refused\n   by a proxy or client.  If the redirection response included a body,\n   then it SHOULD be included in the 506 response.\n\n2.0 Headers\n\n 2.1 'Set-proxy' Response Header\n\n           The 'Set-proxy' header is defined as:\n\n           Set-proxy: \"Set-proxy\" \":\" action [ \";\" parameters ]\n\n**DWM:   I believe you need \"1#parameters\" meaning 1 or more comma\n**DWM:   delimited terms?\n\n           parameters = ( \"scope\" \"=\" scopePattern ) |\n                   ( proxyURI \"=\" URI ) |\n                   lifetime\n\n           lifetime = ( \"seconds\"  \"=\" integer )\n                   | ( \"hits\"      \"=\" integer )\n\n           action =  ( \"DIRECT\"\n                   | \"IPL\"\n                   | \"SET\" )\n\n\n           scopePattern = \"*\" | \"-\" | URIpattern\n\n           URIpattern = character | \"*\"\n\n           character = Any character legal in the definition\n                       of a URL/URI in the context of RFC2068\n\n**DWM:  I can't make sense out of either the syntax description or\n**DWM:  the words below for what you mean to provide syntactically\n**DWM:  or semantically for URIpattern. So other than to say that\n**DWM:  I don't understand, I can't offer a suggestion.\n\n   An example header:\n       Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\",\n           scope=\"http://\", seconds=5\n\n action\n\n   The first item, \"action\" specifies the type or mode of the change.\n   Possible modes are:\n\n\n   DIRECT\n    Attempt to connect directly, with no proxy\n\n\n   IPL\n    Initial Program Load, the client or proxy should attempt to revert\n    back to its default or initial proxy setting.  This is meant to\n    instruct a client to re-fetch its proxy configuration, or PAC file.\n    When set, the accompanying scope field MUST be \"*\" A client receiv-\n    ing this response SHOULD prompt the user for confirmation.\n\n\n    If accompanied by a 'proxyURI' parameter, a proxy or client MAY use\n    the value as a URL containing a configuration to retrieve.  If a\n    client  does so, it MUST prompt the user for confirmation.\n\n\n   SET\n    Set to parameter \"proxyURI\".  The client should use the URL speci-\n    fied for \"proxyURI\" as the proxy.  If the SET mode is specified, the\n    parameter, \"proxyURI\", MUST be present.\n\n Scope\n\n    Scope refers to an expression pattern that specifies which URIs are\n    subject to this header setting.  URIs should be matched against the\n    scope with this rule :\n\n     The scope \"*\" means all requests\n     The scope \"-\" means this EXACT URL ONLY\n\n    Otherwise, the URL is compared with the scope in the following\n    manner.\n\n    The Scope is a prefix of matching URLs.\n\n    The character \"*\" is allowed in the dns name portion of a URL, or in\n    the path portion of the URL, but ONLY when used with a 306, not a\n    305.\n\n    It matches any sequence of characters except '/'.\n\n    This is intended to be a simple matching scheme to allow a prefix\n    match to take place.\n\n    See the examples section in \"Operational Constraints\"\n\n    The lifetime parameter specifies how long the specified proxy should\n    be used.  If lifetime is specified as \"seconds\" then the proxy set-\n    ting remains in effect for 'integer' seconds.  If lifetime is speci-\n    fied in 'hits' then the proxy setting remains in effect for\n    'integer' transactions.\n\n 2.2 Location Header\n\n\n    In the original HTTP/1.1 spec, the 'Location' header was used to\n    indicate the proxy setting.  Its use is DEPRECATED by the 'Set-\n    proxy' header in the context of a 305 response. All new implementa-\n    tions MUST send the Set-proxy header.  Implementations MAY send the\n    'Location' header so as to allow backward compatibility.\n\n\n    If the 'Location' header is specified, it should contain a URI of\n    the proxy.  If the Set-proxy header is not specified, the client\n    should use this proxy for just one request, and only for the origi-\n    nally requested exact URL.\n\n 3.0 Methods\n\n\n    A client or proxy receiving a 305 or 306, should use the OPTIONS\n    method to determine if the server or proxy it is talking to actually\n    is an HTTP/1.1 server supporting 305 and 306 responses.\n\n4.0 Operational Constraints\n\n\n   * Both the 305 and 306 response codes are HOP by HOP.  A proxy server\n     MUST not forward a 305 or 306 respose code (unless it generated the\n     306).\n\n\n   * A webserver MUST NOT send a 306 response under any circumstances\n\n\n   * A proxy server MUST NOT generate a 305 response.\n\n\n   * A client or proxy SHOULD NOT accept a 306 from a proxy that it\n     learned of via a 305 response code.\n\n\n   * A client or proxy MAY maintain state and allow a lifetime to extend\n     beyond a session or restart.\n\n\n   * A 'Set-proxy: IPL' SHOULD override any previous 'Set-proxy' header.\n\n\n   * A 305 or 306 response MAY contain a body containing an explanation\n     of the redirect for clients which do not understand the redirect\n\n\n   * In the absence of any parameter, the following defaults should be\n     used:\n\n       lifetime = this transaction only\n       scope = this exact URL only\n\n\n   * When receiving a 305 response, the client or proxy will enforce the\n     following rule with respect to the scope.\n\n     The scope specified must be more restrictive than the transformed\n     URL in question based on the rightmost slash in the URI.\n\n     Example: (in order of restrictiveness)\n       for URI = http://www.ups.com/services/index.html\n\n       http://www.ups.com/services/  (allowed)\n       http://www.ups.com/services/express/ ( allowed )\n       http://www.ups.com/ (NOT allowed)\n\n\n\nJ. Cohen          HTTP/1.1 305 and 306 Response Codes           [Page 6]\n\n\n\n\n\nINTERNET-DRAFT                                              24 July 1997\n\n\n     Using \"*\" in a 306 response set-proxy: header:\n\n     The scope may be set to:\n         http://*.foo.com/\n         which would apply to all URLs in to domain foo.com\n\n\n\n     If the scope returned with a 305 response is less restrictive than\n     the requested URL, the client may reject the redirection and return\n     506 Redirection Failed.  If the client wished to honor the\n     redirect, it client MUST prompt the user for confirmation before\n     accepting the new proxy setting.\n\n**DWM:  This and the earlier discussion of 506 is confusing at best.\n**DWM:  Who exactly is the client returning 506 to? In normal HTTP\n**DWM:  lingo the client receives responses, it doesn't send them.\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for OPTIONS issu",
            "content": "JM> (8) [For the moment, two possible alternatives here!]\n\n  I'm afraid that I can't decipher whether it is (8a) or (8b) that I\n  like or whether I don't like either one.  Simply stated, I would\n  prefer that proxies never modify either Public or Allow headers;\n  they should always be end-to-end headers.  I choose this alternative\n  because I think it is easier for a client to discover what is going\n  on this way.\n\n  If proxies DO modify the Public and Allow headers, an end-client\n  can't tell whether it is the origin server or the proxy that doesn't\n  support what it wants because it sees the same response from both.\n  While it is true that the client may have another way to reach the\n  origin server, if that is not the case there will be no way for the\n  user to determine that the proxy is the problem.  Since many users\n  are behind proxies they cannot get around, there would be no way for\n  them to discover that the reason they can't use some service is that\n  the proxy is missing something.\n\n  If proxies DO NOT modify Public and Allow headers, the methods\n  supported by the origin server can be discovered by sending an\n  OPTIONS request, and the methods supported by each proxy can be\n  discovered by sending an OPTIONS request with a Max-Forwards\n  header.\n\n  This would be a slight change to RFC 2068 definition for the\n  treatment of these headers by proxies, but as far as I can tell\n  there are no deployed 2068 proxies out there yet, so I don't think\n  that is a practical concern.  Even if there were, a client could\n  detect that a proxy was 2068-only by sending OPTIONS with a\n  Compliance header to the proxy and get back an OPTIONS response\n  without a Compliance header.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "RE: can't really be LAST CALL,&quot;HTTP State Management Mechanism (Rev1) &quot; to Prop",
            "content": "-----Original Message-----\nFrom:Foteos Macrides [SMTP:MACRIDES@SCI.WFBR.EDU]\nSent:Wednesday, July 23, 1997 6:43 PM\nTo:dwm@xpasc.com\nCc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject:Re: can't really be LAST CALL,\"HTTP State Management \nMechanism (Rev1) \" to Propo\n\n\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Wed, 23 Jul 1997, Larry Masinter wrote:\n>>> Too much icing on the cookie, just say no.\n>\n>[...]\n>\n>The difference between the Comment attribute and the CommentURL is \nthe\n>difference between the Windows application which provides a message\n>box with a message like:\n>             \"Unable to write bookmark file\"\n>and one which presents the message:\n>             \"Unable to write bookmark file:\n>              C:\\home\\user\\internet\\bkmrks.fil\n>              because the file already exists and is owned\n>              by another user\"\n>In the first case, only a user familiar with the application \ninternals\n>could guess where to start looking.  In the second case, the average\n>reasonably knowledgable user of the operating system usage would have \n>a good chance at successful problem resolution.\n\nNote that even in that example, you are restricting yourself\nto ASCII characters. :)\n\n\n>If user privacy is important to our protocol effort, we must make it\n>>possible for the user to receive sufficient information for \ninformed\n>>consent. If we don't, the user community will throw their hands up\n>>and take the course of least resistance and all of our concern \nabout\n>>cookie sharing will be moot.\n>>\n>>In other words, I don't consider CommentURL as icing on the cookie,\n>>it is central to any possibility of achieving user control over\n>>privacy.\n\nI doubt it will be considered icing by users whose language\nis not adequately accomodated by the device of try to stuff a body\ninto the value of a Set-Cookie2 header's comment attribute. :)\n\nThe work of the IPWG group and the P3 project so far indicates that \nthe semantics of privacy and the scenarios are complex.  To enable \n\"Free, informed consent\", servers need to be able to display in an \naccessible manner their privacy practices and implications.   A \ncomment alone doesn't do this.\n\nIn addition, I think the CommentURL (with or without additional cookie \nsupport) potentially introduces a mechanism to engage the user in a \nprivacy dialogue leading to a more robust privacy solution: e.g., OPS \nand/or the results of the W3C P3 project...   Of course this could \nalso possibly be dealt with through PEP.  For example a web page could \nhave a link to an P3 (or whatever)-compliant browser or plug-in, which \nwould give the user control over what could be done with information, \ncookies etc.\n\n\n\n"
        },
        {
            "subject": "New Issue REDIRECT",
            "content": "The following discussion has been going on privately, and needs\na wider audience.\n\nI've assigned it the issue name REDIRECTS, and it will appear in\nthe next issues list page.\n- Jim Gettys\n\n\n\nattached mail follows:\nThe HTTP spec says that redirects are to be limited to 5. We have found\nthis causes problems with some sites, like pathfinder. We tried 10 but\nthat didn't work particularly well either. So we currently have it set\nfor 100. Is there any particular reason why we desperately need to lower\nthat figure?\nThanks,\nYaron\n\n\n\nattached mail follows:\nEach redirect causes additional network traffic; the limit is there\nto prevent an infinite loop causing infinite traffic to be generated.\n\nI'd like to understand what PathFinder is trying to do to understand what\nthis limit might be lifted to.  There certainly needs to be some limit of\nsome sort.\n- Jim\n\n\n\nattached mail follows:\nThe argument I have been making with Larry is that we should define a\nminimum number of redirects to be supported by a client but should not\nput a maximum on servers. A server may have a perfectly rational reason\nfor wanting to exceed whatever limit we have placed on redirects.\nHowever, for the sake of interoperability, a server needs to know what\nis the maximum number of redirects a client is guaranteed to support.\n\nYaron\n\n> -----Original Message-----\n> From:jg@pa.dec.com [SMTP:jg@pa.dec.com]\n> Sent:Tuesday, July 22, 1997 12:05 PM\n> To:Yaron Goland\n> Cc:Larry Masinter (E-mail); Roy T. Fielding (E-mail); Jim Gettys\n> (E-mail)\n> Subject:Re: Redirects\n> \n> Each redirect causes additional network traffic; the limit is there\n> to prevent an infinite loop causing infinite traffic to be generated.\n> \n> I'd like to understand what PathFinder is trying to do to understand\n> what\n> this limit might be lifted to.  There certainly needs to be some limit\n> of\n> some sort.\n> - Jim\n\n\n\nattached mail follows:\n>The argument I have been making with Larry is that we should define a\n>minimum number of redirects to be supported by a client but should not\n>put a maximum on servers. A server may have a perfectly rational reason\n>for wanting to exceed whatever limit we have placed on redirects.\n>However, for the sake of interoperability, a server needs to know what\n>is the maximum number of redirects a client is guaranteed to support.\n\nOh, that's a much easier question: the answer is zero.  You cannot force\na user agent to make a network request (because it may cost them money),\nso you cannot guarantee that a client will perform a redirect without\nfirst asking the user for confirmation.\n\n....Roy\n\n\nReceived: by src-mail.pa.dec.com; id AA13862; Fri, 25 Jul 97 01:48:46 -0700\nReceived: by pobox1.pa.dec.com; id AA22084; Fri, 25 Jul 97 01:48:44 -0700\nReceived: from mail5.microsoft.com (mail5.microsoft.com [131.107.3.31])\nby mail2.digital.com (8.7.5/UNX 1.5/1.0/WV) with ESMTP id BAA17245\nfor <jg@pa.dec.com>; Fri, 25 Jul 1997 01:43:11 -0700 (PDT)\nReceived: by mail5.microsoft.com with Internet Mail Service (5.0.1458.49)\nid <PSX6MBM9>; Fri, 25 Jul 1997 01:45:01 -0700\nMessage-Id: <11352BDEEB92CF119F3F00805F14F4850332A78F@RED-44-MSG.dns.microsoft.com>\nFrom: Yaron Goland <yarong@microsoft.com>\nTo: \"'Roy T. Fielding'\" <fielding@kiwi.ics.uci.edu>,\n        Yaron Goland\n <yarong@microsoft.com>\nCc: \"'jg@pa.dec.com'\" <jg@pa.dec.com>,\n        \"Larry Masinter (E-mail)\"\n <masinter@parc.xerox.com>,\n        \"Jim Gettys (E-mail)\" <jg@w3.org>\nSubject: RE: Redirects \nDate: Fri, 25 Jul 1997 01:42:25 -0700\nX-Priority: 3\nX-Mailer: Internet Mail Service (5.0.1458.49)\nX-UIDL: 6a4244389728a59ded607196a8bab141\n\nRoy, do you even understand why your response makes me upset? Was that\nyour goal? To state the painfully obvious as if it was relevant to the\npoint under consideration is just asinine. Clearly the user can stop any\noperation. That isn't the issue. The issue is how many times will the\nsystem even give the user a choice to stop the operation before it steps\nin and stops the redirections itself. Currently we have set that limit\nto 5. I do not believe that is appropriate. Rather I think we should set\nthe minimum to 5. \n\nThat the system will present the user at least 5 redirections before\njust saying \"Something has gone wrong, there is an error, I am ending\nthis operation.\" In cases where the redirection is automatic, then the\nsystem would only be required to automatically redirect up to five\ntimes. \n\nBTW I am not asking for a UI requirement. It is completely appropriate\nfor a browsers, in case of 302 on a POST or similar method, to never\neven present the redirect to the user for confirmation. This is\ncertainly the case when running in batch mode, for example. However in\ncases where it is appropriate to present a dialog, at least five rounds\nof redirections should be supported. If the user wants to end the\nsituation earlier than that, that is the user's prerogative.\n\nOn to other issues, more as an editorial note. Some of the devs have not\nclearly understood that 301/302 means \"Re-execute the method you just\nexecuted on this resource over on that resource.\" Some have thought, in\nfact, that what is really meant is \"Re-execute the following method on\nthis other resource\" and wondered where the header was that specified\nwhat method they should actually execute. For example, you could tell\nsomeone doing a POST on resource A to instead do a PUT on resource B. I\nrealize you will all think these people to be drooling morons. They are\nnot. They are actually quite intelligent. The existence of their\nmisunderstanding indicates that what is implicitly implied in the text,\nshould instead be explicitly stated.\n\nIn fact there was a bit of a flap over a fear that 1.1 servers doing\n301/302s would redirect the user to a 1.0 server w/a CGI script\nexpecting a GET. The problem is, that if a POST, on a 1.1 server, got\nyou the 302, you will execute another POST. Thus confusing the living\nhell out of the target CGI. Then they made a number of proposals to deal\nwith the situation. My response was to say \"HELL NO! If a server maker\nis not aware of the effect of the responses they are generated, in this\ncase needing to generate a 303, then they are broken, screw 'em.\" Should\nbe interesting to see how that goes over.\n\nYaron\n\n> -----Original Message-----\n> From:Roy T. Fielding [SMTP:fielding@kiwi.ics.uci.edu]\n> Sent:Thursday, July 24, 1997 8:43 PM\n> To:Yaron Goland\n> Cc:'jg@pa.dec.com'; Larry Masinter (E-mail); Jim Gettys (E-mail)\n> Subject:Re: Redirects \n> \n> >The argument I have been making with Larry is that we should define a\n> >minimum number of redirects to be supported by a client but should\n> not\n> >put a maximum on servers. A server may have a perfectly rational\n> reason\n> >for wanting to exceed whatever limit we have placed on redirects.\n> >However, for the sake of interoperability, a server needs to know\n> what\n> >is the maximum number of redirects a client is guaranteed to support.\n> \n> Oh, that's a much easier question: the answer is zero.  You cannot\n> force\n> a user agent to make a network request (because it may cost them\n> money),\n> so you cannot guarantee that a client will perform a redirect without\n> first asking the user for confirmation.\n> \n> ....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for CONTENTENCODING issu",
            "content": " I think this proposal is the best solution to the problem.\n\nI would much rather the specification was consistent than there be \na special treatment of Content-Encoding for the benefit of perl\nhackers. I don't see any logic in treating Content-Encoding\ndifferently from the other tags and we would almost certainly \nwant to correct the anomaly in HTTP/1.2. We might as well make\nthe change now.\n\n\n        Phill\n\n\n\n"
        },
        {
            "subject": "RE: New Issue REDIRECT",
            "content": "I say certain things in private e-mail that I would not say on a public\nlist. For example, my last letter would have never appeared in a public\nlist. Do I need to treat all e-mail to you guys as being de-facto\npublic?\n\nYaron\n\n> -----Original Message-----\n> From:jg@Pa.dec.com [SMTP:jg@Pa.dec.com]\n> Sent:Friday, July 25, 1997 9:17 AM\n> To:http-wg@cuckoo.hpl.hp.com\n> Cc:Larry Masinter (E-mail); Roy T. Fielding (E-mail); Yaron Goland\n> Subject:New Issue REDIRECTS\n> \n> The following discussion has been going on privately, and needs\n> a wider audience.\n> \n> I've assigned it the issue name REDIRECTS, and it will appear in\n> the next issues list page.\n> - Jim Gettys\n>  << Message: Redirects >>  << Message: Re: Redirects >>  << Message:\n> RE: Redirects >>  << Message: Re: Redirects  >>  << Message: RE:\n> Redirects  >> \n\n\n\n"
        },
        {
            "subject": "Removing CommentUR",
            "content": "Since this argument seems to have convinced one person,\nI thought I would make it more broadly and see if I could convince\nsomeone else.\n\n\nattached mail follows:\nI think that the problem is an important problem to solve, but that the\nproposal doesn't actually solve it, and is, in fact, impractical.\n\n> Effectively, showing the CommentURL merely involves going to a different\n> page, probably with a different set of cookies.  I don't see what's\n> really hard about it.  Is there something I'm missing?\n\nThe proposal (CommentURL) is easy to implement. I am not claiming that\nit is 'hard'.\nI just think it is not very useful.\n\n> It is likely that at some point in the future I will implement a UA\n> which will support the CommentURL, and I would like the CommentURL\n> to be available.\n\nThe fact that you would like this feature to be available doesn't\nactually address the question of whether the feature actually addresses\nthe problem in a significant way.\nI don't think our adding CommentURLs in the protocol will actually help\nmost users decide whether or not they want to accept cookies.\n\nThat's why I'm opposed to the proposal: it makes the protocol more\ncomplicated while it won't actually be useful. It *could* be useful,\nthere are *some* people for whom it will be useful, but it won't be\nuseful in general, or generally used, most sites won't have CommentURLs,\nmost clients won't have useful interactions with CommentURLs, and we'll\nhave this extra widget in the protocol which doesn't actually do most\npeople any good.\n\nDo you still disagree?\n\nLarry\n--\n\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: New Issue REDIRECT",
            "content": "Ahh... next time remind me to check the \"to\" line. =)\nYaron\n\n> -----Original Message-----\n> From:Yaron Goland \n> Sent:Friday, July 25, 1997 11:28 AM\n> To:'jg@Pa.dec.com'; http-wg@cuckoo.hpl.hp.com\n> Cc:Larry Masinter (E-mail); Roy T. Fielding (E-mail)\n> Subject:RE: New Issue REDIRECTS\n> \n> I say certain things in private e-mail that I would not say on a\n> public list. For example, my last letter would have never appeared in\n> a public list. Do I need to treat all e-mail to you guys as being\n> de-facto public?\n> \n> Yaron\n> \n> -----Original Message-----\n> From:jg@Pa.dec.com [SMTP:jg@Pa.dec.com]\n> Sent:Friday, July 25, 1997 9:17 AM\n> To:http-wg@cuckoo.hpl.hp.com\n> Cc:Larry Masinter (E-mail); Roy T. Fielding (E-mail); Yaron\n> Goland\n> Subject:New Issue REDIRECTS\n> \n> The following discussion has been going on privately, and needs\n> a wider audience.\n> \n> I've assigned it the issue name REDIRECTS, and it will appear in\n> the next issues list page.\n> - Jim Gettys\n>  << Message: Redirects >>  << Message: Re: Redirects >>  <<\n> Message: RE: Redirects >>  << Message: Re: Redirects  >>  << Message:\n> RE: Redirects  >> \n\n\n\n"
        },
        {
            "subject": "RE: New Issue REDIRECT",
            "content": "My apologies to Yaron; I should certainly have checked with everyone\nbefore forwarding the thread to the open mailing list.\n\nWon't happen again.\n- Jim\n\n\n\n"
        },
        {
            "subject": "an (unofficial) cookie draf",
            "content": "I would like to make the I-D cutoff of 7/30 for another cookie draft.\nI've made available what I would like to submit to IETF.  It's\navailable via\n<http://portal.research.bell-labs.com/~dmk/cookie-ver.html>\n(Look below the horizontal line.)\n\nIt would be nice if it were the last (I'm sure you'll all agree:-).\nThe outcome of ongoing public and private discussions may induce\nfurther changes.\n\nThere are text and PostScript versions of\n- this draft (2.63)\n- this draft, with change bars from RFC 2109\n- this draft, with change bars from draft-ieft-http-state-man-mec-02.\n\nMajor changes since state-man-mec-02:\n\n- per Dave Morris, relaxed FQHN stuff\n- Domain= prepends a '.' if none present\n- add CommentURL\n- add a $Port attribute to Cookie.  (I spontaneously added this\nwhen I realized it was needed for consistency with the\n$Path and $Domain.)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "    Although the commentURL attribute would provide a richer context for the cookie to be evaluated in, it is going a step too far.  The comment attribute is sufficient to explain a cookie's purpose.  If it is not, the cookie server can provide a url in the comment attribute that the user/UA can reference for further info. regarding the cookie.  I would consider it bad practice for the url the cookie server sends in the comment attribute to contain cookies, but, content providers can obviously do whatever they want.\n\n    As long as the UA allows for examination of cookies, the user has complete control over what cookies he keeps. If the user allows a cookie to be set because he didn't have a commentURL available for evaluation before accepting the cookie, before he issues another request he can examine his cookies and visit any url in any comment attribute he wants. If at that point the user decides he doesn't want that cookie, he can delete it.\n\n    I'm not convinced of the need for a separate header just so a url can be explicitly provided.\n\nJudson Valeski - Netscape\n\nLarry Masinter wrote:\n\n> Since this argument seems to have convinced one person,\n> I thought I would make it more broadly and see if I could convince\n> someone else.\n>\n>                                                   -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n> Subject: Re: can't really be LAST CALL, \"HTTP State Management Mechanism (Rev1) \" to Propo\n> Date: Wed, 23 Jul 1997 22:53:22 -0700\n> From: Larry Masinter <masinter@parc.xerox.com>\n> Organization: Xerox PARC\n> To: \"Joel N. Weber II\" <devnull@gnu.ai.mit.edu>\n> CC: dmk@bell-labs.com\n> References: <199707240147.VAA23530@mescaline.gnu.ai.mit.edu>\n>\n> I think that the problem is an important problem to solve, but that the\n> proposal doesn't actually solve it, and is, in fact, impractical.\n>\n> > Effectively, showing the CommentURL merely involves going to a different\n> > page, probably with a different set of cookies.  I don't see what's\n> > really hard about it.  Is there something I'm missing?\n>\n> The proposal (CommentURL) is easy to implement. I am not claiming that\n> it is 'hard'.\n> I just think it is not very useful.\n>\n> > It is likely that at some point in the future I will implement a UA\n> > which will support the CommentURL, and I would like the CommentURL\n> > to be available.\n>\n> The fact that you would like this feature to be available doesn't\n> actually address the question of whether the feature actually addresses\n> the problem in a significant way.\n> I don't think our adding CommentURLs in the protocol will actually help\n> most users decide whether or not they want to accept cookies.\n>\n> That's why I'm opposed to the proposal: it makes the protocol more\n> complicated while it won't actually be useful. It *could* be useful,\n> there are *some* people for whom it will be useful, but it won't be\n> useful in general, or generally used, most sites won't have CommentURLs,\n> most clients won't have useful interactions with CommentURLs, and we'll\n> have this extra widget in the protocol which doesn't actually do most\n> people any good.\n>\n> Do you still disagree?\n>\n> Larry\n> --\n>\n> http://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: New Issue REDIRECT",
            "content": "Oh man, I think I'll just skip this discussion for a while\nand go down to the beach instead.\n\n.....Roy\n\np.s. no worries, Yaron, I knew what you intended (and didn't)\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": ">     Although the commentURL attribute would provide a richer context for =\n> the cookie to be evaluated in, it is going a step too far.  The comment a=\n> ttribute is sufficient to explain a cookie's purpose.  If it is not, the =\n> cookie server can provide a url in the comment attribute that the user/UA=\n>  can reference for further info. regarding the cookie.  I would consider =\n> it bad practice for the url the cookie server sends in the comment attrib=\n> ute to contain cookies, but, content providers can obviously do whatever =\n> they want.\n> \n>     As long as the UA allows for examination of cookies, the user has com=\n> plete control over what cookies he keeps. If the user allows a cookie to =\n> be set because he didn't have a commentURL available for evaluation befor=\n> e accepting the cookie, before he issues another request he can examine h=\n> is cookies and visit any url in any comment attribute he wants. If at tha=\n> t point the user decides he doesn't want that cookie, he can delete it.\n\nTo put any meaningful explanation of what the cookie is used for will\nrequire a small paragraph.  That's a small paragraph in every Comment\nheader in every document that goes out of most servers that use\ncookies.  This is not network friendly.\n\nIf you can see the justification for Comment, it should be obvious\nthat regardless of how it's implemented, CommentURL would be better.\nIt's much more versatile.\n\nWith including a URL in the comment, how likely is it that a user will not \nonly be allowed by the browser to open a new browser (presumably before\nmaking a decision about accepting a cookie) but also copy a url off the\nscreen into the new browser, then look at what it says, and then\nmake a decision?  (And what are the odds that they'll even get that\ncookie policy at the URL without another cookie being sent to them,\nand being asked to accept the same cookie?).  How many people\nwill actually do it?\n\nIf you cut CommentURL out, you might as well cut Comment out as well.\n\nKeep CommentURL.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "301/30",
            "content": "1) HTTP 1.0 script is written which expect to get GETs.\n2) HTTP 1.0 resource is programmed to redirect with a 301/302 to the\nHTTP/1.0 script\n3) Server is upgraded to HTTP/1.1 but the HTTP 1.0 resource and the HTTP\n1.0 script are not upgraded.\n4) HTTP/1.1 browser comes along and sends a POST to the HTTP 1.0\nresource and receives a 301/302. HTTP/1.1 browser sends a POST to the\nHTTP 1.0 script. The HTTP 1.0 script gets completely confused because it\nwas expecting a GET and the user never sees the proper data.\n\nMy suggestion is, as horrible as this is going to sound, that we change\nthe definition of 301/302 to redirect to GET and make 303/304 be\nredirect, permanently or temporarily, with the same method. We can't\nforce the whole world to rewrite all their scripts and our users aren't\ngoing to accept \"Well gee, you know, the script is doing the wrong\nthing, it should send a 303 not a 301/302.\"\n\nYaron\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "Jonathan Stark wrote:\n\n> >     Although the commentURL attribute would provide a richer context for =\n> > the cookie to be evaluated in, it is going a step too far.  The comment a=\n> > ttribute is sufficient to explain a cookie's purpose.  If it is not, the =\n> > cookie server can provide a url in the comment attribute that the user/UA=\n> >  can reference for further info. regarding the cookie.  I would consider =\n> > it bad practice for the url the cookie server sends in the comment attrib=\n> > ute to contain cookies, but, content providers can obviously do whatever =\n> > they want.\n> >\n> >     As long as the UA allows for examination of cookies, the user has com=\n> > plete control over what cookies he keeps. If the user allows a cookie to =\n> > be set because he didn't have a commentURL available for evaluation befor=\n> > e accepting the cookie, before he issues another request he can examine h=\n> > is cookies and visit any url in any comment attribute he wants. If at tha=\n> > t point the user decides he doesn't want that cookie, he can delete it.\n>\n> To put any meaningful explanation of what the cookie is used for will\n> require a small paragraph.  That's a small paragraph in every Comment\n> header in every document that goes out of most servers that use\n> cookies.  This is not network friendly.\n\n> The comment a=\n> ttribute is sufficient to explain a cookie's purpose.  If it is not, the =\n> cookie server can provide a url in the comment attribute that the user/UA=>\ncan reference for further info. regarding the cookie.Also, once users realize\nthat they're spending time reading small paragraphs about cookies, they'll\nrealize how unobtrusive and uninteresting the information in the cookie is to\nthem, and stop reading about specific cookies altogether, thus rendering the\ncommentURL obsolete anyway.\n\n> If you can see the justification for Comment, it should be obvious\n> that regardless of how it's implemented, CommentURL would be better.\n> It's much more versatile.\n\n> With including a URL in the comment, how likely is it that a user will not\n> only be allowed by the browser to open a new browser (presumably before\n> making a decision about accepting a cookie) but also copy a url off the\n> screen into the new browser, then look at what it says, and then\n> make a decision?\n\nThe user would have to look at what the commentURL says and make a decision\nanyway.\n\n> (And what are the odds that they'll even get that\n> cookie policy at the URL without another cookie being sent to them,\n> and being asked to accept the same cookie?).  How many people\n> will actually do it?\n\nNot many. But no-one said the UA wouldn't parse the comment attribute, realize\nthere's a url in in and provide essentially the same functionality the\ncommentURL would provide. There would be no copying and pasting.\n\n> If you cut CommentURL out, you might as well cut Comment out as well.\n\nI don't agree here. A cookie should have some means for describing its purpose\n(the comment attribute).\n\nJudson\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Before someone beats me up, yes I know that 304 is already taken. I was\njust trying to be symmetric. You get the idea, lets come up with two new\nnumbers that aren't already programmed into every script on the planet.\nYaron\n\n> -----Original Message-----\n> From:Yaron Goland \n> Sent:Friday, July 25, 1997 3:19 PM\n> To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; Roy T. Fielding\n> (E-mail); Jim Gettys (E-mail); Larry Masinter (E-mail)\n> Subject:301/302\n> \n> 1) HTTP 1.0 script is written which expect to get GETs.\n> 2) HTTP 1.0 resource is programmed to redirect with a 301/302 to the\n> HTTP/1.0 script\n> 3) Server is upgraded to HTTP/1.1 but the HTTP 1.0 resource and the\n> HTTP 1.0 script are not upgraded.\n> 4) HTTP/1.1 browser comes along and sends a POST to the HTTP 1.0\n> resource and receives a 301/302. HTTP/1.1 browser sends a POST to the\n> HTTP 1.0 script. The HTTP 1.0 script gets completely confused because\n> it was expecting a GET and the user never sees the proper data.\n> \n> My suggestion is, as horrible as this is going to sound, that we\n> change the definition of 301/302 to redirect to GET and make 303/304\n> be redirect, permanently or temporarily, with the same method. We\n> can't force the whole world to rewrite all their scripts and our users\n> aren't going to accept \"Well gee, you know, the script is doing the\n> wrong thing, it should send a 303 not a 301/302.\"\n> \n> Yaron\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": " \n\nOn Fri, 25 Jul 1997, Judson Valeski wrote:\n\n>     Although the commentURL attribute would provide a richer context for the cookie to be evaluated in, it is going a step too far.  The comment attribute is sufficient to explain a cookie's purpose.  If it is not, the cookie server can provide a url in the comment attribute that the user/UA can reference for further info. regarding the cookie.  I would consider it bad practice for the url the cookie server sends in the comment attribute to contain cookies, but, content providers can obviously do whatever they want.\n> \n>     As long as the UA allows for examination of cookies, the user has complete control over what cookies he keeps. If the user allows a cookie to be set because he didn't have a commentURL available for evaluation before accepting the cookie, before he issues another request he can examine his cookies and visit any url in any comment attribute he wants. If at that point the user decides he doesn't want that cookie, he can delete it.\n> \n>     I'm not convinced of the need for a separate header just so a url can be explicitly provided.\n\nFirst it is not a separate header, only an attribute associated with a \ncookie. Secondly, has has been pointed out, there is no\ninternationalization support in the comment, thirdly the ability to stick\na URL in comment text ... so what?  If commenturl is a supported\nattribute, service implementors at least have a hint that providing a\nURL is desirable.  UA implementors know that actually implementing \nURL clicking is expected ... suppose the comment includes a URL, do\nyou then expect the user to open a new UA window and cut and paste the\nvalue or do you propose that UA's be required to recognize a URL \nin the comment text and make it clickable ... that is a more complex\nimplementation than what we've proposed and it has all the same worries\nabout nested cookies, etc.\n\n> \n> Judson Valeski - Netscape\n> \n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "On Fri, 25 Jul 1997, Judson Valeski wrote:\n\n> The user would have to look at what the commentURL says and make a decision\n> anyway.\n> \n> > (And what are the odds that they'll even get that\n> > cookie policy at the URL without another cookie being sent to them,\n> > and being asked to accept the same cookie?).  How many people\n> > will actually do it?\n> \n> Not many. But no-one said the UA wouldn't parse the comment attribute, realize\n> there's a url in in and provide essentially the same functionality the\n> commentURL would provide. There would be no copying and pasting.\n\nSure, but now you want us to believe that a UA will parse the comment\nand that is less complex than what is proposed.\n\n> \n> > If you cut CommentURL out, you might as well cut Comment out as well.\n> \n> I don't agree here. A cookie should have some means for describing its purpose\n> (the comment attribute).\n\nAs has been pointed out, a meaningful explanation would require a paragaph\nor more of non-internationalized text, sent with each cookie ... text\nwhich is much harder to isolate from the 'CGI' code and keep current than\na carefully isolated URL identified resource. I believe with a high\ndegree of confidence that CommentURLs would be much heavier in usage\nthan the comment from my perspective as a service author. The CommentURL\nisolates the explanation from the programmer and the code. The comment\nwill come right out of code, be harder to review and keep current,\nnot be sharable as a single site statement of cookie policy, etc.\n\nIf we eliminate something it MUST be the comment attribute as that is\nalmost useless for meaningful communication except as it is extended\nin implementation in include URLs which are parsed out by UAs and\nhandled.  \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "On Fri, 25 Jul 1997, Larry Masinter wrote:\n\n> Since this argument seems to have convinced one person,\n> I thought I would make it more broadly and see if I could convince\n> someone else.\n\nThe suggestion is that CommentURL won't be used much or help many\nusers so it should be dropped.\n\nThe fact is that there has been significant push back on even implementing\nthe IETF cookie specification from the two largest UA vendors. \n\nThere  is also the reasonable hypothesis that most users will, totally\nconfused by the issues and frustrated by the interruptions, simply\nenable all cookies.\n\nOn the basis of lack of deployement or lack of appeal to the general\nuser we should simply drop all concerns about cross domain cookies or\ndrop the cookie issue with a simple RFC which overrides 2068 as not\nimplementable and to be ignored.\n\nI am not advocating that approach, only noting that the same arguments\nmade against the utility of CommentURL apply to the broader set of\nissues.\n\nI do seriously propose dropping the Comment attribute as so inadequate\nfor its stated purpose as to be meaningless. Eliminate the confusion and\nsimplify the protocol.\n\nIn any case, there are clear failings for comment for its stated purpose\nof informing the user of the intended use of the cookie. It would be\nirresponsible protocol design to not provide the more complete approach\nin the protocol.  Protocols insure interoperability and speculation that\na UA might parse the comment field and recognize an arbitrary character\nstring as a URL and so present it to the user is not interoperable \nprotocol design. I would certainly not want to get into design and\nspecificatation of the content of the comment field to make that\nalternataive interoperable. The only technical issue with CommentURL is\nthe question of cookie handling when the commenturl is presented \nduring a cookie inspection dialog. That same exact issue exists for\nURLs imbeded in a comment field.\n\nCommentURL is clean, covers internationalization issues and is infinitely\nextensible from a content perspective. It makes good protocol. We can't\nforce users to use our protocols but we can make sure that the protocol\ndelivers the function described.  Comment does not meet that test.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "David W. Morris wrote:\n\n> cookie. Secondly, has has been pointed out, there is no\n> internationalization support in the comment, thirdly the ability to stick\n> a URL in comment text ... so what?\n\nThe internationalization benefit that comes with the commentURL attribute is a defensible argument and it is quite convincing. I'm wondering if maintaining comment attribute character set information is the solution?\n\n> If commenturl is a supported\n> attribute, service implementors at least have a hint that providing a\n> URL is desirable.  UA implementors know that actually implementing\n> URL clicking is expected ... suppose the comment includes a URL, do\n> you then expect the user to open a new UA window and cut and paste the\n> value or do you propose that UA's be required to recognize a URL\n> in the comment text and make it clickable ...\n\nThis is what I'd propose. I wouldn't consider it a \"requirement\" however. A semi-capable UA would just implement it, if it so desired.\n\n> that is a more complex\n> implementation than what we've proposed and it has all the same worries\n> about nested cookies, etc.\n\nI'm not sure nested cookies are really a concern. If the user is that interested in reading about the cookies being set on his machine, then he can read about every one associated with every commentURL that comes down the pike. If that spins him into a spiral he should tell his content provider not to set cookies when a request for a commentURL comes in. If content providers, malicious or not, don't abide, users will stop viewing commentURLs and then the attribute's very intent has been defeated. Or a simple solution\nwould be to not allow cookies to be set/sent when a request goes out to a commentURL.\n\nI'm beginning to wonder about the need for any comment/commentURL attribute at all. If the user/customer is the one driving this issue (\"I want to know what data is being stored on my machine\"), I'm of the belief that actually he is not, perhaps the same informational URL that has been proposed to be placed in some attribute associated with a cookie should simply be made available on the site the user/customer is visiting (at the discretion of the content provider of course, just like his willingness to fill out a comment\nattribute). Rather than placing the responsibility on the protocol to provide potential confidence about a cookie, shouldn't it be placed on the content provider. Cookies are a mechanism by which one can store potentially stateful information and various bits of data, not one that provides a privacy issue/non-issue communication channel.\n\nJudson Valeski\n\n\n\n"
        },
        {
            "subject": "State Management pre-draft  combinational requiremen",
            "content": "I would like to raise more public discussion about the combinational\nrequirement in the latest State Management pre-draft.\n\nThe current effort at a revision of RFC 2108 began when Koen\npointed out that use of version 1 cookie attributes as in the RFC's\nspecifications for modern Set-Cookie headers are not handled adequately\nby MSIE's implementation for historical cookies.  We all promptly agreed\nthat a revision to ensure backward compatibility with that old implementation\nis important.  In the process of discussing a revision, it also was brought\nout that the blanket port restriction has the side effect of blocking any\ncookie sharing between http and https servers, even when such sharing should\nnot be blocked.  The desireablility of a discard attribute promptly reached\nconsensus, and of a commentURL attribute to accommodate i18n concerns beyond\nthe too meagre comment attribute is being discussed.  The new attributes\n(port, discard and commentURL) all could be incorporated in a simple and\nelegant manner as extensions to the RFC 2108 Set-Cookie header, but to\naccommodate MSIE's parsing problems with that, we decided to create a\nseparate, Set-Cookie2 header for modern cookies.  The issue of how this\naffects UAs which implemented RFC 2108 (e.g., Lynx in its v2.7 and v2.7.1\nformal releases), i.e., if Set-Cookie headers regressed to historical, was\nnot a factor in these discussions.  Though in principle this is to be a\nrevision based implementation experience, it has proceded largely as if\nthere were none, and as if backward compatibility with RFC 2018 is not a\nconsideration (that's OK, don't worry about it :).\n\nThen we received a complaint that having Set-Cookie2 headers for\nmodern cookies and duplicates in Set-Cookie headers for historical UAs\nis a \"problem\" for an organization which makes a practice of setting large\nnumbers of lengthy cookies, so the combinational requirement was born.\nTo get an idea of that \"problem\", try http://www.microsoft.com/ with a\nclean cookie jar.  You will be sent through a series of redirections, and\ncookie replacements, with the ultimate result of your cookie jar acquiring\ntwo seemingly identical cookies for hosts which don't domain match according\neither to RFC 2108 or the current pre-draft.  When I tried today, after the\nmultiple redirections and replacements abated, I ended up with the cookies\nof identical name and value:\n\nDomain=.msn.com\n       ||||||||\nMC1=GUID=5e1a543305d611d188a708002bb74f65\n\nDomain=.microsoft.com\n       ||||||||||||||\nMC1=GUID=5e1a543305d611d188a708002bb74f65\n\nI don't know if those should be considered \"long\" cookie name/value pairs,\nand can only image what might be accomplished if private discussions led\nto the maximum of 5 redirections becoming a minimum, and my UA could be\nlooped through 100 redirections.  Nor am I clear on why the ability to\nload users' often limited resources with large numbers of lenthy, and\nperhaps redundant, cookies should be promoted by the IETF.\n\nThe objections that were posted about the combinational requirement\nmerit more careful consideration, IMHO.  The intent originally was to send\nboth Set-Cookie2 and Set-Cookie headers during a *transitional* period to\nthe modern State Management design,  This is essentially a \"probing\" \nsituation, and as soon as either the UA or server detects that modern\ncookie support is implemented in its State Management partner, only\nSet-Cookie2 headers will be sent to the UA by the server's replies, and\nthe modern Cookie header format will be used in the UAs requests to the\nserver.  The sending of both Set-Cookie2 and Set-Cookie header thus\nwill become limited to just \"first contacts\", which are not likely to\ninvolve large numbers of cookies (except, perhaps in lengthy redirection\nand cookie replacement loops :).\n\nThe principle objection to the combinational requirement is that\nit makes the protocol excessively complex and correspondingly unreliable,\nsuch that it in fact serves to discourage, rather than promote, the\nimplementation of a modern State Management design geared toward more\nnearly adequate protection of users' privacy.   The section on combining\nSet-Cookie and Set-Cookie2 headers (10) says:\n\n  [...]                                           The user agent\n                                                      ^^^^^^^^^^\n  interprets the combined headers as follows.  First, it must establish a\n                                                         ||||^^^^^^^^^^^^\n  one-to-one correspondence between the cookies in the Set-Cookie and\n  ||||||||||^^^^^^^^^^^^^^^\nSet-Cookie2 headers. [...]\n\nHow can the UA establish that *one-to-one* correspondence?  The cookie\nname/value pairs will all be in the Set-Cookie header, and the Set-Cookie2\nheader will have only additional attributes.  The ONLY correspondence check\nwhich a UA can perform is to count the number of version 1 attribute sets\nfolding into a Set-Cookie2 array, and compare that with the number of\nhistorical cookies folded into a Set-Cookie array.  If the two counts\nare not the same, e.g., due to some semi-colon verus comma substitution,\nor any number of possible network transmission glitches, there is NO basis\nhere for efforts at error recovery, or any efforts, whatsoever, for bona fide\ncorrespondence checks.  If the counts are different, for any reason, the\nUA will have no alternative but simply to throw the cookies and attributes\non the floor and hope for better luck next time.\n\n        That section also points out, parenthetically:\n \n        (Note that in this case the Set-Cookie2 response header that\n         the origin server sends does not, by itself, conform to this\n                                 ||||||||^^^^^^^^^^^^^^^^^^^^^^^^^^^^     \n         specification.)\n         ^^^^^^^^^^^^^\nThink about that.  Is a \"transitional\" procedure for sending Set-Cookie2\nheaders which DO NOT CONFORM TO THE SPECIFICATION a way to promote\nimplementation of that specification, or a way to interfere with it's\nimplementation?!?  Think about that carefully, please.\n\n        It is often the case in the real world that the squeeky wheel\ngets oiled first.  But if instead the wheels are being rearranged so\nthat the wagon won't go anywhere, perhaps that is one case in which it\nit truly appropriate to just say no.\n\n                                Fote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: OPTIONS Spe",
            "content": ">JC>  The actual OPTIONS message is contained in the request/response\n>JC>  body.\n>\n>  What about backward compatibility with RFC 2068?  Our current\n>  implementation doesn't expect or send any message body with the\n>  OPTIONS method:\n\nThis is more a reminder to implementers than a response.  The RFC 2068\ndefinition of OPTIONS does allow a body in both the request and the\nresponse -- it just doesn't define what to do with it, and then confuses\nthe issue by a poor example (mea culpa).\n\nWhether or not a message body is present is defined by section 4.3:\n\n   The rules for when a message-body is allowed in a message differ for\n   requests and responses.\n\n   The presence of a message-body in a request is signaled by the\n   inclusion of a Content-Length or Transfer-Encoding header field in\n   the request's message-headers. A message-body MAY be included in a\n   request only when the request method (section 5.1.1) allows an\n   entity-body.\n\n   For response messages, whether or not a message-body is included with\n   a message is dependent on both the request method and the response\n   status code (section 6.1.1). All responses to the HEAD request method\n   MUST NOT include a message-body, even though the presence of entity-\n   header fields might lead one to believe they do. All 1xx\n   (informational), 204 (no content), and 304 (not modified) responses\n   MUST NOT include a message-body. All other responses do include a\n   message-body, although it may be of zero length.\n\nIt is easy to forget about checking for a message body.  I know, because\nI fixed a bug in Apache 1.2.1 last week having to do with this same issue.\nFailing to check is a problem because, if the connection is persistent,\nthe unread message body will be mistaken for the next request.\n\n.....Roy\n\neditorial note: the second sentence in that middle paragraph is bogus.\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "> attribute). Rather than placing the responsibility on the protocol to\n> provide potential confidence about a cookie, shouldn't it be placed on\n> the content provider. Cookies are a mechanism by which one can store\n> potentially stateful information and various bits of data, not one that\n> provides a privacy issue/non-issue communication channel.\n\nThe point is not that the protocol provides confidence by providing either\nthe comment or the commentURL only that the protocol provides the \ncontent provider (aka server) with the ability to easily associate the\ndescription of cookie usage or whatever they'd like to say with the \ncookie.\n\nWhat you propose guarantees that users will never look at the information\nabout cookies.  How do you expect them to find the information on\nNetscape's or IBM's or Micorsoft's sites. The commentURL provides the\nconnection.  Otherwise it's a giant adventure game.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: State Management pre-draft  combinational requiremen",
            "content": " \n\nOn Sat, 26 Jul 1997, Foteos Macrides wrote:\n\n> I would like to raise more public discussion about the combinational\n> requirement in the latest State Management pre-draft.\n\nYes, me too!\n\n> either to RFC 2108 or the current pre-draft.  When I tried today, after the\n> multiple redirections and replacements abated, I ended up with the cookies\n> of identical name and value:\n> \n> Domain=.msn.com\n>        ||||||||\n> MC1=GUID=5e1a543305d611d188a708002bb74f65\n> \n> Domain=.microsoft.com\n>        ||||||||||||||\n> MC1=GUID=5e1a543305d611d188a708002bb74f65\n> \n> I don't know if those should be considered \"long\" cookie name/value pairs,\n\nI certainly don't consider those cookies LONG.\n\n> the modern State Management design,  This is essentially a \"probing\" \n> situation, and as soon as either the UA or server detects that modern\n> cookie support is implemented in its State Management partner, only\n> Set-Cookie2 headers will be sent to the UA by the server's replies, and\n> the modern Cookie header format will be used in the UAs requests to the\n> server.  The sending of both Set-Cookie2 and Set-Cookie header thus\n> will become limited to just \"first contacts\", which are not likely to\n\nExactly my point in my lengthly earlier post.\n\nThe combinatorial rules are complex and will be prone to failure. They\nshould be eliminated.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "> > cookie. Secondly, has has been pointed out, there is no\n> > internationalization support in the comment, thirdly the ability to stick\n> > a URL in comment text ... so what?\n> \n> The internationalization benefit that comes with the commentURL attribute is a defensible argument and it is quite convincing. I'm wondering if maintaining comment attribute character set information is the solution?\n\nSo now you're saying we should add character set information\nto the comment attribute and still forget the commentURL attribute?\nPlease detail your objection to commentURL... It seems so much easier\nand much more versatile than what you're now suggesting.\n> \n> I'm not sure nested cookies are really a concern. If the user is that interested in reading about the cookies being set on his machine, then he can read about every one associated with every commentURL that comes down the pike. If that spins him into a spiral he should tell his content provider not to set cookies when a request for a commentURL comes in. If content providers, malicious or not, don't abide, users will stop viewing commentURLs and then the attribute's very intent has been defeated. Or a simple solution\n> would be to not allow cookies to be set/sent when a request goes out to a commentURL.\n\nYou should probably check out the archive... we've debated the idea of\nnot accepting cookies during these requests, but some have objected,\nsaying basically that it's too dificult to turn cookies off for these\nrequests.\n\nI'm with you 100% here though.  Use CommentURL, and don't allow cookies\nto be set or sent during the retrieval of the data at CommentURL...\n\nLet's get back to the point.\n\nMost people on this list appear to see the benefit in having a comment\nand/or CommentURL.  The question currently is if commentURL should be\nincluded.\n\nWithout some connection between the cookie technology and the polcies\ngoverning their use at this level, (either comment or CommentURL \nor both) the general ignorance of the world towards cookies will continue.  \nNobody wins here except for journalists who like to scare people with how\nevil cookies are.\n\nThe reasons FOR commentURL are many:  (Please add any I missed...)\n1. International Language Support\n2. Less Bandwidth (as compared to Comment)\n3. Much more versatile and \"rich\" than straight text.\n4. A URL is more easily maintained than information in a\nScript, hence it's more likely that the data found\nat CommentURL will be updated than in comment\n\nThe reasons against (and somebody from the other side of the camp\nplease add on here..)\n1. It's too difficult to implement CommentURL\n(Though, strangely, some against CommentURL think it would\nbe better to include CommentURL in Comment, having the\nbrowser search for URL's and handle them appropriately)\n2. Why have commentURL when we have comment...\n\n\nAfter these lists get flushed out, is there a way we can vote on\nit and move on?  It seems like we're beating a dead horse here.\n\nIf it comes down to picking one, I think we'd be crazy not to go with\nCommentURL, but I don't see any real problem with having both.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: Redirect",
            "content": ">>>The argument I have been making with Larry is that we should define a\n>>>minimum number of redirects to be supported by a client but should not\n>>>put a maximum on servers. A server may have a perfectly rational reason\n>>>for wanting to exceed whatever limit we have placed on redirects.\n>>>However, for the sake of interoperability, a server needs to know what\n>>>is the maximum number of redirects a client is guaranteed to support.\n>> \n>>Oh, that's a much easier question: the answer is zero.  You cannot force\n>>a user agent to make a network request (because it may cost them money),\n>>so you cannot guarantee that a client will perform a redirect without\n>>first asking the user for confirmation.\n>\n>Roy, do you even understand why your response makes me upset? Was that\n>your goal? To state the painfully obvious as if it was relevant to the\n>point under consideration is just asinine. Clearly the user can stop any\n>operation. That isn't the issue. The issue is how many times will the\n>system even give the user a choice to stop the operation before it steps\n>in and stops the redirections itself. Currently we have set that limit\n>to 5. I do not believe that is appropriate. Rather I think we should set\n>the minimum to 5. \n\nSorry, if you want me to continue answering your questions, you should\nmake more effort to phrase them correctly and not get upset with me when\nI answer what you actually asked instead of what you intended to ask.\nI never was very good at ESP.\n\nA lot of people seem to think that I have some duty to answer questions\nabout HTTP.  I don't.  Mostly I do so because I like helping other Web\ndevelopers, and other times because it is easier to answer the question\nnow than it is to work-around the resulting browser bug six months from now.\nIn any case, it costs me time, and that is the most valuable thing I don't\nhave right now.\n\n>That the system will present the user at least 5 redirections before\n>just saying \"Something has gone wrong, there is an error, I am ending\n>this operation.\" In cases where the redirection is automatic, then the\n>system would only be required to automatically redirect up to five\n>times. \n\nThe spec doesn't require the agent to end the operation -- it only says\nthat it should pause and get user confirmation that it should continue,\nrather than just blindly redirecting for eternity.  Whether or not the\nnumber 5 is right is a valid question, but there needs to be at least\na note recommending that it be configurable.  In that sense, an\ninfinite number of redirects is currently \"supported\".\n\n>BTW I am not asking for a UI requirement. It is completely appropriate\n>for a browsers, in case of 302 on a POST or similar method, to never\n>even present the redirect to the user for confirmation. This is\n>certainly the case when running in batch mode, for example. However in\n>cases where it is appropriate to present a dialog, at least five rounds\n>of redirections should be supported. If the user wants to end the\n>situation earlier than that, that is the user's prerogative.\n\nBut here is where you lose me.  A redirect is a new request, and therefore\nsubject to the requirement that the user be in control over whether or\nnot a new request is made.  It is therefore impossible for the protocol\nto place any *minimum* on the number of redirects.  If that upsets you,\nI'm sorry, but that is how it is in order to prevent unscrupulous or\nidiotic sites (like pathfinder) from screwing people who pay for their\nnetwork access on a per-request or per-packet basis.  Likewise, an agent\nlike MOMspider will stop traversing the redirects as soon as they leave\nthe traversal space (usually bounded by some URL prefix).  Trying to make\na requirement for something that is so dependent on the context of the\nusage is very hard with HTTP.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": ">1) HTTP 1.0 script is written which expect to get GETs.\n>2) HTTP 1.0 resource is programmed to redirect with a 301/302 to the\n>HTTP/1.0 script\n>3) Server is upgraded to HTTP/1.1 but the HTTP 1.0 resource and the HTTP\n>1.0 script are not upgraded.\n>4) HTTP/1.1 browser comes along and sends a POST to the HTTP 1.0\n>resource and receives a 301/302. HTTP/1.1 browser sends a POST to the\n>HTTP 1.0 script. The HTTP 1.0 script gets completely confused because it\n>was expecting a GET and the user never sees the proper data.\n>\n>My suggestion is, as horrible as this is going to sound, that we change\n>the definition of 301/302 to redirect to GET and make 303/304 be\n>redirect, permanently or temporarily, with the same method. We can't\n>force the whole world to rewrite all their scripts and our users aren't\n>going to accept \"Well gee, you know, the script is doing the wrong\n>thing, it should send a 303 not a 301/302.\"\n\nNo.  These issues were recognized and discussed in detail last year\nand the year before that.  They are not subject to change in the current\ndraft revision.\n\nThe current status hasn't changed in the past two years, so by any\nreasonable definition those scripts (and the browser) have been broken\nfor a long, long time.  The reason that there exists one and only one\nspecial-case method-changing redirect is because, if that were not the\ncase, we would have to duplicate every single redirect code (not just\n301 and 302) just to support that single special case.\n\nThis is guaranteed to cause problems with some existing scripts, but\nthere comes a point when the cost of not changing is far more than the\nshort-term problem of fixing every single CGI script that relies on\nthat bug.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "Jonathan Stark <stark@commerce.net> wrote:\n>[...]\n>You should probably check out the archive... we've debated the idea of\n>not accepting cookies during these requests, but some have objected,\n>saying basically that it's too dificult to turn cookies off for these\n>requests.\n\nI don't recall anyone objecting, at least, not in the public,\narchived discussions, on the grounds that it is too difficult.  The\nobjection was that handling of URLs, including the sending of headers\nin a UA's requests and the processing of headers in the server's\nreplies, is context dependent.  Only the UA \"knows\" the full context,\nand how \"extensive\" it is dependents on a large variety of\nimplementation details which vary across UAs, and on configuration\nand run time options that yield variations within particular\nimplementations.  Most of this discussion has been myopically\nrestricted to one context:  that in which the commentURL is being\nretrieved homologously to an inline or frame and will be presented\nto an interactive user as part of an \"informed consent\" mechanism\nbefore proceding any further with processing of the server's reply\nto the original request.  No one has a crystal ball, but I suspect\nthat will be the rare case, and that if any UA makes that an\nobligatory component of its cookie handling, many users will react\nto it as if it were a seat belt law.  Given past experiences with\n\"unverifiable transactions\" during inlining, it is understandable\nthat this WG would want to be sure that a commentURL will not open\nyet another door to potentially serious invasions of privacy.  The\nspecs, themselves, should ensure that no cookie will be transmitted\ninappropriately, including during commentURL requests.  Others are\nconcerned about problematic loops being created during commentURL\nrequests.  A loop can be broken anywhere in the cycle, the UA is\nthe appropriate place, and any implementor who couldn't deal with\nsomething as obvious as that, in a manner appropriate for the context,\nwhich is co-dependent on details of the implementation and the user's\nconfiguration and run time options, is probabaly not implementing a\nUA you'd want to use, anyway. :)\n\nIn most cases, and particularly first contacts with a site,\nusers may wish to see (or palpate, or listen to) the document before\nmaking any final decisions about cookies that were associated with\nit.  If the document proves not of interest to the user, there may\nbe no point retaining any cookies associated with it, nor in reading\n(or palpating, or listening to) any comments about them (advertisers\nversus users, of course, may disagree about that).  If the user is\ninterested in the document and potentially others via it's links,\nthen review of associated cookies via their commentURLs may be\nappropriate, and less likely to be perceived by the user as yet\nanother unsolicited WebNuisance.  The commentURL might be used days,\nweeks or years after the cookie was received, as part of a \"cleanup\"\nbecause disk space is running out.  A UI might use it for informed\ndecisions on which cookie to dump when making room for a new cookie,\nrather than relying on an arbitrary \"dump the oldest\" heuristic.\n\n\n>[...]\n>The reasons FOR commentURL are many:  (Please add any I missed...)\n>1. International Language Support\n>2. Less Bandwidth (as compared to Comment)\n>3. Much more versatile and \"rich\" than straight text.\n>4. A URL is more easily maintained than information in a\n>Script, hence it's more likely that the data found\n>at CommentURL will be updated than in comment\n>\n>The reasons against (and somebody from the other side of the camp\n>please add on here..)\n>1. It's too difficult to implement CommentURL\n>(Though, strangely, some against CommentURL think it would\n>be better to include CommentURL in Comment, having the\n>browser search for URL's and handle them appropriately)\n>2. Why have commentURL when we have comment...\n\nThat reads like a stacked deck, but indeed reflects how\ncompelling the reasons for versus against have been thus far.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "David W. Morris wrote:\n\n> What you propose guarantees that users will never look at the information\n> about cookies.  How do you expect them to find the information on\n> Netscape's or IBM's or Micorsoft's sites. The commentURL provides the\n> connection.  Otherwise it's a giant adventure game.\n\nI don't know that I've proposed any \"guarantees.\"  What I was getting at was\nthat perhaps content providers should be the ones supplying this link to\ninformation about the cookies they're setting. I don't know/care how they\nwould do it (maybe always having a comment URL hyper-link at the bottom of a\nstateful page or something), the point I was making is that maybe it should be\ntheir responsibility, not ours.\n\nRegarding the possibility of further cookies being sent/set when a request for\na comment URL is made...\nI would like to reiterate that I would consider it bad practice for a content\nprovider to associate cookies with a comment URL, but well within their\nrights. If a comment URL is designed to describe the cookies associated with\nother urls, that should be its only purpose. I see no need to be\nsending/setting cookies with a comment URL; doing so unnecessarily opens a can\nof worms. However, I consider two reactions to this possibility:\n\n1. Business as usual. If the request to a comment URL is made and a set-cookie\nheader is in the response, so be it, the UA takes no action and treats the\ncomment URL like any other. If the user finds himself spinning down a spiral\nof cookie approvals via comment URLs, he can enjoy the ride, or get off.\n\n2. No cookies will be sent or set when a comment URL is in question. The UA\nknows this given url is a comment URL and doesn't send any cookies with the\nrequest for it, nor does it allow any cookies to be set when receiving the\nresponse.\n\nUA reaction #1 is most likely to be implemented, not because #2 is difficult\nby any means, but, because the comment URL is after all simply a url like any\nother.\n\nJudson Valeski\n\n\n\nHaving said that, correct behavior when a request for a comment URL goes out\nwill to not\n\nrom the UA's perspective I'm inclined\n\n>\n>\n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>On Fri, 25 Jul 1997, Larry Masinter wrote:\n>\n>> Since this argument seems to have convinced one person,\n>> I thought I would make it more broadly and see if I could convince\n>> someone else.\n>\n>The suggestion is that CommentURL won't be used much or help many\n>users so it should be dropped.\n>\n>The fact is that there has been significant push back on even implementing\n>the IETF cookie specification from the two largest UA vendors. \n>\n>There  is also the reasonable hypothesis that most users will, totally\n>confused by the issues and frustrated by the interruptions, simply\n>enable all cookies.\n>\n>On the basis of lack of deployement or lack of appeal to the general\n>user we should simply drop all concerns about cross domain cookies or\n>drop the cookie issue with a simple RFC which overrides 2068 as not\n>implementable and to be ignored.\n>\n>I am not advocating that approach, only noting that the same arguments\n>made against the utility of CommentURL apply to the broader set of\n>issues.\n>\n>I do seriously propose dropping the Comment attribute as so inadequate\n>for its stated purpose as to be meaningless. Eliminate the confusion and\n>simplify the protocol.\n\nI don't see why the comment attribute shouldn't be retained\nfor short, telegraph strings like those appended to status lines.\nThat could be helpful when doing a cookie cleanup, at least for\nASCII-speaking users, and the commentURL would be available for\nmore substantive information, hopefully with a negotiable charset\nand language. \n\nMy thoughts on this are based on the hope that there will\nbe follow-through on a PEP or PEP-like extension, down the road,\nwhich would further reduce the number of cookies about which the\nuser must be consulted at time of reception.\n \nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "HTTP status codes (resent",
            "content": "Last week I erroneously sent this message to the wrong address -\ntherefore nobody commented to it :-( \n\nHere there is another try:\n\n% One way or the other, I believe there is going to need to be an IANA registry.\n% Private codes don't work too well, as two different people might\n% still happen to choose the same one.\n\nMaybe we could set up a single HTTP code (x99) for private extensions,\nand write that whoever wants to answer with a x99 code MUST add a \nheader like\n\nMessage-Subcode: <any string>\n\nThis way, things could run more smoothly, even if at the cost of \nsome tens of bytes. Of course, we still need a registry, so that \nextension blessed by someone will be neatly allocated. \n\ncioa, .mau.\n\n\n\n"
        },
        {
            "subject": "Streams connected to http responses",
            "content": "Is it fully up to me how I use the requested data?\n\nCan I connect it to a stream of my choice?\n\nIs there possibly special support for stream like dataflow within HTTP\n(it is probably not related to a protocol how I use the transmitted\n data but maybe ..)\n\n\n \n        Bye   Brusi\n\n            by           E-Mail: ab2@inf.tu-dresden.de\n                         Tel.-priv: 0351-8499347 (Germany/Dresden)\n          \\____\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiation03.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Transparent Content Negotiation in HTTP                 \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-negotiation-03.txt\n       Pages     : 43\n       Date      : 07/25/1997\n\nHTTP allows web site authors to put multiple versions of the same \ninformation under a single URL.  Transparent content negotiation is an \nextensible negotiation mechanism, layered on top of HTTP, for automatically\nselecting the best version when the URL is accessed.  This enables the \nsmooth deployment of new web data formats and markup tags.                 \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-negotiation-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiation-03.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-negotiation-03.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "As I see things, a reasonable solution for Comment-URL could be\nthat servers MUST NOT send any cookie for URLS referenced in Comment-URL,\nand that clients SHOULD discard them if received. \n\nUA, then, MAY let users accept any cookie which come with a Comment-URL\nalready \"accepted\" (and from the same domain, blah blah...). It cannot be\nneither a MUST nor a SHOULD, of course, but it makes sense that if\nyou trust a cookie from a site because you agreed with the policy, it is\nuseless to show it over and over again.\n\n.mau.\n\nps : why is this discussions taking place in http-wg , rather than in\nhttp-state?\n\n\n\n"
        },
        {
            "subject": "Updated proposal for CONTENTENCODING issu",
            "content": "% From: Jeffrey Mogul <mogul@pa.dec.com>\n\n\n%     If no Accept-Encoding field is present in a request, the server MAY\n%     assume that the client will accept any content coding.  In this\n%     case, if \"identity\" is one of the available content-codings, then\n%     the server SHOULD use the \"identity\" content-coding, unless\n%     it has additional information that a different content-coding\n%     is meaningful to the client.\n\nWhile the SHOULD make sense to me, I don't understand the reason for\nthe first MAY. What else could a server do? Reject the request \naltogether?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "LAST CALLs, closed issues, and new HTTP/1.1 draf",
            "content": "Jim Gettys is now editing a revision of the HTTP/1.1 draft\nthat includes text associated with the issues that are closed.\nIn order to get the most effect out of the 7/30 draft,\nit's important to include the resolution of as many issues\nas possible.\n\nThe editing group has identified a number of additional\nproposals that are ready (in our view) for closure, and\nthose issues are now LAST CALL:\n\nCONTENT-ENCODING PROXY-AUTHORIZATION PROXY-LENGTH LANGUAGE-TAG\nT-SPECIALS\n\nThese issues were in LAST CALL before but had substantial\ncomment: QZERO, STATUS100\n\nand these issues: RANGE-ERROR, CLARIFY-NO-CACHE are closed.\n\nThe status and proposed resolution of issues are identified in\n  http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n\nPLEASE, if you wish to discuss an issue, use a separate\nmessage per topic, and identify the issue name in the subject\nline of your message.\n\nI'm hoping Jim will have time to edit in the 'likely'\nresolution of all of the issues, perhaps identifying\nthe ones that we believe are still 'open', so that we\ncan have a single draft that contains most of the\nlikely changes for HTTP/1.1 as we move along standards\ntrack.\n\nI think we're making great progress; I want to thank all\nof you for your participation and focus.\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for CONTENTENCODING issu",
            "content": "    %     If no Accept-Encoding field is present in a request, the server MAY\n    %     assume that the client will accept any content coding.  In this\n    %     case, if \"identity\" is one of the available content-codings, then\n    %     the server SHOULD use the \"identity\" content-coding, unless\n    %     it has additional information that a different content-coding\n    %     is meaningful to the client.\n    \n    While the SHOULD make sense to me, I don't understand the reason for\n    the first MAY. What else could a server do? Reject the request \n    altogether?\n    \nOne alternative interpretation would be that the server could only\nsend the \"identity\" coding (i.e., no Content-Encoding response header)\nif the client sends no Accept-Encoding field.  After some discussion\na few months ago, we rejected this because it would break some\nexisting applications (for example, \"robot\" clients that are used\nto retrieve the contents of an entire site).  Hence, the specification\nexplicitly allows the server to send something when it doesn't have\nan \"identity\" content-coding available.\n\nWe added the advice that if you have a choice of encodings, and one of\nthose choices is \"identity\", and the client hasn't indicated an\nability to decode anything else, then you should normally send\n\"identity\".  This should eliminate the possibility that a client\nreceives something which it doesn't understand, unless nothing\nelse is available.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "On Mon, 28 Jul 1997, Maurizio Codogno wrote:\n\n> As I see things, a reasonable solution for Comment-URL could be\n> that servers MUST NOT send any cookie for URLS referenced in Comment-URL,\n\nIt makes no sense to place any requirement on the server since it has no\nway of knowing the usage context for a particular URL request AND all\nissues can be easily handled by the client.\n\n> and that clients SHOULD discard them if received. \n> \n> UA, then, MAY let users accept any cookie which come with a Comment-URL\n> already \"accepted\" (and from the same domain, blah blah...). It cannot be\n> neither a MUST nor a SHOULD, of course, but it makes sense that if\n> you trust a cookie from a site because you agreed with the policy, it is\n> useless to show it over and over again.\n\nI agree, but useless is a UI usability issue, not a protocol issue.\nTake a look at the exact wording in Dave's latest pre-draft.\n\n> \n> .mau.\n> \n> ps : why is this discussions taking place in http-wg , rather than in\n> http-state?\n\nBecause we are now about to have LAST-CALL and the whole WG must be\ninvolved.\n\n\n\n"
        },
        {
            "subject": "ISSUE: CHARSET DEFAUL",
            "content": "There is one more issue I would like to be made an ISSUE.\n\nIt is the second part of Section 3.7.1, dealing with Text Defaults.\n\nThe wording in this part was arived at after careful deliberation,\nand I don't want to rehash these deliberations.\n\nHowever, part of these deliberations was the fact that \"some older\nHTTP/1.0 clients did not deal properly with an explicit charset\nparameter\" (last paragraph of 3.7.1). The number of such \"older\nclients\" certainly has decereased since this was written. A adjustment\nof the text may therefore be in order.\n\nAlso, the text says \"Some HTTP/1.0 software has interpreted a\nContent-Type header without charset parameter incorrectly to\nmean \"recipient should guess\"\". While this indeed may be\n\"incorrect\" according to some wishful specification, it is\nnot very much up to actual web reality. If current browsers\ndidn't provide me with an option to guess the encoding, I\nwould not be able to view more than 99% of Japanese (or you\nname it) web pages. In addition to this, HTML (both RFC 2070\nand HTML 4.0) defines on its own the priorities for \"charset\"\ninformation; the same applies to XML (with different priorities).\nIt would be desirable if the HTTP spec was compatible with\nthese specs.\n\nIf desired, I can draft some new wording.\n\nRegrads,Martin.\n\n\n\n"
        },
        {
            "subject": "CACHING editorial issue..",
            "content": "I asked Jeff to draft this, as I had trouble seeing what needed to be\ndone.  I'm forwarding it to the list in the attached message for archival\nreasons.\n- Jim\n\n\n\nattached mail follows:\n    Is marked as needing an editorial clarification; I'm not quite sure what\n    clarification is needed (I've read the underlying mail through),\n    as there is a cross reference already to 14.45.\n\nI think it might be clarified by replacing:\n\n  2. It is \"fresh enough\" (see section 13.2). In the default case, this\n     means it meets the least restrictive freshness requirement of the\n     client, server, and cache (see section 14.9); if the origin server\n     so specifies, it is the freshness requirement of the origin server\n     alone.\n\n  3. It includes a warning if the freshness demand of the client or the\n     origin server is violated (see section 13.1.5 and 14.45).\n\n  4.\n\nwith\n\n  2. It is \"fresh enough\" (see section 13.2). In the default case, this\n     means it meets the least restrictive freshness requirement of the\n     client, origin server, and cache (see section 14.9); if the origin\n     server so specifies, it is the freshness requirement of the origin\n     server alone.\n\n     If a stored response is not \"fresh enough\" by the most restrictive\n     freshness requirement of both the client and the origin server, in\n     carefully considered circumstances the cache may still return the\n     response with the appropriate Warning header (see section 13.1.5\n     and 14.45), unless such a response is prohibited (e.g., by a\n     \"no-store\" cache-directive, or by a \"no-cache\"\n     cache-request-directive; see section 14.9).\n\n  3. \n\n(i.e., renumber item 4 as item 3).  I also included the word \"origin\"\nbefore \"server\" in the first paragraph of item #2, to conform to our\nnormal terminology.\n\nThe point of this change is to conform to the language above this list,\nwhich requires a response \"which meets one of the following conditions\"\n(NOTE: the \"which\" should be a \"that\", according to Strunk & White),\nand to make it clear that the use of a Warning is necessary in a case\nwhere one of the parties is trying to relax the requirements imposed by\nanother one.  I.e., in the original itemization, some responses were\nonly legal if they met TWO of the conditions, not just one.\n\nI'm still not sure this section is as clear as it could be, but\nI think we risk reopening a blusterous debate if we try to rewrite\nit again.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "CommentURL questio",
            "content": "Reading through the current thread of mail messages\non including a comment URL as an attribute to a cookie,\nI find I cannot determine whether there is any\nlimitation on the location of the comment URL.  In\nother words, does the comment URL need to be on the\nsame server as the resource producing the cooking\nabout which it comments?\n\nIf not, does the server on which it resides need to\ndomain-match the server from which the cookie is\nproduced?\n\nI raise these issues because there are a number of\nerror conditions which can arise when the comment\nURL does not reside on the same server which do not\notherwise come into play, and I am concerned with\nthe complexity that handling those error conditions\nraises.\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "Ted Hardie wrote:\n> \n> Reading through the current thread of mail messages\n> on including a comment URL as an attribute to a cookie,\n> I find I cannot determine whether there is any\n> limitation on the location of the comment URL.  In\n> other words, does the comment URL need to be on the\n> same server as the resource producing the cooking\n> about which it comments?\n> \n> If not, does the server on which it resides need to\n> domain-match the server from which the cookie is\n> produced?\n> \n> I raise these issues because there are a number of\n> error conditions which can arise when the comment\n> URL does not reside on the same server which do not\n> otherwise come into play, and I am concerned with\n> the complexity that handling those error conditions\n> raises.\n\nThe new (proposed) draft imposes no restrictions on where CommentURL may\nreside.  So I guess you'd better reaise your concerns about error\nconditions.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "New content negotiation drafts availabl",
            "content": "I revised all four internet drafts connected to transparent content\nnegotiation, and put the new versions (.txt, .html, changebars, etc)\nin the usual place:\n\n http://gewis.win.tue.nl/~koen/conneg/\n\nThe new drafts are:\n\n1. draft-ietf-http-negotiation-03.txt \n     `Transparent Content Negotiation in HTTP' \n     Defines the core mechanism. Experimental track. \n\n2. draft-ietf-http-rvsa-v10-01.txt \n     `HTTP Remote Variant Selection Algorithm -- RVSA/1.0' \n     Defines the remote variant selection algorithm version 1.0. \n     Experimental track. \n\n3. draft-ietf-http-feature-reg-02.txt \n     `Feature Tag Registration Procedures' \n     Defines feature tag registration. Best Current Practice track. \n\n4. draft-ietf-http-feature-scenarios-01.txt \n     `Feature Tag Scenarios' \n     Discusses feature tag registration. Informational track.\n\nThere are no major changes in these drafts, except for some\nimprovements in the text and some resynchronisation.  Most of the\nimprovements are due to comments from Graham Klyne.\n\nI hope to freeze 1. and 2. above as experimental RFCs soon after\nMunich.\n\nHappy reading!\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "AUTHCHUNKED proposed resolutio",
            "content": "JG> Could you update your draft in the mailing list (at\nJG> http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q2/0063.html) to deal\nJG> with these comments and resend to the list, so we can last call it?\n\n  I've appended all of section '3.6 Transfer Codings' with the\n  proposed changes below.\n\n  In addition, replace the word 'footer' with 'trailer' in the second\n  bullet item in section 8.2 (which discusses a client aborting the\n  in-progress transmission of a request body when an error is received\n  from the server).\n\n  The resolution also says to move Content-Length from the\n  entity-header list in section 7.1 to general-header in section 4.5.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n3.6 Transfer Codings\n\n   Transfer coding values are used to indicate an encoding\n   transformation that has been, can be, or may need to be applied to an\n   entity-body in order to ensure \"safe transport\" through the network.\n   This differs from a content coding in that the transfer coding is a\n   property of the message, not of the original entity.\n\n          transfer-coding         = \"chunked\" | transfer-extension\n\n          transfer-extension      = token\n\n   All transfer-coding values are case-insensitive. HTTP/1.1 uses\n   transfer coding values in the Transfer-Encoding header field (section\n   14.40).\n\n   Transfer codings are analogous to the Content-Transfer-Encoding\n   values of MIME , which were designed to enable safe transport of\n   binary data over a 7-bit transport service. However, safe transport\n   has a different focus for an 8bit-clean transfer protocol. In HTTP,\n   the only unsafe characteristic of message-bodies is the difficulty in\n   determining the exact body length (section 7.2.2), or the desire to\n   encrypt data over a shared transport.\n\n   The chunked encoding modifies the body of a message in order to\n   transfer it as a series of chunks, each with its own size indicator,\n   followed by an optional trailer containing entity-header fields. This\n   allows dynamically-produced content to be transferred along with the\n   information necessary for the recipient to verify that it has\n   received the full message.\n\n\n        Chunked-Body   = *chunk\n                         last-chunk\n                         trailer\n                         CRLF\n\n        chunk          = chunk-size [ chunk-ext ] CRLF\n                         chunk-data CRLF\n\n        chunk-size     = 1*HEX\n\n        last-chunk     = 1*(\"0\") [ chunk-ext ] CRLF\n\n        chunk-ext      = *( \";\" chunk-ext-name [ \"=\" chunk-ext-value ] )\n        chunk-ext-name = token\n        chunk-ext-val  = token | quoted-string\n        chunk-data     = chunk-size(OCTET)\n\n        trailer         = *entity-header\n\n   The chunked encoding is ended by any chunk whose size is zero,\n   followed by the trailer, which is terminated by an empty line.  The\n   purpose of the trailer is to provide an efficient way to supply\n   information about an entity that is generated dynamically.\n   Applications MUST NOT send header fields in the trailer which are not\n   explicitly defined as being appropriate for the trailer.\n\n   The Content-MD5 header (section 14.16) is appropriate for the trailer.\n\n   The Authentication-Info header defined by RFC 2069 (An Extension to\n   HTTP: Digest Access Authentication), or its successor is appropriate\n   for the trailer.\n\n   An example process for decoding a Chunked-Body is presented in\n   appendix 19.4.6.\n\n   All HTTP/1.1 applications MUST be able to receive and decode the\n   \"chunked\" transfer coding, and MUST ignore transfer coding extensions\n   they do not understand. A server which receives an entity-body with a\n   transfer-coding it does not understand SHOULD return 501\n   (Unimplemented), and close the connection. A server MUST NOT send\n   transfer-codings to an HTTP/1.0 client.\n\n\n\n"
        },
        {
            "subject": "Issue CONTENTENCODIN",
            "content": "About the CONTENT-ENCODING issue which is now in last call: I do not\nthink the draft should be changes in the proposed way.\n\nThe issue is not whether the `original authors of 1.0' intended, but\nforgot to, add q parameters to content encoding.  I don't think so,\nbut we do not need to discuss that.\n\nThe issue is whether q parameters can be added now without breaking too\nmany things.\n\nRoy's message\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0299.html\n\nseems to indicate that adding q parameters might well lead to breakage.\n\nWithout some degree of evidence that this breakage *won't* occur, I\nthink that going through with the CONTENT-ENCODING fix is ill-advised.\n\nThe evidence does not have to be exhaustive, but it has to be better\nthan what I have seen so far.  I argue against making this change in\nparsing rules, because we do not understand its possible effects well\nenough.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "On Mon, 28 Jul 1997, Ted Hardie wrote:\n\n> Reading through the current thread of mail messages\n> on including a comment URL as an attribute to a cookie,\n> I find I cannot determine whether there is any\n> limitation on the location of the comment URL.  In\n> other words, does the comment URL need to be on the\n> same server as the resource producing the cooking\n> about which it comments?\n\nIntentionally no ... with good citizen seals and other future\npossibilities, there may be no correlation we could see at the\nprotocol level and in the near term case, it seems reasonable that\nIBM would have a corporate policy on use of the cookie information,\nowned by the legal folks.  That policy URL would exist on www.ibm.com\nbut be referenced by www.lotus.com, www.hursley.ibm.com, etc.\n\n> \n> If not, does the server on which it resides need to\n> domain-match the server from which the cookie is\n> produced?\n\nAnswered above.\n\n> \n> I raise these issues because there are a number of\n> error conditions which can arise when the comment\n> URL does not reside on the same server which do not\n> otherwise come into play, and I am concerned with\n> the complexity that handling those error conditions\n> raises.\n\nOK, what errors exist which aren't appropriately UI design issues?\nWe have warned about recursion and suggested handling. Beyond that,\nthis URL is just a URL and except where absolutely necessary should\nbehave as any other URL. It shouldn't matter too much whether the user\nclicks the URL in a cookie accept dialog, from a cookie history management\ndialog or from a WWW page.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-negotiation-03.txt (resend",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Transparent Content Negotiation in HTTP                 \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-negotiation-03.txt\n       Pages     : 43\n       Date      : 07/25/1997\n\nHTTP allows web site authors to put multiple versions of the same \ninformation under a single URL.  Transparent content negotiation is an \nextensible negotiation mechanism, layered on top of HTTP, for automatically\nselecting the best version when the URL is accessed.  This enables the \nsmooth deployment of new web data formats and markup tags.                 \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-negotiation-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-negotiation-03.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-negotiation-03.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "On Jul 28,  1:32pm, David W. Morris wrote:\n\n> Intentionally no ... with good citizen seals and other future\n> possibilities, there may be no correlation we could see at the\n> protocol level and in the near term case, it seems reasonable that\n> IBM would have a corporate policy on use of the cookie information,\n> owned by the legal folks.  That policy URL would exist on www.ibm.com\n> but be referenced by www.lotus.com, www.hursley.ibm.com, etc.\n\nThanks for clarifying this.  I certainly see why someone might\nwant to reference a third-party hosted policy, whether it be\nhosted on a \"main\" corporate server, some policy org, or even\nat some site whose policy you just happen to admire.  Inserting\na third pary into a transaction for verification of anything\nhas some known dangers, however (in fact, I have an odd memory\nof Brian Behlendorf raising a related issue a very long time\nago).\n\nMinimally, the user-agent must be able to deal with the situation in\nwhich a comment-url is present but the site is unreachable or very\nslow.  What, in particular, does it do with its connection to the\ncookie-providing site?  If there is a user and she has requested to\napprove cookies, does it close the connection until approval? If not,\ncan it or should it prevent the connection from closing, and if so,\nwhat would be the best method for doing so?  A HEAD against the\nrequested resource to make sure it has not changed?  If the server\ncloses the connection during this processing, should a client continue\nto try to reach the comment-url site and gain acceptance, or should it\npresent an error?  What happens if the user accepts the policy but,\nupon reconnect, a different cookie is presented?  (In general, once a\npolicy has been approved for a specific resource, should a UA consider\nit in force if the same URL is visited, even if a different cookie is\npresented, provided the same policy is referenced?  That may seem like\na no-brainer, but the first view of a cookie at a site may show much\nless than a view twenty items into the shopping basket later.  When\nshould someone be asked to re-view the policy and cookie?)\n\nSorry for entering this discussion late with these questions.  I can\nsee that they are closely related to some of the others which have\nbeen raised, but they seem to me different enough to be worth voicing.\nPlease feel free to point me to archived discussions of these points\nif I missed them in my look at the issue,\n\nbest regards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "Ted Hardie wrote:\n> [...]\n> Minimally, the user-agent must be able to deal with the situation in\n> which a comment-url is present but the site is unreachable or very\n> slow.  What, in particular, does it do with its connection to the\n> cookie-providing site?  If there is a user and she has requested to\n> approve cookies, does it close the connection until approval? If not,\n> can it or should it prevent the connection from closing, and if so,\n> what would be the best method for doing so?  A HEAD against the\n> requested resource to make sure it has not changed?  If the server\n> closes the connection during this processing, should a client continue\n> to try to reach the comment-url site and gain acceptance, or should it\n> present an error?  What happens if the user accepts the policy but,\n> upon reconnect, a different cookie is presented?  (In general, once a\n> policy has been approved for a specific resource, should a UA consider\n> it in force if the same URL is visited, even if a different cookie is\n> presented, provided the same policy is referenced?  That may seem like\n> a no-brainer, but the first view of a cookie at a site may show much\n> less than a view twenty items into the shopping basket later.  When\n> should someone be asked to re-view the policy and cookie?)\n> [...]\n\nThe CommentURL mechanism assists the user in making a decision.  With\nthat in mind, the answer to your questions is, I think, the UA tells the\nuser what happened.  If we're talking about an inspection mechanism at\n\"the port of entry\" (when a cookie accompanies a new page and before the\nuser has viewed the page), the user probably has a choice of whether or\nnot to accept the cookie.  Examining the comment URL is a way for the\nuser to make an informed choice.  If the UA reports it can't fetch the\nCommentURL, the user still has that choice, just with less information\nthan s/he hoped for.\n\nAssuming a sophisticated enough cookie inspection mechanism that would\nlet the user select cookie inspection behavior on a per-site basis, the\nuser can decide whether or not to inspect each cookie from a given site\nas it arrives.  If, after looking at the first cookie from a site, the\nuser decides not to inspect each one, I would expect him/her still to be\nable to inspect the cookies in the cookie jar later.\n\nI think the only guard against a site that describes cookie policy one\nway in one place and differently elsewhere is social pressure.  I don't\nthink the UA should try to guard against it.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Requiring everyone to re-write their scripts is too heavy a price to pay\nfor protocol purity. We must accept that backwards compatibility is a\nrequirement and support 301/302 as they have been deployed. That doesn't\nstop us from introducing 303 as the \"correct\" way to redirect to GET and\nit doesn't stop us from declaring that we will not introduce any new\nresponses dealing with redirect to GET, but we need to support 301/302\nas deployed.\n\nYaron\n\n\n\n> -----Original Message-----\n> From:Roy T. Fielding [SMTP:fielding@kiwi.ics.uci.edu]\n> Sent:Saturday, July 26, 1997 11:03 PM\n> To:Yaron Goland\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: 301/302 \n> \n> >1) HTTP 1.0 script is written which expect to get GETs.\n> >2) HTTP 1.0 resource is programmed to redirect with a 301/302 to the\n> >HTTP/1.0 script\n> >3) Server is upgraded to HTTP/1.1 but the HTTP 1.0 resource and the\n> HTTP\n> >1.0 script are not upgraded.\n> >4) HTTP/1.1 browser comes along and sends a POST to the HTTP 1.0\n> >resource and receives a 301/302. HTTP/1.1 browser sends a POST to the\n> >HTTP 1.0 script. The HTTP 1.0 script gets completely confused because\n> it\n> >was expecting a GET and the user never sees the proper data.\n> >\n> >My suggestion is, as horrible as this is going to sound, that we\n> change\n> >the definition of 301/302 to redirect to GET and make 303/304 be\n> >redirect, permanently or temporarily, with the same method. We can't\n> >force the whole world to rewrite all their scripts and our users\n> aren't\n> >going to accept \"Well gee, you know, the script is doing the wrong\n> >thing, it should send a 303 not a 301/302.\"\n> \n> No.  These issues were recognized and discussed in detail last year\n> and the year before that.  They are not subject to change in the\n> current\n> draft revision.\n> \n> The current status hasn't changed in the past two years, so by any\n> reasonable definition those scripts (and the browser) have been broken\n> for a long, long time.  The reason that there exists one and only one\n> special-case method-changing redirect is because, if that were not the\n> case, we would have to duplicate every single redirect code (not just\n> 301 and 302) just to support that single special case.\n> \n> This is guaranteed to cause problems with some existing scripts, but\n> there comes a point when the cost of not changing is far more than the\n> short-term problem of fixing every single CGI script that relies on\n> that bug.\n> \n> ....Roy\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "On Jul 28,  6:15pm, Dave Kristol wrote:\n\n> The CommentURL mechanism assists the user in making a decision.  With\n> that in mind, the answer to your questions is, I think, the UA tells the\n> user what happened.  If we're talking about an inspection mechanism at\n> \"the port of entry\" (when a cookie accompanies a new page and before the\n> user has viewed the page), the user probably has a choice of whether or\n> not to accept the cookie.  Examining the comment URL is a way for the\n> user to make an informed choice.  If the UA reports it can't fetch the\n> CommentURL, the user still has that choice, just with less information\n> than s/he hoped for.\n\nI understand that the comment URL is not meant to be a true\nverfication mechanism, but, like Larry, I worry that the percentage\ngain in providing it may not match the cost.  To deal with a comment\nURL we will already need advice/rules about providing cookies with the\nresource referenced in the comment URL.  I was trying to point out\nthat we might also need advice/rules about how to treat the open\nconnection to the original resource during an inspection.  If\ninspections are common, we could be asking servers to hold open a\nlarge number of persistent connections while a relatively slow thing\n(a user inspection) happens.  That has a cost.  If the connection is\nmaintained by the UA \"pinging\" the server with a HEAD, we've also got\nbytes on the wire that impact everybody and aren't actually sending\ninformation anywhere.\n\nIn contrast, if we don't provide that advice and connections normally\nclose while inspections occur, there are consequences either to how\ncookies are created (so that the same client is highly likely to get\nthe same cookie back on a request made in a short time frame, rather\nthan highly unlikely as now) or how the UA manages the relationship\nbetween the approved inspection and the cookies it receives.\n\nFrankly, I'm not sure that all of the management cost and user\neducation cost is worth the marginal (and hopefully short-term) gain.\nI fully support inclusion of comments which indicate certification or\neven asserted well-known policies.  But doing this with\nindividually-inspectable URLs does not seem to be a clear win to me.\nI worry in particular that allowing such URLs will encourage every\ncorporate lawyer to have a policy, rather than relying on well know\npolicies; that is, admittedly, probably paranoid, but I was raised by\nlawyers and I know how they can think.  Given the ease of changing\nresources on the web, I would also want to be able to do a HEAD\nagainst the policy in a comment URL once every session I interacted\nwith the resource, just to be sure the policy hadn't changed.  (Note\nthat I do not say I would always use that ability, just that I would\nwant *at least* that ability).  If even a small percentage of the net\nbehaves as I would, we have a lot of additional overhead.\n\nOf course, I may what Phill would call \"a corner case\", but I hope\nyou'll think about the cost vs. gain ratios one more time.\n\nbest regards,\nTed Hardie\nNASA NIC\n\n\nNB: NASA was not raised by lawyers, and some of the ones which raised\nme have since repented.\n\n\n\n"
        },
        {
            "subject": "Re: CommentURL questio",
            "content": "On Mon, 28 Jul 1997, Ted Hardie wrote:\n\n> On Jul 28,  6:15pm, Dave Kristol wrote:\n> \n> > The CommentURL mechanism assists the user in making a decision.  With\n> > that in mind, the answer to your questions is, I think, the UA tells the\n> > user what happened.  If we're talking about an inspection mechanism at\n> > \"the port of entry\" (when a cookie accompanies a new page and before the\n> > user has viewed the page), the user probably has a choice of whether or\n> > not to accept the cookie.  Examining the comment URL is a way for the\n> > user to make an informed choice.  If the UA reports it can't fetch the\n> > CommentURL, the user still has that choice, just with less information\n> > than s/he hoped for.\n> \n> I understand that the comment URL is not meant to be a true\n> verfication mechanism, but, like Larry, I worry that the percentage\n> gain in providing it may not match the cost.  To deal with a comment\n> URL we will already need advice/rules about providing cookies with the\n> resource referenced in the comment URL.  I was trying to point out\n> that we might also need advice/rules about how to treat the open\n> connection to the original resource during an inspection.  If\n> inspections are common, we could be asking servers to hold open a\n> large number of persistent connections while a relatively slow thing\n> (a user inspection) happens.  That has a cost.  If the connection is\n> maintained by the UA \"pinging\" the server with a HEAD, we've also got\n> bytes on the wire that impact everybody and aren't actually sending\n> information anywhere.\n> \n> In contrast, if we don't provide that advice and connections normally\n> close while inspections occur, there are consequences either to how\n> cookies are created (so that the same client is highly likely to get\n> the same cookie back on a request made in a short time frame, rather\n> than highly unlikely as now) or how the UA manages the relationship\n> between the approved inspection and the cookies it receives.\n\nFirst, I would hope, though I must admit I've not tested the question,\nthat UA would receive the complete response w/o consideration for \nwhether the user wished to accept the cookie. It would be bad programming\nto keep the connection open while asking the user about accepting the\ncookie, in particular since there is no suggestion in the cookie\ninspection UIs I've used or in the cookie spec that the response should\nbe suppressed if the user suppresses the cookie. This is already an issue\nif the UA cookie dialogs hold the connection open waiting for user\ninteraction so I don't see it as a significant change in the status quo.\n\n> Frankly, I'm not sure that all of the management cost and user\n> education cost is worth the marginal (and hopefully short-term) gain.\n> I fully support inclusion of comments which indicate certification or\n> even asserted well-known policies.  But doing this with\n> individually-inspectable URLs does not seem to be a clear win to me.\n\nIndividual URLs are the lingua franca (sp?) of the web. As I've said\nbefore, I'm not convinced that the whole official IETF cookie\ndefinition provides much gain over cost when we consider the real\npossiblity that the vast majority of users won't have the opportunity\nto use the facilities and protections we've crafted.\n\nBut we've defined a baseline. Part of our argument has been for\ninformed consent. It is hard to argue in my mind that we support\ninformed consent when the only protocol supported mechanism for \ncommunication between the application/server and the user is a \ncomment string.  URLs are the mechanism of information on the WEB and\nas such are the 'right' approach here as well.\n\n> I worry in particular that allowing such URLs will encourage every\n> corporate lawyer to have a policy, rather than relying on well know\n> policies; that is, admittedly, probably paranoid, but I was raised by\n\nIf you grew up with lawyers, then you know that getting two lawyers\nto agree on sharing a single well known policy is even harder than\ngetting an IETF working group to produce a cookie standard, probably\nby several orders of magnitude. So we can expect lawyers to insist\non having their own statement.\n\n> lawyers and I know how they can think.  Given the ease of changing\n> resources on the web, I would also want to be able to do a HEAD\n> against the policy in a comment URL once every session I interacted\n\nWith a URL to check and track, you would at least be able to use existing\nautomated tools which will check web resources for change. W/o the URL,\nthe only choice would be to read each comment value, which if it was\nseveral lines long, might have a subtle change you'd never see. \n\n> with the resource, just to be sure the policy hadn't changed.  (Note\n> that I do not say I would always use that ability, just that I would\n> want *at least* that ability).  If even a small percentage of the net\n> behaves as I would, we have a lot of additional overhead.\n\nThe difficultly is that I believe it unlikely that a comment value will\nbe changed when the policy changes ... comment values will be buried in\ncode unrelated to policy implementation ... code which is on the front\nline which if not updated carefully could cause breakage. So I'm a\nreal skeptic when it comes to believing the comment. I also know it would\nbe impossible to find a cookie policy on most web sites so the commentURL\nallows a protocol defined interoperable mechanism for connecting the\ncookie to whatever descriptive material the publisher wants the user\nto see.\n\nI think the main benefit of everything we're currently doing with Cookie2\nis to provide a foundation upon which something like Dan Jaye's proposal\ncan build. I believe CommentURL fills a critical gap and complements\nDan's proposals.\n\nDave Morris \n\n\n\n"
        },
        {
            "subject": "Re: an (unofficial) cookie draf",
            "content": "On Fri, 25 Jul 1997, Dave Kristol wrote:\n\n> Major changes since state-man-mec-02:\n> \n> - add a $Port attribute to Cookie.  (I spontaneously added this\n> when I realized it was needed for consistency with the\n> $Path and $Domain.)\n\nI think your words (on page 7 of the .ps chgbar version) aren't quite\nright ...  the port attribute on the setcookie can have three states:\n\n         not specified\n         port\n         port=\"portnum,...\"\n\nIt isn't clear to me that your words allow for the 2nd case since \nthere is no value for the attribute.   (Did the $ get dropped from\nthe text in my postscript version?)  Anyway how about:\n\n    The $port attribute must match the port attribute specified on the\n    corresponding set-cookie2 header. That is, if port was omitted\n    from the set-cookie2 response, $port must be omitted from the\n    cookie: request header otherwise $port must be specified without\n    a value if the set-cookie2 port attribute was specified without\n    a value or $port must be specified with the same value specified\n    for the port attribute.\n\nAs I've stated previously, I continue to oppose the provisions in\nsection 10.1 which require the combination of values between the\nset-cookie and set-cookie2 header fields.\n\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "IETF in Munich: 8/11 &amp; 8/1",
            "content": "We've been given two sessions at IETF:\n\n Monday, August 11 at 0930-1130 (opposite svrloc, roamops, idr,\n                issll)\n Tuesday, August 12 at 1300-1515 (opposite isn, dnsind, drums, \n                bmwg, pppext, ngtrans, idmr, ipc)\n\nOur primary agenda is to review the HTTP/1.1 draft, the other\ndrafts that we're processing, and also to decide on the \nschedule for finishing work.\n\nI'll be putting together a proposed agenda over the next couple\nof days.\n\nPlease remind me if there are other agenda items; in the meanwhile,\nthis might influence your travel plans.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "A server could distinguish between old HTTP/1.0 scripts \nand new HTTP/1.1 ones, and either rewrite the response\nor change the version number, or support another URL that\nthe redirected POST would redirect to, or change the version\nof the response...\n\nEven though it's awkward to do any of these things\ndoesn't mean that it's wrong.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "When I first wrote up a (not widely circulated) draft that introduced\nSet-Cookie2, I made it an independent header.  That is, it was to be\ntreated independent of Set-Cookie, except that Set-Cookie2 for a given\ncookie would supersede Set-Cookie for the same cookie.  I switched to the\n\"additive\" solution that appears in the state-man-mec-xx I-Ds after concern\nwas raised that a server would have to send both Set-Cookie and Set-Cookie2\nall the time, and, if the cookies were big, that would comprise a lot of\nbytes.  The \"additive\" solution was to splice together cookies (actually,\nattributes for cookies) from Set-Cookie2 onto corresponding cookies carried\nin Set-Cookie, so the cookie name and value would only have to appear once,\nwhich reduces the concern about the cookie's length.\n\nFoteos Macrides and Dave Morris have expressed concern about the complexity\nof the additive approach.  Indeed, from private conversation, I understand\nthat Foteos has some implementation experience that would support his point\nof view.\n\nFurthermore, both Foteos and Dave point out that a server would not need to\nsend Set-Cookie and Set-Cookie2 headers with each response.  A server could\nsend both headers the first time it instantiates a cookie and see what\nflavor cookie comes back.  If it's an old-style cookie, the server makes\nfurther responses using Set-Cookie.  If it's a new-style cookie, the server\nmakes further responses using Set-Cookie2.  Both cookies need not be sent.\n\nIn light of their observations, particularly Foteos's concerns about how\nreliably the Set-Cookie and Set-Cookie2 headers can be aligned in\none-to-one correspondence, I think it would be safer to shift from the\ncurrent \"additive\" back to my original \"independent\" headers solution.  I\nbelieve the problem of cookie size can be solved, as I described.\n\nThe one rub is how to handle gracefully a transition from old-style\n(\"Netscape\") cookies to new-style cookies.  The scenario I just described\nabove locks the dialog into either old- or new-style cookies, because the\nserver only sends one header, either Set-Cookie or Set-Cookie2.  What if\nthe user upgrades the UA to one that understands new-style cookies, but the\ncurrent cookie dialogs it engages in use old-style cookies?  We would like\nthe UA and server to begin exchanging new-style cookies.  But neither side\nknows that the other understands new-style cookies.  Who goes first to make\ntheir capabilities known?\n\nI can think of two approaches, and I'm sure there are others.  One approach\nis just to have the server periodically send both Set-Cookie and\nSet-Cookie2 as a probe.  The cookie value might, for example contain the\ntimestamp of the last contact with the server, and the server could send\nboth headers once each day/week/whatever.\n\nSimpler still, a UA that supports new-style cookies should be sending\nCookie: $Version=0\nfollowed by other cookie values.  If the server understands new-style\ncookies, it could respond to further requests with Set-Cookie2.\n\nI invite supporters of the \"additive\" approach out there who have remained\nsilent until now to speak up to defend it.  Otherwise I intend to switch to\nthe \"independent\" approach I describe here.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-rvsa-v1002.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : HTTP Remote Variant Selection Algorithm -- RVSA/1.0     \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-rvsa-v10-02.txt\n       Pages     : 10\n       Date      : 07/28/1997\n\nHTTP allows web site authors to put multiple versions of the same \ninformation under a single URL.  Transparent content negotiation is a \nmechanism for automatically selecting the best version when the URL is \naccessed.  A remote variant selection algorithm can be used to speed up the\ntransparent negotiation process. This document defines the remote variant \nselection algorithm with the version number 1.0.                           \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-rvsa-v10-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-rvsa-v10-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-rvsa-v10-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-feature-scenarios01.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Feature Tag Scenarios                                   \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-feature-scenarios-01.txt\n       Pages     : 8\n       Date      : 07/28/1997\n\nRecent Internet applications, such as the World Wide Web, tie together a \ngreat diversity in data formats, client and server platforms, and \ncommunities.  This has created a need for various kinds of negotiation \nmechanisms, which tailor the data which is exchanged, or the exchange \nprocess itself, to the capabilities and preferences of the parties \ninvolved.               \n                                                   \nExtensible negotiation mechanisms need a vocabulary to identify various \nthings which can be negotiated on.  To promote interoperability, a \nregistration process is needed to ensure that that this vocabulary, which \ncan be shared between negotiation mechanisms, is developed in an orderly, \nwell-specified, and public manner.            \n                             \nThis document discusses requirements and scenarios the registration of this\nvocabulary, which is the vocabulary of feature tags.  Feature tag \nregistration is foreseen as an ongoing, open process which will keep pace \nwith the introduction of new features by software vendors, and other \nparties such as standards bodies.                                          \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-feature-scenarios-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-feature-scenarios-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-feature-scenarios-01.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-feature-reg02.tx",
            "content": " A Revised Internet-Draft is available from the on-line Internet-Drafts \n directories. This draft is a work item of the HyperText Transfer Protocol \n Working Group of the IETF.                                                \n\n       Title     : Feature Tag Registration Procedures                     \n       Author(s) : K. Holtman, A. Mutz\n       Filename  : draft-ietf-http-feature-reg-02.txt\n       Pages     : 12\n       Date      : 07/28/1997\n\nRecent Internet applications, such as the World Wide Web, tie together a \ngreat diversity in data formats, client and server platforms, and \ncommunities.  This has created a need for various kinds of negotiation \nmechanisms, which tailor the data which is exchanged, or the exchange \nprocess, to the capabilities and preferences of the parties involved. \n   \nExtensible negotiation mechanisms need a vocabulary to identify various \nthings which can be negotiated on.  To promote interoperability, a \nregistration process is needed to ensure that that this vocabulary, which \ncan be shared between negotiation mechanisms, is developed in an orderly, \nwell-specified, and public manner.       \n                                  \nThis document defines registration procedures which use the Internet \nAssigned Numbers Authority (IANA) as a central registry for this \nvocabulary, which is the vocabulary of feature tags.                       \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-feature-reg-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-feature-reg-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa:  ftp.is.co.za                    \n                                                \n     o  Europe:  ftp.nordu.net            \n                 ftp.nis.garr.it                 \n                                                \n     o  Pacific Rim: munnari.oz.au               \n                                                \n     o  US East Coast: ds.internic.net           \n                                                \n     o  US West Coast: ftp.isi.edu               \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-feature-reg-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: an (unofficial) cookie draf",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n  > On Fri, 25 Jul 1997, Dave Kristol wrote:\n  > \n  > > Major changes since state-man-mec-02:\n  > > \n  > > - add a $Port attribute to Cookie.  (I spontaneously added this\n  > > when I realized it was needed for consistency with the\n  > > $Path and $Domain.)\n  > \n  > I think your words (on page 7 of the .ps chgbar version) aren't quite\n  > right ...  the port attribute on the setcookie can have three states:\n  > \n  >          not specified\n  >          port\n  >          port=\"portnum,...\"\n  > \n  > It isn't clear to me that your words allow for the 2nd case since \n  > there is no value for the attribute.   (Did the $ get dropped from\n\n(Gee, the '$' appears in mine.  Which '$' is missing in yours?)\nYes, I agree the second case was left out.\n\n\n  > the text in my postscript version?)  Anyway how about:\n  > \n  >     The $port attribute must match the port attribute specified on the\n  >     corresponding set-cookie2 header. That is, if port was omitted\n  >     from the set-cookie2 response, $port must be omitted from the\n  >     cookie: request header otherwise $port must be specified without\n  >     a value if the set-cookie2 port attribute was specified without\n  >     a value or $port must be specified with the same value specified\n  >     for the port attribute.\n\nYeah, something like that.\n  > [...]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for OPTIONS issu",
            "content": "I agree with Scott that Public and Allow should be end-to-end, but it\nwould be nice to hear from some client implementers on this issue...\n\n** Reply to note from \"Scott Lawrence\" <lawrence@agranat.com> Fri, 25 Jul 1997 09:47:21 -0400\n>   \n>   \n> JM> (8) [For the moment, two possible alternatives here!]\n>   \n>   I'm afraid that I can't decipher whether it is (8a) or (8b) that I\n>   like or whether I don't like either one.  Simply stated, I would\n>   prefer that proxies never modify either Public or Allow headers;\n>   they should always be end-to-end headers.  I choose this alternative\n>   because I think it is easier for a client to discover what is going\n>   on this way.\n>   \n>   If proxies DO modify the Public and Allow headers, an end-client\n>   can't tell whether it is the origin server or the proxy that doesn't\n>   support what it wants because it sees the same response from both.\n>   While it is true that the client may have another way to reach the\n>   origin server, if that is not the case there will be no way for the\n>   user to determine that the proxy is the problem.  Since many users\n>   are behind proxies they cannot get around, there would be no way for\n>   them to discover that the reason they can't use some service is that\n>   the proxy is missing something.\n>   \n>   If proxies DO NOT modify Public and Allow headers, the methods\n>   supported by the origin server can be discovered by sending an\n>   OPTIONS request, and the methods supported by each proxy can be\n>   discovered by sending an OPTIONS request with a Max-Forwards\n>   header.\n>   \n>   This would be a slight change to RFC 2068 definition for the\n>   treatment of these headers by proxies, but as far as I can tell\n>   there are no deployed 2068 proxies out there yet, so I don't think\n>   that is a practical concern.  Even if there were, a client could\n>   detect that a proxy was 2068-only by sending OPTIONS with a\n>   Compliance header to the proxy and get back an OPTIONS response\n>   without a Compliance header.\n>   \n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "On Tue, 29 Jul 1997, Foteos Macrides wrote:\n\n> dmk@research.bell-labs.com (Dave Kristol) wrote:\n> >Simpler still, a UA that supports new-style cookies should be sending\n> >Cookie: $Version=0\n> >followed by other cookie values.  If the server understands new-style\n> >cookies, it could respond to further requests with Set-Cookie2.\n> \n> I just tried sending  Cookie: $Version=0; realcookie=realvalue\n> for:\n> \n>    Linkname: HTML Form-Testing Home Page\n>         URL: http://www.research.digital.com/nsl/formtest/home.html\n> \n> and got back:\n>                         Form Test Results for General1\n>                                        \n>    Test results:\n>      * NetscapeCookie - Bad HTTP Cookie value: $Version=0; COOKIE=testvalue\n> [...]\n\n\nWhat if the client sends the $version= by itself? It is legal to send\nmultiple cookie headers and while folding is possible in theory, I'd\nguess it doesn't happen much in practice, but could be wrong, and the\nfolding wouldn't be applied to an unknown header field name...\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "On Tue, 29 Jul 1997, David W. Morris wrote:\n\n> \n> On Tue, 29 Jul 1997, Foteos Macrides wrote:\n> \n> > dmk@research.bell-labs.com (Dave Kristol) wrote:\n> > >Simpler still, a UA that supports new-style cookies should be sending\n> > >Cookie: $Version=0\n> > >followed by other cookie values.  If the server understands new-style\n> > >cookies, it could respond to further requests with Set-Cookie2.\n> > \n> > I just tried sending  Cookie: $Version=0; realcookie=realvalue\n> > for:\n> > \n> >    Linkname: HTML Form-Testing Home Page\n> >         URL: http://www.research.digital.com/nsl/formtest/home.html\n> > \n> > and got back:\n> >                         Form Test Results for General1\n> >                                        \n> >    Test results:\n> >      * NetscapeCookie - Bad HTTP Cookie value: $Version=0; COOKIE=testvalue\n> > [...]\n> \n> \n> What if the client sends the $version= by itself? It is legal to send\n> multiple cookie headers and while folding is possible in theory, I'd\n> guess it doesn't happen much in practice, but could be wrong, and the\n> folding wouldn't be applied to an unknown header field name...\n\nAnyway, getting rejected by a verification site probably isn't a problem?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "Foteos Macrides:\n>\n[...]\n>The problem is that you're counting on $Version=0; being treated\n>as a bad cookie by the old server or script, and don't know what error\n>handling it's using.  It indeed would be desireable for the UA to\n>communicate that it supports version 1 or greater cookie handling in\n>those cases for which it is using cached version 0 cookies and doesn't\n>yet know if the server or script can handle new cookies.  The simplest,\n>most efficient way might be to send:\n>\n>Cookie2: $Version=\"1\"\n>Cookie: realoldnameA=realoldvalueA; realoldnameB=realoldvalueB[; ...]\n\nI very much support this way of doing things.  I think we have to\nassume, until it is proven otherwise, that a lot of old-style CGI\nscripts out there will break on seeing extra version stuff in the\nCookie header.  Some old style scripts may even do consistency checks\nwhich cause them to go into some error recovery mode if they see\ncookies with version stuff in it.\n\nTransmitting the `I can do cookie-2' info in an extra header is the\nonly safe way.\n\n>Fote\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n>[...]\n>The one rub is how to handle gracefully a transition from old-style\n>(\"Netscape\") cookies to new-style cookies.  The scenario I just described\n>above locks the dialog into either old- or new-style cookies, because the\n>server only sends one header, either Set-Cookie or Set-Cookie2.  What if\n>the user upgrades the UA to one that understands new-style cookies, but the\n>current cookie dialogs it engages in use old-style cookies?  We would like\n>the UA and server to begin exchanging new-style cookies.  But neither side\n>knows that the other understands new-style cookies.  Who goes first to make\n>their capabilities known?\n>\n>I can think of two approaches, and I'm sure there are others.  One approach\n>is just to have the server periodically send both Set-Cookie and\n>Set-Cookie2 as a probe.  The cookie value might, for example contain the\n>timestamp of the last contact with the server, and the server could send\n>both headers once each day/week/whatever.\n>\n>Simpler still, a UA that supports new-style cookies should be sending\n>Cookie: $Version=0\n>followed by other cookie values.  If the server understands new-style\n>cookies, it could respond to further requests with Set-Cookie2.\n\nI just tried sending  Cookie: $Version=0; realcookie=realvalue\nfor:\n\n   Linkname: HTML Form-Testing Home Page\n        URL: http://www.research.digital.com/nsl/formtest/home.html\n\nand got back:\n                        Form Test Results for General1\n                                       \n   Test results:\n     * NetscapeCookie - Bad HTTP Cookie value: $Version=0; COOKIE=testvalue\n[...]\n\n\nThe problem is that you're counting on $Version=0; being treated\nas a bad cookie by the old server or script, and don't know what error\nhandling it's using.  It indeed would be desireable for the UA to\ncommunicate that it supports version 1 or greater cookie handling in\nthose cases for which it is using cached version 0 cookies and doesn't\nyet know if the server or script can handle new cookies.  The simplest,\nmost efficient way might be to send:\n\nCookie2: $Version=\"1\"\nCookie: realoldnameA=realoldvalueA; realoldnameB=realoldvalueB[; ...]\n\nIf it's an old server or script, the Cookie2 request header will be\nignored.  Otherwise, the server or script will use Set-Cookie2, the\nUA will not send the Cookie2 probe, and use Cookie: $Version=\"1\"; ... \nso the updating is mutually know by the State Management partners\nwith negligable excess network traffic having been expended, and with\nboth the server/script and UA thereafter enjoyed all the benefits\nof the modern State Management protocol.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "\"David W. Morris\" <dwm@xpasc.com> wrote:\n>\n>On Tue, 29 Jul 1997, David W. Morris wrote:\n>> \n>> On Tue, 29 Jul 1997, Foteos Macrides wrote:\n>> \n>> > dmk@research.bell-labs.com (Dave Kristol) wrote:\n>> > >Simpler still, a UA that supports new-style cookies should be sending\n>> > >Cookie: $Version=0\n>> > >followed by other cookie values.  If the server understands new-style\n>> > >cookies, it could respond to further requests with Set-Cookie2.\n>> > \n>> > I just tried sending  Cookie: $Version=0; realcookie=realvalue\n>> > for:\n>> > \n>> >    Linkname: HTML Form-Testing Home Page\n>> >         URL: http://www.research.digital.com/nsl/formtest/home.html\n>> > \n>> > and got back:\n>> >                         Form Test Results for General1\n>> >                                        \n>> >    Test results:\n>> >      * NetscapeCookie - Bad HTTP Cookie value: $Version=0; COOKIE=testvalue\n>> > [...]\n>> \n>> \n>> What if the client sends the $version= by itself? It is legal to send\n>> multiple cookie headers and while folding is possible in theory, I'd\n>> guess it doesn't happen much in practice, but could be wrong, and the\n>> folding wouldn't be applied to an unknown header field name...\n>\n>Anyway, getting rejected by a verification site probably isn't a problem?\n\nIf it's not a verification site, then the only \"feedback\" you\nmight get is that your \"preferences\" cookie didn't get you a document\nwith your display preferences, or you might infer a cookie exchange\nsnafoo that caused items to fall out of your \"shopping cart\" when you\nattempt to make a purchase and don't get anything, or not everything\nyou wanted to buy, etc., etc.\n\nThe historical implementation did not adhere to standard MIME\nheader construction and parsing principles.  And many scripts for\nhandling historical cookies were written by people who don't know\nthose principles, any more than they know the difference between\n302 versus 303 statuses, but simply used their BrandX UA as if it\nwere a validator for their scripts.  After having field tested a\nUA's implementation of historical Set-Cookie parsing and historical\nCookie transmissions, the implementors may be reluctant to mess\naround with an implementation which works \"well enough\" most of the\ntime.\n\nBy adding an explicit Cookie2: $Version=\"1\" (or equivalent)\nprobe header, historical servers/scripts and UAs can go on conducting\nbusiness as usual, and while the majority of deployed UAs are historical,\nany server/script can use only Set-Cookie headers without concern that\nthat modern UAs will not be detected and a switch to Set-Cookie2 headers\nbe made immediately.  Conversely, designers of servers and script writers\nwho feel that the attributes of the modern State Management protocol are\ncritically important, e.g. for its additional protections against Cookie\nSpoofing (see Section 8.2 of the current pre-draft), or because they need\nthe port attribute honored, etc., can send only Set-Cookie2 headers and\nmake there stateful pages useful to progessively more UAs as their\nimplementors catch up.  Everyone can have his/her cake and eat it too,\neven if initially it's just crumbs for some.\n\nScripts which take advantage of the modern State Management\nprotocol can be written and used immediately, by anyone with programming\nskills, and shared with non-programmers.  Similarly, modern servers are\nreadily extensible.  It thus makes the most sense for the UA to signal\nits cookie handling capabilities to the server/script.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Scripts are deployed and they are not going to get re-written. \n\nServers have no mechanism for determining if a script is 1.0 or 1.1 and\neven if they did, they do not have facilities for taken any special\naction in that case. \n\nAs such clients which want to work against the majority of redirected\nresources, almost all of whom have upgraded to the new Apache servers\nand thus are declaring themselves 1.1, will be forced to treat 301/302\nas redirect to GET.\n\nLets accept the need for backwards compatibility and make it easy for\npeople to upgrade to 1.1. 301/302 have been taken over, we can't undo\nthat. So lets mark them as \"deprecated - backwards compatibility\" and\nmove on with new numbers.\n\nYaron\n\n> -----Original Message-----\n> From:Larry Masinter [SMTP:masinter@parc.xerox.com]\n> Sent:Tuesday, July 29, 1997 1:36 AM\n> To:Yaron Goland\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: 301/302\n> \n> A server could distinguish between old HTTP/1.0 scripts \n> and new HTTP/1.1 ones, and either rewrite the response\n> or change the version number, or support another URL that\n> the redirected POST would redirect to, or change the version\n> of the response...\n> \n> Even though it's awkward to do any of these things\n> doesn't mean that it's wrong.\n> \n> Larry\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Roy T. Fielding:\n>\n  [Yaron:]\n>>My suggestion is, as horrible as this is going to sound, that we change\n>>the definition of 301/302 to redirect to GET and make 303/304 be\n>>redirect, permanently or temporarily, with the same method. We can't\n>>force the whole world to rewrite all their scripts and our users aren't\n>>going to accept \"Well gee, you know, the script is doing the wrong\n>>thing, it should send a 303 not a 301/302.\"\n>\n>No.  These issues were recognized and discussed in detail last year\n>and the year before that.  They are not subject to change in the current\n>draft revision.\n\nWell, they *could* become subject to change based on implementation\nexperience.  \n\nPut it simply, if Yaron can find a some sites which depend on\nredirects after POST requests and break horribly when the POST stays a\nPOST, then I would be very inclined to support:\n\n a) declaring 301/302 obsolete, historic, and unsafe, \n b) allowing browsers to change the 301/302 method into a GET\n    if they think they can be more compatible with obsolete software\n    by doing so, and \n c) assigning the semantics originally intended for 301/302 to\n    fresh codes.\n\n>The current status hasn't changed in the past two years, so by any\n>reasonable definition those scripts (and the browser) have been broken\n>for a long, long time.\n\nCGI script maintenance is a pain, even if these scripts are provably\nbroken in some sense.  I see no reason why 1.1 should inflict CGI\nmaintenance costs which could be avoided by some spec maintenance.\n\n>  The reason that there exists one and only one\n>special-case method-changing redirect is because, if that were not the\n>case, we would have to duplicate every single redirect code (not just\n>301 and 302) just to support that single special case.\n\nI think you mean the above to be a proof that declaring 301 and 302\nobsolete leads to an explosion of complexity, but I fail to follow\nyour line of reasoning.\n\n>This is guaranteed to cause problems with some existing scripts, but\n>there comes a point when the cost of not changing is far more than the\n>short-term problem of fixing every single CGI script that relies on\n>that bug.\n\nI don't think we are anywhere near that point for the 301/302 issue.\nIf someone can show problems with existing scripts, I think we have\nall the reason we need to revise the spec.\n\n>....Roy\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Issues-list item &quot;CACHINGCGI&quot",
            "content": "he following got dropped on the floor, but for good cleanlyness\nreasons, I'm sending it to the list in case someone has some problem with\ndeclaring it closed, as it was marked otherwise in the issues list.\n\n                                - Jim Gettys\n\n\n\n\nattached mail follows:\nI sent the appended message to the list 12 days ago.  While a few other\nmessages with the same Subject: line appeared on the list after\nI posted that message, none were in direct response to it.\n\nTherefore, I suggest that you treat this issue as CLOSED,\nwith the resolution being the two-paragraph update to section\n13.9 proposed in this message.\n\n-Jeff\n------- Forwarded Message\n\nReturn-Path: http-wg-request@cuckoo.hpl.hp.com\nReceived: by acetes.pa.dec.com; id AA18655; Wed, 16 Apr 97 14:47:13 -0700\nReceived: by pobox1.pa.dec.com; id AA08115; Wed, 16 Apr 97 14:47:12 -0700\nReceived: from palrel1.hp.com by mail2.digital.com (5.65 EXP 4/12/95 for V3.2/1.0/WV)id AA19122; Wed, 16 Apr 1997 14:42:56 -0700\nReceived: from cuckoo.hpl.hp.com (cuckoo.hpl.hp.com [15.144.62.116]) by palrel1.hp.com with ESMTP (8.7.5/8.7.3) id OAA02734; Wed, 16 Apr 1997 14:41:50 -0700 (PDT)\nReceived: (from procmail@localhost) by cuckoo.hpl.hp.com (8.7.1/8.7.1) id RAA17737; Wed, 16 Apr 1997 17:41:29 -0400 (EDT)\nResent-Date: Wed, 16 Apr 1997 17:41:29 -0400 (EDT)\nMessage-Id: <9704162132.AA18610@acetes.pa.dec.com>\nTo: http-wg@cuckoo.hpl.hp.com\nFrom: Jeffrey Mogul <mogul@nospam.org>\nSubject: Re: Issues-list item \"CACHING-CGI\" \nIn-Reply-To: Your message of \"Tue, 15 Apr 97 17:12:15 MDT.\"             <9704160012.AA15744@acetes.pa.dec.com> \nDate: Wed, 16 Apr 97 14:32:40 MDT\nSender: mogul@pa.dec.com\nResent-Message-Id: <\"nRG-O3.0.1L4.4UKLp\"@cuckoo>\nResent-From: http-wg@cuckoo.hpl.hp.com\nX-Mailing-List: <http-wg@cuckoo.hpl.hp.com> archive/latest/3081\nX-Loop: http-wg@cuckoo.hpl.hp.com\nPrecedence: list\nResent-Sender: http-wg-request@cuckoo.hpl.hp.com\n\nWow, things are confused.  Part of this is my fault, for writing\nsomething that wasn't as precise as it should have been.\n\nMost of the confusion is on the part of people who do not distinguish\nbetween \"what a CGI script should send\" and \"what a cache should do\nwhen a CGI script doesn't send what it should have sent\".\n\nOF COURSE, CGI-generated responses (and all other origin server\nresponses, too) should include either an explicit \"don't cache\nthis\" marking (e.g., \"Cache-control: max-age=0\") or an explicit\nexpiration time.  Of course, of course, of course.\n\nThat wasn't the question.  The question (which I stated too\ninformally) was \"what happens when the response is NOT clearly\nmarked as to cachability?\"  This is a *different* question.\n\nCurrent practice seems to be split.  Ari says that the CERN\nand Netscape proxies never cache a response without a Last-Modified\nheader. (With HTTP/1.1, this rule would presumably change to\n\"without either a Last-Modifed or Etag header.\")  However,\nthe practice in the Squid world seems to be different.  I'm\nnot sure I fully understand the Squid code, but the version\nI looked at seems to allow caching of a response without\na Last-Modified header.\n\nI was tasked by the working group meeting last week to address the\nspecific issue of CGI, not the larger issue of whether a response\nwithout Last-Modified should be cached.  based on my belief that the\nHTTP/1.1 spec should not discourage caching unnecessarily (reflecting\nwhat Roy wrote earlier, that the Web \"depends on accurate caching to\nreduce network costs\"), I constructed my proposed Note to reflect the\nlooser approach used by Squid.\n\nSo let's take these issues separately.  If someone wants to\npropose a specification change (or a new Note for the spec)\nthat says \"do not cache responses without a Last-Modified header\",\nthat's fine with me, although it would be a good idea to\ncombine this proposal with evidence (from a real-life proxy)\nthat this doesn't significantly reduce caching in today's Internet.\n\nBack to the CACHING-CGI issue.  My original proposal was sloppy\nin that it didn't make a distinction between \"cache and reuse\nwithout revalidation\" or \"cache but must revalidate\".  And I\nforgot to include \"htbin\" as being more or less equivalent\nto \"cgi-bin\" (and yes, there are still lots of htbin URLs in\nactive use).  Also, I violated the informal rule that Notes should\nnot use terms like \"SHOULD\".\n\nHere's a revised version, to replace the second paragraph\nin section 13.9:\n\nSome HTTP/1.0 cache operators have found that it is dangerous\nto cache and reuse without revalidation responses to requests\nfor URLs that include any of the strings \"cgi-bin\", \"htbin\", or\n\"?\", because applications have traditionally used these URLs in\nconjunction with operations with significant side effects for\nGET or HEAD methods.  However, if such a response includes an\nexplicit, future, expiration time, then this implies that the\nresponse may be cached and reused without revalidation until it\nexpires.  If such a response includes a Last-Modified or Etag\nheader, this implies that the response may be reused after\nrevalidation (or without revalidation if explicitly fresh).\n\nA cache MUST NOT assign a heuristic expiration time to a\nresponse for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n\"?\" in its rel_path part.  If such a response does not \ncarry an explicit expiration time, it must be treated as\nif it expires immediately.\n\nThis does two things: (1) it clarifies that a cache can indeed\nfollow its usual caching with \"?\" and \"cgi-bin\" responses, if\nthey are explicitly marked to allow caching, and (2) we tighten\nup the rules on assigning heuristic expiration times for such\nresponses, because of the known risks of this specific situation.\n\n- -Jeff\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>Roy T. Fielding:\n>>\n>  [Yaron:]\n>>>My suggestion is, as horrible as this is going to sound, that we change\n>>>the definition of 301/302 to redirect to GET and make 303/304 be\n>>>redirect, permanently or temporarily, with the same method. We can't\n>>>force the whole world to rewrite all their scripts and our users aren't\n>>>going to accept \"Well gee, you know, the script is doing the wrong\n>>>thing, it should send a 303 not a 301/302.\"\n>>\n>>No.  These issues were recognized and discussed in detail last year\n>>and the year before that.  They are not subject to change in the current\n>>draft revision.\n>\n>Well, they *could* become subject to change based on implementation\n>experience.  \n\nOK, here's some implementation experience.\n\nWay back when when 303 persisted through 2 or 3 HTTP/1.1\ndrafts, we thought, \"Great, we can implement that and start handling\n301/302 validly.\"  Upon formal release of a version with that change,\nwe were barraged with false bug reports that Lynx doesn't work with\nsuch and such search service, but it works fine with BrandX and\nBrandY.  In the next formal release, when prompting about whether\nto redirect a form submission with POST content, we included another\noption (I'll use the en versions of the prompts):\n\n Server asked for redirection of POST content to [URL]\nP)roceed, use G)ET or C)ancel:\n\nWe felt that\n\nP)roceed, U)se 303 or C)ancel:\n\nwas too arcane, and that\n\n respect the protocol and F)ail,\nviolate the protocol and S)ucceed, or C)ancel:\n\nwas too snide.  We've had no problems since offering the three options\n(with the first prompt for en users).\n\nIf a change were to be made, I don't see any reason for changing\n301.  It's almost never used for form ACTIONs (I've never encountered it)\nand if it's a permanent change, presumeably future submissions of a form\nwith METHOD=POST should still use POST.\n\nSo it's a question of simply swapping the meanings of 302 and 303.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "[Fwd: 301/302",
            "content": "-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\n\nattached mail follows:\nKoen Holtman wrote:\n> \n> >The current status hasn't changed in the past two years, so by any\n> >reasonable definition those scripts (and the browser) have been broken\n> >for a long, long time.\n> \n> CGI script maintenance is a pain, even if these scripts are provably\n> broken in some sense.  I see no reason why 1.1 should inflict CGI\n> maintenance costs which could be avoided by some spec maintenance.\n> \n> Koen.\n\nWell, what about when foo.cgi is running on a 1.1 server, and\ndoesnt give a content-length.\nIs the 1.1 server responsible to detect and chunk that ?\n\n-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\n\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: 301/302",
            "content": "Josh Cohen wrote:\n\n> Koen Holtman wrote:\n> >\n> > >The current status hasn't changed in the past two years, so by any\n> > >reasonable definition those scripts (and the browser) have been broken\n> > >for a long, long time.\n> >\n> > CGI script maintenance is a pain, even if these scripts are provably\n> > broken in some sense.  I see no reason why 1.1 should inflict CGI\n> > maintenance costs which could be avoided by some spec maintenance.\n> >\n> > Koen.\n> \n> Well, what about when foo.cgi is running on a 1.1 server, and\n> doesnt give a content-length.\n> Is the 1.1 server responsible to detect and chunk that ?\n\nAn HTTP/1.1 server can either\n- send Connection: close, return the content without chunking, and\nclose the connection; or\n- chunk the content\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: [Fwd: 301/302",
            "content": "On Tuesday, July 29, 1997 3:43 PM, Josh Cohen [SMTP:josh@netscape.com]\nwrote:\n> -- \n> Well, what about when foo.cgi is running on a 1.1 server, and\n> doesnt give a content-length.\n> Is the 1.1 server responsible to detect and chunk that ?\n\nThat's the approach that we decided to take in our server.   It's done\npretty\nmuch across the board with non-file-based content:  CGI, ISAPI\napplications\nor ADI modules (ADI is our own server API)\n\nSteve Wingard\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: 301/302",
            "content": "Dave Kristol wrote:\n> \n> > Well, what about when foo.cgi is running on a 1.1 server, and\n> > doesnt give a content-length.\n> > Is the 1.1 server responsible to detect and chunk that ?\n> \n> An HTTP/1.1 server can either\n>         - send Connection: close, return the content without chunking, and\n>                 close the connection; or\n>         - chunk the content\n> \nYes, but wasnt this the intent of chunking in the first place?\n(dyamic content)\n\n> Dave Kristol\n\n-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: 301/302",
            "content": "Josh Cohen wrote:\n> \n> Dave Kristol wrote:\n> >\n> > > Well, what about when foo.cgi is running on a 1.1 server, and\n> > > doesnt give a content-length.\n> > > Is the 1.1 server responsible to detect and chunk that ?\n> >\n> > An HTTP/1.1 server can either\n> >         - send Connection: close, return the content without chunking, and\n> >                 close the connection; or\n> >         - chunk the content\n> >\n> Yes, but wasnt this the intent of chunking in the first place?\n> (dyamic content)\n\nWe seem to be talking past each other, or something.  I don't understand\nyour remark.\n\nYes, the intent of chunking is dynamic content.  Well, actually, in my\nmind the purpose of chunking is to provide a way to demarcate the end of\ndynamic content so a server can keep its connection open.  So the\npreferred behavior is that, if an HTTP/1.1 server supports CGIs, for\nexample, and the CGI does not provide a Content-Length, and the client\nis also HTTP/1.1, the server can add a \"Transfer-Encoding: chunked\"\nheader to the response, chunk the output, and keep the connection open. \nBut it's not required.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "Foteos Macrides wrote:\n\n> [...]\n> \n>         The problem is that you're counting on $Version=0; being treated\n> as a bad cookie by the old server or script, and don't know what error\n> handling it's using.  It indeed would be desireable for the UA to\n> communicate that it supports version 1 or greater cookie handling in\n> those cases for which it is using cached version 0 cookies and doesn't\n> yet know if the server or script can handle new cookies.  The simplest,\n> most efficient way might be to send:\n> \n> Cookie2: $Version=\"1\"\n> Cookie: realoldnameA=realoldvalueA; realoldnameB=realoldvalueB[; ...]\n> \n> If it's an old server or script, the Cookie2 request header will be\n> ignored.  Otherwise, the server or script will use Set-Cookie2, the\n> UA will not send the Cookie2 probe, and use Cookie: $Version=\"1\"; ...\n> so the updating is mutually know by the State Management partners\n> with negligable excess network traffic having been expended, and with\n> both the server/script and UA thereafter enjoyed all the benefits\n> of the modern State Management protocol.\n\nI like this approach.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "I've put together (a bit hurriedly) a new prospective state management\nI-D that has my proposed wording for \"independent\" handling of\nSet-Cookie and Set-Cookie2.  It also includes Foteos Macrides's idea\nabout Cookie2.  The changes are all in section 10.1, \"Compatibility with\nExisting Implementations.\"\n\nThere are also some other minor changes to reflect a private discussion\nwith Dave Morris.  I changed the term \"domain name\" to be \"host domain\nname\", which is the term that RFC 2068 and RFC 1123 use.\n\nThe document can be reached below the horizontal rule at\n<http://portal.research.bell-labs.com/~dmk/cookie-ver.html>, with the\nusual change bars available.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "As Foteos hinted, swapping the meaning of 302 and 303 is a solution\nto the implementation problem.  I don't think it would affect Apache much.\nHowever, it would require universal agreement among the rest of the\nimplementers, and it would require recycling HTTP/1.1 as Proposed\nand not as a Draft Standard.  It is not something to be taken lightly.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": ">Here's a revised version, to replace the second paragraph\n>in section 13.9:\n>\n>Some HTTP/1.0 cache operators have found that it is dangerous\n>to cache and reuse without revalidation responses to requests\n>for URLs that include any of the strings \"cgi-bin\", \"htbin\", or\n>\"?\", because applications have traditionally used these URLs in\n>conjunction with operations with significant side effects for\n>GET or HEAD methods.  However, if such a response includes an\n>explicit, future, expiration time, then this implies that the\n>response may be cached and reused without revalidation until it\n>expires.  If such a response includes a Last-Modified or Etag\n>header, this implies that the response may be reused after\n>revalidation (or without revalidation if explicitly fresh).\n>\n>A cache MUST NOT assign a heuristic expiration time to a\n>response for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n>\"?\" in its rel_path part.  If such a response does not \n>carry an explicit expiration time, it must be treated as\n>if it expires immediately.\n\nI'm pretty sure I said this before, but I don't know what list.\nI am completely opposed to this change.  It is inaccurate to say that\ncaching and reusing such responses is \"dangerous\".  The *only* reason\n*some* caches do not provide heuristic caching of such responses is\nbecause the presence of query-based parameters make it unlikely to get\na second \"hit\" on the cache, and because the the absence of a Last-Modified\n(and now Etag) makes it impossible to do an efficient update.  In any case,\nthis is an optimization which is dependent on the context and number of\nusers of the cache, and not a requirement of the protocol.\n\nThe protocol already provides mechanisms for marking a response as\nnon-cachable.  All other responses to a GET request are cachable.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote:\n>As Foteos hinted, swapping the meaning of 302 and 303 is a solution\n>to the implementation problem.  I don't think it would affect Apache much.\n>However, it would require universal agreement among the rest of the\n>implementers, and it would require recycling HTTP/1.1 as Proposed\n>and not as a Draft Standard.  It is not something to be taken lightly.\n\nUnfortunately, when I get stuff I wrote back from a list server\nand re-read it, it often becomes clear that I don't understand what\nI'm talking about.  For a 301 on a POST, does that really mean\nsubstitute the new RequestURI for all future submissions, or only\nwhen the content is identical to that of the current submission?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": ">Unfortunately, when I get stuff I wrote back from a list server\n>and re-read it, it often becomes clear that I don't understand what\n>I'm talking about.  For a 301 on a POST, does that really mean\n>substitute the new RequestURI for all future submissions, or only\n>when the content is identical to that of the current submission?\n\nAll future submissions -- 301 is a \"fix your damn links\" response. ;)\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Feedback on: draft-cohen-http30530602.tx",
            "content": "David W. Morris wrote:\n> \nDave, thanks for the comments, Ive added your minor editing changes\nas well as the following specific things you raised.\n\n>  1.3 506 Redirection Failed\n> \n>    The 506 response is returned when a redirection fails or is refused\n>    by a proxy or client.  If the redirection response included a body,\n>    then it SHOULD be included in the 506 response.\n> \n> **DWM:  This and the earlier discussion of 506 is confusing at best.\n> **DWM:  Who exactly is the client returning 506 to? In normal HTTP\n> **DWM:  lingo the client receives responses, it doesn't send them.\nIve added the following text:\n\nThis response is returned by a proxy, to a downstream proxy or\nclient, when it cannot or chooses not to honor a redirection.\n\n> **DWM:  I can't make sense out of either the syntax description or\n> **DWM:  the words below for what you mean to provide syntactically\n> **DWM:  or semantically for URIpattern. So other than to say that\n> **DWM:  I don't understand, I can't offer a suggestion.\n> \nIve added these examples:\n\nAn example header:\n    Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\", \nscope=\"http://\", seconds=5\n\n Scope Meaning: all URLS beginning with \"http://\"\n\nAnother example header:\n    Set-proxy: SET ; proxyURI = \"http://proxy.me.com:8080/\", \nscope=\"http://*.ups.com/\", seconds=5\n\n Scope Meaning: all URLS beginning which are for \n  hosts in the ups.com domain.\n\n-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: [Fwd: 301/302",
            "content": "On Tue, 29 Jul 1997, Josh Cohen wrote:\n\n\n  [NON-Text Body part not included]\n\nThe question was whether a 1.0 response w/o content length had to \nbe fixed up by a proxy .... my interpretation is that there are\nthree choices:\n\n1.  Accumulate the body and create a correct content-length before\n    forwarding.\n2.  Add a cunked transfer encoding header and accumulate chunks\n    and forward then with proper length fields\n3.  Don't report the response as HTTP/1.1 in the status line\n\nAll of this of course presumes that the request was HTTP/1.1.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: SetCookie2: &quot;additive&quot; vs. &quot;independent&quot",
            "content": "On Tue, 29 Jul 1997, Dave Kristol wrote:\n\n> Foteos Macrides wrote:\n> \n\n[...] cookie2: $version=\"1\"\n\n> I like this approach.\n\nI have no problem with the approach ... obviously this marker is only\nrequired if the UA doesn't already know cookie2 is desired.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n\n> >Here's a revised version, to replace the second paragraph\n> >in section 13.9:\n> >\n> >Some HTTP/1.0 cache operators have found that it is dangerous\n> >to cache and reuse without revalidation responses to requests\n> >for URLs that include any of the strings \"cgi-bin\", \"htbin\", or\n> >\"?\", because applications have traditionally used these URLs in\n> >conjunction with operations with significant side effects for\n> >GET or HEAD methods.  However, if such a response includes an\n> >explicit, future, expiration time, then this implies that the\n> >response may be cached and reused without revalidation until it\n> >expires.  If such a response includes a Last-Modified or Etag\n> >header, this implies that the response may be reused after\n> >revalidation (or without revalidation if explicitly fresh).\n> >\n> >A cache MUST NOT assign a heuristic expiration time to a\n> >response for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n> >\"?\" in its rel_path part.  If such a response does not \n> >carry an explicit expiration time, it must be treated as\n> >if it expires immediately.\n> \n> I'm pretty sure I said this before, but I don't know what list.\n> I am completely opposed to this change.  It is inaccurate to say that\n> caching and reusing such responses is \"dangerous\".  The *only* reason\n> *some* caches do not provide heuristic caching of such responses is\n> because the presence of query-based parameters make it unlikely to get\n> a second \"hit\" on the cache, and because the the absence of a Last-Modified\n> (and now Etag) makes it impossible to do an efficient update.  In any case,\n> this is an optimization which is dependent on the context and number of\n> users of the cache, and not a requirement of the protocol.\n> \n> The protocol already provides mechanisms for marking a response as\n> non-cachable.  All other responses to a GET request are cachable.\n\nI can't speak for the motivation of old cache authors, but I can speak as\nan HTTP/1.0 application author from before any RFCs when one had to\nreverse engineer everything and the empirical behavior I observed was that\nGET requests which included a query part were not cached.\n\nI support the behavior for handling HTTP/1.1 responses strictly \nconforming to Roy's position but I believe somthing like Jeff's\nproposed wording is necessary when the 1.1 cache is covering a 1.0\nserver.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Yaron Goland wrote:\n> \n> Scripts are deployed and they are not going to get re-written.\n> \n> Servers have no mechanism for determining if a script is 1.0 or 1.1 and\n> even if they did, they do not have facilities for taken any special\n> action in that case.\n\nWhich is, of course, a deficiency in CGI (and related) specs. If anyone\never gets around to sorting them out, they really ought to have a\nversion number somewhere in the conversation between server and script.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "New Editorial issue MESSAGEBOD",
            "content": "In working on the Options specification, Roy and Jeff came across a\nproblem in the spec that needed clearification.\n\nFor the record, here is the result.\n- Jim\n\n\n\nattached mail follows:\nAre you suggesting I incorporate your Options spec into draft 08?\n\nNo, this is a bug fix to a separate section, one that Roy discovered\nwhile replying to something with this Subject.  It is to replace\nthis sentence:\n>                               A message-body MAY be included in a\n>       request only when the request method (section 5.1.1) allows an\n>       entity-body.\nwhich you can probably find using a text-search.\n\nRoy suggests:\n     A message-body SHOULD NOT be included in a request unless the\n     request method is defined as allowing an entity-body.\n\nor\n\n     A server SHOULD read and forward a message-body on any request;\n     if the request method does not include defined semantics for an\n     entity-body, then the message-body SHOULD be ignored when handling\n     the request.\n\nI'm not sure why we shouldn't just say both things (robustness\nprinciple: don't send garbage, but don't choke on it, either).\n\nActually, on reflection, I'm not sure how I would implement\na server that can figure out that a GET without a content-length,\nbut which has a bogus body, isn't actually two GETs.  I.e.,\nif a bad client sends\n\nGET /foo.html HTTP/1.1\nHost: foo.com\n\nGET /bar.html HTTP/1.1\nHost: foo.com\n\n\nis that (1) two GETs on a persistent connection, or (2) one\nGET with a bogus body and no Content-length?  Or do the\nrules for persistent connections not allow this? I skimmed\nRFC2068, and I don't think we require this!\n\nAnyway, I think perhaps the best wording is:\n\n     A message-body MUST NOT be included in a request if the\n     specification of the request method does not allow\n     sending an entity-body in requests.\n\nThis means GET, HEAD, and DELETE, right?\n\n-Jeff\n\nP.S.: maybe I need some sleep, too.\n\n\n\n"
        },
        {
            "subject": "Cutoff for Draft 08 of the HTTP/1.1 spec",
            "content": "I'm now beginning FINAL preparation to submit draft 08 to the ID drafts\nrepository later today.\n\nAnything not in the draft this instant will have to wait for draft 09.\nSorry.\n\nWhen the draft is in, I'll also update the issues list with current status.\nBelow is extracted from the abstract of the draft.\n\nI just wanted to take this opportunity to thank the working group.  As you\nwill see from the following summary of where we are, the end is in sight.\nI'll be seeing many of you in Munich.\n- Jim Gettys\n\n\nThis draft does not resolve all open issues in the HTTP/1.1 specification \nrequiring closure before HTTP/1.1 goes to draft standard.  It does, however, \nclose most of them, and note where in the document there are still significant \nissues under discussion.  \n\nThe most significant outstanding issue is OPTIONS; there is a separate internet \ndraft on the topic that you should review NOT incorporated into this draft \n(though editorial notes identify where changes may occur).  This draft is \ndraft-ietf-http-options-00.txt.\n\nAlso an issue: AGE-CALCULATION; Roy Fielding has issued an ID on the topic; \nJeff Mogul intends to issue a draft as well.\n\nThe editorial group is very interested in feedback on the sample table of \nrequirements in this draft (issue REQUIREMENTS, section 19.9). Is it useful?  \nHow could it be improved?\n\nOpen or drafting issues not incorporated into this draft include: REDIRECTS, \nENCODING-NOT-CONNEG, DATE_IF_MODIFIED, 403VS404, PUT-RANGE, HOST, \nAGE-CALCULATION, RE-AUTHENTICATION-REQUESTED, VARY\n\nIssues incorporated into this draft where there is still controversy are \nnoted in bold italic with an editor's note.  These are issues: \nCONTENT-ENCODING, CACHING-CGI.\n\nIssues incorporated into this draft being working group last called are: \nAUTH-CHUNKED, RETRY-AFTER, PROXY-REDIRECT\n\nStill open or drafting issues are: REDIRECTS\n\nClosed issues incorporated into this draft include: PROXY-AUTHORIZATION, \nPROXY-LENGTH, LANGUAGE-TAG, TSPECIALS, STATUS100, QZERO, RANGE-ERROR, \nCLARIFY-NO-CACHE, COMMENT, CONTENT-LOCATION, QUOTED-BACK, CACHE-CONTRA, \nCACHE-DIRECTIVE, BYTE-RANGE, LWS-DELIMITER, CRLF, MAX-AGE, 100DATE, \nDISPOSITION, CHUNKED, CACHING, WARNINGS, VERSION, PROXY-MAXAGE, \nCHARSET-WILDCARD, PADDING, CONNECTION, RANGES, WARNING-8859, SHOULD-8859, \nX-BYTERANGES, MULTIPLE-TRANSFER-CODINGS, LINK_HEADER.  \n\nEditorial issues still open include: CLEANUP, UTF-8, URL-SYNTAX, ENTITY, \nDOCKDIGEST, 1310_CACHE.\n\nEditorial issues closed include: ACCEPT-RANGES, KEEP-ALIVE, BNFNAME, KEYWORDS, \nRESPONSE-VERSION, XREF, COMMON-HEADERS, NO-CACHE, FIX-REF, PERSIST-CONFUSED, \nCONNECTION2, GMT-UTC, PROXY-FORWARD, REFERER-SEC, CHUNK-EXT, REMOVE_19.6, \nIDEMPOTENT, REF-SIGCOMM, 1521-OBSOLETE, MESSAGE-BODY \n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Why, you could have different configuration directives, so that\n.cgi files would be HTTP/1.0 and .cgi11 files would be HTTP/1.1,\nor .asp would be HTTP/1.0 and .asq would be HTTP/1.1, etc.\n\nIt's easy for servers to have a mechanism for determining if\na script is 1.0 or 1.1.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: New Editorial issue MESSAGEBOD",
            "content": "On Wed, 30 Jul 1997, Jim Gettys wrote:\n\n> In working on the Options specification, Roy and Jeff came across a\n> problem in the spec that needed clearification.\n> \n> For the record, here is the result.\n> - Jim\n> \n> \n>including the following from Jeff Mogul:\n\n>>  Anyway, I think perhaps the best wording is:\n>>\n>>       A message-body MUST NOT be included in a request if the\n>>       specification of the request method does not allow\n>>       sending an entity-body in requests.\n>>\n>>This means GET, HEAD, and DELETE, right?\n\nMany moons ago we had a discussion about creating a SAFE GET method.\nDuring that discussion it was clear to me that any method which\ndidn't explictly exclude a request body could include one. IFF the\ncontent-length and/or transfer-coding: chunked was specified.\nIn other words, for historical reasons, GET couldn't have a body and\nthere were words to that effect. I didn't specifically look for HEAD\nor DELETE so I can't comment now, but I did convince myself that any\nother 'modern' method could have a body under HTTP/1.1 because HTTP/1.1\nrequires an explicitly stated content length. (header or chunked\nencoding).\n\nAnd it is that explict length which allows proxies, servers, etc. to\nsort out the question that Jeff raised. On a persistent connection,\nthe transaction is bogus if it has a body and doesn't have explict\nlength.\n\nThere is no hope for extensible methods if the rules don't allow content\nwith unknown methods.\n\nSo I think the wording needs to be something like (if this isn't there\nalready):\n\n   Any request with a method which doesn't explicity exclude a content\n   body may include a body. Any request which includes a body MUST\n   explictly state the length with either a Content-length: header\n   field or Transfer-encoding: chunked.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Case (in)sensitivity in (Non)Compliance header",
            "content": "Jeffrey Mogul <mogul@pa.dec.com> writes:\n\n>14.ZZZ Compliance\n>...\n>       compliance-namespace = token\n>...\n>       \"RFC\"   Compliance is with an RFC, specified by an RFC number.\n>               For example, \"rfc=1945\".\n>\n>       \"HDR\"   Compliance is with a named HTTP header.  For example,\n>               \"HDR=Authorization\".  There is no IANA registry for\n>               HTTP header names, but to avoid potential namespace\n>               confusion, only those HTTP headers listed in an\n>               IETF standards-track document should be used in\n>               this namespace.\n\nThe examples imply that compliance-namespace is to be treated in a\ncase-insensitive manner, although that isn't stated anywhere.  I hate\nto keep harping on this issue, but living in the mainframe world where\nupper case tends to be the default for such things, I keep finding\nexamples of case-insensitive tokens being handled as if they were always\nlower case (e.g., media types in HTTP Content-Type headers by at least\none of the Big Two browsers).\n\nIf the RFC is going to publish the initial contents of the Compliance\nregistry, we should include the BNF for those initial entries.  I\nsuggest something like the following:\n\n   \"RFC\"   Compliance is with an RFC, specified by an RFC number.\n           For example, \"rfc=1945\".\n\n              compliance-RFC = \"RFC\" \"=\" RFC-number\n              RFC-number = 1*DIGIT\n\n           Leading zeroes are permitted and ignored in RFC-number (i.e.,\n           comparisons are numeric, not string).  The only parameters\n           allowed on compliance-RFC are COND and UNCOND.\n\n   \"HDR\"   Compliance is with a named HTTP header.  For example,\n           \"HDR=Authorization\".  There is no IANA registry for\n           HTTP header names, but to avoid potential namespace\n           confusion, only those HTTP headers listed in an\n           IETF standards-track document should be used in\n           this namespace.\n\n              compliance-HDR = \"HDR\" \"=\" field-name\n\n           field-name is treated in a case-insensitive manner.  The only\n           parameters allowed on compliance-HDR are COND and UNCOND.\n\n   \"PEP\"   Compliance is with a PEP-specified extension, identified\n           using a quoted-string containing the PEP extension\n           declaration.\n\n              compliance-PEP = \"PEP\" \"=\" something-PEPpy ; ;-)\n\n           ...\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Wed, 30 Jul 1997, Larry Masinter wrote:\n\n> Why, you could have different configuration directives, so that\n> .cgi files would be HTTP/1.0 and .cgi11 files would be HTTP/1.1,\n> or .asp would be HTTP/1.0 and .asq would be HTTP/1.1, etc.\n> \n> It's easy for servers to have a mechanism for determining if\n> a script is 1.0 or 1.1.\n\nThe problem is servers which have not been upgraded receiving unexpected\nrequest method transforms from clients which have been upgraded.\n\nGiven our rather confusing wording in the past about 301 and 302, we\nhave a legacy of servers which won't expect what we now want them\nto receive.\n\nYaron's proposal makes sense to me. This is another bit of historical\ncruft in the protocol.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "The meaning of 301 (was Re: 301/302",
            "content": "On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n\n> >Unfortunately, when I get stuff I wrote back from a list server\n> >and re-read it, it often becomes clear that I don't understand what\n> >I'm talking about.  For a 301 on a POST, does that really mean\n> >substitute the new RequestURI for all future submissions, or only\n> >when the content is identical to that of the current submission?\n> \n> All future submissions -- 301 is a \"fix your damn links\" response. ;)\n> \n> ....Roy\n\nThat makes sense, although I doubt it is implemented like that anywhere.\n\nTo take this a step further, I assume that also means that a 301\nredirection remains in effect not only over variations of POST content,\nbut also over variation of method.  I.e. (if the same resource allows\nmultiple methods) a POST to a URL gets a 301 response, this would[*]\ninfluence what the client does if it is later asked to do a GET on\nthe same URL.  And vice versa.\n\n[*] Actually \"could\" instead of \"would\", since no client is required to\nremember (cache) 301 responses.\n\nIs that a correct understanding?\n\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: Another try at OPTION",
            "content": "At 04:47 PM 07/22/97 MDT, Jeffrey Mogul wrote:\n\n>\"PEP\"Compliance is with a PEP-specified extension, identified\n>using a quoted-string containing the PEP extension\n>declaration.\n\nThere is no reason to have a special type for PEP extensions. Information\nabout PEP extensions (called extension policies) can be transmitted via any\nnumber of mechanisms as pure metadata that doesn't take any action in the\nmessage where they are included.\n\nAn extension is applied to a message by adding an extension declaration,\nwhich differs from a policy in that the extension is being used in the\nmessage.\n\nThere is work going on designing a generic metadata model by combining the\nexperiences from PICS and XML, but as this is not finished yet, the PEP\nspec defines it's own mechanism for distributing metainformation (in PEP,\nit's called a \"policy\" describing which extensions either MUST or MAY be\napplied to a particular resource.)\n\n>    proxy4.microscape.com responds:\n>HTTP/1.1 200 OK\n>Date: Tue, 22 Jul 1997 20:21:51 GMT\n>Server: SuperProxy/1.0\n>Public: OPTIONS, GET, HEAD, PUT, POST, TRACE\n>Compliance: rfc=1543, rfc=2068, hdr=set-proxy\n>Compliance: hdr=wonder-bar-http-widget-set\n>Compliance: PEP=\"http://foobar.pep.org/pepmeister/\"\n>Content-Length: 0\n\nWith PEP-Info, the server would send:\n\n       proxy4.microscape.com responds:\nHTTP/1.1 200 OK\nDate: Tue, 22 Jul 1997 20:21:51 GMT\nServer: SuperProxy/1.0\nPublic: OPTIONS, GET, HEAD, PUT, POST, TRACE\nCompliance: rfc=1543, rfc=2068, hdr=set-proxy\nCompliance: hdr=wonder-bar-http-widget-set\nPEP-Info: {{id \"http://foobar.pep.org/pepmeister/\"}\n    {for \"/\" *} {strength must}}\nContent-Length: 0\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for OPTIONS issu",
            "content": "Jeffrey Mogul <mogul@pa.dec.com> writes:\n\n>14.QQQ Non-Compliance\n>...\n>        Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\n>\n>        proxy-host = host [ \":\" port ]\n>\n>        non-compliance-option = compliance-option \"@\" proxy-host\n\nThis implies that non-compliance-option allows parameters, e.g.:\n\n   Non-Compliance: MyNameSpace=MyThing;MyParam@Somebody.Else.Proxy\n\nIs this an accident of syntax?  If not, what is the interpretation of\nsuch a header?  Two possibilities come to mind immediately:\n\n   1) Somebody.Else.Proxy doesn't support the MyNameSpace=MyThing item.\n\n   2) Somebody.Else.Proxy supports the MyNameSpace=MyThing item, but\n      doesn't support it with the MyParam parameter.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n\n> As Foteos hinted, swapping the meaning of 302 and 303 is a solution\n> to the implementation problem.  I don't think it would affect Apache much.\n> However, it would require universal agreement among the rest of the\n> implementers, and it would require recycling HTTP/1.1 as Proposed\n> and not as a Draft Standard.  It is not something to be taken lightly.\n\nI hope the idea of just \"swapping\" 302 and 303 is not being entertained\nseriously.  303 is a clean thing and doesn't need to be fixed - don't\ndump the problem on those who have tried to do the right thing.[1]\n\nIf 302 is in such a mess that it effectively cannot be uses for the\npurpose it was always intended to serve, by the protocol designers (in\nconnection with non-GET methods) - why, use a new status code for what is\nneeded (but don't reuse 303!).\n\nMore specific proposal:\n\n - Assign a new code (say, 307 - or whichever is the first free number).\n   This code means \"This resource has moved, and we really mean it.\n   Re-send full messages to the new address (not just empty envelopes).\"\n   I.e. what 302 was supposed to mean.[2]\n\n - Mark 302 as DEPRECATED.  Servers and scripts should use 303 or 307\n   instead, as appropriate.  But for compatibility, not send 307 to\n   older clients.[3].\n\n - Describe 302 as a \"General Redirection\".  For GET requests, the meaning\n   is as for 303.  For other methods, the outcome is UNSPECIFIED.\n   - Add some notes explaining this. \"Most, but not all, clients will\n   treat a 302 in response to a POST like 303, but don't rely on it.\"\n\n - Clients are required to understand 303 and 307.  Also for\n   compatibility,  they are required to treat 302 in response to a GET\n   like a 303.  If they get 302 in response to another method - well\n   it's up to them how they interpret \"General Redirection\".[4]\n\n - There is no need to change anything about 301 or 303.\n\nNotes:\n[1] There probably aren't many who use 303.  But at least the lynx mailing\n    list has directed people with problems to read the HTTP specs (RFC \n    1945, then the 1.1 draft and later RFC), to read the \"Note:\"s in the\n    301 and 302 descriptions, and to use 303.\n\n[2] Services needing the \"full redirection\" behavior of POST, PUT, etc.\n   will be new services.  They cannot reliably use 302 for this today\n   anyway, so it's not asking too much to make them use a new status code.\n   They are the ones to profit from it.\n\n[3] Of course that's a problem, if the server cannot reliably detect the\n    client's protocol version.  However, this would only affect services\n    that really need the 307 behavior - the old \"official\" 302 behavior -\n    and they cannot get that reliably today.  So this doesn't make it\n    worse for anyone.  Services requiring the 307 behavior would probably \n    (initially) operate in a controlled environment where out-of-band \n    information about client version is available (or it can be guaranteed\n    that there are no proxies etc.)\n\n[5] This may seem bad, but I think it's a reflection of how things are.\n   This change would give existing implementations a better status than they \n   have with the current wording of RFC 2068 (*and* 1945): Instead of\n   clearly acting \"erroneously\", they would just be using a deprecated\n   feature.  At the same time this change avoids putting implementations\n   in the wrong who have tried to do what RFCs 2068 and 1945 say about 302 -\n   therefore this change may be possible without going back to \"Proposed\".\n\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Larry Masinter wrote:\n> \n> Why, you could have different configuration directives, so that\n> .cgi files would be HTTP/1.0 and .cgi11 files would be HTTP/1.1,\n> or .asp would be HTTP/1.0 and .asq would be HTTP/1.1, etc.\n> \n> It's easy for servers to have a mechanism for determining if\n> a script is 1.0 or 1.1.\n\nTrue, and we could have decided that there was no point in having\nversion numbers in HTTP, because we could just have a naming covention\nfor servers, www.* == HTTP/0.9, www10.* == HTTP/1.0, www11.* ==\nHTTP/1.1.\n\nIt may work, but its junk.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Jim Gettys:\n>\n\n>he following got dropped on the floor, but for good cleanlyness\n>reasons, I'm sending it to the list in case someone has some problem with\n>declaring it closed, as it was marked otherwise in the issues list.\n\nHi Jim,\n\nYes, I have a problem with marking this issue as closed, so please\ndon't mark it as closed.\n\nLike Roy, I am opposed to the wording:\n\n> A cache MUST NOT assign a heuristic expiration time to a\n> response for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n> \"?\" in its rel_path part. \n\nThe strongest wording I would support here would be `A cache should\n[lowercase!] be careful in assigning ...'.\n\n\n>                                - Jim Gettys\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Klaus,\n\nI thinks this would be the best solution we have so far.  It allows\nlegacy server scripts and clients to work properly with HTTP 1.1 while\npaving the way for 1.1 users to invoke the new and correct behaviors\nwe've defined.\n\nrom an implementation stand point it would be trivial to add an\nadditional status code redirect.  But it remains to be seen whether we\ncan make this change in consensus.\n\nJust my 2-cents (i.e. not those of my company, etc..),\n-Arthur Bierer\n\n> -----Original Message-----\n> From:Klaus Weide [SMTP:kweide@tezcat.com]\n> Sent:Wednesday, July 30, 1997 10:34 AM\n> To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: 301/302 \n> \n> On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n> \n> > As Foteos hinted, swapping the meaning of 302 and 303 is a solution\n> > to the implementation problem.  I don't think it would affect Apache\n> much.\n> > However, it would require universal agreement among the rest of the\n> > implementers, and it would require recycling HTTP/1.1 as Proposed\n> > and not as a Draft Standard.  It is not something to be taken\n> lightly.\n> \n> I hope the idea of just \"swapping\" 302 and 303 is not being\n> entertained\n> seriously.  303 is a clean thing and doesn't need to be fixed - don't\n> dump the problem on those who have tried to do the right thing.[1]\n> \n> If 302 is in such a mess that it effectively cannot be uses for the\n> purpose it was always intended to serve, by the protocol designers (in\n> connection with non-GET methods) - why, use a new status code for what\n> is\n> needed (but don't reuse 303!).\n> \n> More specific proposal:\n> \n>  - Assign a new code (say, 307 - or whichever is the first free\n> number).\n>    This code means \"This resource has moved, and we really mean it.\n>    Re-send full messages to the new address (not just empty\n> envelopes).\"\n>    I.e. what 302 was supposed to mean.[2]\n> \n>  - Mark 302 as DEPRECATED.  Servers and scripts should use 303 or 307\n>    instead, as appropriate.  But for compatibility, not send 307 to\n>    older clients.[3].\n> \n>  - Describe 302 as a \"General Redirection\".  For GET requests, the\n> meaning\n>    is as for 303.  For other methods, the outcome is UNSPECIFIED.\n>    - Add some notes explaining this. \"Most, but not all, clients will\n>    treat a 302 in response to a POST like 303, but don't rely on it.\"\n> \n>  - Clients are required to understand 303 and 307.  Also for\n>    compatibility,  they are required to treat 302 in response to a GET\n>    like a 303.  If they get 302 in response to another method - well\n>    it's up to them how they interpret \"General Redirection\".[4]\n> \n>  - There is no need to change anything about 301 or 303.\n> \n> Notes:\n> [1] There probably aren't many who use 303.  But at least the lynx\n> mailing\n>     list has directed people with problems to read the HTTP specs (RFC\n> \n>     1945, then the 1.1 draft and later RFC), to read the \"Note:\"s in\n> the\n>     301 and 302 descriptions, and to use 303.\n> \n> [2] Services needing the \"full redirection\" behavior of POST, PUT,\n> etc.\n>    will be new services.  They cannot reliably use 302 for this today\n>    anyway, so it's not asking too much to make them use a new status\n> code.\n>    They are the ones to profit from it.\n> \n> [3] Of course that's a problem, if the server cannot reliably detect\n> the\n>     client's protocol version.  However, this would only affect\n> services\n>     that really need the 307 behavior - the old \"official\" 302\n> behavior -\n>     and they cannot get that reliably today.  So this doesn't make it\n>     worse for anyone.  Services requiring the 307 behavior would\n> probably \n>     (initially) operate in a controlled environment where out-of-band \n>     information about client version is available (or it can be\n> guaranteed\n>     that there are no proxies etc.)\n> \n> [5] This may seem bad, but I think it's a reflection of how things\n> are.\n>    This change would give existing implementations a better status\n> than they \n>    have with the current wording of RFC 2068 (*and* 1945): Instead of\n>    clearly acting \"erroneously\", they would just be using a deprecated\n>    feature.  At the same time this change avoids putting\n> implementations\n>    in the wrong who have tried to do what RFCs 2068 and 1945 say about\n> 302 -\n>    therefore this change may be possible without going back to\n> \"Proposed\".\n> \n> \n>     Klaus\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Dave Morris writes:\n\n    I support the behavior for handling HTTP/1.1 responses strictly \n    conforming to Roy's position but I believe somthing like Jeff's\n    proposed wording is necessary when the 1.1 cache is covering a 1.0\n    server.\n\nI would go along with this, except that I see no way that a proxy\nin a chain of several proxies can reliably determine whether the\norigin server is HTTP/1.0.  E.g., if you have this configuration:\n\nOrigin_server----Proxy_p1-----Proxy_p2------client\n\nhow does Proxy_p2 know for sure that the Origin_server is not\nHTTP/1.0?\n\nGiven that, there certainly are many cases where a proxy\ncan reliably know that the origin server is definitely HTTP/1.1.\nFor example, Proxy_p1 knows that it's talking to the origin\nserver, and it knows what HTTP-version is sent in the response\nstatus-line.  Or, if the response includes a header that could\nonly have been added by an HTTP/1.1 server (e.g., Cache-Control).\n\nSo, one possible rewording of the proposed resolution would be\nto change:\n\nA cache MUST NOT assign a heuristic expiration time to a\nresponse for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n\"?\" in its rel_path part.  If such a response does not \ncarry an explicit expiration time, it must be treated as\nif it expires immediately.\n\nto this:\n\nA cache MUST NOT assign a heuristic expiration time to a\nresponse for a URL that includes the strings \"htbin\",\n\"cgi-bin\", or \"?\" in its rel_path part, if the response might\nhave been generated by an HTTP/1.0 (or earlier) origin server.\nIf such a response does not carry an explicit expiration time,\nit must be treated as if it expires immediately.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE VARY: Proposed wordin",
            "content": "Roy, Jim,\n\nThis mail contains some discussion - the next mail contains the full wording.\n\n>>When the cache receives a subsequent request whose Request-URI specifies\n>>one or more cache entries including a Vary header field, the cache MUST NOT\n>>use such a cache entry to construct a response to the new request unless\n>>all of the field-names in the cached Vary header field are present in the\n>>new request, and all of the stored selecting request-headers from the\n>>previous request match the corresponding request-headers in the new\nrequest. \n>\n>That's not quite right -- if the field-name is missing from both the\n>old request and the new request, then they still match.  We should also\n>be referring to the requested resource rather than the Request-URI.\n\nThe rule should be that if a selecting request-header is not present in a\nnew request then this always matches any value that this request-header had\nin the original request. This follows the accept* rule that no accept\nheader means \"anything goes\" and allows for clients that don't know about a\nspecific selecting request-header still to obtain a cached version.\nSomething like this:\n\n   When the cache receives a subsequent request whose Request-URI\n   specifies one or more cache entries including a Vary header\n   field, the cache MUST NOT use such a cache entry to construct\n   a response to the new request unless all of the selecting\n   request-headers present in the new request match the\n   corresponding stored request-headers in the original request. \n\n>>A Vary field value of \"*\" signals that unspecified parameters not limited\n>>to the request-headers (e.g., the network address of the client), play a\n>>role in the selection of the response representation. Subsequent requests\n>>on that resource can only be properly interpreted by the origin server, and\n>>thus a cache MUST forward a (possibly conditional) request even when it has\n>>a fresh response cached for the resource. The \"*\" value MUST NOT be\n>>generated by a proxy server; it may only be generated by an origin server.\n>\n>This section is great up until this last paragraph.  Since it just repeats\n>what is already said in the first paragraph, we should delete it.  As\n>mentioned above, we cannot place a MUST requirement on the cache making\n>a new request (i.e., the cache must have the right to deny service).\n>Also, the requirement in the last sentence is bogus, unless we mean\n>to apply it to all values of Vary (i.e., The Vary field MUST NOT be\n>set or modified by any application other than the origin server.).\n>The existing wording is always a contradiction for the CERN server.\n\nThere is nothing wrong in that a cache \"creates\" a new representation by\nconverting it on the fly and store both versions in its cache. For example,\na cache can translate a GIF image into a PNG image and maintain both\nversions in the cache. The remaining metainformation is cloned from the\noriginal version but the cache can add or modify a Vary header to indicate\nthat there now are two representations.\n\nThe further away the cache gets from the origin server, the chance of\n\"going around it\" increases but this will not lead to a correctness problem\n- only a conditional request to the origin server. However, a value of \"*\"\ndoesn't make sense. It would have the same result as a must-revalidate\ncache control directive.\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: ISSUE VARY: Proposed wordin",
            "content": "This text is to completely replace the existing wording in section 13.6 and\n14.43 and to edit section 12.1 and 12.3\n\n**********\n\n12.1 Server-driven Negotiation\n\nChange the last two paragraphs from\n\n   HTTP/1.1 origin servers MUST include an appropriate Vary header\n   field (section 14.43) in any cachable response based on server-\n   driven negotiation. The Vary header field describes the \n   dimensions over which the response might vary (i.e. the \n   dimensions over which the origin server picks its \"best guess\" \n   response from multiple representations).\n\n   HTTP/1.1 public caches MUST recognize the Vary header field\n   when it is included in a response and obey the requirements \n   described in section 13.6 that describes the interactions\n   between caching and content negotiation.\n\nto\n\n   The Vary header field can be used to express the parameters used\n   to select a representation subject to server-driven negotiation.\n   See section 13.6 for use of the Vary header field by caches and\n   section 14.43 for use of the Vary header field by servers.\n\n12.3 Transparent Negotiation\n\nChange the last paragraph from\n\n   This specification does not define any mechanism for\n   transparent negotiation, though it also does not prevent any \n   such mechanism from being developed as an extension and used \n   within HTTP/1.1. An HTTP/1.1 cache performing transparent \n   negotiation MUST include a Vary header field in the response \n   (defining the dimensions of its variance) if it is cachable to \n   ensure correct interoperation with all HTTP/1.1 clients. The \n   agent-driven negotiation information supplied by the origin \n   server SHOULD be included with the transparently negotiated \n   response.\n\nto\n\n   This specification does not define any mechanism for\n   transparent negotiation, though it also does not prevent any \n   such mechanism from being developed as an extension and used \n   within HTTP/1.1.\n\n13.6 Caching Negotiated Responses\n\nUse of server-driven content negotiation (section 12), as indicated by the\npresence of a Vary header field in a response, alters the conditions and\nprocedure by which a cache can use the response for subsequent requests.\nSee section 14.43 for use of the Vary header field by servers.\n\nA server SHOULD use the Vary header field to inform a cache of what\nrequest-header fields were used to select among multiple representations of\na cachable response subject to server-driven negotiation. The set of header\nfields named by the Vary field value is known as the \"selecting\"\nrequest-headers.\n\nWhen the cache receives a subsequent request whose Request-URI specifies\none or more cache entries including a Vary header field, the cache MUST NOT\nuse such a cache entry to construct a response to the new request unless\nall of the selecting request-headers present in the new request match the\ncorresponding stored request-headers in the original request.\n\nThe selecting request-headers from two requests are defined to match if and\nonly if the selecting request-headers in the first request can be\ntransformed to the selecting request-headers in the second request by\nadding or removing linear whitespace (LWS) at places where this is allowed\nby the corresponding BNF, and/or combining multiple message-header fields\nwith the same field name following the rules about message headers in\nsection 4.2.\n\nA Vary header field-value of \"*\" always fail to match and subsequent\nrequests on that resource can only be properly interpreted by the origin\nserver.\n\nIf the selecting request header fields for the cached entry do not match\nthe selecting request header fields of the new request, then the cache MUST\nNOT use a cached entry to satisfy the request unless it first relays the\nnew request to the origin server in a conditional request and the server\nresponds with 304 (Not Modified), including an entity tag or\nContent-Location that indicates which entity should be used.\n\nIf an entity tag was assigned to a cached representation, the forwarded\nrequest SHOULD be conditional and include the entity tags in an\nIf-None-Match header field from all its cache entries for the resource.\nThis conveys to the server the set of entities currently held by the cache,\nso that if any one of these entities matches the requested entity, the\nserver can use the ETag header field in its 304 (Not Modified) response to\ntell the cache which entry is appropriate. If the entity-tag of the new\nresponse matches that of an existing entry, the new response SHOULD be used\nto update the header fields of the existing entry, and the result MUST be\nreturned to the client.\n\nIf any of the existing cache entries contains only partial content for the\nassociated entity, its entity-tag SHOULD NOT be included in the\nIf-None-Match header field unless the request is for a range that would be\nfully satisfied by that entry.\n\nIf a cache receives a successful response whose Content-Location field\nmatches that of an existing cache entry for the same Request-URI, whose\nentity-tag differs from that of the existing entry, and whose Date is more\nrecent than that of the existing entry, the existing entry SHOULD NOT be\nreturned in response to future requests, and should be deleted from the cache.\n\n14.43 Vary\n\nThe Vary field value indicates the set of request-header fields that fully\ndetermines, while the response is fresh, whether a cache may use the\nresponse to reply to a subsequent request without revalidation. For\nuncachable or stale responses, the Vary field value advises the user agent\nabout the criteria that were used to select the representation. A Vary\nfield value of \"*\" implies that a cache cannot determine from the request\nheaders of a subsequent request whether this response is the appropriate\nrepresentation. See section 13.6 for use of the Vary header field by caches.\n\n       Vary  = \"Vary\" \":\" ( \"*\" | 1#field-name )\n\nAn HTTP/1.1 server SHOULD include a Vary header field with any cachable\nresponse that is subject to server-driven negotiation. Doing so allows a\ncache to properly interpret future requests on that resource and informs\nthe user agent about the presence of negotiation on that resource. A server\nMAY include a Vary header field with a non-cachable response that is\nsubject to server-driven negotiation, since this might provide the user\nagent with useful information about the dimensions over which the response\nvaries at the time of the response.\n\nA Vary field value consisting of a list of field-names signals that the\nrepresentation selected for the response is based on a selection algorithm\nwhich considers ONLY the listed request-header field values in selecting\nthe most appropriate representation. A cache MAY assume that the same\nselection will be made for future requests with the same values for the\nlisted field names, for the duration of time in which the response is fresh.\n\nThe field-names given are not limited to the set of standard request-header\nfields defined by this specification. Field names are case-insensitive.\n\nA Vary field value of \"*\" signals that unspecified parameters not limited\nto the request-headers (e.g., the network address of the client), play a\nrole in the selection of the response representation. The \"*\" value MUST\nNOT be generated by a proxy server; it may only be generated by an origin\nserver.\n\n**********\n\nHenrik\n\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: The meaning of 301 (was Re: 301/302",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n>\n>> >Unfortunately, when I get stuff I wrote back from a list server\n>> >and re-read it, it often becomes clear that I don't understand what\n>> >I'm talking about.  For a 301 on a POST, does that really mean\n>> >substitute the new RequestURI for all future submissions, or only\n>> >when the content is identical to that of the current submission?\n>> \n>> All future submissions -- 301 is a \"fix your damn links\" response. ;)\n>> \n>> ....Roy\n>\n>That makes sense, although I doubt it is implemented like that anywhere.\n\nCurrent versions of Lynx have it implemented it like that.  They\ndo not \"fix your damn links\" in the sense of editing source files, because\nthat would be inapproriate or barred for a large percentage of the Lynx\nuser base, but for the remainder a user's session the 301's Location\nwill be substituted for the original RequestURI.  That's why it seemed\nlike a good idea to confirmed that this is really intended for a 301 on\na POST.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n>\n>> As Foteos hinted, swapping the meaning of 302 and 303 is a solution\n>> to the implementation problem.  I don't think it would affect Apache much.\n>> However, it would require universal agreement among the rest of the\n>> implementers, and it would require recycling HTTP/1.1 as Proposed\n>> and not as a Draft Standard.  It is not something to be taken lightly.\n>\n>I hope the idea of just \"swapping\" 302 and 303 is not being entertained\n>seriously.  303 is a clean thing and doesn't need to be fixed - don't\n>dump the problem on those who have tried to do the right thing.[1]\n>[...]\n>[1] There probably aren't many who use 303.  But at least the lynx mailing\n>    list has directed people with problems to read the HTTP specs (RFC \n>    1945, then the 1.1 draft and later RFC), to read the \"Note:\"s in the\n>    301 and 302 descriptions, and to use 303.\n\nIt's a matter of personal opinion whether passing the buck to\nthe user via an option to use the 303 behavior for a 302 is a solution\nto the problem, or just a \"workaround kludge\" for an unsolved problem.\nI certainly didn't intend for that to be treated as anything more than\none of Fote's workaround kludges.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": "Don't panic, I'm not marking it closed.\n\nThat is why I sent it to the list, in case there were issues that\nhad not been aired due to the mishandling of the issue.\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for OPTIONS issu",
            "content": "Ross Patterson writes:\n\n    >14.QQQ Non-Compliance\n    >...\n    >        Non-Compliance =  \"Non-Compliance\" \":\" 1#non-compliance-option\n    >\n    >        proxy-host = host [ \":\" port ]\n    >\n    >        non-compliance-option = compliance-option \"@\" proxy-host\n    \n    This implies that non-compliance-option allows parameters, e.g.:\n    \n       Non-Compliance: MyNameSpace=MyThing;MyParam@Somebody.Else.Proxy\n    \n    Is this an accident of syntax?  If not, what is the interpretation of\n    such a header?  Two possibilities come to mind immediately:\n    \n       1) Somebody.Else.Proxy doesn't support the MyNameSpace=MyThing item.\n    \n       2) Somebody.Else.Proxy supports the MyNameSpace=MyThing item, but\n  doesn't support it with the MyParam parameter.\n\nGood point. I think interpretation #2 makes more sense, since\nthe proxy could have sent\n\nNon-Compliance: MyNameSpace=MyThing@Somebody.Else.Proxy\n\nif it meant #1.\n\nSo I'm adding this to the definition of Non-Compliance:\n\n   If the compliance-option in a non-compliance-option includes one or\n   more option-param(s) (see section 14.ZZZ), then the proxy server's\n   non-compliance is limited to the scope of the option-param(s).  If\n   the compliance-option does not include an option-param, then the\n   proxy server is asserting non-compliance with the option in\n   general.\n   \n   For example, a response with:\n   \n   Compliance: rfc=9999;uncond\n   Non-Compliance: rfc=9999;uncond@proxy.foo.net\n   \n   states that proxy.foo.net is not unconditionally compliant with\n   RFC9999, but does not imply that proxy.foo.net is not\n   conditionally compliant with RFC9999.  If the proxy is not even\n   conditionally compliant with RFC9999, it should instead send\n   \n   Compliance: rfc=9999;uncond\n   Non-Compliance: rfc=9999@proxy.foo.net\n   \n   when forwarding the response.\n   \nThanks\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Case (in)sensitivity in (Non)Compliance header",
            "content": "Ross Patterson writes:\n\n    The examples imply that compliance-namespace is to be treated in a\n    case-insensitive manner, although that isn't stated anywhere.  I\n    hate to keep harping on this issue, but living in the mainframe\n    world where upper case tends to be the default for such things, I\n    keep finding examples of case-insensitive tokens being handled as\n    if they were always lower case (e.g., media types in HTTP\n    Content-Type headers by at least one of the Big Two browsers).\n\nIf you look at RFC, section 2.1 (Augmented BNF), you'll find this\ndefinition:\n\n    \"literal\"\n Quotation marks surround literal text. Unless stated otherwise,\n      the text is case-insensitive.\n    \nI.e., anywhere in the HTTP specification, if the BNF includes a\nquoted literal, that literal is supposed to be case-insensitive.\n\nHowever, since I didn't give a formal BNF for the compliance-namespace\nvalues, and the set could be extended in the future, I agree that it's\nworth stating this (case-insensitivity) explicitly.\n\nAnd I'll also include a BNF for the RFC and HDR namespaces.  (Per\nHenrik's comments, I'm removing the \"PEP\" namespace from the next\ndraft.)\n\nThanks\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Updated proposal for OPTIONS issu",
            "content": "Jeffrey Mogul <mogul@pa.dec.com> writes:\n\n>Good point. I think interpretation #2 makes more sense, since\n>the proxy could have sent\n>\n>       Non-Compliance: MyNameSpace=MyThing@Somebody.Else.Proxy\n>\n>if it meant #1.\n\nCool, although I was kinda hoping for a \"Whoa!  We didn't mean for\nparameters to be supported on Non-Compliance!\"\n\n>   For example, a response with:\n>\n>          Compliance: rfc=9999;uncond\n>          Non-Compliance: rfc=9999;uncond@proxy.foo.net\n>\n>   states that proxy.foo.net is not unconditionally compliant with\n>   RFC9999, but does not imply that proxy.foo.net is not\n>   conditionally compliant with RFC9999.  If the proxy is not even\n>   conditionally compliant with RFC9999, it should instead send\n>\n>          Compliance: rfc=9999;uncond\n>          Non-Compliance: rfc=9999@proxy.foo.net\n>\n>   when forwarding the response.\n\nI guess this means that a client needs to sort out all the Non-Compliance\nheaders it receives from proxies in a response and determine for itself\nhow parameters interact.  COND and UNCOND will probably be OK, but I\nwonder about other parameters, especially the ever-present \"token\"\nparameter.\n\nI guess I weigh in as thinking that parameters should not be permitted\nin Non-Compliance headers.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "ISSUE: 403vs40",
            "content": "Referring to\n\nhttp://www.w3.org/Protocols/HTTP/Issues/#403VS404\n\nThis edit requires changes to section 10.4.4 and 10.4.5.\n\nChange section 10.4.4 from\n\n   The server understood the request, but is refusing to fulfill\n   it. Authorization will not help and the request SHOULD NOT be\n   repeated. If the request method was not HEAD and the server\n   wishes to make public why the request has not been fulfilled,\n   it SHOULD describe the reason for the refusal in the entity.\n   This status code is commonly used when the server does not wish\n   to reveal exactly why the request has been refused, or when no\n   other response is applicable.\n\nto\n\n   The server understood the request, but is refusing to fulfill\n   it. Authorization will not help and the request SHOULD NOT be\n   repeated. If the request method was not HEAD and the server\n   wishes to make public why the request has not been fulfilled,\n   it SHOULD describe the reason for the refusal in the entity.\n   If the server does not wish to make this information available\n   to the client, the status code 404 (Not Found) can be used\n   instead. \n\nChange section 10.4.5 from\n\n   The server has not found anything matching the Request-URI.\n   No indication is given of whether the condition is temporary\n   or permanent. If the server does not wish to make this\n   information available to the client, the status code 403\n   (Forbidden) can be used instead. The 410 (Gone) status code\n   SHOULD be used if the server knows, through some internally\n   configurable mechanism, that an old resource is permanently\n   unavailable and has no forwarding address.\n\nto\n\n   The server has not found anything matching the Request-URI. No\n   indication is given of whether the condition is temporary or\n   permanent. The 410 (Gone) status code SHOULD be used if the\n   server knows, through some internally configurable mechanism,\n   that an old resource is permanently unavailable and has no\n   forwarding address. This status code is commonly used when the\n   server does not wish to reveal exactly why the request has been\n   refused, or when no other response is applicable.\n\nThanks\n\nHenrik\n\nPS: The bandwidth is so bad from here that it effectively is a one-way\nchannel - this means that I may not get to read any responses the next\ncouple of days :-(\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Issue 1310_CACH",
            "content": "Jim, Jeff,\n\nReferring to Issue\n\nhttp://www.w3.org/Protocols/HTTP/Issues/#1310_CACHE\n\nI would like to link this to Jeff's new section on idempotent methods but\nas I haven't seen it (and don't believe I will) then please check for\nconsistency in the last proposed paragraph.\n\nAlso, I fail to see why the URI in a location header can invalidate a\ncached entry as written. If a resource is moved and there is a location\nheader in the response, then the cached entry should be updated to reflect\nthis, but this is already described in section 13.4.\n\nChange section 13.10, 1st paragraph from\n\n   The effect of certain methods at the origin server may cause\n   one or more existing cache entries to become non-transparently\n   invalid. That is, although they may continue to be \"fresh,\"\n   they do not accurately reflect what the origin server would\n   return for a new request.\n\nto\n\n   The effect of certain methods performed on a resource may cause\n   one or more existing cache entries to become non-transparently\n   invalid. That is, although they may continue to be \"fresh,\"\n   they do not accurately reflect what the origin server would\n   return for a new request.\n\nChange section 13.10, 4th paragraph from\n\n   Some HTTP methods may invalidate an entity. This is either\n   the entity referred to by the Request-URI, or by the Location\n   or Content-Location response-headers (if present).\n   These methods are:\n\n   o PUT\n   o DELETE\n   o POST\n\n   In order to prevent denial of service attacks, an invalidation\n   based on the URI in a Location or Content-Location header MUST\n   only be performed if the host part is the same as in the\n   Request-URI.\n\nto\n\n   All non-idempotent methods SHOULD invalidate a cached entity\n   identified either by the Request-URI, or by a Content-Location\n   header (if present).\n\n   In order to prevent denial of service attacks, an invalidation\n   based on the URI in Content-Location header MUST only be\n   performed if the host part is the same as in the Request-URI.\n\nThanks,\n\nHenrik\n--\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld Wide Web Consortium, MIT/LCS NE43-346\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: The meaning of 301 (was Re: 301/302",
            "content": "On Wed, 30 Jul 1997, Foteos Macrides wrote:\n\n> Klaus Weide <kweide@tezcat.com> wrote:\n> >On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n> >\n> >> >Unfortunately, when I get stuff I wrote back from a list server\n> >> >and re-read it, it often becomes clear that I don't understand what\n> >> >I'm talking about.  For a 301 on a POST, does that really mean\n> >> >substitute the new RequestURI for all future submissions, or only\n> >> >when the content is identical to that of the current submission?\n> >> \n> >> All future submissions -- 301 is a \"fix your damn links\" response. ;)\n> >> \n> >> ....Roy\n> >\n> >That makes sense, although I doubt it is implemented like that anywhere.\n> \n> Current versions of Lynx have it implemented it like that.  \n\nNot for POST submissions, as far as I can see.  Lynx is actually\nconverting the 301 to 302 internally in that case.  Try test form\nat <URL: http://sol.slcc.edu/~kweide/test301main.html>.\n\n       Klaus\n\n> They\n> do not \"fix your damn links\" in the sense of editing source files, because\n> that would be inapproriate or barred for a large percentage of the Lynx\n> user base, but for the remainder a user's session the 301's Location\n> will be substituted for the original RequestURI.  That's why it seemed\n> like a good idea to confirmed that this is really intended for a 301 on\n> a POST.\n> \n> Fote\n\n\n\n"
        },
        {
            "subject": "multipart/x-mixedreplac",
            "content": " \nHello,\n\nI wanted to know which servers support the multipart/x-mixed-replace\nmime type.\n\ncan some one provide me with some more info. \n\n\nCheers !!!\n   Nirmal\n   npatil@ee.tamu.edu\n   (409)-260-9165\n\n\n\n"
        },
        {
            "subject": "RE: Issue 1310_CACH",
            "content": "> ----------\n> Change section 13.10, 4th paragraph from\n> \n>    Some HTTP methods may invalidate an entity. This is either\n>    the entity referred to by the Request-URI, or by the Location\n>    or Content-Location response-headers (if present).\n>    These methods are:\n> \n>    o PUT\n>    o DELETE\n>    o POST\n> \n>    In order to prevent denial of service attacks, an invalidation\n>    based on the URI in a Location or Content-Location header MUST\n>    only be performed if the host part is the same as in the\n>    Request-URI.\n> \n> to\n> \n>    All non-idempotent methods SHOULD invalidate a cached entity\n>    identified either by the Request-URI, or by a Content-Location\n>    header (if present).\n> \n>    In order to prevent denial of service attacks, an invalidation\n>    based on the URI in Content-Location header MUST only be\n>    performed if the host part is the same as in the Request-URI.\n> \nThis would be wrong. PUT is idempotent, as is DELETE, and both of them\nneed to invalidate what is in the Request-URI, or Content-Location\nheader (if present).\n\n\n\n"
        },
        {
            "subject": "POST proble",
            "content": "I'm experiencing trouble with the POST request. I am currently practicing\nWeb Client Programming, so any help would be appreciated. I am trying to\nsubmit a site to the following CGI program:\n\nhttp://saturn.opentext.net/cgi-bin/submit.pl\n\nusing the POST method. It works fine when I use a form, but when I try to\ndo it with a Perl script - no success. The form can be found at:\n\nhttp://saturn.opentext.net/main/submitURL.html\n\nI am almost sure I did everything right. Could anyone please try to make\nthis thing work. Thanks in advance.\n\n\nTomer Shiran\n- President and CEO of Netscent Corporation\n  http://www.netscent.com/\n- Co-author of \"Learn Advanced JavaScript Programming\"\n  ISBN: 1-55622-552-0\n  http://www.geocities.com/~yehuda/\n\n\n\n"
        },
        {
            "subject": "Three new IDs re. URL i18",
            "content": "Dear friends,\n\nYesterday I have submitted three Internet-drafts (deadlines\nmake me work :-) connected to the topic of internationalization\nof internet identifiers. They should soon appear in the IETF\nanouncements and on the respective ftp servers. You can already\nobtain them as:\n\nftp://ftp.ifi.unizh.ch/outgoing/draft-duerst-dns-i18n-01.txt\nftp://ftp.ifi.unizh.ch/outgoing/draft-duerst-i18n-norm-00.txt\nftp://ftp.ifi.unizh.ch/outgoing/draft-duerst-query-i18n-00.txt\n\nThe first draft describes a method to internationalize DNS\neasily without having to change the core DNS software in any\nway. It is an update of a draft that expired a few weeks ago.\nWith enough consensus, this could go into experimental phase\nrather quickly.\n\nThe second draft is an *initial attempt* at defining normalization\nand additional guidelines for international identifiers. This\nis necessary to eliminate bad user surprises, but does not have\nvery high priority as the normalization rules can be defined so\nthat most current systems and users anyway only produce already \nnormalized identifiers.\n\nThe third draft contains material about the problem of internationalizing\nthe query component in URLs. This is a particularly important, but\nalso particularly tricky part of URL internationalization. The\ndraft describes a convention currently used by most browsers\n(and working in many cases), and some additions to the HTTP\nprotocol (intended to be version-independent) for a safe upgrade\nto UTF-8 URLs. The basic idea should be sound; the syntax and\nterminology may need more work.\n\nI am glad about any feedback directly back to me or on any of\nthe lists I am posting this information. I appologize to those\nthat get this information more than once. I will also be in\nMunich and look forward to talk to anybody interested.\nHowever, because most parties seriously interested in URL\ninternationalization (their number is steadily increasing :-)\nhave indicated that they won't be able to attend Munich,\nI have unfortunately had to cancel a planned official meeting\n(the specific topic of the meeting was that of the second draft\nabove). I hope that a meeting will become reality at a later\nIETF. One occasion where there will certainly be a lot of\ndiscussion about internationalized URLs, just because many\nof the players happen to attend or be around, will be the\nupcomming 11th International Unicode Conference in early\nSeptember in San Jose.\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "there are many more elegant ways to distinguish between\nold and new versions of user-written scripts, too, but\nmy point was to counter the claim that it was _impossible_,\nnot to invent an elegant possibility.\n\nIn general, if there are going to be any changes are\nbug fixes, we have to deal with scripts that were written\nagainst older specs. There are \".asis\" CERN server scripts\nthat attempt to emit not just the body and a few directives\nbut the entire HTTP response. Clearly, if those scripts\nare not rewritten but aren't conformant, they can't be\ncalled HTTP. So at some point you _must_ distinguish scripts\n(and servers) by their version, and either label older\nscripts as \"not HTTP/1.x\" or else patch up their output.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "HTTPWG new charter milestone",
            "content": "It's quite a while after the deadline for a revised charter/milestone\nlist, but here is my best estimate as to the schedule for HTTP-WG:\n\nJuly 1997:\"Hit Metering\" to IESG as Proposed Standard\n   (draft-ietf-http-hit-metering)\n\nAugust 1997:\n  \"State Management Mechanism\" to IESG as Proposed Standard.\n   (draft-ietf-http-state-man-mec)\n\n\nAug, 1997:\n  discuss revised HTTP/1.1 internet draft\n          revised PEP internet draft\n          revised Content Negotiation Requirements draft\n          revised TCN draft\n\n\nSeptember 1997:\n-  Revised draft on Connection Management HTTP\n-  Revised Content Negotiation Requirements draft\n\n\nNovember 1997: all documents to IESG:\n  + HTTP/1.1, as two or more documents, to become Draft Standard\n  + requirements for content negotiation, to become Informational RFC\n  + Transparent Content Negotiation, to become Experimental RFC\n  + Feature registration, to become Best Current Practice\n  + Connection Management, to become Informational or Proposed Standard\n  + PEP, to become either Proposed Standard or Experimental\n\n+ Dec 1997:\n    Working group to suspend, close, or continue, depending\n   on additional review necessary.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION: draft-ietf-http-options00.tx",
            "content": ">--NextPart\n>\n>A New Internet-Draft is available from the on-line Internet-Drafts directories.\n>This draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n>\n>Title: Specification of HTTP/1.1 OPTIONS messages\n>Author(s): J Mogul and S Lawrence and J Cohen\n>Filename: draft-ietf-http-options-00.txt\n>Pages: 13\n>Date: 30-Jul-97\n>\n>RFC2068 defined a new OPTIONS method for HTTP/1.1.  The\n>        purpose of OPTIONS is to allow a client to determine the\n>        options and/or requirements associated with a resource, or\n>        the capabilities of a server, without implying a resource\n>        action or initiating a resource retrieval.  However,\n>        RFC2068 did not defined a specific syntax for using OPTIONS\n>        to make such a determination.  This proposal clarifies the\n>        original specification of OPTIONS, adds several new HTTP\n>        message headers to provide syntactic support for OPTIONS,\n>        and establishes new IANA registries to avoid namespace\n>        conflicts.\n>\n>Internet-Drafts are available by anonymous FTP.  Login wih the username\n>'anonymous' and a password of your e-mail address.  After logging in,\n>type 'cd internet-drafts' and then\n>'get draft-ietf-http-options-00.txt'.\n>A URL for the Internet-Draft is:\n>ftp://ds.internic.net/internet-drafts/draft-ietf-http-options-00.txt\n>\n>Internet-Drafts directories are located at:\n>\n>Africa:ftp.is.co.za\n>\n>Europe: ftp.nordu.net\n>ftp.nis.garr.it\n>\n>Pacific Rim: munnari.oz.au\n>\n>US East Coast: ds.internic.net\n>\n>US West Coast: ftp.isi.edu\n>\n>Internet-Drafts are also available by mail.\n>\n>Send a message to:mailserv@ds.internic.net.  In the body type:\n>'FILE /internet-drafts/draft-ietf-http-options-00.txt'.\n>\n>NOTE:The mail server at ds.internic.net can return the document in\n>MIME-encoded form by using the 'mpack' utility.  To use this\n>feature, insert the command 'ENCODING mime' before the 'FILE'\n>command.  To decode the response(s), you will need 'munpack' or\n>a MIME-compliant mail reader.  Different MIME-compliant mail readers\n>exhibit different behavior, especially when dealing with\n>'multipart' MIME messages (i.e. documents which have been split\n>up into multiple messages), so check your local documentation on\n>how to manipulate these messages.\n>\n>\n>Below is the data which will enable a MIME compliant mail reader\n>implementation to automatically retrieve the ASCII version of the\n>Internet-Draft.\n>\n>--NextPart\n>Content-Type: Multipart/Alternative; Boundary='OtherAccess'\n>\n>OtherAccess\n>Content-Type:  Message/External-body;\n>access-type='mail-server';\n>server='mailserv@ds.internic.net'\n>\n>Content-Type: text/plain\n>Content-ID:<19970730155915.I-D@ietf.org>\n>\n>ENCODING mime\n>FILE /internet-drafts/draft-ietf-http-options-00.txt\n>\n>--OtherAccess\n>Content-Type:Message/External-body;\n>name='draft-ietf-http-options-00.txt';\n>site='ds.internic.net';\n>access-type='anon-ftp';\n>directory='internet-drafts'\n>\n>Content-Type: text/plain\n>Content-ID:<19970730155915.I-D@ietf.org>\n>\n>--OtherAccess--\n>\n>--NextPart--\n>\n>\n>\n\n\n\n"
        },
        {
            "subject": "Re: HTTPWG new charter milestone",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>It's quite a while after the deadline for a revised charter/milestone\n>list, but here is my best estimate as to the schedule for HTTP-WG:\n\nWould you please clarify the situation with respect to\nKoen's \"safe\" header draft.  That has been implemented in released\nversions of Lynx, can be implemented immediately by any server/script\nwhich processes form submissions with METHOD=POST, and poses no backward\ncompatibility problems for UAs which haven't implemented it.  Koen and\nI both posted messages requesting this WG to move forward on that, but\nno reply has been forthcoming as yet.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Draft 08 submitted to IETF ID editor",
            "content": "Subject (and abstract) says pretty much it all.  Varous forms of\nthe document available off of the issues list page.  For those of\nyou involved in drafting text, the easiest way to review they were\nincorporated properly is if you can use  Microsoft Word 97; if you can, there\nare comments attached to each change in the document, and hyperlinks\nfrom the comments back to the issues list to make your reviews easier.\n\nBy my count, there are ~20 issues of all sorts still to be completed\n(out of 82 in the current count).  Most have almost all of the solutiondone\nfor them. Lets see how many we can reach closure on before Munich, shall we?\n\nI don't plan to read mail again before Monday.  Gotta teak a break\nafter the push to get the draft out.  I'll see many of you\nin Munich.\nYour editor,\n- Jim Gettys\n\n\n    Hypertext Transfer Protocol -- HTTP/1.1\n\n            Abstract\n\nThe Hypertext Transfer Protocol (HTTP) is an application- level\nprotocol for distributed, collaborative, hypermedia information\nsystems. It is a generic, stateless, object- oriented protocol which\ncan be used for many tasks, such as name servers and distributed\nobject management systems, through extension of its request methods. A\nfeature of HTTP is the typing and negotiation of data representation,\nallowing systems to be built independently of the data being\ntransferred.\n\nHTTP has been in use by the World-Wide Web global information\ninitiative since 1990. This specification defines the protocol\nreferred to as \"HTTP/1.1\".\n\nThe issues list for HTTP/1.1 can be found at:\nhttp://www.w3.org/Protocols/HTTP/Issues/.\n\nThis draft does not resolve all open issues in the HTTP/1.1\nspecification requiring closure before HTTP/1.1 goes to draft\nstandard.  It does, however, close most of them, and note where in the\ndocument there are still significant issues under discussion.  The\nbest way to view this document is to get a copy of the Word 97\ndocument found at: http://www.w3.org/Protocols/HTTP/1.1/diff-v11-\nRFC2068to08.doc; all issues are noted as comments in the source\ndocument, with hyperlinks to the Issues list.\n\nThe most significant outstanding issue is OPTIONS; there is a separate\ninternet draft on the topic that you should review NOT incorporated\ninto this draft (though editorial notes identify where changes may\noccur).  This draft is draft-ietf-http-options-00.txt.\n\nAlso an issue: AGE-CALCULATION; Roy Fielding has issued an ID on the\ntopic; Jeff Mogul intends to issue a draft as well.\n\nThe editorial group is very interested in feedback on the sample table\nof requirements in this draft (issue REQUIREMENTS, section 1.9). Is it\nuseful?  How could it be improved?\n\nOpen or drafting issues not incorporated into this draft include:\nREDIRECTS, ENCODING-NOT-CONNEG, DATE_IF_MODIFIED, 403VS404, PUT-RANGE,\nHOST, AGE-CALCULATION, RE- AUTHENTICATION-REQUESTED, VARY\n\nIssues incorporated into this draft where there is still controversy\nare noted in bold italic with an editor's note.  These are issues:\nCONTENT-ENCODING, CACHING-CGI.\n\nIssues incorporated into this draft being working group last called\nare: AUTH-CHUNKED, RETRY-AFTER, PROXY-REDIRECT\n\nClosed issues incorporated into this draft include: PROXY-\nAUTHORIZATION, PROXY-LENGTH, LANGUAGE-TAG, TSPECIALS, STATUS100,\nQZERO, RANGE-ERROR, CLARIFY-NO-CACHE, COMMENT, CONTENT-LOCATION,\nQUOTED-BACK, CACHE-CONTRA, CACHE- DIRECTIVE, BYTE-RANGE,\nLWS-DELIMITER, CRLF, MAX-AGE, 100DATE, DISPOSITION, CHUNKED, CACHING,\nWARNINGS, VERSION, PROXY-MAXAGE, CHARSET-WILDCARD, PADDING,\nCONNECTION, RANGES, WARNING-8859, SHOULD-8859, X-BYTERANGES,\nMULTIPLE-TRANSFER- CODINGS, LINK_HEADER.\n\nEditorial issues still open include: CLEANUP, UTF-8, URL-SYNTAX,\nENTITY, DOCKDIGEST, 1310_CACHE.\n\nEditorial issues closed include: ACCEPT-RANGES, KEEP-ALIVE, BNFNAME,\nKEYWORDS, RESPONSE-VERSION, XREF, COMMON-HEADERS, NO-CACHE, FIX-REF,\nPERSIST-CONFUSED, CONNECTION2, GMT-UTC, PROXY-FORWARD, REFERER-SEC,\nCHUNK-EXT, REMOVE_19.6, IDEMPOTENT, REF-SIGCOMM, 1521-OBSOLETE,\nMESSAGE-BODY\n\nApologies for the extreme length; Microsoft Word exhibited a fatal bug\nwhenever trying to adjust margins when converting to ascii text;\ntherefore, the margins are extreme and the document very long in\nascii.  Get the Postscript version off the Issues list!\n\n\n\n"
        },
        {
            "subject": "Re: HTTPWG new charter milestone",
            "content": "Foteos Macrides:\n>\n>Larry Masinter <masinter@parc.xerox.com> wrote:\n>>It's quite a while after the deadline for a revised charter/milestone\n>>list, but here is my best estimate as to the schedule for HTTP-WG:\n>\n>Would you please clarify the situation with respect to\n>Koen's \"safe\" header draft.  That has been implemented in released\n>versions of Lynx, can be implemented immediately by any server/script\n>which processes form submissions with METHOD=POST, and poses no backward\n>compatibility problems for UAs which haven't implemented it.  Koen and\n>I both posted messages requesting this WG to move forward on that, but\n>no reply has been forthcoming as yet.\n\nQuick comment on the status of the safe draft: I submitted a new\nversion (draft-holtman-http-safe-02.txt) to the ID editor yesterday,\nwith slightly improved wording, mainly because the old one had expired.\n\nI think that SAFE should be added to the 1.1 issue list.\n\n>Fote\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTPWG new charter milestone",
            "content": "Please be so kind as to remove me from this mailing list.\n\nThank you.\n\nKoen Holtman wrote:\n\n> Foteos Macrides:\n> >\n> >Larry Masinter <masinter@parc.xerox.com> wrote:\n> >>It's quite a while after the deadline for a revised\n> charter/milestone\n> >>list, but here is my best estimate as to the schedule for HTTP-WG:\n> >\n> >       Would you please clarify the situation with respect to\n> >Koen's \"safe\" header draft.  That has been implemented in released\n> >versions of Lynx, can be implemented immediately by any server/script\n>\n> >which processes form submissions with METHOD=POST, and poses no\n> backward\n> >compatibility problems for UAs which haven't implemented it.  Koen\n> and\n> >I both posted messages requesting this WG to move forward on that,\n> but\n> >no reply has been forthcoming as yet.\n>\n> Quick comment on the status of the safe draft: I submitted a new\n> version (draft-holtman-http-safe-02.txt) to the ID editor yesterday,\n> with slightly improved wording, mainly because the old one had\n> expired.\n>\n> I think that SAFE should be added to the 1.1 issue list.\n>\n> >                               Fote\n>\n> Koen.\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Larry Masinter wrote:\n> \n> there are many more elegant ways to distinguish between\n> old and new versions of user-written scripts, too, but\n> my point was to counter the claim that it was _impossible_,\n> not to invent an elegant possibility.\n\nFair enough, though I don't think I made the claim that it was\nimpossible, merely that CGI was broken because we have to invent\nsomething outside it to deal with the problem.\n\n> In general, if there are going to be any changes are\n> bug fixes, we have to deal with scripts that were written\n> against older specs. There are \".asis\" CERN server scripts\n> that attempt to emit not just the body and a few directives\n> but the entire HTTP response. Clearly, if those scripts\n> are not rewritten but aren't conformant, they can't be\n> called HTTP. So at some point you _must_ distinguish scripts\n> (and servers) by their version, and either label older\n> scripts as \"not HTTP/1.x\" or else patch up their output.\n\nSurely if one never changes the semantics of existing headers it is not\nnecessary to know which version is being used?\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Absolutely, but unfortunately we can't change the thousands of scripts\nalready out there and I don't believe we can just say \"They are broken,\ntough.\" As Lynx has already found out, if you don't support 301/302 in\nthe broken sense you just don't work with the content out there. 301/302\nare historical artifacts and we are stuck with them.\nYaron\n\n> -----Original Message-----\n> From:Ben Laurie [SMTP:ben@algroup.co.uk]\n> Sent:Wednesday, July 30, 1997 1:26 AM\n> To:Yaron Goland\n> Cc:'Larry Masinter'; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: 301/302\n> \n> Yaron Goland wrote:\n> > \n> > Scripts are deployed and they are not going to get re-written.\n> > \n> > Servers have no mechanism for determining if a script is 1.0 or 1.1\n> and\n> > even if they did, they do not have facilities for taken any special\n> > action in that case.\n> \n> Which is, of course, a deficiency in CGI (and related) specs. If\n> anyone\n> ever gets around to sorting them out, they really ought to have a\n> version number somewhere in the conversation between server and\n> script.\n> \n> Cheers,\n> \n> Ben.\n> \n> -- \n> Ben Laurie                Phone: +44 (181) 994 6435  Email:\n> ben@algroup.co.uk\n> Freelance Consultant and  Fax:   +44 (181) 994 6472\n> Technical Director        URL: http://www.algroup.co.uk/Apache-SSL\n> A.L. Digital Ltd,         Apache Group member (http://www.apache.org)\n> London, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "RE: 301/30",
            "content": "Agreed.\nYaron\n\n> -----Original Message-----\n> From:Klaus Weide [SMTP:kweide@tezcat.com]\n> Sent:Wednesday, July 30, 1997 10:34 AM\n> To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: 301/302 \n> \n> On Tue, 29 Jul 1997, Roy T. Fielding wrote:\n> \n> > As Foteos hinted, swapping the meaning of 302 and 303 is a solution\n> > to the implementation problem.  I don't think it would affect Apache\n> much.\n> > However, it would require universal agreement among the rest of the\n> > implementers, and it would require recycling HTTP/1.1 as Proposed\n> > and not as a Draft Standard.  It is not something to be taken\n> lightly.\n> \n> I hope the idea of just \"swapping\" 302 and 303 is not being\n> entertained\n> seriously.  303 is a clean thing and doesn't need to be fixed - don't\n> dump the problem on those who have tried to do the right thing.[1]\n> \n> If 302 is in such a mess that it effectively cannot be uses for the\n> purpose it was always intended to serve, by the protocol designers (in\n> connection with non-GET methods) - why, use a new status code for what\n> is\n> needed (but don't reuse 303!).\n> \n> More specific proposal:\n> \n>  - Assign a new code (say, 307 - or whichever is the first free\n> number).\n>    This code means \"This resource has moved, and we really mean it.\n>    Re-send full messages to the new address (not just empty\n> envelopes).\"\n>    I.e. what 302 was supposed to mean.[2]\n> \n>  - Mark 302 as DEPRECATED.  Servers and scripts should use 303 or 307\n>    instead, as appropriate.  But for compatibility, not send 307 to\n>    older clients.[3].\n> \n>  - Describe 302 as a \"General Redirection\".  For GET requests, the\n> meaning\n>    is as for 303.  For other methods, the outcome is UNSPECIFIED.\n>    - Add some notes explaining this. \"Most, but not all, clients will\n>    treat a 302 in response to a POST like 303, but don't rely on it.\"\n> \n>  - Clients are required to understand 303 and 307.  Also for\n>    compatibility,  they are required to treat 302 in response to a GET\n>    like a 303.  If they get 302 in response to another method - well\n>    it's up to them how they interpret \"General Redirection\".[4]\n> \n>  - There is no need to change anything about 301 or 303.\n> \n> Notes:\n> [1] There probably aren't many who use 303.  But at least the lynx\n> mailing\n>     list has directed people with problems to read the HTTP specs (RFC\n> \n>     1945, then the 1.1 draft and later RFC), to read the \"Note:\"s in\n> the\n>     301 and 302 descriptions, and to use 303.\n> \n> [2] Services needing the \"full redirection\" behavior of POST, PUT,\n> etc.\n>    will be new services.  They cannot reliably use 302 for this today\n>    anyway, so it's not asking too much to make them use a new status\n> code.\n>    They are the ones to profit from it.\n> \n> [3] Of course that's a problem, if the server cannot reliably detect\n> the\n>     client's protocol version.  However, this would only affect\n> services\n>     that really need the 307 behavior - the old \"official\" 302\n> behavior -\n>     and they cannot get that reliably today.  So this doesn't make it\n>     worse for anyone.  Services requiring the 307 behavior would\n> probably \n>     (initially) operate in a controlled environment where out-of-band \n>     information about client version is available (or it can be\n> guaranteed\n>     that there are no proxies etc.)\n> \n> [5] This may seem bad, but I think it's a reflection of how things\n> are.\n>    This change would give existing implementations a better status\n> than they \n>    have with the current wording of RFC 2068 (*and* 1945): Instead of\n>    clearly acting \"erroneously\", they would just be using a deprecated\n>    feature.  At the same time this change avoids putting\n> implementations\n>    in the wrong who have tried to do what RFCs 2068 and 1945 say about\n> 302 -\n>    therefore this change may be possible without going back to\n> \"Proposed\".\n> \n> \n>     Klaus\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "I think we now have a proposed solution for 301/302 (add 307),\nand hope it gets onto the issue list so that we can LAST CALL it.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: Issue 1310_CACH",
            "content": "Henrik writes:\n\n    I would like to link this to Jeff's new section on idempotent\n    methods but as I haven't seen it (and don't believe I will) then\n    please check for consistency in the last proposed paragraph.\n\nAs Paul Leach pointed out, section 13.10 has nothing at all to\ndo with idempotency; all that matters is whether the method\ncauses a change in the value of a resource.\n\nIt's also important to note that the purpose of the rules in\n13.10 is NOT to provide perfect cache consistency when a\nresource is changed; as the second paragraph of the section\nstates, this is impossible when there are multiple cached paths\nto the origin server.  The purpose of 13.10 is to \"reduce the\nlikelihood of erroneous behavior.\"\n\nWith that in mind:\n\nHenrik proposes to change section 13.10, 1st paragraph from\n\n   The effect of certain methods at the origin server may cause\n   one or more existing cache entries to become non-transparently\n   invalid. That is, although they may continue to be \"fresh,\"\n   they do not accurately reflect what the origin server would\n   return for a new request.\n\nto\n\n   The effect of certain methods performed on a resource may cause\n   one or more existing cache entries to become non-transparently\n   invalid. That is, although they may continue to be \"fresh,\"\n   they do not accurately reflect what the origin server would\n   return for a new request.\n\nI'm not sure this helps.  It's the effect *at the origin server*\nthat is not necessarily visible to the caches.  However, it's\nprobably possible to make this a little clearer.  How about:\n\n   The effect of certain methods performed on a resource at the origin\n   server may cause one or more existing cache entries to become\n   non-transparently invalid. That is, although the cache entries may\n   continue to be \"fresh,\" they do not accurately reflect what the\n   origin server would return for a new request on that resource.\n\nHenrik proposes to change section 13.10, 4th paragraph from\n\n   Some HTTP methods may invalidate an entity. This is either\n   the entity referred to by the Request-URI, or by the Location\n   or Content-Location response-headers (if present).\n   These methods are:\n\n   o PUT\n   o DELETE\n   o POST\n\n   In order to prevent denial of service attacks, an invalidation\n   based on the URI in a Location or Content-Location header MUST\n   only be performed if the host part is the same as in the\n   Request-URI.\n\nto\n\n   All non-idempotent methods SHOULD invalidate a cached entity\n   identified either by the Request-URI, or by a Content-Location\n   header (if present).\n\n   In order to prevent denial of service attacks, an invalidation\n   based on the URI in Content-Location header MUST only be\n   performed if the host part is the same as in the Request-URI.\n\nAs Paul pointed out, this is wrong, because it is based on the\nwrong criteria.  However, the original language is slightly\nmisleading, because the phrase \"may invalidate an entry\" doesn't\nquite match the use in the previous paragraph in 13.10:\n\n   In this section, the phrase \"invalidate an entity\" means that the\n   cache should either remove all instances of that entity from its\n   storage, or should mark these as \"invalid\" and in need of a mandatory\n   revalidation before they can be returned in response to a subsequent\n   request.\n\nSo I'd replace:\n\n   Some HTTP methods may invalidate an entity. This is either\n   the entity referred to by the Request-URI, or by the Location\n   or Content-Location response-headers (if present).\n   These methods are:\n\nwith:\n\n   Some HTTP methods MUST cause a cache to invalidate an entity. This\n   is either the entity referred to by the Request-URI, or by the\n   Location or Content-Location response-headers (if present).  These\n   methods are:\n\nFinally, Henrik asks:\n\n    Also, I fail to see why the URI in a location header can invalidate\n    a cached entry as written. If a resource is moved and there is a\n    location header in the response, then the cached entry should be\n    updated to reflect this, but this is already described in section\n    13.4.\n\nOK, let's use your example.  Suppose the effect of a POST method on\nresource http://x.com/y is to move the resource to a new location\nhttp://x.com/z.  After this operation has taken place at the origin\nserver, any cache entries for *either* http://x.com/y or\nhttp://x.com/z could be bogus.  It's not clear what it would mean\nto \"update the cached entry to reflect this.\"  The safest approach\nis to say \"none of the cache entries for any of these URLs can\nbe used without revalidation.\"\n\nTo be frank, this whole section is (as noted above) somewhat of\na lost cause.  HTTP does not have a formal mechanism for a server\nto invalidate a cache entry that isn't the Request-URI of the\ncurrent request, and there are many situations where this might\nbe a useful mechanism.  We should probably leave it to WEBDAV\nto hack away at this, although (based on my previous research on\ncache consistency in distributed file systems) I'm dubious that\nany version of HTTP/1.x could actually solve the problem.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "> \n> Surely if one never changes the semantics of existing headers it is not\n> necessary to know which version is being used?\n\nNo; we may not change the semantics of headers, but we're changing\nthe requirements for what it means to be 1.1 compliant, vs. 1.0.\nIf you don't know that you're 1.1 compliant -- that your script\nsatisfies all of the requirements that are MUST for 1.1, then you\nneed to label the response as 1.0.\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Larry Masinter wrote:\n> \n> >\n> > Surely if one never changes the semantics of existing headers it is not\n> > necessary to know which version is being used?\n> \n> No; we may not change the semantics of headers, but we're changing\n> the requirements for what it means to be 1.1 compliant, vs. 1.0.\n> If you don't know that you're 1.1 compliant -- that your script\n> satisfies all of the requirements that are MUST for 1.1, then you\n> need to label the response as 1.0.\n\nOK. It still seems to me that the correct thing to do is to fix CGI. A\nsimple thing to do would be to add a version header:\n\nCGI-Version: 1.1\n\nAbsence of the header means the script is 1.0 compliant. This is not an\nHTTP header - the server would strip it, I assume, and doctor other\nheaders as needed.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Fri, 1 Aug 1997, Ben Laurie wrote:\n\n> \n> OK. It still seems to me that the correct thing to do is to fix CGI. A\n> simple thing to do would be to add a version header:\n> \n> CGI-Version: 1.1\n> \n> Absence of the header means the script is 1.0 compliant. This is not an\n> HTTP header - the server would strip it, I assume, and doctor other\n> headers as needed.\n> \n\nThe current (widely deployed) version of CGI is called version 1.1.\nThis has nothing to do with HTTP versions.  Thus something other\nthan \"CGI-Version: 1.1\" would be advisable to avoid confusion.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Yaron Goland <yarong@microsoft.com> wrote:\n>Absolutely, but unfortunately we can't change the thousands of scripts\n>already out there and I don't believe we can just say \"They are broken,\n>tough.\" As Lynx has already found out, if you don't support 301/302 in\n>the broken sense you just don't work with the content out there. 301/302\n>are historical artifacts and we are stuck with them.\n\nPlease consider the historical recorded when discussing these\nartifacts.  The 301/302 statuses are homologous to the /htbin, /cgi-bin,\n'?' non-caching rules, which as Dave Morris pointed out were in effect\nbefore there was any HTTP RFC.\n\nPermanent redirection of form ACTIONs is a rarity, but 302\nwas and still is widely used with forms for searching Internet\nresources and then redirecting the UA to the appropriate resource.\nThe dumping of POST content and use of GET for that redirection\nmakes it totally safe.  It is thus a very popular information\nsharing design.\n\nThe HTTP/1.0 RFC changed the meaning of 302, requiring POST\ncontent to be redirected as well, and the POST method to be retained.\nMoreover, it did not include a status for doing what *all* existing\napplications were doing for 302.  They had no need for the behavior\ndefined in the HTTP/1.0 RFC, and did have a need for the behavior\nit eliminated.  Understandably, the behavior defined in the HTTP/1.0\nRFC was ignored by all applications, include those whose implementors\nearnestly place a high value on standards for interoperability.\n\nThis issue was raised in discussions of early HTTP/1.1 drafts,\nbut instead of fixing the bug of a misdefined 302, a 303 for the desired\nbehavior was added.  The 303 is not, in any valid sense of the word, a\nnew HTTP/1.1 status.  It is the valid 302 behavior, misnumbered.\n\nObjections to retaining the misdefinition for 302, and instead\nmisnumbering it as 303, were raised during that discussion.\n\nThe reply was just no.\n\nIn that case I took just no for an answer, and conducted the\nexperiment.\n\nIt was a fiasco.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "agenda for Munic",
            "content": "Here are the topics for Munich, in the order in which\nI'd like to visit them. Before we allocate times to each\ntopic, I'd like to make sure I didn't leave anything out.\n\n-  Review of draft\n   summary of issues closed & included\n   comments from community\n   conformance summary\n-  open issues\n   (e.g., PUT-RANGE, REAUTHENTICATION-REQUESTED,\n    REDIRECTS, SAFE, CONTENT-ENCODING, RE-VERSION)\n-  content negotiation\n-  PEP\n-  State management\n-  new issues\n-  Network management\n-  NG status report\n-  WG schedule\n\n\n\n"
        },
        {
            "subject": "Re: agenda for Munic",
            "content": "  Unfortunately, I won't be able to make Munich; I will attempt to\n  summarize my views (where I have any) on the open issues over the\n  next few days, but I'm interested in what you mean to cover under\n  the topic 'Network Management'.\n\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Implementation Experience Content Encodin",
            "content": "I've reached a point of frustration with content encoding..\n\nconsider an http/1.1 server that receives this request\n\nHEAD /m013/compressfly.cgi?file=idhm&style=gzip HTTP/1.0\n\nand generates this response\n\nHTTP/1.1 200 OK\nDate: Fri, 01 Aug 1997 17:46:27 GMT\nServer: ATCm005/1.0d\nContent-Encoding: gzip\nConnection: close\nContent-Type: text/plain\n\n<< whole bunch of gzip encoded plain text here>>\n<close of connection>\n\n----\n\nthe problem is that the windows versions of msie and netscape (version\n4 for both) aren't able to remove that encoding and show the\ntext/plain underneath.. (netscape show's the gzip content, and msie\ngives an error message)... a content-Encoding of x-gzip doesn't change\nthe behavior.. UNIX versions of netscape and lynx don't seem to have a\nproblem... this run contrary to the \"Note\" of 14.3 in draft 8 that\ncalls \"gzip\" and \"compress\" commonly understood content-codings by\nHTTP/1.0 clients (which are to be viable alternatives if there is no\nAccept-Encoding field present in the request)..\n\nNow I'd always thought that note was correct in practice, but the\nabove indicates otherwise.. anybody know what the\nstate-of-the-industry is here?\n\n-P\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "John Franks wrote:\n> \n> On Fri, 1 Aug 1997, Ben Laurie wrote:\n> \n> >\n> > OK. It still seems to me that the correct thing to do is to fix CGI. A\n> > simple thing to do would be to add a version header:\n> >\n> > CGI-Version: 1.1\n> >\n> > Absence of the header means the script is 1.0 compliant. This is not an\n> > HTTP header - the server would strip it, I assume, and doctor other\n> > headers as needed.\n> >\n> \n> The current (widely deployed) version of CGI is called version 1.1.\n> This has nothing to do with HTTP versions.  Thus something other\n> than \"CGI-Version: 1.1\" would be advisable to avoid confusion.\n\nSorry - I realised that as I was hitting send... I'd suggest\nCGI-HTTP-Version instead.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie                Phone: +44 (181) 994 6435  Email:\nben@algroup.co.uk\nFreelance Consultant and  Fax:   +44 (181) 994 6472\nTechnical Director        URL: http://www.algroup.co.uk/Apache-SSL\nA.L. Digital Ltd,         Apache Group member (http://www.apache.org)\nLondon, England.          Apache-SSL author\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": "In a previous episode Patrick McManus said...\n:: \n:: I've reached a point of frustration with content encoding..\n:: \n:: consider an http/1.1 server that receives this request\n:: \n:: HEAD /m013/compressfly.cgi?file=idhm&style=gzip HTTP/1.0\n:: \n\n   ^^^ That's obviously GET.. sorry bout the cut'n'paste job.\n\n-P\n\n:: and generates this response\n:: \n:: HTTP/1.1 200 OK\n:: Date: Fri, 01 Aug 1997 17:46:27 GMT\n:: Server: ATCm005/1.0d\n:: Content-Encoding: gzip\n:: Connection: close\n:: Content-Type: text/plain\n:: \n:: << whole bunch of gzip encoded plain text here>>\n:: <close of connection>\n:: \n:\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": ">>>>> \"PM\" == Patrick McManus <mcmanus@appliedtheory.com> writes:\n\nPM> I've reached a point of frustration with content encoding..\n\nPM> consider an http/1.1 server that receives this request\n\nPM> HEAD /m013/compressfly.cgi?file=idhm&style=gzip HTTP/1.0\n\nPM> and generates this response\n\nPM> HTTP/1.1 200 OK\nPM> Date: Fri, 01 Aug 1997 17:46:27 GMT\nPM> Server: ATCm005/1.0d\nPM> Content-Encoding: gzip\nPM> Connection: close\nPM> Content-Type: text/plain\n\nPM> << whole bunch of gzip encoded plain text here>>\nPM> <close of connection>\n\n  First of all, there should be no body in response to a HEAD request,\n  but assuming that you meant GET, there is still the issue that\n  you've apparently taken instructions from the human user here.\n  Presumably the 'file' and 'style' parameters came from a form, and\n  the user selected 'gzip' from a menu; the compressfly.cgi program\n  just did as it was told...\n\n  If I were writing such a script, I would check for the presence of\n  an Accept-Encoding header which included 'gzip'; finding one, I\n  would send the response you show above and expect it to work.\n  Without one, I would send:\n\nHTTP/1.1 200 OK\nDate: Fri, 01 Aug 1997 17:46:27 GMT\nServer: ATCm005/1.0d\nConnection: close\nContent-Type: application/x-gzip\n\n<< whole bunch of gzip encoded plain text here>>\n<close of connection>\n\n  and let the browser do whatever it does with the gzip mime type.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": "In a previous episode Scott Lawrence said...\n:: \n:: \n:: >>>>> \"PM\" == Patrick McManus <mcmanus@appliedtheory.com> writes:\n:: \n\n:: PM> consider an http/1.1 server that receives this request\n:: \n:: PM> HEAD /m013/compressfly.cgi?file=idhm&style=gzip HTTP/1.0\n\n(that's meant to be \"GET\")\n\n:: \n:: PM> and generates this response\n:: \n:: PM> HTTP/1.1 200 OK\n:: PM> Date: Fri, 01 Aug 1997 17:46:27 GMT\n:: PM> Server: ATCm005/1.0d\n:: PM> Content-Encoding: gzip\n:: PM> Connection: close\n:: PM> Content-Type: text/plain\n:: \n:: PM> << whole bunch of gzip encoded plain text here>>\n:: PM> <close of connection>\n:: \n::   you've apparently taken instructions from the human user here.\n::   Presumably the 'file' and 'style' parameters came from a form, and\n::   the user selected 'gzip' from a menu; the compressfly.cgi program\n::   just did as it was told...\n:: \n\ndon't worry about what the URL means.. it's just a test scenario to\nforce some different behaviorial choices.. you are correct that it's\nkind of contrived.. but test cases can be like that.\n\n\n::   If I were writing such a script, I would check for the presence of\n::   an Accept-Encoding header which included 'gzip'; finding one, I\n::   would send the response you show above and expect it to work.\n::   Without one, I would send:\n\n:: Content-Type: application/x-gzip\n\nThat's nice for you, but my content has a type of text/plain so I want\nto label it like that.  14.3 of the current draft tells me\n\n            If no Accept-Encoding field is present in a request, the\n            server MAY assume that the client will accept any content\n            coding.  In this case, if \"identity\" is one of the available\n            content-codings, then the server SHOULD use the \"identity\"\n            content-coding, unless it has additional information that a\n            different content-coding is meaningful to the client.\n\n            Note: If the request does not include an Accept-Encoding\n            field, and if the \"identity\" content-coding is unavailable,\n            then preference should be given to content-codings commonly\n            understood by HTTP/1.0 clients (i.e., \"gzip\" and\n            \"compress\"); some older clients improperly displaymessages\n            sent with other content-encodings.  The server may alsomake\n            this decision based on information about the particular\n            user-agent or client.\n\nand I don't have an identity version of my resource hanging\naround.. (I deleted it because in this bizarre case disk space is\nmighty precious) so I sent back gzip and all hell broke loose on a\ncouple mighty popular windows browsers.\n\nSo am I doing something wrong, or is the spec misleading with its\nnote?\n\n-P\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Fri, 1 Aug 1997, Ben Laurie wrote:\n\n> Larry Masinter wrote:\n> > satisfies all of the requirements that are MUST for 1.1, then you\n> > need to label the response as 1.0.\n> \n> OK. It still seems to me that the correct thing to do is to fix CGI. A\n> simple thing to do would be to add a version header:\n> \n> CGI-Version: 1.1\n\n(nb., this suggestion mixes CGI and HTTP versions ...)\n\n> \n> Absence of the header means the script is 1.0 compliant. This is not an\n> HTTP header - the server would strip it, I assume, and doctor other\n> headers as needed.\n\nI think this issue is much larger than whether the server know that \nan individual CGI script complies with HTTP/1.1 in what it generates.\nThis just as easily fixed with out band server configuration choices.\nAs a separate subject, it may be appropriate for some group to standardize\nthe CGI API and include a version, but that isn't our task and such a\nchange won't help this problem...\n\nIf a script was written to empirical behavior, it (actually, the target\nbeing referenced) may expect a 302 response to POST to return as a GET.\n\nThat server may be accessed by a proxy which changes the HTTP status line\nversion to 1.1 making any necessary updates to header fields.\n\nIf a *client* which sees the 302 status on what is marked as an HTTP/1.1\nresponse adopts rigid 302 handling, an existing HTTP/1.0\nserver/application will break. Who gets blamed? It works with my old\nbrowser (still if I re-install it, etc.) and not the new one. In the\nuser's mind the blame rests with the browser, not an out of date server.\n\nThe 307 proposal works. Lets do it.\n\nDave MOrris\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": ">>>>> \"PM\" == Patrick McManus <mcmanus@appliedtheory.com> writes:\n\n\nPM> That's nice for you, but my content has a type of text/plain so I want\nPM> to label it like that.  14.3 of the current draft tells me\n\nPM>             If no Accept-Encoding field is present in a request, the\nPM>             server MAY assume that the client will accept any content\nPM>             coding....\n\nPM> and I don't have an identity version of my resource hanging\nPM> around.. (I deleted it because in this bizarre case disk space is\nPM> mighty precious) so I sent back gzip and all hell broke loose on a\nPM> couple mighty popular windows browsers.\n\nPM> So am I doing something wrong, or is the spec misleading with its\nPM> note?\n\n  The spec says you MAY assume that the client will accept any\n  encoding; it doesn't promise that will work, or place any\n  requirement on clients to do anything about it.  If I were you I'd\n  add gunzip-on-the-fly to my server so that you can send real\n  text/plain.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": "In a previous episode Scott Lawrence said...\n:: \n:: \n:: >>>>> \"PM\" == Patrick McManus <mcmanus@appliedtheory.com> writes:\n\n:: \n:: PM>             If no Accept-Encoding field is present in a request, the\n:: PM>             server MAY assume that the client will accept any content\n:: PM>             coding....\n:: \n:: PM> and I don't have an identity version of my resource hanging\n:: PM> around.. (I deleted it because in this bizarre case disk space is\n:: PM> mighty precious) so I sent back gzip and all hell broke loose on a\n:: PM> couple mighty popular windows browsers.\n:: \n:: PM> So am I doing something wrong, or is the spec misleading with its\n:: PM> note?\n:: \n::   The spec says you MAY assume that the client will accept any\n::   encoding; it doesn't promise that will work, or place any\n::   requirement on clients to do anything about it.  \n\nyou're quite right that it doesn't _promise_ anything. It does however\nsay _more_ than just the 'MAY assume' part.. this is from 14.3:\n\n            Note: If the request does not include an Accept-Encoding\n            field, and if the \"identity\" content-coding is unavailable,\n            then preference should be given to content-codings commonly\n            understood by HTTP/1.0 clients (i.e., \"gzip\" and\n            \"compress\"); some older clients improperly display messages\n            sent with other content-encodings.  The server may also make\n            this decision based on information about the particular\n            user-agent or client.\n\nthis to me says that in no uncertain terms, common 1.0 clients support\ngzip and compress. If that doesn't include netscape for windows then\nwe need to work on our definition of 'commonly understood'.\n\nhere's the rub: UA's sending the accept-encoding header is a rarer\nthing than them being able to decode the corresponding\ncontent-encoding.. ns for unix doesn't send the header, but it can do\nthe decoding fine.. ns and msie for windows don't send the header and\nseem to struggle with decoding.. lynx both sends the header and\ndecodes it (what a concept!)..\n\nI understand that this is sort of the way of life, and maybe a UA\nmatrix is what I have to live with in the server code (as the last\nsentence of the note mentions) but from a --working group\nperspective-- we have a spec that offers implementation advice (the\nnote: ) that I've found inaccurate wrt current practice.\n\n::  If I were you I'd\n::   add gunzip-on-the-fly to my server so that you can send real\n::   text/plain.\n\noh, it's there.. I'd just like to avoid using it when I can ;)\n\nthanks for your help,\n-P\n\n\n\n"
        },
        {
            "subject": "Re: Implementation Experience Content Encodin",
            "content": "Patrick McManus points out that the advice given in the\n-08 draft of the HTTP/1.1 spec, for Accept-Encoding, is\n(1) consistent with what many servers do, and (2)\ninconsistent with what many browsers can handle.\n\n(Note that I can't seem to find a copy of draft -08, so I'm\nworking from the text of the proposed solution from the Issues list.)\n\nIn particular, this Note\n\n            Note: If the request does not include an Accept-Encoding\n            field, and if the \"identity\" content-coding is unavailable,\n            then preference should be given to content-codings commonly\n            understood by HTTP/1.0 clients (i.e., \"gzip\" and\n            \"compress\"); some older clients improperly display messages\n            sent with other content-encodings.  The server may also make\n            this decision based on information about the particular\n            user-agent or client.\n\nis misleading, because several important HTTP/1.0 clients apparently\ndo NOT understand gzip and compress.  We should probably fix this\nimplication; how about:\n\n    Note: If the request does not include an Accept-Encoding\n    field, and if the \"identity\" content-coding is unavailable,\n    then preference should be given to content-codings commonly\n    understood by some (but not all) HTTP/1.0 clients (i.e.,\n    \"gzip\" and \"compress\"); some older clients improperly\n    display messages sent with other content-encodings.  The\n    server may also make this decision based on information\n    about the particular user-agent or client.\n\nHowever, there are really two different specification issues here:\n\n(A) If the server has a choice of content-codings, including\n\"identity\", and the client does not send an Accept-Encoding header,\nand the server cannot figure things out from the User-Agent header,\nshould the spec insist that the server send \"identity\", or should\nwe leave it up to the good sense of the server?\n\n(B) If the server does NOT have \"identity\" available, and it cannot\ntell from the request what content-codings the client will accept\n(i.e., no Accept-encoding and/or User-agent headers), what should\nthe server do?\n\nFor case (A), the draft says:\n    If no Accept-Encoding field is present in a request, the server MAY\n    assume that the client will accept any content coding.  In this\n    case, if \"identity\" is one of the available content-codings, then\n    the server SHOULD use the \"identity\" content-coding, unless\n    it has additional information that a different content-coding\n    is meaningful to the client.\n\nI.e., we do insist that if \"identity\" is available, and the client\nhasn't given solid information that it will accept something else,\na server SHOULD send \"identity\" (i.e., no Content-Encoding header).\n\nOf course, we cannot make this apply retroactively to older\nservers (especially HTTP/1.0 servers).  And we can't fix the\ndeployed base of browsers, either.  So, for the time being,\nit's basically up to the good sense of the Web site maintainer\nnot to do anything too silly.\n\nFor case (B) (i.e., no \"identity\" content-coding is available, and the\nclient hasn't given solid information that it will accept something\nelse), we basically punt; we let the server do what it wants to do.\nThere was a long discussion of what would happen to existing \"robot\"\nclients, which do not actually display or interpret what they\nretrieve (e.g., automatic mirroring software). If we were to insist\nthat the server return an error in this case, then existing robot\nsoftware would break.\n\nOne could argue that it would be better to break robots than to\ndeliver garbage screenfuls to human users ... but that would\nbe a change to the status quo.\n\nBottom line:\n(1) users ought to insist on browsers that don't display garbage\nwhen they get a Content-coding that they don't understand, and\nthey ought to insist on browsers that send explicit Accept-encoding\nheaders when appropriate.\n\n(2) Web site operators shouldn't send back compressed content-codings\nunless either (a) the client explicitly asks for that, or (b) that's\nthe only thing available.  And Web site operators shouldn't let\nthemselves end up with \"that's the only thing available\" too often.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: agenda for Munic",
            "content": "It's the responsibility of groups in the applications area\nto ensure that systems deployed using the protocols they've\npromoted are managable. So I'd like a status report on\nHTTP server network management, and I put it on the agenda.\nMaybe there's nothing there to say. Is there?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: agenda for Munic",
            "content": "On Fri, 1 Aug 1997, Larry Masinter wrote:\n\n> promoted are managable. So I'd like a status report on\n> HTTP server network management, and I put it on the agenda.\n> Maybe there's nothing there to say. Is there?\n\nIf you haven't already done that, it might be good to solicit a \nstatus update from the SNMP working group working on HTTP MIBs.\nSorry for the lack of precision in my group naming.\n\nAnd hopefully such a report will be nicely sumarized in the minutes so\nthose of us not there can learn as well.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Using Content-Encoding and ContentDisposition togethe",
            "content": "The following question comes up when Content-Encoding and\nContent-Disposition are used together in a message:\n\nTo which layer in the two-layer, ordered encoding model:\n\n                entity-body := Content-Encoding( Content-Type( data ) )\n\nof 7.2.1 (draft -08) does a suggested filename in Content-Disposition\napply?  As an example, let's take the gziped text format of the draft \nitself, available at\n<http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-08.txt.gz>.\nThanks by the way for providing it in this format, works great with Lynx.\n\nThis is currently being served with headers (among others)\n\n  Content-Encoding: gzip\n  Content-Type: text/plain\n\nIf one wanted to add a Content-Disposition header, should that be\n\n  Content-Disposition: xxx; filename=\"draft-ietf-http-v11-spec-08.txt.gz\"\n\nor\n\n  Content-Disposition: xxx; filename=\"draft-ietf-http-v11-spec-08.txt\"\n\nCurrent usage (typically larger files, which will be saved in\ncompressed form) suggests the first alternative.  If Content-Encoding\nbecomes more of an on-the-fly thing, the second alternative makes more\nsense - especially if proxies can change the coding.  (In the second case\na client wishing to save to disk in compressed form would probably append\nthe \".gz\" suffix to the suggested name, as required or \"natural\" for the\nplatform.)\n\nSince this is a HTTP specific question which doesn't occur for mail,\nthe HTTP spec should probably say something about it.\nIt may become an impediment to implementing Content-Encoding in clients,\nif different implementations choose different answers.  (for all I know\nthere may be today more widespread support for Content-Disposition than\nfor Content-Encoding in user agents.)\n\n   Klaus\n\n\n\n"
        },
        {
            "subject": "spec-08EBNF analysi",
            "content": "I've checked the EBNF in draft-ietf-http-v11-spec-08.txt for formal syntax and \ncontext errors. Here is the result:\n\n1) Some formal errors which were introduced by line formatting. They are \n   probably correct in the original (unformatted) document, aren't they?\n   (see context-diff below)\n\n2) in 'expect-params',  'Set-Proxy' and 'character':\n   Some forgotten '\"', '<>' and the like. (see context-diff below)\n\n3> An omitted ')' in 'Accept-Charset' (see below)\n\n4) Mispellings: (see below) \n      in 'chunk-extension': 'chunk-ext-value' instead of 'chunk-ext-val'\n      in 'codings': 'content-codings' instead of 'content-codings'\n      in 'Allow' and 'Public': 'method' instead of 'Method'\n      'Connection-header' is declared instead of 'Connection' \n      \n5) The following identifier are not declared at all:\n      in 'lifetime' : 'integer'\n      in 'keepalive-param' : 'param-name'\n\n6) This identifier is declared in another document. That is mentioned in the \n   text: \n      in 'From' : 'mailbox' (RFC 822 / RFC 1123)\n\nBye,\n\nJacob\n\n============================= draft-ietf-http-v11-spec-08.diff ============== \n--- draft-ietf-http-v11-spec-08.txtSat Aug  2 15:56:34 1997\n+++ draft-ietf-http-v11-spec-08b1.txtSat Aug  2 15:56:27 1997\n@@ -1582,7 +1582,7 @@\n \n \n                    chunk-extension= *( \";\" chunk-ext-name [ \"=\" \n-                                    chunk-ext-value ] )\n+                                    chunk-ext-val ] )\n                    chunk-ext-name = token\n                    chunk-ext-val  = token | quoted-string\n                    chunk-data     = chunk-size(OCTET)\n@@ -2502,8 +2502,7 @@\n                                   | \"200\"   ; OK\n                                   | \"201\"   ; Created\n                                   | \"202\"   ; Accepted\n-                                  | \"203\"   ; Non-Authoritative\n-            Information\n+                                  | \"203\"   ; Non-Authoritative Information\n \n             Fielding, et al                                    [Page 43]\n \n@@ -4404,8 +4403,7 @@\n             INTERNET-DRAFT            HTTP/1.1  Wednesday, July 30, 1997\n \n \n-                   challenge      = auth-scheme 1*SP realm *( \",\" auth-\n-            param )\n+                   challenge      = auth-scheme 1*SP realm *( \",\" auth-param )\n \n                    realm          = \"realm\" \"=\" realm-value\n                    realm-value    = quoted-string\n@@ -6389,8 +6387,7 @@\n             user agents.\n \n                    Accept-Charset = \"Accept-Charset\" \":\"\n-                             1#( ( charset | \"*\" [ \";\" \"q\" \"=\" qvalue ]\n-            )\n+                             1#( ( charset | \"*\" ) [ \";\" \"q\" \"=\" qvalue ] )\n \n             Character set values are described in section 3.4. Each\n             charset may be given an associated quality value which\n@@ -6439,7 +6436,7 @@\n                    Accept-Encoding  = \"Accept-Encoding\" \":\"\n                                     1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n \n-                   codings          = ( content-codings | \"*\" )\n+                   codings          = ( content-coding | \"*\" )\n \n             Examples of its use are:\n \n@@ -6658,7 +6655,7 @@\n             INTERNET-DRAFT            HTTP/1.1  Wednesday, July 30, 1997\n \n \n-                   Allow          = \"Allow\" \":\" 1#method\n+                   Allow          = \"Allow\" \":\" 1#Method\n \n             Example of use:\n \n@@ -6762,8 +6759,7 @@\n             all recipients along the request/response chain. It is not\n             possible to specify a cache-directive for a specific cache.\n \n-                   Cache-Control   = \"Cache-Control\" \":\" 1#cache-\n-                  directive\n+                   Cache-Control   = \"Cache-Control\" \":\" 1#cache-directive\n \n                    cache-directive = cache-request-directive\n                                    | cache-response-directive\n@@ -7286,7 +7282,7 @@\n \n             The Connection header has the following grammar:\n \n-               Connection-header = \"Connection\" \":\" 1#(connection-token)\n+               Connection = \"Connection\" \":\" 1#(connection-token)\n                connection-token  = token\n \n             HTTP/1.1 proxies MUST parse the Connection header field\n@@ -7972,8 +7968,7 @@\n             root \"/\" URL of a server for multiple host names on a single\n             IP address.\n \n-                   Host = \"Host\" \":\" host [ \":\" port ]    ; Section\n-            3.2.2\n+                   Host = \"Host\" \":\" host [ \":\" port ]    ; Section 3.2.2\n \n             A \"host\" without any trailing port information implies the\n             default port for the service requested (e.g., \"80\" for an\n@@ -8166,8 +8161,7 @@\n             As a special case, the value \"*\" matches any current entity\n             of the resource.\n \n-                   If-None-Match = \"If-None-Match\" \":\" ( \"*\" | 1#entity-\n-            tag )\n+                   If-None-Match = \"If-None-Match\" \":\" ( \"*\" | 1#entity-tag )\n \n             If any of the entity tags match the entity tag of the entity\n             that would have been returned in the response to a similar\n@@ -8524,7 +8518,7 @@\n             field (section 14.7) MAY be used to indicate methods allowed\n             for a particular URI.\n \n-                   Public         = \"Public\" \":\" 1#method\n+                   Public         = \"Public\" \":\" 1#Method\n \n             Example of use:\n \n@@ -9266,7 +9260,7 @@\n                   expectation-extension =  token [ \"=\" \n                               ( token | quoted-string ) *expect-params ]\n \n-                  expect-params =  \";\" token [ = ( token | quoted-string ) ]\n+                  expect-params =  \";\" token [ \"=\" ( token | quoted-string ) ]\n \n             Fielding, et al                                   [Page 154]\n \n@@ -9321,10 +9315,10 @@\n             The Set-Proxy response-header  is used to carry information\n             to redirect a client to use a different proxy.\n \n-                  Set-Proxy: \"Set-Proxy\" \":\" action [ \";\" 1#parameters ]\n+                  Set-Proxy= \"Set-Proxy\" \":\" action [ \";\" 1#parameters ]\n \n                   parameters   = ( \"scope\" \"=\" scopePattern ) |\n-                                 ( proxyURI \"=\" URI ) | lifetime\n+                                 ( \"proxyURI\" \"=\" URI ) | lifetime\n \n                   lifetime     = ( \"seconds\"  \"=\" integer )\n                                  | ( \"hits\"      \"=\" integer )\n@@ -9341,8 +9335,8 @@\n \n                   URIpattern   = character | \"*\"\n \n-                  character    = Any character legal in the definition\n-                                 of a URL/URI in the context of RFC2068\n+                  character    = <Any character legal in the definition\n+                                 of a URL/URI in the context of RFC2068>\n \n             An example header:\n\n=============================================================================\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-state-man-mec03.txt,.p",
            "content": "Title: HTTP State Management Mechanism (Rev1)\nAuthor(s): D. Kristol, L. Montulli\nFilename: draft-ietf-http-state-man-mec-03.txt,.ps\nPages: 22\nDate: 1997-08-01\n\nThis document specifies a way to create a stateful session with HTTP\nrequests and responses.  It describes two new headers, Cookie and Set-\nCookie2, which carry state information between participating origin\nservers and user agents.  The method described here differs from\nNetscape's Cookie proposal, but it can interoperate with HTTP/1.0 user\nagents that use Netscape's method.  (See the HISTORICAL section.)\n \nThis document reflects implementation experience with RFC 2109 and\nobsoletes it.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-state-man-mec-03.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-state-man-mec-03.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-state-man-mec-03.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-state-man-mec-03.txt\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "In a previous episode Klaus Weide said...\n\n\n:: <http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-08.txt.gz>.\n\n:: This is currently being served with headers (among others)\n:: \n::   Content-Encoding: gzip\n::   Content-Type: text/plain\n:: \n:: If one wanted to add a Content-Disposition header, should that be\n:: \n::   Content-Disposition: xxx; filename=\"draft-ietf-http-v11-spec-08.txt.gz\"\n:: \n:: or\n:: \n::   Content-Disposition: xxx; filename=\"draft-ietf-http-v11-spec-08.txt\"\n\n\nI'd argue strongly that the second one is the right way, though the\n19.6.1 definition of Content-Disposition doesn't help much. If it\nisn't the second one, than there is no way (well, cd is just a hint,\nbut you get the idea) to save an unencoded version to disk that was\ntransferred encoded.. a typical reason for saving the resource to disk\nis for interoperations with other software on the client machine, and\nthen if you've got no other local software that knows how to the\ndecoding, you're SOL.. that's going to be a problem for platforms like\nwindows and mac where the web UA may be the only thing going that\nunderstands 'deflate' on a typical installation..\n\neven using the second option as the right way doesn't prevent the UA from\nsaving a compressed version for an entity that was transferred encoded\n(a checkbox in the save-as dialog indicating that would be a good\nimplementation in my mind..)\n\n\n:: Current usage (typically larger files, which will be saved in\n:: compressed form) suggests the first alternative.  If\n\nfwiw - netscape for unix implements it's save-as for the above\nresource as plain text. (the second option, though it isn't driven by\na content-disposition: header)\n\n-P\n\n\n\n"
        },
        {
            "subject": "multipart/byterange",
            "content": "A question has come up, is it legal for a server to send a\nmultipart/byteranges with a single range? This apparently causes some\nvery ugly code on the client side if it is legal. The following is an\nexcerpt from section 19.2 of RFC 2068:\n\n19.2 Internet Media Type multipart/byteranges\n\n   When an HTTP message includes the content of multiple ranges (for\n   example, a response to a request for multiple non-overlapping\n   ranges), these are transmitted as a multipart MIME message. The\n   multipart media type for this purpose is called\n   \"multipart/byteranges\".\n\nMy reading of this section is that a multipart/byteranges can only be\nsent if there are byte rangeS but there has still been some question.\n\n1) Is it correct that multipart/byteranges can only be used if there are\nmultiple, as in more than one, byteranges?\n2) Either way, can we put in a sentence to clarify the matter?\n\nThanks,\nYaron\n\n\n\n"
        },
        {
            "subject": "Re: multipart/byterange",
            "content": "    A question has come up, is it legal for a server to send a\n    multipart/byteranges with a single range? This apparently causes some\n    very ugly code on the client side if it is legal. The following is an\n    excerpt from section 19.2 of RFC 2068:\n    \n    19.2 Internet Media Type multipart/byteranges\n    \n       When an HTTP message includes the content of multiple ranges (for\n       example, a response to a request for multiple non-overlapping\n       ranges), these are transmitted as a multipart MIME message. The\n       multipart media type for this purpose is called\n       \"multipart/byteranges\".\n    \n    My reading of this section is that a multipart/byteranges can only be\n    sent if there are byte rangeS but there has still been some question.\n    \n    1) Is it correct that multipart/byteranges can only be used if there are\n    multiple, as in more than one, byteranges?\n\n    2) Either way, can we put in a sentence to clarify the matter?\n\nI believe that I wrote this section, and it certainly never occurred\nto me that handling a \"multipart/byteranges\" with only one part would\ncause implementation difficulties.  After all, if the client is\nprepared to deal with N parts in a multipart type, what makes N = 1\nharder than, say, N = 2 or N = 13?\n\nCan you perhaps explain a bit more about why this makes the client\ncode so ugly?  I'm not strongly opposed to changing the HTTP spec to\nprohibit 1-part \"multipart/byteranges\" values, but I think doing\nso would add complexity to the spec, and isn't really consistent\nwith the robustness principle.  Also, I'm certainly not a MIME\nexpert, and I don't know if MIME has a rule that either explicitly\nrequires or prohibits N = 1 in multipart types.\n\nHave other client implementors found this hard? easy? didn't realize\nuntil now that it might be a real problem?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: agenda for Munic",
            "content": "Like David Morris said, I thought that work was going on elsewhere.\n(though our server does provide SNMP GET on the MIB)\n\n** Reply to note from Larry Masinter <masinter@parc.xerox.com> Fri, 1 Aug 1997 15:53:29 PDT\n>   \n> It's the responsibility of groups in the applications area\n> to ensure that systems deployed using the protocols they've\n> promoted are managable. So I'd like a status report on\n> HTTP server network management, and I put it on the agenda.\n> Maybe there's nothing there to say. Is there?\n>   \n> Larry\n> -- \n> http://www.parc.xerox.com/masinter\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: multipart/byterange",
            "content": "On Mon, 4 Aug 1997, Jeffrey Mogul wrote:\n\n> I believe that I wrote this section, and it certainly never occurred\n> to me that handling a \"multipart/byteranges\" with only one part would\n> cause implementation difficulties.  After all, if the client is\n> prepared to deal with N parts in a multipart type, what makes N = 1\n> harder than, say, N = 2 or N = 13?\n\nIn this case, I think the client determines if it will EVER ask for more\nthan 1 range in a single request, doesn't it?\n\nAs I recall, multipart is more complex to handle than singlepart (which\nwould simply be the content?) so I expect that a client could decide\nto never request > 1 range and hence not need to support multipart at all\n(that would likely be my first choice for the kinds of simple clients\nI've written.) From what I've read of the motivation for byte ranges,\nI would expect the majority of usage to be a single range per request.\nPipelining etc. makes breaking multiple ranges into multiple requests\nfairly minimal from a performance perspective.\n\nSo assuming my memory is correct and historical byte ranges allow\nmultipart to be avoided for a single range, I would favor restricting the\nuse of multipart to N > 1.  Keep simple requests simple.\n\nYaron will have to comment on whether his case matches mine, if not there\nare two reasons...\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: multipart/byterange",
            "content": "David has absolutely hit the nail on the head. \n\nWe don't handle multipart/byteranges. We NEVER ask for more than one\nrange. Having to put in a parser for multipart/byteranges into the level\nof the stack which handles generic HTTP (in our case that would be\nWinInet) would be extremely difficult. That level in the stack doesn't\ndo the sort of heavyweight parsing needed for multipart. It is really\ndesigned for quick and dirty parsing on the level of \"Identify headers\nand body, return.\"\n\nGiven that others are in the same situation it would seem reasonable to\nput in language requiring that multipart/byteranges not be used if a\nsingle range is being returned.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Monday, August 04, 1997 11:36 AM\n> To:Jeffrey Mogul\n> Cc:Yaron Goland; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; Rajeev\n> Dujari; http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: multipart/byteranges \n> \n> \n> \n> On Mon, 4 Aug 1997, Jeffrey Mogul wrote:\n> \n> > I believe that I wrote this section, and it certainly never occurred\n> > to me that handling a \"multipart/byteranges\" with only one part\n> would\n> > cause implementation difficulties.  After all, if the client is\n> > prepared to deal with N parts in a multipart type, what makes N = 1\n> > harder than, say, N = 2 or N = 13?\n> \n> In this case, I think the client determines if it will EVER ask for\n> more\n> than 1 range in a single request, doesn't it?\n> \n> As I recall, multipart is more complex to handle than singlepart\n> (which\n> would simply be the content?) so I expect that a client could decide\n> to never request > 1 range and hence not need to support multipart at\n> all\n> (that would likely be my first choice for the kinds of simple clients\n> I've written.) From what I've read of the motivation for byte ranges,\n> I would expect the majority of usage to be a single range per request.\n> Pipelining etc. makes breaking multiple ranges into multiple requests\n> fairly minimal from a performance perspective.\n> \n> So assuming my memory is correct and historical byte ranges allow\n> multipart to be avoided for a single range, I would favor restricting\n> the\n> use of multipart to N > 1.  Keep simple requests simple.\n> \n> Yaron will have to comment on whether his case matches mine, if not\n> there\n> are two reasons...\n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "Re: multipart/byterange",
            "content": "    We don't handle multipart/byteranges. We NEVER ask for more than\n    one range. Having to put in a parser for multipart/byteranges into\n    the level of the stack which handles generic HTTP (in our case that\n    would be WinInet) would be extremely difficult. That level in the\n    stack doesn't do the sort of heavyweight parsing needed for\n    multipart. It is really designed for quick and dirty parsing on the\n    level of \"Identify headers and body, return.\"\n\nAha, I misunderstood your question.  Certainly if a client only\nmakes requests for single contiguous ranges, it shouldn't have\nto be able to parse multipart/byteranges responses.\n\nI misunderstood your question because I thought you were talking\nabout a case where the server might coaelesce two requested\n(and overlapping) ranges into a one-part \"multipart\" response.\n\n    Given that others are in the same situation it would seem\n    reasonable to put in language requiring that multipart/byteranges\n    not be used if a single range is being returned.\n    \nThe language is already there, although in a different part of the\nspec (quoting from RFC2068, not draft -08, which I don't have handy):\n\n   4.4 Message Length:\n\n   When a message-body is included with a message, the length of that\n   body is determined by one of the following (in order of precedence):\n\n   [...]\n\n   4. If the message uses the media type \"multipart/byteranges\", which is\n     self-delimiting, then that defines the length. This media type MUST\n     NOT be used unless the sender knows that the recipient can parse it;\n     the presence in a request of a Range header with multiple byte-range\n     specifiers implies that the client can parse multipart/byteranges\n     responses.\n\nOne could argue that this \"MUST\" ought to be more obvious (although\nI found it quickly using a text-search on \"multipart/byteranges\").\nBut I think this is exactly what you want, right?\n\nI.e., what matters to you is NOT that a multipart/byteranges have\nmore than one subrange (since one could, in principle, break up\na single range into multiple contiguous subranges), but that the\nserver never send any multipart/byteranges responses to a client\nthat isn't prepared to parse them.  Right?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "RE: multipart/byterange",
            "content": "I would just say \"YES\" but I'm not sure anyone would know what I'm\nsaying yes to. =)\n\nHowever Jeff, you understand the situation completely. All I am asking\nfor is a clearer statement of the painfully obvious.\n\nThanks,\nYaron\n\n> -----Original Message-----\n> From:Jeffrey Mogul [SMTP:mogul@pa.dec.com]\n> Sent:Monday, August 04, 1997 2:48 PM\n> To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: multipart/byteranges \n> \n>     We don't handle multipart/byteranges. We NEVER ask for more than\n>     one range. Having to put in a parser for multipart/byteranges into\n>     the level of the stack which handles generic HTTP (in our case\n> that\n>     would be WinInet) would be extremely difficult. That level in the\n>     stack doesn't do the sort of heavyweight parsing needed for\n>     multipart. It is really designed for quick and dirty parsing on\n> the\n>     level of \"Identify headers and body, return.\"\n> \n> Aha, I misunderstood your question.  Certainly if a client only\n> makes requests for single contiguous ranges, it shouldn't have\n> to be able to parse multipart/byteranges responses.\n> \n> I misunderstood your question because I thought you were talking\n> about a case where the server might coaelesce two requested\n> (and overlapping) ranges into a one-part \"multipart\" response.\n> \n>     Given that others are in the same situation it would seem\n>     reasonable to put in language requiring that multipart/byteranges\n>     not be used if a single range is being returned.\n>     \n> The language is already there, although in a different part of the\n> spec (quoting from RFC2068, not draft -08, which I don't have handy):\n> \n>    4.4 Message Length:\n> \n>    When a message-body is included with a message, the length of that\n>    body is determined by one of the following (in order of\n> precedence):\n> \n>    [...]\n> \n>    4. If the message uses the media type \"multipart/byteranges\", which\n> is\n>      self-delimiting, then that defines the length. This media type\n> MUST\n>      NOT be used unless the sender knows that the recipient can parse\n> it;\n>      the presence in a request of a Range header with multiple\n> byte-range\n>      specifiers implies that the client can parse multipart/byteranges\n>      responses.\n> \n> One could argue that this \"MUST\" ought to be more obvious (although\n> I found it quickly using a text-search on \"multipart/byteranges\").\n> But I think this is exactly what you want, right?\n> \n> I.e., what matters to you is NOT that a multipart/byteranges have\n> more than one subrange (since one could, in principle, break up\n> a single range into multiple contiguous subranges), but that the\n> server never send any multipart/byteranges responses to a client\n> that isn't prepared to parse them.  Right?\n> \n> -Jeff\n\n\n\n"
        },
        {
            "subject": "RE: Using Content-Encoding and ContentDisposition togethe",
            "content": "I also agree that the second one is correct. We should not confuse\ncontent-encoding with the actual file. After all I could easily be\ndealing with a server where I, a server side app, send the server a\nplain text file with a content-disposition of .txt and the smart server\nknowing it is talking to a UA that supports compression decides to\ncompress on the fly.\nYaron\n\n> -----Original Message-----\n> From:Patrick McManus [SMTP:mcmanus@AppliedTheory.com]\n> Sent:Saturday, August 02, 1997 11:12 AM\n> To:kweide@tezcat.com\n> Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject:Re: Using Content-Encoding and Content-Disposition\n> together\n> \n> In a previous episode Klaus Weide said...\n> \n> \n> ::\n> <http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-08.txt.\n> gz>.\n> \n> :: This is currently being served with headers (among others)\n> :: \n> ::   Content-Encoding: gzip\n> ::   Content-Type: text/plain\n> :: \n> :: If one wanted to add a Content-Disposition header, should that be\n> :: \n> ::   Content-Disposition: xxx;\n> filename=\"draft-ietf-http-v11-spec-08.txt.gz\"\n> :: \n> :: or\n> :: \n> ::   Content-Disposition: xxx;\n> filename=\"draft-ietf-http-v11-spec-08.txt\"\n> \n> \n> I'd argue strongly that the second one is the right way, though the\n> 19.6.1 definition of Content-Disposition doesn't help much. If it\n> isn't the second one, than there is no way (well, cd is just a hint,\n> but you get the idea) to save an unencoded version to disk that was\n> transferred encoded.. a typical reason for saving the resource to disk\n> is for interoperations with other software on the client machine, and\n> then if you've got no other local software that knows how to the\n> decoding, you're SOL.. that's going to be a problem for platforms like\n> windows and mac where the web UA may be the only thing going that\n> understands 'deflate' on a typical installation..\n> \n> even using the second option as the right way doesn't prevent the UA\n> from\n> saving a compressed version for an entity that was transferred encoded\n> (a checkbox in the save-as dialog indicating that would be a good\n> implementation in my mind..)\n> \n> \n> :: Current usage (typically larger files, which will be saved in\n> :: compressed form) suggests the first alternative.  If\n> \n> fwiw - netscape for unix implements it's save-as for the above\n> resource as plain text. (the second option, though it isn't driven by\n> a content-disposition: header)\n> \n> -P\n\n\n\n"
        },
        {
            "subject": "RE: Using Content-Encoding and ContentDisposition togethe",
            "content": "On Mon, 4 Aug 1997, Yaron Goland wrote:\n\n> I also agree that the second one is correct. We should not confuse\n> content-encoding with the actual file. After all I could easily be\n> dealing with a server where I, a server side app, send the server a\n> plain text file with a content-disposition of .txt and the smart server\n> knowing it is talking to a UA that supports compression decides to\n> compress on the fly.\n\nMe too ... because as an application which created a compressed file\nwanted it to stay compressed when the UA received it.  I recently \nreceived a file of xx.txt.gz ... had the UA ungzip it to show it and\nthen save it as xx.txt.gz but uncompressed.\n\nBut I'm not sure that the right solution isn't a new set of transfer\nencodings or something similar.  It seems to me that 'content-' headers\nmostly deal with describing the logical content and the confusion here\nis because we are describing both logical content and transfer encoding.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "RE: multipart/byterange",
            "content": "On Mon, 4 Aug 1997, Yaron Goland wrote:\n\n> I would just say \"YES\" but I'm not sure anyone would know what I'm\n> saying yes to. =)\n\nI will just say yes to second Yaron's affirmation of Jeff's response.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-options01.tx",
            "content": "Title: Specification of HTTP/1.1 OPTIONS messages\nAuthor(s): J. Mogul, S. Lawrence, J. Cohen\nFilename: draft-ietf-http-options-01.txt\nPages: 12\nDate: 1997-08-04\n\nRFC2068 defined a new OPTIONS method for HTTP/1.1.  The\n        purpose of OPTIONS is to allow a 'client to determine the\n        options and/or requirements associated with a resource, or\n        the capabilities of a server, without implying a resource\n        action or initiating a resource retrieval.'  However,\n        RFC2068 did not defined a specific syntax for using OPTIONS\n        to make such a determination.  This proposal clarifies the\n        original specification of OPTIONS, adds several new HTTP\n        message headers to provide syntactic support for OPTIONS,\n        and establishes new IANA registries to avoid namespace\n        conflicts.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-options-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-options-01.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-options-01.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-options-01.txt\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "David W. Morris:\n>\n>On Mon, 4 Aug 1997, Yaron Goland wrote:\n>\n>> I also agree that the second one is correct. We should not confuse\n>> content-encoding with the actual file. After all I could easily be\n>> dealing with a server where I, a server side app, send the server a\n>> plain text file with a content-disposition of .txt and the smart server\n>> knowing it is talking to a UA that supports compression decides to\n>> compress on the fly.\n>\n>Me too ... because as an application which created a compressed file\n>wanted it to stay compressed when the UA received it.  \n\nWell, this is not really a case where we have to agree on what the most\ncorrect way is.  19.6.1 documents current practice, it is not\nnormative, so if anything, it should give hints about what to do with\ncurrent browsers.\n\n>I recently \n>received a file of xx.txt.gz ... had the UA ungzip it to show it and\n>then save it as xx.txt.gz but uncompressed.\n\nI just sent gzipped content with\n\n Content-Type: text/html\n Content-Encoding: gzip\n Content-Disposition: attachment; filename=\"fname.html.gz\"\n\nto a Netscape 3.0.1 on Unix.  On saving, the save as filename box\nsuggested \"fname.html\", and the unencoded content got saved (the URL\nwas something without fname.html in it).\n\nNow for the fun part: I then sent the same gzipped html with\n\n Content-Type: text/html\n Content-Disposition: attachment; filename=\"fname.html.gz\"\n\nand now Netscape displayed and saved the unzipped version, as\nexpected, but it _also_ suggested a filename of \"fname.html\".\nApparently there is something in there which chops the .gz from a\nfilename no matter what.\n\nBased on this, I am very reluctant to add informational text to the\nspec about how you should use content-disposition and content-encoding\ntogether.\n\n>Dave Morris\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-digest-aa-rev00.tx",
            "content": "Title: An Extension to HTTP : Digest Access \n                          Authentication\nAuthor(s): J. Franks, A. Luotonen, P. Leach, J. Hostetler, \n                          P. Hallam-Baker\nFilename: draft-ietf-http-digest-aa-rev-00.txt\nPages: 18\nDate: 1997-07-30\n\nThe protocol referred to as 'HTTP/1.0' includes the specification for\n   a Basic Access Authentication scheme.  This scheme is not considered\n   to be a secure method of user authentication, as the user name and\n   password are passed over the network as clear text.  A specification\n   for a different authentication scheme is needed to address this\n   severe limitation.  This document provides specification for such a\n   scheme, referred to as 'Digest Access Authentication'.  Like Basic,\n   Digest access authentication verifies that both parties to a\n   communication know a shared secret (a password); unlike Basic, this\n   verification can be done without sending the password in the clear,\n   which is Basic's biggest weakness. As with most other authentication\n   protocols, the greatest sources of risks are usually found not in the\n   core protocol itself but in policies and procedures surrounding its\n   use. \n\n   This is the final draft of any document under this name.  Digest and\n   Basic Authentication from the HTTP/1.1 specification will be combined\n   and issued as a document titled 'Authentication in HTTP'.Our intent\n   is that RFC 2068 and RFC 2069 will go to draft standard as a pair of\n   documents, but with all authentication schemes (Digest and Basic)\n   documented together in a single place.  This transition has not yet\n   taken place.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-digest-aa-rev-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-digest-aa-rev-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-digest-aa-rev-00.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-digest-aa-rev-00.txt\n\n\n\n"
        },
        {
            "subject": "Agenda for Munic",
            "content": "We have two sessions (well, one two-hour and two one-hour\nsessions):\n8/11    930-1130 Garmisch\n8/12   1300-1400 Garmisch\n       1415-1515 Garmisch\n\nNote 8/14 1300-1500: HTTP-NG Requirements BOF\n\nHere is my current best guess as to times for various\nagenda topics. There are an enormous number of internet\ndrafts.\n\n8/11  9:30-11:30\n9:30-9:45:  review of agenda\n9:45-10:15: review of current draft, closed issues, conformance table\n     draft hasn't shown up on Internet drafts directory (!)\n     draft-ietf-http-digest-aa-rev-00.txt\n      contains digest updates\n    http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n      has the issue list\n\n     The following I-Ds are available:\n\n         draft-ietf-http-warning-00.txt\n         draft-ietf-http-options-01.txt\n\n draft-fielding-http-age-00.txt\n  draft-cohen-http-305-306-responses-00.txt\n         draft-mogul-http-revalidate-01.txt\n draft-holtman-http-wildcards-00.txt\n draft-holtman-http-safe-01.txt\n  draft-harada-http-xconnfrom-00.txt\n\n10:15-11:30: Review of open issues, new issues\n   (e.g., PUT-RANGE, REAUTHENTICATION-REQUESTED,\n    REDIRECTS, SAFE, CONTENT-ENCODING, RE-VERSION)\n\n8/12: 1300-1330\n    content negotiation\n         draft-ietf-http-negotiate-scenario-01.txt\n draft-ietf-http-negotiation-03.txt\n draft-ietf-http-rvsa-v10-02.txt\n draft-ietf-http-feature-reg-02.txt\n      1330-1400\n    PEP\n         draft-ietf-http-pep-04.txt\n      1415-1445\n         State Management\n         draft-ietf-http-state-man-mec-03.txt\n  draft-ietf-http-jaye-trust-state-00.txt\n      1445-1515\n         Working group schedule\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "Gentlemen,\n\nI support your goal of replacing the clear-text\npassword method in HTTP with something stronger, but I\nwonder about why you didn't consider something stronger.\nSeveral password-based protocols are known that\nare much better than the one described in this\ndocument:\n\n> Title: An Extension to HTTP : Digest Access \n>                           Authentication\n> Author(s): J. Franks, A. Luotonen, P. Leach, J. Hostetler, \n>                           P. Hallam-Baker\n>\n>The protocol referred to as 'HTTP/1.0' includes the specification for\n>   a Basic Access Authentication scheme.  This scheme is not considered\n>   to be a secure method of user authentication, as the user name and\n>   password are passed over the network as clear text.  A specification\n>   for a different authentication scheme is needed to address this\n>   severe limitation.  This document provides specification for such a\n>   scheme, referred to as 'Digest Access Authentication'.  Like Basic,\n>   Digest access authentication verifies that both parties to a\n>   communication know a shared secret (a password); unlike Basic, this\n>   verification can be done without sending the password in the clear,\n>   which is Basic's biggest weakness. ...\n\nSo far, the above introduction seems good.\n\n> ... As with most other authentication\n>   protocols, the greatest sources of risks are usually found not in the\n>   core protocol itself but in policies and procedures surrounding its\n>   use. \n\nHowever, this last sentence seems like an attempt to justify\nuse of a weak method, and as such it ignores the fact\nthat relative risks vary greatly in different situations.\nIt may very well be the case that the \"greatest sources of risk\"\n*are* in the core protocol as described, *and* can be\nprevented by an ammended proposal.\n\nOnly some of these risks are discussed in the document,\nand no mention is made of stronger protocols that can solve\nsuch problems.  This seems inappropriate for a document\nfocused on security issues.\nFYI, the site at <http://world.std.com/~dpj/> describes\nseveral modern password protocols.\n\n-- David\n\n\n\n"
        },
        {
            "subject": "Re: Agenda for Munic",
            "content": "On Wed, 6 Aug 1997, Larry Masinter wrote:\n\n> Here is my current best guess as to times for various\n> agenda topics. There are an enormous number of internet\n> drafts.\n\nI suggest that draft-duerst-query-i18n-01.txt, which has now\nfinally appeared on the anouncing list, be added to this list.\nI suggest that it is added after Koen's\ndraft-holtman-http-safe-01.txt because it deals with the\nsame topic of form upload and internationalization.\nNote that technically, the draft is more related to negotiation,\nand is intended to apply independently of HTTP/1.0 or 1.1.\n\nThere was originally a plan to have a separate BOF on this\ntopic, but it unfortunately had to be cancelled because\na lot of specific experts (in particular internationalization\nexperts from the browser side) were inable to attend.\n\nI therefore don't think it makes sense to allocate much time\nto it; two or three minutes would be nice. The main aim at\nthe moment is to have people to take notice of the problem\nand the proposed solution.\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "On Wed, 6 Aug 1997, David Jablon wrote:\n\n> Gentlemen,\n> \n> I support your goal of replacing the clear-text\n> password method in HTTP with something stronger, but I\n> wonder about why you didn't consider something stronger.\n> Several password-based protocols are known that\n> are much better than the one described in this\n> document:\n> \n\nTo quote from the draft:\n\n   \"Digest Authentication does not provide a strong authentication\n   mechanism.  That is not its intent.  It is intended solely to replace\n   a much weaker and even more dangerous authentication mechanism: Basic\n   Authentication.  An important design constraint is that the new\n   authentication scheme be free of patent and export restrictions.\"\n\nThe necessity to avoid any patent and export restrictions is\nfundamental.  In particular, protocols which make any use of\npublic-key techniques are not acceptable.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Agenda for Munic",
            "content": "> I therefore don't think it makes sense to allocate much time\n> to it; two or three minutes would be nice. The main aim at\n> the moment is to have people to take notice of the problem\n> and the proposed solution.\n\nThere are enough 'announcement of other topics' items that I'll\nmake time for them. However, the working group time is extremely\nlimited, and suggest that you attempt to schedule a BOF anyway\nto cover this and related I18N issues, which seem to cut across the\ntopics of HTTP, URLs, and HTML.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "At 09:12 AM 8/6/97 -0500, John Franks wrote:\n>On Wed, 6 Aug 1997, David Jablon wrote:\n>\n>> Gentlemen,\n>> \n>> I support your goal of replacing the clear-text\n>> password method in HTTP with something stronger, but I\n>> wonder about why you didn't consider something stronger.\n>> Several password-based protocols are known that\n>> are much better than the one described in this\n>> document:\n>> \n>\n>To quote from the draft:\n>\n>   \"Digest Authentication does not provide a strong authentication\n>   mechanism.  That is not its intent.  It is intended solely to replace\n>   a much weaker and even more dangerous authentication mechanism: Basic\n>   Authentication.  An important design constraint is that the new\n>   authentication scheme be free of patent and export restrictions.\"\n>\n>The necessity to avoid any patent and export restrictions is\n>fundamental.  In particular, protocols which make any use of\n>public-key techniques are not acceptable.\n\nWhy?\n\nAs I understand export regulations, no authentication-only method\nis export controlled.  As for patent restrictions, have you\nactually done an investigation into these?\n\nI'd like to better understand your concerns here, with regard\nto both patents and public-key techniques.  To rule out the entire\ncategory of public-key assisted methods seems extremely\nlimiting, and a clear rationale for such a fundamental\nrestriction is certainly missing from the draft.\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "John,\n\nI find it absurd to categorically preclude \"any use of\npublic-key techniques\" for password authentication in HTTP.\nIn light of the importance of HTTP, and the fact that\npasswords are widely-used on the web, I question the\nmotivation behind \"Digest Authentication\".\n\nWhy limit HTTP password verification to a broken method?\nIf you're going to the trouble to change it, why not\ndo it right?\n\nBut at least your explanation was more revealing of intent\nthan the draft.  It is enlightening that the authors of\nthe draft would choose to be more restrictive than\nthe U.S. government, which has never limited export of\nstrong password authentication.\n\nAt 09:12 AM 8/6/97 -0500, John Franks wrote:\n>To quote from the draft:\n>\n>   \"Digest Authentication does not provide a strong authentication\n>   mechanism.  That is not its intent.  It is intended solely to replace\n>   a much weaker and even more dangerous authentication mechanism: Basic\n>   Authentication.  An important design constraint is that the new\n>   authentication scheme be free of patent and export restrictions.\"\n>\n>The necessity to avoid any patent and export restrictions is\n>fundamental.  In particular, protocols which make any use of\n>public-key techniques are not acceptable.\n\nSo again, I ask:\n\nWhy impose these restrictions?\n\nThe authors have failed to investigate whether freely\navailable or cheaply available strong alternatives exist,\nso your excuse about \"fundamental restrictions\" just\ndoesn't cut it.\n\nI can only presume that the vendors behind this proposal\nwould rather support a weak password method than a strong\none, in line with an unwritten agenda.\n\nAs I wrote:\n\n>> Several password-based protocols are known that\n>> are much better than the one described in this\n>> document\n\nTo be specific, I can name EKE, SPEKE, \"secret public-key\"\ntechniques, OKE, SRP-2, and several others.  In the spirit of\nhonesty and openness, I'll do my part.  My motivation\nis in part due to the fact that I'm the author of one\nof these methods.\n\nIt is wrong for the authors of this document to dismiss\nthe entire class of strong techniques without adequate\nexplanation.  To do so without even acknowledging their\nexistence is a true public disservice.\n\n-- David\n\n------------------------------------\nDavid Jablon\nhttp://world.std.com/~dpj/\ndpj@world.std.com\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>[...]\n>Well, this is not really a case where we have to agree on what the most\n>correct way is.  19.6.1 documents current practice, it is not\n>normative, so if anything, it should give hints about what to do with\n>current browsers.\n\nThat section references only RFC 1806, which describes the\n\"attachment\" and \"inline\" disposition types.  My recollection of\na long-ago message from Lou is that Netscape based its implementation\non the file upload RFC's \"file\" disposition type.  What is the\nappropriate disposition type to use in HTTP Content-Disposition\nheaders and META elements, and can information about that be included\nin 19.6.1?\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "Please stop.\n\nThe specification does _not_ precluded other mechanisms for password\nauthentication for HTTP. Many web sites use stronger mechanisms for\nsecurity in HTTP, usually by using https with alternative methods of\nlogging in.\n\nAll we've done is set the minimum to be something that everyone can do,\neasily.\n\nThis is a topic that we addressed at length in the past. The topic\nis closed. There will be no further discussion in the HTTP working\ngroup of 'alternatives to digest authentication'.\n\nThere *was* another working group on HTTP security, and they\nalso closed. If you want to start work on this topic, on raising\nthe requirement for HTTP security to be a stronger authentication\nmechanism, I suggest you call for a BOF and try to get the interested\nimplementor community to come.\n\nIf you wish to suggest some editorial change to the wording of the\ndraft that goes beyond what is already there, then please make some\nspecific suggestions; however, check your conspiracy theories at the\ndoor.\n\nThanks,\n\nLarry\n(as chair, HTTP-WG)\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "David Jablon <dpj@world.std.com> writes:\n\n>At 09:12 AM 8/6/97 -0500, John Franks wrote:\n>>The necessity to avoid any patent and export restrictions is\n>>fundamental.  In particular, protocols which make any use of\n>>public-key techniques are not acceptable.\n>...\n>\n>I can only presume that the vendors behind this proposal\n>would rather support a weak password method than a strong\n>one, in line with an unwritten agenda.\n\nAssume what you like, but please note that the draft is the product of\nthis working group, not an individual, and that the first two authors\nlisted are from non-commercial organizations.  This working group\ncertainly has no unwritten agenda - we have enough trouble trying to\ndefine the written one!\n\n>To be specific, I can name EKE, SPEKE, \"secret public-key\"\n>techniques, OKE, SRP-2, and several others.  In the spirit of\n>honesty and openness, I'll do my part.  My motivation\n>is in part due to the fact that I'm the author of one\n>of these methods.\n\nAre you asserting, as an author of one of the above, that it is free of\npatent and other intellectual-property restrictions?  That's the\ncriterion that's been offered as to the rejection of public-key\ntechniques, not concerns about US export laws.\n\nI'm not a patent attorney, just a programmer whose interests lie partly\nin this area, but from what I've read it's essentially impossible to\nwork in public-key cryptography without running into a patent belonging\nto either RSA or Diffie.  If you've come up with something so\nfundamentally different that it doesn't infringe on them, and have\nchosen to share that technique with the rest of us without restriction,\nI thank you, and I expect that the working group would listen calmly and\nreasonably to whatever proposals you might have to offer.  Those of us\nwho've been following and participating in the debates to date have\ncertainly never maintained that Digest Authentication was the be-all and\nend-all, rather simply that it is better than Basic Authentication and\nfreely implementable and distributable.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "Larry Masinter <masinter@parc.xerox.com> writes:\n\n>This is a topic that we addressed at length in the past. The topic\n>is closed. There will be no further discussion in the HTTP working\n>group of 'alternatives to digest authentication'.\n\nI stand corrected.  Larry's the working group chair, so he's obviously\nright about the scope of our charter at this point.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n>koen@win.tue.nl (Koen Holtman) wrote:\n>>[...]\n>>Well, this is not really a case where we have to agree on what the most\n>>correct way is.  19.6.1 documents current practice, it is not\n>>normative, so if anything, it should give hints about what to do with\n>>current browsers.\n>\n>That section references only RFC 1806, which describes the\n>\"attachment\" and \"inline\" disposition types.  My recollection of\n>a long-ago message from Lou is that Netscape based its implementation\n>on the file upload RFC's \"file\" disposition type.  What is the\n>appropriate disposition type to use in HTTP Content-Disposition\n>headers and META elements, and can information about that be included\n>in 19.6.1?\n\nI tracked down Lou's message (appended) and was remembering\nit correctly.  So, how about some current practice guidance/hints\nabout that?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\nDate: Fri, 08 Nov 1996 11:05:40 -0800\nResent-from: http-wg@cuckoo.hpl.hp.com\nFrom: Lou Montulli <montulli@netscape.com>\nSubject: Re: HTTP header suggestion/request\n\nMegaZone wrote:\n> \n> I've noticed more and more software vendors using CGI applications to allow\n> users to download software.  Unfortuately at this time there isn't a way\n> to tell user-agents to save the code under a name other than the script name.\n> This problematic especially when there is more than one selection on a form.\n> \n> I would like to suggest a new header for HTTP/1.1:\n> Save-As:\n> \n> A CGI application could return this header with a file name (and possibly\n> path, although that has security implications - Save-As: /etc/passwd for\n> instance.  Though it would fail on a well maintained system, it is still a\n> risk) which would be used by the user agent instead of the script name for\n> saving.\n> \n> I would have a number of uses for this tag, and I've seen numerous sites\n> that have the same aplication.  I think this would be a very useful addition.\n> \n\nThere is a solution that should already serve your purpose.\n\nThe Content-disposition header can contain a \"filename\"\nparameter for naming a file.  When the Navigator sees this\nparameter it will use it as a default filename for saving.\n\nThe following use should work when returned from a CGI script:\n\nContent-disposition: file; filename=foo.exe\n\nThe Navigator only uses the filename parameter, everything\nelse in the header is currently ignored.\n\nThe Content-disposition header is also used in HTTP file upload\nand is documented in Larry Masinter's RFC on file upload.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "draft-moore-mime-cdisp-01.txt is intended to move 'content-disposition'\nonto standards track. Since HTTP-WG is debating this header and has some\nambiguities with its use in practice, it might be good to bring HTTP's\nusage in line with whatever the standard is, and vice versa.\n\n This is probably an 'issue' for both HTTP-WG and IESG\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "On Wed, 6 Aug 1997, Foteos Macrides wrote:\n> Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n> >koen@win.tue.nl (Koen Holtman) wrote:\n> >>[...]\n> >>Well, this is not really a case where we have to agree on what the most\n> >>correct way is.  19.6.1 documents current practice, it is not\n> >>normative, so if anything, it should give hints about what to do with\n> >>current browsers.\n> >\n> >That section references only RFC 1806, which describes the\n> >\"attachment\" and \"inline\" disposition types.  My recollection of\n> >a long-ago message from Lou is that Netscape based its implementation\n> >on the file upload RFC's \"file\" disposition type.  What is the\n> >appropriate disposition type to use in HTTP Content-Disposition\n> >headers and META elements, and can information about that be included\n> >in 19.6.1?\n> \n> I tracked down Lou's message (appended) and was remembering\n> it correctly.  So, how about some current practice guidance/hints\n> about that?\n> \n> Fote\n> \n> Date: Fri, 08 Nov 1996 11:05:40 -0800\n> Resent-from: http-wg@cuckoo.hpl.hp.com\n> From: Lou Montulli <montulli@netscape.com>\n> Subject: Re: HTTP header suggestion/request\n> \n[ ... ] \n> The following use should work when returned from a CGI script:\n> \n> Content-disposition: file; filename=foo.exe\n> \n> The Navigator only uses the filename parameter, everything\n> else in the header is currently ignored.\n[ ... ]\n\nRFC 1808 and also draft-moore-mime-cdisp-01.txt say this:\n\n   Disposition parameters are valid for all dispositions.  (In contrast\n   to [RFC 1521] content-type parameters, which are defined on a per-\n   content-type basis.) Thus, for example, the `filename' parameter\n   still means the name of the file to which the part should be written,\n   even if the disposition itself is unrecognized.\n\nSo it seems that Netscape's client follows the RFC in that regard (or did\nso, at the time of writing).\n\nUse of Content-Disposition in file uploads is a different topic, I think.\nIf a file upload uses this header, it is probably within a \nmultipart/form-data structure, and thereby out of scope for the HTTP\nprotocol.(?)\n\nBy the way, is Content-Disposition really a response-header?\nIt seems to make more sense to say it is an entity-header.\n\n        Klaus\n\n\n\n"
        },
        {
            "subject": "Administriva: httpwg mailing list and spammer",
            "content": "Folks,\n\nThere has been an increasing amount of spam sent to the http-wg mailing list.\nSeveral people on the list have asked why I allow spam to be sent.\n\nFirstly, it is difficult to distinguish between spam and legitimate postings.\nSecondly the http-wg mailing list is currently open and non-moderated.\n\nThere are several solutions:\n\n* We can just delete spam messages.\n\n* The list could be changed to a closed list where only people subscribed to\n  the list may post.\n\n* The list could be moderated.\n\n* Somebody could come up with a good way of determining spam, or alternatively \n  legitimate postings to the list.\n\nI'm open to all sensible suggestions.\n--\n(http-wg list owner)-- ange -- <><\n\nhttp://www-uk.hpl.hp.com/people/angeange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "On Aug 7, 12:27am, Klaus Weide wrote:\n> RFC 1808 and also draft-moore-mime-cdisp-01.txt say this:\n>\n>    Disposition parameters are valid for all dispositions.  (In contrast\n>    to [RFC 1521] content-type parameters, which are defined on a per-\n>    content-type basis.) Thus, for example, the `filename' parameter\n>    still means the name of the file to which the part should be written,\n>    even if the disposition itself is unrecognized.\n>\n> So it seems that Netscape's client follows the RFC in that regard (or did\n> so, at the time of writing).\n>\n> Use of Content-Disposition in file uploads is a different topic, I think.\n> If a file upload uses this header, it is probably within a\n> multipart/form-data structure, and thereby out of scope for the HTTP\n> protocol.(?)\n>\n> By the way, is Content-Disposition really a response-header?\n> It seems to make more sense to say it is an entity-header.\n>\n>         Klaus\n>\n>\n>-- End of excerpt from Klaus Weide\n\nIn past mail conversations with Netscape developers, Netscape (up to 3.01, I\nhaven't tried 4.0) only recognizes the Content-Disposition header if and only\nif the Content-Type is Octet-stream.  Otherwise the browser tries to make-up a\nfilename based on the basename of the current URL.\n\nAs for C-D in a multipart, I do not see where it is out of scope.  The headers\nare in an alternate delivery mechanism that is supported by by the community.\n\n\n\nKevin\n--\n=============================================================================\nKevin J. DyerDraper Laboratory  MS 23.\nEmail: <kdyer@draper.com>        555 Tech. Sq.\nPhone: 617-258-4962Cambridge, MA 02139\nFAX: 617-258-2121\n-----------------------------------------------------------------------------\n      \"If the women don't find you handsome, they should       Red\n       at least find you handy.\"                               Green\n=============================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Wed, 6 Aug 1997, Foteos Macrides wrote:\n>> Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n>> >koen@win.tue.nl (Koen Holtman) wrote:\n>> >>[...]\n>> >>Well, this is not really a case where we have to agree on what the most\n>> >>correct way is.  19.6.1 documents current practice, it is not\n>> >>normative, so if anything, it should give hints about what to do with\n>> >>current browsers.\n>> >\n>> >That section references only RFC 1806, which describes the\n>> >\"attachment\" and \"inline\" disposition types.  My recollection of\n>> >a long-ago message from Lou is that Netscape based its implementation\n>> >on the file upload RFC's \"file\" disposition type.  What is the\n>> >appropriate disposition type to use in HTTP Content-Disposition\n>> >headers and META elements, and can information about that be included\n>> >in 19.6.1?\n>> \n>> I tracked down Lou's message (appended) and was remembering\n>> it correctly.  So, how about some current practice guidance/hints\n>> about that?\n>>[...]\n>> From: Lou Montulli <montulli@netscape.com>\n>> Subject: Re: HTTP header suggestion/request\n>> \n>[ ... ] \n>> The following use should work when returned from a CGI script:\n>> \n>> Content-disposition: file; filename=foo.exe\n>> \n>> The Navigator only uses the filename parameter, everything\n>> else in the header is currently ignored.\n>[ ... ]\n>\n>RFC 1808 and also draft-moore-mime-cdisp-01.txt say this:\n>\n>   Disposition parameters are valid for all dispositions.  (In contrast\n>   to [RFC 1521] content-type parameters, which are defined on a per-\n>   content-type basis.) Thus, for example, the `filename' parameter\n>   still means the name of the file to which the part should be written,\n>   even if the disposition itself is unrecognized.\n>\n>So it seems that Netscape's client follows the RFC in that regard (or did\n>so, at the time of writing).\n>\n>Use of Content-Disposition in file uploads is a different topic, I think.\n>If a file upload uses this header, it is probably within a \n>multipart/form-data structure, and thereby out of scope for the HTTP\n>protocol.(?)\n\nThe point was that current RFC-defined disposition types are\n\"attachment\", \"inline\", and \"form-data\" (RFCs 1806 and 1867), and new,\nas yet undefined disposition types were anticipated in RFC 1806.  I\nhad (probably mis-)interpreted Lou's message to be suggesting \"file\"\nfor the then new use of Content-Disposition as an HTTP server reply\nheader to indicate a \"suggested output file name\" in cases of save to\ndisk or download operations being performed by the UA for the reply\nbody.  \"attachment\" is fine for that case, and a disposition type is\nincluded purely for integrity of the header's syntax.\n\n\n>By the way, is Content-Disposition really a response-header?\n>It seems to make more sense to say it is an entity-header.\n\nThe Netscape innovation of using a Content-Disposition HTTP\nreply header is very useful in the cases for which the last symbolic\nelement of the RequestURI is not a good basis upon which a UA can\nconcoct (sp?) a suggested output file name, e.g., because that element\nis a script name, or the last element of PATH_INFO for a script.\n\nI see no reason to restrict that functionality to replies\nwith a binary Content-Type, and it isn't restricted to that case\nin Lynx.\n\nI can't think of a reason for the UA to pay attention to\nanything more than the filename=value parameter in a Content-Disposition\nHTTP reply header.  Based on the Content-Type reply header, the\nContent-Encoding reply header if present, and the context in which\nthe UA sent the request, the UA has everything it needs to use or\n\"doctor\" the value appropriately and securely for the platform on\nwhich the UA is running, as it does otherwise for the last symbolic\nelement of the RequestURI.  That includes stripping or adding any\ncompression \"suffix tokens\" from the suggested output file name\nbased on whether or not the context of the request lead to an\nuncompression of a gzipped or Unix compressed reply.  The released\nversions of Lynx aren't doing such stripping or adding, but I added\nthat yesterday in a field test code set.\n\nDetails of an HTTP Content-Disposition reply header beyond\na filename=value parameter might become important someday, but not\nfor just setting a suggested (default) output file name which a user\ncan edit before actually using it.\n\ndraft-moore-mime-cdisp-01.txt is an update of RFC 1806 and\ndoes not appear be taking RFC 1867 (file upload) into account (does\nnot reference it, nor discuss the \"form-data\" disposition type), nor\nthe use discussed in Section 19.6.1 for HTTP/1.1.  The last-modified\nparameter in the cdisp draft might be relevant, but HTTP has its\nown explicit header for that.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: http-digest-aa-rev00.tx",
            "content": "Larry, Phillip, Paul, John, and other authors of the draft,\n\nI apologize for raising an old issue with the WG,\nand for implying that the work behind this was not\nwell considered.  I just looked at the draft as a\nstandalone document, and did not find clear support\nor explanation for the resulting design.\n\nThe combined responses of John, Paul, Phillip, and Larry\nprovide a much clearer reason for the design decisions,\nhowever I might disagree with the outcome.\nMy concern comes from the fact that the rationale\nfor restriction to this method is not captured in\nthe draft.\n\nSo, without further insult, or re-hashing of old issues,\nI'd suggest that the draft be amended slightly to\ninclude more of the careful considerations of the\nauthors.  (specific suggestions at end)\n\n\nFrank wrote:\n>The necessity to avoid any patent and export restrictions is\n>fundamental.  In particular, protocols which make any use of\n>public-key techniques are not acceptable.\n\nIn questioning this rationale (in obviously a\ntoo-nasty way) I stimulated the following responses:\n\nPaul wrote:\n>1. Digest Auth standardization was started a fair while ago, and even\n>then its intent was to standardize an existing practice. Many\n>potentially interesting changes were shot down because they weren't\n>backwards compatible.\n\nThis should be captured in the draft.  It definitely makes\nthe work sound more diligent.\n\nPaul:\n>2. The statements in the draft about weakness were made primarily to\n>appease certain parties who would not agree that any shared secret\n>authentication mechanism deserved to be called \"strong\", due to the\n>difficulty of distributing the secrets.\n\nI think including accurate factual statements of weakness is very\nimportant.  Opinions on proper forms of certification and\nkey distribution tend to be a little more controversial, and\nI'd avoid using \"strong\" and \"weak\" in this regard in such\ndocuments.\n\nPaul:\n>3. I, at least, believe there is a place for shared secret based\n>authentication. Many sites in the web to not use authentication on\n>anything except credit card pyment pages because Basic is stupid, and\n>SSL slows down there sites by orders of magnitude. Thus, we were\n>explicitlu after a shared secret mechanism, and hence, for the purpose\n>for which Digest was intended, it was not our responsibility to explore\n>public key alternatives.\n\n... and Phillip:\n>I can assure David that those involved have a great deal of knowledge of \n>and expertise in the use of public key encryption techniques. Even if there\n>were absolutely no patent restrictions I would still want to have a shared\n>secret scheme available. Without special hardware a public key operation\n>takes several hundred milliseconds on comparatively upscale hardware\n>platforms. It is simply not acceptable that the only secure authentication\n>mechanism restrict such servers to one or two hits per second.\n\nThe speed requirement should be in the draft.\n\nPaul:\n>4. At the time we were writing the spec, we knew of no public key based\n>mechanism that was appropriately unencumbered by patent restrictions,\n>and in all the time the spec has been out, no one until now has\n>suggested one to us.\n>To simply blatantly claim we were derelict in our duty is boorish.\n\nOk.  Sorry I implied you guys were neglectful, and I hope\nyou'll not hold my rudeness against me.  I probably wouldn't\nhave said what I said if the draft contained a full rationale.\nIt seems important to capture careful reasoning in\nprint to insure that future readers also don't jump to\nconclusions.\n\nLarry wrote:\n> If you wish to suggest some editorial change to the wording of the\n> draft that goes beyond what is already there, then please make some\n> specific suggestions; however, check your conspiracy theories at the\n> door.\n\nOk.  But \"conspiracy theories\"?  My statement was simply that\nthe draft had a \"hidden agenda\", which is true, for readers\nwho didn't follow the working group from the start.  Frank, Paul,\nand Philip have made the agenda clear in their responses.\nI think the draft should incorporate their points.\n\nSpecific suggestions\n--------------------\nThe method was chosen for a variety of reasons, as discussed\nin the draft, but the discussion should also include at least\none paragraph that incorporates the following reasons\n(restated appropriately by the authors):\n\n1) to be compatibile with and standardize on existing practice.\n\n2) because implementations of public-key based methods\nwere found to be too slow.\n\n3) because no unpatented public-key alternatives were known\nto the authors, and\n\n4) because patent licensing was out of the question,\nbecause of (...)\n\n5) to avoid export regulations (specifically ...)\n\nAlso, there is discussion about brute-force attack on the\nencrypted password database, but a potentially larger\nthreat is an eavesdropper performing a brute-force attack\non the challenge and response messages.  The document\nshould describe this weakness too.\n\nThanks.\n\n------------------------------------\nDavid Jablon\nhttp://world.std.com/~dpj/\ndpj@world.std.com\n\n\n\n"
        },
        {
            "subject": "ContentDisposition suggested text change",
            "content": "On Tue, 5 Aug 1997, Koen Holtman wrote:\n\n> David W. Morris:\n> >\n> >On Mon, 4 Aug 1997, Yaron Goland wrote:\n> >\n> >> I also agree that the second one is correct. We should not confuse\n> >> content-encoding with the actual file. After all I could easily be\n> >> dealing with a server where I, a server side app, send the server a\n> >> plain text file with a content-disposition of .txt and the smart server\n> >> knowing it is talking to a UA that supports compression decides to\n> >> compress on the fly.\n> >\n> >Me too ... because as an application which created a compressed file\n> >wanted it to stay compressed when the UA received it.  \n> \n> Well, this is not really a case where we have to agree on what the most\n> correct way is.  19.6.1 documents current practice, it is not\n> normative, so if anything, it should give hints about what to do with\n> current browsers.\n> \n> >I recently \n> >received a file of xx.txt.gz ... had the UA ungzip it to show it and\n> >then save it as xx.txt.gz but uncompressed.\n> \n> I just sent gzipped content with\n> \n>  Content-Type: text/html\n>  Content-Encoding: gzip\n>  Content-Disposition: attachment; filename=\"fname.html.gz\"\n> \n> to a Netscape 3.0.1 on Unix.  On saving, the save as filename box\n> suggested \"fname.html\", and the unencoded content got saved (the URL\n> was something without fname.html in it).\n> \n> Now for the fun part: I then sent the same gzipped html with\n> \n>  Content-Type: text/html\n>  Content-Disposition: attachment; filename=\"fname.html.gz\"\n> \n> and now Netscape displayed and saved the unzipped version, as\n                                           ^^^^^^^^\n(I think you mean \"un-gunzipped\" here )\n> expected, but it _also_ suggested a filename of \"fname.html\".\n> Apparently there is something in there which chops the .gz from a\n> filename no matter what.\n> \n> Based on this, I am very reluctant to add informational text to the\n> spec about how you should use content-disposition and content-encoding\n> together.\n> \n> Koen.\n\nI propose the following changes.  Maybe they can accomodate your\nreservations?\n\nReplace (last paragraph before 19.6.1):\n            A number of other headers, such as Content-Disposition and\n            Title, from SMTP and MIME are also often implemented (see\n            RFC 2076 [37]).\n\nwith:\n            A number of other headers, such as Message-Id and \n            Content-Disposition, from Mail and MIME are also often \n            implemented (see RFC 2076 [37]).\n\n[Title is not a header from Mail or MIME; also, SMTP does not define \nheaders.]\n\nReplace (first paragraph of 19.6.1):\n            The Content-Disposition response-header field has been\n            proposed as a means for the origin server to suggest a\n            default filename if the user requests that the content is\n            saved to a file.  This usage is derived from the definition\n            of Content-Disposition in RFC 1806 [35].\n\nwith:\n            The Content-Disposition entity-header field in a response is\n            understood by some user agents as a means for the \n            origin server to suggest a default filename if the user \n            requests that the content is saved to a file.  This usage is \n            derived from the definition of Content-Disposition in \n            RFC 1806 [35].\n\n[ This leaves it open how C-D would be used in a request ]\n\nNear the end of 19.6.1, replace:\n            If this header is used in a response with the\n            application/octet-stream content-type, the implied\n            suggestion is that the user agent should not display the\n            response, but directly enter a `save response as..'  dialog.\n \nwith:\n            If this header is used in a response with the\n            application/octet-stream content-type, the implied\n            suggestion is that the user agent should enter a \n            `save response as..' dialog, offering the suggested filename\n            (after possible modification) as default.\n\n[ The previous formulation gives the impression that it is normal\nbehavior to \"display\" application/octet-stream content, and also that\nin absense of the C-D header a dialog is entered more \"indirectly\"\n(whatever that means).]\n\nFinally, add the following at the end:\n\n            According to RFC 1806, \"the suggested filename should be used\n            as a basis for the actual filename, where possible\", but it\n            may need modification for reasons of security, filesystem\n            restrictions, and local conventions.\n\n    If a response message also has a Content-Encoding\n            entity-header, the filename parameter should suggest an\n            appropriate name for the data obtained after removing the\n            encoding(s) from the entity-body.\n    On systems with a convention for naming files in a given\n    encoding, for example a \".gz\" filename suffix for the\n    gzip content-coding, user agents should add such a suffix \n            (if not already present) if the content is saved in\n            encoded form;  some user agents also remove such a suffix \n            from the suggested filename if it is present, if they choose\n            to save the content in decoded form.\n\n\n   Klaus\n\n\n\n"
        },
        {
            "subject": "Comments on OPTIONS draf",
            "content": "Below are my comments on draft-ietf-http-options-01.txt.\n\n1. As currently specified, the \"RFC\" compliance token is ambiguous.  For \nexample, if a client discovers that an HTTP/1.1 server \"unconditionally\" \nsupports RFC 2068, what can they assume from this?  I assume this means \nthat all MUST behavior is supported, but what about MAY and SHOULD \nbehavior?  Does \"unconditional\" mean that all features in Appendix 19.6 of \nRFC 2068 are supported (PATCH, LINK, UNLINK), or does \"unconditional\" apply \nonly to the mainstream methods?  As for headers, if a server \nunconditionally supports RFC 2068, does this mean the server supports the \nContent-MD5 header (which is \"MAY\" functionality)?\n\nI suspect that each RFC will have issues similar to these.  What would be \nhelpful is to provide a document which describes *exactly* what it means to \nhave unconditional compliance with each RFC, and to have this information \nbe part of the registry maintained by IANA, much as each MIME type has an \nassociated description of that type.\n\n2. Similarly, the meaning of the \"cond\" option-param is completely \nambiguous.  As near as I can tell, if a client receives a Compliance header \nwhich states \"rfc=2068;cond\" the client can assume absolutely nothing about \nthe server's behavior beyond what is stated in the Public header, or in HDR \ncompliance tokens.\n\nI think the token space for option-params also needs to be controlled by \nIANA, and each token needs to have a description of what that token means. \n In the WebDAV working group, it is likely that locking capability will be \noptional (MAY) capability, but will still be widely supported.  I'd like to \nhave the option of defining an option-param to indicate that locking is (or \nis not) supported, for example, \"rfc=nnnn;cond;nolock\"\n\n3. It is possible to express contradictory information between the Public \nand the Compliance header. For example, the Public header could list all \nRFC 2068 methods except PUT, and the Compliance header could state \"rfc=  \n2068;uncond\".  I think there should be a sentence indicating that this is \nan error, or precedence rules should be created, such as, \"the Compliance \nheader takes precedence over the Public header.\"\n\n4. I agree with Henrik that the \"PEP\" compliance token is premature.  If a \nregistry is created to handle compliance tokens, and if PEP requires such a \ntoken (currently unclear), then the PEP RFC can have a PEP compliance token \nadded to the registry.\n\n5. BNF nits.  The production:\n\noption-params = 1#(\";\" option-param)\n\nWas intended to be:\n\noption-params = 1*(\";\" option-param)\n\nI don't think the intent was to have a comma then a semicolon for repeated \noptions.  Similarly, the production:\n\nCompliance = \"Compliance\" \":\" (\"*\" | *(compliance-option))\n\nshould really be:\n\nCompliance = \"Compliance\" \":\" (\"*\" | 1#(compliance-option))\n\nto be consistent with the examples.  Furthermore, I added the \"1\" since I \ndon't believe the intent is to allow a Compliance header to be blank.\n\n- Jim Whitehead <ejw@ics.uci.edu>\n\n\n\n"
        },
        {
            "subject": "Re: Using Content-Encoding and ContentDisposition togethe",
            "content": "Foteos Macrides:\n>\n>Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n>>koen@win.tue.nl (Koen Holtman) wrote:\n>>>[...]\n>>>Well, this is not really a case where we have to agree on what the most\n>>>correct way is.  19.6.1 documents current practice, it is not\n>>>normative, so if anything, it should give hints about what to do with\n>>>current browsers.\n>>\n>>That section references only RFC 1806, which describes the\n>>\"attachment\" and \"inline\" disposition types.  My recollection of\n>>a long-ago message from Lou is that Netscape based its implementation\n>>on the file upload RFC's \"file\" disposition type.  What is the\n>>appropriate disposition type to use in HTTP Content-Disposition\n>>headers and META elements, and can information about that be included\n>>in 19.6.1?\n>\n>I tracked down Lou's message (appended) and was remembering\n>it correctly.  So, how about some current practice guidance/hints\n>about that?\n\nI just traced some references, and the file-upload RFC (rfc1867)\nmentions a `file' disposition type but uses `attachment' in its\nexamples.  draft-moore-mime-cdisp-01.txt mentions only `inline' and\n`attachment'.  Based on this, I think that keeping the section as is,\nwith `attachment', is best.\n\n>Fote\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on OPTIONS draf",
            "content": "An addendum to my previous post:\n\nRoy was kind enough to point out to me that RFC 2068, in Section 1.2, \ncontains definitions of \"unconditional\" (all MUST and SHOULD) and \n\"conditional\" (all MUST) compliance, which removes some of my objections \n(points #1 and #2) for RFC 2068.  However, other RFCs (notably RFC 1945) do \nnot have such definitions, and for these RFCs my criticisms still hold.  In \nfact, since RFC 1945 (shown in examples in the draft) is an informational \nRFC, it is unclear whether it should be listed in a Compliance header at \nall.\n\nThe definitions of unconditional and conditional in RFC 2068 do not remove \nall ambiguity -- for example, the Range header is listed as a \"MAY\" \ncapability in RFC 2068, but \"origin servers and intermediate caches SHOULD \nsupport bytes ranges.\"  So, is the Range header an unconditional \ncapability?\n\nPlus, I would still like to have the ability to have new protocols define \ntheir own option tokens.\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "draft-ieft-http-options00.tx",
            "content": "I read through the I-D, thinking about how an origin server that\nsupports cookies (i.e., RFC 2109) might respond to\n\nOPTIONS * HTTP/1.1\nCompliance: rfc=2109\n\nThe problem is that (I believe) most of the support is not in the\nserver itself, but in CGIs.  Consequently, the server software may not\nbe able to answer authoritatively about whether RFC 2109 is supported,\nbecause that may depend on what each individual CGI does.\n\nWhat advice would the authors (or others) give?\n\nNit:  p.4, section 3.2:  I think the phrase \"originating sender\" is\na poor one.  From one perspective, the originating sender is the\nclient that initiated the request.  But I think it's intended to mean\nthe server that responded to the OPTIONS method.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: draft-ieft-http-options00.tx",
            "content": "Dave Kristol:\n>\n>I read through the I-D, thinking about how an origin server that\n>supports cookies (i.e., RFC 2109) might respond to\n>\n>OPTIONS * HTTP/1.1\n>Compliance: rfc=2109\n>\n>The problem is that (I believe) most of the support is not in the\n>server itself, but in CGIs.  Consequently, the server software may not\n>be able to answer authoritatively about whether RFC 2109 is supported,\n>because that may depend on what each individual CGI does.\n>\n>What advice would the authors (or others) give?\n\nI think that in this case, the server should respond with something\nthat says `I don't know whether I'm compliant with 2109'.  I have not\nstudied the draft closely yet, but I believe a server could say this\nby sending an empty compliance header in the response.\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "&quot;Warning&quot; in draft 0",
            "content": "Some comments on the text added to resolve the WARNINGS issue:\n\n1) Without the motivation which is given in \n   draft-ietf-http-warning-00.txt, it is hard to understand how\n   the warn-date is supposed to work.\n\n2) It is specified _when_ a server MUST send warn-date, but all\n   the following questions are left open:\n\n   - What to put in there if warn-date has to be generated (!)\n\n   - What to to when a Warning with a warn-date is received, and\n     the warn-date does not force deletion of the warning-value:\n\n     Should a caching proxy store Warnings without the warn-date, or \n     with it?  Possibly update it?\n\n     Assuming that messages are stored with warn-dates, is the stored\n     warn-date updated on successful verification?\n\n     Should a proxy remove warn-date before forwarding to a 1.1 client\n     (which may be another proxy)?\n\n   I can try to guess which answers are the right ones to make it work.\n   But when I try to apply the usual rules for end-to-end headers -\n           \"A cache or non-caching proxy SHOULD NOT modify an\n            end-to-end header unless the definition of that header\n            requires or specifically allows that.\" from 13.5.2 -\n   I come to different results.  So I think it needs to be spelled out\n   explicitly what is required and allowed (unless I am the only reader\n   confused..).\n\n3) There seems to be no rule which allows to get rid of 2XX Warnings,\n   at least not those without a warn-date.  So if a cache entry is \n   sucessfully validated sevaral times, and each of the 304 responses has\n   a 2XX Warning, the cache would have to keep all of them even if they\n   are just duplicates.\n\n4) What should a client do when it receives an NXX warn-code, where N\n   is not 1 or 2?\n\n5) Whatever the answers to the above are, several places mentioning\n   end-to-end headers need some reformulation (adding \"except for Warning\n   headers\" or similar).  Also the rules added to 13.5.3 Combining Headers\n   for \"Warning headers\" don't look right, they should be rewritten\n   to operate on 'warning-value's.  The following is one header [or\n   header field], not three:\n \n  Warning: 110 P1 \"It's old\", 214 P2 \"It's mangled\", 299 P3 \"I warn you too\"\n\n   So what works for all other end-to-end headers in 13.5.3, replace\n   all-or-nothing without need to split the headers, doesn't work for\n   Warning.  It may make more sense to not call Warning an end-to-end\n   header at all, since it is so different.  There could be three\n   categories, 'Hop-by-hop headers', 'End-to-end headers', and 'Warning\n   headers'.  I think that could eliminate some formulation headaches.\n\n   ---\nThere, that should be enough complaining for today.\n\n\n      Klaus\n\n\n\n"
        },
        {
            "subject": "RE: State Management pre-draft  combinational requiremen",
            "content": "I really must object to the statement that IE has \"parsing problems\"\nwith cookies. IE's implementation is consistent with the documentation\nfor cookies released by Netscape. It happens that David Kristol decided\nto rely upon an undocumented feature in Netscape's implementation in\norder to maintain backwards compatibility. As often happens with\nundocumented features, Microsoft did not implement the feature the same\nway. The reason RFC 2108 will be replaced is not because IE did anything\nwrong, it is because David did not investigate why it was that his new\nspec failed with IE.\n\nAs for combinatorial vs additive, I wonder as to the utility of the\ncurrent cookie spec. Given the realities of future implementations it\nwould seem more reasonable to write a solid specification of the current\ncookie mechanism and leave the matter there.\n\nYaron\n\n> -----Original Message-----\n> From:Foteos Macrides [SMTP:MACRIDES@sci.wfbr.edu]\n> Sent:Saturday, July 26, 1997 2:37 PM\n> To:http-wg@cuckoo.hpl.hp.com;\n> http-state@lists.research.bell-labs.com\n> Subject:State Management pre-draft - combinational requirement\n> \n> requirement in the latest State Management pre-draft.\n> \n> The current effort at a revision of RFC 2108 began when Koen\n> pointed out that use of version 1 cookie attributes as in the RFC's\n> specifications for modern Set-Cookie headers are not handled\n> adequately\n> by MSIE's implementation for historical cookies.  We all promptly\n> agreed\n> that a revision to ensure backward compatibility with that old\n> implementation\n> is important.  In the process of discussing a revision, it also was\n> brought\n> out that the blanket port restriction has the side effect of blocking\n> any\n> cookie sharing between http and https servers, even when such sharing\n> should\n> not be blocked.  The desireablility of a discard attribute promptly\n> reached\n> consensus, and of a commentURL attribute to accommodate i18n concerns\n> beyond\n> the too meagre comment attribute is being discussed.  The new\n> attributes\n> (port, discard and commentURL) all could be incorporated in a simple\n> and\n> elegant manner as extensions to the RFC 2108 Set-Cookie header, but to\n> accommodate MSIE's parsing problems with that, we decided to create a\n> separate, Set-Cookie2 header for modern cookies.  The issue of how\n> this\n> affects UAs which implemented RFC 2108 (e.g., Lynx in its v2.7 and\n> v2.7.1\n> formal releases), i.e., if Set-Cookie headers regressed to historical,\n> was\n> not a factor in these discussions.  Though in principle this is to be\n> a\n> revision based implementation experience, it has proceded largely as\n> if\n> there were none, and as if backward compatibility with RFC 2018 is not\n> a\n> consideration (that's OK, don't worry about it :).\n> \n> Then we received a complaint that having Set-Cookie2 headers for\n> modern cookies and duplicates in Set-Cookie headers for historical UAs\n> is a \"problem\" for an organization which makes a practice of setting\n> large\n> numbers of lengthy cookies, so the combinational requirement was born.\n> To get an idea of that \"problem\", try http://www.microsoft.com/ with a\n> clean cookie jar.  You will be sent through a series of redirections,\n> and\n> cookie replacements, with the ultimate result of your cookie jar\n> acquiring\n> two seemingly identical cookies for hosts which don't domain match\n> according\n> either to RFC 2108 or the current pre-draft.  When I tried today,\n> after the\n> multiple redirections and replacements abated, I ended up with the\n> cookies\n> of identical name and value:\n> \n> Domain=.msn.com\n>        ||||||||\n> MC1=GUID=5e1a543305d611d188a708002bb74f65\n> \n> Domain=.microsoft.com\n>        ||||||||||||||\n> MC1=GUID=5e1a543305d611d188a708002bb74f65\n> \n> I don't know if those should be considered \"long\" cookie name/value\n> pairs,\n> and can only image what might be accomplished if private discussions\n> led\n> to the maximum of 5 redirections becoming a minimum, and my UA could\n> be\n> looped through 100 redirections.  Nor am I clear on why the ability to\n> load users' often limited resources with large numbers of lenthy, and\n> perhaps redundant, cookies should be promoted by the IETF.\n> \n> The objections that were posted about the combinational\n> requirement\n> merit more careful consideration, IMHO.  The intent originally was to\n> send\n> both Set-Cookie2 and Set-Cookie headers during a *transitional* period\n> to\n> the modern State Management design,  This is essentially a \"probing\" \n> situation, and as soon as either the UA or server detects that modern\n> cookie support is implemented in its State Management partner, only\n> Set-Cookie2 headers will be sent to the UA by the server's replies,\n> and\n> the modern Cookie header format will be used in the UAs requests to\n> the\n> server.  The sending of both Set-Cookie2 and Set-Cookie header thus\n> will become limited to just \"first contacts\", which are not likely to\n> involve large numbers of cookies (except, perhaps in lengthy\n> redirection\n> and cookie replacement loops :).\n> \n> The principle objection to the combinational requirement is that\n> it makes the protocol excessively complex and correspondingly\n> unreliable,\n> such that it in fact serves to discourage, rather than promote, the\n> implementation of a modern State Management design geared toward more\n> nearly adequate protection of users' privacy.   The section on\n> combining\n> Set-Cookie and Set-Cookie2 headers (10) says:\n> \n>   [...]                                           The user agent\n>                                                       ^^^^^^^^^^\n>   interprets the combined headers as follows.  First, it must\n> establish a\n> \n> ||||^^^^^^^^^^^^\n>   one-to-one correspondence between the cookies in the Set-Cookie and\n>   ||||||||||^^^^^^^^^^^^^^^\n> Set-Cookie2 headers. [...]\n> \n> How can the UA establish that *one-to-one* correspondence?  The cookie\n> name/value pairs will all be in the Set-Cookie header, and the\n> Set-Cookie2\n> header will have only additional attributes.  The ONLY correspondence\n> check\n> which a UA can perform is to count the number of version 1 attribute\n> sets\n> folding into a Set-Cookie2 array, and compare that with the number of\n> historical cookies folded into a Set-Cookie array.  If the two counts\n> are not the same, e.g., due to some semi-colon verus comma\n> substitution,\n> or any number of possible network transmission glitches, there is NO\n> basis\n> here for efforts at error recovery, or any efforts, whatsoever, for\n> bona fide\n> correspondence checks.  If the counts are different, for any reason,\n> the\n> UA will have no alternative but simply to throw the cookies and\n> attributes\n> on the floor and hope for better luck next time.\n> \n>         That section also points out, parenthetically:\n>  \n>         (Note that in this case the Set-Cookie2 response header that\n>          the origin server sends does not, by itself, conform to this\n>                                  ||||||||^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> \n>          specification.)\n>          ^^^^^^^^^^^^^\n> Think about that.  Is a \"transitional\" procedure for sending\n> Set-Cookie2\n> headers which DO NOT CONFORM TO THE SPECIFICATION a way to promote\n> implementation of that specification, or a way to interfere with it's\n> implementation?!?  Think about that carefully, please.\n> \n>         It is often the case in the real world that the squeeky wheel\n> gets oiled first.  But if instead the wheels are being rearranged so\n> that the wagon won't go anywhere, perhaps that is one case in which it\n> it truly appropriate to just say no.\n> \n>                                 Fote\n> \n> ======================================================================\n> ===\n>  Foteos Macrides            Worcester Foundation for Biomedical\n> Research\n>  MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n> ======================================================================\n> ===\n\n\n\n"
        },
        {
            "subject": "REVERSIO",
            "content": "-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\nGreetings,\n\nI apologize for the lateness in posting this to the list.\n\nIf you view the open issues for HTTP/1.1 and the agenda for the Munich\nmeeting, you'll notice an issue called 'RE-VERSION'.  After discussion\nwith the HTTP editors, I was encouraged to raise this issue on the list\nand as an open item for HTTP.\n\nThe RE-VERSION essentiall boils down to the meaning of the\nversion in an HTTP/1.1 response.\n\nI, and the editors, understand that this issue was heatedly and\ndeeply discussed previously, however, over the course of time, \nI do not beleive that the previous resolution is sufficient.\n\nTo summarize, the draft now indicates that the response version\nshould be the highest version supported by the server, NOT\nthe version of the actual HTTP response.\n\nMy problem with this is based on the assumption in the draft\nwhich dictates that the response version is hop-by-hop.\n\nOverall, I beleive that the response version should indicate\nthe version of the response message.\n\nThe problems encountered so far, in addition to any confusion\nbecause the 'response version', isnt the response version,\nare as follows:\n\nA client sends a 1.0 request, a 1.1 server responds with \na 1.1 response. ( minus certain http/1.1 features/headers).\n\nGET http://foo/foo.html HTTP/1.0\nblah: blahvalue\n\nHTTP/1.1 200 Ok\nblah: blahvalue\n\ncontent...\n\nWhen going through a proxy, how is the proxy supposed to maintain\nthe hop-by-hop nature of the responses?\n\nWorse, if a 1.0 proxy is in the stream, it will blindly pass\nthe 1.1 response code back to the client, and clearly fails\nto honor the hop-by-hop nature of the response.\n(this is the case with squid, netscape proxy, and presumable others)\nPreviously, the 1.0 didnt clearly define what the behaviour\nshould be in this case.\n\nOn deciding what to send in a 1.1 response to a 1.0 client,\nthere is no canonical list of things which a \nserver must not send when sending a 1.1 labeled response\nto a 1.0 server. There will surely be more than just\nchunked encoding.. This seems like a nightmare for\nproxy/server implementors dictating constant maintenance\nreleases to block new server reponse features/headers\nfrom being sent to 1.0 clients.\n( how can it be prepared to block something which hasn't\n  been invented yet?)\n\nThe only reason I could find in the previous discussion\nfor this behavior was to allow partial implementations\nto take advantage of some 1.1 features, but not all.\nThese days, since a great deal of the most contentious\nfeatures have been separated from the core http/1.1 draft,\nI dont think its too much to ask for a reasonably\ncompliant implementation that can at least deal with\nall the core http/1.1 headers/features.\n\nLets say Im implement a client that understands\ncache-control ( IMHO a valuable 1.1 feature ),\nbut is 1.0 otherwise.  Should a server send\na cache-control: directive or the usual ugliness\nof removing the last-modified: header? (or some other way).\nOr should it send both? (subverting the usefulness\nof the cache-control making it redundant)\n\nFinally, with things the way they are, does the response\nversion actually mean anything useful?  How useful is knowing\nwhat the highest version supported by the server?\nWouldnt knowing the version of the actual response message\nbe more useful and less ambiguous?\n\nThe way I see it we have these options:\n\n1) leave as-is, keeping the broken hop-by-hop behaviour\n   ( but vigorously specify what must not be sent to a \n1.0 client in a 1.1 labeled response )\n\n2) make the response version = the request version.\n\n3) create an additional 'response-version: ' header\n   to indicate the message version in use.\n\n4) keep the same rules in the draft, but make our version\n   2.0. ( which will mean that the server must send a 1.x\nresponse to a 1.x client )\n\n\nPersonally, I think the right thing to do is #2.\nYes, this will have some effect on the current implementations.\nHowever:\n\n1) the effect shouldnt put them in any worse position than they\n   already are. ( still broken )\n\n2) The IETF process draft/draft standard rfc/proposed standard rfc etc,\n   is meant to allow changes.  Implementing a draft implies that changes\n   will need to be made.\n\n\n(Im sure Im missing some issues or other stuff.. feel free to comment :)\n\n\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: Removing CommentUR",
            "content": "Jonathan Stark wrote:\n\n> You should probably check out the archive... we've debated the idea of\n> not accepting cookies during these requests, but some have objected,\n> saying basically that it's too dificult to turn cookies off for these\n> requests.\n\nI'm not sure what would be difficult about turning off cookies for commentURL requests.\n\n> Most people on this list appear to see the benefit in having a comment\n> and/or CommentURL.  The question currently is if commentURL should be\n> included.\n\nKeep in mind that most people on this list know/care about cookies and their content. We're a unique user group who has a vested interest in cookies. I don't think the average user knows/cares about cookies to this degree and we may be getting a bit carried away proposing that commentURL be implemented.\n\n> Without some connection between the cookie technology and the polcies\n> governing their use at this level, (either comment or CommentURL\n> or both) the general ignorance of the world towards cookies will continue.\n> Nobody wins here except for journalists who like to scare people with how\n> evil cookies are.\n\nThe commentURL may be considered the fine print that no-one will spend the time to read anyway. I also don't think content providers are going to want to share the meaning of their cookies (perhaps for marketing purposes).\n\n> The reasons FOR commentURL are many:  (Please add any I missed...)\n>         1. International Language Support\n\nIt is for this reason that I believe comment should be yanked.\n\n> If it comes down to picking one, I think we'd be crazy not to go with\n> CommentURL, but I don't see any real problem with having both.\n\nHaving both is a waste of parsing code.\n\nJudson\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Josh's #2 prefered choice matches my position ... I wouldn't mind if\na header were added to label the characteristics of the origin server.\nOnly required if the response level is less than the server's\nability. Possibly not terribly useful.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Thu, 7 Aug 1997, David W. Morris wrote:\n\n> \n> Josh's #2 prefered choice matches my position ... I wouldn't mind if\n> a header were added to label the characteristics of the origin server.\n> Only required if the response level is less than the server's\n> ability. Possibly not terribly useful.\n> \n\nI agree completely.  It always made more sense for the version number\nto reflect the response rather than the capabilities of the server.\nDuring the \"contentious\" discussion two arguments were made for\nsending the highest version of which the server is capable.  Neither\nmade much sense to me: 1) This is the way it was planned when HTTP/1.0\nwas created; and 2) An HTTP/1.1 client might choose to lie and say it\nis 1.0 if it thinks it is talking to a 1.0 server and then if the\nserver is really 1.1 the client will never know unless the server\nreturns the highest version of which it is capable.\n\nActually, I think the original intent was that a 1.0 client should be\nable to read a 1.1 response and function properly by ignoring anything\nit didn't understand.  Of course, chunking makes this fail spectacularly.\n\nAs a final observation I would point out that a single program can\nbe a server for multiple protocols.  (In fact a few years ago I\nwrote a combination gopher/http server.)   Thus a single program could\nbe both an HTTP/1.0 and an HTTP/1.1 server each part complying with\nthe relevant specification.  Which \"virtual protocol\" is used would\ndepend on the request version header.  This seems to me to be completely\ncompatible with both HTTP/1.0 and HTTP/1.1 specs and functionally\nequivalent to Josh's #2 choice (i.e version number represents response\nversion).\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Q:Which headers go with which Methods",
            "content": "A request-line MUST contain a method.\nAfter that request-headers can follow.\nBut can every request-header be combined with every request-Method?\n\nWaht does the 'PUT http:..\n       Accept: ..'\n mean?\n\nIs there a document that describes which header can go with which\nStart-line ?\n\nI thought that request-headers can be seen as parameters for \nrequest-methods.\n\n \n        Bye   Brusi\n\n            by           E-Mail: ab2@inf.tu-dresden.de\n                         Tel.-priv: 0351-8499347 (Germany/Dresden)\n          \\____\n\n\n\n"
        },
        {
            "subject": "Re: Comments on OPTIONS draf",
            "content": "> ... What would be \n> helpful is to provide a document which describes *exactly* what it\n> means to have unconditional compliance with each RFC, and to have\n> this information  be part of the registry maintained by IANA,\n> much as each MIME type has an associated description of that type.\n\nYes, please. If the RFC really does self-describe compliance,\nthen 'a document which describes *exactly* ...' can be short.\nOtherwise it might have to be longer.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "My previous response to this thread may have been a little too\nflip.  There is a serious issue here which has been discussed\nbefore but maybe not really resolved.  It deserves our attention.\n\nThere are two distinct pieces of information that might be\ncommunicated by a version header.  First there is the SENDER VERSION\n(SV) which is the highest version the sender is capable of supporting.\nSecond there is the ENTITY VERSION (EV) which is the lowest version\nfor which a receiver supporting only that version can still\nsuccessfully receive the entity.  Of course EV <= SV.\n\nWith the current spec the version header is hop-by-hop so the receiver\nof a transaction can compute EV = min { SV(sender), SV(receiver)}.\nThis complicates life for proxies, though.  It means that a 1.m proxy\nmust be capable of translating an entity with EV == 1.m to an entity\nwith EV == 1.n for all n <= m.  This is because hop-by-hop implies a\n1.m proxy talking to a 1.m server can only ask for a response with \nEV == 1.m, even though it may acting on behalf of a 1.n client with\nn < m.\n\nThis seems to put a heavy burden on proxies.  The current spec precudes\nthe possibility of a proxy pushing this burden onto the server where\nit would be easier to deal with (in general it will be easier to\n*generate* an entity with EV == 1.n for all n <= m than it is to\n*translate* an entity from EV == 1.k to EV == 1.j for all j <= k <= m).\n\nOf course, at present we only have versions 1.0 and 1.1 so the proxy\nneed only be able to translate an entity with EV == 1.1 to one with EV\n== 1.0.  This requires removing any chunking, but Josh Cohen asks if\nit requires anything else.  I don't know.  We need to understand and\ndocument exactly what is required for this translation.\n\nA concern that I have is that, if the version header is hop-by-hop and\nmust contain SV, then we have precluded a proxy design that would\nallow a proxy to request a lower EV than its SV.  There are many\nreasons a proxy might want to do this including separate caching of\nversions with different EV's instead of translating.\n\nThis discussion is relevant even for HTTP/1.1.  A 1.1 proxy between a\n1.0 client and a 1.1 server will always have to unchunk the server's\nresponse even if it is uncacheable and just being passed through.\nThere is no real reason for requiring the proxy to accept this burden\nand refusing to let it request an unchunked version from the server if\nit knows it needs one.\n\nIt seems to me that one way to remedy this is to somehow communicate\n(up to) three version numbers: The SV of the sender, the EV of the\ncurrent transaction, and, for requests, an RV or Requested Version to\nbe used in the response.  It would also be reasonable so say that\nthe EV of a request is the requested version for the response and\nthat would simplify things somewhat.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on OPTIONS draf",
            "content": "Jim Whitehead writes:\n    1. As currently specified, the \"RFC\" compliance token is\n    ambiguous.  For example, if a client discovers that an HTTP/1.1\n    server \"unconditionally\" supports RFC 2068, what can they assume\n    from this?  I assume this means that all MUST behavior is\n    supported, but what about MAY and SHOULD behavior?  Does\n    \"unconditional\" mean that all features in Appendix 19.6 of RFC 2068\n    are supported (PATCH, LINK, UNLINK), or does \"unconditional\" apply\n    only to the mainstream methods?  As for headers, if a server\n    unconditionally supports RFC 2068, does this mean the server\n    supports the Content-MD5 header (which is \"MAY\" functionality)?\n\n    I suspect that each RFC will have issues similar to these.  What\n    would be helpful is to provide a document which describes *exactly*\n    what it means to have unconditional compliance with each RFC, and\n    to have this information be part of the registry maintained by\n    IANA, much as each MIME type has an associated description of that\n    type.\n\nJust to amplify what other people have already said: not only\ndoes RFC2068 define what it means by MUST/SHOULD/MAY, but there\nis another document, RFC2119 (\"Key words for use in RFCs to\nIndicate Requirement Levels\", by Scott Bradner) that makes this\nexplicit for other RFCs, as well.  RFC2119 is a \"Best Current\nPractices\", which means it is not obligatory, and it doesn't\nexplicitly use the terms \"conditional\" or \"unconditional\".\n\nHowever, I think there is general agreement that Brader's\ndocument is the appropriate way to define MUST/SHOULD/MAY,\nand so in the next revision of the OPTIONS draft, I'll add,\nafter the existing text:\n\n    The option-param is used to provide additional parameters.\n    Unconditional compliance with a compliance-option is indicated\n    using the \"uncond\" option-param; for example, \"rfc=1945;uncond\".\n    Conditional compliance is indicated using the \"cond\" option-param;\n    for example, \"HDR=Authorization;uncond\".  Additional option-param\n    values might be defined as part of another specification.\n\nlanguage mostly stolen from RFC2068:\n\n    For the purposes of this header field, an implementation that\n    satisfies all the MUST and all the SHOULD requirements for its\n    protocols is defined as \"unconditionally compliant\"; one that\n    satisfies all the MUST requirements but not all the SHOULD\n    requirements for its protocols is defined as \"conditionally\n    compliant.\"  See also RFC2119 for a discussion of the terms\n    MUST and SHOULD.\n\nAs for \"MAY\" functionality: the whole point of MAY is that it\nallows something, but in no way requires (or bans) anything,\nso I don't think it's worth including a compliance-option for\nthis.  Regarding your specific example of Content-MD5, one\ncould either use\nCompliance: HDR=Content-MD5\nor one could (in principle, and only if this is really necesary)\nwrite up an RFC describing exactly what we agree that it means to\ncomply with Content-MD5, and then use that RFC #.\n\n    I think the token space for option-params also needs to be\n    controlled by IANA, and each token needs to have a description of\n    what that token means.\n     In the WebDAV working group, it is likely that locking capability\n     will be optional (MAY) capability, but will still be widely\n    supported.  I'd like to have the option of defining an option-param\n    to indicate that locking is (or is not) supported, for example,\n    \"rfc=nnnn;cond;nolock\"\n\nI actually think it would be better to have an RFC mmmm that describes\nlocking, and then use:\nCompliance: rfc=nnnn,rfc=mmmm\nrather than trying to get too fancy with parameters.  This is\nespecially true for anything that might involve the proxies.\n\nIt might still be worth having an IANA registry for the token\nspace.\n\n    3. It is possible to express contradictory information between the\n    Public and the Compliance header. For example, the Public header\n    could list all RFC 2068 methods except PUT, and the Compliance\n    header could state \"rfc= 2068;uncond\".  I think there should be a\n    sentence indicating that this is an error, or precedence rules\n    should be created, such as, \"the Compliance header takes precedence\n    over the Public header.\"\n\nGood point.\n\n    4. I agree with Henrik that the \"PEP\" compliance token is\n    premature.  If a registry is created to handle compliance tokens,\n    and if PEP requires such a token (currently unclear), then the PEP\n    RFC can have a PEP compliance token added to the registry.\n    \nYour message says \"comments on draft-ietf-http-options-01.txt\",\nbut the PEP stuff was only in draft -00.txt, and had already\nbeen removed when I submitted the draft -01.txt document.\n\n    5. BNF nits.  The production:\n    \n    option-params = 1#(\";\" option-param)\n    \n    Was intended to be:\n    \n    option-params = 1*(\";\" option-param)\n    \n    I don't think the intent was to have a comma then a semicolon for repeated \n    options. \n\nRight, my mistake (haste makes waste).\n\n    Similarly, the production:\n    \n    Compliance = \"Compliance\" \":\" (\"*\" | *(compliance-option))\n    \n    should really be:\n    \n    Compliance = \"Compliance\" \":\" (\"*\" | 1#(compliance-option))\n    \n    to be consistent with the examples.  Furthermore, I added the \"1\" since I \n    don't believe the intent is to allow a Compliance header to be blank.\n    \nAlmost.  The \"*\" should be a \"#\", but it isn't a \"1#\" because\nit definitely is meaningful to have an emply Compliance header in\na response.\n\nDigression: I considered using different header names for requests\nand responses (e.g., meaning \"Do-you-comply-with\" and\n\"Yes-I-comply-with\"); using just one header name (\"Compliance\")\nwas partly my laziness, and partly because I get flamed whenever\nI suggest that we need more headers, rather than fewer.  But\nmaybe using two different names would help people to understand\nthis proposal more easily.\n\nAnyway, in the context of the current proposal, it says in section\n2:\n      - A (new) Compliance header is proposed, which allows a\n        client to specify exactly what options it is asking about,\n        and which allows a server to specify exactly what subset of\n        those options are supported.\n\nbut in the definition of the Compliance header, it says:\n    In a request\n    message, this header lists the set of options that a client\n    wishes to know about.  In a response message, this header\n    lists the set of options that the server complies with.\n\nI guess this should say \"lists the subset of the requested\noptions\", rather than \"lists the set of options\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: draft-ieft-http-options00.tx",
            "content": "    I read through the I-D, thinking about how an origin server that\n    supports cookies (i.e., RFC 2109) might respond to\n    \n    OPTIONS * HTTP/1.1\n    Compliance: rfc=2109\n    \n    The problem is that (I believe) most of the support is not in the\n    server itself, but in CGIs.  Consequently, the server software may not\n    be able to answer authoritatively about whether RFC 2109 is supported,\n    because that may depend on what each individual CGI does.\n    \n    What advice would the authors (or others) give?\n    \nrom the point of view of the protocol, the client can't necessarily\ntell if there's a CGI program involved, and if I wanted to be a\npurist about things, I would say that this is well and good and\nthe right thing from a layered-architecture point of view.  I.e.,\ndon't expose \"implementation details\" in the protocol design.\n\nOf course, real life isn't so neat.  It sounds like the best\napproach would be to have Compliance supported in the interface\nbetween the CGIs and the server per se, but in the absence of\nthat, I guess the best advice I can give is:\nDon't claim compliance with things if, for this resource, a CGI\nsub-program will be invoked and you're not sure if the\nsub-program will screw things up.\non the theory that it is better to indicate non-compliance when\nyou might comply, than to claim compliance when you might not.\n\n    Nit:  p.4, section 3.2:  I think the phrase \"originating sender\" is\n    a poor one.  From one perspective, the originating sender is the\n    client that initiated the request.  But I think it's intended to mean\n    the server that responded to the OPTIONS method.\n\nRight, I was looking for a phrase that mean \"server that first sent\nthe OPTIONS reply\", which could be different from the \"origin server\"\nfor the URI (e.g., because of Max-Forwards).  Any suggestions for\na more precise short phrase?  Or should we just use the long one?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "inconsistent use of MAY &amp; SHOULD in 14.36.2 Range Retrieval Request",
            "content": "Jim Whitehead points out:\n\n    The definitions of unconditional and conditional in RFC 2068 do not\n    remove all ambiguity -- for example, the Range header is listed as\n    a \"MAY\" capability in RFC 2068, but \"origin servers and\n    intermediate caches SHOULD support bytes ranges.\"  So, is the Range\n    header an unconditional capability?\n    \nI believe what you are referring to is (from RFC2068; unchanged in\ndraft-ietf-http-v11-spec-08.txt):\n\n    14.36.2 Range Retrieval Requests\n\n            HTTP retrieval requests using conditional or unconditional\n            GET methods may request one or more sub-ranges of the\n            entity, instead of the entire entity, using the Range\n            request header, which applies to the entity returned as the\n            result of the request:\n\n                  Range = \"Range\" \":\" ranges-specifier\n\n            A server MAY ignore the Range header. However, HTTP/1.1\n            origin servers and intermediate caches SHOULD support byte\n            ranges when possible, since Range supports efficient\n            recovery from partially failed transfers, and supports\n            efficient partial retrieval of large entities.\n\nThis does look like an inconsistency.  \"SHOULD support ... wherever\npossible\" is almost a tautology, anyway.  I think the SHOULD here\nshould be \"ought to\", since Range support was clearly intended\nto be optional.\n\nWhat that means for the OPTIONS stuff is complicated by the\nfact that we already invented \"Accept-Ranges\" as a way for the\nserver to indicate that is willing to accept Range headers.\nAnyway, the Range header was carefully defined so that it would\n(more or less) work whether it was ignored or not.\n\n-Jeff\n\nP.S.: We're planning, sometime \"soon\", to organize a \"compliance-word\naudit\" of the HTTP/1.1 spec, to catch just this sort of thing, since we\nare aware that there are many other places where the words\nMUST/SHOULD/MAY are misused; some of these are simply the result of a\nglobal replacement by a text editor one day.\n\n\n\n"
        },
        {
            "subject": "Re: New Editorial issue MESSAGEBOD",
            "content": "Jeff wrote:\n>Roy suggests:\n>     A message-body SHOULD NOT be included in a request unless the\n>     request method is defined as allowing an entity-body.\n>\n>or\n>\n>     A server SHOULD read and forward a message-body on any request;\n>     if the request method does not include defined semantics for an\n>     entity-body, then the message-body SHOULD be ignored when handling\n>     the request.\n>\n>I'm not sure why we shouldn't just say both things (robustness\n>principle: don't send garbage, but don't choke on it, either).\n\nIncluding both is fine with me.\n\n>Actually, on reflection, I'm not sure how I would implement\n>a server that can figure out that a GET without a content-length,\n>but which has a bogus body, isn't actually two GETs.  I.e.,\n>if a bad client sends\n>\n>GET /foo.html HTTP/1.1\n>Host: foo.com\n>\n>GET /bar.html HTTP/1.1\n>Host: foo.com\n>\n>is that (1) two GETs on a persistent connection, or (2) one\n>GET with a bogus body and no Content-length?  Or do the\n>rules for persistent connections not allow this? I skimmed\n>RFC2068, and I don't think we require this!\n\nA request message body is indicated by the presence of Content-Length\nor Transfer-Encoding, as described in section 4.2 (Message Body).\nGranted, it doesn't say MUST, but it is easy to implement.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Issues-list item &quot;CACHINGCGI&quot",
            "content": ">> >A cache MUST NOT assign a heuristic expiration time to a\n>> >response for a URL that includes the strings \"htbin\", \"cgi-bin\", or\n>> >\"?\" in its rel_path part.  If such a response does not \n>> >carry an explicit expiration time, it must be treated as\n>> >if it expires immediately.\n>> \n>> I'm pretty sure I said this before, but I don't know what list.\n>> I am completely opposed to this change.  It is inaccurate to say that\n>> caching and reusing such responses is \"dangerous\".  The *only* reason\n>> *some* caches do not provide heuristic caching of such responses is\n>> because the presence of query-based parameters make it unlikely to get\n>> a second \"hit\" on the cache, and because the the absence of a Last-Modified\n>> (and now Etag) makes it impossible to do an efficient update.  In any case,\n>> this is an optimization which is dependent on the context and number of\n>> users of the cache, and not a requirement of the protocol.\n>> \n>> The protocol already provides mechanisms for marking a response as\n>> non-cachable.  All other responses to a GET request are cachable.\n>\n>I can't speak for the motivation of old cache authors, but I can speak as\n>an HTTP/1.0 application author from before any RFCs when one had to\n>reverse engineer everything and the empirical behavior I observed was that\n>GET requests which included a query part were not cached.\n\nIn most cases, yes, but the relevant question is why they are not cached.\nThe reason given by all of the cache maintainers I talked to at the\nGeneva, Chicago, Boston, Paris, and Santa Clara WWW conferences is that\nattempts to cache those responses led to poor hit frequency on the cache.\nIn other words, it is a performance optimization and does not need to\nbe written into the protocol.\n\n>I support the behavior for handling HTTP/1.1 responses strictly \n>conforming to Roy's position but I believe somthing like Jeff's\n>proposed wording is necessary when the 1.1 cache is covering a 1.0\n>server.\n\nWhy?  Why is it \"necessary\" to force a cachable response to be non-cachable?\nHTTP/1.0 had its own mechanisms for indicating cachability, and the presence\nor absence of \"?\" or \"cgi-bin\" or \"htbin\" does not indicate anything.\nIt is a fundamentally bad idea to associate any URL pattern with a particular\nHTTP functionality, because it breaks the separation between those two\nprotocols.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "response w/o contentlengt",
            "content": ">The question was whether a 1.0 response w/o content length had to \n>be fixed up by a proxy .... my interpretation is that there are\n>three choices:\n>\n>1.  Accumulate the body and create a correct content-length before\n>    forwarding.\n>2.  Add a cunked transfer encoding header and accumulate chunks\n>    and forward then with proper length fields\n>3.  Don't report the response as HTTP/1.1 in the status line\n>\n>All of this of course presumes that the request was HTTP/1.1.\n\nI truly wish people would check the specification before making\nsuch interpretations.  An HTTP/1.1 response is not required to have\na content-length or a transfer-encoding.  A response without one of\nthose length indicators is, as in HTTP/1.0, delimited by closing the\nconnection.  See section 4.4 of RFC 2068.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "I don't see why we need to re-hash this issue every six months.\nThese arguments are not new, already have responses in the archive,\nand do not justify such a major change to the protocol design.\n\n>To summarize, the draft now indicates that the response version\n>should be the highest version supported by the server, NOT\n>the version of the actual HTTP response.\n\nTo reiterate, it has been that way since 1993.\n\n>My problem with this is based on the assumption in the draft\n>which dictates that the response version is hop-by-hop.\n\nIt is not an assumption -- it is a requirement of HTTP.\n\n>Overall, I beleive that the response version should indicate\n>the version of the response message.\n\nIt does.  The major number defines the version of the response message.\nThe only reason we have two numbers is so that one can indicate capability.\n\n>The problems encountered so far, in addition to any confusion\n>because the 'response version', isnt the response version,\n>are as follows:\n>\n>A client sends a 1.0 request, a 1.1 server responds with \n>a 1.1 response. ( minus certain http/1.1 features/headers).\n>\n>GET http://foo/foo.html HTTP/1.0\n>blah: blahvalue\n>\n>HTTP/1.1 200 Ok\n>blah: blahvalue\n>\n>content...\n>\n>When going through a proxy, how is the proxy supposed to maintain\n>the hop-by-hop nature of the responses?\n\nA proxy forwards the response, and thus by definition is capable\nof maintaining the hop-by-hop nature.  It is not a difficult task.\n\n>Worse, if a 1.0 proxy is in the stream, it will blindly pass\n>the 1.1 response code back to the client, and clearly fails\n>to honor the hop-by-hop nature of the response.\n>(this is the case with squid, netscape proxy, and presumable others)\n>Previously, the 1.0 didnt clearly define what the behaviour\n>should be in this case.\n\nThat simply isn't true unless you have changed the proxy recently.\n1.0 required that the proxy send it's own version in any forwarded\nrequest or response.  The CERN, Netscape, and Apache proxies all obeyed\nthat restriction last time they were checked.  If Squid managed to scew\nthat up even after three RFCs have been published on the subject, then\nthey can't be trusted to have implemented anything right.\n\nWe simply cannot get around this requirement. ANY proxy which fails\nto indicate its own HTTP-version is incapable of supporting ANY\nfuture enhancements to the protocol, period.  HTTP will be dead.\nThink about it: the only thing that does not get forwarded by a\nvalid proxy is the Request-Line/Status-Line.  ANY other indication\nof a version number, e.g. some version number in a header, WILL be\nforwarded by an old proxy.  If we can't rely on HTTP-version, what\nmakes you think we can rely on anything else?\n\n>On deciding what to send in a 1.1 response to a 1.0 client,\n>there is no canonical list of things which a \n>server must not send when sending a 1.1 labeled response\n>to a 1.0 server.\n\nSure there is -- RFC 2068.  Everywhere it says that you can't send\nsomething to an HTTP/1.0 client.  They are very easy to find using a\ntext search.\n\n>There will surely be more than just\n>chunked encoding.. This seems like a nightmare for\n>proxy/server implementors dictating constant maintenance\n>releases to block new server reponse features/headers\n>from being sent to 1.0 clients.\n>( how can it be prepared to block something which hasn't\n>  been invented yet?)\n\nThat is FUD.  It is the responsibility of the protocol to make sure\nthat things which are new are not sent to implementation that do not\nunderstand them.  As proof:\n\n   3.6 Transfer Codings\n      A server MUST NOT send transfer-codings to an HTTP/1.0 client.\n\n   4.4 Message Length\n      For compatibility with HTTP/1.0 applications, HTTP/1.1 requests\n      containing a message-body MUST include a valid Content-Length header\n      field unless the server is known to be HTTP/1.1 compliant.\n\n   8.1.3 Proxy Servers\n      A proxy server MUST NOT establish a persistent connection with an\n      HTTP/1.0 client.\n\n   8.2 Message Transmission Requirements\n      An HTTP/1.1 (or later) server that receives a request from a\n      HTTP/1.0 (or earlier) client MUST NOT transmit the 100 (continue)\n      response;\n\n   10.1 Informational 1xx\n      Since HTTP/1.0 did not define any 1xx\n      status codes, servers MUST NOT send a 1xx response to an HTTP/1.0\n      client except under experimental conditions.\n\nThat's all there is to it.  Which one of those requirements is difficult\nfor an HTTP/1.1 proxy to implement?\n\n>Finally, with things the way they are, does the response\n>version actually mean anything useful?  How useful is knowing\n>what the highest version supported by the server?\n\nUnlike the server, a user agent cannot easily know in advance that a\ngiven server supports a given capability.  If a future incarnation of\nthis WG, or some fool programmer who believes that adding incompatible\nenhancements to the protocol can be done on their own outside the WG,\nmanages to screw up the protocol such that it is unsafe for a client\nto send its own highest version on the first request, the fact that the\nserver will respond with its highest minor version allows the client\nto start with a lower-version request and improve its later requests\nin safety.\n\nMoreover, it allows us to eliminate client implementations that are\ncurrently standing in the way of more evolutionary improvements to HTTP.\n\n>Wouldnt knowing the version of the actual response message\n>be more useful and less ambiguous?\n\nNo.  The version of the message is indicated by the first number.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">My previous response to this thread may have been a little too\n>flip.  There is a serious issue here which has been discussed\n>before but maybe not really resolved.  It deserves our attention.\n\nIt was resolved, twice.  I consider it closed.\n\n>With the current spec the version header is hop-by-hop so the receiver\n>of a transaction can compute EV = min { SV(sender), SV(receiver)}.\n>This complicates life for proxies, though.  It means that a 1.m proxy\n>must be capable of translating an entity with EV == 1.m to an entity\n>with EV == 1.n for all n <= m.  This is because hop-by-hop implies a\n>1.m proxy talking to a 1.m server can only ask for a response with \n>EV == 1.m, even though it may acting on behalf of a 1.n client with\n>n < m.\n\nYes, that is the cost of progress.  In order to take advantage of more\nadvanced protocol features, a proxy must make up for its weaker clients.\nThere is no way around it.  In fact, that should be considered a valuable\nfeature by anyone who purchases a proxy.\n\n>This seems to put a heavy burden on proxies.  The current spec precudes\n>the possibility of a proxy pushing this burden onto the server where\n>it would be easier to deal with (in general it will be easier to\n>*generate* an entity with EV == 1.n for all n <= m than it is to\n>*translate* an entity from EV == 1.k to EV == 1.j for all j <= k <= m).\n\nThat would be stupid.  If a proxy has no use for the advanced features,\nthen why has it implemented them?  For example, the chunked encoding\nallows for more reliable transfer of dynamic content, particularly\nwhen the content is cachable.  You are suggesting that a 1.1 proxy\nshould disable that capability just because the client of the current\nrequest is 1.0, even though the benefits received from chunked content\napply to all future requests as well.\n\nIn general, there is no requirement that a client (including a proxy)\nsend a 1.1 request instead of 1.0.  There can't be, since then the\nclient would be using the 1.0 protocol and not HTTP/1.1.  Thus, the\nconcern you mention is unfounded.\n\n>Of course, at present we only have versions 1.0 and 1.1 so the proxy\n>need only be able to translate an entity with EV == 1.1 to one with EV\n>== 1.0.  This requires removing any chunking, but Josh Cohen asks if\n>it requires anything else.  I don't know.  We need to understand and\n>document exactly what is required for this translation.\n\nDone that -- it took me ten minutes (not counting the two years arguing\nabout RFC 2068).\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Revised Agenda for Munic",
            "content": "Monday 8/11  9:30-11:30  Garmisch\n 9:30- 9:45: review of agenda\n 9:45-10:15: review of current draft and closed issues\n\n     Note: The Internet Drafts directory doesn't have\n        draft-ietf-http-v11-spec-08.txt\n      or draft-ietf-http-v11-spec-rev-00.txt\n     but does have draft-ietf-http-digest-aa-rev-00.txt\n\n     http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\n\n     contains the issue list and a pointer to the latest draft.\n\n10:15-11:30: Review of open issues, new issues\n    (e.g., PUT-RANGE, REAUTHENTICATION-REQUESTED,\n    REDIRECTS, SAFE, CONTENT-ENCODING, RE-VERSION)\n\n    Including issues in the following internet drafts:\n\n         draft-fielding-http-age-00.txt\n         draft-cohen-http-305-306-responses-00.txt\n         draft-holtman-http-safe-01.txt\n         draft-harada-http-xconnfrom-00.txt\n--------------------------------------------------\nTuesday 8/12 13:00-15:15 Garmisch\n13:00-13:30 Content Negotiation\n     draft-ietf-http-negotiate-scenario-01.txt\n     draft-ietf-http-negotiation-03.txt\n     draft-ietf-http-rvsa-v10-02.txt\n     draft-ietf-http-feature-reg-02.txt\n13:30-14:00 PEP\n     draft-ietf-http-pep-04.txt\n14:00-14:15 Announcements of other issues\n    Query Internationalization: Martin Duerst\n          draft-duerst-query-i18n-00.txt\n14:15-14:45 State Management\n   draft-ietf-http-state-man-mec-03.txt\n   draft-ietf-http-jaye-trust-state-00.txt\n14:45-15:15 Working group schedule\n   Charter milestones, etc.\n\n\n\n"
        },
        {
            "subject": "Re: Revised Agenda for Munic",
            "content": "Please add\n   Jonathan Rosenberg\n       draft-schulzrinne-http-status-reg-00.txt\n\nto the Tuesday 1400-1415 section.\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Roy, \n\nThe version number design which you have promulgated since 1993 gets\nrehashed every six months because:\n\n  1) It is counterintuitive to the point of being confusing,\n  2) It is flawed,\n  3) It greatly complicates some kinds of proxy design, and \n  4) It has no  discernible function.\n\nWe can probably live with these problems, but one price we will pay\nis that this aspect of the protocol will get rehashed at least every\nsix months as new people wrestle with what ought to be a trivial part\nof the protocol.\n\nOn Sat, 9 Aug 1997, Roy T. Fielding wrote:\n\n> John Franks wrote\n> \n> > This is because hop-by-hop implies a\n> > 1.m proxy talking to a 1.m server can only ask for a response with \n> > EV == 1.m, even though it may acting on behalf of a 1.n client with\n> > n < m.\n> \n> Yes, that is the cost of progress.  In order to take advantage of more\n> advanced protocol features, a proxy must make up for its weaker clients.\n> There is no way around it.  In fact, that should be considered a valuable\n> feature by anyone who purchases a proxy.\n> \n\nBut later you say the opposite:\n> \n> In general, there is no requirement that a client (including a proxy)\n> send a 1.1 request instead of 1.0.  There can't be, since then the\n> client would be using the 1.0 protocol and not HTTP/1.1.  Thus, the\n> concern you mention is unfounded.\n> \n\nIt is specious to say that a 1.1 proxy can send a 1.0 version number\nbecause then it becomes a 1.0 proxy.  What is really happening is that\nthe proxy is lying in its version header because that is the only way\nit can request the response it wants from the server.  This is one\nexample of a flaw in the version header design: a proxy may sometimes\nneed to lie about its capabilities in order to get the response it\nwants.\n\nThe reason that the version header comes up again and again is not\nthat difficult to understand.  What implementors NEED to know in\nprocessing a transaction is the *version of that entity*, i.e. the\nlowest version number such that a client or proxy of that version can\nhandle this entity.  The current version header gives only an upper\nbound estimate for this number. It is a non-trivial (and potentially\nerror-prone) task to calculate this entity version.  If versions 1.x\nwith x > 1 come into being this problem will get harder.\n\nApparently the original design intent was that the major version\nnumber would be the \"entity version\" and the minor number would\nindicate capability.  This is manifestly no longer the case.\n\nThere is substantial evidence that new implementors *expect* the\nversion header of a response to contain the entity version (which they\nneed and which the origin server knows) rather than a statement of\ncapability.  When they discover this is not the case (either by\nreading the spec or when something breaks) they are annoyed by the\nextra work, seemingly gratuitously created for them, and they come\nhere to complain.  This is unlikely to change anytime soon.\n\nI do not believe that statements like \"this is the cost of progress\"\nor \"this is the way we have done it since 1993\" are responsive.  The\nfact that the archive is full of such statements does not constitute\na resolution of the issue.\n\nOn the other hand RFC 2145 by Mogul et al. was a big step forward, in\nthat it at least specified precisely what the current spec says.\n\nThe bottom line, though, is that there are three pieces of version\ninformation related to an HTTP transaction: 1) the capability version\nof the sender, 2) the entity version of the transaction, and, for\nrequests, 3) the requested version of the response.  We have specified\nthat the capability version goes in the header, but it is actually the\nother two which are required for interoperability.  Often, but not\nalways, some of the numbers coincide.  Often, but not always, it is\npossible to derive all three numbers from the collection of all\nheaders.  However, even when this is possible, there is no simple\nalgorithm for doing so.  Even RFC 2145 suggests that in certain,\n(perhaps exceptional) circumstances clients and servers can lie about\ntheir capabilities and send an incorrect version header.  Though it is\nnot explained, the purpose is presumably to correctly communicate the\nentity or request version when the correspondent will otherwise compute\nthem incorrectly.\n\nThe current specification is workable, but problematic.  All the\nissues mentioned above have, indeed, been raised before and are in\nthe archives.  Emphatically denying their existence may temporarily\nsilence the topic on the mailing list, but doesn't help implementors.\nThe fact that the issue gets rehashed every six months might be taken\nas prima facie evidence that the design could be improved.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sat, 9 Aug 1997, John Franks wrote:\n\n> The current specification is workable, but problematic.  All the\n> issues mentioned above have, indeed, been raised before and are in\n> the archives.  Emphatically denying their existence may temporarily\n> silence the topic on the mailing list, but doesn't help implementors.\n> The fact that the issue gets rehashed every six months might be taken\n> as prima facie evidence that the design could be improved.\n\nYes.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">>>>> \"JF\" == John Franks <john@math.nwu.edu> writes:\n\nJF> The version number design which you have promulgated since 1993 gets\nJF> rehashed every six months because:\n\nJF>   1) It is counterintuitive to the point of being confusing,\n\n  That is a particularly subjective statement.  I find the version\n  numbering rules as they stand now to be so obvious and intuitive\n  that they hardly need discussion at all.  Reasonable people differ,\n  but in this case the 'intuitive'ness is not (IMHO) a usefull\n  criteria.\n\nJF> It is specious to say that a 1.1 proxy can send a 1.0 version number\nJF> because then it becomes a 1.0 proxy.  What is really happening is that\nJF> the proxy is lying in its version header because that is the only way\nJF> it can request the response it wants from the server.\n\n  I fail to see the problem - if it wants a 1.0 response, it can send\n  a 1.0 request; if it wants a 1.1 response it can send a 1.1 request.\n\nJF> The reason that the version header comes up again and again is not\nJF> that difficult to understand.  What implementors NEED to know in\nJF> processing a transaction is the *version of that entity*, i.e. the\nJF> lowest version number such that a client or proxy of that version can\nJF> handle this entity.  The current version header gives only an upper\nJF> bound estimate for this number. It is a non-trivial (and potentially\nJF> error-prone) task to calculate this entity version.  If versions 1.x\nJF> with x > 1 come into being this problem will get harder.\n\n  This is simply not true by the specification - the responder is\n  required to send a response which may be correctly interpreted by an\n  implementation of the protocol version _in the request_.  The\n  requestor need not use the version number in the response for\n  _anything_; it knows what version it requested and can interpret the\n  response by those rules.\n\nJF> Apparently the original design intent was that the major version\nJF> number would be the \"entity version\" and the minor number would\nJF> indicate capability.  This is manifestly no longer the case.\n\n  The intent was plainly nothing of the kind, as has been spelled out\n  in this forum and various IETF documents repeatedly.\n\nJF> There is substantial evidence that new implementors *expect* the\nJF> version header of a response to contain the entity version (which they\nJF> need and which the origin server knows) rather than a statement of\nJF> capability.  When they discover this is not the case (either by\nJF> reading the spec or when something breaks) they are annoyed by the\nJF> extra work, seemingly gratuitously created for them, and they come\nJF> here to complain.  This is unlikely to change anytime soon.\n\n  It is true that we cannot protect ourselves from the complaints of\n  developers who do not read the specification before they write thier\n  bugs.  That is no reason to change anything at all.\n\nJF> The bottom line, though, is that there are three pieces of version\nJF> information related to an HTTP transaction: 1) the capability version\nJF> of the sender, 2) the entity version of the transaction, and, for\nJF> requests, 3) the requested version of the response.\n\n  There is no such thing as the 'entity version of the response' and\n  there is no need for any.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">The version number design which you have promulgated since 1993 gets\n>rehashed every six months because:\n>\n>  1) It is counterintuitive to the point of being confusing,\n\nYou seem to have figured it out.  You just don't agree with it.\n\n>  2) It is flawed,\n\nNo it isn't.\n\n>  3) It greatly complicates some kinds of proxy design, and \n\nNo it doesn't.\n\n>  4) It has no  discernible function.\n\nBullshit.\n\n>We can probably live with these problems, but one price we will pay\n>is that this aspect of the protocol will get rehashed at least every\n>six months as new people wrestle with what ought to be a trivial part\n>of the protocol.\n\nThe versioning semantics are central to the design of HTTP to support\nfuture standard extensions to the protocol, including the extensions\nfrom HTTP/1.0 to HTTP/1.1.  It is this design that allows us to introduce\nthings like transfer encoding, 1xx responses, and PEP/Mandatory header\nfields.  The HTTP-version is the only field which is required NOT to be\nforwarded blindly by any HTTP proxy, regardless of version.  ALL of those\nrules I listed yesterday depend on the minor protocol version indicating\nthe *capability* of the sender.  Without these versioning semantics, the\nentire HTTP design collapses and we will never be able to improve HTTP,\nnot even with a major version change, except via an external indicator\non the URL scheme or DNS to indicate a new protocol can be used.\n\nThe fact that some people seem incapable of understanding that should\nnot require the WG to endlessly cycle through the same discussion.\nThe fact that some people think all version numbers have the same purpose\nis not an excuse to dismantle a working design.  The fact that some\nimplementers choose not to read the specifications before attempting\nto write an implementation does not justify changing the specification\nto correspond to their bugs which will clearly never be able to\ninteroperate with future versions of the protocol.\n\n>> > This is because hop-by-hop implies a\n>> > 1.m proxy talking to a 1.m server can only ask for a response with \n>> > EV == 1.m, even though it may acting on behalf of a 1.n client with\n>> > n < m.\n>> \n>> Yes, that is the cost of progress.  In order to take advantage of more\n>> advanced protocol features, a proxy must make up for its weaker clients.\n>> There is no way around it.  In fact, that should be considered a valuable\n>> feature by anyone who purchases a proxy.\n>> \n>\n>But later you say the opposite:\n>> \n>> In general, there is no requirement that a client (including a proxy)\n>> send a 1.1 request instead of 1.0.  There can't be, since then the\n>> client would be using the 1.0 protocol and not HTTP/1.1.  Thus, the\n>> concern you mention is unfounded.\n\nHow you can think that those two statements oppose one another is beyond\nmy understanding.  The only difference between an HTTP/1.0 proxy and an\nHTTP/1.1 proxy is that the latter declares its conformance with the 1.1\nspecification.  Every feature of HTTP/1.1 is also an HTTP/1.0 feature.\nEvery header field defined in the HTTP/1.1 specification is also an\nHTTP/1.0 header field.  And yes, that also applies to Transfer-Encoding\n(try it and see for yourself).  Whereas an HTTP/1.1 server is required\nnot to send a Transfer-Encoding to an HTTP/1.0 client, an HTTP/1.0\nserver can do so at will.  Granted, it would be stupid, but it wouldn't\nmake it any less stupid by adding an \"entity version\" to the message.\n\nAn HTTP entity is completely described by the entity-header fields that\nare included with the entity.  *Any* change to the protocol that requires\nunderstanding of a particular aspect of the entity, which is not defined\nby an existing version of the protocol, will also require incrementing \nthe minor protocol version.  Adding an \"entity version\" would be no more\nrevealing to the recipient than allowing a mandatory header field to be\npassed on to an old client.\n\n>It is specious to say that a 1.1 proxy can send a 1.0 version number\n>because then it becomes a 1.0 proxy.  What is really happening is that\n>the proxy is lying in its version header because that is the only way\n>it can request the response it wants from the server.  This is one\n>example of a flaw in the version header design: a proxy may sometimes\n>need to lie about its capabilities in order to get the response it\n>wants.\n\nWhat flaw?  The proxy is lying because it doesn't want the capabilities\nrequired of an HTTP/1.1 proxy.  If it had those capabilities, it wouldn't\nneed to lie, nor is there any reason for a fully-capable proxy to lie.\nAs I explained before, those requirements exist for the *benefit* of\nproxies.\n\n>The reason that the version header comes up again and again is not\n>that difficult to understand.  What implementors NEED to know in\n>processing a transaction is the *version of that entity*, i.e. the\n>lowest version number such that a client or proxy of that version can\n>handle this entity.  The current version header gives only an upper\n>bound estimate for this number. It is a non-trivial (and potentially\n>error-prone) task to calculate this entity version.  If versions 1.x\n>with x > 1 come into being this problem will get harder.\n\nYou must be talking about some other protocol, since none of those\nstatements are true about HTTP/1.1.\n\n>Apparently the original design intent was that the major version\n>number would be the \"entity version\" and the minor number would\n>indicate capability.  This is manifestly no longer the case.\n\nThe major version number indicates the message format, which is the\ncase and has always been the case.  Look at an HTTP/1.1 message (any\nvalid message) and point out the part which is not valid HTTP/1.0\nother than the HTTP-version field.\n\n>There is substantial evidence that new implementors *expect* the\n>version header of a response to contain the entity version (which they\n>need and which the origin server knows) rather than a statement of\n>capability.  When they discover this is not the case (either by\n>reading the spec or when something breaks) they are annoyed by the\n>extra work, seemingly gratuitously created for them, and they come\n>here to complain.  This is unlikely to change anytime soon.\n\nSo, what you are telling me is that we should stop all work on improving\nHTTP because some implementers are untrained savages, apparently reared\nby animals in a jungle, and unable to read the English language presented\nquite clearly in three separate RFCs.  Personally, I think implementers\nwho don't think they already know more about the protocol than the\nprotocol's designers will have the sense to actually read what has been\nwritten on the subject and implement accordingly.  If they do so, I can\nguarantee they will interoperate with other compliant implementations.\nIf they don't, there will be no interoperability, and thus no reason to\nhave a standard or waste time discussing it in this WG.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-1715\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "RE: REVERSIO",
            "content": "Gentlemen,\nI'm sorry to interfere in that long and heated discussion which\nseems to get less and less productive at each reply.\n\nI'm just unable to refrain my restlessness when I learn\nthat beeing \"unable to read the English language presented\nquite clearly in three separate RFCs\" describes people who are\n\"untrained savages, apparently reared by animals in a jungle\".\n\nAre you, even marginally, aware that one source of confusion\nMIGHT indeed reside in the difficulties that some savages have to\nunderstand all the subtleties of English ? I've been told that in very\nremote places, people still happened to speak and read some kind\nof other primitive languages, which might account for the difficulties\nthey have either to contribute positively to English discussions\nor to catch the fineness of your literature.\n\nI don't find it very courteous to insult savages on toga praetexta\nthat their English is not a fluent as yours, you SHOULD perhaps\nopen some discussions on how to provide translated versions of \nthe RFCs to help those savages to achieve your indisputable wisdom.\n\nRegards\n\n---------------------------------------------------\nJohn S?RAPHIN\nR?gie Autonome des Transports Parisiens - RATP\nInformation and Telecommunications Department\nmailto:jse@ratp.fr\n---------------------------------------------------\nThe opinions expressed hereby are strictly personal\nand might not be those of my employers.\n.\n\n----------\nDe : Roy T. Fielding\nDate?:dimanche 10 ao?t 1997 07:23\nA :John Franks\nCc :HTTP Working Group\nObjet :Re: RE-VERSION \n\n<snip>\nSo, what you are telling me is that we should stop all work on improving\nHTTP because some implementers are untrained savages, apparently reared\nby animals in a jungle, and unable to read the English language presented\nquite clearly in three separate RFCs.  Personally, I think implementers\nwho don't think they already know more about the protocol than the\nprotocol's designers will have the sense to actually read what has been\nwritten on the subject and implement accordingly.  If they do so, I can\nguarantee they will interoperate with other compliant implementations.\nIf they don't, there will be no interoperability, and thus no reason to\nhave a standard or waste time discussing it in this WG.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92697-3425    fax:+1(714)824-1715\n    http://www.ics.uci.edu/~fielding/\n\n</snip>\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sat, 9 Aug 1997, Roy T. Fielding wrote:\n\n[Josh Cohen <josh@netscape.com>:]\n> >To summarize, the draft now indicates that the response version\n> >should be the highest version supported by the server, NOT\n> >the version of the actual HTTP response.\n> \n> To reiterate, it has been that way since 1993.\n> \n> >My problem with this is based on the assumption in the draft\n> >which dictates that the response version is hop-by-hop.\n> \n> It is not an assumption -- it is a requirement of HTTP.\n[ -- snip -- ] \n> \n> >Worse, if a 1.0 proxy is in the stream, it will blindly pass\n> >the 1.1 response code back to the client, and clearly fails\n> >to honor the hop-by-hop nature of the response.\n> >(this is the case with squid, netscape proxy, and presumable others)\n> >Previously, the 1.0 didnt clearly define what the behaviour\n> >should be in this case.\n> \n> That simply isn't true unless you have changed the proxy recently.\n> 1.0 required that the proxy send it's own version in any forwarded\n> request or response.  The CERN, Netscape, and Apache proxies all obeyed\n> that restriction last time they were checked.  If Squid managed to scew\n> that up even after three RFCs have been published on the subject, then\n> they can't be trusted to have implemented anything right.\n\nA quick check with reality.  localhost:8080 is CERN/3.0, localhost:squid\nis Squid 1.1.1, none of them \"changed recently\".\n\n$ telnet localhost 8080\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nHEAD http://www.apache.org/ HTTP/1.0\n\nHTTP/1.1 200 OK\nDate: Sun, 10 Aug 1997 06:45:08 GMT\nServer: Apache/1.3a2-dev\nCache-Control: max-age=86400\nExpires: Mon, 11 Aug 1997 06:45:08 GMT\nConnection: close\nContent-Type: text/html\n\nConnection closed by foreign host.\n$ telnet localhost squid\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nHEAD http://www.apache.org/ HTTP/1.0\n\nHTTP/1.1 200 OK\nDate: Sun, 10 Aug 1997 06:47:07 GMT\nServer: Apache/1.3a2-dev\nCache-Control: max-age=86400\nExpires: Mon, 11 Aug 1997 06:47:07 GMT\nConnection: close\nContent-Type: text/html\n\nConnection closed by foreign host.\n\nI have also checked, for both proxies, that\n- this is not a particularity of the HEAD method (It happens with GET,\n  too)\n- for GET, if the request is repeated and the response served from the\n  proxy's cache, the response is still sent with \"HTTP/1.1\".\n\nThis shows that at least these two 1.0 proxies do not send their own\nversion in a forwarded (OR cached) RESPONSE.\n\nNow for whether they downgrade the version to their own in a REQUEST -\nyes, they both do: (only shown for CERN/3.0)\n\n$ telnet localhost 8080\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nHEAD http://www.apache.org/ HTTP/1.1\nHost: www.apache.org\n\nHTTP/1.1 200 OK\nDate: Sun, 10 Aug 1997 07:07:25 GMT\nServer: Apache/1.3a2-dev\nCache-Control: max-age=86400\nExpires: Mon, 11 Aug 1997 07:07:25 GMT\nConnection: close\nContent-Type: text/html\n\nConnection closed by foreign host.\n$\n\nThis is what goes over the wire:\n\n        HEAD / HTTP/1.0\n        Host: www.apache.org\n        User-Agent: Proxy gateway CERN-HTTPD/3.0 libwww/2.17\n\nSo both proxies act according to the expectation (aka: requirement) in one\ndirection but not in the other.\n\nA HTTP 1.0 client accessing a 1.1 origin server through such a proxy\nwill never improperly receive (for example) a chunked response, since the\nserver won't send it through that proxy.\n\nThe only harm I can see is with a HTTP 1.1 (or higher) client, which could\nbe misled into believing that it is talking to a 1.1 proxy.  As a result\nit might attempt to use a 1.1-only feature in a later request through that\nproxy - for example sending a chunked request body in a POST.\n\n> We simply cannot get around this requirement. ANY proxy which fails\n> to indicate its own HTTP-version is incapable of supporting ANY\n> future enhancements to the protocol, period.  HTTP will be dead.\n> Think about it: the only thing that does not get forwarded by a\n> valid proxy is the Request-Line/Status-Line.  ANY other indication\n> of a version number, e.g. some version number in a header, WILL be\n> forwarded by an old proxy.  \n\nAnd, as shown above, so will version numbers _in responses_, by at least\ntwo 1.0 proxy implementations.\n\n> If we can't rely on HTTP-version, what\n> makes you think we can rely on anything else?\n\nIn general I think the definition of HTTP-version is fine, as set out in\nRFCs 1945, 2068, and 2145.  But the existence of old (pre-1945) proxies\nwhich do not obey RFC 1945\n                                     \"Since the protocol version\n   indicates the protocol capability of the sender, a proxy/gateway must\n   never send a message with a version indicator which is greater than\n   its native version;\"\nfor response messages should not just be denied.\n\nHigher version clients and servers could work around it by agreeing on a\nspecial connection token for detection, as described below.  Admittedly\nit's ugly and wastes bytes - but as far as I can see it would work, under\nthe stated assumptions.  (It would make all proxies of this kind \ndetectable reliably by any >1.0 client if all >1.0 servers would implement\nit.  Of course it's already too late for that.)\n\nAssuming that only (some) 1.0 proxies are broken, in the way shown above,\nthat such proxies still downgrade the REQUEST version correctly, and that\nthey do NOT understand the Connection header:\n\n   All HTTP/1.N, N > 0 servers send the following IF and only IF the\n   request was a HTTP/1.0 request [possible addition: AND it is NOT known\n   that the next-hop client is NOT a \"broken\" 1.0 proxy], AND IF the\n   server does not choose to downgrade its response version to HTTP/1.0\n   (as allowed by RFC 2145 2.3 last para):\n\n   Connection: If-you-are-gt-1.0-Dont-trust-Version-gt-1.0-for-next-hop\n\n   If a HTTP/X.Y, X.Y > 1.0 client receives this token in a HTTP/1.N, N>0\n   response to a request (for it did not downgrade its request version to\n   1.0 as allowed by RFC 2145 2.3 third para), it can assume that the\n   next-hop server is really a 1.0 proxy.\n\n   And of course in reality the token should be shorter.  Maybe more like\n\n   Version-kludge: my.host.name.com 1.1  (thrown in for debugging)\n   Connection: version-kludge, close\n\nThis is one idea that would require cooperation (or a protocol change) from\nservers not directly involved.  However, since the problem is only with\ndetecting the correct version of a next-hop proxy, there often will be\nother means - out-of-band information or configuration, checking directly\nwith OPTIONS or TRACE if a proxy appears to be >1.0, or similar.\n\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "JF> The bottom line, though, is that there are three pieces of version\nJF> information related to an HTTP transaction: 1) the capability version\nJF> of the sender, 2) the entity version of the transaction, and, for\nJF> requests, 3) the requested version of the response.\n\n  As David W. Morris points out, I misquoted in my response; to\n  correct...\n\n  There is no such thing as the 'entity version of the transaction'.\n  Indeed, to make the statement even more concise, there is no such\n  thing as the 'entity version'.\n\n  The requestor has a version, and sends it (which may, at the\n  requestors option for whatever reason, be less than the highest\n  version it could use); the responder has a version and sends it, but\n  the response itself must always be valid according to the rules of\n  version sent by the requestor.  What could be simpler?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "  Klaus Weide <kweide@tezcat.com> provides us with traces showing two\n  deployed proxy implementations that plainly do not send correct HTTP\n  version numbers on thier forwarded responses, and suggests a\n  modification to 1.1 to provide a way to detect this broken behaviour\n  so that correct implementations can work through it.\n\n  Those implementations are and always have been _broken_.  They\n  plainly do not conform to the specifications, and should be fixed\n  and replaced.  It is true that as 1.1 origin servers and especially\n  1.1 clients come into wider use, these broken 1.0 proxies will begin\n  to cause problems; the client users will generally be the ones to\n  suffer for it.  With IE4 preview versions being distributed I\n  expect that this is already happening.\n\n  If I were implementing a 1.1 browser, I would spend a few minutes\n  adding an interface that used TRACE or OPTIONS to at least help\n  users discover what the problem really is, and I'd put up some help\n  pages about it (on 1.0 servers :-).  Then I'd add a link on the help\n  pages to my (correct) proxy product so that users could complain to\n  the adminstrator of the firewall or ISP whose broken proxy they are\n  stuck behind.  In short, view this as a sales opportunity.\n\n  These non-conforming proxies are no reason to do anything at all to\n  the definition of HTTP.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Scott Lawrence wrote:\n\n> \n>   The requestor has a version, and sends it (which may, at the\n>   requestors option for whatever reason, be less than the highest\n>   version it could use); the responder has a version and sends it, but\n>   the response itself must always be valid according to the rules of\n>   version sent by the requestor.  What could be simpler?\n> \n\nNothing could be simpler or more obvious! That is why a number of\npeople have been arguing this is what the spec SHOULD say.  \nUnfortunately, it says nothing of the kind and Scott like many\npeople before him has been confused by it.\n\nThere is already an entire RFC devoted to trying to explain the\nmeaning of the version header precisely because the meaning isn't\nthe \"obvious\" one.  Let me quote:\n\n\n   \"An HTTP client SHOULD send a request version equal to the highest\n   version for which the client is at least conditionally compliant, and\n   whose major version is no higher than the highest version supported\n   by the server, if this is known. ...\n\n   An HTTP client MAY send a lower request version, if it is known that\n   the server incorrectly implements the HTTP specification, but only\n   after the client has determined that the server is actually buggy.\"\n\nThus, sadly, Scott's assertion \n\n  \"The requestor has a version, and sends it (which may, at the\n  requestors option for whatever reason, be less than the highest\n  version it could use)\"\n\nis not true.  The requestor can only send a lower version number\nif it knows the server is buggy.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sat, 9 Aug 1997, Roy T. Fielding wrote:\n\n> \n> >It is specious to say that a 1.1 proxy can send a 1.0 version number\n> >because then it becomes a 1.0 proxy.  What is really happening is that\n> >the proxy is lying in its version header because that is the only way\n> >it can request the response it wants from the server.  This is one\n> >example of a flaw in the version header design: a proxy may sometimes\n> >need to lie about its capabilities in order to get the response it\n> >wants.\n> \n> What flaw?  The proxy is lying because it doesn't want the capabilities\n> required of an HTTP/1.1 proxy.  If it had those capabilities, it wouldn't\n> need to lie, nor is there any reason for a fully-capable proxy to lie.\n> As I explained before, those requirements exist for the *benefit* of\n> proxies.\n> \n\nOne concrete example:\n\nA 1.1 proxy cannot request an unchunked\nresponse from a 1.1 server without violating RFC 2145.  So far I\nhave seen two responses to this, 1) It's a feature not a bug, and\n2) Become a 1.0 proxy and send a 1.0 request.\n\nNeither of these is a reasonable answer.  There are many valid reasons\na 1.1 server might sometimes want a 1.0 response (e.g. to reduce\nde-chunking overhead if the response is only going to be passed to a\n1.0 client).  Answer 2) is a clear violation of RFC 2145.  The current\nspec is flawed because an implementor is required either to forgo some\nproxy funcitionality or to lie and violate RFC 2145.\n\nThis problem will be exacerbated as we add additional transfer encodings\nand move to versions 1.x, with x > 1.\n\n> Without these versioning semantics, the\n> entire HTTP design collapses and we will never be able to improve HTTP,\n> not even with a major version change, except via an external indicator\n> on the URL scheme or DNS to indicate a new protocol can be used.\n\nCan you back this statement up?  Suppose the semantics were that the\nrequest version header indicated the desired response version\n(usually, but not always the highest version the requestor is capable\nof) and the response version header indicated the version of the\nresponse (usually the minimum of the request version and the\nresponder's capability).  Can you give some concrete examples of\nhow \"the entire HTTP design collapses\" with this semantics?\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "John Franks wrote:\n> >The version number design which you have promulgated since 1993 gets\n> >rehashed every six months because:\n> >\n\n <snip>\n\n> \n> >  4) It has no  discernible function.\n\nOn Sat, 9 Aug 1997, Roy T. Fielding wrote:\n> \n> Bullshit.\n> \n\nWhen the version number is really representing the \"capability of the\nserver\" and is not equal to the version of the response, I have yet to\nsee a concrete example of how it might be used, if neither client nor\nserver is buggy and RFC 2145 is complied with.\n\nI would like to hear of a SINGLE hypothetical or real example of how\nthe response version header can be used when it is really representing\n\"capability of the sender\", that is, when the minor version number of\nthe response is strictly greater than the minor version number of the\nrequest (so the response is of a version lower than the server is\ncapable).  Such an example should assume any clients, servers and\nproxies involved are not buggy and are unconditionally compliant with\nRFC 2145.\n\nIt is because I know of no such examples that I claimed that making\nthe version header of a response represent capability rather \nthan the version of the response has \"no discernible function.\"\nI am ready to withdraw that statement if someone can provide examples\nof the functionality of a \"capability\" semantics.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Scott Lawrence wrote:\n\n>   Klaus Weide <kweide@tezcat.com> provides us with traces showing two\n>   deployed proxy implementations that plainly do not send correct HTTP\n>   version numbers on thier forwarded responses, and suggests a\n>   modification to 1.1 to provide a way to detect this broken behaviour\n>   so that correct implementations can work through it.\n> \n>   Those implementations are and always have been _broken_.  They\n>   plainly do not conform to the specifications, and should be fixed\n>   and replaced.\n\nI question that \"always have been\" part.  Correct me if I am wrong but\nAFAIK one of those implementations was written long before \"the\nspecification\".  One could just as well claim that RFC 1845 is broken,\nsince it failed in its claim to reflect \"common usage\" and to describe\n\"the features that seem to be consistently implemented\".\n\n>                  It is true that as 1.1 origin servers and especially\n>   1.1 clients come into wider use, these broken 1.0 proxies will begin\n>   to cause problems; the client users will generally be the ones to\n>   suffer for it.  With IE4 preview versions being distributed I\n>   expect that this is already happening.\n\nThey seem to be doing fine without help from old 1.0 proxies -\n<http://www.apache.org/dist/patches/apply_to_1.2.1/msie_4_0b2_fixes.patch>.\nBy the way, that workaround at first look (according to the included\ndescription of the problem) seems to be a practical application of the\nexemption in RFC 2145 2.3 last para:\n  \"An HTTP server MAY send a lower response version, if it is known or\n   suspected that the client incorrectly implements the HTTP\n   specification, but this should not be the default, and this SHOULD\n   NOT be done if the request version is HTTP/1.1 or greater.\"\nOn second look, it seems that the desired \"downgrading\" effect for the\nclient in question could also be achieved by sending \"Connection: close\"\nand not sending chunked entities, while still responding with \"HTTP/1.1\".\n\n>   If I were implementing a 1.1 browser, I would spend a few minutes\n>   adding an interface that used TRACE or OPTIONS to at least help\n>   users discover what the problem really is, and I'd put up some help\n>   pages about it (on 1.0 servers :-).  Then I'd add a link on the help\n>   pages to my (correct) proxy product so that users could complain to\n>   the adminstrator of the firewall or ISP whose broken proxy they are\n>   stuck behind.  In short, view this as a sales opportunity.\n\nThat might not be such a good idea after all.  While you have been\nbusy to make your product apparently harder to use, your competitors\na, b, and c came up with the clever idea to auto-detect such a proxy \n(maybe with TRACE or OPTIONS or just 'HEAD /') and just treat it as 1.0.  \nI think a, b, and c will look better than you.\n\n>   These non-conforming proxies are no reason to do anything at all to\n>   the definition of HTTP.\n\nI am not saying they are a reason to change the meaning of versions\nfrom that in the RFC.  (And the connection token idea was just that,\nan idea.)  But if HTTP 1.1 aspires to be widely deployed and to\n\"support the wide diversity of [cache and proxy] configurations\nalready deployed\" (RFC 2068 1.4), then it should at least offer some\nadvice how to deal with those old proxies, instead of insisting that\nthey first all have to go away before things can work.\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Sat, 9 Aug 1997, Roy T. Fielding wrote:\n>>[...] \n>> That simply isn't true unless you have changed the proxy recently.\n>> 1.0 required that the proxy send it's own version in any forwarded\n>> request or response.  The CERN, Netscape, and Apache proxies all obeyed\n>> that restriction last time they were checked.  If Squid managed to scew\n>> that up even after three RFCs have been published on the subject, then\n>> they can't be trusted to have implemented anything right.\n>\n>A quick check with reality.  localhost:8080 is CERN/3.0, localhost:squid\n>is Squid 1.1.1, none of them \"changed recently\".\n>[...]\n>This shows that at least these two 1.0 proxies do not send their own\n>version in a forwarded (OR cached) RESPONSE.\n>\n>Now for whether they downgrade the version to their own in a REQUEST -\n>yes, they both do: (only shown for CERN/3.0)\n>[...]\n>So both proxies act according to the expectation (aka: requirement) in one\n>direction but not in the other.\n>\n>A HTTP 1.0 client accessing a 1.1 origin server through such a proxy\n>will never improperly receive (for example) a chunked response, since the\n>server won't send it through that proxy.\n>\n>The only harm I can see is with a HTTP 1.1 (or higher) client, which could\n>be misled into believing that it is talking to a 1.1 proxy.  As a result\n>it might attempt to use a 1.1-only feature in a later request through that\n>proxy - for example sending a chunked request body in a POST.\n>\n>[... suggested protocol mod ...]\n>\n>This is one idea that would require cooperation (or a protocol change) from\n>servers not directly involved.  However, since the problem is only with\n>detecting the correct version of a next-hop proxy, there often will be\n>other means - out-of-band information or configuration, checking directly\n>with OPTIONS or TRACE if a proxy appears to be >1.0, or similar.\n\nIn an earlier round of discussions about this, when that false\nclaim was checked against reality, and the consequent problem of it\nleading to inappropriately chunked POSTs was raised, the suggested\nworkaround kludge was your second, i.e., for the client to send an\nOPTIONS request, and proceed with the chunked POST only if it doesn't\nget back a 400 from the broken proxy.  That'll work with the CERN proxy,\nbut I don't know about Squid, or if any other proxies are similarly broken\nin reality (from Josh's original message, it seems likely the Netscape\nproxy also passes through the origin server's status line for the response,\nwithout changing the minor version number to it's own).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sat, 9 Aug 1997, Roy T. Fielding wrote:\n\n> The major version number indicates the message format, which is the\n> case and has always been the case.  Look at an HTTP/1.1 message (any\n> valid message) and point out the part which is not valid HTTP/1.0\n> other than the HTTP-version field.\n\nA valid HTTP/1.1 request message with an entity and a \"Transfer-Encoding:\nchunked\" header (and therefore without a content-length) is not a valid \nHTTP/1.0 request, according to RFC 1945 7.2.2.\n\n   Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Ben Laurie wrote:\n\n> Klaus Weide wrote:\n> > <http://www.apache.org/dist/patches/apply_to_1.2.1/msie_4_0b2_fixes.patch>.\n> > By the way, that workaround at first look (according to the included\n> > description of the problem) seems to be a practical application of the\n> > exemption in RFC 2145 2.3 last para:\n> >   \"An HTTP server MAY send a lower response version, if it is known or\n> >    suspected that the client incorrectly implements the HTTP\n> >    specification, but this should not be the default, and this SHOULD\n> >    NOT be done if the request version is HTTP/1.1 or greater.\"\n> > On second look, it seems that the desired \"downgrading\" effect for the\n> > client in question could also be achieved by sending \"Connection: close\"\n> > and not sending chunked entities, while still responding with \"HTTP/1.1\".\n> \n> Maybe so, I see no virtue in that. What is the point of complying to the\n> standard while working around a broken beta? The solution we implemented\n> was expedient.\n\nI am not complaining about the workaround - I agree that it is better to\nhave a working solution than to have none but comply to the standard.\n\n> We should, perhaps, note that, if the directive is used,\n> the server no longer complies with RFC 2145.\n\nI think it still complies conditionally, it's just a SHOULD.\n\nI was more examining the standard, not your solution.  If a working\nworkaround for those two problems would *require* violation of the SHOULD\nin 2145 2.3 last para, then this would be a good reason for relaxing that\nSHOULD requirement - because Apache should not be \"punished\" for such a\nworkaround.  As it is (if I understand correctly), the workaround could\nbe changed, at least in principle, so as no to affect compliance.\nThere may be other good reasons why the SHOULD should be dropped, it just\nturns out that this example isn't a good case for it.\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Klaus Weide wrote:\n> <http://www.apache.org/dist/patches/apply_to_1.2.1/msie_4_0b2_fixes.patch>.\n> By the way, that workaround at first look (according to the included\n> description of the problem) seems to be a practical application of the\n> exemption in RFC 2145 2.3 last para:\n>   \"An HTTP server MAY send a lower response version, if it is known or\n>    suspected that the client incorrectly implements the HTTP\n>    specification, but this should not be the default, and this SHOULD\n>    NOT be done if the request version is HTTP/1.1 or greater.\"\n> On second look, it seems that the desired \"downgrading\" effect for the\n> client in question could also be achieved by sending \"Connection: close\"\n> and not sending chunked entities, while still responding with \"HTTP/1.1\".\n\nMaybe so, I see no virtue in that. What is the point of complying to the\nstandard while working around a broken beta? The solution we implemented\nwas expedient. We should, perhaps, note that, if the directive is used,\nthe server no longer complies with RFC 2145.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 994 6435|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 994 6472|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, John Franks wrote:\n\n> When the version number is really representing the \"capability of the\n> server\" and is not equal to the version of the response, I have yet to\n> see a concrete example of how it might be used, if neither client nor\n> server is buggy and RFC 2145 is complied with.\n> \n> I would like to hear of a SINGLE hypothetical or real example of how\n> the response version header can be used when it is really representing\n> \"capability of the sender\", that is, when the minor version number of\n> the response is strictly greater than the minor version number of the\n> request (so the response is of a version lower than the server is\n> capable).  Such an example should assume any clients, servers and\n> proxies involved are not buggy and are unconditionally compliant with\n> RFC 2145.\n\nAssume a client which is compliant with protocol version 1.N, and which\nadditionally implements some (but not all) features of protocol version\n1.M where M > N. Assume further that the client wants to use a feature F\nof 1.M which requires that the next-hop server is (at least) 1.M.  The\nclient cannot send HTTP/1.M in a request since it is not compliant with\n1.M.  But once it has received a HTTP/1.M response from a server, it\nwill know that it can use F in communicating with that server.\n\nA more concrete example, for N=0 and M=1:\nA client implements HTTP/1.0 and in addition can do chunked *en*coding\naccording to HTTP 1.1.  In order to upload something with\n\"Transfer-Encoding: chunked\", it first has to know that the server \nwill understand it, and the only currently defined way how the client can\nknow this is through a HTTP/1.1 response.\n\nSuch an \"enhanced 1.0\" client will never *receive* chunked content from\na 1.1 server, but I believe that is an orthogonal issue.  A 1.1 server is\nrequired to to able to receive and decode \"chunked\", whether the client\nis 1.0 or 1.1.  (As it happens, for this example a client would have to\neither violate the wording in RFC 1945 about content-length in requests or\nsend a meaningless content-length and violate a RFC 2068 requirement, but\nthere may be better examples where no such violation occurs.)\n\n\n    Klaus \n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Foteos Macrides wrote:\n\n> In an earlier round of discussions about this, when that false\n> claim was checked against reality, and the consequent problem of it\n> leading to inappropriately chunked POSTs was raised, the suggested\n> workaround kludge was your second, i.e., for the client to send an\n> OPTIONS request, and proceed with the chunked POST only if it doesn't\n> get back a 400 from the broken proxy.  That'll work with the CERN proxy,\n> but I don't know about Squid, or if any other proxies are similarly broken\n> in reality (from Josh's original message, it seems likely the Netscape\n> proxy also passes through the origin server's status line for the response,\n> without changing the minor version number to it's own).\n\nWell, it (Squid 1.1.1) doesn't give a 400 response, but the response\ndefinitely makes clear that this is no HTTP 1.1 server :)\nActually any response to OPTIONS which does not start with a valid\n\"HTTP/1.1\" (or higher) Status-Line should be enough to discredit a\nproxy's previous response with such a version.\n\n       Klaus\n\n$ telnet localhost squid\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nOPTIONS / HTTP/1.1\nHTTP/1.0 0 Cache Detected Error\nContent-type: text/html\n\n<HTML><HEAD><TITLE>ERROR: Invalid HTTP Request</TITLE></HEAD>\n[ some more lines of HTML snipped ]\n\n\n\n"
        },
        {
            "subject": "a positive &quot;no thanks&quot; to cookies",
            "content": "Fact: some people hate cookies.  They keep telling their browsers not to\naccept them.  \n\nA simple-minded way to write some kinds of server-side applications is\nto make sure a browser has a cookie by re-issuing it under certain\nconditions, such as not receiving a cookie header in the request.\nIt's annoying to have to have server side state to indicate whether\nyou think you've already sent someone a cookie, in order to avoid\nannoying a user who may be deliberately rejecting cookies.\n\nShouldn't there be something in the cookie-related part of the protocol\nso a client can tell a server not to send a certain cookie?  It would\nbe nice if it could specified at the level of a particular cookie, so\na user could be particular about which ones to accept and reject.\n\nThen, if a user told a browser not to accept a cookie, the browser\ncould include a header line on subsequent requests whose meaning would\nbe something like \"Hey! Remember that cookie you tried to send me?\nWell, don't!\"\n\nI'm not proposing a specific implementation - just wondering if\nthere's any reaction here to such a thought.\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Klaus Weide wrote:\n\n> On Sun, 10 Aug 1997, John Franks wrote:\n> \n> > I would like to hear of a SINGLE hypothetical or real example of how\n> > the response version header can be used when it is really representing\n> > \"capability of the sender\", that is, when the minor version number of\n> > the response is strictly greater than the minor version number of the\n> > request.\n> \n\n> A client implements HTTP/1.0 and in addition can do chunked *en*coding\n> according to HTTP 1.1.  In order to upload something with\n> \"Transfer-Encoding: chunked\", it first has to know that the server \n> will understand it, and the only currently defined way how the client can\n> know this is through a HTTP/1.1 response.\n> \n> Such an \"enhanced 1.0\" client will never *receive* chunked content from\n> a 1.1 server, but I believe that is an orthogonal issue.  A 1.1 server is\n> required to be able to receive and decode \"chunked\", whether the client\n> is 1.0 or 1.1.  (As it happens, for this example a client would have to\n> either violate the wording in RFC 1945 about content-length in requests or\n> send a meaningless content-length and violate a RFC 2068 requirement, but\n> there may be better examples where no such violation occurs.)\n> \n\nI stand corrected.  There does exist at least a hypothetical example\nwhere the \"capability semantics\" of the version header can be of use.\nThe functionality seems very limited given the problems it has caused\nand the existence of an OPTIONS header which might be better suited for\nthe purpose, but at least this semantics is not intrinsically  useless.\n\nPerhaps it would be helpful to have a sentence in the specification\nsaying \"The capability semantics of the response version header is\nintended (solely?) for the benefit of clients which wish to implement\ncertain features not compatible with the HTTP version with which they\nare conditionally compliant.\"  Had I been aware of this I would not\nhave claimed it had no discernible use.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "On Sun, 10 Aug 1997, Shel Kaphan wrote:\n> Fact: some people hate cookies.  They keep telling their browsers not to\n> accept them.  \n[...]\n> Then, if a user told a browser not to accept a cookie, the browser\n> could include a header line on subsequent requests whose meaning would\n> be something like \"Hey! Remember that cookie you tried to send me?\n> Well, don't!\"\n\nAre repeated pop-up warnings a protocol problem?  Seems to me they're a UI\nproblem, not a protocol problem.  The browser _could_ (though indications\nare the browser companies don't see the value in this) provide a better\nset of cookie-rejection options to, for instance, avoid bugging the user\nwith repeated pop-ups. \n\n<hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "According to John Franks,\n> \n> the purpose, but at least this semantics is not intrinsically  useless.\n> \n> Perhaps it would be helpful to have a sentence in the specification\n> saying \"The capability semantics of the response version header is\n> intended (solely?) for the benefit of clients which wish to implement\n> certain features not compatible with the HTTP version with which they\n> are conditionally compliant.\"  Had I been aware of this I would not\n\nwhen a client is 1.0 and the server is 1.1, then this\nwould be true, the response header has no meaning except the\nserver's highest version (for subsequent responses) since it\ncan assume a 1.0 entity respondse since it gave a 1.0 request.\n\nHowever, when a client is 1.1 and the server is 1.0, the client\ncant make assumptions about the reponse, it uses the response\nversion to notice that it is talkig to a 1.0 server.\n\n> \n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp.        josh@geeks.org\n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "hedlund@best.com writes:\n > \n > On Sun, 10 Aug 1997, Shel Kaphan wrote:\n > > Fact: some people hate cookies.  They keep telling their browsers not to\n > > accept them.  \n > [...]\n > > Then, if a user told a browser not to accept a cookie, the browser\n > > could include a header line on subsequent requests whose meaning would\n > > be something like \"Hey! Remember that cookie you tried to send me?\n > > Well, don't!\"\n > \n > Are repeated pop-up warnings a protocol problem?  Seems to me they're a UI\n > problem, not a protocol problem.  The browser _could_ (though indications\n > are the browser companies don't see the value in this) provide a better\n > set of cookie-rejection options to, for instance, avoid bugging the user\n > with repeated pop-ups. \n > \n > <hedlund@best.com>\n > \n\n\nPoint taken. Furthermore, if the browser people are not up to the\ntask you suggest, chances are they will not be up to the task I\nsuggested either!\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Sun, 10 Aug 1997, Foteos Macrides wrote:\n>\n>> In an earlier round of discussions about this, when that false\n>> claim was checked against reality, and the consequent problem of it\n>> leading to inappropriately chunked POSTs was raised, the suggested\n>> workaround kludge was your second, i.e., for the client to send an\n>> OPTIONS request, and proceed with the chunked POST only if it doesn't\n>> get back a 400 from the broken proxy.  That'll work with the CERN proxy,\n>> but I don't know about Squid, or if any other proxies are similarly broken\n>> in reality (from Josh's original message, it seems likely the Netscape\n>> proxy also passes through the origin server's status line for the response,\n>> without changing the minor version number to it's own).\n>\n>Well, it (Squid 1.1.1) doesn't give a 400 response, but the response\n>definitely makes clear that this is no HTTP 1.1 server :)\n>Actually any response to OPTIONS which does not start with a valid\n>\"HTTP/1.1\" (or higher) Status-Line should be enough to discredit a\n>proxy's previous response with such a version.\n\nOK.  I don't know what's the proper terminology, but in effect\nit's punting down to an HTTP/0.9 response, so if the UA uses the rule,\n\"Assume an old, non-compliant HTTP/1.0 proxy is interposed if you don't\nget back a 200 status.\", the OPTIONS probe should work.\n\nYour example, in another message, of a UA which sends a \"minor\n= n\" request but wants to know if it can use any \"minor > n\" functionality\nwhich it has implemented thus far is a very realistic case, and gets at\nthe reasoning behind the versioning rules (based on my recollections of\nprevious outbreaks of these debates :), so I agree with John that it would\nhelp to spell out that reasoning rather than leaving it in a \"That goes\nwithout saying, dummy!\" category.  That's the case which is most\nproblematic when an old HTTP/1.0 proxy is interposed and passes through\nan origin server's HTTP/1.[> 0] response status unmodified.  If you get\nback Connection: close, you don't know if it's a direct response to your\nHTTP/1.0 request, so you can't avoid the overhead of an OPTIONS probe.\nIf the UA did send an HTTP/1.1 request, then it could infer that something\nis fishy.  Is there a reason to worry about these things in HTTP/1.1\nexcept in the case of chunking (though that's an enhancement hoped to\nbe widely implemented ASAP, so I don't intend to minimize the issue)?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "REVERSIO",
            "content": "RE-VERSION:\n\nI had written a long message outlining my reasons for raising\nthe RE-VERSION issue, but after writing it, and with the help\nof John Klensin, Ive realized that the real issue\nhere isnt the response version exactly.  The problem\nI have is that in the spec, 1.1 proxy behaviour when talking\nto a 1.0 client and 1.1 server is underspecified.\n\nIt isnt clear how a proxy should act in generating response\nversions, from cached or origin responses, how it should\ndetermine cache applicability, and still make best advantage\nof 1.1 features, in a mixed version situation.\n\nIf we require that 1.1 proxies always upgrade requests\nto their highest version, then this issue will go away.\n(at least from me )  If this is required, then the proxy\nwill have all the information it needs to determine how\nto act, and it will be the proxy's responsibility to\ndowngrade the response to a lower version client.\n\n\n\n"
        },
        {
            "subject": "Re: draft-ieft-http-options00.tx",
            "content": "> DMK:\n>     Nit:  p.4, section 3.2:  I think the phrase \"originating sender\" is\n>     a poor one.  From one perspective, the originating sender is the\n>     client that initiated the request.  But I think it's intended to mean\n>     the server that responded to the OPTIONS method.\n> \nJeff Mogul:\n> Right, I was looking for a phrase that mean \"server that first sent\n> the OPTIONS reply\", which could be different from the \"origin server\"\n> for the URI (e.g., because of Max-Forwards).  Any suggestions for\n> a more precise short phrase?  Or should we just use the long one?\n\nHow about simply, \"the server that first sent the OPTIONS reply.\" :-)?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">A 1.1 proxy cannot request an unchunked\n>response from a 1.1 server without violating RFC 2145.  So far I\n>have seen two responses to this, 1) It's a feature not a bug, and\n>2) Become a 1.0 proxy and send a 1.0 request.\n\nThat is true, and it's also on purpose.  In order to reliably transfer\ndata, it is necessary to know the actual length of the data being\ntransferred (data error detection is handled at the transport layer).\nFailure to supply a length may result in prematurely truncated responses\nbeing misinterpreted as full responses, which was (and still is) a\nsignificant problem with HTTP/1.0.  The minimum change necessary to\nenable sending dynamically-generated data in efficient, length-delimited\nchunks was to require that all HTTP/1.1 applications be prepared to\nreceive chunked messages; a server may respond with a Length Required\nerror (to prevent yet another form of denial of service attack).\n\nIn other words, it is not an optional feature of the protocol.  Either\nthe proxy is willing to accept chunked or it is not an HTTP/1.1 proxy.\nI have yet to hear an actual proxy implementer claim that it is an\nunreasonable burden.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">A valid HTTP/1.1 request message with an entity and a \"Transfer-Encoding:\n>chunked\" header (and therefore without a content-length) is not a valid \n>HTTP/1.0 request, according to RFC 1945 7.2.2.\n\nYep, that's true, except that it isn't a valid HTTP/1.1 request if it is\nsent to an HTTP/1.0 server.  Point taken, though.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">This shows that at least these two 1.0 proxies do not send their own\n>version in a forwarded (OR cached) RESPONSE.\n>\n>Now for whether they downgrade the version to their own in a REQUEST -\n>yes, they both do: (only shown for CERN/3.0)\n\nWell, I guess that explains the confusion.  I have talked to the developers\nseveral times and each time was assured that the proxy sent its own\nversion in the message.  It didn't occur to me that they would do one thing\non the request and a different thing on the response.  The *really*\nboneheaded thing is I just checked Apache's proxy and it does the same\nstupid thing, and I could have fixed that code myself.  *sigh*  That covers\njust about every proxy, if we include Gauntlet as being broken both ways.\n\n>The only harm I can see is with a HTTP 1.1 (or higher) client, which could\n>be misled into believing that it is talking to a 1.1 proxy.  As a result\n>it might attempt to use a 1.1-only feature in a later request through that\n>proxy - for example sending a chunked request body in a POST.\n\nI have a feeling that chunked requests will only be usable by specialized\napplications.  A normal CGI script cannot accept chunked, because CGI/1.1\nitself requires a Content-Length.\n\nI hesitate to point out the effect it will have on the caching requirements.\n\nI'm not too keen on relying on a connection kluge.  Given that the critical\nparts of the versioning semantics are in the request chain and not (yet)\nin the response chain, how about the following compromise:\n\n   Follow the existing versioning requirements as-is, except that if the\n   request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n\nThat will allow for future extensions to the protocol in minor versions\nbeyond 1.1 to be protected, but avoid breakage with HTTP/1.0 proxies\nand the occasionally idiotic client that pukes on HTTP/1.1.  It will\nprobably make Jeff happier as well, since it makes detecting an HTTP/1.0\ncache much easier.\n\nIt also requires recycling the HTTP/1.1 specification as Proposed, but\nthat is just a procedural issue.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "On Sun, 10 Aug 1997, Shel Kaphan wrote:\n\n> Fact: some people hate cookies.  They keep telling their browsers not to\n> accept them.  \n\nAn alternative I proposed was that servers beable to stipulate that\ncookies are required for an application to function. I forget the\ndetails of what I suggested or the brief discussion thread.\n\nBut one must ask why users hate cookies and wonder what UI and/or server\nand hence perhaps protocol support might change that situation.\n\nLarry Masinter recently 'suggested' that perhaps getting a better\nsolution to state management warranted a new WG. If the UA vendors\nexpected to provide a high percentage of the desk top UAs were \ninterested, I believe there a number of avenues left untested in \nterms of providing better support from all perspectives for state\nmanagement.\n\nAs I recently noted in a post to the StateManagement sub-group list,\nit seems that the UA design point for cookies is to share then between\nall open browser windows. That really does make it hard to manage a\nsession.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "Shel Kaphan wrote:\n> \n> Fact: some people hate cookies.  They keep telling their browsers not to\n> accept them.\n> \n> A simple-minded way to write some kinds of server-side applications is\n> to make sure a browser has a cookie by re-issuing it under certain\n> conditions, such as not receiving a cookie header in the request.\n> It's annoying to have to have server side state to indicate whether\n> you think you've already sent someone a cookie, in order to avoid\n> annoying a user who may be deliberately rejecting cookies.\n> \n> Shouldn't there be something in the cookie-related part of the protocol\n> so a client can tell a server not to send a certain cookie?  It would\n> be nice if it could specified at the level of a particular cookie, so\n> a user could be particular about which ones to accept and reject.\n> \n> Then, if a user told a browser not to accept a cookie, the browser\n> could include a header line on subsequent requests whose meaning would\n> be something like \"Hey! Remember that cookie you tried to send me?\n> Well, don't!\"\n> \n> I'm not proposing a specific implementation - just wondering if\n> there's any reaction here to such a thought.\n\nIsn't this rather self-defeating? This mechanism could then be used\ninstead of the cookie.\n\nCheers,\n\nBen.\n\n-- \nBen Laurie            |Phone: +44 (181) 994 6435|Apache Group member\nFreelance Consultant  |Fax:   +44 (181) 994 6472|http://www.apache.org\nand Technical Director|Email: ben@algroup.co.uk |Apache-SSL author\nA.L. Digital Ltd,     |http://www.algroup.co.uk/Apache-SSL\nLondon, England.      |\"Apache: TDG\" http://www.ora.com/catalog/apache\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "On Mon, 11 Aug 1997, David W. Morris wrote:\n\n> \n> \n> On Sun, 10 Aug 1997, Shel Kaphan wrote:\n> \n> > Fact: some people hate cookies.  They keep telling their browsers not to\n> > accept them.  \n> \n> An alternative I proposed was that servers beable to stipulate that\n> cookies are required for an application to function. I forget the\n> details of what I suggested or the brief discussion thread.\n> \n> But one must ask why users hate cookies and wonder what UI and/or server\n> and hence perhaps protocol support might change that situation.\n\nUsers hate it because of _persistent_ cookies: Cookies that are requested\nto remain valid for literally YEARS. I routinely refuse *ANY* cookie that\nwill not disappear when I shutdown the browser. You can solve the problem\nby giving the users the options to protect their privacy by allowing them\nto simply refuse persistent and third party cookies. \n\nTo Netscape's credit they now have the option to silently turn off third\nparty cookies. To Microsoft's discredit they still do not allow silent\nrejection of third party cookies and have implemented a mis-leading dialog\nthat can result in people turning ON cookies by default when what they\nwanted to do was to turn them OFF silently.\n\nBut the fundamental privacy invasion is cookie persistence since it allows\nprofiles to be assembled over LONG periods of time without the informed\nconsent of the user.  UAs should be REQUIRED to provide ways for users to\nsay 'never accept a persistent cookie' and 'never accept a third party\ncookie' with making the user get battered with 'Would you like to accept' \ndialogs. Better yet would be the ability to turn cookies permanently on or\noff on a site by site basis.\n\nI guess what I am trying to say is that cookie are easily abused in\nprivacy violating ways while providing the users very little control. Add\nin NS's and Microsoft's public hostility to proposals for increasing user\nprivacy and the creation of user paranoia about cookies is an obvious\noutcome. \n\nFYI: As an ISP we have blocked cookies in our HTTP proxies. I suspect more\nthan a few other ISPs have also done this. This is really the ultimate\nresponse of ISPs to the browser makers' \"we're not going to respect your\nusers' privacy and you can't make us\" attitude. We can _and have_ in\neffect removed *ALL* cookie functionality from their browsers.  I could\neasily see European ISPs doing the same thing en mass to comply with\nEurope's personal data privacy laws. Wouldn't it be so much simpler for\nthe browser makers' to simply do the _RIGHT_ thing and give users strong,\ndetailed and informed control of cookies?\n\n  +-------------------------------------------------------------------+\n  |                                                                   |\n  | A server (billionclick.com) different than the one currently      |\n  | being browsed (myserver.com) has requested a cookie               |\n  | (ssdflskdjfs=sdsdf) that will persist until 12 December, 2010.    |\n  |                                                                   |\n  | The server states the following reason for the cookie request:    |\n  |                                                                   |\n  | \"This cookie is used for tracking advertising exposure of banner  |\n  |  advertisments and targeting banner ads to potential interest.    |\n  |  No personal identifying information is being accumulated nor     |\n  |  is the information being used in any other way.\"  [More Info]    |\n  |                                                                   |\n  | [x] Accept all future cookies from billionclick.com               | \n  |     [x] Allow cookies to persist after browser shutdown           |  \n  |     [ ] Don't allow cookies to persist after browser shutdown     |      \n  | [ ] Ask for all future cookies from billionclick.com              |\n  |     [ ] Allow this cookie to persist after browser shutdown       |\n  |     [ ] Don't allow this cookie to persist after browser shutdown |\n  | [ ] Refuse all cookies from billionclick.com                      |\n  |                                                                   |\n  +-------------------------------------------------------------------+\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "> in the response chain, how about the following compromise:\n> \n>    Follow the existing versioning requirements as-is, except that if the\n>    request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n\nand this is for proxy, server or both?\n\n> \n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp.        josh@geeks.org\n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">> in the response chain, how about the following compromise:\n>> \n>>    Follow the existing versioning requirements as-is, except that if the\n>>    request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n>\n>and this is for proxy, server or both?\n\nBoth.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "According to Roy T. Fielding,\n> \n> >> in the response chain, how about the following compromise:\n> >> \n> >>    Follow the existing versioning requirements as-is, except that if the\n> >>    request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n> >\n> >and this is for proxy, server or both?\n> \n> Both.\n> \n> ....Roy\n> \nUmm, not to further complicate things..\nWe discussed this at the \\wg and afterwards in an offline discussion,\nand it seems that if we assume that proxies will UPGRADE the request\nto their highest version, then leaving things as is will be livable.\nThis is what 2145 seems to say..\\\n\n(this allows a proxy to use 1.1 when ever possible to the origin\nserver and be able to satisfy 1.0 and 1.1 clients on cache hits\nwith no ambiguity.. )\n\nThe only case still uncovered is the chunked upload..\nSince that is a less common and more expensive operation, would\nit be wise to say that before you do a request like that, you\nshould or must check the server version with OPTIONS?\n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp.        josh@geeks.org\n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote:\n>>> in the response chain, how about the following compromise:\n>>> \n>>>    Follow the existing versioning requirements as-is, except that if the\n>>>    request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n>>\n>>and this is for proxy, server or both?\n>\n>Both.\n\nThat's a bit more than a compromise.  It's abandoning the ability\nof a UA to use HTTP/1.0 in requests and then size up the situation based\non HTTP/1.[> 0] in responses.  The shortcut of old HTTP/1.0 proxies acting\nas true proxies for the request, but like a tunnel for http server responses,\nis unfortunate, though understandable.  During the transition from so-called\nHTTP/0.9 (body only) to HTTP/1.0 (status line and headers), the response\nmight or might not have included more than a body, and so parsing it for\na status line which if present would have HTTP/1.0 and not be changed\nseemed like pointless overheader and vulnerability to a parsing foul\nup.  In the transition from HTTP/1.0 to HTTP/1.1, it is not pointless,\nand the proxy implementers forgot that they had taken a shortcut, or\nwhere not around during that first transition and didn't infer that a\nshortcut, rather than overt compliance with the versioning requirement,\nhad been taken.  It also won't help with already deployed HTTP/1.1 servers.\nIt seems better to stick with the OPTIONS probe, make unmistakeably clear\nin the comments/explanations sections of the HTTP/1.1 RFC, itself, that a\nproxy is expected to use its OWN, both major AND MINOR, version in both\nthe request AND RESPONSE directions, make clear the reasoning for that,\nand explain use of the OPTIONS probe with the hope that in a year or two\nor three it won't be needed any longer for old, non-compliant HTTP/1.0\nservers/proxies.\n\nAs far as \"UAs that originate a request, e.g., browsers\" are\nconcerned, it's risky to chunk a POST submission even if it's known\nthat only compliant HTTP/1.1 servers/proxies are in the request chain\n(the origin server might not \"put it all together and determine a\nContent-Length\" before invoking a CGI script), and for PUT, that\nwould act equivalently to an OPTIONS probe, wouldn't it?\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Roy T. Fielding wrote:\n\n> >> in the response chain, how about the following compromise:\n> >> \n> >>    Follow the existing versioning requirements as-is, except that if the\n> >>    request is HTTP/1.0 (and only HTTP/1.0), then make the response HTTP/1.0.\n> >\n> >and this is for proxy, server or both?\n> \n> Both.\n> \n\nSigh.  This discussion has often been more strident than necessary\nand I am probably as guilty as anyone in that regard.  But I wonder\nif at this point we couldn't stop, take a deep breath, step back\nand assess where we are.\n\nWe have a specification which has apparently been in existence and\nunchanged since 1993.  It has been one of the most contentious parts\nof the specification if we judge by the mailing list.\n\nWe now learn that essentially all proxy implementors have incorrectly\nimplemented this specification.  Indeed, they seem all to have\nimplemented it in the SAME incorrect way.\n\nBefore we add a band-aid on top of a band-aid to the current spec,\nwouldn't it be prudent to entertain the possibility that the\nspecification might be improved in the light of implementation\nexperience since 1993?  Isn't it possible that communicating strictly\nmore version information than is in the current header and making\nsure that the information is clearly labelled would serve us better\nin the long run?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Foteos Macrides wrote:\n\n> \n> As far as \"UAs that originate a request, e.g., browsers\" are\n> concerned, it's risky to chunk a POST submission even if it's known\n> that only compliant HTTP/1.1 servers/proxies are in the request chain\n> (the origin server might not \"put it all together and determine a\n> Content-Length\" before invoking a CGI script),\n\nI am not commenting on the riskiness, but aren't origin servers which\nare HTTP/1.1 and CGI/1.1 compliant required to do exactly that?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "> > On Sun, 10 Aug 1997, Shel Kaphan wrote:\n> > \n> > > Fact: some people hate cookies.  They keep telling their browsers not to\n> > > accept them.  \n> > \n...\n> > But one must ask why users hate cookies and wonder what UI and/or server\n> > and hence perhaps protocol support might change that situation.\n\nSpeaking only for myself, I don't like them because they're too\noften used only to track user behavior rather than provide me\nsome service. \n\nIn that case, I'd like to save cookies and replay them, only if my\nrequest URL can contain a small advertisement, ala' Yahoo, et al's\npages. That way, I too can recover the cost of maintaining state for a\n_commercial_ entity, for _their_ benefit.\n\nUntil then, I'll turn them off. Less state on both ends ought to\nmake the processing response time faster, if nothing else.\n\nJoe\n\n----------------------------------------------------------------------\nJoe Touch - touch@isi.edu    http://www.isi.edu/~touch/\nISI / Project Leader, ATOMIC-2, LSAM       http://www.isi.edu/atomic2/\nUSC / Research Assistant Prof.                http://www.isi.edu/lsam/\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "ISSUE 1 - server hint for POST/chunked\nISSUE 2 - what my problems are/were.\nSUMMARY - How I feel about re-raising this issue now.\n\nISSUE 1\nOn the use of server version hint..\nEven though you can look at the server version from a previous\nrequest, to determine if you are speaking to a 1.1 server \n(perhaps to do the chunked POST), you cant be sure that the\nversion number is really what it said it was..\n\nThe issue is related to the 301/302 issue for POST redirects\nto GETs from CGIs.  CGI/NPH may send their own response line,\nand advertise a different version than the server's own version.\nSo, on a 1.1 server, if a CGI sends a 1.0 response line, no real harm, but\nif you looked at it to base your POST on, you would mistakenly\nassume that the server is 1.0.\nHowever, if the server is 1.0, and the CGI sends a 1.1 response,\nyou would mistakenly send the POST/chunked and fail.\nEffectively, the server response version becomes the\n\"URI\" version number.\n\nNot entity version, and not server version.\n\nThe only way to be sure, ( or surest ) IMHO is to use OPTIONS.\n\nGranted, Ive raised another fringe case, but the POST chunked\nis a fringe case, and it can be remedied by using OPTIONS\ninstead of a previous request's server version advertisement.\n\nMy point here is that it essentially makes this server version\nadvertisement useless, therefore, philosophically, I beleive\nthe response version should be the entity-version.\n\nThe server implementors I have spoken to beleive that\nthis hint, might be useful, and only 1 real example\nhas come up.\n\nAs a proxy implementor, the issue of figuring out what\nthe entity version is and when I can send it to what \nversion client is a real problem, that exists today,\nand life would be easier if the response version\nwas something useful like entity-version.\n\nISSUE 2\nMy original problems werent about the POST issue, but how a\nproxy would manage its cache.  When it records the entity\nin the cache, it would have a complicated heuristic to determine\nthe entity version. \nDealing with a messy version number and deciding when\nan entity that you got from a 1.0 request is OK to serve\na subsequent 1.1 client request is tricky.\n\nIf the client is 1.0 and the server is 1.1, the cache entry\nwould be written as version 1.1, but you'd need to also store\nthe client's original request version so you can correctly\ndetermine that it is, in fact, a 1.0 entity.\n\nIf the client is 1.1 and the server is 1.0, then the\nresponse version is correct in indicating the entity version\nnumber.\n\nHowever, if the proxies UPGRADE all requests to *its* highest\nversion number, then this problem mostly goes away.\nThis is what RFC1945 says, and I think Roy has been saying\nall along.\n\nMy previous impression was that a proxy would \n\"Downgrade on request\" but that upgrading the request\nwas not encouraged.\n(And others also had similar feelings).\n\nSUMMARY\nIf the proxies are specified to UPGRADE the request to their\nhighest version, then I can live with the current version spec.\n\nI still think that fundamentally, the entity-version is much\nmore useful, than the ambiguous, unreliable server version hint,\nand that in a perfect world, the response version should \nbe that.. ( the response entity version )\nTime will tell if 'entity-version' or 'server version'\nis more useful, but if we stay our course and define\nthe response-version to be the server-version, then\nwe shouldnt hack things up to accomodate old \nimplementations.\nMy personal belief is that entity version is much more\nuseful than server version to solve the problems\nthat exist today.\n\nHowever, my personal opinions aside,it is late in the game \nand things mostly work. Therefore, as long as proxies upgrade \ntheir requests, I dont beleive that we absolutely must to\nchange the response version to save the universe. (any longer)\n\n\n\n\n-- \n-----------------------------------------------------------------------------\nJosh R Cohen /Server Engineer        josh@early.com\nNetscape Communications Corp.        josh@geeks.org\n(This message is sent from my private email account to reach me for \nbusiness related issues, mailto:josh@netscape.com )\n-----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: a positive &quot;no thanks&quot; to cookies",
            "content": "Ben Laurie writes:\n > \n > Isn't this rather self-defeating? This mechanism could then be used\n > instead of the cookie.\n > \n > Cheers,\n > \n > Ben.\n > \n\nYes, of course you're right, if it were per-cookie instead of per-server.\nIf there were a generic \"don't send cookies\" command, that would not convey\nany state information.\n\nAs Marc Hedlund pointed out, we don't need anything in the protocol\nfor this at all -- the same UI mechanisms that would have to be built\nfor that could entirely handle the problem. \n\nSorry to waste time by bringing this up -- the dangers of \"thinking out loud\"\nin a group like this.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "John Franks <john@math.nwu.edu> wrote:\n>On Mon, 11 Aug 1997, Foteos Macrides wrote:\n>\n>> \n>> As far as \"UAs that originate a request, e.g., browsers\" are\n>> concerned, it's risky to chunk a POST submission even if it's known\n>> that only compliant HTTP/1.1 servers/proxies are in the request chain\n>> (the origin server might not \"put it all together and determine a\n>> Content-Length\" before invoking a CGI script),\n>\n>I am not commenting on the riskiness, but aren't origin servers which\n>are HTTP/1.1 and CGI/1.1 compliant required to do exactly that?\n\nThat statement was nothing more than a personal opinion \"shared\"\nwith implementors on the list.  It is based on the following reasoning:\n\n(1) This WG consistently has backed off from getting involved\nwith a \"CGI standard\", so being \"CGI/1.1 compliant\" can't be a rigorous\nrequirement, and it seems \"risky\" to assume that the HTTP/1.1 protocol\novertly addresses all needs of CGI scripts.\n\n(2) One might infer that the basic needs are met, because a\nnumber of the WG participants care about meeting them, but Roy recently\nposted a message saying:\n\n\"I have a feeling that chunked requests will only be usable by\n specialized applications.  A normal CGI script cannot accept\n chunked, because CGI/1.1 itself requires a Content-Length.\"\n\nany he's far more familiar with the thousands of lines in the -08 draft\nand associated LAST_CALL drafts than I. :)\n\n(3) If the UA were to issue an OPTIONS request concerning\nCGI compliance, the response could say no more than one about\ncookies:  \"I hope so, but who knows what the author of the script\nput into it?!?\"\n\n\nSo, it seems \"safer\" to stick with unchuncked, HTTP/1.0 POSTs,\nfor now.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Josh wrote:\n\n> \n> My point here is that it essentially makes this server version\n> advertisement useless, therefore, philosophically, I beleive\n> the response version should be the entity-version.\n> \n\nInstead of debating which is more useful, might it not be sensible to\nconsider sending BOTH in a clearly labelled fashion.\n\n> \n> As a proxy implementor, the issue of figuring out what\n> the entity version is and when I can send it to what \n> version client is a real problem, that exists today,\n> and life would be easier if the response version\n> was something useful like entity-version.\n> \n\nI think this issue can be tricky.  Keep in mind that a 1.1 server\nresponding to a 1.2 proxy is free to send an entity with a\n1.1 version header, but using ANY 1.2 features it wants.  Indeed\nit was precisely to facilitate this that the \"capability semantics\"\nof the version header was designed.\n\nCompare this with sending both.  If a 1.1 server sends an entity to a\n1.2 proxy which says the server version is 1.1 but the entity version\nis 1.2, then it is completely clear that the server is only\nconditionally compliant up to 1.1 but it is using 1.2 features so the\nentity sent is only guaranteed to be useful, as sent, for a 1.2 agent.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Josh wrote:\n\n> My original problems werent about the POST issue, but how a\n> proxy would manage its cache.  When it records the entity\n> in the cache, it would have a complicated heuristic to determine\n> the entity version. \n> Dealing with a messy version number and deciding when\n> an entity that you got from a 1.0 request is OK to serve\n> a subsequent 1.1 client request is tricky.\n> \n   <snip>\n> \n> If the client is 1.1 and the server is 1.0, then the\n> response version is correct in indicating the entity version\n> number.\n> \n\nI don't believe this is correct, Josh.  A 1.0 server responding\nto a 1.1 proxy may use any 1.1 feature it chooses and is still\nrequired to send a 1.0 version header.  Indeed, encouraging this\nbehavior is the rationale of the current specification.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">I am not commenting on the riskiness, but aren't origin servers which\n>are HTTP/1.1 and CGI/1.1 compliant required to do exactly that?\n\n10.4.12 411 Length Required\n\n   The server refuses to accept the request without a defined Content-\n   Length. The client MAY repeat the request if it adds a valid\n   Content-Length header field containing the length of the message-body\n   in the request message.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">We now learn that essentially all proxy implementors have incorrectly\n>implemented this specification.  Indeed, they seem all to have\n>implemented it in the SAME incorrect way.\n\nYes, but your assumption that this is due to the specification is wrong.\nIt is more likely due to the fact that all of these implementations\nare based on the original CERN proxy.  It is more a case of us\nstill relying on lemming standards than standard specifications.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, John Franks wrote:\n\n> I stand corrected.  There does exist at least a hypothetical example\n> where the \"capability semantics\" of the version header can be of use.\n> The functionality seems very limited given the problems it has caused\n> and the existence of an OPTIONS header which might be better suited for\n> the purpose, but at least this semantics is not intrinsically  useless.\n> \n> Perhaps it would be helpful to have a sentence in the specification\n> saying \"The capability semantics of the response version header is\n> intended (solely?) for the benefit of clients which wish to implement\n> certain features not compatible with the HTTP version with which they\n> are conditionally compliant.\"  Had I been aware of this I would not\n> have claimed it had no discernible use.\n> \n> John Franks Dept of Math. Northwestern University\n> john@math.nwu.edu\n\nInstead of \n  \"features not compatible with the HTTP version with which they\n   are conditionally compliant\",\nthat should be \n  \"features compatible with a HTTP version with which they are\n   not conditionally compliant\".\n\nAs for the (solely?) - Roy Fielding wrote on Saturday in\n<9708090425.aa26104@paris.ics.uci.edu>:\n\n> Unlike the server, a user agent cannot easily know in advance that a\n> given server supports a given capability.  If a future incarnation of\n> this WG, or some fool programmer who believes that adding incompatible\n> enhancements to the protocol can be done on their own outside the WG,\n> manages to screw up the protocol such that it is unsafe for a client\n> to send its own highest version on the first request, the fact that the\n> server will respond with its highest minor version allows the client\n> to start with a lower-version request and improve its later requests\n> in safety.\n\nDoes that count?\n\n\n      Klaus    \n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">My personal belief is that entity version is much more\n>useful than server version to solve the problems\n>that exist today.\n\nI still don't understand why you think that there exists such a thing\nas an \"entity version\".  There are HTTP/0.9 messages and HTTP/1.x\nmessages and (sometime next century, apparently) HTTP/2.x messages.\nThe entity has no version because it is payload and is completely\ndescribed by the entity header fields (which, BTW, can be extended\nindependently of the protocol version numbers).  Placing a version\nnumber on those fields would be even less useful than the MIME-Version\nheader field of MIME.\n\nFor proxies, I think the best design is for them to store each response\nin their own internal format (whatever is easiest/smallest/quickest for\nthat particular application).  One thing I would never expect would be\nfor it to store the response with the chunked transfer encoding still\nintact -- it is, in all cases, more efficient and effective to dechunk\nthe response as it is being stored and save the Content-Length (and\ntrailer) of the result as header fields.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Sun, 10 Aug 1997, Foteos Macrides wrote:\n> Klaus Weide <kweide@tezcat.com> wrote:\n> >Well, it (Squid 1.1.1) doesn't give a 400 response, but the response\n> >definitely makes clear that this is no HTTP 1.1 server :)\n> >Actually any response to OPTIONS which does not start with a valid\n> >\"HTTP/1.1\" (or higher) Status-Line should be enough to discredit a\n> >proxy's previous response with such a version.\n> \n> OK.  I don't know what's the proper terminology, but in effect\n> it's punting down to an HTTP/0.9 response, so if the UA uses the rule,\n> \"Assume an old, non-compliant HTTP/1.0 proxy is interposed if you don't\n> get back a 200 status.\", the OPTIONS probe should work.\n\nIt need not be a 200 status - any \"HTTP/1.N\" response with N>0 would\nbe sufficient to detect that a next-hop server is not one of those proxies.\n\nMy understanding is that no HTTP/1.1 server is required to implement\nany specific method.  (So it could still be compliant if it responded\nto all methods with \"HTTP/1.1 405 Method not allowed\" or \"HTTP/1.1 501\nNot Implemented\".  Of course that's not very useful.)\n\n> Your example, in another message, of a UA which sends a \"minor\n> = n\" request but wants to know if it can use any \"minor > n\" functionality\n> which it has implemented thus far is a very realistic case, and gets at\n> the reasoning behind the versioning rules (based on my recollections of\n> previous outbreaks of these debates :), so I agree with John that it would\n> help to spell out that reasoning rather than leaving it in a \"That goes\n> without saying, dummy!\" category.  That's the case which is most\n> problematic when an old HTTP/1.0 proxy is interposed and passes through\n> an origin server's HTTP/1.[> 0] response status unmodified.  \n\nIf you mean that an old HTTP/1.0 proxy is more problematic for a\n\"halfway there\" HTTP/1.0 UA than for a HTTP/1.1 UA, then I don't\nunderstand why.  I think such a proxy could screw up more things\nin a   \n            HTTP/1.1     <->  HTTP/1.0(noncompliant)  <->  HTTP/1.1  \nchain than in a\n         HTTP/1.0(+1/2)  <->  HTTP/1.0(noncompliant)  <->  HTTP/1.1  \nchain.\n\n> If you get\n> back Connection: close, you don't know if it's a direct response to your\n> HTTP/1.0 request, so you can't avoid the overhead of an OPTIONS probe.\n> If the UA did send an HTTP/1.1 request, then it could infer that something\n> is fishy.  Is there a reason to worry about these things in HTTP/1.1\n> except in the case of chunking (though that's an enhancement hoped to\n> be widely implemented ASAP, so I don't intend to minimize the issue)?\n\nI don't understand the \"Connection: close\" example - specifically why\ngetting back a \"Connection: close\" would indicate fishiness.\nfishiness.  As far as I understand any HTTP/1.1 server can send this\nin the response to any HTTP/1.1 request, although normally they don't\n(at least for the first one on a new connection.)\n\n\n        Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, John Franks wrote:\n\n> On Mon, 11 Aug 1997, Josh wrote:\n> > \n> > My point here is that it essentially makes this server version\n> > advertisement useless, therefore, philosophically, I beleive\n> > the response version should be the entity-version.\n> > \n> \n> Instead of debating which is more useful, might it not be sensible to\n> consider sending BOTH in a clearly labelled fashion.\n\nI would like to see a definition of what is meant by \"entity version\".\nSo far it's a very diffuse term to me.\n\nCan it be defined in an exact way?  Other than as a simple\nmathematical function like MIN(version-in-request,version-in-response)\nwhich every client/proxy/server can trivially compute?  If yes, for what\nis it useful?  How can its knowledge improve the function of a client or\nproxy?  Is it a hop-by-hop or an end-to-end property?\n\nAlso it seems the term \"entity version\" is very misleading, because\n(whatever exactly it is) it appears to comprise more things than \"entity\"\nin the sense of the HTTP spec.\n\n\n        Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Klaus Weide wrote:\n\n> On Mon, 11 Aug 1997, John Franks wrote:\n> \n> > On Mon, 11 Aug 1997, Josh wrote:\n> > > \n> > > My point here is that it essentially makes this server version\n> > > advertisement useless, therefore, philosophically, I beleive\n> > > the response version should be the entity-version.\n> > > \n> > \n> > Instead of debating which is more useful, might it not be sensible to\n> > consider sending BOTH in a clearly labelled fashion.\n> \n> I would like to see a definition of what is meant by \"entity version\".\n> So far it's a very diffuse term to me.\n> \n> Can it be defined in an exact way?\n\nI will try.  Let's focus only on responses and only on 1.X agents for\nnow.  Informally the entity version is the smallest minor version\nnumber such that a user agent or proxy of that version can understand\nall features used in the response, provided those features exist in\nsome version of HTTP/1.*.\n\nMore formally, the entity version of a response is 1.N provided\nevery HTTP header or footer in the entity is defined in HTTP/1.N and\nat least one header or footer in the entity is not defined in\nHTTP/1.(N-1).  For the purposes of this definition a header is\nan HTTP header provided it is defined in HTTP/1.X for some X.\n\n\n> Also it seems the term \"entity version\" is very misleading, because\n> (whatever exactly it is) it appears to comprise more things than \"entity\"\n> in the sense of the HTTP spec.\n> \n\nI am not quite sure why you say this.  Remember that in HTTP-ese an\nentity is the entity-body together with all headers and footers.\nOperationally the entity is everything that goes over the wire in a\ntransaction.  I can think of one case where you are correct though.\nGiven the entity:\n\n\n   HTTP/1.1 200 OK\n   Server: Blah\n   Date: Mon, 11 Aug 1997 19:33:55 GMT\n   Last-modified: Fri, 25 Jul 1997 13:43:15 GMT\n   Content-type: text/html\n   Content-length: 3052\n\n   ...3052 bytes of stuff...\n\nthere is no way to tell if there is an implicit \"Connection: close\"\nheader (which there would be if the entity version is 1.0) or an\nimplict \"Connection: keep-alive\" (which there would be if the \nentity version is 1.1).  Thus in this case the entity version would\ncontain information not derivable from the entity alone.  In other\ncases I think the entity version could, at least in principle, be\nderived from the entity.  Am I wrong?\n\nIn any case, if the entity version were included in some header\nthen for self-referential reasons the entity version would \ndepend only on the entity.  :)\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>On Sun, 10 Aug 1997, Foteos Macrides wrote:\n>> Klaus Weide <kweide@tezcat.com> wrote:\n>> >Well, it (Squid 1.1.1) doesn't give a 400 response, but the response\n>> >definitely makes clear that this is no HTTP 1.1 server :)\n>> >Actually any response to OPTIONS which does not start with a valid\n>> >\"HTTP/1.1\" (or higher) Status-Line should be enough to discredit a\n>> >proxy's previous response with such a version.\n>> \n>> OK.  I don't know what's the proper terminology, but in effect\n>> it's punting down to an HTTP/0.9 response, so if the UA uses the rule,\n>> \"Assume an old, non-compliant HTTP/1.0 proxy is interposed if you don't\n>> get back a 200 status.\", the OPTIONS probe should work.\n>\n>It need not be a 200 status - any \"HTTP/1.N\" response with N>0 would\n>be sufficient to detect that a next-hop server is not one of those proxies.\n>\n>My understanding is that no HTTP/1.1 server is required to implement\n>any specific method.  (So it could still be compliant if it responded\n>to all methods with \"HTTP/1.1 405 Method not allowed\" or \"HTTP/1.1 501\n>Not Implemented\".  Of course that's not very useful.)\n>\n>> Your example, in another message, of a UA which sends a \"minor\n>> = n\" request but wants to know if it can use any \"minor > n\" functionality\n>> which it has implemented thus far is a very realistic case, and gets at\n>> the reasoning behind the versioning rules (based on my recollections of\n>> previous outbreaks of these debates :), so I agree with John that it would\n>> help to spell out that reasoning rather than leaving it in a \"That goes\n>> without saying, dummy!\" category.  That's the case which is most\n>> problematic when an old HTTP/1.0 proxy is interposed and passes through\n>> an origin server's HTTP/1.[> 0] response status unmodified.  \n>\n>If you mean that an old HTTP/1.0 proxy is more problematic for a\n>\"halfway there\" HTTP/1.0 UA than for a HTTP/1.1 UA, then I don't\n>understand why.  I think such a proxy could screw up more things\n>in a   \n>            HTTP/1.1     <->  HTTP/1.0(noncompliant)  <->  HTTP/1.1  \n>chain than in a\n>         HTTP/1.0(+1/2)  <->  HTTP/1.0(noncompliant)  <->  HTTP/1.1  \n>chain.\n>\n>> If you get\n>> back Connection: close, you don't know if it's a direct response to your\n>> HTTP/1.0 request, so you can't avoid the overhead of an OPTIONS probe.\n>> If the UA did send an HTTP/1.1 request, then it could infer that something\n>> is fishy.  Is there a reason to worry about these things in HTTP/1.1\n>> except in the case of chunking (though that's an enhancement hoped to\n>> be widely implemented ASAP, so I don't intend to minimize the issue)?\n>\n>I don't understand the \"Connection: close\" example - specifically why\n>getting back a \"Connection: close\" would indicate fishiness.\n>fishiness.  As far as I understand any HTTP/1.1 server can send this\n>in the response to any HTTP/1.1 request, although normally they don't\n>(at least for the first one on a new connection.)\n\nI was addressing the suggestion that a new header (not easy\nin LAST_CALL) or simple analyses of existing HTTP/1.1 headers, might\nhelp avoid the overhead of sending an OPTIONS probe simply to check if\na non-compliant HTTP/1.0 proxy is interposed.  Those proxies do send\nHTTP/1.0 in the request direction, so you would expect Connection: close,\nand never Transfer-Encoding: chunked in an HTTP/1.1 origin server's\nresponse for a document that includes a form.  However, if the UA used\nHTTP/1.0 rather than HTTP/1.1 in its request, that would be expected\nanyway, so you have no additional information about the situation.\n\nAs far as getting back 405 or 501 because that's possible based\non What is Written, I thought we were in LAST_CALL for moving HTTP/1.1\nfrom Proposed Standard to Draft Standard, hoping to catch any potentially\nserious implementation problems, and interpretation of the Word is not my\nforte. :) :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Foteos Macrides wrote:\n> Klaus Weide <kweide@tezcat.com> wrote:\n[ about the \"OPTIONS probe\" for non-compliant 1.0 proxies: ] \n> >\n> >It need not be a 200 status - any \"HTTP/1.N\" response with N>0 would\n> >be sufficient to detect that a next-hop server is not one of those proxies.\n> >\n> >My understanding is that no HTTP/1.1 server is required to implement\n> >any specific method.  (So it could still be compliant if it responded\n> >to all methods with \"HTTP/1.1 405 Method not allowed\" or \"HTTP/1.1 501\n> >Not Implemented\".  Of course that's not very useful.)\n[ big snip ]\n> \n> As far as getting back 405 or 501 because that's possible based\n> on What is Written, I thought we were in LAST_CALL for moving HTTP/1.1\n> from Proposed Standard to Draft Standard, hoping to catch any potentially\n> serious implementation problems, and interpretation of the Word is not my\n> forte. :) :)\n\nIt would actually be reasonable behavior for a proxy-only server,\ni.e. a server which is not also configure to act as an origin server,\nto respond with an error status to all requests which don't use the\nabsoluteURI form for Request-URI.  And the non-absoluteURI form is\nrequired for the OPTIONS probe, to make sure it is really the next-hop\nproxy who answers.\n\nThis leads to another question.  Does the proposed OPTIONS probe\nactually *work* for an Apache/1.[23]* server acting as proxy?  As Roy\nhas revealed Apache-as-proxy is one of the cases where HTTP/1.1\nresponse status is improperly forwarded.  I suspect (but cannot test)\nthat the same server would respond as HTTP/1.1 when checked with\nOPTIONS, making the probe worthless.\n\n\n   Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Klaus Weide <kweide@tezcat.com> wrote:\n>[...]\n>This leads to another question.  Does the proposed OPTIONS probe\n>actually *work* for an Apache/1.[23]* server acting as proxy?  As Roy\n>has revealed Apache-as-proxy is one of the cases where HTTP/1.1\n>response status is improperly forwarded.  I suspect (but cannot test)\n>that the same server would respond as HTTP/1.1 when checked with\n>OPTIONS, making the probe worthless.\n\nAh!  An implementation issue, not a terminology issue. :)\n\nIt *is* an HTTP/1.1 server, so one would not predict a serious\nproblem during this transition, just as the shortcut had no apparent\nproblem for the old HTTP/1.0 proxies during the HTTP/0.9 -> HTTP/1.0\ntransition.  It would become a problem for an HTTP/1.1 -> HTTP/1.2\ntransition.  But one can also predict that it will be fixed ASAP,\nand hopefully the current Apache servers will all have been replaced\nby then. :)\n\nBy \"serious\" problem I mean that sharing of worthwhile information\nfails (just to be clear about my terminology :).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, John Franks wrote:\n\n> On Mon, 11 Aug 1997, Klaus Weide wrote:\n> \n> > I would like to see a definition of what is meant by \"entity version\".\n> > So far it's a very diffuse term to me.\n> > \n> > Can it be defined in an exact way?\n> \n> I will try.  Let's focus only on responses and only on 1.X agents for\n> now.  Informally the entity version is the smallest minor version\n> number such that a user agent or proxy of that version can understand\n> all features used in the response, provided those features exist in\n> some version of HTTP/1.*.\n> \n> More formally, the entity version of a response is 1.N provided\n> every HTTP header or footer in the entity is defined in HTTP/1.N and\n> at least one header or footer in the entity is not defined in\n> HTTP/1.(N-1).  For the purposes of this definition a header is\n> an HTTP header provided it is defined in HTTP/1.X for some X.\n\nOk thanks, that's a start.\n\nNow it remains to be shown for what this would be useful.  (It also\nremains to be seen whether Josh agrees with your definition :) )\n\nAs given by your formal definition, the \"entity version\" (which should be\ncassed something else, see below) can be trivially derived from the\nmessage.  It just requires tables of all headers defined by the various\nprotocol versions, nothing else.  Therefore it is totally redundant.\n\n> > Also it seems the term \"entity version\" is very misleading, because\n> > (whatever exactly it is) it appears to comprise more things than \"entity\"\n> > in the sense of the HTTP spec.\n> > \n> \n> I am not quite sure why you say this.  Remember that in HTTP-ese an\n> entity is the entity-body together with all headers and footers.\n> Operationally the entity is everything that goes over the wire in a\n> transaction. \n\nNo.  The entity is the entity-body together with the *entity*-headers,\n*not* all headers.  According to the Official Guide to HTTP-ese.\nI don't think that is what you mean.\n\n> I can think of one case where you are correct though.\n> Given the entity:\n> \n> \n>    HTTP/1.1 200 OK\n>    Server: Blah\n>    Date: Mon, 11 Aug 1997 19:33:55 GMT\n>    Last-modified: Fri, 25 Jul 1997 13:43:15 GMT\n>    Content-type: text/html\n>    Content-length: 3052\n> \n>    ...3052 bytes of stuff...\n> \n> there is no way to tell if there is an implicit \"Connection: close\"\n> header (which there would be if the entity version is 1.0) or an\n> implict \"Connection: keep-alive\" (which there would be if the \n> entity version is 1.1).  Thus in this case the entity version would\n> contain information not derivable from the entity alone.  In other\n> cases I think the entity version could, at least in principle, be\n> derived from the entity.  Am I wrong?\n\nSo your formal definition above has failed to describe what you mean.\nThere is one thing which cannot be trivially derived from the message\nitself as given, and the definition does not account for it.\n\nAnyway, the property of the response which you express as an implicit\nConnection token is unambiguously defined one way or the other, each\ntime such a response message is received, by the version number in the\nstatus line.  So again this is redundant information.\n\n> In any case, if the entity version were included in some header\n> then for self-referential reasons the entity version would \n> depend only on the entity.  :)\n\nAs defined so far, I don't see any use for it...\n\n\n         Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Klaus Weide wrote:\n\n> On Mon, 11 Aug 1997, John Franks wrote:\n> \n> > More formally, the entity version of a response is 1.N provided\n> > every HTTP header or footer in the entity is defined in HTTP/1.N and\n> > at least one header or footer in the entity is not defined in\n> > HTTP/1.(N-1).  For the purposes of this definition a header is\n> > an HTTP header provided it is defined in HTTP/1.X for some X.\n> \n> Ok thanks, that's a start.\n> \n\nYou are correct that it depends on more than the entity headers.\nIt also depends on the response header and the general headers.\nI don't really care what it is called. Let's call it a \"response\nversion\" for now.\n\nA revised formal definition:\n\nThe response version of a response is 1.N provided\nevery HTTP header or footer in the response is defined in HTTP/1.N and\nat least one header or footer in the response is not defined in\nHTTP/1.(N-1).  For the purposes of this definition a header is\nan HTTP header provided it is defined in HTTP/1.X for some X.\n\n> Now it remains to be shown for what this would be useful.  (It also\n> remains to be seen whether Josh agrees with your definition :) )\n> \n> As given by your formal definition, the \"entity version\" (which should be\n> cassed something else, see below) can be trivially derived from the\n> message.  It just requires tables of all headers defined by the various\n> protocol versions, nothing else.  Therefore it is totally redundant.\n> \n\nIt requires tables of all general, response and entity headers and all\npossible values and extensions of those headers and the version of\nHTTP which defines each of them.  Each value and extension of each\nheader must be looked up in the table.  Whether this is \"trivial\" to\nimplement is a question I will leave to proxy implementors.  However,\nas a server implementor, I can tell you that the response version is a\ntrivial byproduct of a server producing the response.\n\n> > Given the entity:\n> > \n> > \n> >    HTTP/1.1 200 OK\n> >    Server: Blah\n> >    Date: Mon, 11 Aug 1997 19:33:55 GMT\n> >    Last-modified: Fri, 25 Jul 1997 13:43:15 GMT\n> >    Content-type: text/html\n> >    Content-length: 3052\n> > \n> >    ...3052 bytes of stuff...\n> > \n> > there is no way to tell if there is an implicit \"Connection: close\"\n> > header (which there would be if the entity version is 1.0) or an\n> > implict \"Connection: keep-alive\" (which there would be if the \n> > entity version is 1.1).  Thus in this case the entity version would\n> > contain information not derivable from the entity alone.  In other\n> > cases I think the entity version could, at least in principle, be\n> > derived from the entity.  Am I wrong?\n> \n> So your formal definition above has failed to describe what you mean.\n> There is one thing which cannot be trivially derived from the message\n> itself as given, and the definition does not account for it.\n> \n\nBut the definition *would* account for it if it were part of any HTTP\nresponse header.  If it is never a part of any response header the \nquestion of what the definition accounts for is completely moot.  You\nare picking nits.\n\n> Anyway, the property of the response which you express as an implicit\n> Connection token is unambiguously defined one way or the other, each\n> time such a response message is received, by the version number in the\n> status line.\n> \n\nThis is not correct.  It is not unambiguously defined by the the\nversion number of the response header (the \"status line\") as you\nassert.  Perhaps you meant it was determined by the version number of\nthe request header in the request which elicited the response.  I am\nnot sure since the request header contains no \"status\".\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Q: Methods and Headers",
            "content": "Where is defined what request-headers go with which request-methods ?\n\nA   PUT /something\n    Accept: Other/things\n\nwhould make not much sense for this request. \n \n        Bye   Brusi\n\n            by           E-Mail: ab2@inf.tu-dresden.de\n                         Tel.-priv: 0351-8499347 (Germany/Dresden)\n          \\____\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, John Franks wrote:\n\n> A revised formal definition:\n> The response version of a response is 1.N provided\n> every HTTP header or footer in the response is defined in HTTP/1.N and\n> at least one header or footer in the response is not defined in\n> HTTP/1.(N-1).  For the purposes of this definition a header is\n> an HTTP header provided it is defined in HTTP/1.X for some X.\n> \n> On Mon, 11 Aug 1997, Klaus Weide wrote:\n> > \n> > As given by your formal definition, the \"[response] version\"\n> > ... can be trivially derived from the\n> > message.  It just requires tables of all headers defined by the various\n> > protocol versions, nothing else.  Therefore it is totally redundant.\n> > \n> \n> Whether this is \"trivial\" to\n> implement is a question I will leave to proxy implementors.\n> as a server implementor, I can tell you that the response version is a\n> trivial byproduct of a server producing the response.\n>\n\n\nAnd I should have added that redundancy is a key ingredient of robustness.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Foteos Macrides wrote:\n\n[about Apache 1.[23] ]\n> Ah!  An implementation issue, not a terminology issue. :)\n> \n> It *is* an HTTP/1.1 server, so one would not predict a serious\n> problem during this transition, just as the shortcut had no apparent\n> problem for the old HTTP/1.0 proxies during the HTTP/0.9 -> HTTP/1.0\n> transition.  It would become a problem for an HTTP/1.1 -> HTTP/1.2\n> transition.  \n\nAccording to the \"New Features in Apache 1.2\" page, \n  Aside from the optional proxy module (which operates as HTTP/1.0),\n  Apache is conditionally compliant with the HTTP/1.1 [...]\nI am not sure what this means, i.e. does it still act as an HTTP/1.1\nserver when the proxy is used, aside from the version number question,\nand as far as a client needs to know.  For the concrete (although \ncurrently rather theoretical) question about accepting chunked encoding,\nwould the proxy decode a request body before forwarding it.\n\n> But one can also predict that it will be fixed ASAP,\n> and hopefully the current Apache servers will all have been replaced\n> by then. :)\n> \n> By \"serious\" problem I mean that sharing of worthwhile information\n> fails (just to be clear about my terminology :).\n> \n> Fote\n\n       Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">>>>> \"KW\" == Klaus Weide <kweide@tezcat.com> writes:\n\nKW> This leads to another question.  Does the proposed OPTIONS probe\nKW> actually *work* for an Apache/1.[23]* server acting as proxy?\n\n  I sent probes to www.agranat.com running our 1.1 server via\n  www.apache.org (Apache/1.3a2-dev).\n\n    Sent:\n        OPTIONS http://www.agranat.com HTTP/1.1\n        Host: www.agranat.com\n        Connection: close\n        Max-Forwards: 0\n        User-Agent: wwwreq/1.7\n\n    Response:\n        HTTP/1.0 200 OK\n        Date: Tue, 12 Aug 1997 01:28:12 GMT\n        Server: Agranat-EmWeb/R3_0alpha6\n        Allow: HEAD, GET, OPTIONS\n\n  it downgrades the request when it passes it on (according to the log\n  on our server it received a 1.0 request), and then downgrades the\n  response when it is returned.  Here is a direct probe of the same\n  origin server:\n\n    Sent:\n        OPTIONS * HTTP/1.0\n        User-Agent: wwwreq/1.7\n\n    Response:\n        HTTP/1.1 200 OK\n        Date: Tue, 12 Aug 1997 01:46:00 GMT\n        Server: Agranat-EmWeb/R3_0alpha6\n        Public: HEAD, GET, POST, TRACE, OPTIONS\n\n  [had to modify my test script to send 1.0 OPTIONS :-) ]\n  so we returned 1.1 to the 1.0 request, and Apache made it 1.0 again.\n\n  If I probe Apache directly with a 1.1 OPTIONS:\n\n    Sent:\n        OPTIONS * HTTP/1.1\n        Host: www.apache.org\n        Connection: close\n        User-Agent: wwwreq/1.7\n\n    Response:\n        HTTP/1.1 200 OK\n        Date: Tue, 12 Aug 1997 01:37:30 GMT\n        Server: Apache/1.3a2-dev\n        Content-Length: 0\n        Allow: GET, HEAD, OPTIONS, TRACE\n        Connection: close\n\n  it responds as 1.1.\n\n  I tried putting a 'Max-Forwards: 0' header in the probe through\n  Apache, and it ignored it and forwarded it anyway.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, John Franks wrote:\n> On Mon, 11 Aug 1997, Klaus Weide wrote:\n> > On Mon, 11 Aug 1997, John Franks wrote:\n> > \n> \n> You are correct that it depends on more than the entity headers.\n> It also depends on the response header and the general headers.\n> I don't really care what it is called. Let's call it a \"response\n> version\" for now.\n> \n> A revised formal definition:\n> \n> The response version of a response is 1.N provided\n> every HTTP header or footer in the response is defined in HTTP/1.N and\n> at least one header or footer in the response is not defined in\n> HTTP/1.(N-1).  For the purposes of this definition a header is\n> an HTTP header provided it is defined in HTTP/1.X for some X.\n> \n> > Now it remains to be shown for what this would be useful.  (It also\n> > remains to be seen whether Josh agrees with your definition :) )\n> > \n> > As given by your formal definition, the \"entity version\" (which should be\n> > cassed something else, see below) can be trivially derived from the\n> > message.  It just requires tables of all headers defined by the various\n> > protocol versions, nothing else.  Therefore it is totally redundant.\n> \n> It requires tables of all general, response and entity headers and all\n> possible values and extensions of those headers and the version of\n> HTTP which defines each of them.  \n\nThat is something *quite* different than what you wrote before (and\nabove).  I really understood that you were talking about just the\npresence or absense or a header field.  True, it didn't make much\nsense.  But does it make more sense by looking at all possible values\nand extensions?\n\n> Each value and extension of each\n> header must be looked up in the table.  Whether this is \"trivial\" to\n> implement is a question I will leave to proxy implementors.  However,\n> as a server implementor, I can tell you that the response version is a\n> trivial byproduct of a server producing the response.\n\nThis requires that for every token your server may put in a header, it\nhas to have the \"first-appeared-in-version-1.X\" information.  So for\nexample if the server sends a \"Content-Encoding: gzip\" header, that\ndoesn't make the \"response version\" 1.1; but if the server sends\n\"Content-Encoding: deflate\", it makes the \"response version\" 1.1.\nI wouldn't call that trivial.  I would call it absurd.\n\n[ snipped exchange about imaginary connection: close ]\n> > So your formal definition above has failed to describe what you mean.\n> > There is one thing which cannot be trivially derived from the message\n> > itself as given, and the definition does not account for it.\n> \n> But the definition *would* account for it if it were part of any HTTP\n> response header.  If it is never a part of any response header the \n> question of what the definition accounts for is completely moot.  You\n> are picking nits.\n\nNo, it seems I really don't understand what you mean.\nThat last peragraph is incomprehensible to me (except for the last\nsentence :) )\n\n> > Anyway, the property of the response which you express as an implicit\n> > Connection token is unambiguously defined one way or the other, each\n> > time such a response message is received, by the version number in the\n> > status line.\n> \n> This is not correct.  It is not unambiguously defined by the the\n> version number of the response header (the \"status line\") as you\n> assert.  Perhaps you meant it was determined by the version number of\n> the request header in the request which elicited the response.  I am\n> not sure since the request header contains no \"status\".\n\nActually I should have said \"unambiguously defined by the version number\nin the status line together with the version number which was used in the\nrequest\".  Meaning that if the version in the staus line is 1.0, then\nconnection: close is implied.\n\n ---\n\nSo far this has been a completely theoretical conversation about\nnonexistent things.  But, if I understand this much correctly, you are\nproposing some new kind of header field (\"Response-Version:\" ?).\nIf and when somebody actually tries to write this down as an RFC-usable\nalgorithm, it'll hopefully become clear whether it is totally useless\nor a really great thing.\n\n     Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Mon, 11 Aug 1997, Klaus Weide wrote:\n\n> On Mon, 11 Aug 1997, John Franks wrote:\n> > On Mon, 11 Aug 1997, Klaus Weide wrote:\n> > \n> > A revised formal definition:\n> > \n> > The response version of a response is 1.N provided\n> > every HTTP header or footer in the response is defined in HTTP/1.N and\n> > at least one header or footer in the response is not defined in\n> > HTTP/1.(N-1).  For the purposes of this definition a header is\n> > an HTTP header provided it is defined in HTTP/1.X for some X.\n> > \n> > > Now it remains to be shown for what this would be useful.  (It also\n> > > remains to be seen whether Josh agrees with your definition :) )\n> > > \n> > > As given by your formal definition, the \"entity version\" (which should be\n> > > cassed something else, see below) can be trivially derived from the\n> > > message.  It just requires tables of all headers defined by the various\n> > > protocol versions, nothing else.  Therefore it is totally redundant.\n> > \n> > It requires tables of all general, response and entity headers and all\n> > possible values and extensions of those headers and the version of\n> > HTTP which defines each of them.  \n> \n> That is something *quite* different than what you wrote before (and\n> above).  \n\nNo it isn't.\n\n> I really understood that you were talking about just the\n> presence or absense or a header field.  True, it didn't make much\n> sense.  But does it make more sense by looking at all possible values\n> and extensions?\n> \n> \n> This requires that for every token your server may put in a header, it\n> has to have the \"first-appeared-in-version-1.X\" information.  So for\n> example if the server sends a \"Content-Encoding: gzip\" header, that\n> doesn't make the \"response version\" 1.1; but if the server sends\n> \"Content-Encoding: deflate\", it makes the \"response version\" 1.1.\n> I wouldn't call that trivial.  I would call it absurd.\n> \n\nIs this a troll?  Or do you seriously not understand.  Neither gzip\nnor deflate is *defined* in any version of HTTP of which I am aware.\nNeither affects the message version.  By contrast the header value\n\"chunked\" for Transfer-encoding is defined in HTTP/1.1.  Its presence\nmakes the message version (at least) 1.1.\n\nThe work done by the 1.1 server *must* be done to construct valid\nresponses for both 1.0 and 1.1 clients.  The issue being discussed\nis must a proxy start over from scratch in doing this work.\nThe present spec requires a proxy to start over from scratch.\nI question the wisdom of that.\n\n> \n> So far this has been a completely theoretical conversation about\n> nonexistent things.\n\nOne can argue over whether message version in HTTP is a \"thing\" but it\ncertainly exists.  It may not be adequately defined.  I think it isn't\nand that is why I made an attempt at offering a definition.  I know\nthat Roy is fond of repeating that no message version exists beyond the\nmajor version number but this inconsistent with\ndraft-ietf-http-v11-spec-08.\n\nThat specification uses the terms \"HTTP/1.1 message\" and\n\"HTTP/1.1 request\" numerous times and uses \"HTTP/1.0 request\"\nonce.  The meaning of this document would certainly be changed\nif all occurences of 1.0 and 1.1 in these phrases were changed\nto 1.X.  \n\nIt might be useful to clarify what \"HTTP/1.1 message\" means as used in\nthe spec.  But I believe that one thing it does NOT mean is any\nmessage generated by a 1.1 application and containing a 1.1 version\nheader.  If that were the meaning then the sentence\n\n   \"Persistent connections are the default for HTTP/1.1 messages,\"\n\nfor example, would be false.  There are other uses of the phrase\n\"HTTP/1.1 message\" in normative prescriptions using MUST and SHOULD,\nwhich I don't believe the authors intended to apply to a message\nsent from a 1.1 application to a 1.0 application.\n\nSo we could define the \"message version\" to be 1.1 if the message\nis an HTTP/1.1 message as specified in draft-ietf-http-v11-spec-08.\nMore generally I suppose the message version of an HTTP/1.N message\nis 1.N.  But I don't find that enlightening.\n\nIn my opinion, it is precisely because we have tried to avoid defining\nwhat a 1.1 message version is (or equivalently, if you prefer, what an\nHTTP/1.1 message is) that the version issue has confused so many\npeople.  It must be nearly impossible to write an HTTP/1.1 application\nwithout at least conceptualizing an \"HTTP/1.1 message\".  The spec\nauthors seemed to have had the concept. In my view we would be much\nbetter off in terms of robustness and probably also efficiency if we\ncarefully defined it and if applications communicated the fact that\nthey were sending a 1.0 or 1.1 message.  It would also be nice if\nproxies could request any version which the server is capable of\ngenerating.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Tue, 12 Aug 1997, Josh wrote:\n\n> \n> I request to a 1.1 server:\n> GET http://www.foo.com/fo.html HTTP/1.0\n> \n> So, what version is this ?:\n> \n> HTTP/1.N 200 OK\n> Server: toaster/2.0 \n> Content-Length: 1024\n> Cache-Control: minfree=0\n> \n> \n\nIf the request is to a 1.1 server the response should be HTTP/1.1...\nI'll assume that is what you meant.  Then if \"Cache-Control: minfree=0\"\nis defined in HTTP/1.N the message version by my definition would be\n1.N.  A 1.M client being told that the message version is 1.N when M < N\nis not too useful, but it knows to expect headers it does not\nunderstand.  However, the 1.N version information might be of use to a\nproxy between the two.\n\nIf you meant what is the message version if (as I believe is currently\nthe case) \"Cache-Control: minfree=0\" is undefined in any version of\nHTTP then I don't know.  I guess a more careful definition would take\naccount of that \"experimental\" case.  \n\n> \n    Klaus Weide wrote:\n> > > As given by your formal definition, the \"[message] version\"\n> > > can be trivially derived from the\n> > > message.  It just requires tables of all headers defined by the various\n> > > protocol versions, nothing else.  Therefore it is totally redundant.\n> > > \n> > \n> Well, the table will presumably evolve over time, with\n> protocol revisions, extensions, what have you...\n> In the proxy, you wont have the same table to generate the\n> version as you do in the server..  One may\n> be older than the other and may compute a different\n> version..\n> If the computed version number was attached to the message/entity\n> there wouldnt be a need to recompute it or any ambiguity.\n> \n\nAnd at the very least the redundancy adds robustness.\n\n> \n> At this point, its academic.  I beleive that the response\n> version should be the entity/message version, it seems \n> that others feel it should be the server's version.\n> Currently, its the server's version\n> \n\nAgreed.\n\n> Despite which is better, it would take a serious compelling\n> breakage to change it, which isnt the case.  \n> \n\nAgreed.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "What is a &quot;message version&quot; (was Re: REVERSION",
            "content": "On Tue, 12 Aug 1997, John Franks wrote:\n> On Mon, 11 Aug 1997, Klaus Weide wrote:\n> \n> > On Mon, 11 Aug 1997, John Franks wrote:\n> > > On Mon, 11 Aug 1997, Klaus Weide wrote:\n> > > \n> > > The response version of a response is 1.N provided\n> > > every HTTP header or footer in the response is defined in HTTP/1.N and\n> > > at least one header or footer in the response is not defined in\n> > > HTTP/1.(N-1).  For the purposes of this definition a header is\n> > > an HTTP header provided it is defined in HTTP/1.X for some X.\n> > \n[ ... snipped liberally ... ]\n> > That is something *quite* different than what you wrote before (and\n> > above).  \n> \n> No it isn't.\n\nI probably don't understand what you mean by \"defined in HTTP/1.X\".\nYour use of the word \"defined\" seems to need an explanation.\n\n[ ... large cuts ... ]\n> Is this a troll?  Or do you seriously not understand.  Neither gzip\n> nor deflate is *defined* in any version of HTTP of which I am aware.\n> Neither affects the message version.  By contrast the header value\n> \"chunked\" for Transfer-encoding is defined in HTTP/1.1.  Its presence\n> makes the message version (at least) 1.1.\n\nFor the purpose of HTTP, \"deflate\" for Content-Encoding is defined in\nRFC 2068 3.5 (by reference to other documents).  Why is one relevant\nand the other isn't?  (Aside from the the fact that the header\nTransfer-Encoding, itself, is only defined in HTTP/1.1).\n\n> The work done by the 1.1 server *must* be done to construct valid\n> responses for both 1.0 and 1.1 clients.  The issue being discussed\n> is must a proxy start over from scratch in doing this work.\n> The present spec requires a proxy to start over from scratch.\n> I question the wisdom of that.\n\nRoy gave a list of 5 requirements a few days ago, extracted from RFC 2068,\nand only 4 of them apply to responses (maybe now that's only 3 left,\nif unsolicited \"100 continue\" is dead).  That doesn't look like much\nwork.  Or are there any other items which should be added to his list?\n\nIf you expect requirements to become drastically more difficult for\nversions >1.1, could you give an example of the kinds of changes you\nexpect and how a proxy might be helped by an explicit \"message version\"?\n\nBut yes, a proxy must start over from scratch doing this \"work\", \nbecause all those are hop-by-hop questions.  A compliant 1.1 proxy also\nhas to be a compliant 1.1 server to its clients, so it has to be\nresponsible for generating only valid responses.  Is your sugggestion\nabout allowing a proxy to switch to tunnel behavior under certain\nconditions?\n\n> > So far this has been a completely theoretical conversation about\n> > nonexistent things.\n> \n> One can argue over whether message version in HTTP is a \"thing\" but it\n> certainly exists.  \n\nI was trying to avoid yet another use of the word \"entity\"...\n\n> It may not be adequately defined.  I think it isn't\n> and that is why I made an attempt at offering a definition.  I know\n> that Roy is fond of repeating that no message version exists beyond the\n> major version number but this inconsistent with\n> draft-ietf-http-v11-spec-08.\n> \n> That specification uses the terms \"HTTP/1.1 message\" and\n> \"HTTP/1.1 request\" numerous times and uses \"HTTP/1.0 request\"\n> once.  The meaning of this document would certainly be changed\n> if all occurences of 1.0 and 1.1 in these phrases were changed\n> to 1.X.  \n> \n> It might be useful to clarify what \"HTTP/1.1 message\" means as used in\n> the spec.  \n\nYes.\n\n> But I believe that one thing it does NOT mean is any\n> message generated by a 1.1 application and containing a 1.1 version\n> header.  \n\nIn my reading that is what it DOES mean.  Searching through the text for\nthose strings, I found no place which required a different reading, other\nthan possibly the sentence you quote (below).  That sentence maybe should\nbe reformulated (I find the meaning unclear; maybe an implied \"to a\nHTTP 1.1 recipient\" is missing).\n\n> If that were the meaning then the sentence\n> \n>    \"Persistent connections are the default for HTTP/1.1 messages,\"\n> \n> for example, would be false.  There are other uses of the phrase\n> \"HTTP/1.1 message\" in normative prescriptions using MUST and SHOULD,\n> which I don't believe the authors intended to apply to a message\n> sent from a 1.1 application to a 1.0 application.\n> \n> So we could define the \"message version\" to be 1.1 if the message\n> is an HTTP/1.1 message as specified in draft-ietf-http-v11-spec-08.\n> More generally I suppose the message version of an HTTP/1.N message\n> is 1.N.  But I don't find that enlightening.\n> \n> In my opinion, it is precisely because we have tried to avoid defining\n> what a 1.1 message version is (or equivalently, if you prefer, what an\n> HTTP/1.1 message is) that the version issue has confused so many\n> people.  It must be nearly impossible to write an HTTP/1.1 application\n> without at least conceptualizing an \"HTTP/1.1 message\".  The spec\n> authors seemed to have had the concept. \n\nI see no clear indication of that, but we could just ask them.\n\n> In my view we would be much\n> better off in terms of robustness and probably also efficiency if we\n> carefully defined it and if applications communicated the fact that\n> they were sending a 1.0 or 1.1 message.  It would also be nice if\n> proxies could request any version which the server is capable of\n> generating.\n\nSo far it is a vague concept in search of a definition.\n\nYour last sentence points to the only practical appplication of this\nconcept which has been given so far (AFAIK) which might be useful.\n(There may be others, see my question above re tunnel.)  You gave as an\nexample that a proxy may want to prevent getting chunked responses.\nI don't know whether that would make the response a \"1.0 message\" since\nthe concept is not well defined.  But even if such a negotiation mechanism\nwere useful (maybe more so in the future, for some hypothetical feature of\nHTTP/1.2),  it doesn't have to be phrased in terms of \"message version\".\nA general mechanism to turn specific features on or off would be much more\nuseful than that, IMHO.\n\n\n      Klaus\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "Small clarification...the Apache proxy does downgrade responses to\nHTTP/1.0, but stores them in the cache with the original version,\nmeaning that it only screws up when the response is being delivered\nfrom the cache.\n\nNote that www.apache.org is *not* a proxy.  Apache when operating as\na proxy is a completely different beast (protocol-wise) than the\norigin server, even if they are listening to the same port.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">According to the \"New Features in Apache 1.2\" page, \n>  Aside from the optional proxy module (which operates as HTTP/1.0),\n>  Apache is conditionally compliant with the HTTP/1.1 [...]\n>I am not sure what this means, i.e. does it still act as an HTTP/1.1\n>server when the proxy is used, aside from the version number question,\n\nNope.  A client talking to it as a proxy will only see an HTTP/1.0\nproxy (and that's all it gets in the way of functionality).  It always\nsends HTTP/1.0 in requests, so receiving chunked is not a problem.\nIt will respond with 411 if it receives chunked from a client.\n\nHowever, a properly functioning HTTP/1.0 proxy is going to behave\nvery much like an HTTP/1.1 proxy, aside from the lack of a Via field.\nFor example, the following is going through a local test proxy:\n\n   OPTIONS http://www.apache.org/ HTTP/1.1\n   Host: www.apache.org\n\n   HTTP/1.0 200 OK\n   Date: Tue, 12 Aug 1997 23:24:41 GMT\n   Server: Apache/1.3a2-dev\n   Cache-Control: max-age=86400\n   Expires: Wed, 13 Aug 1997 23:24:41 GMT\n   Content-Length: 0\n   Allow: GET, HEAD, OPTIONS, TRACE\n   Connection: close\n\nThat is an HTTP/1.1 UA talking to an HTTP/1.0 proxy, forwarding an\nHTTP/1.0 request to an HTTP/1.1 server (www.apache.org), which returns\nan HTTP/1.1 response, which is then downgraded to HTTP/1.0 when it goes\nback through the proxy.  So, although the Apache proxy does not puke on\nOPTIONS, it does return HTTP/1.0.\n\nOn the flip side, however, if the proxy has been configured to deny\nthe request for the URL, then the error response will be coming from\nthe origin server and not the proxy. :(   E.g., the same without perms:\n\n   GET http://www.apache.org/ HTTP/1.1\n   Host: www.apache.org\n\n   HTTP/1.1 403 Forbidden\n   Date: Tue, 12 Aug 1997 23:17:56 GMT\n   Server: Apache/1.2.3-dev\n   Transfer-Encoding: chunked\n   Content-Type: text/html\n\nLikewise, it is the origin server that answers requests for \"*\".\n\n   OPTIONS * HTTP/1.1\n   Host: www\n\n   HTTP/1.1 200 OK\n   Date: Tue, 12 Aug 1997 23:39:55 GMT\n   Server: Apache/1.2.3-dev\n   Content-Length: 0\n   Allow: GET, HEAD, OPTIONS, TRACE\n\nwhich is, ummm, kind of a bummer if you are using OPTIONS to determine\nif the proxy is HTTP/1.1 compliant.\n\nThis brings up a general problem I hadn't thought about before, which\nis how does the proxy indicate that it is answering as itself and not\non behalf of the origin server?  It would look like an HTTP/1.1\nresponse, but without the last Via entry.  Perhaps we need a way to\nindicate that using another field, or a special value for Via.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">The response version of a response is 1.N provided\n>every HTTP header or footer in the response is defined in HTTP/1.N and\n>at least one header or footer in the response is not defined in\n>HTTP/1.(N-1).  For the purposes of this definition a header is\n>an HTTP header provided it is defined in HTTP/1.X for some X.\n\nIt sounds to me like you are trying to solve the PEP problem within\na single version number, which simply doesn't work in HTTP.\n\nThere are hundreds of possible extensions that may require understanding\non the part of the recipient, but the mechanism for indicating that\nshould identify the required extensions, not come up with some\nmin/max number based on the HTTP-version.  The HTTP-version semantics\nwere restricted to the \"message level\" -- what is necessary for two\nHTTP applications to communicate -- because anything more than that\nwas clearly a rathole and better solved by a Mandatory header field\n(present in the HTTP spec back in December 1994) or Dave Kristol's\nextension mechanism which later evolved into PEP.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": ">On the flip side, however, if the proxy has been configured to deny\n>the request for the URL, then the error response will be coming from\n>the origin server and not the proxy. :(\n\nWhat I meant by that is the side of the Apache proxy that thinks it\nis an HTTP/1.1 origin server, rather than the proxy code itself.\n\nBTW, in case I didn't mention it, I think combining a proxy and an origin\nserver into a single application is a terrible idea.  Don't let the\nCERN example fool you into supporting it, even if the ex-CERN users\nare constantly begging at your door.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: REVERSIO",
            "content": "On Tue, 12 Aug 1997, Roy T. Fielding wrote:\n\n> >The response version of a response is 1.N provided\n> >every HTTP header or footer in the response is defined in HTTP/1.N and\n> >at least one header or footer in the response is not defined in\n> >HTTP/1.(N-1).  For the purposes of this definition a header is\n> >an HTTP header provided it is defined in HTTP/1.X for some X.\n> \n> It sounds to me like you are trying to solve the PEP problem within\n> a single version number, which simply doesn't work in HTTP.\n> \n> There are hundreds of possible extensions that may require understanding\n> on the part of the recipient...\n\nI don't think what I have been saying is that difficult to understand.\n\nAn HTTP/1.1 application will frequently, but not always, send very\ndifferent messages in response to requests which have header versions\n1.1, 1.2, 1.3, but which are otherwise identical.  I believe this is\nthe assumption of the spec authors because the current versioning\nsemantics is useless otherwise.  If a 1.1 application will always send\nthe same response to a 1.1, 1.2, or 1.3, etc. request, it has no need to\nknow whether the capabilites of the requestor are greater than 1.1.\n\nHowever, I believe the most common case will be that a 1.1 application\nwill send identical responses to 1.1, 1.2 or 1.3 requests, if those\nrequests differ only in their version number.\n\nThere is currently no way for a 1.1 application to respond to a 1.2\nrequest and say, \"This is identical to the response I would have sent\nto a 1.1 request,\" or \"this is the response I send to 1.2 requests and\nis not the same as I would send to an identical 1.1 request even\nthough I am a 1.1 application.\"\n\nIt is pointless to argue about how difficult it is for a server\nto figure out this information, since it *must* figure it out to\nfunction.\n\nI think this information would be useful to communicate.  You can\ncertainly disagree with that, but I don't understand claims that it\ndoes not exist.\n\nWhether this information would aid proxy implementors is probably a\nquestion best left to proxy implementors.  I believe that it would be\nmore important if applications were permitted to send requests for\nlower versions than the application's version, which is a capability I\nwould like to see.\n\nHowever, in my opinion, communicating this information would at the\nvery least improve robustness.  Even formally specifying this\ninformation and clearly distinguishing it from the header version\n(which could be given a more descriptive name) would prevent most of\nthe confusion that a number of implementors and participants on this\nlist (myself included) have apparently suffered.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Q: Methods and Headers",
            "content": "Accept headers (and content negotiation in general) can be used on any\nrequest to indicate the requestors preferences for responses to the\nrequest. So an accept-language on a PUT, for example, could indicate the\npreferred languages for error messages if the PUT were to fail.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "FW: revised trusted cookie spe",
            "content": "Delivery of this message failed the first time... Here is a resend:\n\n\n\nHere is the spec:\n\n \n\n\nAs promised:\n\nSummary of changes:\n\nTries to be consistent with vocabulary work of the IPWG at the CDT and \nthe Vocab and Architecture working groups or the P3 project (aka OPS). \nDoes NOT incorporate PICS 2.0 or PICSrulz.  Assumes PICS 1.1.\n\nUse of the PICS-Label header for trust labels instead of \nSet-PICS-Cookie.\n\nUses W3C Digital Signatures working draft for signing of trust \nlabels.\n\nDiscusses user privacy preferences and server privacy practices.\n\nIncludes four well-known privacy practice ratings.\n\nDOES NOT REQUIRE A TRUST AUTHORITY... but... to be determined by \nmarket?\n\nDistinguishes between no exchange of information and\nno exchange of personally identifying information.\n\nRecommends default user privacy preferences for acceptance of cookies \nfrom verifiable transactions if server practice is noexchange, \nanonymousexchange, or noshare or unrated.\n\nRecommends a default user privacy preference for acceptance of cookies \nfrom unverifiable transactions only if server practice is noexchange \nor anonymousexchange and trust label has been digitally signed by a \nrecognized trust authority.\n\nFixes numerous errors by author in first draft pointed out by DMK and \nothers.\n\nMore explicit references to draft-ietf-http-state-mgt-mec-03.\n\nDOES NOT include:\n  Attribute group or attribute level rules for cookie handling.\n\n  Grammar for expressing who has what authorization to modify cookie \ndata,\n  duration of data on the server, consequences of the data, etc.\n\n  Revocation of server-collected data, etc.\n\n  Stuff coming in PICS-2.0, and P3.\n\n  Level of enforcement of trust authorities; to be established by \nmarket?  Audits,\n  selective audits with contracts and penalties...\n\n\n\n\n\n\n\n\n\n\nHTTP Working Group                      Daniel Jaye\nINTERNET DRAFT                    Engage Technologies\n\n\n<draft-ietf-http-jaye-trust-state-01.txt>\nAugust 12, 1997                 Expires February 12, 1998\n\n\n       HTTP Trust Mechanism for State Management (Rev 1)\n\n\n\n\n             Status of this Memo\n\n   This document is an Internet-Draft.  Internet-Drafts are\n   working documents of the Internet Engineering Task Force\n   (IETF), its areas, and its working groups.  Note that other\n   groups may also distribute working documents as Internet-\n   Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time.  It is inappropriate to use Internet-\n   Drafts as reference material or to cite them other than as\n   ``work in progress.''\n\n   To learn the current status of any Internet-Draft, please\n   check the ``1id-abstracts.txt'' listing contained in the\n   Internet- Drafts Shadow Directories on ftp.is.co.za (Africa),\n   nic.nordu.net (Europe), munnari.oz.au (Pacific Rim),\n   ds.internic.net (US East Coast), or ftp.isi.edu (US West\n   Coast).\n\n   This is author's draft 2.02.\n\n\nABSTRACT\n\nHTTP TRUST MECHANISM PROPOSAL FOR STATE MANAGEMENT\nMarch 30, 1997\n\n1. ABSTRACT\n\nThis document specifies an addition to the state management protocol\nspecified in draft-ietf-http-state-man-mec-03[Kristol].  The intent is\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 1]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\nto provide a mechanism that allows user agents to determine the privacy \npractices of a server and to accept or reject cookies based on those \npractices.  Allowing the user to establish preferences for how to handle\ncookies based on the server's practices provides a practical mechanism \nto provide users control over the privacy implications of cookies.\n\nTo provide verification of server privacy practices, we assume the\nexistence of one or more independent Trust Authorities.  The authority \nestablishes PICS ratings representing server privacy practices. It then \nissues trust labels, in the form of digitally signed PICS labels, to \norganizations for specific domains and paths based on the server privacy \npractices.  The Trust Authority must be able to audit domains to \nverify their adherence to a given level.  Passing these trust labels \nalong with cookies allows the user agent to support cookie handling \npreferences based on trusted privacy practices.\n\nThis document describes how PICS-headers are used in conjunction with\nSet-Cookie or Set-Cookie2 headers in [Kristol] to provide trust labels\nto certify the privacy practices of servers regarding cookies.\n\n\n2. TERMINOLOGY\n\nThe terms user agent, client, server, and origin are used as in [Kristol].\nThe terms domain-match, verifiable transaction and unverifiable \ntransaction are defined in [Kristol], and those definitions are also used \nhere.\n\nThe term trust label is used to mean a digitally signed PICS label[PICS].\nThe term Trust Authority represents the authority who established the \nvalid PICS label values and assigns digitally signed trust labels to \ndomains.\n\n\n3. OUTLINE\n\nThe server sends a Set-Cookie and/or a Set-Cookie2 header to the user \nAgent along with a PICS-Label header containing the trust label.  The \nuser agent may use the well-known public key of the Trust Authority to \ndecrypt the signature of the trust label to verify the identity and \npractices of the server.  The user agent may then use that information \nto guide the acceptance or rejection of the cookie.\n\n3.1  Syntax: General\n\nThis specification describes how the PICS-Label header, described in \n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 2]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\n[PICS], is used to convey the privacy practices of the server to the user\nagent.  The syntax of the state management header, Set-cookie2, is \nspecified in [Kristol].  The new PICS-Label header syntax is specified \nbelow:\n\ntrusted-cookie  = 1*Set-cookie2 trust-label\ntrust-label     = \"PICS-Label:\" labellist\n\n\"labellist\" is as specified in the PICS 1.1 label syntax in [PICS], except as extended by the digital signatures working draft [DSIG], some options are not used, and we require some of the optional elements, as specified below.  The trust-label applies to the immediately preceding Set-Cookie2 label. \n\nWe indicate here the parts of the PICS label syntax we use, with the \nchanges to indicate required options. We eliminate those parts not \nused here. We allow only options on the labels themselves, not the \ndocument, since these labels are specifically Trust-Labels.\n\nlabellist       = \"(\" version 1*service-info \")\"\nversion         = \"PICS-1.1\"\nservice-info    = serviceID \"label\" 1*label\nserviceID       = quotedURL\nlabel           = labelattr \"ratings\" \"(\" privacy-practice \")\" [sigblock]\nlabelattr       = \"by\" quotedname\n                  \"gen\" boolean\n                  \"for\" quotedURL\n                  \"on\" quoted-ISO-date\n                  \"exp\" quoted-ISO-date\nprivacy-practice  \n                = \"noexchange 1\"\n                | \"anonymousexchange 1\"\n                | \"noshare 1\"\n                | \"thirdpartyexchange 1\"\n                | rating\n\n\"quotedname\", \"quotedURL\", \"rating\",  and \"quoted-ISO-date\" are as defined\nin the PICS specification [PICS].  ServiceID references a quoted URL that\nrepresents the Trust Authority.  The labelattr clauses are optional in the\nPICS spec, but all are required here. \"for\" is the URL or root URL for \nwhich this label applied.  \"by\" is the email address of the issuing trust \nauthority.  The \"gen\" boolean indicates whether the label is for only the \nURL in the \"for\", or for all URL's for which the specified one is a \nprefix.  (\"True\" indicates subdomains included.)  \"on\" is the date the \nlabel was issued.  \"exp\" is the date the label expires.  SigBlock is the \ndigital signature extension as described in the digital signature working\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 3]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\ndraft[DSIG].  \n\nFour well-known privacy-practice values are described here to provide \nrecognized values that should be handled by user agents.    \n\nThe \"noexchange 1\" rating indicates that the trust authority has verified\nthat the server will not use the cookie to collect or transmit your \npersonal information.  \n\nThe \"anonymousexchange 1\" rating indicates that the trust authority has \nverified that the server will not use the cookie to collect or transmit \npersonally identifying information (e.g., name, address, telephone number,\nemail address, etc.) but may collect anonymous or aggregated personal \ninformation (e.g., gender, geographic region, approximate age, derived \ndata such as clickstream, etc.) or implicit information (such as web usage\npatterns) as long as it will never be associated with personally \nidentifying information.  The server may collect IP Addresses but they\nmust not be associated with personal information or implicit information\nwith personally identifying information to be elegible for this rating.\n\nThe \"noshare 1\" rating indicates that the trust authority has verified\nthat the server may use the cookie to collect or transmit personally \nidentifying information (e.g., name, address, telephone number, email \naddress, etc.) but will never share that information with companies \nother than the company to which the user provided the information.  \n\nThe \"thirdpartyexchange 1\" rating indicates that the trust authority has \nverified that the server may use the cookie to collect or transmit \npersonally identifying information (e.g., name, address, telephone \nnumber, email address, etc.) and may or will share that information with \nthird parties.  The trust authority should provide information about the \npurposes for which that information is being used.\n\nThe SigBlock must contain the SigCrypto token within the SigData block.  \nThe SigCrypto token must contain the encrypted trust-label-data described\nbelow.\n\ntrust-label-data = for-URL on-date exp-date privacy-practice\n\nfor-URL          = quotedURL\nexp-date         = quoted-ISO-date\n\nfor-URL is the URL to which the privacy practice applies as listed in the \n\"for\" attribute in \"labelattr\".  Exp-date is the expiration date of the \ntrust label as listed in the \"exp\" attribute of \"labelattr\".\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 4]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\nAll other items above are as described in the PICS label syntax [PICS] or \nin the Digital Signatures working draft [DSIG].\n\n3.2Server Role\n\nA server communicates its privacy practices by sending an unsigned or \nsigned trust label immediately following the cookie header(s).  The trust \nlabel is assumed to apply to all cookies in the response that match the \ndomain and path of the trust label according to the matching rules for \nmatching cookies to request URI's described in [Kristol].\n\nAny server wishing to provide digitally signed trust labels must request \nsuch labels from a Trust Authority.  The Trust Authority here must have \nthe ability to evaluate the server domain and determine the trust rating \nfor which a label will be issued. That evaluation takes place outside the\nprotocol described here, as does the actual granting of the labels to the\norigin server.\n\nThe labels should expire no more than twelve months and no less than one\nmonth after they are issued.  The server should store the trust labels and\nonly request a new trust label from the Trust Authority when the current\ntrust label is about to expire.\n\n3.3 User Agent Role\n\nThe user agent receives a cookie headers followed by a trust labels from \nan origin server.\n\n3.3.1 Interpreting the trust-label\nUser agents interpret cookies as described in RFC 2109.  In addition \nto the cookie attributes, the user agent must now interpret the \ntrust labels as well.  If the user receives a trust label with a \nrecognized privacy practice rating, it is assumed to be a trust label \nfor the cookies in the response as long as the domain and path of the \ncookie match the domain and path of the trust label according to the \nmatching rules described in [Kristol].  To help verify the \ntrustworthiness of the server, the user agent may look for a digital \nsignature and use the trust authority's well known public key to \ndecrypt the trust-label-data from the SigCrypto term.\n\nThe user agent obtains that public key outside this protocol.  Given \nthat we expect a few well-known Trust Authorities, the user agent \nimplementer should store public keys from standard trust authorities \nto avoid extra round trips. \n\nThe digital signature is valid if the decrypted trust-label-data \n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 5]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\nsatisfies the following criteria:\n\n1) that the domain portion of the URL specified in the for-URL attribute \ndomain matches the domain of the cookie according to the matching \nrules as sort forth in [Kristol];\n\n2) that the path portion of the URL specified in the for-URL attribute is\ncompatible with the path of the cookie.  If the trust label is generic, \nthen the for-URL path must be a prefix of the cookie's path.  If the \ntrust label is not generic, then it must match exactly. \n\n3) that the \"on-date\" attribute of the trust label is less than or equal\nto the current date;\n\n4) that the \"exp-date\" attribute of the trust-label-data is greater than\nor equal to the current date.\n\nIf the digital signature is invalid, then the cookie should be rejected.\n\nIf the user agent is set to accept all cookies then all trust label \nprocessing can be skipped.\n\n3.3.2Accepting or rejecting Cookies\nIn addition to the rules for rejecting cookies specified in [Kristol], a\nuser or a user-designated agent should be able to designate preferences\nfor accepting or rejecting cookies based on the privacy-practice of the \nserver, whether the transaction is verifiable or unverifiable, and whether\nthe privacy-practice is signed by a recognized Trust Authority.  \n\nUser Agents should have default preferences that allow \"noexchange 1\", \n\"anonymousexchange 1\", and \"noshare 1\" rated cookies to be accepted from\nverifiable transactions and that allow \"noexchange 1\" and \n\"anonymousexchange 1\" rated cookies to be accepted from unverifiable \ntransactions.\n\nThe User Agent should have a default preference to reject \"third-party-\nexchange\" cookies from unverifiable transactions.  \n\nFor example, a user may wish to accept cookies rated anonymousexchange by\na recognized trust authority, rather than relying on an unsigned trust \nlabel or a trust label signed by an unrecognized entity.\n\n3.3.3  User intervention\nThe user agent may prompt the user to verify that it wishes to reject a \ncookie in certain conditions where the cookie is being rejected based on a \ndefault preference.\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 7]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\nUser agents that solicit user input for cookie handling may wish to \ndisplay the URL of the rating service to better inform the user of the \nmeaning of the privacy ratings for the server.\n\n3.3.4  Cookie request header syntax\nThe syntax for the Cookie request header has not been modified.\n\n3.4  Trust Authority Role\n\nThe Trust Authority referred to in this document must be a neutral third\nparty that can be trusted to accurately characterize the privacy\nbehavior of web sites.  The issuing of trust labels occurs outside the\nscope of this protocol, but the protocol depends on user trust in that\nauthority.  The Trust Authority must understand the scope in which a\ntrust label applies to ensure that for all situations in which the trust \nlabel would be deemed to be applicable, the server(s) are in fact \noperating in accordance with the specified privacy rating.\n\n3.4.1 Issuing trust labels\nOn receiving a trust label request, the authority should verify the \nprivacy practices of the site requesting the trust label and issue the \nappropriate trust label.  To issue the trust label, the trust authority \nassembles the trust-label-data, it canonicalizes whitespace for the \ntrust-label-data, and it encrypts the trust-label-data for the site \nrequest using its private key and the algorithm specified in the attribution of the digital signature.  The encryption method must be a public-private key pair with a well-known public key to eliminate round-trips to the trust authority. \n\n3.4.2 Revocation of trust labels\nTrust labels must have expiration dates.  When a trust label is issued,\nthe Trust Authority must receive agreement from the requesting\norganization that the privacy practices for which the trust label was \nassigned will be maintained until the trust label expires, the domain \nbecomes inactive, or those cookies are no longer set or examined by the \norganization's servers.\n\n3.4.3 Discovery of privacy-practice ratings\nPrivacy-practice ratings are well known values established by each Trust Authority, several of which are proposed in this document.\n\n\n4. EXAMPLES\n\n4.1 Example 1\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 7]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\n1.  User Agent preferences:\n\n    In this example, the user agent has a preference for automatically\n    accepting cookies from domains that have valid ratings of \n    \"anonymousexchange 1\" or \"noshare 1\".\n\n2.  User Agent -> Server\n\n      POST /acme/login HTTP/1.1\n      Host: www.acme.com\n      [form data]\nUser identifies self via a form.\n\n3.  Server -> User Agent\n      HTTP/1.1 200 OK\n      Set-Cookie: Customer=\"WILE_E_COYOTE\"; Max-Age = 94608000; \n        Version=\"1\"; Path=\"/acme\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"paranoid@aaa.org\" gen true\n        for \"http://www.acme.com/\"\n        exp \"1997.12.31T23:59-0000\" \n        ratings (noshare 1))\n\n    A cookie that includes the users identity and an unsigned PICS\n    label header are sent back to the user agent with the request.  The \n    Cookie is accepted because rating \"noshare 1\" is acceptable according \n    to the privacy preferences of the user agent.\n\n4.2 Example 2\n\n1.  User Agent preferences:\n\n    In this example, the user agent has a preference for automatically \n    accepting cookies that are rated \"noexchange 1\", \n    \"anonymousexchange 1\", or \"noshare 1\" or from cookies in \n    unverifiable transactions that are rated \"noexchange 1\" or \n    \"anonymousexchange 1\" by www.aaa.org.\n\n2.  User Agent -> Server\n\n      POST /acme/login HTTP/1.1\n      Host: acme.com\n      [form data]\n\n    User requests page with embedded IMG SRC reference to\n    \"http://www.roadrunnermaps.com/cgi-bin/maps?TER=deserts&FE=cliffs\"\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 8]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\n3.  Server -> User Agent\n      HTTP/1.1 200 OK\n      Set-PICS-Cookie: Customer=\"0000000123\"; Max-Age = 94608000; \n        Version=\"1\"; Path=\"/birds\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"paranoid@aaa.org\" gen true\n        for \"http://www.acme.com/\"\n        exp \"1997.12.31T23:59-0000\" \n        ratings (noshare 1))\n\n    A Cookie reflecting the users identity is transmitted with an \n    Unsigned trust label back to the user agent.  The Cookie is accepted\n    by user agent because the rating \"noshare 1\" is compatible with the \n    user agent privacy preference.\n\n4.  User Agent -> Server\n\n      GET cgi-bin/maps?TERR=deserts&FEAT=cliffs HTTP/1.1\n      Host: roadrunnermaps.com\n\n    User requests an image via CGI script from a third party map provider.\n    This is an unverifiable transaction.\n\n5.  Server -> User Agent (unverifiable transaction)\n      HTTP/1.1 200 OK\n      Set-PICS-Cookie: Customer=\"0000000123\"; Max-Age = 94608000; \n        Version=\"1\" \n      PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n        by \"paranoid@aaa.org\" gen true\n        for \"http://www.acme.com/\"\n        exp \"1997.12.31T23:59-0000\" \n        extension \n          (optional \"http://www.w3.org/PICS/DSig/sigblock-1_0.html\"\n            (\"AttribInfo\" \n              (\"http://www.w3.org/PICS/DSig/X509.html\" \n                \"base64-x.509-cert\"))\n            (\"Signature\" \"http://www.aaa.org/trust.html\" \n              (\"byName\" \"aaapublickey\") \n              (\"SigCrypto\" \"8E53B19D35A3F19D35A38930E53FD35A7B215B2158\")))\n        ratings (anonymousexchange 1))\n\n    A cookie containing the user's system generated id number is \n    transmitted with a signed label back to user agent.  The cookie is \n    accepted by user agent because rating \"anonymous\" is acceptable to the\n    user agent's privacy preferences for cookie policy for unverifiable \n    transactions.\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 9]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\n5. SECURITY CONSIDERATIONS\n\n5.1 Revocation of trust labels\n\nA site could receive a trust label for a particular trust level rating \nand later change its policies before the trust label has expired.  To \naddress this need, Trust Authorities should execute agreements with \ntrust label recipients to provide legal remedies to discourage this \nbehavior.\n\n\n6. SUMMARY\n\nThis document presents an extension to the state management protocol \ndefined in RFC2109.  It describes only changes to that protocol. Any \nparts of the state management not explicitly described here are assumed\nto remain as defined in RFC 2109.\n\nThe protocol described here allows a user agent to verify that the \norigin server is using cookies in a manner consistent with the privacy \nexpectations of the user, by providing a certificate. or trust label, \nissued by a trusted authority.\n\n\n7. ACKNOWLEDGEMENTS\n\nThis document represents contributions by Toby Bloom, as well as input \nfrom Dave Kristol, Yaron Goland, Jonathan Stark, and Dan Connolly.\n\n\n8. REFERENCES\n\n[PICS] Jim Miller et al, PICS Label Distribution Label Syntax and \nCommunication Protocols, Version 1.1, REC-PICS-labels-961031\nhttp://www.w3.org/PICS/labels.html\n\n[Kristol] Kristol, David M., HTTP State Management Mechanism (rev 1). \nInternet Draft <draft-ietf-http-state-man-mec-03.txt>\nftp://ietf.org/internet-drafts/draft-ietf-http-state-man-mec-03.txt\n\n[DSIG] Philip DesAutels et al, DSIG 1.0 Signature Labels, Version 1.0, \nWD-DSIG-label-970605\nhttp:/www.w3.org/TR/WD-DSIG-label.html/\n\n\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 9]\n\n\n\n\n\nINTERNET DRAFT HTTP Trust Mechanism for State Mgt (Rev1) August 12, 1997\n\n\n\n9. AUTHOR'S ADDRESS\n\nDaniel Jaye \nEngage Technologies \n100 Brickstone Square \n1st Floor\nAndover, MA 01810\n\ndjaye@engagetech.com\n508 684-3641 voice\n508 684-3636 fax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJaye            draft-ietf-jaye-trust-state-01.txt            [Page 11]\n\n\n\n"
        },
        {
            "subject": "Short report, HTTP WG meetin",
            "content": "The HTTP working group is making good progress toward completing\nthe transition from Proposed to Draft Standard. At the meeting, we\nreviewed the open issues and made good progress on many of them.\nSome proposals for extensions to HTTP will result in 'Experimental'\nRFCs.A \"HTTP/1.1 interoperability test\" is being planned for testing\nimplementations (browser, proxy, origin server) against each other.\n\nRFC 2109 (state management) has technical difficulties; it seems likely\nthat we will recycle with a new draft that will be closer to current\n(interoperable) implementations, and with a revised (but not weakened)\ndiscussion of privacy considerations.\n\nContent negotiation has a broader scope than HTTP, and some of the work\non it will result in work that may extend beyond HTTP-WG; others will\nresult in Experimental RFCs. Our revised charter calls for being mainly\ncomplete with HTTP/1.1 by the December IETF, and with the possibility\nthat the group may close or become dormant soon after.\n\nLarry Masinter\nHTTP-WG Chair\n\n\n\n"
        },
        {
            "subject": "&quot;virtual connect-athon&quot",
            "content": "With regard to a discussion of MHTML testing, \n Einar Stefferud wrote:\n\n> Should we consider organizing some kind of virtual\n> connect-a-thon over the Internet?\n\nand Jamie Zawinski replied:\n\n> That's worked nicely for S/MIME.  The end result was a chart of\n> products known to interoperate with each other.  (Of course, you\n> need more than two participants for it to make sense...)\n\nMany folks are interested in organizing a \"virtual connect-a-thon\" for\nHTTP/1.1. Do you have any pointers to people who have run such a thing?\n\nGuidelines, anecdotes, descriptions of actual events, etc. would be\nuseful. It's easy to speculate, but a little history would help.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "At the HTTP working group meeting, I took off my \"virtual\" chair's hat\nand put on a \"opinionated working group member\" hat, and ranted about\ncommentURLs. I want to extend that rant:\n\n\nI'm opposed to commentURLs in cookies.\nI'm opposed to comment strings in cookies.\nI'm opposed to trusted cookies, too.\n\nI believe that we should recommend \"browsers should not return\ncookies to sites that are not trusted with private information\"\nand that trust can be established using a variety of means:\n(a) the site sent you the cookie (b) you have some other way of \nestablishing a site's privacy policy.\n\nEstablishing the privacy policy might be accomplished by\nusing a PICS-Label or by obtaining it via some other link,\nhaving the privacy rating INSIDE THE DOCUMENT that contains\nthe links (\"we assert that this document only links to sites\nwith the following privacy policy\") or any of a variety of\nmeans outside the HTTP protocol.\n\nBut assertions of privacy policies do not belong *inside* the\nstate management mechanism.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>At the HTTP working group meeting, I took off my \"virtual\" chair's hat\n>and put on a \"opinionated working group member\" hat, and ranted about\n>commentURLs. I want to extend that rant:\n>\n>I'm opposed to commentURLs in cookies.\n>I'm opposed to comment strings in cookies.\n>I'm opposed to trusted cookies, too.\n>\n>I believe that we should recommend \"browsers should not return\n>cookies to sites that are not trusted with private information\"\n>and that trust can be established using a variety of means:\n>(a) the site sent you the cookie (b) you have some other way of \n>establishing a site's privacy policy.\n>\n>Establishing the privacy policy might be accomplished by\n>using a PICS-Label or by obtaining it via some other link,\n>having the privacy rating INSIDE THE DOCUMENT that contains\n>the links (\"we assert that this document only links to sites\n>with the following privacy policy\") or any of a variety of\n>means outside the HTTP protocol.\n>\n>But assertions of privacy policies do not belong *inside* the\n>state management mechanism.\n\nThis self-avowed opinionated rant is invalid because it retains\nthe myopic view that the commentURL is simply for legalistic statements\nabout a site's privacy policy.  Please read the actual draft:\n\nCommentURL=\"http_URL\"\n     Optional.  Because cookies can be used to derive or store private\n     ^^^^^^^^\n     information about a user, the CommentURL attribute alows an origin\n     server to document how it intends to use the cookie.  The user can\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n     inspect the information identified by the URL to decide whether to\n     initiate or continue a session with this cookie.\n              ^^^^^^^^^^^\n\nStatements such as \"This cookie maintains your display preferences.\", or\n\"This cookie servers as a shopping cart.\",  or \"This cookie performs\ntrade secrets which cannot be revealed to you.\", or \"This cookie serves\nsolely to track you from here to eternity.\" would be more helpful, not\nsimply for decisions on whether to accept a cookie, but even more\nimportantly for decisions during \"clean ups\" or cookie replacements\ndue to limited resources -- and their inclusion is \"Optional.\"\n\nProviders should not be prevented from making such information\navailable in a manner which allows charset/language negotiation and\nvia a simple, consistent UA mechanism, rather than requiring users to\nhunt around in unspecified documents for it, with no assurance that\nwhat's said in some such documents applies to a particular cookie.\n\nYou are trying to block something which has been discussed at\nlength already, is wanted by some implementors, and need not be used\nby those who don't want it.\n\nIn your own immortal words, \"Please stop!\" :) :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "commentURL, again",
            "content": "Consensus in Munich (which does not include many concerned with this\nissue) was that comment/commentURL is to be taken out of the spec, which I\nbelieve is the right thing to do. How is this formally done? Let's do it.\n\nWhy it shouldn't be in RFC 2109:\n- RFC 2109 is concerned with the functionality of cookies/stateful\nsessions.\n- It is not the protocol's place to include this type of explanatory\ninformation to the user (if it belongs anywhere, it belongs in another\nspec related to privacy concerns and cookies; as suggested by Larry\nMisinter).\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": ">         This self-avowed opinionated rant is invalid because it retains\n> the myopic view that the commentURL is simply for legalistic statements\n> about a site's privacy policy. \n\nFoteos,\n\nI know what the Comment and the CommentURL are *for*. There are a lot of\nthings that they are useful *for*. But just because a salad-shooter and\nan apple-corer are very useful kitchen appliances doesn't mean that we\nshould add them to HTTP.\n\nI'm sure it is dandy that a CommentURL *could* tell you \"This cookie\nmaintains your display preferences\" and \"This cookie performs\ntrade secrets which cannot be revealed to you.\" It could also tell me\n\"This cookie was designed by that great web site designer Joe Coolsite\nwho will build cookies for you too!\" A CommentURL could include\nadvertisements for local bakeries! It could shine your shoes! It could\nwash your dog!\n\nNo, the problem isn't that Comments and CommentURLs don't have\nconceivable uses.\n\n>        Providers should not be prevented from making such information\n> available in a manner which allows charset/language negotiation and\n> via a simple, consistent UA mechanism, rather than requiring users to\n> hunt around in unspecified documents for it, with no assurance that\n> what's said in some such documents applies to a particular cookie.\n\nI agree 100%. I want the way a site tells me what it is doing with my\nprivate information to be available via a simple, consistent UA\nmechanism. I don't want one mechanism for cookies, another mechanism for\ncontent negotiation, a third mechanism for deciding whether to supply my\nemail address as the password for anonymous FTP, another mechanism for\ndeciding whether I want to supply personal information in forms I fill\nout using a web browser, another mechanism for deciding whether I want\nto supply personal information when interacting with a Java applet. I\nwant just what you're calling for: a single, consistent UA mechanism,\nadapted for local preferences for charset and language, but I want it to\nbe useful for all of those mechanisms. Putting in \"Comment\" and/or\n\"CommentURL\" inside Set-Cookie does nothing to help out with any of the\nother situations in which privacy is also an issue, and is quite\npossibly inconsistent or incompatible with those other situations.\n\nRegards,\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: commentURL, again",
            "content": "> Consensus in Munich (which does not include many concerned with this\n> issue) was that comment/commentURL is to be taken out of the spec, which I\n> believe is the right thing to do. How is this formally done? Let's do it.\n\nWhat happened in Munich was not 'consensus', but a straw poll of the\nroom.\nIt was uniform, but of course, most of the concerned parties weren't\nthere.\n\nThe way in which things are formally done is that issues are raised on\nthe mailing list, and when the mailing list converges, we call for\nconsensus.\n\n> Why it shouldn't be in RFC 2109:\n> - RFC 2109 is concerned with the functionality of cookies/stateful\n> sessions.\n\nRFC 2109 is concerned with both the functionality of cookies & stateful\nsessions and also addressing the privacy considerations that come with\nthat functionality.\n> - It is not the protocol's place to include this type of explanatory\n> information to the user (if it belongs anywhere, it belongs in another\n> spec related to privacy concerns and cookies; as suggested by Larry\n> Misinter).\n\nIt most definitely *is* the responsibility of a protocol designer to\naddress\nthe security issues surrounding the use of the protocol, and within\nthe IETF, working groups are required to address security and privacy\nconsiderations within protocol documents.\n\nIt is my personal (not WG-chair) opinion that this particular privacy\nconcern should be addressed elsewhere within HTTP, but that the 'fix'\ndoesn't belong directly within the Set-Cookie protocol element.\n\nIt is my WG-chair opinion that progress on the protocol itself\nhas been held hostage to the current language in the protocol\ndescription dealing with the privacy issue, and that one\nway to make progress might be to split the document but not\nthe specification. (This would allow the privacy considerations\nsection to be revised even if the protocol specification\nwas not.)\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>I'm sure it is dandy that a CommentURL *could* tell you \"This cookie\n>maintains your display preferences\" and \"This cookie performs\n>trade secrets which cannot be revealed to you.\" It could also tell me\n>\"This cookie was designed by that great web site designer Joe Coolsite\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>who will build cookies for you too!\"\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe purpose of the commentURL is to provide information\nabout a *particular* cookie.  Information does not lie solely in\nbytes or multi-byte characters received over the wire.  It lies\nin their interaction with a human intellect.  If that last statement\nwere recieved as the sole content of the \"document\" from a commentURL,\nin conjuction with a human intellect which evaluates it, it does\nprovide a form of information relevant to a variety of decisions,\nand not simply ones about \"privacy\", which might be made concerning\nthat particular cookie.  The quality of the decision is co-dependent\non the quality of the human's intellect.\n\n\n>A CommentURL could include\n>advertisements for local bakeries! It could shine your shoes! It could\n>wash your dog!\n\nThose also, when evaluated by a human's intellect, would\nprovide some basis for decisions on whether to accept, or discard\na previously accepted, cookie.  And, again, the reasons for invoking\na decision-making process need *not* be related to \"privacy\", per se.\n\n\n>[...] Putting in \"Comment\" and/or\n>\"CommentURL\" inside Set-Cookie does nothing to help out with any of the\n>other situations in which privacy is also an issue, and is quite\n>possibly inconsistent or incompatible with those other situations.\n\nWhat will it take to get across that \"privacy\" is not the only\nissue here, nor necessarily a central one when a commentURL is sought\nto assist in making a decision?\n\nAlso, it would be inside Set-Cookie2.  The Big Two are free to\ncontinue using just Set-Cookie.\n\nInformation does not lie in a single source.  This statement\nprovided information to it's readers:\n\nI also don't think content providers are going to want to share the\nmeaning of their cookies (perhaps for marketing purposes).\n-- valeski@netscape.com (Judson Valeski)\n\nAs did this statement:\n\nConsensus in Munich (which does not include many concerned with\nthis issue) was that comment/commentURL is to be taken out of\nthe spec, which I believe is the right thing to do. How is this\nformally done?  Let's do it.\n-- valeski@netscape.com (Judson Valeski)\n\nAs did this statement:\n\nWhat happened in Munich was not 'consensus', but a straw poll\nof the room.\nIt was uniform, but of course, most of the concerned parties\nweren't there.\n\nBut the combination of those statements, and others retrieved from memory,\nall processed through a human intellect, constitute more information than\neach in isolation, and a progressively better basis for an actual decision\nwhen one is solicited from a human being.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": ">         What will it take to get across that \"privacy\" is not the only\n> issue here, nor necessarily a central one when a commentURL is sought\n> to assist in making a decision?\n\nFirst, privacy considerations were the primary motivation for initially\ndisallowing cookies in the first place, and then adding Comment and then\nCommentURL as a way for letting users conditionally accept cookies. If\nyou're now claiming that privacy considerations are not the central\nissue for whether a user might want to view the resource pointed to by a\ncommentURL, well, what is?\n\nIf there were other more important considerations, they weren't part\nof the justification used to get the working group to go down this\nparticular rathole. I understand completely that you have other things\nthat you would like to ask content providers to tell you about\ntheir content and cookies. But if the browser makers mainly don't\nimplement it, or if the users don't usually use it (even if the\nbrowser makers do implement it) then the content providers won't\nprovide it, no matter how carefully we add the provisions.\n\nSecondly, if there are any other factors for which users might want\nsome kind of conditional compliance, then these are also protocol\nelements with which the mechanisms of conditional compliance should\nbe consistent. Right now, users choose 'accept Java' and 'load images'\nusing different dialogs and browser options, and there is no protocol \nelement that lets users decide whether they want to allow Java code. \nShould there be a \"comment\" or \"commentURL\" associated with each \nelement of Java, so that I could conditionally decide whether I want \nto allow a site's Java to execute on my machine, based on a decision \nof whether it is 'essential' or 'purely decorative'?\n\n>       Also, it would be inside Set-Cookie2.  The Big Two are free to\n> continue using just Set-Cookie.\n\nIf the makers of an overwhelming percentage of the deployed\nsoftware for web browsing don't intend to show Comment or CommentURL\ndata during cookie-choosing, then why in the world would any significant\nnumber of content providers ever bother providing them? It just\nmakes no sense.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>>         What will it take to get across that \"privacy\" is not the only\n>> issue here, nor necessarily a central one when a commentURL is sought\n>> to assist in making a decision?\n>\n>First, privacy considerations were the primary motivation for initially\n>disallowing cookies in the first place, and then adding Comment and then\n>CommentURL as a way for letting users conditionally accept cookies. If\n>you're now claiming that privacy considerations are not the central\n>issue for whether a user might want to view the resource pointed to by a\n>commentURL, well, what is?\n\nI do not know what was \"the primary motivation for initially\ndisallowing cookies\", nor was aware that anyone or anything associated\nwith the IETF had disallowed them.  Perhaps more germaine, I do not\nknow what were the primary motivations for developing RFC 2109, nor\nwhat was discussed during the bulk of its development, nor have any\nway to access and review those discussions.  That all was done by\na \"sub-group\" without archiving of its discussions.  This matter\ndid not get fully \"on-track\" w.r.t to IETF standardization principles\nuntil discussions about fixing the bugs in RFC 2109 had commenced,\nand the more formally structured HTTP-State sub-group with a formal,\nreliably archived mail-list were \"implemented\".\n\nI do know, and if need be could dig up the archived messages\nupon which I base my \"knowledge\", that the arguments for commentURL\nwhich led to it's inclusion in draft-ietf-http-state-man-mec-03.txt\nwhere not restricted to concerns about privacy, nor to the special\ncase in which a user might wish to read it before accepting a cookie.\nA full implementation of cookie support would include the possibility\nof storing it to disk, and thus utilization of perhaps limited personal\nresouces.  It also expands system overhead associated with scanning\nthe set of accepted cookies for whether they should be included in\na UA's http/https requests, and the network overhead, and possibly\nper packet $costs to the user, of including them in request/reply\n(entity?) headers.  Even in the case of cookies from a site which\nthe user, for whatever reasons, fully \"trusts\", the user might at\nintervals, or at the end of each session, wish to review any persistent\ncookies, so that only those for which a clear, and worthwhile (from\nthe user's perspective, of course) purpose can be inferred will\nactually be retained, e.g., because they will maintain the user's\ndisplay preferences for future sessions, or are the \"shopping cart\"\nfor an as yet uncompleted shopping spree.   The commentURL also\ncould be used to assist in cases for which a cookie might need to\nbe dumped to make room for a new cookie, without relying solely on\na mindless \"dump the oldest\" heuristic.  We do seem to disagree about\nwhether the absence of explicit information which might have been\nprovided, or the substitutions of advertisements or doublespeak, can\nthemselves be a form of information for intelligent human beings who\nhave powers of inferrence within the \"context\" of their personal value\nsystems.\n\n\n>If there were other more important considerations, they weren't part\n>of the justification used to get the working group to go down this\n>particular rathole.\n\nOne must expect problems with initial attempts to maintain\nstate in what was an inherently stateless protocol, just as one\nmust expect problems with initial attempts to implement persistent\nconnections, but this does not make them ratholes.\n\n\n> I understand completely that you have other things\n>that you would like to ask content providers to tell you about\n>their content and cookies. But if the browser makers mainly don't\n>implement it, or if the users don't usually use it (even if the\n>browser makers do implement it) then the content providers won't\n>provide it, no matter how carefully we add the provisions.\n>\n>Secondly, if there are any other factors for which users might want\n>some kind of conditional compliance, then these are also protocol\n>elements with which the mechanisms of conditional compliance should\n>be consistent. Right now, users choose 'accept Java' and 'load images'\n>using different dialogs and browser options, and there is no protocol \n>element that lets users decide whether they want to allow Java code. \n>Should there be a \"comment\" or \"commentURL\" associated with each \n>element of Java, so that I could conditionally decide whether I want \n>to allow a site's Java to execute on my machine, based on a decision \n>of whether it is 'essential' or 'purely decorative'?\n\nThe IETF abandoned (or readily let itself be pushed out of) its\nformerly important role in HTML standardization.  Thus, to use your own\nimmortal word, this comparison to 'accept Java' and 'load images' is\n\"bogus\".\n\n\n>>       Also, it would be inside Set-Cookie2.  The Big Two are free to\n>> continue using just Set-Cookie.\n>\n>If the makers of an overwhelming percentage of the deployed\n>software for web browsing don't intend to show Comment or CommentURL\n>data during cookie-choosing, then why in the world would any significant\n>number of content providers ever bother providing them? It just\n>makes no sense.\n\nThe IETF is not an organization dependent of membership fees\nfrom vendors.  It is, as you once wrote to me, \"whoever shows up and\ncares\".  It seeks to document standards for interoperability on the\nInternet, on behalf of implementors be they large and important to\nWall Street, or small (\"fringe\", from some people's perpspective).\nThe objectives of interoperability have been achieved in\ndraft-ietf-http-state-man-mec-03.txt.  Any provider can send\nSet-Cookie headers in the historical format for the Big Two UAs,\nand upon receiving a Cookie2: $Version=\"1\" request header from a\nfringe UA, switch to the superior state management protocol via\nSet-Cookie2 headers.  We are for the most part talking about\nscripts written by, or for, providers, and they can be implemented\nimmediately without backward compatibility problems.  You have\nnot, IMHO, explained convincingly why you choose to keep obstructing\nthis.\n\nWe both have a problem with some things not making sense to\nus.  It makes no sense to me to have a yet-to-chartered WG for\ndiscussing the \"process\" of URL standardizing, but no WG for discussing\nthe \"substance\" of drafts, and instead discussing them in a closed out\nURL-WG which is not archiving those discussions reliably.  I don't see\nhow interoperable results are likely to come from this perversion of the\nIETF standardization process.  And frankly, I'm worried that the same\nbad situation will apply when this HTTP-WG closes out.\n\nBut the State Management effort did get back on track,\nhas succeeded, and the -03 draft should move on to an appropriate\nFoo Standard status while this HTTP-WG is still chartered.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "RE: FW: revised trusted cookie spe",
            "content": ">I agree 100%. I want the way a site tells me what it is doing with my\n>private information to be available via a simple, consistent UA\n>mechanism. I don't want one mechanism for cookies, another mechanism\nfor\n>content negotiation, a third mechanism for deciding whether to supply\nmy\n>email address as the password for anonymous FTP, another mechanism for\n>deciding whether I want to supply personal information in forms I fill\n>out using a web browser, another mechanism for deciding whether I want\n>to supply personal information when interacting with a Java applet. I\n>want just what you're calling for: a single, consistent UA mechanism,\n>adapted for local preferences for charset and language, but I want it\nto\n>be useful for all of those mechanisms. Putting in \"Comment\" and/or\n>\"CommentURL\" inside Set-Cookie does nothing to help out with any of the\n>other situations in which privacy is also an issue, and is quite\n>possibly inconsistent or incompatible with those other situations.\n\nI agree -- one mechanism for handling private information would be much\nbetter than separate mechanisms for cookies, Java, etc.  It should also\nbe pursued by another working group, so that http-state can handle the\nrest of the revisions to the cookie spec.  This has been a tremendously\ncontentious issue, which should be handled in general purpose fashion\nrather than on a case-by-case basis (which is what commentURL does).\n\n\nThanks for writing the above, Larry -- I was going to ask you exactly\nwhy you objected to commentURL, but your email stated it very clearly\n(and changed my mind to boot).\n==========================================================\nMark Leighton Fisher          Thomson Consumer Electronics\nfisherm@indy.tce.com          Indianapolis, IN\n\"Browser Torture Specialist, First Class\"\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "Fisher Mark <FisherM@exch1.indy.tce.com> wrote:\n>>I agree 100%. I want the way a site tells me what it is doing with my\n>>private information to be available via a simple, consistent UA\n>>mechanism. I don't want one mechanism for cookies, another mechanism for\n>>content negotiation, a third mechanism for deciding whether to supply my\n>>email address as the password for anonymous FTP, another mechanism for\n>>deciding whether I want to supply personal information in forms I fill\n>>out using a web browser, another mechanism for deciding whether I want\n>>to supply personal information when interacting with a Java applet. I\n>>want just what you're calling for: a single, consistent UA mechanism,\n>>adapted for local preferences for charset and language, but I want it\n>to\n>>be useful for all of those mechanisms. Putting in \"Comment\" and/or\n>>\"CommentURL\" inside Set-Cookie does nothing to help out with any of the\n>>other situations in which privacy is also an issue, and is quite\n>>possibly inconsistent or incompatible with those other situations.\n>\n>I agree -- one mechanism for handling private information would be much\n>better than separate mechanisms for cookies, Java, etc.  It should also\n>be pursued by another working group, so that http-state can handle the\n>rest of the revisions to the cookie spec.  This has been a tremendously\n>contentious issue, which should be handled in general purpose fashion\n>rather than on a case-by-case basis (which is what commentURL does).\n\nYou are agreeing with a misunderstanding or misrepresentation\nof what is in draft-ietf-http-state-man-mec-03.txt.  Efforts to deal\nwith the \"trust\" issue within it *consistently* have been rejected,\nfor a variety of reasons, and instead are \"postponed\" to future efforts\nto address this contentious issue:\n\nSorry, I'm not willing to entertain the idea of credentials\nand trusted parties for this version of the spec.  Maybe it's\nreasonable for the future, but lacking a complete description\nof the credentials infrastructure, your proposal isn't practical\nnow.\n-- dmk@bell-labs.com (Dave Kristol) 26-Mar-1997\n\n\nI believe Jaye's proposal can be defined as an overlay to\nRFC 2109 (or its successor).  Since (I presume) the use of\ncertified cookies will be optional, there will still need to\nbe a definition of what happens with uncertified cookies.\nIt will take awhile to work out the details of Jaye's proposal,\nand I think tying the progress of RFC 2109 to jaye-trust-state-00.txt\n(and its successors) will delay a state management standard\nneedlessly.\nFrom a purely procedural standpoint, it's easier to get\nagreement on smaller things than bigger ones.  Imagine, for a\nmoment, that we try to merge RFC 2109 and jaye-trust-....  As\nlong as the full document is under discussion, the whole thing\nwill be subject to tinkering (and arguing).  Better to nail down\na substantial portion of the document, namely RFC 2109, and set\nit aside as done.\n-- dmk@bell-labs.com (Dave Kristol) 21-May-1997\n\nI would rather not have the standards merged. Jaye's proposal\nis something I am extremely interested in implementing. 2109\nis something I would rather not have to deal with. By keeping\nthe standards separate I get to more easily pick and choose.\n-- yarong@microsoft.com (Yaron Goland) 21-May-1997\n\nWhat I actually had in mind for RFC2109' and jaye-trust-...\nwas more like this:  RFC2109' would be much like state-man-mec-01.\nThere wouldn't be any specific forward references to new mechanisms. \nWhen/if jaye-trust-... becomes an RFC, it would make reference to\nspecific sections of RFC2109' and how it supersedes them.  For\nexample, there might be a section that says that if a certified\ncookie carries a trust level that is at or above a configured\nlevel in the user agent, then the rules about unverified transactions,\nwhich otherwise might be violated, would be satisfied.  (Obviously\nthe words would be much more detailed.)\n-- dmk@bell-labs.com (Dave Kristol) 21-May-1997\n\nMy preference is for Set-PICS-Cookie (or some other header name)\nbe a strict add-on to RFC 2109 (or successors).\n-- dmk@bell-labs.com (Dave Kristol) 30-May-1997\n\n\nand so on and so on, through draft-ietf-http-state-man-mec-03.txt!!!!!\n\nThere is not such thing as a \"trusted cookie spec\" -- subject\nline for this thread notwithstanding.  That is a figment of Larry's\nand now also your imaginations, probably due to unresolved approach-\navoidance conflict.  But the bottom line is that its still avoided.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": ">        I do not know what was \"the primary motivation for initially\n> disallowing cookies\", nor was aware that anyone or anything associated\n> with the IETF had disallowed them.  Perhaps more germaine, I do not\n> know what were the primary motivations for developing RFC 2109, nor\n> what was discussed during the bulk of its development, nor have any\n> way to access and review those discussions.  That all was done by\n> a \"sub-group\" without archiving of its discussions.  This matter\n> did not get fully \"on-track\" w.r.t to IETF standardization principles\n> until discussions about fixing the bugs in RFC 2109 had commenced,\n> and the more formally structured HTTP-State sub-group with a formal,\n> reliably archived mail-list were \"implemented\".\n\nRFC 2109 was a product of the HTTP working group. As with all working\ngroup products, someone or some group actually will go off and write\na specification, and then the working group will review the\nspecification.\nThe review is open. What happens in the minds or conversations among\nthose who develop the specification is not. \n\nSection 4.3.2 \"Rejecting Cookies\" and 4.3.3 \"Cookie Management\" talk\ndirectly about the motivation for the features therein. 4.3.2 says\n\"To prevent possible security or privacy violations\", 4.3.3 says \n\"Privacy considerations dictate that the user have considerable\ncontrol over cookie management.\" No other rationale is given.\nThe RFC itself is the best authority for what the \"primary motiviation\"\nis for rejecting cookies, and it says so clearly. (I said 'disallowing'\nbut should have said 'rejecting', and perhaps my miswording is\nthe source of your confusion.)\n\nIn any case, the RFC itself is pretty self-explanatory.\n\n>        I do know, and if need be could dig up the archived messages\n> upon which I base my \"knowledge\", that the arguments for commentURL\n> which led to it's inclusion in draft-ietf-http-state-man-mec-03.txt\n> where not restricted to concerns about privacy, nor to the special\n> case in which a user might wish to read it before accepting a cookie.\n\nYes, the arguments for \"commentURL\" were as a response to difficulties\nwith \"Comment\" in 2109. Personally, I'd rather see \"Comment\" removed\nthan to continue to add capabilities to it.\n\nI'd stop \"obstructing\" this, except that a room full of people in Munich\nseemed to nod when I suggested changing direction here.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "> > [...]  I do not\n> > know what were the primary motivations for developing RFC 2109, nor\n> > what was discussed during the bulk of its development, nor have any\n> > way to access and review those discussions.  That all was done by\n> > a \"sub-group\" without archiving of its discussions.  This matter\n> > did not get fully \"on-track\" w.r.t to IETF standardization principles\n> > until discussions about fixing the bugs in RFC 2109 had commenced,\n> > and the more formally structured HTTP-State sub-group with a formal,\n> > reliably archived mail-list were \"implemented\".\n\nThe state subgroup was formed out of http-wg; see\n<http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0144.html> and\nfollowing messages in the \"Revised Charter\" thread; and the minutes of the\nDecember 1995 IETF (Dallas), which directly mention the subgroup\nformation: \n<http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0465.html>.\n\nThe state subgroup had several conference calls on which I took minutes; I\ncan probably dig them up if needed, though commentURL did not arise out of\nthose discussions.\n\n[Larry Masinter writes:]\n> In any case, the RFC itself is pretty self-explanatory.\n\nOne hopes!  \n\n> Yes, the arguments for \"commentURL\" were as a response to difficulties\n> with \"Comment\" in 2109. Personally, I'd rather see \"Comment\" removed\n> than to continue to add capabilities to it.\n\nI still think this whole discussion is totally pointless without buy-in\nfrom vendors.  \n\n<hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "comments on jaye-trust-state0",
            "content": "Following are my comments for Dan Jaye's recently posted\n(pre-) draft-ietf-http-jaye-trust-state-01.txt.\n\nGeneral:\n1) Be consistent with capitalization for \"Trust Authority\", \"user agent\",\n\"SigBlock\".\n\nSpecific, interlinear:\n\n[...]\n> ABSTRACT\n> \n> HTTP TRUST MECHANISM PROPOSAL FOR STATE MANAGEMENT\n> March 30, 1997\n  ==============  probably not!\n\n[...]\n> \n> 2. TERMINOLOGY\n> \n> The terms user agent, client, server, and origin are used as in [Kristol].\n                                                  ^-- \"server\"\n\n[...]\n> 3.1  Syntax: General\n[...]\n\n> trusted-cookie  = 1*Set-cookie2 trust-label\n> trust-label     = \"PICS-Label:\" labellist\n\nThis syntax possibly suggests two things that are both false:\n1) That the trust label is a continuation of Set-Cookie2; or\n2) That the trust label must immediately follow Set-Cookie2.\n\nAlthough a server may emit PICS-Label immediately after Set-Cookie2, it\nmay not be in that position at the client, due to proxy header\nshuffling.  The only guarantee is that the relative order of\nSet-Cookie2 headers will be preserved, as will the relative order of\nPICS-Label headers.  But they well may get separated (particularly with\nheader folding).\n\nThere would appear to be a possible danger with using \"PICS-Label\".\nSuppose the origin server wants to emit one PICS-Label to label the\ncontent, and another to label the cookie.  Header folding would group\nall PICS-Label headers together, making it harder to pick out which\nones apply to cookies, and which to content.\n\n> \n> \"labellist\" is as specified in the PICS 1.1 label syntax in [PICS],\n> except as extended by the digital signatures working draft [DSIG], some\n> options are not used, and we require some of the optional elements, as\n> specified below.  The trust-label applies to the immediately preceding\n> Set-Cookie2 label.\n\nAs I said, the last sentence might be false.\n\n[...]\n> label           = labelattr \"ratings\" \"(\" privacy-practice \")\" [sigblock]\n\nIn the PICS spec, \"sigblock\" -> \"SigBlock\", as you have it below.\n\n[...]\n\n> authority.  The \"gen\" boolean indicates whether the label is for only the \n> URL in the \"for\", or for all URL's for which the specified one is a \n> prefix.  (\"True\" indicates subdomains included.)  \"on\" is the date the \n        ^-- (i.e., \"generic\") -- since you use that term later.\n\n> label was issued.  \"exp\" is the date the label expires.  SigBlock is the \n> digital signature extension as described in the digital signature working\n\n> Four well-known privacy-practice values are described here to provide \n> recognized values that should be handled by user agents.    \n> \n[...]\n> The \"anonymousexchange 1\" rating indicates that the trust authority has \n[...]\n> identifying information.  The server may collect IP Addresses but they\n> must not be associated with personal information or implicit information\n> with personally identifying information to be elegible for this rating.\n\nThis last sentence is very confusing; please reword.\n\n[...]\n> The \"thirdpartyexchange 1\" rating indicates that the trust authority has \n[...]\n> third parties.  The trust authority should provide information about the \n> purposes for which that information is being used.\n\nHow should it do that?  (Via ServiceID?  If so, say so.)\n\nThere seems to be the implication that the privacy-practice values\nform a partial ordering, and that a more-stringent privacy-practice\nin a trust label should be acceptable to a user agent that is set to\naccept less-stringent ones, but that ordering and treatment isn't\nspelled out explicitly and should be.\n\n> The SigBlock must contain the SigCrypto token within the SigData block.  \n> The SigCrypto token must contain the encrypted trust-label-data described\n> below.\n> \n> trust-label-data = for-URL on-date exp-date privacy-practice\n> \n> for-URL          = quotedURL\n> exp-date         = quoted-ISO-date\n> \n> for-URL is the URL to which the privacy practice applies as listed in the \n> \"for\" attribute in \"labelattr\".  Exp-date is the expiration date of the \n> trust label as listed in the \"exp\" attribute of \"labelattr\".\n\nThe trust-label-data also needs the \"gen\" attribute to denote the\nscope of the label.  Otherwise, a web site could obtain a trust label\nfor a single URL and try to pass it off as a label for an entire\nhierarchy.\n\nA nit:  the on-date in the PICS spec. is the date the signature was\ncreated, not the date the trust label (in our case) becomes effective.\nBut it would surely be \"less that or equal to the current date\"\n(sect. 3.3.1), rendering that test useless.\n\n[...]\n> 3.2Server Role\n> \n> A server communicates its privacy practices by sending an unsigned or \n> signed trust label immediately following the cookie header(s).  The trust \n> label is assumed to apply to all cookies in the response that match the \n> domain and path of the trust label according to the matching rules for \n> matching cookies to request URI's described in [Kristol].\n\nIt would help to spell out that \"domain and path\" are derived from the\n\"for\" URL in the labelattr.  (It took me a couple of readings to realize\nthat.)\n\nOne of the contentious issues with RFC 2109 has been the concept of\n\"related servers\" (although it isn't called that) to which a user agent\ncan send a cookie.  That's the business with the domain-matching, the\n\"two-dot rule\", etc.  The trust-label mechanism is even *more*\nrestrictive, since the label applies to a single host.  Won't that\nrestriction be even more objectionable?\n\n[...]\n> 3.3 User Agent Role\n> \n> The user agent receives a cookie headers followed by a trust labels from \n\n===> ^ (delete)    ^ (delete)\n\n> an origin server.\n> \n> 3.3.1 Interpreting the trust-label\n> User agents interpret cookies as described in RFC 2109.  In addition \n[...]\n> matching rules described in [Kristol].  To help verify the \n> trustworthiness of the server, the user agent may look for a digital \n> signature and use the trust authority's well known public key to \n> decrypt the trust-label-data from the SigCrypto term.\n\nappend:  and compare the information there against the plaintext trust\nlabel.\n\n[...]\n> The digital signature is valid if the decrypted trust-label-data \n> satisfies the following criteria:\n\nI would have thought the digital signature is valid *if the decrypted\ntrust-label-data matches the plaintext labelattr*.  The rest of these\ntests should apply whether or not there's a SigBlock.  So this spec.\nneeds to describe how the digital signature gets verified.\n\n> \n> 1) that the domain portion of the URL specified in the for-URL attribute \n> domain matches the domain of the cookie according to the matching \n> rules as sort forth in [Kristol];\n\n1) That the domain portion of the URL specified in the \"for\"\nattribute of labelattr domain-matches the Domain attribute of\nthe Set-Cookie2 response header, according to the matching\nrules in [Kristol].\n> \n> 2) that the path portion of the URL specified in the for-URL attribute is\n> compatible with the path of the cookie.  If the trust label is generic, \n> then the for-URL path must be a prefix of the cookie's path.  If the \n> trust label is not generic, then it must match exactly. \n\n    2) That the path portion of the URL specified in the \"for\"\n    attribute of labelattr is compatible with the Path attribute of the\n    Set-Cookie2 response header.  That is, if the trust label is\n    generic, then the \"for\" path must be a prefix of the cookie's Path\n    attribute.  If the trust label is not generic, then it must match\n    the cookie's Path attribute exactly.\n> \n> 3) that the \"on-date\" attribute of the trust label is less than or equal\n> to the current date;\n\nSee earlier remark.\n\n> \n> 4) that the \"exp-date\" attribute of the trust-label-data is greater than\n> or equal to the current date.\n> \n> If the digital signature is invalid, then the cookie should be rejected.\n\n    If the digital signature is invalid, or if any of the above tests\n    fails, then the cookie should be rejected.\n\n> \n> If the user agent is set to accept all cookies then all trust label \n> processing can be skipped.\n> \n> 3.3.2Accepting or rejecting Cookies\n[...]\n\n> The User Agent should have a default preference to reject \"third-party-\n> exchange\" cookies from unverifiable transactions.  \n\nNeed a definition of \"third-party-exchange\"; or omit this statement, since\nit appears in RFC 2109.\n\n> \n> For example, a user may wish to accept cookies rated anonymousexchange by\n> a recognized trust authority, rather than relying on an unsigned trust \n> label or a trust label signed by an unrecognized entity.\n\nI don't understand the point this statement is trying to make.\n\nIf a trust label's expiration is sooner than the cookie's, I think the\ncookie's expiration should be adjusted to match that of the trust label.\nAlternatively, the UA should not send a cookie whose trust label has\nexpired.\n\n[...]\n> 4. EXAMPLES\n> \n> 4.1 Example 1\n[...]\n> 3.  Server -> User Agent\n>       HTTP/1.1 200 OK\n>       Set-Cookie: Customer=\"WILE_E_COYOTE\"; Max-Age = 94608000; \n\n        Set-Cookie2: Customer=\"WILE_E_COYOTE\"; Max-Age = 94608000; \n\n>         Version=\"1\"; Path=\"/acme\" \n[...]\n> \n>     A cookie that includes the users identity and an unsigned PICS\n>     label header are sent back to the user agent with the request.  The \n   === -> \"is\"\n>     Cookie is accepted because rating \"noshare 1\" is acceptable according \n      ====== -> \"cookie\"\n>     to the privacy preferences of the user agent.\n\n... assuming the user agent doesn't require signed trust labels.\nIs there really any value in unsigned trust labels?\n\n[...]\n> \n> 4.2 Example 2\n[...]\n> 3.  Server -> User Agent\n>       HTTP/1.1 200 OK\n>       Set-PICS-Cookie: Customer=\"0000000123\"; Max-Age = 94608000; \n\n=============== -> Set-Cookie2\n\n>         Version=\"1\"; Path=\"/birds\" \n>       PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n>         by \"paranoid@aaa.org\" gen true\n>         for \"http://www.acme.com/\"\n>         exp \"1997.12.31T23:59-0000\" \n>         ratings (noshare 1))\n> \n>     A Cookie reflecting the users identity is transmitted with an \n>     Unsigned trust label back to the user agent.  The Cookie is accepted\n\n      ======== -> \"unsigned\"\n\n>     by user agent because the rating \"noshare 1\" is compatible with the \n>     user agent privacy preference.\n> \n> 4.  User Agent -> Server\n> \n>       GET cgi-bin/maps?TERR=deserts&FEAT=cliffs HTTP/1.1\n>       Host: roadrunnermaps.com\n\nGET /cgi-bin/maps?TER=deserts&FE=cliffs HTTP/1.1\nHost: www.roadrunnermaps.com\n\n(Changes to be consistent with step 2, which I omitted.)\n\n> \n>     User requests an image via CGI script from a third party map provider.\n>     This is an unverifiable transaction.\n> \n> 5.  Server -> User Agent (unverifiable transaction)\n>       HTTP/1.1 200 OK\n>       Set-PICS-Cookie: Customer=\"0000000123\"; Max-Age = 94608000; \n\n=============== -> Set-Cookie2\n\n>         Version=\"1\" \n>       PICS-Label: (PICS-1.1 \"http://www.aaa.org\" label \n>         by \"paranoid@aaa.org\" gen true\n>         for \"http://www.acme.com/\"\n\nThis \"for\" does not match the request URL *or* the (implicit) Domain\nattribute of the Set-Cookie2!  (www.acme.com vs. www.roadrunnermaps.com)\n\n>         exp \"1997.12.31T23:59-0000\" \n>         extension \n>           (optional \"http://www.w3.org/PICS/DSig/sigblock-1_0.html\"\n>             (\"AttribInfo\" \n>               (\"http://www.w3.org/PICS/DSig/X509.html\" \n>                 \"base64-x.509-cert\"))\n>             (\"Signature\" \"http://www.aaa.org/trust.html\" \n>               (\"byName\" \"aaapublickey\") \n>               (\"SigCrypto\" \"8E53B19D35A3F19D35A38930E53FD35A7B215B2158\")))\n>         ratings (anonymousexchange 1))\n> \n\n[...]\n> 6. SUMMARY\n> \n> This document presents an extension to the state management protocol \n    \n    This document presents an extension to the HTTP State Management\n    Mechanism\n\n> defined in RFC2109.  It describes only changes to that protocol. Any \n> parts of the state management not explicitly described here are assumed\n\n^-- mechanism\n\n> to remain as defined in RFC 2109.\n> \n\n[...]\n> 8. REFERENCES\n[...]\n> \n> [Kristol] Kristol, David M., HTTP State Management Mechanism (rev 1). \n\nRev\n\n> Internet Draft <draft-ietf-http-state-man-mec-03.txt>\n> ftp://ietf.org/internet-drafts/draft-ietf-http-state-man-mec-03.txt\n\nDon't let Lou Montulli off the hook!  He's a listed co-author.\n\n\n\n"
        },
        {
            "subject": "tcpdum",
            "content": "I am a Ph.D student at Georgia Tech.\nWe are simulating WWW traffic and generating artificial traffic according \nto the data getting from real traffic.\n\nBy inspecting TRACE DUMP (tcpdump),\nI have found 2 strange questions.\n\n1) when finishing their connection, some of connections end up with by \nsending RST abnormally. \n\nUsually, server sends FIN to close connection.\nClients sends FIN to server with ACK and waiting sever's ACK.\nThen, timeout happens, client sends RST and close connection.\nI have noticed quite many times in tcpdump this abnormality.\nIs it due to the difference of TCP version or else?\n\n2) Port NO.\nTcp assign port NO. sequencially.\nIn case,in a client, there are more than 1 http connections, how TCP \nassign the port NO?\n\nhttp-1: 4500 (main)\nhttp-1: 4501~ (successive inlines)\n\nif another http-2 starts at the moment http-1 is in main,\nwill http-2's port be 4501, or else?\n\nYour answer will help me very much in various way for me to research.\nThank you\n\n\n\n"
        },
        {
            "subject": "19.9 requirements summar",
            "content": "has any drafting been done of a fuller replacement for 19.9 than\nwhat's in id 8?\n\n-P\n\n\n\n"
        },
        {
            "subject": "Re: 19.9 requirements summar",
            "content": "has any drafting been done of a fuller replacement for 19.9 than\nwhat's in id 8?\n\nI circulated a draft of about half of the requirements for section 13\n(caching) about 2 or 3 months ago, as a \"test of concept\".  It got\nreasonably positive response.\n\nHowever, we decided not to include a full listing in draft -08,\nbecause\n(1) we felt that getting the spec right was more important\nthan writing the requirements summary, and we could not\nfinish both before the July 30 cutoff date.\n\n(2) The spec was changing enough (relative to RFC2068)\nthat any summary based on RFC2068 would be inaccurate.\nWe did not have a procudure in place to ensure that we\nwould be able to know what exactly would have to be updated.\n\nOf course, those two excuses are (mostly) moot now.  The real\nproblem is to find a sufficient number of people who have time\ntime to draft portions of the requirements summary.  I initially\nwent through RFC2068 and divided the effort up among various\nmembers of the editorial group, but (as far as I know) none of\nus (including me) have actually done anything.  We'd certainly\nbe interested in volunteers, although I think it will be hard\nto absorb too many different authorial \"voices\" in this process.\n\nI think I was also nominated to work on the details of the format.\nOne possibility is to put the master copy into a spreadsheet, which\ngives us some freedom to reformat things (especially because Jim\nGettys maintains the master copy of the HTTP/1.1 spec in Word).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 19.9 requirements summar",
            "content": "I'm hoping that the 'requirements summary' might also be a checklist for\nthe interoperability testing that various WG members claim that they\nwant to do in the next month or so. That is, we can determine not only\nwhether or not people implement a feature, but whether it is\ninteroperable with other implementations.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "PUT multipar",
            "content": "In Munich, it came to my attention (thanks Yaron) that Paul Leach is\nprivately circulating a draft about what PUT of a multipart entity\nmeans.  This being news to me, I would like to see it sooner rather\nthan later, since, according to Yaron, our interpretation differs from\nPaul's.\n\nThanks,\n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "RE: FW: revised trusted cookie spe",
            "content": "-----Original Message-----\nFrom:Larry Masinter [SMTP:masinter@parc.xerox.com]\nSent:Monday, August 18, 1997 2:05 PM\nTo:Jaye, Dan\nCc:'http-wg@cuckoo.hpl.hp.com'; \n'http-state@lists.research.bell-labs.com'\nSubject:Re: FW: revised trusted cookie spec\n\nAt the HTTP working group meeting, I took off my \"virtual\" chair's \nhat\nand put on a \"opinionated working group member\" hat, and ranted about\ncommentURLs. I want to extend that rant:\n\n\nI'm opposed to commentURLs in cookies.\nI'm opposed to comment strings in cookies.\nI'm opposed to trusted cookies, too.\n\nI believe that we should recommend \"browsers should not return\ncookies to sites that are not trusted with private information\"\nand that trust can be established using a variety of means:\n(a) the site sent you the cookie (b) you have some other way of\nestablishing a site's privacy policy.\n\nEstablishing the privacy policy might be accomplished by\nusing a PICS-Label or by obtaining it via some other link,\nhaving the privacy rating INSIDE THE DOCUMENT that contains\nthe links (\"we assert that this document only links to sites\nwith the following privacy policy\") or any of a variety of\nmeans outside the HTTP protocol.\n\nBut assertions of privacy policies do not belong *inside* the\nstate management mechanism.\n\nMy proposal does not put the privacy policy inside the state mgt \nmechanism.  A separate PICS-Label header is used.  It merely \nestablishes how you relate cookie handling to privacy policies.  Do \nyou think it is unnecessary to establish that link (from within the \nhttp protocol)?\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Re: FW: revised trusted cookie spe",
            "content": "It was hard to separate your quotation from your statement, but I think\nyou said:\n\n> My proposal does not put the privacy policy inside the state mgt \n> mechanism.  A separate PICS-Label header is used.  It merely \n> establishes how you relate cookie handling to privacy policies.  Do \n> you think it is unnecessary to establish that link (from within the \n> http protocol)?\n\nI think a privacy policy should be more comprehensive than merely a\npolicy about cookie handling, so a Pics-Label header that's solely\nuseful for labelling cookies seems pretty useless to me.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: FW: revised trusted cookie spe",
            "content": "-----Original Message-----\nFrom:Larry Masinter [SMTP:masinter@parc.xerox.com]\nSent:Thursday, August 28, 1997 4:04 AM\nTo:Jaye, Dan\nCc:'http-wg@cuckoo.hpl.hp.com'\nSubject:Re: FW: revised trusted cookie spec\n\nIt was hard to separate your quotation from your statement, but I \nthink\nyou said:\n\n> My proposal does not put the privacy policy inside the state mgt\n> mechanism.  A separate PICS-Label header is used.  It merely\n> establishes how you relate cookie handling to privacy policies.  Do \n> you think it is unnecessary to establish that link (from within the \n> http protocol)?\n\nI think a privacy policy should be more comprehensive than merely a\npolicy about cookie handling, so a Pics-Label header that's solely\nuseful for labelling cookies seems pretty useless to me.\n\nGeneral privacy policy is addressed in by the P3 in the W3C.  This \nonly addresses cookies because the new cookie spec implements privacy \npolicies that are incomplete.  A standard PICS Label header is used \nbecause privacy policies will eventually be indicated using PICS \nlabels...\n\nLarry\n--\nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "David W. Morris wrote:\n\n> On Fri, 1 Aug 1997, Ben Laurie wrote:\n>\n> > Larry Masinter wrote:\n> > > satisfies all of the requirements that are MUST for 1.1, then you\n> > > need to label the response as 1.0.\n> >\n> > OK. It still seems to me that the correct thing to do is to fix CGI. A\n> > simple thing to do would be to add a version header:\n> >\n> > CGI-Version: 1.1\n>\n> (nb., this suggestion mixes CGI and HTTP versions ...)\n>\n> >\n> > Absence of the header means the script is 1.0 compliant. This is not an\n> > HTTP header - the server would strip it, I assume, and doctor other\n> > headers as needed.\n>\n> I think this issue is much larger than whether the server know that\n> an individual CGI script complies with HTTP/1.1 in what it generates.\n> This just as easily fixed with out band server configuration choices.\n> As a separate subject, it may be appropriate for some group to standardize\n> the CGI API and include a version, but that isn't our task and such a\n> change won't help this problem...\n>\n> If a script was written to empirical behavior, it (actually, the target\n> being referenced) may expect a 302 response to POST to return as a GET.\n>\n> That server may be accessed by a proxy which changes the HTTP status line\n> version to 1.1 making any necessary updates to header fields.\n>\n> If a *client* which sees the 302 status on what is marked as an HTTP/1.1\n> response adopts rigid 302 handling, an existing HTTP/1.0\n> server/application will break. Who gets blamed? It works with my old\n> browser (still if I re-install it, etc.) and not the new one. In the\n> user's mind the blame rests with the browser, not an out of date server.\n>\n> The 307 proposal works. Lets do it.\n>\n\nI'm definately coming late to this discussion, but I have some strong\nthoughtsto offer.\n\n99.2% of the browsers in use today including all versions of netscape and IE\nreissue POST redirects with a GET.  It would be incredibly fool hardy to\ntry and change this behaviour now.  It is far easier to swap the meaning of\n303 and 301/302 than it is to fix every CGI in the world as well as every old\nbrowser in the world.   I doubt that any commercial vendor is willing to\nrelease a product that will break a large number of sites simply to claim\ncompliance with this spec.\n\nHow is this issue going to get resolved?  This tread died out almost a month\nago yet there is no solution yet.  The current situation is unworkable.\n\n:lou\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Thu, 28 Aug 1997, Lou Montulli wrote:\n\n> > The 307 proposal works. Lets do it.\n> >\n> \n> I'm definately coming late to this discussion, but I have some strong\n> thoughtsto offer.\n> \n> 99.2% of the browsers in use today including all versions of netscape and IE\n> reissue POST redirects with a GET.  It would be incredibly fool hardy to\n> try and change this behaviour now.  It is far easier to swap the meaning of\n> 303 and 301/302 than it is to fix every CGI in the world as well as every old\n> browser in the world.   I doubt that any commercial vendor is willing to\n> release a product that will break a large number of sites simply to claim\n> compliance with this spec.\n> \n> How is this issue going to get resolved?  This tread died out almost a month\n> ago yet there is no solution yet.  The current situation is unworkable.\n\nI thought we had reached concensus that 302 would be redefined to current\npractice that 301 and 303 were correctly defined AND that a new\ncode (307) would mean what 302 currently says.\n\nBut I don't recall anyone declaring concensus or providing actual proposed\nwording changes.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Uhh..\nMy impression at munich was that we agreed to swap the meanings of\n301/302 in the 1.1 spec.\n\nCan someone check the minutes?\n(jim??)\n\nDavid W. Morris wrote:\n> \n> On Thu, 28 Aug 1997, Lou Montulli wrote:\n> \n> > > The 307 proposal works. Lets do it.\n> > >\n> >\n> > I'm definately coming late to this discussion, but I have some strong\n> > thoughtsto offer.\n> >\n> > 99.2% of the browsers in use today including all versions of netscape and IE\n> > reissue POST redirects with a GET.  It would be incredibly fool hardy to\n> > try and change this behaviour now.  It is far easier to swap the meaning of\n> > 303 and 301/302 than it is to fix every CGI in the world as well as every old\n> > browser in the world.   I doubt that any commercial vendor is willing to\n> > release a product that will break a large number of sites simply to claim\n> > compliance with this spec.\n> >\n> > How is this issue going to get resolved?  This tread died out almost a month\n> > ago yet there is no solution yet.  The current situation is unworkable.\n> \n> I thought we had reached concensus that 302 would be redefined to current\n> practice that 301 and 303 were correctly defined AND that a new\n> code (307) would mean what 302 currently says.\n> \n> But I don't recall anyone declaring concensus or providing actual proposed\n> wording changes.\n> \n> Dave Morris\n\n-- \n-----------------------------------------------------------------------------\nJosh Cohen <josh@netscape.com>      Netscape Communications Corp.\nhttp://people.netscape.com/josh/\n                                \"You can land on the sun, but only at night\"\n\n\n\n\napplication/x-pkcs7-signature attachment: S/MIME Cryptographic Signature\n\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Thu, 28 Aug 1997, David W. Morris wrote:\n\n> On Thu, 28 Aug 1997, Lou Montulli wrote:\n> \n> > How is this issue going to get resolved?  This tread died out almost a month\n> > ago yet there is no solution yet.  The current situation is unworkable.\n> \n> I thought we had reached concensus that 302 would be redefined to current\n> practice that 301 and 303 were correctly defined AND that a new\n> code (307) would mean what 302 currently says.\n> \n> But I don't recall anyone declaring concensus or providing actual proposed\n> wording changes.\n\nThe message where I proposed what is summarized above is at \n<URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0402.html>.\n(For some reason it doesn't show up immediately when one follows the\nlinks into the archive from the Issues Page.)  There were several replies\nthat indicated agreement.\n\nIf the people present in Munich came to a different consensus, I would\nvery much like to know why.  The Issues Page currently has \"Consensus at\nIETF seems to be to swap the codes, rather than making the protocol\ncruftier.\"  Can someone explain why it is better to make 303 as currently\nspecified unusable, rather than just adjusting the specification as\nmuch as necessary (as dictated by reality)?\n\nMy guess is that nobody was present who wants to implement or use the\n\"true redirection\" functionality (what 302 currently says, proposed for\n307).  Maybe there is nobody using it, and there will never be an\nimplementation.  In that case it would be more honest to just completely\nget rid of it, instead of just pushing it around, first to 302, then to\n303.  Specifying something for 303 which won't get used, and in addition\nis completely contrary to RFC 2068, doesn't reduce cruft.\n\nIf \"true\" (but temporary) redirection is to be retained as a useful\nfeature of the protocol - I assume everyone who wants to implement it\nwould prefer to use a new, \"clean\" status code, rather than one for which\nat least one RFC has specified the exact opposite.\n\n     Klaus\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": ">If the people present in Munich came to a different consensus, I would\n>very much like to know why.  The Issues Page currently has \"Consensus at\n>IETF seems to be to swap the codes, rather than making the protocol\n>cruftier.\"  Can someone explain why it is better to make 303 as currently\n>specified unusable, rather than just adjusting the specification as\n>much as necessary (as dictated by reality)?\n\nI wasn't at Munich, but I can guess at the reasons.\n\n   1) If 302 (as implemented) really means See Other, then why confuse\n      users by also having 303 have that definition?\n\nFoteos asked a while back for a little historical perspective.  I've been\ntoo busy lately, but I'll describe the *real* history of 302 now because\nI am tired of people accusing us of making an arbitrary decision.\n\nThe 1993 specification lists both 302 and 303, with the 302 code being\ndescribed as a \"redirection that may be altered on occasion\" and 303\ndescribed as a method changing redirect in which a \"Method\" header field\nwas used to give the UA the new method to use, and the response body\ncontained the parameters that the UA should send in its redirected\nrequest body.\n\nFor obvious security reasons, that definition of 303 was never implemented\nand the code was later deprecated.  There was no clear statement for 302\nregarding whether or not the redirected request should always be a GET\n(as in See Other) or a repeat of the original request.  When Henrik and I\nstarted rewriting the HTTP/1.0 specification in October 1994, all of the\ncode we had access to redirected with the same method, so that is how we\ndefined it.\n\nThe issue did not arise in the WG until 23 March 1995,\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q1/0189.html\n\nand was later repeated on 18 June 1995, when Bill Perry asked about it\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q2/0196.html\n\nand Henrik gave a convincing reason why he implemented it as described\nin the specification, and why we should use 303 for \"See Other\". \nConsensus was that different people implemented it differently, and\ntherefore a decision was made that the most natural definition would be\ngiven as 302 and a new status code 303 would be used for \"See Other\".\n\nThe discussion arose again on 1 September 1995 when Chris Lilley posed an\nalternative solution in\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0548.html\n\nand the only response was mine (regarding we already solved it with 303).\nThe next time it was discussed was 30 Jan 1996\n\n   http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0173.html\n\nPlease note that at no time during this period did any of the Browser\ndevelopers say that they were not going to implement 303, or that they\npreferred some solution other than 303.  We have an interoperability\nproblem now because, apparently, none of the major browser vendors read\nspecifications in 1995.\n\nPersonally, I don't care what decision is made, provided that BEFORE\nany change is made to the document that we receive EXPLICIT WRITTEN\nCONFIRMATION FROM EVERY SINGLE MAJOR WWW SOFTWARE ORGANIZATION that\nthe given solution is either\n\n   a) How their existing software is implemented.\n\n   b) How their next version of the software *will* be implemented.\n\nI can tell you right now that Apache 1.2 is implemented according to\nRFC 2068, but also that it would be no great hardship for us to swap\nthe meaning of 302 and 303 in the next version.  We won't be happy about\nit, but it's not the first time we have had to change our implementation\njust because some browser developers can't read or are somehow unable to\nparticipate in an open WG.\n\n   2) If 303 has no definition and is not currently being used, then\n      why not use it for temporary redirects?\n\nThe answer depends on whether there exist any deployed implementations\nof 303 that would be adversely affected by the change.  I know of none\nother than the W3C libwww, and I don't know if it would be a problem.\nHenrik is probably the only one who can answer that.\n\n>My guess is that nobody was present who wants to implement or use the\n>\"true redirection\" functionality (what 302 currently says, proposed for\n>307).  Maybe there is nobody using it, and there will never be an\n>implementation.  In that case it would be more honest to just completely\n>get rid of it, instead of just pushing it around, first to 302, then to\n>303.  Specifying something for 303 which won't get used, and in addition\n>is completely contrary to RFC 2068, doesn't reduce cruft.\n\nRephrasing, that would be\n\n   3) If there is no known reason to temporarily redirect a non-GET\n      request, then why do we need a status code for it?\n\nThe answer to that depends on whether there will ever be a new method\nthat needs a temporary redirect, and it is impossible to know that.\nTwo+ years ago we anticipated the needs of future applications and\ndeveloped a solution to satisfy those needs.  It just wasn't implemented\nby Netspryglassoft.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "\"Roy T. Fielding\" <fielding@kiwi.ics.uci.edu> wrote:\n>>Klaus Weide <kweide@tezcat.com> wrote:\n>>If the people present in Munich came to a different consensus, I would\n>>very much like to know why.  The Issues Page currently has \"Consensus at\n>>IETF seems to be to swap the codes, rather than making the protocol\n>>cruftier.\"  Can someone explain why it is better to make 303 as currently\n>>specified unusable, rather than just adjusting the specification as\n>>much as necessary (as dictated by reality)?\n>\n>I wasn't at Munich, but I can guess at the reasons.\n>\n>   1) If 302 (as implemented) really means See Other, then why confuse\n>      users by also having 303 have that definition?\n>\n>Foteos asked a while back for a little historical perspective.  I've been\n>too busy lately, but I'll describe the *real* history of 302 now because\n>I am tired of people accusing us of making an arbitrary decision.\n>\n>The 1993 specification lists both 302 and 303, with the 302 code being\n>described as a \"redirection that may be altered on occasion\" and 303\n>described as a method changing redirect in which a \"Method\" header field\n>was used to give the UA the new method to use, and the response body\n>contained the parameters that the UA should send in its redirected\n>request body.\n>\n>For obvious security reasons, that definition of 303 was never implemented\n>and the code was later deprecated.  There was no clear statement for 302\n>regarding whether or not the redirected request should always be a GET\n>(as in See Other) or a repeat of the original request.  When Henrik and I\n>started rewriting the HTTP/1.0 specification in October 1994, all of the\n>code we had access to redirected with the same method, so that is how we\n>defined it.\n>\n>The issue did not arise in the WG until 23 March 1995,\n>\n>   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q1/0189.html\n>\n>and was later repeated on 18 June 1995, when Bill Perry asked about it\n>\n>   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q2/0196.html\n>\n>and Henrik gave a convincing reason why he implemented it as described\n>in the specification, and why we should use 303 for \"See Other\". \n>Consensus was that different people implemented it differently, and\n>therefore a decision was made that the most natural definition would be\n>given as 302 and a new status code 303 would be used for \"See Other\".\n>\n>The discussion arose again on 1 September 1995 when Chris Lilley posed an\n>alternative solution in\n>\n>   http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0548.html\n>\n>and the only response was mine (regarding we already solved it with 303).\n>The next time it was discussed was 30 Jan 1996\n>\n>   http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q1/0173.html\n>\n>Please note that at no time during this period did any of the Browser\n>developers say that they were not going to implement 303, or that they\n>preferred some solution other than 303.  We have an interoperability\n>problem now because, apparently, none of the major browser vendors read\n>specifications in 1995.\n>[...]\n\nSee the appended, additional message from that set, also highly\ngermane to your review.\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n<URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q1/0199.html>\nThu, 23 Mar 95 17:47:59 -0600\n   \n  Jeffrey Perry writes:\n  >What appears to happen when a POST is redirected is the client attempts a\n  >GET at the new URL so that the user can fill-in the form again (it may have\n  >changed when it moved)\n\n  There are several pages on the Web (\"The Revolving Door\" comes to mind)\n  which, on the basis of form input, redirect you to one of a number of\n  different pages.  These pages then need to be retrieved using GET.  This\n  usage appears to be much more common than a redirect where the client is\n  supposed to POST again on the target system.\n\n  At a very early stage of development, Enhanced Mosaic would actually do\n  another POST after a redirect.  In every case that this happened, it\n  generated incorrect results (like an \"Action not allowed\" response).  I\n  think that switching to GET after a redirect is current practice for most\n  browsers.\n\n  --\n  Jim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Interoperability Even",
            "content": "  I think that it would be a good idea to hold an HTTP/1.1\n  interoperability demonstration event.  The goal would be to showcase\n  the commitment of all participants to the 1.1 standard and to\n  discover interoperability prolems.  This kind of event has been done\n  before for other network protocols, and is a great way to get media\n  attention and to 'focus the minds' of developers.\n\n  There are a number of things to be agreed upon; I present my first\n  cut at such a list just to get the ball rolling:\n\n  Venue (When & Where):\n\n    It seems to me that the most natural venue would be\n    Networld/Interop, since the Interop part of that event was\n    originally organized around just this sort of thing.  The last\n    Interop in Las Vegas featured an interoperability demonstration of\n    multicast IP vendors.  However, the next one in Atlanta (Oct) is\n    too soon, and the one after that in Las Vegas (May 98) is too\n    late.\n\n    Of course, theoretically at least we could just do the whole thing\n    over the Internet... if all particapants used published host names\n    and had a telephone contact for each host name...\n\n  Goals:\n\n    I'd like to see a list of demonstration goals; probably the MUST\n    and SHOULD list from HTTP/1.1 would be a good starting point,\n    which means that there would be more than one set of goals: origin\n    servers, proxies, and browsers.  (If we did get together\n    physically we could organize it that way physically - browsers at\n    one end, origin servers at the other, and proxies in between :).\n\n  Non-Goals:\n\n    I'd also like to see some ground rules for things we agree _not_\n    to do so that we keep the message positive and simple enough for\n    our friends in the media to get right.\n\n    This is about HTTP/1.1, so demonstrations of integration with\n    mail, ftp, video streaming and other non-HTTP operations should be\n    out of bounds (or at least clearly separate from the HTTP\n    demonstrations).\n\n    This is not about making other vendors look bad; it is about\n    making the protocol look good.  To the extent that there needs to\n    be any final statement from the event organization, it should come\n    from neutral third parties (perhaps we could recruit some\n    interested academics who are not connected to any vendor?).\n\n  I've copied this to the W3C http list just to pick up any interested\n  parties there; perhaps this could actually be done under thier\n  auspices?\n\n  Thoughts?\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "My memory was the consensus in Munich was just to swap the codes,\nrather than crufting up the protocol more.  \n\nThe discussion was of order:\n\nEither introducing a new code, or doing the swap will work fine,\nbut given the cat isn't really out of the bag (yet), that people\npresent preferred not to have yet another wart in the protocol.\n\nThe people present included both apps area directors, and representatives\nof both Netscape and Microsoft.\n\nAnyone else at the meeting have a different memory of the discussion?\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "BTW, I did update the Issues list right after Munich, and it confirms\nmy memory (at least it means I remember what I remembered!).\n\nThe issues list is your friend...\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": "(Interoperability event)\n\n>   Goals:\n> \n>     I'd like to see a list of demonstration goals; probably the MUST\n>     and SHOULD list from HTTP/1.1 would be a good starting point,\n\nI agree.\n\n\n>     which means that there would be more than one set of goals:\n\n  origin servers -- Compatibility, thread management and memory usage\n  proxies        -- Compatibility, tendency to kill pipeline\n  browsers       -- User Agent testing scripts, cache behaviour, speed\n\n>   Non-Goals:\n> \n>     I'd also like to see some ground rules for things we agree _not_\n>     to do so that we keep the message positive and simple enough for\n>     our friends in the media to get right.\n\n> \n>     This is about HTTP/1.1, so demonstrations of integration with\n>     mail, ftp, video streaming and other non-HTTP operations should be\n>     out of bounds (or at least clearly separate from the HTTP\n>     demonstrations).\n\n\nAlso, we should keep-away from virtual protocols (RSVP over IP)\nand network bandwidth reservation.\n\n> \n>     This is not about making other vendors look bad; it is about\n>     making the protocol look good.  \n\n\"It's better to look good than to feel good.\" -- Fernando :) :)\n\n>     To the extent that there needs to\n>     be any final statement from the event organization, it should come\n>     from neutral third parties (perhaps we could recruit some\n>     interested academics who are not connected to any vendor?).\n\nAgree\n\n> \n>   I've copied this to the W3C http list just to pick up any interested\n>   parties there; perhaps this could actually be done under thier\n>   auspices?\n> \n>   Thoughts?\n\nMy feelings here are that this is a LINUX like activity: This should be\ndone over the net, and immediately -- let's create a list\nof servers, their download locations and installation notes,\n-- Let's move this to a different list (or create another list)\n\nOpen items:\n\n\n1. A definitive list of the available for download HTTP 1.1 compliant\nservers (IIS4, Apache, Jigsaw, etc.)\n\n2. HTTP 1.1 compliant browsers (communicator, ie, etc.) list\nand download sites.\n\n3. Participating webmasters list\n\n4. Hardware inventory for the servers from (3)\n\nOnce this is in place, we can draft up a testing plan.\nI can set up a couple of servers for this to handle \na list as soon as (or if) we hit consensus.\n\nWho's for an HTTP 1.1 free pizza and stone soup party?\n;)\n\n--------\n\nTurner Rentz, III\nAdvanced Technology and Research, Inc.\nhttp://www.atr.net\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": ">>>>> \"TR\" == \"Turner Rentz <Turner> writes:\n\n>> Goals:\n\nTR>   origin servers -- Compatibility, thread management and memory usage\n\n  By thread management I assume you refer to persistent connection\n  usage - that seems like a good idea (how often does server X force a\n  close?).\n\n>> Thoughts?\n\nTR> My feelings here are that this is a LINUX like activity: This should be\nTR> done over the net, and immediately -- let's create a list\nTR> of servers, their download locations and installation notes,\nTR> -- Let's move this to a different list (or create another list)\n\n  There is a list of servers available on the net today at the\n  HTTP/1.1 Implementors Forum @ W3C:\n\n    http://www.w3.org/Protocols/HTTP/Forum/\n\n  Until Larry tells us this needs to move, I think that we can\n  continue to use this list.\n\nTR> 4. Hardware inventory for the servers from (3)\n\n  I think that the event organizers need provide no more than a\n  network; vendors will expect to bring thier own platform hardware.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": "I think we were really thinking more of an 'interoperability test'\nthan an 'interoperability demonstration'. The goal is 'figure\nout whether people's HTTP/1.1 implementations work with each other,\nand if not, what's wrong'.\n\nA secondary goal is 'document interoperable use of each HTTP/1.1\nfeature with independent implementations', in order for us to\nmove to 'draft standard' status. That means that a checklist going\ndown HTTP/1.1 headers and seeing if they're emitted, responded\nto correctly, etc., in various combinations would be a good idea.\n\nI think it's important that organizations be able to test\nimplementations anonymously, so that there's no press leakage or story\ntelling.\n\nLarry\n\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Jim Gettys:\n>\n>My memory was the consensus in Munich was just to swap the codes,\n>rather than crufting up the protocol more.  \n>\n>The discussion was of order:\n>\n>Either introducing a new code, or doing the swap will work fine,\n>but given the cat isn't really out of the bag (yet), that people\n>present preferred not to have yet another wart in the protocol.\n>\n>The people present included both apps area directors, and representatives\n>of both Netscape and Microsoft.\n>\n>Anyone else at the meeting have a different memory of the discussion?\n\nHere is an excerpt from the meeting minutes I sent to Larry.  My notes\nconfirm what you recall.\n\n 1 301-302 Problems with redirecting requests.\n\n Josh Cohen reported on the conclusions reached in the offline\n discussion about this issue.  The general conclusion reached was that\n the text in the -08 draft should be changed, either by swapping the\n 302 and 303 codes, or by allocating a new code and declaring 302\n obsolete.  The swapping option was preferred.  It was decided to send\n mail to the list soon, proposing the preferred solution, so that this\n issue could be closed.  Yaron Goland urged the working group to close\n this issue quickly.\n\nNote that is was not decided, as far as I recall, *who* should send the\nmessage referred to above to the list.  I have not seen such a message\nyet.\n\n>- Jim\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": ">>>>> \"LM\" == Larry Masinter <masinter@parc.xerox.com> writes:\n\nLM> I think we were really thinking more of an 'interoperability test'\nLM> than an 'interoperability demonstration'. The goal is 'figure\nLM> out whether people's HTTP/1.1 implementations work with each other,\nLM> and if not, what's wrong'.\n\nLM> A secondary goal is 'document interoperable use of each HTTP/1.1\nLM> feature with independent implementations', in order for us to\nLM> move to 'draft standard' status. That means that a checklist going\nLM> down HTTP/1.1 headers and seeing if they're emitted, responded\nLM> to correctly, etc., in various combinations would be a good idea.\n\n  That being the case, I assume that we would like this to take place\n  before the December IETF (which I was going to suggest as another\n  possible venue).  Organizing a physical get-together that soon may\n  be challenging unless someone steps up quickly (all you academics\n  out there - this is your chance to become to HTTP/1.1 what UNH is to\n  FDDI and Scott Bradner at Harvard is to Bridge/Router testing!).\n\nLM> I think it's important that organizations be able to test\nLM> implementations anonymously, so that there's no press leakage or story\nLM> telling.\n\n  As I noted in my last note, a few of us have publicized the\n  locations of 1.1 origin servers that may be used for testing by\n  anyone (we log the Server line from requests to our server, but we\n  don't talk with anyone about the specifics of what we see).  We've\n  all seen postings here resulting from various people doing testing\n  with them.  I've asked for publicly usable 1.1 proxies and not\n  gotten any response.\n\n  We've also done some testing with the 1.1 clients that we've been\n  able to get, and communicated issues to the authors where we found\n  them, but as I said in my first note, an event at a fixed place and\n  time tends to focus the attention of all concerned and produce more\n  definitive results than the kind of ad hoc testing that is going on\n  now.\n\n  I think that there is some value in a publically visible event\n  because for the good of the net as whole we would like to see 1.1\n  displace 1.0 as quickly as possible; the only way that will happen\n  is if the web users and especially server operators see it as\n  something worth upgrading to get.  Perception is important.  That\n  having been said, the public event could easily be separate and much\n  later than the kind of test you suggest, Larry, and perhaps that is\n  the better course to pursue.  We are eager to participate in either\n  or both.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "Klaus Weide:\n>\n[...]\n>The message where I proposed what is summarized above is at \n><URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0402.html>.\n>(For some reason it doesn't show up immediately when one follows the\n>links into the archive from the Issues Page.)  There were several replies\n>that indicated agreement.\n>\n>If the people present in Munich came to a different consensus, I would\n>very much like to know why.\n\nBeing one of the people in Munich, I can report that we had no good\nreasons to prefer swapping over allocating a new code, it is just that\nwe preferred swapping slightly more.  I also recall that we thought\nthat swapping was the solution most often mentioned on the mailing\nlist.\n\nI do not think it is useful to spend too much time arguing which of\nthe two is the better solution, we need to pick one and move on. \n\nKlaus, please tell us whether you can live with the spec using the\nswapping solution.  If you think that swapping is evil, we can try to\nget consensus on the 307 solution.  I for one can live with both\nsolutions, and I think this is true for all people I discussed it with\nin Munich.\n\n>     Klaus\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": "SDL> As I noted in my last note, a few of us have publicized the\nSDL> locations of 1.1 origin servers that may be used for testing by\nSDL> anyone (we log the Server line from requests to our server, but we\nSDL> don't talk with anyone about the specifics of what we see).  We've\nSDL> all seen postings here resulting from various people doing testing\nSDL> with them.  I've asked for publicly usable 1.1 proxies and not\nSDL> gotten any response.\n\n  Having checked with my boss, I can now do this one better.  Any\n  vendor interested in doing interoperability testing with our 1.1\n  origin server anonymously _off_ the Internet should contact me.  I\n  can produce a binary version of our server that we can provide you\n  to experiment with.  All we ask is that:\n\n  - You don't give it to anyone else.\n\n  - Any problems you find with it you tell us, and only us (if we\n    disagree about whether or not something is a problem we can bring\n    the question to this list together).\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "On Fri, 29 Aug 1997, Koen Holtman wrote:\n\n> Klaus Weide:\n> >\n> [...]\n> >The message where I proposed what is summarized above is at \n> ><URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0402.html>.\n> >(For some reason it doesn't show up immediately when one follows the\n> >links into the archive from the Issues Page.)  There were several replies\n> >that indicated agreement.\n> \n> Being one of the people in Munich, I can report that we had no good\n> reasons to prefer swapping over allocating a new code, it is just that\n> we preferred swapping slightly more.  I also recall that we thought\n> that swapping was the solution most often mentioned on the mailing\n> list.\n> \n> I do not think it is useful to spend too much time arguing which of\n> the two is the better solution, we need to pick one and move on. \n> \n> Klaus, please tell us whether you can live with the spec using the\n> swapping solution.  If you think that swapping is evil, we can try to\n> get consensus on the 307 solution.  I for one can live with both\n> solutions, and I think this is true for all people I discussed it with\n> in Munich.\n\nGiven that, I would much really prefer to see consensus reached on the 307\nproposal.\n\nThe swapping proposal has the advantage that it looks simpler.  It would\nbe a nice thing if this was the first HTTP RFC to be published, with no\nprior history.  It is also easier to remember as a catchword, so I can\nunderstand why it dominated the discussion.\n\nIMO the alternative\n - takes account of the fact that previous RFCs have specified otherwise,\n - is better for those who have taken previous specs about 302 seriously,\n - is better for those who have taken RFC 2068 about 303 seriously,\n - is better for possible future use of 307 (now 302) functionality.\n\nAs an example for the 2nd and 3rd point, I offer Lynx as client\nimplementation.  A claim that no other (including server side) current\nimplementation of 303 exists which wwoul break (\"The cat isn't really out\nof the bag\") would have to show this not only for servers in the usual \nsense, but for all CGI scripts etc.  \n\nThe disadvantages:\n - one more response code used,\n - an \"aesthetically\" less pleasing spec (\"another wart\"),\n - slightly less trivial editing changes.\n\nI think the advantages outweigh the disadvantages.\n\n\n    Klaus\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n>Klaus Weide:\n>>\n>[...]\n>>The message where I proposed what is summarized above is at \n>><URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/1997q3/0402.html>.\n>>(For some reason it doesn't show up immediately when one follows the\n>>links into the archive from the Issues Page.)  There were several replies\n>>that indicated agreement.\n>>\n>>If the people present in Munich came to a different consensus, I would\n>>very much like to know why.\n>\n>Being one of the people in Munich, I can report that we had no good\n>reasons to prefer swapping over allocating a new code, it is just that\n>we preferred swapping slightly more.  I also recall that we thought\n>that swapping was the solution most often mentioned on the mailing\n>list.\n>\n>I do not think it is useful to spend too much time arguing which of\n>the two is the better solution, we need to pick one and move on. \n>\n>Klaus, please tell us whether you can live with the spec using the\n>swapping solution.  If you think that swapping is evil, we can try to\n>get consensus on the 307 solution.  I for one can live with both\n>solutions, and I think this is true for all people I discussed it with\n>in Munich.\n\nKlaus can answer for himself, but he shouldn't be on the spot\nalone.  I proposed the swap, and perhaps my verbose writting style\ncreated the impression that this was the solution most often mentioned\non the list.  However, that was not my impression, e.g.:\n\nKlaus,\nI thinks this would be the best solution we have so far.  It allows\nlegacy server scripts and clients to work properly with HTTP 1.1 while\npaving the way for 1.1 users to invoke the new and correct behaviors\nwe've defined.\nFrom an implementation stand point it would be trivial to add an\nadditional status code redirect.  But it remains to be seen whether we\ncan make this change in consensus.\n-- Arthur Bierer <arthurbi@MICROSOFT.com>\n\nAgreed.\n-- Yaron Goland <yarong@microsoft.com>\n\nI think we now have a proposed solution for 301/302 (add 307), and\nhope it gets onto the issue list so that we can LAST CALL it.\n-- Larry Masinter <masinter@parc.xerox.com>\n\nThe 307 proposal works. Lets do it.\n-- David W. Morris <dwm@xpasc.com>\n\nSo I did a field test implementation, and Klaus has incorporated that into\nthe Lynx development code set:\n\n* The IETF has indicated intent to adopt KW's \"status 307\" proposal for\n  dealing with the status 302 problems, so HTTP.c and HTAlert.c implement\n  that now.  The 302 status is \"General (temporary) Redirection\" which\n  can be handled as 303 at the UA's discretion, so we do that in all cases,\n  rather than prompting the user whether to do that when the 302 is for\n  a POST submission.  The 307 (and 305) for POST submissions invokes a\n  prompt whether to P)roceed or C)ancel, without the Use G)ET option.\n  The 301 is treated as permanent, normally, but for POST submissions\n  still is treated as temporary and invokes a prompt which includes the\n  Use G)ET option, because scripts written to empirical behavior may\n  still be expecting that (it's virtually never encountered in redirections\n  of POSTs, though, because the document containing the form typically\n  is redirected, so the user only sees the form if a local copy has been\n  made, and it would be best to C)ancel and get a new local copy of the\n  document from the correct http/https URL, to serve as a BASE for the\n  form's ACTION). - FM\n\n\nThis also avoids breaking Lynx v2.6, v2.7, and v2.7.1 (though Lynx is\nnot a \"MAJOR WWW SOFTWARE ORGANIZATION\" :).\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Even",
            "content": "We have (at least) three kinds of agents:\n\n  clients, proxies, origin servers\n\nand probably 'proxy' has several variations. Let's suppose\nwe have C1, C2,..Cnc, P1, P2, P3, and O1, O2, ... Ono. We should\ntest each HTTP operation with at least a reasonable set\nof combinations of headers and header values through all\nnc * np * no combinations client, proxy, and origin server.\nWe should test chains of 0, 1, 2, ... n proxies, against\nHTTP/1.0 versions of legacy clients, proxies & servers.\n\nI'd like to test entity tags, if-match, vary headers, \ncaching, cache timeout, last-modified, redirects, etc.\n\nWe should do some stress testing, aborted connections, persistent\nconnections, connections in the face of (faked) network failures.\n\nI think we could manage to get 6-7 clients, 8-9 proxy servers,\nand 10 origin servers.\n\nWe probably should generate some kind of dummy test client,\ndummy test proxy and dummy test server for systematically\nrunning through various error messages.\n\nI don't know how we can test cookies.\n\nLet's plan on making this a regular event, rather than getting\nit all right the first time.\n\nI'd rather mainly plan to do it on the net rather than a physical\nevent.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "Vary and server parsed document",
            "content": "It is common for servers to parse some documents they serve and\ncreate variations based on headers from the request, the IP address\nof the request, etc.  Under 1.1 the exact length of the resulting\ndocument is not known in advance so typically \"Transfer-encoding: chunked\"\nwill be used.\n\nThe Vary header is designed to list the dimensions along which the\nresponse might vary.  Like the length, this might be difficult to\npredict in advance, or might change as the document changes.\n\nOn the other hand, calculating the proper Vary header as the document\nis being parsed would be simple.  \n\nWould it be possible/reasonable to allow the Vary header in the\ntrailer of a chunked transfer encoding?  This could make many more\nthings cachable and would also make for many more accurate Vary\nheaders in non-cachable responses.\n\nUNREALTED QUERY:  Is there a consensus on the spelling of cachable/cacheable?\nBoth versions are used in draft 08.  Is there an authority we can\nappeal to?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": ">This also avoids breaking Lynx v2.6, v2.7, and v2.7.1 (though Lynx is\n>not a \"MAJOR WWW SOFTWARE ORGANIZATION\" :).\n\nYes it is.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "\"Roy T. Fielding\" <fielding@kleber.ics.uci.edu> wrote:\n>>This also avoids breaking Lynx v2.6, v2.7, and v2.7.1 (though Lynx is\n>>not a \"MAJOR WWW SOFTWARE ORGANIZATION\" :).\n>\n>Yes it is.\n\nOh.  Good.\n\nIt would be a shame if Klaus were now coordinating the development\nof something that's just a figment of our imaginations. :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: 301/30",
            "content": "------------------\n>Note that is was not decided, as far as I recall, *who* should send the\n>message referred to above to the list.  I have not seen such a message\n>yet.\n------------------\n\nI think that person 'is me.\nI've been Out of the Office, though, Ill be back on tuesdey\nAt this pointh, it seems thet the Cat is out of the bag\n\nJosh Cohen\n<josh@netscape.com>\nNetscape Communications\n(via my newton)\n \n\n\n\n"
        },
        {
            "subject": "DRAFT Minutes, HTTP Working Group, Munich IET",
            "content": "These are the *DRAFT* minutes of the Munich IETF, edited by\nLarry Masinter. Many thanks to Koen Holtman, Martin Du\"rst and\nChantelle Cooper, who provided timely drafts of their notes and minutes.\nSend all complaints to the editor.\n\nMonday, August 11\n\nThe group focused on the issues in the issue list\n        http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/\nNumbers in [brackets] are the issue number in the issue list.\n\nThe 08 version of http/1.1 draft did not appear in the\ninternet drafts directory before the meeting; the Internet\nDrafts editor had problems processing last minute submissions.\n\n[1] 301-302, [2] VERSION, [3] RE-VERSION were discussed Monday,\nbut see notes on Tuesday session.\n\n[4] REDIRECTS (How many redirects required?)\nWe discussed the issue of the limit on the number of redirects which\na server can expect a client to implement. Proposal:\nthis belonged in an application profile (of HTTP, HTML, etc.) and not\nin the HTTP spec, which should have mandatory (SHOULD, MUST)\nredirects removed and replaced with an implementation note.\n\n[5] LINK_HEADER\nThere seems to be a conflict between HTML and HTTP regarding the\ninterpretation of Link header. Resolution: Remove the Link\nheader from the main HTTP/1.1 spec. Dan Connolly will submit an\nDraft to align HTTP and HTML definitions of this header.\nThe draft may proceed on standards track independently.\n\n[6] CONTENT-ENCODING\nThe room seemed to be happy with the proposed resolution;\nalso, it seems to be consistent with Apache & MSIE implementations.\n\n[7] CACHING-CGI\nWe got into this state because some origin servers might have counted\non the (simple optimization) that proxies (usually) didn't cache\nURLs with queries or /cgi/ in them. But this was never part of the\nspecification. We concluded that the new text in the -08\nrevision (requiring caches to treat responses as uncacheable based on\nsome patterns in the request URL) should not stay as a requirement.\nJim Gettys will revert the requirements for treating responses as\nuncacheable to those in the 2068 RFC.  He may rework the new URL \npattern text into an implementation note.\n\n[8] WARNINGS\nKlaus Weide sent comments to the mailing list which should be\nresponded to. There are several issues that are left to be resolved.\n\n[10] OPTIONS\n(See notes discussion on Tuesday)\n\n[11] DATE-IF-MODIFIED, [12] 403VS404: these were declared\n'editorial', see next draft for proposed wording.\n\n[13] PUT-RANGE\nProposed resolution: warn that using PUT with byte ranges\nis dangerous and not compatible with 1.0 or servers that don't\naccept-ranges.\nThe WEBDAV group worked on this, but seem to have decided\nthat it was too complex. It was reported that Paul Leach\nPaul Leach will submit a proposal soon; the WG will wait\nand see how complex Paul's proposal might be.\n\n[14] HOST\nJim Gettys will draft proposed text, to the effect\n(\"Proxy can add, but not rewrite\")\n\n[15] AGE-CALCULATION\nThis issue has been debated at length. Jeff Mogul will ubmit an internet\ndraft countering the position on this issue taken in Roy Fielding's\ndraft. Once we have two drafts, we will evaluate them and decide.\n\n[16] RE-AUTHENTICATION-REQUESTED\nShould there be a revalidation request code? This needs mailing\nlist discussion. It's marked as 'drafting'.\n\n[17] SAFE [18] UAHINT\nGiven the current status, Keith Moore's suggested these be released as\nExperimental RFCs. Larry Masinter will initiate this.\n\n[19] AUTH-CHUNKED\nThe proposed fix is in draft 08, and ready for last call.\n\n[20] REQUIRE-DIGEST\nThere needs to be some way to require that Digest authentication\nbe used. The draft has been issued is ready for last call; John\nFranks had comments which need to be responded to.\n\n[21] PROXY-REDIRECT\nThere was a lot of discussion about security and privacy issues,\nand the 08 draft does not address these issues.\nIn particular, the scope restrictions in 305 processing are not\nrestrictive enough, and alternative scoping rules are needed.\nThere may be a privacy concern with respect to redirection of\nclient requests to other servers without informing or asking\nthe user for consent.\nThere's a security difficulty with a permanent redirection to a proxy\nwithout user recognition or consent.\n\nYaron Goland had doubts about defining the 306 functionality inside\nHTTP; formats different from the Set-Proxy header format may be\nmore appropriate as a means of reconfiguring client caches.\nIt was decided to continue this discussion outside the meeting.\n\n[25] PROXY-LENGTH\nThe content-length additions to the 08 draft caused some\nself-contradictions. Koen Holtman will send a message about\nthis to the mailing list.\n\n[34] QUOTED-BACK ([32] COMMENT)\nA compatibility note seemed to be missing from the 08 draft.\nJim Gettys will add this note, with appropriate cross-referencing.\n\n[204] DISPOSITION RFC 1806 \nKeith Moore's Content-Disposition document, which is currently in the\nRFC pipeline, doesn't mention the HTTP usage or syntax.\nLarry Masinter and Keith will put together a paragraph to be added\nto the C-D draft, either before or after it goes to RFC.\n\n[59] CLOSE (Who should close the connection?)\nJim Gettys promised a revised internet draft soon.\n\n[201] REQUIREMENTS (Need table of requirements like RFC 1122 and 1123)\nWe discussed creating the index of HTTP requirements. The requirements\ntable can also serve as a checklist to document independent\nimplementations for each HTTP/1.1 feature.\n\nWe discussed having a 'HTTP/1.1 interoperability test' event,\nas a means of documenting compliance and getting implementor\nfeedback about remaining problems. There was a lot of interest.\nQuentin Clark offered to help organize this, in order to be\nable to report on the results of interoperability by the Dec 97\nIETF.\n\n[**] Basic/Digest\nThere was some confusion about the status of Basic & Digest\nauthentication\nin the -08 draft. The goal is to keep \"HTTP/1.1\" as two documents,\none on the main protocol and a separate draft on \"authentication\",\nbut that both would be required for compliance with HTTP/1.1.\n\nTuesday 8/12\n---------------\n\nCONTENT NEGOTIATION\nKoen Holtman gave an overview of the current Transparent CN drafts\n(draft-ietf-http-negotiation-03.txt, draft-ietf-http-rvsa-v10-02.txt),\nand the history, goals, and test implementations.  \n\nJim Gettys noted that the \"Alternates\" header, and the feature\ntag registration mechanism might progress faster outside of the\nrest of TCN. We decided to split draft-ietf-http-negotiation-03.txt\ninto two drafts, one with the Alternates header which might move\nto Proposed Standard while the rest of TCN can move to Experimental\nstatus.\n\nWe discussed the feature registration drafts\n(draft-ietf-http-feature-reg-02.txt,\ndraft-ietf-http-feature-scenarios-01.txt).  It has wide\napplicability and should go to BCP. Some questions, such as \"should\nfeatures be URLs?\" and \"are the types used the right ones?\" may need\nfurther examination.\n\nTed Hardie and Graham Klyne will expand the current \ndraft-ietf-http-negotiate-scenario-01.txt into a more general\nrequirement and scenarios document that will cover other cases of\nfeature and capability negotiation; it will likely become\nan Informational RFC.\n\nThere is interest in forming a 'negotiation subgroup', which\nmay continue working independent of HTTP-WG.\n\n[2] VERSION [3] RE-VERSION (Version number returned by servers)\nThere was an offline discussion Monday afternoon, reported\non Tuesday.\n\nRFC 2145 might need clarification of the result of the interactions\nbetween CGI scripts, proxies, servers; old CGI scripts working\nwith new servers.\n\nThe entity (or \"message payload\") version number should supercede\nthe server's version number. The HTTP versioning requirements\nin RFC2145 will probably not cause problems for proxies.\nSome language should be added to the 1.1 spec to make it more clear\nwhat proxies should do, and what they cannot be expected to do,\nwhen upgrading and downgrading responses between 1.1 and 1.0.\nIt was suggested that a proxy's cache entries be upgraded\nto the highest version of the client request & server response,\nas a solution to the ambiguity of labelling cache entries.\n\n\n[1] 301-302 (Problems with redirecting requests.)\nJosh Cohen reported on the conclusions of an offline discussion.\n302's historical use has not been changed by most servers and\nscript writers to the new definition in RFC 2068.\n\nThe text in the -08 draft should be changed, either by \na) reverse the meanings of 302 & 303\nb) Deprecate 302 and add 307\nThe consensus of the meeting was to take path (a).\nThe issue will be raised on the mailing list.\n\n[**] PEP discussion\nDiscussion of Henrik's new draft of 14 July centered around\nthe question of how much complexity is needed for\n a) querying about the availability of protocol extensions\n b) negotiating on and detecting the use of protocol extensions\n\nYaron Goland said that the current PEP spec was so complex that he\nfeared that lots of people would only implement subsets, with the\nsubset implementations ending up being incompatible with each other.\n\nSome alternatives to PEP were discussed:\n - the proposed OPTIONS mechanism\n - the old Mandatory header field mechanism\n - an IANA-based header field name registry \n\nOPTIONS is not a subset of PEP capabilities, and isn't intended to be.\n\nIt was the general feeling that something simpler than PEP might well\nbe sufficient for meeting the HTTP protocol extension needs of the\ninternet community, but that the WG did not have sufficient data to\nknow for sure.  Jim Gettys asserted that most potential users of PEP\nwere not in contact with the HTTP-WG.\n\nSome people reported that the JEPI initiative, which was planning to\nuse PEP, is basically `dead'.  Someone from the RTSP group reported\nthat they had implemented a subset of PEP, and that it could be\npossible that the RTSP needs would also be satisfied by OPTIONS or the\nMandatory header.\n\nJim Gettys said that he felt that header clashes due to independently\ndeveloped protocol extension were a real potential problem, and that\nthe the http-wg ought to address this problem in some way, either with\nPEP or with something else.  Koen Holtman remarked that the current\nand previous PEP drafts caused their own header clash problems because\nthey basically allocated all remaining header field names for potential\nuse by PEP, and that this would have to be fixed if work on PEP were to\nbe continued.\n\nKeith Moore remarked that, from a distance, it looked to him as if PEP\nwas a solution in search of a problem, and that PEP review might be\nout of scope for HTTP-WG.\n\n[10] OPTIONS\nJosh Cohen reported on his motivation for proposing the OPTIONS\nmechanism: he discovered that he needed it when specifying the proxy\nredirect mechanisms.  There is an urgent need for a mandatory HTTP/1.1\nextensions discovery mechanism, and that it should be bundled with the\nmain HTTP/1.1 spec. Josh wanted to keep the mechanism very simple,\nso that quick progress could be made, and feared that having a general\nregistration mechanism in addition to the `RFC=' and `HDR=' vocabulary\nin the current options draft would slow things down too much.\n\nDoes the current draft really addresses the necessary option/extension\ndiscovery scenarios?  For example, what if a server does not implement\nsome SHOULDs in an RFC? Which SHOULDs are ignored?\nYaron Goland noted that WEBDAV defined three levels of compliance,\nso that the `RFC=' mechanism in the current options draft would not be\nsufficient for WEBDAV discovery needs.\n\nLarry Masinter (not speaking as the chair) noted that OPTIONS\ndefined yet another name space, and that the namespace of options\ncould probably use the proposed feature tag registry, and asked\nthe working group to consider whether this was desirable. For that\nmatter, the namespace could be shared with PEP extensions.\n\n\nKoen Holtman cautioned against rushing out a mechanism too quickly,\nwithout careful consideration of all details. \n\nWe agreed to finish work on OPTIONS quickly, and consider it as part of\nthe HTTP/1.1 spec or as a separate draft.\n\n[***] Query Internationalization\n\nMartin Du\"rst briefly reviewed the issues in internationalization\nof web form submission, and the possible solutions, as recorded in\nhis recent internet drafts.  This issue goes beyond HTTP, but\nthe proposed the proposed solution involves a negotiation mechanism\nbased on a HTTP header (Query-UTF-8).\n\nThose interested should discuss this issue on the \nwww-international@w3.org mailing list.\n\n[***] Status codes sharing\nJohnathan Rosenberg was going to present draft-schulzrinne-http...\nbut was missing from the room; we wound up not discussing the issue.\n\n[***] State Management\n\n  [--] Cookie Comment/CommentURL\n\nThe working group mailing list seemed to have been filled with\ndiscussion\nof the Comment-URL feature.\n\nLarry Masinter gave his (personal) views on the comment and commentURL\nmechanisms. While it is a very good thing for service authors to\ninform users about the cookie-related privacy policies of the site,\nthe comment and commentURL mechanisms are not the appropriate\nmechanisms to convey this information, and that these mechanisms\nshould be removed from the draft.  There was some agreement on\nremoving comment and commentURLs from the audience.\n\nKoen Holtman said that, lacking a concrete proposal for an alternative\nmeans of conveying the information, he did not mind if comment and\ncommentURLs stayed in.\n\n [--] Set-Cookie2\n\nThere's some concern that discussion of the technical protocol in\nRFC 2109 was being held up due to concern about the privacy\nconsiderations and provisions in it. The objection to RFC 2109\nis that it does not reflect current practice. Yaron Goland\nasserted that the changes in 2109 from Netscape's first cookie\nspec are not wanted and will not be implemented.\n\nLarry Masinter suggested separating the draft into two parts,\nalthough releasing them simultaneously. One part would discuss\nthe protocol, and the other draft would discuss the privacy\nconsiderations.\n\nDan Jaye reported on his recent revision of the trust-state draft: he\nsaid he would mail draft-ietf-jaye-trust-state-01.txt to the list\nsoon (an attempt to mail it before the meeting failed).  Dan Jaye and\nYaron Goland asserted that implementers wanted to use PICS-based\ncookie labeling as a privacy technology, and not the proposals in the\nstate management draft.  Yaron Goland said that, in his opinion, the\neffort on revising the Set-Cookie2 based state management draft should\nbe stopped.  In stead, the http-wg should concentrate on writing a\nstandard which documents the current Cookie header practice, limiting\nitself to security issues.  In his opinion, the http-wg should not try\nto decide on privacy mechanisms, but instead document whatever privacy\nmechanisms the browser implementers would end up using. \n\nIt was noted that Cookies as used in practice present a security\nissue since they are used for authentication, and might represent\npasswords in the clear.\n\n[**] WG SCHEDULE & MILESTONES\nLarry Masinter led a discussion of the WG schedule.  The goal is to\nmove HTTP/1.1 to draft standard in November 1997.  The content\nnegotiation work should also be completed in November 1997. \nIt was suggested that we could complete work on every document\noutside of the main non-HTTP/1.1 working items (for example TCN,\nstate management) before the next IETF meeting in December.\nClosing the WG at or before the December 1997 IETF meeting is\nnot realistic.  Ongoing implementation efforts and planned\ninteroperability tests may lead to additional HTTP/1.1 issues, and it\nis unlikely that the http-wg will be able to close all these issues\nbefore December.\n\nCurrent schedule:\n  07/97: Hit metering to Proposed\n  08/97: additional drafts\n  09/97: additional drafts\n  11/97: HTTP/1.1 -> Draft Standard\n         Content negotiation drafts -> Experimental\n\nThe working group meeting ended with an announcement of the\nHTTP-NG requirements BOF on Thursday.\n\n\n\n"
        },
        {
            "subject": "Proposal for Weekly Internet HTTP/1.1 Test",
            "content": "  So far, the response to my survey questions has been underwhelming.\n\n  I am hoping that this is because there was not a sufficiently\n  concrete proposal.  If it was because you just haven't gotten to it,\n  feel free to do so.  In any case, I'd like to put forward a somewhat\n  more structured idea in hopes that it will generate some more\n  interest.  I don't consider any part of the following to be cast in\n  stone; please suggest improvements.\n\n  ================\n\n  Each Thursday from 16 October 1997 through 20 November 1997\n  inclusive will be a Test Day; anyone who plans to participate must\n  send mail by 23:55 GMT on the preceding Tuesday to a central\n  registrar.  The registrar will forward the information by email on\n  Wednesday to all who have signed up.  Each participant provides:\n\n    Organization:\n\n    User-Agent or Server string: (may be approximate)\n\n    HTTP Role: [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address: (DNS host names and/or IP addresses for the HTTP\n              implementation)\n\n    Time: (GMT times the system will be active or available)\n\n    Contact Name:  (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours: (GMT times the contact is available, should overlap\n                    the span in Time, but may be a subset)\n\n    Notes: other relevant information, possibly including URLs for\n           information on where to find background material.\n\n  A single organization may register as many participating systems as\n  they need, but should register separately for different roles or\n  substantially different configurations so that other participants\n  can distinguish them easily (for example, my company will probably\n  make available one origin server that is using a clock and one that\n  is not).\n\n  Note that HTTP Version is included; it will be useful to have 1.0\n  (and even 0.9) implementations participating in all roles,\n  especially if they are instrumented to provide traces.\n\n  Some ground rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week.\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec03: $Version and pat",
            "content": "Gisle Aas <aas@bergen.sn.no> wrote on 19 Sep 1997 11:59:27 +0200:\n\n  > dmk@research.bell-labs.com (Dave Kristol) writes:\n  > \n  > > Gisle Aas <aas@bergen.sn.no> wrote on Mon, 15 Sep 1997 21:41:31 +0200:\n  > > \n  > >   > I have tried to implement support for cookies in libwww-perl based on\n  > >   > draft-ietf-http-state-man-mec-03.txt and have trouble finding answers\n  > >   > to the following two questions.\n  > >   > \n  > >   >   1) What to do for \"Cookie: $Version=?\" if the matching \"Set-Cookie2\"\n  > >   >      fields have different value of their Version attributes?  What if\n  > >   >      you have a mix of old \"Set-Cookie\" and \"Set-Cookie2\" fields that\n  > >   >      match.\n  > > \n  > > Think of sending separate Cookie headers for each Set-Cookie[2].  The\n  > > $Version you send with each Cookie header corresponds to the one you\n  > > received in the Set-Cookie[2] for that cookie.  So a $Version applies\n  > > to all cookie-value's that follow it lexically in a Cookie header.\n  > \n  > or just use \",\" as separator before a new $Version attribute?\n  > \n  >   Cookie: $Version=1; foo=bar; $Path=\"/foo\", $Version=2; foo=bar\n\nYes, we're saying the same thing.  If there are multiple Cookie headers,\nthey may be folded together using a ',' separator.  The end result would\nbe what you've shown.\n\n  > \n  > >   >   2) Can the path attribute contain URL escapes (%XX) as substitues for\n  > >   >      the \"real\" chars?\n  > >   >      Does '\"Set-Cookie2: ...;  Path=\"/foo%2f%62ar\"' match for a URL\n  > >   >      like http://www/%66oo/bar?   If not, what are the rules?\n  > > \n  > > You are correct that the specification does not say.  It should.\n  > > Should we allow/require the value for the Path attribute to be\n  > > URL-encoded?\n  > \n  > what I have done now is to let URL-encoded chars and unencoded chars\n  > match and then let \"%2F\" and \"/\" be the exception.  Perhaps \";\" should\n  > be special too?\n\nI'm not sure I understand what you're saying.  It sounds like you're saying\nthe % escapes wouldn't be interpreted, except sometimes.  So\nPath=\"/%66oo\"\nandPath=\"/foo\" \nwould be different, but\nPath=\"/foo%2F\"\nandPath=\"/foo/\"\nwould be the same.\n\nThat would be silly, IMO.  Rather, it would be better either to\ninterpret all % escapes or none.\n\nA look at RFC 822 for quoted-string shows that the stuff inside the\nquotes can be essentially anything.  In particular, it (qtext) can be any\nCHAR except \", \\, or CR, and those can be specified (quoted-pair) as an\nescaped pair, as in \"\\\\\\\"\" for the string \\\".  So there really isn't any\nneed for URL encoding.  (I was probably sleeping when I answered your\nquestion the first time. :-)\n\n  > \n  > \n  > I have another question.  draft-ietf-http-state-man-mec-03 says:\n  > \n  > | If multiple cookies satisfy the criteria above, they are ordered in the\n  > | Cookie header such that those with more specific Path attributes precede\n  > | those with less specific.  Ordering with respect to other attributes\n  > | (e.g., Domain) is unspecified.\n  > \n  > First of all I don't understand why you want to impose this order.\n  > Can you comment on that?\n\nYes.  The order derives from the order described in Netscape's original\ncookie spec.:  \"When sending cookies to a server, all cookies with a\nmore specific path mapping should be sent before cookies with less\nspecific path mappings.\"\n\n  > \n  > Does this apply to cookies both with a specified and a default path?\n\nYes.\n\n  > Does paths belonging to different domains have to be ordered by most\n  > specific path?\n\nAccording to my reading of section 4.3.4, yes.\n\n  > \n  > Consider these:\n  > \n  >   Set-Cookie2: foo=bar1; Path=\"/foo\"; Domain=\"www.acme.com\"; Version=1\n  >   Set-Cookie2: foo=bar2; Path=\"/foo/bar\"; Domain=\".acme.com\"; Version=1\n  >   Set-Cookie2: foo=bar3; Version=1; (response from http://www.acme.com/foo/bar/baz/)\n  > \n  > and a request for\n  > \n  >   http://www.acme.com/foo/bar/baz/x.html\n  > \n  > Does the order have to be\n  > \n  >   Cookie: $Version=1;\n  >              foo=bar3;\n  >              foo=bar2; $Path=\"/foo/bar\"; $Domain=\".acme.com\";\n  >              foo=bar1; $Path=\"/foo\"; $Domain=\"www.acme.com\"\n  > \n  > or is it unspecified (which I hope :-)?\n\nI think it has to be as you've shown it, based on how I read the spec.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec03: $Version and pat",
            "content": "dmk@research.bell-labs.com (Dave Kristol) writes:\n\n> Gisle Aas <aas@bergen.sn.no> wrote on 19 Sep 1997 11:59:27 +0200:\n> \n>   > what I have done now is to let URL-encoded chars and unencoded chars\n>   > match and then let \"%2F\" and \"/\" be the exception.  Perhaps \";\" should\n>   > be special too?\n> \n> I'm not sure I understand what you're saying.  It sounds like you're saying\n> the % escapes wouldn't be interpreted, except sometimes.  So\n> Path=\"/%66oo\"\n> andPath=\"/foo\" \n> would be different, but\n> Path=\"/foo%2F\"\n> andPath=\"/foo/\"\n> would be the same.\n\nNo, I meant the opposite.  \"%2F\" denotes a literal slash in a path\nsegment.  \"/\" is the path segment separator.  They are not the same\nthing and I don't think they should match (but \"%2f\" and \"%2F\" is the\nsame thing).  \"%66\" and \"f\" is encodings for the same unreserved\ncharacter, so they match.\n\ndraft-fielding-url-syntax-07.txt says:\n\n| 2.3. Unreserved Characters\n|\n|   Data characters which are allowed in a URL but do not have a reserved\n|   purpose are called unreserved.  These include upper and lower case\n|   letters, decimal digits, and a limited set of punctuation marks and\n|   symbols.\n|\n|      unreserved  = alphanum | mark\n|\n|      mark        = \"$\" | \"-\" | \"_\" | \".\" | \"!\" | \"~\" |\n|                    \"*\" | \"'\" | \"(\" | \")\" | \",\"\n|\n|   Unreserved characters can be escaped without changing the semantics\n|   of the URL, but this should not be done unless the URL is being used\n|   in a context which does not allow the unescaped character to appear.\n\n[...]\n\n| 4.3.2. Path Component\n|\n|   The path component contains data, specific to the site (or the scheme\n|   if there is no site component), identifying the resource within the\n|   scope of that scheme and site.\n|\n|      path          = [ \"/\" ] path_segments\n|\n|      path_segments = segment *( \"/\" segment )\n|      segment       = *pchar *( \";\" param )\n|      param         = *pchar\n|\n|      pchar         = unreserved | escaped | \":\" | \"@\" | \"&\" | \"=\" | \"+\"\n|\n|   The path may consist of a sequence of path segments separated by a\n|   single slash \"/\" character.  Within a path segment, the characters\n|   \"/\", \";\", \"=\", and \"?\" are reserved.  Each path segment may include a\n|   sequence of parameters, indicated by the semicolon \";\" character.\n|   The parameters are not significant to the parsing of relative\n|   references.\n\nWhich might suggest that \"/\", \";\", \"=\", and \"?\" should not match their\nURL-encoded versions.\n\n\n> A look at RFC 822 for quoted-string shows that the stuff inside the\n> quotes can be essentially anything.  In particular, it (qtext) can be any\n> CHAR except \", \\, or CR, and those can be specified (quoted-pair) as an\n> escaped pair, as in \"\\\\\\\"\" for the string \\\".  So there really isn't any\n> need for URL encoding.  (I was probably sleeping when I answered your\n> question the first time. :-)\n\nRFC-2068 says:\n\n|          CTL            = <any US-ASCII control character\n|                           (octets 0 - 31) and DEL (127)>\n|\n|          TEXT           = <any OCTET except CTLs,\n|                           but including LWS>\n|\n|          quoted-string  = ( <\"> *(qdtext) <\"> )\n|\n|          qdtext         = <any TEXT except <\">>\n\nwhich makes me believe that CTLs are hard to represent literally in a\nquoted-string.\n\n\n\n>   > \n>   > I have another question.  draft-ietf-http-state-man-mec-03 says:\n>   > \n>   > | If multiple cookies satisfy the criteria above, they are ordered in the\n>   > | Cookie header such that those with more specific Path attributes precede\n>   > | those with less specific.  Ordering with respect to other attributes\n>   > | (e.g., Domain) is unspecified.\n>   > \n>   > First of all I don't understand why you want to impose this order.\n>   > Can you comment on that?\n> \n> Yes.  The order derives from the order described in Netscape's original\n> cookie spec.:  \"When sending cookies to a server, all cookies with a\n> more specific path mapping should be sent before cookies with less\n> specific path mappings.\"\n> \n>   > \n>   > Does this apply to cookies both with a specified and a default path?\n> \n> Yes.\n> \n>   > Does paths belonging to different domains have to be ordered by most\n>   > specific path?\n> \n> According to my reading of section 4.3.4, yes.\n\nOK, I'll update my implementation to reflect this.\n\nThanks, for you answers!\n\nRegards,\nGisle\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec03: $Version and pat",
            "content": "Gisle Aas <aas@bergen.sn.no> wrote:\n\n  > dmk@research.bell-labs.com (Dave Kristol) writes:\n  > \n  > > Gisle Aas <aas@bergen.sn.no> wrote on 19 Sep 1997 11:59:27 +0200:\n  > > \n  > >   > what I have done now is to let URL-encoded chars and unencoded chars\n  > >   > match and then let \"%2F\" and \"/\" be the exception.  Perhaps \";\" should\n  > >   > be special too?\n  > > \n  > > I'm not sure I understand what you're saying.  It sounds like you're saying\n  > > the % escapes wouldn't be interpreted, except sometimes.  So\n  > > Path=\"/%66oo\"\n  > > andPath=\"/foo\" \n  > > would be different, but\n  > > Path=\"/foo%2F\"\n  > > andPath=\"/foo/\"\n  > > would be the same.\n  > \n  > No, I meant the opposite.  \"%2F\" denotes a literal slash in a path\n  > segment.  \"/\" is the path segment separator.  They are not the same\n  > thing and I don't think they should match (but \"%2f\" and \"%2F\" is the\n  > same thing).  \"%66\" and \"f\" is encodings for the same unreserved\n  > character, so they match.\n  > \n  > draft-fielding-url-syntax-07.txt says:\n  > \n  > | 2.3. Unreserved Characters\n  > |\n  > |   Data characters which are allowed in a URL but do not have a reserved\n  > |   purpose are called unreserved.  These include upper and lower case\n  > |   letters, decimal digits, and a limited set of punctuation marks and\n  > |   symbols.\n  > [...]\n  > | 4.3.2. Path Component\n  > |\n  > |   The path component contains data, specific to the site (or the scheme\n  > |   if there is no site component), identifying the resource within the\n  > |   scope of that scheme and site.\n  > |\n  > |      path          = [ \"/\" ] path_segments\n  > |\n  > |      path_segments = segment *( \"/\" segment )\n  > |      segment       = *pchar *( \";\" param )\n  > |      param         = *pchar\n  > |\n  > |      pchar         = unreserved | escaped | \":\" | \"@\" | \"&\" | \"=\" | \"+\"\n  > [...]\n  > Which might suggest that \"/\", \";\", \"=\", and \"?\" should not match their\n  > URL-encoded versions.\n  >\n  > [DMK's inappropriate citation of RFC 822 deleted]\n\nOkay, I see your point now.  I think what you're saying, in fact, is\nany character that is not a pchar must be escaped, and that an explicit\nnon-pchar does not match its escaped form.\n\nSo now we need words to say that.  How about these, for 4.3.4, under\nPath Selection:\n\nTwo paths match if\n- respective characters match exactly; or\n- respective characters are represented by % escapes, and the values\nof the % escapes are equal; or\n- one path has an explicit character, the second path has a % escape,\nthe % escape evaluates to the character, and the character\nis a pchar (RFC 2068).\nOtherwise the paths do not match.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec03: $Version and pat",
            "content": "dmk@research.bell-labs.com (Dave Kristol) wrote:\n>Gisle Aas <aas@bergen.sn.no> wrote:\n>\n>> dmk@research.bell-labs.com (Dave Kristol) writes:\n>> \n>> > Gisle Aas <aas@bergen.sn.no> wrote on 19 Sep 1997 11:59:27 +0200:\n>> > \n>> >   > what I have done now is to let URL-encoded chars and unencoded chars\n>> >   > match and then let \"%2F\" and \"/\" be the exception.  Perhaps \";\" should\n>> >   > be special too?\n>> > \n>> > I'm not sure I understand what you're saying.  It sounds like you're saying\n>> > the % escapes wouldn't be interpreted, except sometimes.  So\n>> > Path=\"/%66oo\"\n>> > andPath=\"/foo\" \n>> > would be different, but\n>> > Path=\"/foo%2F\"\n>> > andPath=\"/foo/\"\n>> > would be the same.\n>> \n>> No, I meant the opposite.  \"%2F\" denotes a literal slash in a path\n>> segment.  \"/\" is the path segment separator.  They are not the same\n>> thing and I don't think they should match (but \"%2f\" and \"%2F\" is the\n>> same thing).  \"%66\" and \"f\" is encodings for the same unreserved\n>> character, so they match.\n>> \n>> draft-fielding-url-syntax-07.txt says:\n>> \n>> | 2.3. Unreserved Characters\n>> |\n>> |   Data characters which are allowed in a URL but do not have a reserved\n>> |   purpose are called unreserved.  These include upper and lower case\n>> |   letters, decimal digits, and a limited set of punctuation marks and\n>> |   symbols.\n>> [...]\n>> | 4.3.2. Path Component\n>> |\n>> |   The path component contains data, specific to the site (or the scheme\n>> |   if there is no site component), identifying the resource within the\n>> |   scope of that scheme and site.\n>> |\n>> |      path          = [ \"/\" ] path_segments\n>> |\n>> |      path_segments = segment *( \"/\" segment )\n>> |      segment       = *pchar *( \";\" param )\n>> |      param         = *pchar\n>> |\n>> |      pchar         = unreserved | escaped | \":\" | \"@\" | \"&\" | \"=\" | \"+\"\n>> [...]\n>> Which might suggest that \"/\", \";\", \"=\", and \"?\" should not match their\n>> URL-encoded versions.\n>>\n>> [DMK's inappropriate citation of RFC 822 deleted]\n>\n>Okay, I see your point now.  I think what you're saying, in fact, is\n>any character that is not a pchar must be escaped, and that an explicit\n>non-pchar does not match its escaped form.\n>\n>So now we need words to say that.  How about these, for 4.3.4, under\n>Path Selection:\n>\n>Two paths match if\n>- respective characters match exactly; or\n>- respective characters are represented by % escapes, and the values\n>of the % escapes are equal; or\n>- one path has an explicit character, the second path has a % escape,\n>the % escape evaluates to the character, and the character\n>is a pchar (RFC 2068).\n>Otherwise the paths do not match.\n\nYou're getting into the can of worms with which the MHTML-WG has\nbeen struggling, and would be unwise, IHMO, to take it on explicily in\nthe cookie specs as well, rather than leaving it as now, with an implicit\ndefinition of \"match\" as \"whatever the current HTTP specs define that\nto mean for the path field of http[s] URLs (presently in Section 3.2.3\nof RFC 2068 and the -08 revision draft)\".\n\nTechnically, semi-colons should be hex escaped when they are\nnot parameter delimiters, but a URL parameter presently is defined\nonly for ftp URLs (;type=[A, I, or D]) and virutally all deployed UAs\ntreat any unescaped semi-colons \"in\"/\"near\" http[s] URL path fields\nas ordinary characters belonging to the immediately preceding element.\nNote also, that Section 3.2.3, through the -08 draft, specifies parameter\nhandling in http URLs according to RFC 1738 and RFC 1808, but the -07\nURL draft (as in its preceding -0n versions) has changed that in\nrelation to \"current practice\", i.e., it now describes the apparant\nresolving behavior of deployed UAs which in fact are not paying any\nattention to semi-colons as parameter delimiters in http[s] URLs\n(because there aren't any parameters defined for that scheme :).\nPresumeably, the HTTP/1.1 draft's Section 3.2.3 will be updated\nhomologously before it moves on to Draft Standard.\n\nThe -07 URL draft also has wording changes to make clear\nthat URLs are sequences of characters, not necessarily octets, in\nanticipation of explicit standards for internationalized URLs.  When\nthe latter standards are forthcoming (perhaps with parameters to\nindicate charsets) the matching and parameter handling rules might\nneed further revisions.  So, again, it MAY be better to handle the\nissue of path matching via indirect reference to HTTP specs, rather\nthan trying to spell it out explicitly within the cookie specs. :)\n\nFote\n\n=========================================================================\n Foteos Macrides            Worcester Foundation for Biomedical Research\n MACRIDES@SCI.WFBR.EDU         222 Maple Avenue, Shrewsbury, MA 01545\n=========================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Age calculation in HTTP: two Internet Draft",
            "content": "Larry Masinter <masinter@parc.xerox.com> writes:\n\n>I would like to resolve a long-standing controversy in the HTTP\n>working group, by trying to assess the group consensus.\n>\n>    draft-fielding-http-age-00.txt\n>       Roy Fielding, \"Age Header Field in HTTP/1.1\"\n>\n>    draft-mogul-http-age-00.txt\n>       Jeff Mogul, \"Generation of the Age header field in HTTP/1.1\"\n\nAfter reviewing both drafts and dredging my mail logs for the on-list\ndebate, I have to side with Jeff Mogul's take on the issue, both in\nterms of analysis and in his recommendation of \"Interpretation C\".\nI understand Roy Fielding's concern about heirarchical proxies and\nbandwidth, but from my company's experience I have to believe that the\ncorrect approach is to always return the right response no matter how\nmuch bandwidth it wasted.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "POS",
            "content": "Hi All,\n\nI'm a new member of this mailing list and and also new to the HTTP protocol.\n\nMy reason for joining this list is becuase I have some questions I hope you\ncan help me with.\n\nI have recently been working on a program which connects to HTTP servers\nand downloads pages (in a simular way to a browser) so far this has been\nquite easy I have browsed the web and got information on HTTP commands and\nmanaged to get my program to download pages that are potected with basic\nauthentication (pages from my own web site). \n\nI have something like this..\n\nGET /private/index.htm HTTP/1.0\nAuthorization: Basic KryjDHwsyuH286fkOl=\n\nHowever I would now like to push things to the server in the same way as\nthe browser does when you fill out forms (POST/PUT) however I seem to be\ntotally stumped.\n\nCan anyone help?  Just a basic example of what commands are sent to the\nserver when a form is filled out on a browser would be enough.\n\nThanx In Advance.\n-NICK-\n\n\n\n"
        },
        {
            "subject": "cookies and securit",
            "content": "I have a novice question on the use of cookies as a security facility.\nI have not followed the discussion thoroughly, so maybe this\ninformation is already available somewhere. Then please excuse me\nand point me to the proper docs.\n\nI am maintain a number of web pages with some restricted information\nbut the security need not be very tight. We expect that some users\ngive their information to collegues, and in most cases the userids\nand passwords are the same for whole groups. \n\nPeople tend to forget their passwords, and also being nagged\nfor the access information is irritating.\n\nSo I wondered if I could use cookies as authorization. That is,\nthe first time restricted information is accessed, the user needs\nto give the proper userid/password, but later on, if the proper\ncookie is given, this is satisfactory, and the access is granted.\nFrom time to time, say with an interval of some months, the\nusers need to give the userid/passwd again - which then may have\nbeen changed to prevent old users, not allowed anymore, to access the\ninformation.\n\nIt this a recommended practice in some cases, and can this be done\nwith current technology, and how?\n\nKeld Simonsen\n\n\n\n"
        },
        {
            "subject": "Re: draft-ietf-http-state-man-mec03: $Version and pat",
            "content": "Foteos Macrides <MACRIDES@SCI.WFBR.EDU> wrote:\n  > [concerning comparison of Path in cookie spec.]\n\n  > You're getting into the can of worms with which the MHTML-WG has\n  > been struggling, and would be unwise, IHMO, to take it on explicily in\n  > the cookie specs as well, rather than leaving it as now, with an implicit\n  > definition of \"match\" as \"whatever the current HTTP specs define that\n  > to mean for the path field of http[s] URLs (presently in Section 3.2.3\n  > of RFC 2068 and the -08 revision draft)\".\n\nIt probably makes sense to give *some* kind of definition of \"match\"\nin the cookie spec.  Making reference to RFC 2068 sounds like a good\nidea.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "some thoughts on extensions on  REQUEST line",
            "content": "Hello,\nI am software developer developing wide range of applications\nin different platforms and technologies for last eight years. \nFor the last one year I am actively involved in developing web \nbased applications.\n\nI had some thoughts on making browsing more friendlier in\nthis diversified world. I would like to share my thoughts here.\nI hope I am doing the right thing. If this is not what I should\nhave done please ignore this mail. Or if this is already \naddressed, please let me know how this is handled.\n\nIn current scenario, there is no information to the web server\nfrom where the request is coming from, what I mean is the\ngeographical location (apart from IP address) and also the\nlanguage of preference of the user. \n\nSo, my thought was to add a line of info, which will provide \nthe language of preference and geographical location if \nprovided by the request originator (If browsers can extract \nthis info from user's preferences). This information can \nbe used by the 'site's to automatically redirect the request\nto appropriate page which will serve the proper pages.\n\nPossible use of this extensions would be in the \nsites of following organizations:\n\n* Multinational companies can have a single web site which\nroutes the users automatically to their countries local sites.\n\n* Information providers with 'local' interest, like yellow\npages, sale information, advertisements of local interest etc.\n\nThis extension would really make the information on the web\nmore friendlier and 'locale' and would increase the popularity\nand accessibility.\n\n\nThanks,\n\n- Sanjath\n\n\n-- \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nSanjatha R Shringeri.  mailto:sanjath@itsprojects.com \n    Influence Software http://www.influencesw.com\n845, Stewart Drive, Suite D, Sunnyvale, CA, 94086\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n"
        },
        {
            "subject": "Re: some thoughts on extensions on  REQUEST line",
            "content": ">       So, my thought was to add a line of info, which will provide \n>       the language of preference and geographical location if \n>       provided by the request originator (If browsers can extract \n>       this info from user's preferences). This information can \n>       be used by the 'site's to automatically redirect the request\n>       to appropriate page which will serve the proper pages.\n\nLanguage preferences are a part of http and html, and html\nmarkup has been defined to address some internationalization\nissues.\n\nMultilingual sites like you describe are not universal, but\nthey are possible with some clients and servers available today.\n\nGeographic location, so far as I know, hasn't been a widely\nconsidered factor in the more general issue of content negotiation.\n\nI suppose it might help implement an Internet pizza server ;)\n\nThere are privacy issues around both location and language.\n\n\n\n"
        },
        {
            "subject": "Re: some thoughts on extensions on  REQUEST line",
            "content": "   Geographic location, so far as I know, hasn't been a widely\n   considered factor in the more general issue of content negotiation.\n\nGeographic location won't ever be completely accurate.  I'm in the Hawaii\ntimezone right now, but I'm sending this message from a machine in the\nEastern time zone, so if you read the headers you'll think I'm on the\neast coast.\n\nEven if there was a way for me to tell the computers what timezone\nI'm on on the per user level I doubt I'd bother.\n\n\n\n"
        },
        {
            "subject": "Re: some thoughts on extensions on  REQUEST line",
            "content": "Sanjath <sanjath@itsol.itsprojects.com> writes:\n\n>       So, my thought was to add a line of info, which will provide\n>       the language of preference\n\nTake a look at RFC 2068 section 14.4, which describes the Accept-Language\nheader field in HTTP 1.1.  This allows a client to specify the what\nlanguage(s) it will accept for response documents.\n\n>                                  and geographical location if\n>       provided by the request originator\n\nThis has been discussed before, at least somewhat, but nothing ever came\nof it.  If you're interested in the topic, you might start with a\nsearch of the list archives at\n<URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/>.\n\nRoss Patterson\nSterling Software, Inc.\nVM Software Division\n\n\n\n"
        },
        {
            "subject": "Re: some thoughts on extensions on REQUEST line",
            "content": "On Wed, 24 Sep 1997, Ross Patterson wrote:\n\n> Sanjath <sanjath@itsol.itsprojects.com> writes:\n> \n> >       So, my thought was to add a line of info, which will provide\n> >       the language of preference\n> \n> Take a look at RFC 2068 section 14.4, which describes the Accept-Language\n> header field in HTTP 1.1.  This allows a client to specify the what\n> language(s) it will accept for response documents.\n\nYes indeed.\n\n\n> >                                  and geographical location if\n> >       provided by the request originator\n> \n> This has been discussed before, at least somewhat, but nothing ever came\n> of it.  If you're interested in the topic, you might start with a\n> search of the list archives at\n> <URL: http://www.ics.uci.edu/pub/ietf/http/hypermail/>.\n\nThere are issues on a variety of levels:\n\nOn the lowest level, it would be nice if a website could exist\non various servers, and redirection could be automatic to the\nclosest one, to save bandwidth (or should we call this bandlength :-?).\nSomething in DNS might help to do that, or something in HTTP, or\nwhatever. Proxies play some role. Currently, it's mostly by hand,\ne.g. on the Apache site.\n\nCloser to the user, questions range from automatic tax calculation\non price lists, different price lists for different countries,\nmeasures (length, weight, clothes and shoe sizes,...), and so on.\nSome discussions with examples have been taken place on the list\nwww-international@w3.org. You might want to check the archive of\nthat list, too.\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Weekly Internet HTTP/1.1 Test",
            "content": "  The response has been encouraging.  I will volunteer to perform the\n  role of registrar, and will send a reminder on Friday, Oct 10th to\n  send in your notice of participation for the first 'test day' on the\n  16th (a number of respondents, including myself, have indicated that\n  mid-October would be a good time for them to start).\n\n  It has been pointed out that server vendors will mostly be passive\n  participants other than testing against thier own and others servers\n  with publically available clients.  If client vendors wish to make\n  available downloadable copies of thier clients, then some of us on\n  the server side will certainly take advantage of that to help you\n  test your clients.\n\n  To repeat; I will be asking for you to send in the following, filled\n  in so that other participants will know who is out there to test\n  with.  Also, again - please suggest improvements to any part of this\n  plan.\n\n  ================\n\n    Organization:\n\n    User-Agent or Server string: (may be approximate)\n\n    HTTP Role: [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version: (versions earlier than 1.1 _will_ be usefull)\n\n    Address: (DNS host names and/or IP addresses for the HTTP\n              implementation)\n\n    Time: (GMT times the system will be active or available)\n\n    Contact Name:  (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours: (GMT times the contact is available, should overlap\n                    the span in Time, but may be a subset)\n\n    Notes: other relevant information, possibly including URLs for\n           information on where to find background material.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week.\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n--\nScott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\nAgranat Systems, Inc.        Engineering            http://www.agranat.com/\n\n\n\n"
        },
        {
            "subject": "Re: Weekly Internet HTTP/1.1 Test",
            "content": "I would like to strongly encourage EVERYONE engaged in HTTP/1.1\nimplementations to participate in the interoperability tests.\n\nI would like to develop a checklist of 'HTTP features' implemented\nand TESTED FOR INTEROPERABILITY so that we can document the \ninteroperable implementations of HTTP/1.1 features.\n\nDigest authentication, pipelining, status codes, etc.\n\nIf, in the course of development, you created some 'test harnesses'\nfor testing our products, could you perhaps point them at other\npeople's servers?\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Age calculation in HTTP: two Internet Draft",
            "content": "I have been reticent to discuss this issue further because I already\ndiscussed it to death, twice.  Nevertheless, since Jeff spent some\nvery valuable time generating a draft, I will make another attempt to\nexplain the problem.\n\n>   Roy Fielding has taken issue with the statement ``HTTP/1.1 caches\n>   MUST send an Age header in every response.'' [2].  Fielding correctly\n>   points out that the use of the word ``caches'' in this sentence is\n>   incorrect.  He lists two possible rewordings for this sentence:\n>\n>      a) An HTTP/1.1 server that includes a cache MUST send an Age\n>         header field in every response.\n>\n>      b) An HTTP/1.1 server that includes a cache MUST include an Age\n>         header field in every response generated from its own cache.\n>\n>   There is, however, a third possible rewording, if one realizes that\n>   the original drafting error was to sloppily substitute the word\n>   ``cache'' where ``proxy'' was meant (mea culpa):\n>\n>      c) An HTTP/1.1 proxy MUST send an Age header field in every\n>         response.\n>\n>   Note that an HTTP proxy does not necessarily include a cache.  The\n>   other sentences referring to ``caches'' in that paragraph reflect the\n>   same drafting error.\n\nI excluded that possibility because it would require that all proxies\nimplement complete compliance with the Cache-Control and Expires header\nfields, including those that do not and never will cache a message.\nSuch a requirement cannot be justified.\n\n>3.1 Quantifying the error of the Age estimate: Interpretation C\n>   If interpretation (c) is used (i.e., the change proposed above in\n>   section 2), then the value of the Age header field might overestimate\n>   the actual amount of time since the response was generated at the\n>   origin server.  One can calculate the size of the estimation error\n>   for a path containing N HTTP/1.1 proxies:\n>\n>       Error_C = Mean_RTT * N * (N + 1)/2\n>\n>   where Mean_RTT is the mean round-trip time (RTT) between neighboring\n>   pairs of proxies.  Note that this measures the error in the Age value\n>   at the final recipient client, not the error at the last HTTP/1.1\n>   proxy on the path.\n\nThat is entirely bogus.  I gave a complete summary of the age\ncalculation, as defined in the RFC 2068, in my draft (Section 2).\nBecause the age calculation involves the maximum of the clock difference\nbetween the proxy and the origin or the value of the received Age,\nthe estimation error is unbounded with interpretation (c).  For N=3,\nit is in fact\n\n    max(skewUA,max(skewA,max(skewB,max(skewC,0)+d)+(c+d))+(b+c+d))\n\nwhere only the variables b,c,d represent round-trip time.  skewUA, skewA,\nskewB, and skewC, are all unbounded clock skew time errors.  It is those\nvalues that dominate the equation -- Mean_RTT is insignificant and\nirrelevant to this argument, aside from pointing out that interpretation (c)\nresults in overestimation even when clock synchronization is perfect.\n\nWhat I find bizarre is that Jeff spent a great deal of effort demonstrating\nthat the above is unbounded with present HTTP servers.  Clock skew is\nthe enemy of the age calculation, which is why the Age header field must\nnot have an additive dependence on the clock skew of every recipient.\n\n>3.2 Quantifying the error of the Age estimate: Interpretation B\n>   If interpretation (b) is used, then the value of the Age header field\n>   might underestimate the actual amount of time since the response was\n>   generated at the origin server.\n\nThat is also true of interpretation (c).  The only difference is that\na proxy is likely to have a better clock than a UA.\n\n>   The scenario that generates the\n>   greatest underestimate is when the path between the origin server and\n>   the HTTP/1.1 client's cache includes an HTTP/1.0 proxy cache, and the\n>   client has a skewed clock.  In particular, the client's clock is set\n>   into the past by some amount.\n\nCorrection: The only scenario in which interpretation (b) *might* result\nin an underestimate of age which is worse than that of (c) is where one\nor more HTTP/1.0 caches are in the request chain AND the response came\nfrom one of those HTTP/1.0 caches in which it resided for some time AND\nthe user agent's system clock is running behind the origin server's clock.\nIn this one case, (c) would compensate for the clock skew if there existed\nan HTTP/1.1 proxy between the user agent and the HTTP/1.0 cache generating\nthe response AND the HTTP/1.1 proxy is better-synchronized to the origin\nserver clock.  All of these coincidences combined would still have NO\nimpact on the correctness of the response unless the application required\nthe freshness criteria to be strictly enforced for that response AND\nthe current response would be different.\n\n>3.4 Implications of overestimating the Age value: Interpretation C\n>   What happens if the Age value is overestimated?  If this happens,\n>   some \"fresh\" responses appear to be \"stale\", and so unnecessary cache\n>   misses may be generated.  Except in the case where the network is\n>   partitioned, this is a performance problem, but does not lead to the\n>   delivery of responses with the wrong body or headers.  (When the\n>   network is partitioned, caches are allowed to return stale values\n>   with an appropriate Warning, so this is irrelevant to the current\n>   discussion).\n\nIf the network is already congested, and caching is being used to reduce\nthat congestion, then failure to cache will result in denial of service.\nThat is denial of service for every protocol using the same network wire,\nnot just HTTP traffic, and many corporations (and several nations) install\nproxy caches specifically to prevent this scenario.\n\nIf inaccurate Age values cause such congestion, cache administrators\nwill be compelled to ignore Age altogether.  That is why following (c)\nwill result in less semantic transparency in the long run.  In order\nto be trusted, Age must be independent of clock skew.\n\nOther benefits of (b) that were not mentioned include that (b) is a more\naccurate (hence trusted) estimate for all other cases, and that lack of\nan Age header field in a response from an all-HTTP/1.1 chain would\nindicate that the response has been obtained from the origin.\n\n>3.5 Implications of underestimating the Age value: Interpretation B\n>   What happens if the Age value is underestimated?  If this happens,\n>   some \"stale\" responses appear to be \"fresh\", and are returned to the\n>   client without any Warning.  The client, in this case, naively\n>   obtains the wrong response value.\n\nNo, it retains the stale value.  There is no implication that, just because\na response is stale, that it necessarily is invalid or even different from\nwhat the client would have received directly from the origin.  The maximum\nlength of time the response would remain stale is (Expires - Date) or\nthe value of the max-age cache-directive, both of which are defined by\nthe origin server and not subject to UA clock-skew errors.\n\nFurthermore, we can classify all applications of HTTP as either\n\n  1) requires freshness preservation to preserve correctness\nor\n  2) does not require freshness preservation to preserve correctness\n\nFor the first case to work in practice, the origin server MUST prevent\nan HTTP/1.0 proxy from caching the response.  We even provided special\nmeans for an origin server to do so using max-age overriding of Expires,\nand using Via to indicate the presence of an HTTP/1.0 application.\nIf such prevention is not implemented, then the HTTP/1.0 cache will\nprovide stale responses to clients, thus contradicting the requirement\nof freshness preservation.  For the second case, underestimation of age\nis irrelevant.\n\nSince the above fact eliminates the only scenario in which (c) is useful,\nand in all other scenarios (c) is detrimental to the HTTP system,\ninterpretation (b) is the only rational choice.\n\n>   In other fields of computer science (for example, compilers or\n>   multiprocessor caches), aggressive optimization always depends on a\n>   reliable understanding of the situation at hand.  That is, if one\n>   cannot be sure that the transformation done by the optimization\n>   preserves the semantics of the system, one cannot safely do that\n>   optimization (and, consequently, one may be prevented from doing a\n>   lot of other optimizations that are enabled by the first one).  Even\n>   when optimizations are done \"speculatively\", this always involves\n>   being able to check the results for semantic correctness before\n>   committing them.\n\nThere are a lot of optimizations that one cannot do with HTTP.  Those\noptimizations were sacrificed for the sake of a system that does not\ndepend on immediate connectivity and low latency, as do all of the systems\nto which you refer.  Sacrificing the benefits of HTTP in order to\nobtain the benefits of other existing protocols or non-HTTP applications\nis a mistake.  HTTP has very different design constraints, and should\nbe judged according to those constraints, or replaced outright by one\nof those other protocols that better meets the application's needs.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "HTTPWG recommends &quot;SAFE&quot; and &quot;UAHINT&quot; for &quot;Experimental&quot",
            "content": "Unless there is strong objection, I plan to ask that the IESG\nconsider:\n\nftp://ietf.org/internet-drafts/draft-holtman-http-safe-03.txt\nftp://ietf.org/internet-drafts/draft-ietf-http-uahint-01.txt\n\nto become Experimental RFCs, on the recommendation of the HTTP\nworking group.\n\n(More 'last calls' to come, but these are the two simplest cases.)\n\n\n\n"
        },
        {
            "subject": "Calculation of age header",
            "content": "I'm hoping we can resolve this soon, by widening the discussion\nfrom \"Roy vs. Jeff\" to a larger group. It would be very useful\nfor HTTP/1.1 implementors to be explicit about their plans and\nexpectations. Is there some \"conservative in what you send\nand liberal in what you accept (or expect)\" possible compromise\nhere?\n\nComment from Ari:\n\n> ---\n> I much prefer Fielding's draft where all proxies that generate the\n> response from their cache must add an Age: header.\n> \n> I don't want to require *all* proxies, including non-caching ones, to\n> add the Age: header, which is what Mogul's draft suggests.  Since it's\n> just tunneling it doesn't really add any information that the upstream\n> proxy wouldn't be able to derive from its network delay.  Furthermore,\n> this wouldn't affect just non-caching proxies, but also requests that\n> we'd like to tunnel without caching in a caching proxy.\n\n\n\n"
        },
        {
            "subject": "Question about HTTP/1.1 and proxy server",
            "content": "Hi,\n\nAs I don't seem to get any answer to my question within the usenet, you\nmight\nbe able to help me here.\n\nIs there any way, on the server side, to identify if a request\noriginated by an http\nclient has passed through a proxy ? Is there anything that a proxy\nchanges within\nthe HTTP request/header before forwarding it that could be used ?\n\nMany thx in advance\nrgds,\n- Francois Babin\nNortel Europe SA\nfrancois_babin@nortel.com\n(+33) 1 41 99 18 71\n\n\n\n"
        },
        {
            "subject": "Re: Question about HTTP/1.1 and proxy server",
            "content": "> As I don't seem to get any answer to my question within the usenet,\n> you might be able to help me here.\n> \n> Is there any way, on the server side, to identify if a request\n> originated by an http client has passed through a proxy ? Is there\n> anything that a proxy changes within the HTTP request/header before\n> forwarding it that could be used ?\n\nYes, but depending on the version of the proxy server software, this\ninformation may be in different places.  Here are the possibilities:\n\n- \"Via:\" header (HTTP/1.1)\n- \"Forwarded:\" header (some HTTP/1.0 implementations)\n- \"User-Agent: ... via <proxy-software>\" (CERN proxy)\n\nCheers,\n--\nAri Luotonen, Mail-Stop MV-061Opinions my own, not Netscape's.\nNetscape Communications Corp.ari@netscape.com\n501 East Middlefield Roadhttp://people.netscape.com/ari/\nMountain View, CA 94043, USANetscape Proxy Server Development\n\n\n\n"
        },
        {
            "subject": "Re: Calculation of age header",
            "content": "Here are my comments on the Age issue.\n\n1. First, the ambiguity of the words `1.1 cache' in 2086 is more than\njust an editorial error, and it should not be resolved by just\nreplacing the words with `1.1 proxy', which is what the original\neditor of that text meant to write.  If Jeff had originally written\n`1.1 proxy', I would have complained in the last call of 2068.  See\nthe end of\n http://www.ics.uci.edu/pub/ietf/http/hypermail/1996q3/0450.html \nfor some remarks on the history of these words.\n\n2. Like Roy and Ari, I am against requiring that all proxies, even\nnon-caching ones, always add Age.  A firewall proxy (which may not\neven have an internal clock!) should not be burdened by such a\nrequirement.\n\n3. The age header value calculation in the 1.1 spec has three parts,\nwhich I label as follows:\n\n  LOCAL_CLOCK: correct for a too low age value in the received Age\n               header (which can be too low due to storage in an\n               intermediate 1.0 cache) by calculating the age of the\n               response as the difference between the Date header and\n               the local clock, and using this value for the age if it\n               is higher\n  the calculation is:\n      apparent_age = max(0, response_time - date_value);\n      corrected_received_age = max(apparent_age, age_value);\n\n\n  ADD_DELAY: add the delay between the sending of the request message\n             and the receipt of the response message to the age.\n  the calculation is:\n      response_delay = response_time - request_time;\n      corrected_initial_age = corrected_received_age + response_delay;\n\n  ADD_RESIDENT: add the time the response was resident in local cache\n                memory\n  the calculation is:\n      resident_time = now - response_time;\n      current_age   = corrected_initial_age + resident_time;\n\nAs far as I can see, Jeff and Roy take the following positions:\n\nJeff: every proxy MUST do LOCAL_CLOCK and ADD_DELAY, and ADD_RESIDENT\nif applicable.\n\nRoy: every proxy serving a response from its local cache MUST do\nLOCAL_CLOCK, ADD_DELAY, and ADD_RESIDENT.\n\nMy own position is:\n\nKoen: every proxy MAY do LOCAL_CLOCK.  Every proxy serving a response\nfrom its local cache MUST do ADD_DELAY and ADD_RESIDENT.\n\nConsidering the three different actions in isolation:\n\n4. There is no controversy over when a proxy should do ADD_RESIDENT.\n\n5. As far as ADD_DELAY is concerned:\n  - I think there is agreement among Jeff, Roy, and me that\n    a proxy, when, serving a response from cache memory, MUST do\n    ADD_DELAY, because not doing so may lead to underestimating the\n    age.\n  - Jeff's draft has a calculation which shows that always doing\n    ADD_DELAY, even when not serving from cache memory, does not\n    contribute that much to overestimating the age.  I agree with the\n    conclusion, though I have some doubts about th calculation (it\n    fails to take into account a delay due to the DNS lookup time in\n    the final proxy, for example, and don't know right away if it is\n    also applicable if you have a transatlantic link in between.)\n  - Though doing ADD_DELAY in every proxy is not that bad, Jeff's\n    draft does not have any argument or scenario which shows that\n    doing ADD_DELAY in every proxy is actually good for something.\n  - My main concern is that doing ADD_DELAY in every proxy is\n    unnecessary for correctness, and therefore confusing for\n    implementers.  I also find that aesthetically, it is worse than\n    standardising a header name with a spelling error.\n  - In my opinion, the spec should require only proxies serving a\n    response from cache memory to do ADD_DELAY, but I could live with\n    a spec which requires all proxies to do it, _IF_ a large note for\n    implementers is added which confirms that this requirement is\n    actually silly.\n\n6. As far as LOCAL_CLOCK is concerned:\n  - This is the most controversial issue\n  - Jeff argues that every proxy should do LOCAL_CLOCK\n  - He backs this up with a complicated scenario involving a 1.0 cache\n    and a bad user agent clock.\n  - I agree this scenario has a point, but Jeff's inference that,\n    therefore, every proxy should do LOCAL_CLOCK goes to far for\n    me.  I would conclude from this scenario that it is _sometimes_ a\n    good idea for a proxy to do LOCAL_CLOCK.\n  - Roy says that always doing LOCAL_CLOCK in every proxy is too\n    expensive (in terms of overestimating ages which lead  to loss of\n    cache efficiency), and he prefers a less safer, but cheaper system\n    in which only some proxies do LOCAL_CLOCK.\n  - In the second paragraph of section 3.2 of his draft, Jeff seems to\n    argue that it is not possible to have a system which is cheaper,\n    but as safe as, doing LOCAL_CLOCK in every proxy.  But this is\n    wrong. It is possible to be cheaper, and still as safe.  For\n    example:\n       - if a proxy only does LOCAL_CLOCK when detecting (using the\n         via header) that the upstream chain is not a pure 1.1 chain,\n         then this will be cheaper (less change of overestimating the\n         age) but still as safe.\n       - it is possible to for a proxy or user agent to make a\n         conservative measurement of the delta-time between its own\n         clock and the clock of the origin server (by, for example,\n         doing an OPTIONS request on the origin server and looking at\n         the Date header in the response).  By using this delta-time:\n\n           apparent_age = max(0, response_time - date_value + delta-time);\n\n         the calculation which corrects for possible storage in a 1.0\n         cache could be made more accurate, while still as safe\n  - I conclude that the LOCAL_CLOCK correction is a hack to correct\n    for breakage due to 1.0 caches in the chain.  This hack is neither\n    optimal, not will it be necessary anymore in pure 1.1 chains, as\n    Roy has already pointed out.  In pure 1.1 chains it is only a\n    potential source of unnecessary performance degradation.\n  - The spec should not require any proxy to ever do LOCAL_CLOCK.  It\n    should merely encourage a proxy to do LOCAL_CLOCK if:\n      - the proxy knows or suspects that there is a 1.0 proxy cache\n        upstream\n      - and if it knows or suspects that the user agent has a very bad\n        clock, and no sophisticated mechanism to correct for its bad\n        clock\n    Proxies should also be encouraged to do something more\n    sophisticated than LOCAL_CLOCK.\n\nIn summary, I like Roy's proposal better than Jeff's proposal, but I\nwould like it best if the age header value calculation gets rewritten\nas:\n\n      age = age_value;\n\n      // A proxy MAY use a better way of calculating apparent_age\n      // than the formulae below\n      apparent_age = max(0, response_time - date_value);\n\n      // A proxy MAY use any heuristic to compute the \n         correct_age_for_1.0_cache boolean\n      correct_age_for_1.0_cache = \n            <any heuristic, or the value of a configuration option>;\n\n      if(correct_age_for_1.0_cache) age = max(age, apparent_age);\n\n      if( taking_response_from_cache_memory || I_am_a_user_agent )\n      {\n        response_delay = response_time - request_time;\n        age = age + response_delay;\n      }\n\n      if( taking_response_from_cache_memory )\n      {\n        resident_time = now - response_time;\n        age = age + resident_time;\n      }\n\n      new_age_value = age;\n\nand I would like the Age header part of the spec to read:\n\nA HTTP/1.1 caching proxy MUST send an Age header in every response\ntaken from its cache memory.  A HTTP/1.1 non-caching proxy, and a\nHTTP/1.1 caching proxy which is relating an upstream response rather\nthan taking it from cache memory, MAY add and Age header to the\nresponse if non is present, and MAY update the Age header which is\npresent when the age adjustment calculation in the proxy yields a\nchanged value.\n\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Is MHTML only for email",
            "content": "The title of RFC 2110 is \"MIME E-mail Encapsulation of Aggregate Documents,\nsuch as HTML (MHTML)\". The title thus says that this standard is for e-mail\nonly. In several places inside RFC 2110 the proposed standard says that it\nis for sending HTML in e-mail.\n\nHowever, the recent discussion about inheritance of Content-Base has made\nus aware that multipart/related may be sent via HTTP too. That is, it is\nfeasible that an HTTP server sends a multipart/related containing both HTML\ntext and embedded objects as separate body parts within a\nmultipart/related.\n\nQuestion 1: Is it the intention that HTTP can be used in this way?\n\nQuestion 2: Should MHTML be amended to say that it covers all sending of\nHTML in MIME, not only through SMTP but also through HTTP and possibly\nother protocols, for example FTP?\n\nQuestion 3: A main principle of MHTML is that if a HTML document is sent\nas part of a multipart/related, lookup of links in the HTML document\nshould first go to other body parts within the multipart/related,\nand ordinary HTTP lookup should only be done if there is no match\nin another body part. Is this principle true also when multipart/related\nis sent via HTTP?\n\nQuestion 4: If the answer to question 2 is \"yes\", does this mean that\nthe IETF MHTML group should liaise with some other IETF group on this\nissue? I am sending a copy of this message to the mailing list for the\nIETF working group on HTTP. Since I am not a member of that group,\nplease post responses also to the mhtml mailing list.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "Speaking as Chair of MHTML:\n\nrom Jacob's message Sat, 4 Oct 1997 10:55:10 +0200:\n}\n}The title of RFC 2110 is \"MIME E-mail Encapsulation of Aggregate Documents,\n}such as HTML (MHTML)\". The title thus says that this standard is for e-mail\n}only. In several places inside RFC 2110 the proposed standard says that it\n}is for sending HTML in e-mail.\n}\n}However, the recent discussion about inheritance of Content-Base has made\n}us aware that multipart/related may be sent via HTTP too. That is, it is\n}feasible that an HTTP server sends a multipart/related containing both HTML\n}text and embedded objects as separate body parts within a\n}multipart/related.\n}\n}Question 1: Is it the intention that HTTP can be used in this way?\n}\n\nI say yes, though in the beginning we could not see far enough ahead\nto address the more general issues.  But, what is the difference,\nafter receipt, when the MIME Multipart/Related is shipped in an SMTP,\nFTP, SMXP, DISKETTE, CDROM, or HTTP \"pipe\".  \n\nWho should care how it will be shipped.\nWho should care how it was shipped?  \n\nThe main difference is whether the \"pipe\" offers responsive\ninteraction with the server, such that \"missing\" URI objects\ncan/cannot easily be resolved by just asking for the \"missing\" URI\nitem across the available \"pipe\".\n\nMail and HTTP just appear to be the extreme ends of the spectrum.  The\nothers seems to fall in between.  But, certainly we do not intend that\nthe IETF Applications Directorate should be a party to making it hard\nfor Internet Application users to work together by exchanging\nApplication Information Objects in MIME wrappings.\n\n}\n}Question 2: Should MHTML be amended to say that it covers all sending of\n}HTML in MIME, not only through SMTP but also through HTTP and possibly\n}other protocols, for example FTP?\n}\n\nYES!  Full Stop!\n\n}\n}Question 3: A main principle of MHTML is that if a HTML document is sent\n}as part of a multipart/related, lookup of links in the HTML document\n}should first go to other body parts within the multipart/related,\n}and ordinary HTTP lookup should only be done if there is no match\n}in another body part. Is this principle true also when multipart/related\n}is sent via HTTP?\n}\n\nI think, but of course leave it to be decided, that it should be true\nin all Multipart/Related compound objects that do contain additional\nrelated parts.\n\n}\n}Question 4: If the answer to question 2 is \"yes\", does this mean that\n}the IETF MHTML group should liaise with some other IETF group on this\n}issue? I am sending a copy of this message to the mailing list for the\n}IETF working group on HTTP. Since I am not a member of that group,\n}please post responses also to the mhtml mailing list.\n}\n\nYES!  \n\nAnd we have been doing just that with some cross WG participation.\nLarry Masinter is Chair of HTTP WG and he has been a long time\nparticipant in our MHTML WG.  Some MHTML WG people have also been\nparticipants in the HTTP WG, although I have personally withdrawn from\nparticipation in the HTTP sessions or list discussions.\n\nWhat has made all this such a clear issue just now is that we finally\ngot to the crux issues between MHTML and HTTP Specifications where we\nneed to jointly be truly inventive to find a way out of the current\ncircumstances.\n\nIn short, we have a very real conflict between two critically\nimportant installed application bases, both of which promise not to be\nreplaced by the other, so they must be given a way to interwork.  \n\nSo, this is our joint task, between MHTML and HTTP WGs, with some help\nfrom our APP ADs, who have both be participating in our work.\n\nCheers...\\Stef\n\nPS: I notice that the CALSH (Calendar) folk are facing similar\n    difficulties, so we should alert them to our problems, in case\n    they have not yet noticed.  I think some CALSCH WG folk are\n    reading our MHTML and HTTP lists.\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "Jacob,\n\n1) HTTP vs. \"web browsing\"\nIt would be useful to consider separating \"HTTP\" out from the\napplication of \"web browsing\", in the same way that \"SMTP\" is\nseparate from the application of \"mail\". Currently, the world\ncommonly uses \"HTTP\" and \"HTML\" and \"URL\" and various other\ncommon components to deploy the \"web browsing\" application.\nHTTP is also used for many other applications, and the \"web \nbrowsing\" application can be supported using many other protocols,\nas indicated by the URL scheme employed.\n\nHTTP places no restrictions on what media types it transports.\nMHTML defines a new media type, \"multipart/related\". The\ndefinition of \"multipart/related\" must be independent of\nthe protocols which are used to transport it.\n\nI would urge that the definition of \"multipart/related\" be separated\nfrom the application of \"mailing someone a web page\" (MHTML).\nThe definition of \"multipart/related\" should make clear that it\nis a general extension to MIME multipart, and applies to *all*\napplications that use MIME (including web browsing).\n\nI would object to having the MHTML say that it 'applies to HTTP'\nsince such a statement would contribute to the existing confusion\nbetween the transport protocol (HTTP), the media types it is used\nto transport, and the applications that are being supported by\nsuch a combination.\n\nRegards,\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "At 10.42 -0700 97-10-04, Einar Stefferud wrote:\n> And we have been doing just that with some cross WG participation.\n> Larry Masinter is Chair of HTTP WG and he has been a long time\n> participant in our MHTML WG.  Some MHTML WG people have also been\n> participants in the HTTP WG, although I have personally withdrawn from\n> participation in the HTTP sessions or list discussions.\n\nLarry: Can you tell us whether the HTTP working group has any objections\nto changing MHTML to say that it applies to transfer of MIME also through\nHTTP. Of ocurse assuming that the current problem with recursive\ninheritance of Content-Base is resolved.\n\n------------------------------------------------------------------------\nJacob Palme <jpalme@dsv.su.se> (Stockholm University and KTH)\nfor more info see URL: http://www.dsv.su.se/~jpalme\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "I agree with Larry's analysis and conclusions (though so agreeing may\nsurprise almost everyone;-)...  Our agreeing may actually be shocking!\n\nLarry and I have not discussed what I am going to say here, but I hope\nhe will find that it agrees with his statements included below.  We\ndid not discuss his message before he mailed it either;-)...\n\nI think that we have been slowly converging on our positions over\ntime, to the point of understanding that HTTP and SMTP/RFC822 and\nother different application transports must be kept separate from MIME\nContent-Typing.  \n\nPerhaps part of what we are recognizing is that there are two classes\nof \"TRANSPORT\" protocols.  One just above the IP level that deals with\ndata streams and user datagrams that deal with interoperability, and\nanother class just below MIME that deals with transport of Application\nInformation Objects to provide what I like to call \"interworkability\".\n\nThe whole idea of MIME Content-typing is to achieve transport\nindependence from this application transport level, and our new\nMultipart/Related specs (et al) must fully support this idea.\n\nThat \"web browsing\" and \"mail\" are very different user paradigms for\nexchanging end user application information should be now be a well\nestablished fact, and the need to be able to use both to exchange the\nsame information objects in different situations should also be well\nestablished.  Neither of our two primary application transports (HTTP\nand SMTP/RFC822) will ever replace the other and we should expect that\nmany new variations will be invented between these two extremes to add\nlots more variety to the ways that we transport MIME wrapped entities.\n\nSo, MIME looks to me to be a higher level second \"neck\" for the\nInternet \"hourglass\" model, with IP at one neck and MIME at the other.\nLooking at things this way seems to be very helpful to me.\n\nThe main thing is to step \"out of the box\" and see that the Internet\nProtocol Suite need not be limited to having only one \"neck\".\n\nCheers...\\Stef\n\n\nrom your message Sun, 5 Oct 1997 11:03:52 PDT:\n}\n}Jacob,\n}\n}1) HTTP vs. \"web browsing\"\n}It would be useful to consider separating \"HTTP\" out from the\n}application of \"web browsing\", in the same way that \"SMTP\" is\n}separate from the application of \"mail\". Currently, the world\n}commonly uses \"HTTP\" and \"HTML\" and \"URL\" and various other\n}common components to deploy the \"web browsing\" application.\n}HTTP is also used for many other applications, and the \"web\n}browsing\" application can be supported using many other protocols,\n}as indicated by the URL scheme employed.\n}\n}HTTP places no restrictions on what media types it transports.\n}MHTML defines a new media type, \"multipart/related\". The\n}definition of \"multipart/related\" must be independent of\n}the protocols which are used to transport it.\n}\n}I would urge that the definition of \"multipart/related\" be separated\n}from the application of \"mailing someone a web page\" (MHTML).\n}The definition of \"multipart/related\" should make clear that it\n}is a general extension to MIME multipart, and applies to *all*\n}applications that use MIME (including web browsing).\n}\n}I would object to having the MHTML say that it 'applies to HTTP'\n}since such a statement would contribute to the existing confusion\n}between the transport protocol (HTTP), the media types it is used\n}to transport, and the applications that are being supported by\n}such a combination.\n}\n}Regards,\n}\n}Larry\n}--\n}http://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "My initial reactions to your questions Jacob are:\n>Question 1: Is it the intention that HTTP can be used in this way?\n\n\nYes. We have implemented it (Internet Explorer 4.0 for Win32 supports it). I\nbelive the Netscape folks said that Navigator/Communicator would support\nthis too.\n\n>Question 2: Should MHTML be amended to say that it covers all sending of\n>HTML in MIME, not only through SMTP but also through HTTP and possibly\n>other protocols, for example FTP?\n\nYes. It should be generic for anything that supports MIME. One however must\nquestion whether FTP really supports MIME, but for example it is clearly\nsupported for POP3 & IMAP.\n\n>Question 3: A main principle of MHTML is that if a HTML document is sent\n>as part of a multipart/related, lookup of links in the HTML document\n>should first go to other body parts within the multipart/related,\n>and ordinary HTTP lookup should only be done if there is no match\n>in another body part. Is this principle true also when multipart/related\n>is sent via HTTP?\n\nYes.\n\n>Question 4: If the answer to question 2 is \"yes\", does this mean that\n>the IETF MHTML group should liaise with some other IETF group on this\n>issue? I am sending a copy of this message to the mailing list for the\n>IETF working group on HTTP. Since I am not a member of that group,\n>please post responses also to the mhtml mailing list.\n\nIt certainly would be a good idea for other folks to be aware of this work.\nI'm not sure what other MIME related groups should be told about it. I also\nthink that we have enough of the \"right people\" who are pay attention in\nthis group to apply these results broadly, especially given that folks like\nmyself, Larry and Roy are (/ have become) involved. I think posting messages\nto various groups is appropriate, but I don't see any need for\nformal/informal liason.\n\nAlex\n\n\n\n"
        },
        {
            "subject": "Re: Q re draft 08's AcceptEncodin",
            "content": "Adrian Havill:\n>\n>On page 107 of\n><URL:http://www.w3.org/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-08.txt> the\n>syntax diagram lists:\n>\n>Accept-Encoding  = \"Accept-Encoding\" \":\" 1#( codings [ \";\" \"q\" \"=\" qvalue ] )\n>codings          = ( content-codings | \"*\" )\n>\n>But then goes on to list the examples:\n>\n>Accept-Encoding: compress, gzip\n>Accept-Encoding:\n>Accept-Encoding: *\n>Accept-Encoding: compress;q=0.5, gzip;q=1.0\n>Accept-Encoding: gzip=1.0; identity=0.5; *;q=0\n>\n>Is example 2 legal according to the syntax above? I assumed the \"1#\" meant \n>\"one or more.\"\n\nExample 2 above is correct, the syntax is incorrect.  The syntax line\nshould have read\n\n Accept-Encoding = \"Accept-Encoding\" \":\" #( codings [ \";\" \"q\" \"=\" qvalue ] )\n                                        ^^^\n\nI'll make sure that this problem gets added to the editor's to do list.\n\n>Adrian Havill <URL:http://www.threeweb.ad.jp/>\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "spoofing cookie",
            "content": "I recently asked about how you could use cookies to make some\nlightweight security, and got the answer that cookies are easy to spoof\nand thus very insecure. You can just as a server ask for another servers\ncookie, and then you can spoof the original server. \n\nMy idea was that this kind of spoofing could be prevented, if\nthe client stored the cookie with an identification of the server.\nThen to spoof you need to do IP spoofing, which can be done,\nbut which is close to being criminal.\n\nIs that something to list in a \"best practice\" section somewhere?\n\nkeld\n\n\n\n"
        },
        {
            "subject": "Re: spoofing cookie",
            "content": "Jonathan Stark writes:\n\n> > \n> > I recently asked about how you could use cookies to make some\n> > lightweight security, and got the answer that cookies are easy to spoof\n> > and thus very insecure. You can just as a server ask for another servers\n> > cookie, and then you can spoof the original server. \n> > \n> > My idea was that this kind of spoofing could be prevented, if\n> > the client stored the cookie with an identification of the server.\n> > Then to spoof you need to do IP spoofing, which can be done,\n> > but which is close to being criminal.\n> > \n> > Is that something to list in a \"best practice\" section somewhere?\n> > \n> > keld\n> > \n> \n> This concept works well with static IP addresses, but totally \n> breaks when you get a different dynamically allocated IP address the \n> next time you dial into your ISP.  \n> \n> The IP is different, but no spoofing has occured, and the cookie\n> is valid.\n\nYou mean the client IP address? I was only thinking on having\nthe server IP name recorded together with the cookie, \nand then the client could make a reverse DNS lookup to locate\nthe cookie. Hmm, maybe it does not work for proxies...\nThen you could do something relevant to the DNS part of the url\nthat has been issued...\n\nKeld\n\n\n\n"
        },
        {
            "subject": "Re: spoofing cookie",
            "content": "> \n> I recently asked about how you could use cookies to make some\n> lightweight security, and got the answer that cookies are easy to spoof\n> and thus very insecure. You can just as a server ask for another servers\n> cookie, and then you can spoof the original server. \n> \n> My idea was that this kind of spoofing could be prevented, if\n> the client stored the cookie with an identification of the server.\n> Then to spoof you need to do IP spoofing, which can be done,\n> but which is close to being criminal.\n> \n> Is that something to list in a \"best practice\" section somewhere?\n> \n> keld\n> \n\nThis concept works well with static IP addresses, but totally \nbreaks when you get a different dynamically allocated IP address the \nnext time you dial into your ISP.  \n\nThe IP is different, but no spoofing has occured, and the cookie\nis valid.\n\nJonathan\n\n\n\n"
        },
        {
            "subject": "Re: Calculation of age header",
            "content": "Larry asked for implementers opinions; we have two versions in the field:\n\n1) ICS 4.2 - which does NOT claim 1.1-compatability as a proxy - generates an\nAge: header on every cacheable response that flows through the\n    proxy iff caching is enabled.\n2) A beta of our next version generates an Age: header on every response that\nflows through it (Mogul's option C).\n\nWe think that both of these make the Age: header useless.\nFor example, with only 1 proxy in the chain and fairly rapid response times,\nthe Age: header just shows the amount of clock skew (which\nMogul shows to be very common) in the response, which is not informative.\nAge, in our opinion, is intended to allow you to detect how long the response\nhas been sitting in a proxy cache, not how long it has been in transit (which\nis information that can be derived by the client anyway).\n\nWe think that Fielding has the right idea (Option B), since that gives a\nmeaningful interpretation to Age:, and plan to ship our next release that\nway.\n\nNow, while I'm on the subject, if a proxy *KNOWS* that it has a synchronized\nclock, it can detect and adust for skew (as long as it is not a multiple of 60\nminutes - you cannot really deal effectively with bad timezone settings).\n\nRegards,\nRichard Gray\nDomino Go Webserver\n\n\n\n"
        },
        {
            "subject": "http compressio",
            "content": "Hi\nI am wondering if in HTTP, there are some scheme that compresse the data \nin HTTP level. I have some statistic of request data size of HTTP is 300byte.\nI think this data may by compressed so that saves BW.\nPlease help me.\n\n\n\n"
        },
        {
            "subject": "Weekly Internet HTTP/1.1 Test",
            "content": "Friday 10 October is coming up soon; is there a comprehensive\nrequirements summary I have missed?  I remember seeing a proposal, but\nit was geared towards getting feedback on the format.  There was not\nmuch discussion, and nothing since.\n\nDoes this mean that nothing has happened or that some work is happening\nprivately and just has not seen the light of day yet?\n\n** Reply to note from \"Scott Lawrence\" <lawrence@agranat.com> Wed, 24 Sep 1997 18:35:52 -0400\n>   \n>   The response has been encouraging.  I will volunteer to perform the\n>   role of registrar, and will send a reminder on Friday, Oct 10th to\n>   send in your notice of participation for the first 'test day' on the\n>   16th (a number of respondents, including myself, have indicated that\n>   mid-October would be a good time for them to start).\n>   \n>   It has been pointed out that server vendors will mostly be passive\n>   participants other than testing against thier own and others servers\n>   with publically available clients.  If client vendors wish to make\n>   available downloadable copies of thier clients, then some of us on\n>   the server side will certainly take advantage of that to help you\n>   test your clients.\n>   \n>   To repeat; I will be asking for you to send in the following, filled\n>   in so that other participants will know who is out there to test\n>   with.  Also, again - please suggest improvements to any part of this\n>   plan.\n>   \n>   ================\n>   \n>     Organization:\n>   \n>     User-Agent or Server string: (may be approximate)\n>   \n>     HTTP Role: [origin, proxy, tunnel, client, robot, ...]\n>   \n>     HTTP Version: (versions earlier than 1.1 _will_ be usefull)\n>   \n>     Address: (DNS host names and/or IP addresses for the HTTP\n>               implementation)\n>   \n>     Time: (GMT times the system will be active or available)\n>   \n>     Contact Name:  (person to contact with an issue)\n>   \n>     Contact Email:\n>   \n>     Contact Phone:\n>   \n>     Contact Hours: (GMT times the contact is available, should overlap\n>                     the span in Time, but may be a subset)\n>   \n>     Notes: other relevant information, possibly including URLs for\n>            information on where to find background material.\n>   \n>   Test Day Ground Rules:\n>   \n>     - Participating systems may be configured to allow access only by\n>       announced participants for that week.\n>   \n>     - Participants will, on request, make a reasonable effort to\n>       provide whatever relevant log or trace data they have to other\n>       participants to resolve problems.\n>   \n>     - If a problem or possible issue with another participant\n>       implementation is found, that issue will be communicated to the\n>       contact for that implementation promptly.  Only if the\n>       involved participants disagree on the correct behavior or if\n>       the involved participants agree to do so will the issue be\n>       brought to the working group mailing list.\n>   \n>     - Any other disclosure of problems found with other\n>       implementations during these tests is poor form.\n>   \n>     - Communicating positive results to other participants is\n>       strongly encouraged.\n>   \n> --\n> Scott Lawrence           EmWeb Embedded Server       <lawrence@agranat.com>\n> Agranat Systems, Inc.        Engineering            http://www.agranat.com/\n>   \n> \n \n\nRichard L. Gray\nchocolate - the One True food group\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "At 12:05 PM 10/5/97 -0700, Einar Stefferud wrote:\n\n>I think that we have been slowly converging on our positions over\n>time, to the point of understanding that HTTP and SMTP/RFC822 and\n>other different application transports must be kept separate from MIME\n>Content-Typing.  \n\nExcellent!\n\n>Perhaps part of what we are recognizing is that there are two classes\n>of \"TRANSPORT\" protocols.  One just above the IP level that deals with\n>data streams and user datagrams that deal with interoperability, and\n>another class just below MIME that deals with transport of Application\n>Information Objects to provide what I like to call \"interworkability\".\n\nWhy just TWO classes?  Within this expanded idea of a \"transport\" protocol\n(which I accept) I would suggest that \"transport\" protocols can be\nidentified at several levels within an extended protocol stack.\n\nAs new \"applications\" are constructed and become widely deployed, I would\nhope to see layered architectures within those applications which separate\nissues of control, data transport, information representation, user\ninterface, etc.  Hence these new applications would define protocols which\nthemselves can be used as a basis for future developments.\n\nI think what you describe is just a step in a continuing evolutionary\nprocess of building new applications upon the infrastructure of previous\napplications.\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Quality factor",
            "content": "The following thoughts emerged from an off-line discussion about the use of\nquality factors in content negiatiation.\n\nThe premise for what follows is the assertion that the only practical use\nfor a quality factor is to rank some set of alternatives according to\npreference.\n\nSimple sequencing of of alternatives (e.g. per Multipart/Alternative) may\nnot be possible because the sender may not be able to locate (hence\npresent) the alternatives in order of quality.  Therefore some separate\nranking mechanism is required.\n \nI suggest that in this case a 3-digit (max) number is insufficient, as with\na significant number of alternatives an implementation will soon run out of\nspace within which to slot further entries between existing entries.  I\nestimate that a perverse presentation would run out ranking space after\nabout 10 entries (log2(1001)).\n\nBEGIN COMMENT\n\nWhy log2(1001)?\n\nAssume that the alternatives are discovered by the sender in some arbitrary\norder, and that the sender must allocate quality factor values to rank the\nalternatives as they are discovered.  (e.g. information about each\nalternative is being sent out as soon the alternative is discovered.)  Then\neach new q-factor allocation must be placed between the q-factors of some\nexisting pair of alternatives (where 0 and 1 are notional initial entries).\n\nWithout any prior knowledge of the order in which entries will be\npresented, each new q-factor should be allocated mid-way between the\nq-factors of its immediate ranking predecessor and successor, hence\ndividing the available ranking space by 2.  Assuming a worst-case order of\npresentation, when the n'th alternative has been ranked, the remaining\navailable ranking space next to that entry is reduced to approximately\n2^(-n) of the original ranking space.\n\nThe original ranking space available using a number in [0,1] with\nthree-digit precision contains 1001 possible values.\n\nWhen 1001*2^(-n) is less than 1, no more values are available in the\nranking space (under the assumption of worst-case presentation order).\n    1001 * 2^(-n) < 1  [take base2 logs]\n => log2(1001) + (-n) < 0\n => n > log2(1001)\n(corresponds to no remaining ranking space)\n\nEND COMMENT\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Re: Weekly Internet HTTP/1.1 Test",
            "content": "On Wed, 8 Oct 1997 rlgray@raleigh.ibm.com wrote:\n\n> Friday 10 October is coming up soon; is there a comprehensive\n> requirements summary I have missed?  I remember seeing a proposal, but\n> it was geared towards getting feedback on the format.  There was not\n> much discussion, and nothing since.\n\n  I plan to send out the note Friday evening - I'm at Interop this week\n  (booth 1801 if anyone would like to stop by).\n\n  I'll be acting as the registrar at httptest@agranat.com\n\n\n\n"
        },
        {
            "subject": "Re:  http compressio",
            "content": "On Tue, 7 Oct 1997, Hyoung-Kee Choi <hkchoi@cc.gatech.edu> wrote:\n> I am wondering if in HTTP, there are some scheme that compresse the data \n> in HTTP level. I have some statistic of request data size of HTTP is 300byte.\n> I think this data may by compressed so that saves BW.\n\nThe current HTTP standard does not support any compression of HTTP\nitself.  HTTP/1.x is a text-based protocol without any compression.\nThis might change with HTTP-NG...\n\nHowever, there are several products that are using a compressed\nversion of HTTP between a pair of proxies (so that the proprietary\nprotocols are used only between the proxies, and not visible to other\nhosts).  As far as I know, these solutions have not been submitted to\nthe IETF or W3C.\n\nI am working on such a solution, with two proxies talking to each\nother over a slow link (cellular phones).  The proxies are compressing\nredundant fields in successive HTTP requests and responses in order to\nminimize the amount of data that is transmitted over the slow link.\nOf course, the data is also compressed, but this is a separate issue.\nI am wondering if anybody else on this working group is dealing with\nsimilar things and if it would be interesting to discuss HTTP\ncompression and maybe submit a draft about this...\n\nIf there were some guidelines about how HTTP compression should be\ndone, different proxies could interoperate and some clients or servers\ncould also support this modified protocol.  Note: compressing HTTP\nheaders is mostly useful for very slow links and applications\ntransfering small amounts of data (for the usual WWW applications, the\naverage size of the data transmitted with HTTP is more than ten times\nlarger than the HTTP headers).\n\nRegards,\n-Raphael\n--\n| Raphael Quinet                     E-mail: Raphael.Quinet@eed.ericsson.se |\n| Research Department / Mobility Applications Lab    Phone: +49 2407575-216 |\n| Ericsson Eurolab Deutschland GmbH                  Fax:   +49 2407575-400 |\n| Ericsson Allee, 1, D-52134 Herzogenrath, Germany          \"eviv bulgroz!\" |\n\n\n\n"
        },
        {
            "subject": "Re: Quality factor",
            "content": "On Thu, 9 Oct 1997, Graham Klyne wrote:\n\n> I suggest that in this case a 3-digit (max) number is insufficient, as with\n> a significant number of alternatives an implementation will soon run out of\n> space within which to slot further entries between existing entries.  I\n> estimate that a perverse presentation would run out ranking space after\n> about 10 entries (log2(1001)).\n\nHello Graham,\n\nYour arguments give one important aspect of the problem. There are\nothers.\n\nThe fact that only a coarse granularity should be used in\ncase of Laguage preferences to avoid giving too much traceable\ninformation has been discussed here already.\n\nVarious aspects of quality are usually combined by multiplication;\nthis means that each q value has to be appropriately scaled\n(e.g. if for languages, fr has q=0.6 and en has q=0.8, and for types,\nwe have q=0.5 for text/html and q=0.7 for text/pdf, then a French\nPDF wins against an English HTML). Note that this scaling\nalso allows to change the importance of each factor, by using\nexponents (if we want language to be twice as important as type,\nwe square the q's for language, get q=0.36 for fr, and q=0.64\nfor en, and now the English HTML wins against the French PDF).\n\nFrom a user perspective, I would like to claim that in the higher\nquality area, fine distinctions are more important and easier than\nin the low quality area. They are more important because there is\na higher probability that one of these will actually be choosen,\nand easier because the quality differences are actually relevant\nto the user in that area. This suggests that in your case, where\nyou have to assign quality sequentially, you could divide intervals\nasymetrically (e.g. 70%/30%) instead of half-half. So the first\ndocument would get quality 30%, the second would get 9% if it is\nlower and 51% if it is better, and so on. This would allow you\nto rank differently almost 20 documents if each subsequent one\nturns out to be even better than the previous one. If the\nsubsequent ones turn out to be worse, you won't be able to\nmake any distinctions anymore after the 5th or so document,\nbut that won't be too bad.\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "Re: Is MHTML only for email",
            "content": "I can see your point about the possibility of proliferating transports\nbetween TCP and MIME, but we need to be careful to rememeber that in\nthe hourglass model of the Internet, there has to be only one protocol\nat each neck.  I propose IP and MIME as two such \"necks\".\n\nI can speculate that there may be additional such necks above MIME,\nbut I do not see any more necks between IP and MIME.\n\nI have no concern about proliferating transports like SMTP, FTP, HTTP,\nSMXP, et al, between the IP and MIME necks.  These have the effect of\nsupporting usefully different styles and modes of interworking among\napplication users, without causing the application information objects\nthat they work with to become dependent on such verious transports.\n\nBut, it is critical to recognize the parallels between necks which\nprovide tagging and bagging tools to provide \"good-enough\" typing, and\n\"datagram\" chunking, which at the MIME level allows us to deal with\nstructured content (which parallels Object Modeling in APIs) and the\nIP neck, where datagram chunking allows us to deal with both streaming\ncontent and with user datagrams, among other things.\n\nWe also need to remind ourselves to not confuse PROTOCOL with APIs.\n\nBest...\\Stef\n\nPS: As MHTML Chair, I wonder if we are getting a bit far off topic,\n    though I am vitally interested in these issues.  Is this a topic\n    that should be sponsored by some higher level group like the IETF\n    APP Directorate?  I think we are really working on an abstract\n    model for application support protocol, which as a meta level\n    issue does not have a natural home in any of the APP WGs.\n\nrom your message Thu, 09 Oct 1997 09:23:10 +0100:\n}\n[snip]...[snip]...[snip]...[snip]...\n}\n}>Perhaps part of what we are recognizing is that there are two classes\n}>of \"TRANSPORT\" protocols.  One just above the IP level that deals with\n}>data streams and user datagrams that deal with interoperability, and\n}>another class just below MIME that deals with transport of Application\n}>Information Objects to provide what I like to call \"interworkability\".\n}\n}Why just TWO classes?  Within this expanded idea of a \"transport\" protocol\n}(which I accept) I would suggest that \"transport\" protocols can be\n}identified at several levels within an extended protocol stack.\n}\n}As new \"applications\" are constructed and become widely deployed, I would\n}hope to see layered architectures within those applications which separate\n}issues of control, data transport, information representation, user\n}interface, etc.  Hence these new applications would define protocols which\n}themselves can be used as a basis for future developments.\n}\n}I think what you describe is just a step in a continuing evolutionary\n}process of building new applications upon the infrastructure of previous\n}applications.\n}\n}GK.\n}---\n}\n}------------\n}Graham Klyne\n}GK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Re: Quality factor",
            "content": "At 02:52 PM 10/9/97 +0100, Martin J. D?rst wrote:\n>On Thu, 9 Oct 1997, Graham Klyne wrote:\n>\n>> I suggest that in this case a 3-digit (max) number is insufficient, as with\n>> a significant number of alternatives an implementation will soon run out of\n>> space within which to slot further entries between existing entries.  I\n>> estimate that a perverse presentation would run out ranking space after\n>> about 10 entries (log2(1001)).\n>\n>Hello Graham,\n>\n>Your arguments give one important aspect of the problem. There are\n>others.\n\nOf course;  I wasn't trying to overturn anything, just draw attention to a\npossible consequence of the current approach.\n\n>The fact that only a coarse granularity should be used in\n>case of Laguage preferences to avoid giving too much traceable\n>information has been discussed here already.\n\nIn private discussions, it has been suggested to me that language issues\nare a poor basis upon which to develop a \"quality\" rating system -- there\nis just too much subjectivity involved.\n\n>Various aspects of quality are usually combined by multiplication;\n[...]\n\nIt was a *premise* of my posting (one with which you may well disagree)\nthat the quality factors were used to simply rank alternatives.  It had\nbeen suggested to me in offline discussion that other document selection\nsystems which attempted to perform arithmetic manipulation of quality\nfactors gained limited benefit from such manipulations (unfortunately, I\ndon't remember details of the example system offered).\n\nFrom a user perspective, I would like to claim that in the higher\n>quality area, fine distinctions are more important and easier than\n>in the low quality area.  [...] This suggests that in your case, where\n>you have to assign quality sequentially, you could divide intervals\n>asymetrically (e.g. 70%/30%) instead of half-half. [...]\n\nYou make a good point here -- thank you for pointing it out.\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "Re: Quality factor",
            "content": "Graham Klyne:\n>\n>The following thoughts emerged from an off-line discussion about the use of\n>quality factors in content negiatiation.\n>\n>The premise for what follows is the assertion that the only practical use\n>for a quality factor is to rank some set of alternatives according to\n>preference.\n\nI don't agree with this premise, but we have discussed this before.  I\ndo want to comment on the reasoning below however:\n\n>Simple sequencing of of alternatives (e.g. per Multipart/Alternative) may\n>not be possible because the sender may not be able to locate (hence\n>present) the alternatives in order of quality.  Therefore some separate\n>ranking mechanism is required.\n> \n>I suggest that in this case a 3-digit (max) number is insufficient, as with\n>a significant number of alternatives an implementation will soon run out of\n>space within which to slot further entries between existing entries.  I\n>estimate that a perverse presentation would run out ranking space after\n>about 10 entries (log2(1001)).\n\nYour reasoning assumes that the sender has to send out the source\nquality value of a variant as soon as the variant itself is sent.  If\nyou wait sending source quality values until all variants have been\nsent, the problem does not exist anymore.  And I see no logical reason\nwhy you could not wait: the recipient can't do anything until the last\nsource quality value is received anyway.\n\n>GK.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Quality factor",
            "content": "On Thu, 9 Oct 1997, Graham Klyne wrote:\n\n> In private discussions, it has been suggested to me that language issues\n> are a poor basis upon which to develop a \"quality\" rating system -- there\n> is just too much subjectivity involved.\n\nThanks for bringing up these private discussions. There could be\nvarious issues of subjectivity you are alluding to. One thing is\nthat some people are very convinced that languagage, in various\nways, is very important, more important than other things, and\nthat this can affect some discussions.\nThe second thing is that language abilities, and therefore preferences,\ndiffer widely for each individual, so this would mean that language\nis a particularly good example for the preference side, if we need\nsomething where preferences are clearly varying. Of course, for your\nproblem of granularity, we would need something where preferences\nhave only fine distinctions.\nThe third thing I could think of that you are alluding to is that\nthe rating of the quality of a document with respect to language\nis very subjective, i.e. there are widely differing oppinions\nabout what is \"good English\", \"good French\", and so on. In this\nrespect, I think I would have to oppose; other quality things\nsuch as typographic design (in formats where this matters),\ndocument structure, audio quality, and so on, may also easily\nbe subject to such subjectivities.\n\nThere may be other aspects of subjectivity in language, I would\nlike to hear about them.\n\nIn conclusion, I guess it is fair to say that a \"quality\" rating\nsystem that is developed ONLY on the basis of language issues\nwill not be a good solution. But a \"quality\" rating system that\nignores language issues will probably be as bad, if not worse.\n\n\n> >Various aspects of quality are usually combined by multiplication;\n> [...]\n> \n> It was a *premise* of my posting (one with which you may well disagree)\n> that the quality factors were used to simply rank alternatives.  It had\n> been suggested to me in offline discussion that other document selection\n> systems which attempted to perform arithmetic manipulation of quality\n> factors gained limited benefit from such manipulations (unfortunately, I\n> don't remember details of the example system offered).\n\nI can very well agree with your statement. The examples we usually\nsee, including the ones I made, are idealistic toy examples. The\nunderlying problem is that even for the human user, it's difficult\nto decide whether French Postscript or English HTML is preferred.\nAs long as the quality difference isn't above a certain threshhold,\nthere is probably not much of a real preference.\nAlso, there might be some drawing in Postscript that doesn't show\nwell in HTML, or some important link or form in HTML that isn't\nas convenient in Postscript, and the human end user may not know\nwhat is more important to him/her until he has looked at one\n(or both!) versions of the document. So how should the arithmetic\nbe able to figure this out :-?\n\n\nRegards,Martin.\n\n\n\n"
        },
        {
            "subject": "making progress on cookie",
            "content": "Things have been very quiet on the cookie front.  I have been busy with\nother projects, but I am now able to return to the fray.\n\nAt issue is how to make progress on a successor to RFC 2109.  One\nproposal is to split draft-ietf-http-state-man-mec-03 into two pieces:\n\n1) a description of the wire protocol; and\n2) a description of the privacy considerations of cookies.\n\nThe second document would comprise approximately these sections of\nstate-man-mec-03:\n- 4.3.5 Sending Cookies in Unverifiable Transactions\n- 7 Privacy\n\nThe groundrules would be that each of the two documents could/should be\ndiscussed separately, but that the IESG would not allow either to\nbecome an RFC until agreement had been reached on both.\n\nI'm soliciting discussion of this approach before I invest the time\nto split the document in two.  What do you think of this approach?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "If the IESG will not move them forward except in tandem,\nI don't really see the point in splitting the draft.  Has\nthe IESG indicated that they would prefer this approach\nbecause they might later be revved independantly?  Or\nis there pressure to do this so that implementations could\nclaim compliance with the wire protocol without\nclaiming compliance with the privacy sections?\nregards,\nTed Hardie\nNASA NIC\n\n\nOn Oct 10,  4:44pm, Dave Kristol wrote:\n> Subject: making progress on cookies\n> Things have been very quiet on the cookie front.  I have been busy with\n> other projects, but I am now able to return to the fray.\n>\n> At issue is how to make progress on a successor to RFC 2109.  One\n> proposal is to split draft-ietf-http-state-man-mec-03 into two pieces:\n>\n> 1) a description of the wire protocol; and\n> 2) a description of the privacy considerations of cookies.\n>\n> The second document would comprise approximately these sections of\n> state-man-mec-03:\n> - 4.3.5 Sending Cookies in Unverifiable Transactions\n> - 7 Privacy\n>\n> The groundrules would be that each of the two documents could/should be\n> discussed separately, but that the IESG would not allow either to\n> become an RFC until agreement had been reached on both.\n>\n> I'm soliciting discussion of this approach before I invest the time\n> to split the document in two.  What do you think of this approach?\n>\n> Dave Kristol\n>-- End of excerpt from Dave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "  > If the IESG will not move them forward except in tandem,\n  > I don't really see the point in splitting the draft.  Has\n  > the IESG indicated that they would prefer this approach\n  > because they might later be revved independantly?  Or\n  > is there pressure to do this so that implementations could\n  > claim compliance with the wire protocol without\n  > claiming compliance with the privacy sections?\n\nTo quote one of the Appl. Area Directors:  \"The point of serializing\nthese efforts is to focus the working group's discussion.\"  In other\nwords, you can nail down the wire protocol without getting side-tracked\nby arguments about privacy stuff, and vice versa.  So you could do one\nfirst, then the other, but neither would progress to RFC until both\nsets of issues had been settled.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "On Oct 10,  5:30pm, Dave Kristol wrote:\n\n> To quote one of the Appl. Area Directors:  \"The point of serializing\n> these efforts is to focus the working group's discussion.\"  In other\n> words, you can nail down the wire protocol without getting side-tracked\n> by arguments about privacy stuff, and vice versa.  So you could do one\n> first, then the other, but neither would progress to RFC until both\n> sets of issues had been settled.\n>\n> Dave Kristol\n\nThat can work, if there is an iron gavel willing to declare things\nout of order and make it stick.  The cookie discussions have consistently\nbeen retarded in their progress by folks entering them and re-raising\ntopics which have been discussed.  In an open process, it is hard to\ntell people that their voice would have been heard had they started\nparticipating earlier.  Especially where issues of privacy are concerned,\nthat can seem like disguised censorship.\n\nI believe we need to make progress on cookies, and if the ADs want to\nsplit the document to focus the discussion, that's fine by me.  If\nit doesn't work we are in no worse shape than we are in now.\nregards,\nTed Hardie\nNASA NIC\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "I'm not sure I see the point of splitting the document if the wire\nprotocol document can't progress w/o the privacy portion. Furthermore,\nI believe portions of the wire protocol are meaningless w/o the privacy\nportion.\n\nAnd, given that there were privacy concerns to begin with which motivated\nthe privacy related restrictions, I think we would need concensus\nfrom everyone who feels strongly about the privacy restrictions that\nthe wire protocol can go forward w/o the privacy specifications.\nOtherwise, there is no point in splitting the document because it doesn't\naddress the composite issues which motivated the specification in the\nfirst place.\n\nDave Morris\n\nOn Fri, 10 Oct 1997, Dave Kristol wrote:\n\n> Things have been very quiet on the cookie front.  I have been busy with\n> other projects, but I am now able to return to the fray.\n> \n> At issue is how to make progress on a successor to RFC 2109.  One\n> proposal is to split draft-ietf-http-state-man-mec-03 into two pieces:\n> \n> 1) a description of the wire protocol; and\n> 2) a description of the privacy considerations of cookies.\n> \n> The second document would comprise approximately these sections of\n> state-man-mec-03:\n> - 4.3.5 Sending Cookies in Unverifiable Transactions\n> - 7 Privacy\n> \n> The groundrules would be that each of the two documents could/should be\n> discussed separately, but that the IESG would not allow either to\n> become an RFC until agreement had been reached on both.\n> \n> I'm soliciting discussion of this approach before I invest the time\n> to split the document in two.  What do you think of this approach?\n> \n> Dave Kristol\n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "On Fri, 10 Oct 1997, David W. Morris wrote:\n\n> I'm not sure I see the point of splitting the document if the wire\n> protocol document can't progress w/o the privacy portion. Furthermore,\n> I believe portions of the wire protocol are meaningless w/o the privacy\n> portion.\n\nPerhaps I am not paying close enough attention, but I don't see the\nsections that you mean. The strict 'how do you keep state' requirement\nseems to split on the adminstrative level from the 'who is entitled to\nreceive that state' at the next metalevel up. It is possible for some\nkinds of proposals that have not been placed on the table at this time\n(things like cryptographically certified lists of specific machines\nentitled to recieve the state) could mix the two levels, but right now\nthey are still seem quite seperable administratively. \n\nCould you give an example of something in the current proposal that mixes\nthe two levels? Or are you suggesting that the split will block the\nexploration of options such as the one I mentioned?\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "I suggested the split myself as a way of determining consensus\nindependently and serially. It also allows one document to\nbe revised, if necessary, without touching the other.\n\nIt might be that we could combine the documents before they\nissue as an RFC, but it's important to separate the technical\naspects from the political ones to make progress.\n\nLarry\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "An alternative proposal is to take the signed cookie draft and combine\nit with the protocol draft and put that up as the standard. That way we\ndon't have to argue over heuristics which prevent legitimate\nfunctionality and instead use a policy based system backed up with\nauthentication.\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Friday, October 10, 1997 3:16 PM\n> To:Dave Kristol\n> Cc:http-state@lists.research.bell-labs.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; http-wg@cuckoo.hpl.hp.com\n> Subject:Re: making progress on cookies\n> \n> I'm not sure I see the point of splitting the document if the wire\n> protocol document can't progress w/o the privacy portion. Furthermore,\n> I believe portions of the wire protocol are meaningless w/o the\n> privacy\n> portion.\n> \n> And, given that there were privacy concerns to begin with which\n> motivated\n> the privacy related restrictions, I think we would need concensus\n> from everyone who feels strongly about the privacy restrictions that\n> the wire protocol can go forward w/o the privacy specifications.\n> Otherwise, there is no point in splitting the document because it\n> doesn't\n> address the composite issues which motivated the specification in the\n> first place.\n> \n> Dave Morris\n> \n> On Fri, 10 Oct 1997, Dave Kristol wrote:\n> \n> > Things have been very quiet on the cookie front.  I have been busy\n> with\n> > other projects, but I am now able to return to the fray.\n> > \n> > At issue is how to make progress on a successor to RFC 2109.  One\n> > proposal is to split draft-ietf-http-state-man-mec-03 into two\n> pieces:\n> > \n> > 1) a description of the wire protocol; and\n> > 2) a description of the privacy considerations of cookies.\n> > \n> > The second document would comprise approximately these sections of\n> > state-man-mec-03:\n> > - 4.3.5 Sending Cookies in Unverifiable Transactions\n> > - 7 Privacy\n> > \n> > The groundrules would be that each of the two documents could/should\n> be\n> > discussed separately, but that the IESG would not allow either to\n> > become an RFC until agreement had been reached on both.\n> > \n> > I'm soliciting discussion of this approach before I invest the time\n> > to split the document in two.  What do you think of this approach?\n> > \n> > Dave Kristol\n> > \n> > \n\n\n\n"
        },
        {
            "subject": "cookie",
            "content": "Dave -- I think this is an excellent idea.  Policy makers, users, and\nadvocates alike have been concerned with the privacy implications of\ncookies.  The lack of a clear discussion that those outside the technical\ncommunity can follow has produced confusion, some misinformation, in\naddition to legitimate concern.  I don't know if what you have in mind\nwould fill this role, but I do believe a clear discussion  of the privacy\nimplications is important.  Thanks.\n\n>Things have been very quiet on the cookie front.  I have been busy with\n>other projects, but I am now able to return to the fray.\n>\n>At issue is how to make progress on a successor to RFC 2109.  One\n>proposal is to split draft-ietf-http-state-man-mec-03 into two pieces:\n>\n>1) a description of the wire protocol; and\n>2) a description of the privacy considerations of cookies.\n>\n>The second document would comprise approximately these sections of\n>state-man-mec-03:\n>- 4.3.5 Sending Cookies in Unverifiable Transactions\n>- 7 Privacy\n>\n>The groundrules would be that each of the two documents could/should be\n>discussed separately, but that the IESG would not allow either to\n>become an RFC until agreement had been reached on both.\n>\n>I'm soliciting discussion of this approach before I invest the time\n>to split the document in two.  What do you think of this approach?\n>\n>Dave Kristol\n\n\n                                                       \\|/\n\n                                                      (@ @)\n                                          --------oOOo-(_)-oOOo-----------\nDeirdre Mulligan                        Someone may be watching you online\nStaff Counsel                           Check out: http://www.cdt.org/privacy/\nCenter for Democracy and Technology\n1634 Eye Street, NW\n11th Floor\nWashington, DC 20006\n(v) +1.202.637.9800\n(f) +1.202.637.0968\nhttp://www.cdt.org/\n\n\n\n"
        },
        {
            "subject": "Interoperability Test 101",
            "content": "  The first HTTP/1.1 Multi-Vendor Internet Test Day is Oct 16.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Oct 14.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  One special request to proxy vendors; it would be very usefull if\n  there were proxies available that were configured to forward only to\n  another proxy server so that we can see the results of chained\n  proxies, including chains of mixed vendors and mixed HTTP revisions.\n  If this is too cumbersome to set up for this week, please think\n  about it for the future.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string: (may be approximate)\n\n    HTTP Role: [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address: (DNS host names and/or IP addresses for the HTTP\n              implementation)\n\n    Time: (GMT times the system will be active or available)\n\n    Contact Name:  (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours: (GMT times the contact is available, should overlap\n                    the span in Time, but may be a subset)\n\n    Notes: other relevant information, possibly including URLs for\n           information on where to find background material.\n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "On Fri, 10 Oct 1997, Yaron Goland wrote:\n\n> An alternative proposal is to take the signed cookie draft and combine\n> it with the protocol draft and put that up as the standard. That way we\n> don't have to argue over heuristics which prevent legitimate\n> functionality and instead use a policy based system backed up with\n> authentication.\n\nThis alternative would not be a complete solution since it would drop\nthe default specification for cookie privacy when the cookie presented\nwas not signed.\n\nI have no problem with an alternative which includes completing work\non the signed cookie proposal but I see that as additional specification\nand not replacing some form of the existing privacy specifications.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "Benjamin Franz wrote:\n> [...]\n> Could you give an example of something in the current proposal that mixes\n> the two levels? Or are you suggesting that the split will block the\n> exploration of options such as the one I mentioned?\n\nIt wasn't so much that the text in the document were intertwined, but\nrather that the arguments on the mailing list(s) about the technical and\nprivacy issues got intertwined.  The idea was to disentangle them and\ndeal with them separately.  The hope was that doing so would lead to\nsome measure of progress.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "I understand the concerns regarding unsigned cookies but at the same\ntime I do not believe we can create restrictions that are not arbitrary.\nFor example, the two hierarchy level restriction. As such I believe the\nbest we can do is state \"You want security? Use a signature.\"\n\nHow many systems do you know that go out of there to specify security in\nsituations where the user intentionally chooses not to use any security?\n\nYaron\n\n> -----Original Message-----\n> From:David W. Morris [SMTP:dwm@xpasc.com]\n> Sent:Saturday, October 11, 1997 12:32 PM\n> To:Yaron Goland\n> Cc:Dave Kristol; http-state@lists.research.bell-labs.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com;\n> http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com; http-wg@cuckoo.hpl.hp.com\n> Subject:RE: making progress on cookies\n> \n> \n> \n> On Fri, 10 Oct 1997, Yaron Goland wrote:\n> \n> > An alternative proposal is to take the signed cookie draft and\n> combine\n> > it with the protocol draft and put that up as the standard. That way\n> we\n> > don't have to argue over heuristics which prevent legitimate\n> > functionality and instead use a policy based system backed up with\n> > authentication.\n> \n> This alternative would not be a complete solution since it would drop\n> the default specification for cookie privacy when the cookie presented\n> was not signed.\n> \n> I have no problem with an alternative which includes completing work\n> on the signed cookie proposal but I see that as additional\n> specification\n> and not replacing some form of the existing privacy specifications.\n> \n> Dave Morris\n\n\n\n"
        },
        {
            "subject": "RE: making no progress on cookie",
            "content": "On Sat, 11 Oct 1997, Yaron Goland wrote:\n> I understand the concerns regarding unsigned cookies but at the same\n> time I do not believe we can create restrictions that are not arbitrary.\n> For example, the two hierarchy level restriction. \n\nI agree that the two-dot rule you refer to is a compromise that neither\nfully protects privacy nor consistently ascertains the need to do so.  It\nwas the best solution we found, and the other privacy restrictions in the\nRFC were intended to make up for this compromise.  I do not agree that the\nprivacy restrictions are arbitrary, though I would be foolish but to agree\nthat they are controversial. \n\n> As such I believe the best we can do is state \"You want security? Use a\n> signature.\"\n> \n> How many systems do you know that go out of there to specify security in\n> situations where the user intentionally chooses not to use any security?\n\nSecurity is not the same thing as privacy.\n\nThere are plenty of systems that protect privacy even if the user does not\nexplicitly request privacy protection.  That is my understanding of the\nintent of the privacy restrictions in the cookie rfc.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Hold your cookietal",
            "content": "To members of the HTTP-WG@cuckoo.hpl.hp.com mailing list:\n\nPlease do not post additional messages on the subject of\n'cookies' UNTIL THERE ARE NEW DRAFTS.\n\nWhen there are new drafts, we will discuss them one at a time.\nI reserve judgement as to the order.\n\nLarry (attempting to moderate WG mailing list traffic).\n-- \nhttp://www.parc.xerox.com/masinter\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "The combination of the wire protocol and privacy concerns regarding cookies\nled to quite a divergence\nfrom the technical landscaping of the draft in working group discussions.\nThat is why splitting the two\nissues out seams most reasonable. At least this way it will be crystal\nclear what is at issue in a discussion:\nthe semantics of the protocol or the political/privacy/security/whatever\nramifications of cookies.\n\nAll the wire protocol portion would need to address in order to accommodate\nits other half is a\nmechanism (if necessary) for allowing whatever is decided (if anything) in\nthe privacy draft to take shape.\n\nIf nothing more than a tool to facilitate concise discussion about the two\ndirections of the current draft,\nit should be split.\n\nJudson Valeski\n\n\n\n"
        },
        {
            "subject": "Re: Quality factor",
            "content": "At 11:19 AM 10/10/97 +0100, Martin J. D?rst wrote:\n>On Thu, 9 Oct 1997, Graham Klyne wrote:\n>\n>> In private discussions, it has been suggested to me that language issues\n>> are a poor basis upon which to develop a \"quality\" rating system -- there\n>> is just too much subjectivity involved.\n\n[...]\n>In conclusion, I guess it is fair to say that a \"quality\" rating\n>system that is developed ONLY on the basis of language issues\n>will not be a good solution. But a \"quality\" rating system that\n>ignores language issues will probably be as bad, if not worse.\n\nI think that is a fair comment.\n\n[...]\n>Also, there might be some drawing in Postscript that doesn't show\n>well in HTML, or some important link or form in HTML that isn't\n>as convenient in Postscript, and the human end user may not know\n>what is more important to him/her until he has looked at one\n>(or both!) versions of the document. So how should the arithmetic\n>be able to figure this out :-?\n\nI think that captures the sense in which it was put to me that arithmetic\nhas limited value for combining quality factors.\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "At 6:50 PM -0700 10/10/97, Yaron Goland wrote:\n>An alternative proposal is to take the signed cookie draft and combine\n>it with the protocol draft and put that up as the standard. That way we\n>don't have to argue over heuristics which prevent legitimate\n>functionality and instead use a policy based system backed up with\n>authentication.\n\nAs I've said before, I don't think this would be a positive step.  If we're\nhaving trouble making progress on the current specification, trying to make\nprogress on an even more complex one will be that much more difficult.\n\nI agree with Dave Morris's point that not all applications need or want\nsigned cookies.  I prefer to regard the signed cookies proposal as an\nadd-on.  I think it can mesh relatively smoothly with the (successor to)\nRFC 2109.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on cookie",
            "content": "At 9:30 AM -0700 10/12/97, Judson Valeski wrote:\n>The combination of the wire protocol and privacy concerns regarding cookies\n>led to quite a divergence\n>from the technical landscaping of the draft in working group discussions.\n> [...]\n\nI'm unsure what you mean by \"led to quite a divergence from the technical\nlandscaping of the draft...\"  Were the privacy provisions all present in\nNetscape's original proposal?  No, but some were.  And if you're implying\nthat the privacy concerns were never discussed adequately on the list,\nthat's simply not so.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "Privacy - I get scared whenever a public organization tries to decide\nwhat appropriate \"privacy\" is. That is a consumer decision, not an IETF\none. The IETF's job is to provide secure interoperable protocols, not to\ndecide for users what the appropriate level of privacy is. Consumers\nmake choices, they choose to use one product over another. It is in that\nchoice that they choose how to protect their privacy. That choice\nincludes choosing products which default to accepting all cookies.\n\n2109 - The problems with the current spec have nothing to do with\ncomplexity. They have to do with an attempt to patch a fundamentally\nbroken protocol. Building a cookie mechanism on signed cookies provides\na protocol which delivers security without interfering in the UI and\nfeature decisions of software makers.\n\nYaron\n\n> -----Original Message-----\n> From:Dave Kristol [SMTP:dmk@bell-labs.com]\n> Sent:Sunday, October 12, 1997 1:59 PM\n> To:Yaron Goland\n> Cc:http-state@lists.research.bell-labs.com;\n> http-wg@cuckoo.hpl.hp.com\n> Subject:RE: making progress on cookies\n> \n> At 6:50 PM -0700 10/10/97, Yaron Goland wrote:\n> >An alternative proposal is to take the signed cookie draft and\n> combine\n> >it with the protocol draft and put that up as the standard. That way\n> we\n> >don't have to argue over heuristics which prevent legitimate\n> >functionality and instead use a policy based system backed up with\n> >authentication.\n> \n> As I've said before, I don't think this would be a positive step.  If\n> we're\n> having trouble making progress on the current specification, trying to\n> make\n> progress on an even more complex one will be that much more difficult.\n> \n> I agree with Dave Morris's point that not all applications need or\n> want\n> signed cookies.  I prefer to regard the signed cookies proposal as an\n> add-on.  I think it can mesh relatively smoothly with the (successor\n> to)\n> RFC 2109.\n> \n> Dave Kristol\n> \n\n\n\n"
        },
        {
            "subject": "RE: making progress on cookie",
            "content": "At 02:27 PM 10/12/97 -0700, Yaron Goland wrote:\n>Privacy - I get scared whenever a public organization tries to decide\n>what appropriate \"privacy\" is. That is a consumer decision, not an IETF\n>one. The IETF's job is to provide secure interoperable protocols, not to\n>decide for users what the appropriate level of privacy is.\n\n(Please excuse if I stray off-topic for a moment...)\n\nThis comment strikes a chord with something I caught on the radio (BBC\nRadio 4) yesterday -- a program about privacy and privacy legislation.  One\nof the concluding comments was that privacy rules only work (or have\nmeaning) in a fundamentally trusting environment.  Without the widespread\nexpectation and honouring of trust in dealing with private information,\npeople would simply stop giving out such information.\n\nThis suggests that privacy is and must be a \"consumer decision\", and the\ntechnical infrastructure needs to provide a framework for that choice to be\nexercised. (Or people may stop using that framework?)\n\n(On a personal note, I run my browser with cookies disabled, so I get a\nprompt every time I hit a site that asks to set a cookie.  A consequence of\nthis is that some sites which set large numbers of cookies often get\nabandoned after the 4th or 5th such attempt.)\n\nGK.\n---\n\n------------\nGraham Klyne\nGK@ACM.ORG\n\n\n\n"
        },
        {
            "subject": "draft '08' =&gt; draft-ietf-http-v11-spec-rev00.",
            "content": "The internet-drafts submission before Munich bounced (or got\ndropped in an Internet Drafts Editor's stack overflow), and\nso we've been without an 'official' draft. Even though Jim's\nabout to embark on draft 09 soon, it was important to remedy\nthis.\n\nConventionally, I believe, when an Internet Draft becomes\nan RFC, the version series is discontinued, and a new \ndocument name is used. So I submitted the draft (in\ntext and pdf) as draft-ietf-http-v11-spec-rev-00.* rather than\nas -spec-08.*.\n\nLarry\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-hit-metering04.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: Simple Hit-Metering and Usage-Limiting for HTTP\nAuthor(s): J. Mogul, P. Leach\nFilename: draft-ietf-http-hit-metering-04.txt\nPages: 35\nDate: 15-Oct-97\n\n        This document proposes a simple extension to HTTP, using a\n        new ``Meter'' header, which permits a limited form of\n        demographic information (colloquially called\n        ``hit-counts'') to be reported by caches to origin servers,\n        in a more efficient manner than the ``cache-busting''\n        techniques currently used.  It also permits an origin\n        server to control the number of times a cache uses a cached\n        response, and outlines a technique that origin servers can\n        use to capture referral information without\n        ``cache-busting.''\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-hit-metering-04.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-hit-metering-04.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-hit-metering-04.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-ietf-http-hit-metering04.tx",
            "content": "Before anyone wastes their time reading this new draft of the\nhit-metering document, I should point out that the ONLY change\nin this draft (relative to draft-ietf-http-hit-metering-03.txt)\nis the inclusion of this statement (in the \"Terminology\" section):\n\n   The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHOULD\", SHOULD NOT\",\n   \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be\n   interpreted as described in RFC 2119 [1].\n\nat the request of the IESG (and the consequential renumbering of\nthe bibliography, since \"Bradner\" sorts before all of other cited\nauthors.)\n\nI should have updated one or two of the other references when I\nhad the chance, but I guess this can be done when the document\nreaches the RFC stage.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "draft-fielding-url-syntax08 issu",
            "content": "This came up in a private exchange a few months ago, but it isn't\nresolved in url-syntax-08.\n\nAccording to RFC 1034, Sect. 3.1, the string \"foo.\" is an \"absolute\ndomain name.\" (Note the trailing '.'.)  url-syntax-08 provides no valid\nway to use \"foo.\" as a hostname in a URL, as in\nhttp://foo./bar\n\nThe problem is that grammar non-terminal \"toplabel\" can not end in\n'.' as it's currently specified.  I think it should allow the final '.'.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Content negotiation, features, and related item",
            "content": "Thanks to Graham Klyne helping sort out the status of the various \ndrafts and helping organize some of the work around it.\n\nA goal is to move content-negotiation from out of the context of\nbeing entirely HTTP-centric, and focus some of the work around\nthe requirements of _other_ protocols (fax, printing, perhaps 'push')\nthat also require content 'negotiation' in its broader sense:\nprotocol elements that describe the capabilities, characteristics\nand preferences of recipients, that describe the representation\nof entities and alternatives that a sender might send, and extensions\nthat label additional characteristics of content that are not\ncaptured by the MIME media type.\n\nIn order to ensure broad review, I am planning to ask for a BOF\nat the Washington IETF to see if there is reason to create a new\n'content negotiation' working group on this topic, or if the current\ndrafts can proceed to RFC without such a group.\n\nHere's a status update and proposed direction for the various drafts:\n\ndraft-ietf-http-alternates-00.txt\n  The Alternates Header Field\n    Koen Holtman, TUE\n    Andrew Mutz, Hewlett-Packard\n    September 15, 1997\nTarget: Experimental RFC\n\nOne question is whether \"Alternates\" might be recast as a shorthand\nfor an existing structure, multipart/alternative, where all or all-but-one\nof the parts is expressed as a message/external-body; that is, map\nthis into existing MIME semantics. (Web clients don't actually do\nmultipart/alternative, so this doesn't really help much for the HTTP\nmodel, but it might work better for other MIME processors.)\nTed Hardie & Andy Mutz have been working offline on this.\nThere is some question as to the appropriateness of the {} braces\nsyntax, and it may yet do with a note (in pushing this forward\nas Experimental) that the syntax might change before this actually\nmoves to standards track. I believe Andy's working on a new draft\nwith Ted's help.\n\nQUESTION: Are there any objections to proceeding with this draft\n  as an Experimental RFC? \n\n<draft-ietf-http-negotiate-scenario-01.txt>\nScenarios for the Delivery of Negotiated Content using HTTP\n    Edward Hardie, NASA NIC\n    July 18, 1997\n    Target: Informational RFC\n    Status: Drafted, comments received on HTTP list, revised\n    Action: Will be extended to talk about other scenarios\n            Graham Klyne is working on this one with Ted.\n\nPlease review this document when the next version comes out.\n\n<draft-ietf-http-negotiation-04.txt>\n  Transparent Content Negotiation in HTTP\n   Koen Holtman, TUE\n   Andrew Mutz, Hewlett-Packard\n   September 15, 1997\n   Target: Experimental RFC\n   Status: Drafted, many comments received on HTTP list, revised\n   Action: Should the Alternates header description just reference\n      the Alternates RFC? We should be clear about the nature of\n      the 'Experimental' status, for both Alternates and TCN in a\n      preface.\n\nQUESTION: Are there any objections to proceeding with this draft\n  as an Experimental RFC? \n\n<draft-ietf-http-rvsa-v10-02.txt>\n HTTP Remote Variant Selection Algorithm -- RVSA/1.0\n  Koen Holtman, TUE\n  Andrew Mutz, Hewlett-Packard\n  July 28, 1997\nTarget: Experimental RFC\nStatus: Drafted, comments received on HTTP list, revised\nAction: Reflect changes in negotation content features drafts?\n\nQUESTION: Are there any objections to proceeding with this draft\n  as an Experimental RFC? \n\n\n<draft-ietf-http-feature-scenarios-01.txt>\n   Feature Tag Scenarios\n   Koen Holtman, TUE\n   July 28, 1997\n   Status: Drafted, comments received on HTTP list, revised\n   Target: Informational RFC\n   Action: Dan Wing (IETF fax) is working with Andy Mutz to\n    update this to include additional (non-HTTP) scenarios.\n\nPlease review this draft when the next version is available.\n\n<draft-ietf-http-feature-reg-02.txt>\n  Feature Tag Registration Procedures\n   Koen Holtman, TUE\n   Andrew Mutz, Hewlett-Packard\n   July 28, 1997\n   Target: BCP (covering just registration)\n   Status: Drafted, comments received on HTTP list, revised.\n   Action: This document includes both the 'registration procedure'\n     and also the definition of some initial feature tags. These\n     should be separated. Ted Hardie is working on this.\n\n<new document>\n   Content Features For negotiation\n   [based on work in <draft-ietf-http-feature-reg-02.txt>]\n   Target: Standard RFC\n   Action: this document will include the initial feature definitions\n     (color, size, resolution, etc.) and establish the concept of\n     content features. It will absorb the features defined in\n     draft-mutz-http-attributes-02.txt (\"User Agent Display Attributes\").\n\n <draft-wing-smtp-capabilities-00.txt>\n Capabilities Exchange over SMTP\n    Dan Wing\n    Neil Joffe, Cisco Systems, Inc.\n    August 26, 1997\n    Target: Standard RFC\n    Status: This document describes a way of using content negotiation\n     in SMTP delivery, in the case of direct connection of sender to\n     recipient.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v11-spec-rev00.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\nThis draft is a work item of the HyperText Transfer Protocol Working Group of the IETF.\n\nTitle: Hypertext Transfer Protocol -- HTTP/1.1\nAuthor(s): J. Mogul, T. Berners-Lee, R. Fielding, \n                          H. Frystyk, J. Gettys\nFilename: draft-ietf-http-v11-spec-rev-00.txt\nPages: 182\nDate: 16-Oct-97\n\nThe Hypertext Transfer Protocol (HTTP) is an application-\nlevel protocol for distributed, collaborative, hypermedia\ninformation systems. It is a generic, stateless, object-\noriented protocol which can be used for many tasks, such as\nname servers and distributed object management systems,\nthrough extension of its request methods. A feature of HTTP\nis the typing and negotiation of data representation,\nallowing systems to be built independently of the data being\ntransferred.\n\nInternet-Drafts are available by anonymous FTP.  Login wih the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n\"get draft-ietf-http-v11-spec-rev-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-rev-00.txt\n\nInternet-Drafts directories are located at:\n\nAfrica:ftp.is.co.za\n\nEurope: ftp.nordu.net\nftp.nis.garr.it\n\nPacific Rim: munnari.oz.au\n\nUS East Coast: ds.internic.net\n\nUS West Coast: ftp.isi.edu\n\nInternet-Drafts are also available by mail.\n\nSend a message to:mailserv@ds.internic.net.  In the body type:\n\"FILE /internet-drafts/draft-ietf-http-v11-spec-rev-00.txt\".\n\nNOTE:The mail server at ds.internic.net can return the document in\nMIME-encoded form by using the \"mpack\" utility.  To use this\nfeature, insert the command \"ENCODING mime\" before the \"FILE\"\ncommand.  To decode the response(s), you will need \"munpack\" or\na MIME-compliant mail reader.  Different MIME-compliant mail readers\nexhibit different behavior, especially when dealing with\n\"multipart\" MIME messages (i.e. documents which have been split\nup into multiple messages), so check your local documentation on\nhow to manipulate these messages.\n\n\nBelow is the data which will enable a MIME compliant mail reader\nimplementation to automatically retrieve the ASCII version of the\nInternet-Draft.\n\n\n\n\n\n\nMessage/External-body attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Interoperability Test 102",
            "content": "  The _second_ HTTP/1.1 Multi-Vendor Internet Test Day is Oct 23.\n\n  We had a half dozen participants for the first day - as one of them,\n  I certainly found it to be usefull.  We did not have any proxies.  I\n  think that testing both through 1.0 and 1.1 proxies is critical to\n  our being able to claim that the protocol has been demonstrated to\n  operate correctly - so much of 1.1 is motivated by that.\n\n  If you plan to participate, please register by filling out the\n  survey form at the end of this mail, and sending it to\n  'httptest@agranat.com' by your close of business on Tuesday, Oct 21.\n  I will combine them and forward them as a single mailing on\n  Wednesday to all who signed up.  If you will have multiple instances\n  of the same implementation and configuration active, send that as\n  one registration; if you will have different implementations or\n  configurations active please send those separately so that it is\n  clear just what each active system should be.\n\n  If you participated the first time, please send a new registration\n  (you did keep a copy, didn't you? :).\n\n  All are welcome, including 1.0 implementations (testing backward\n  compatibility is important too).\n\n  Some notes from the first day:\n\n    Origin Servers - it would be helpfull if the content available\n       from  your test server was in some way self-descriptive so\n       that, for example, testers can tell which pages should be\n       cachable, which should produce chunked output, and so forth.\n\n    Proxies - PLEASE Participate.\n       It would be very usefull if there were proxies available that\n       were configured in chains, but for now I'll settle for just\n       having a couple one-hop proxy paths we can test.\n\n  Test Day Ground Rules:\n\n    - Participating systems may be configured to allow access only by\n      announced participants for that week (if you will be using a\n      dynamically assigned IP address, indicate this and try to\n      specify the range from which it will be chosen).\n\n    - Participants will, on request, make a reasonable effort to\n      provide whatever relevant log or trace data they have to other\n      participants to resolve problems.\n\n    - If a problem or possible issue with another participant\n      implementation is found, that issue will be communicated to the\n      contact for that implementation promptly.  Only if the\n      involved participants disagree on the correct behavior or if\n      the involved participants agree to do so will the issue be\n      brought to the working group mailing list.\n\n    - Any other disclosure of problems found with other\n      implementations during these tests is poor form.\n\n    - Communicating positive results to other participants is\n      strongly encouraged.\n\n  Additional suggestions welcome.\n\n  ================================================================\n\n\n    Organization:\n\n    User-Agent or Server string:\n        (may be approximate)\n\n    HTTP Role:\n        [origin, proxy, tunnel, client, robot, ...]\n\n    HTTP Version:\n\n    Address:\n        (DNS host names and/or IP addresses for the HTTP implementation)\n\n    Time:\n        (GMT times the system will be active or available)\n\n    Contact Name:\n        (person to contact with an issue)\n\n    Contact Email:\n\n    Contact Phone:\n\n    Contact Hours:\n        (GMT times the contact is available, should overlap\n         the span in Time, but may be a subset)\n\n    Notes:\n        other relevant information, possibly including URLs for\n        information on where to find background material.\n\n\n\n"
        },
        {
            "subject": "FYI [Internet-Drafts&#64;ietf.org: I-D ACTION:draft-pohlmann-http-traffic-satellite00.txt",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts directories.\n\n\nTitle: HTTP Traffic over Satellite\nAuthor(s): R. Pohlmann\nFilename: draft-pohlmann-http-traffic-satellite-00.txt\nPages: 3\nDate: 14-Oct-97\n\nInternet use over satellites (LEO, MEO and GEO) has not\nreceived enough attention.  Satellites have an inherent\nbroadcasting ability useful for transmitting Internet\ntraffic.  They can send information to many locations\nsimultaneously depending on the coverage beam of the\nsatellite.  The issue of Internet use over satellites needs\nto be addressed so that future implementations of networking\nprotocols can be used with this medium.  The vast majority\nof Internet traffic is TCP traffic resulting from the HTTP\nprotocol so this issue will be addressed here.  It is hoped\nthat this facilitates the transition from the use of\nterrestial networks, to networks incorporating satellite\nlinks.\n\n\n\n"
        },
        {
            "subject": "Re: Interoperability Test 102",
            "content": ">  Some notes from the first day:\n> \n>     Origin Servers - it would be helpfull if the content available\n>        from  your test server was in some way self-descriptive so\n>        that, for example, testers can tell which pages should be\n>        cachable, which should produce chunked output, and so forth.\n\nHaving run one of the clients in this test I'd like to second this\nrequest. Ideally, if the folks who provide servers could even give a\nrough idea beforehand of what's on their server, that would help me (and\nI presume other clients) in preparing some tests. For example it would\nbe useful to know what request methods can be tested and on which URLs\n(this includes the URLs for various forms).\n\nAlso, since I've noticed that a number of problems turn up in conjuction\nwith dynamic content (cgi scripts, SSI, etc). I would therefore like to\nsee more of this. Specifically, cgi's which produce a Content-length\nheader, cgi's which don't, cgi's which return 3xx responses, cgi's with\nvarying cache-control headers, etc would be useful. I know this means\nextra work for all those putting up servers, but those of us with\nclients have some work trying to put together, run and debug tests\nagainst the servers (I'm not complaining, just pointing out).\n\n\n  Cheers,\n\n  Ronald\n\n\n\n"
        },
        {
            "subject": "question in Frenc",
            "content": "Hello,\nIs there anybody who would answer some specific question about HTTP in\nFrench ?\nThanks\nKarine Solovieff\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-ietf-http-req-sum00.tx",
            "content": "Dave Kristol writes:\n    I have one quibble.  There are some lines that have double negatives\n    and that are, therefore, hard to interpret.  Three examples:\n    \n    If no-transform, don't add\nContent-Encoding, Content-Length,\nContent-Range, Content-Type            13.5.2   na   MN   na  2\n    Don't use 200 with partial resp.           13.8     na   MN   MN  1\n    GET/HEAD: no side effects                  13.9     SN   na   na\n    \n    I recommend always stating the requirement in the positive sense.  So\n    these lines would read:\n    \n    If no-transform, add\nContent-Encoding, Content-Length,\nContent-Range, Content-Type            13.5.2   na   MN   na  2\n    Use 200 with partial resp.                 13.8     na   MN   MN  1\n    GET/HEAD: has side effects                 13.9     SN   na   na\n    \n    This way the SN/MN negations are clearer, I think.\n\nI thought about this, and decided that it was probably safer to\nhave the textual description state the rule, rather than have it\nstate what we don't want people to do (with the \"MUST NOT\" in the\ncolumns to the right negating the apparent meaning of the left\nside).\n\nI mean, if one took your version, and just read it without paying\nmuch attention to the \"MN\"s to the side, one might naively believe\nthat it allows/requires adding \"Content-Type\" when no-transform\nis present.\n\nBottom line: this summary isn't going to be 100% intelligible on\nits own to a naive reader, no matter what we do.  It's only usable\nwith a copy of the spec present, open, and *read* by the user.\n\n-Jeff\n\nP.S.: Anyone who wants to actually contribute a significant\nportion of the summary would probably have more influence over\nthe design (hint, hint).\n\n\n\n"
        },
        {
            "subject": "Re: req-sum0",
            "content": "Patrick McManus writes:\n\n    the spreadsheet format is great, but will it include the glossary\n    of footnote resolutions? as that is likely to be perpetually\n    evolving as well.\n\nMy remarks in the Internet-Draft about using a spreadsheet were\nnot intended to describe a form directly available to \"users\".\nWhat I wrote was:\n   The use of a spreadsheet would allow separate editing of the\n   requirements summary, followed by semi-automatic insertion into the\n   master copy of the HTTP/1.1 specification.\nI.e., it would be a convenience for the editorial process of\nthe HTTP/1.1 specification.\n\nI also wrote\n   It might also allow\n   semi-automatic extraction of subset requirements application to\n   specific kinds of implementations.\nwhich I suppose could be useful to people who are trying to\ngenerate application-specific subsets of the requirements summary.\n\nHowever, since this requirements summary is not meant to be\nused independently of the formal specification (and anyone who\ntries to use it independently will get no sympathy from me!),\nI don't think it makes sense to try to make it entirely self-contained.\n\nI.e., don't expect to see the glossary of footnotes included in any\nspreadsheet representation.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Protocol Action: Simple Hit-Metering and UsageLimiting for HTTP  to Proposed Standar",
            "content": "  The IESG has approved the Internet-Draft \"Simple Hit-Metering and \n  Usage-Limiting for HTTP\" <draft-ietf-http-hit-metering-04.txt> as a \n  Proposed Standard. This document is the product of the HyperText Transfer\n  Protocol Working Group. The IESG contact persons are Keith Moore and \n  Harald Alvestrand.                                                       \n \n \nTechnical Summary\n\n  This document defines an new HTTP header to:\n\n  o allow cache servers to report \"hit counts\" to origin servers.\n  o allow origin servers to request that cache servers limit\n    the number of times that a particular cached document is\n    used\n  o allow cache servers to indicate their willingness to maintain\n    hit counts and/or honor usage limiting requests\n\n  It is hoped that this extension will encourage origin servers\n  to avoid \"cache busting\" techniques and allow more documents\n  to be cached.\n\nWorking Group Summary\n\n  There was concensus in the working group on this proposal.\n  Several implementations exist or are planned.\n\n  One person objected that this proposal decreases the efficiency\n  of every HTTP transaction between a cache server and an origin\n  server for marginal gain -- i.e. to obtain information of dubious\n  value.  However, nothing requires either a cache server or an\n  origin server to support this extension, and individual installations\n  can presumably make the decision to use it or not depending on the\n  perceived (or measured) cost versus benefit.  The same person also\n  voiced other objections, favoring other design decisions than those\n  made in this document.\n\n  However, there were no other objections to the proposal in the\n  working group.\n\nProtocol Quality\n\n  Keith Moore reviewed the specification for IESG.\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation, features, and related item",
            "content": "Larry Masinter:\n>\n[...]\n>Here's a status update and proposed direction for the various drafts:\n\n[...]\n\n><draft-ietf-http-feature-reg-02.txt>\n>  Feature Tag Registration Procedures\n>   Koen Holtman, TUE\n>   Andrew Mutz, Hewlett-Packard\n>   July 28, 1997\n>   Target: BCP (covering just registration)\n>   Status: Drafted, comments received on HTTP list, revised.\n>   Action: This document includes both the 'registration procedure'\n>     and also the definition of some initial feature tags. These\n>     should be separated. Ted Hardie is working on this.\n\n???  The above document does not define any initial feature tags, so\nthere is no need to work on separating them.  Or am I missing\nsomething obvious here?\n\n\nKoen.\n\n\n\n"
        }
    ]
}