{
    "mails": [
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">> Chuck Shotton writes:\n>> \n>> > IMHO, it should state that CR, LF, and CRLF should all be interpreted\n>> > equally as EOL when used as line ends. This avoids any problems with\n>> > machine dependent EOL symbols, and fairly represents the current practice.\n>> > (It also avoids forcing clients and especially servers to do line-by-line\n>> > translations of EOL for all outgoing response information, which is a BIG\n>> > performance hit.)\n>> \n>> Sounds reasonable to me.\n> \n> I object.\n> \n> So far, CRLF and LF have been understood as linebreaks.  In other\n> words, LF is a linebreak, with possibly a preceding CR.  This is fine,\n> even when used intermixed.\n> \n> If we change this in the proposed fashion, you will have ambiguity if\n> these are inconsistently used;  imagine a situation when you have a\n> file that begins with LF:\n> \n> ...CRLF\n> Blaa: foobarCR\n> CR\n> LF\n> \n> This won't be ambiguous if you force the use of CRs and LFs to be\n> consistent, but I think it's better to allow LFs and CRLFs intermixed,\n> rather than allow CRs, LFs and CRLFs, but only one of them at a time.\n> \n> -- Cheers, Ari --\n\nWe are only talking about the object-body here -- the spec already\nrequires that headers be CRLF terminated (with LF-only tolerated).\n\nThe suggestion up above is for clients to look for line breaks in the\nobject-body (i.e. when displaying text/plain) according to a perl regular\nexpression like:\n\nif ( /\\r?\\n?/ ) { print \"\\n\"; }\n\n(i.e. match CRLF, CR, or LF (but not LFCR) and treat it as a newline).\n\nIn other words \n\n    if ((c = getc(IN)) == LF)\n       putc('\\n');\n    elsif (c = CR)\n    {\n       if ((c2 = getc(IN)) == LF)\n          putc('\\n');\n       else\n       {\n          putc('\\n');\n          putc(c2);\n       }\n    }\n    else output(c);\n\n(well, okay, so I've ignored EOF, but you get the drift).\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "> Highlighting a few issues, which I hope will not create the image that\n> I am just trying to disagree with Roy on everything :-)...\n\nWell, it's better than just agreeing with me -- I hate that.  ;-)\n\n> - If-Modified-Since.  Part of the whole point of how this mechanism\n>   was defined is that servers that don't support it will just ignore\n>   it and return the whole object, which may sometimes be inefficient\n>   but won't break anything.  I think servers \"should\" implement this\n>   feature.  \"Must\" is too strong for a feature that increases\n>   efficiency but won't break anything by its absence.\n\nNo, the object was to remain compatible with *existing* servers -- from the\nvery beginning it was assumed that all new servers would have to implement\nit so that caching mechanisms could effectively switch from using HEAD\nto using the conditional GET.  In fact, I regularly tell people that\nthey MUST upgrade their existing server because the old versions do\nnot support it.  This is not something open for debate -- it could make\nthe difference between the web making or breaking trans-atlantic\n(and other miserably overloaded) network links.\n\n> - Non-ASCII characters in headers.  I don't think this is a big deal\n>   at all, though I'll be surprised if there isn't already somebody\n>   somewhere using non-ASCII in the comment section of the From: line\n>   or something, and I hope it's being done according to 1522 instead\n>   of somebody assuming the character set used in his particular nation\n>   is the universal character set for the whole world.\n\nI suppose so.\n\n> - HTTP-Dates.  It's not that including the day of the week is\n>   unfathomably difficult, but changing things in general.  It's\n>   confusing to say \"An rfc1123-date in HTTP actually only allows a\n>   restrictive subset of what RFC 1123 specifies,\" and for little if\n>   any gain.\n> \n>   I am uncomfortable with deviating from existing specifications\n>   without more compelling reasons for doing so.  I mean, heck, if we\n>   just want a date that's easy to parse, how about an integer of the\n>   number of seconds since the beginning of 1970?  Easy to implement,\n>   at least under UNIX. :-)\n\nThere is no deviation -- its just a subset.  That way, HTTP messages\ncan go outside HTTP with no problem, and inside HTTP we have a completely\nunambiguous date format which will last beyond the year 1999.\nWe could, of course, require that clients and servers be capable of\nparsing all date formats (a la USENET's get_date() function), but I\ndon't think I'd live through the pummeling I'd get from all the programmers\nat every Web conference for the next 50 years or so.  ;-)\n\n> - Canonicalization of content.\n> \n>   I'll drop this if everyone else thinks I'm just being a pedantic\n>   dork, but I really believe the purpose of a specification is to\n>   establish precise, correct behavior in which neither clients nor\n>   servers need to do heuristic guessing about what means what.\n\nThat is what ftp does.  It may look good in theory, but it simply isn't\nnecessary in practice.  That doesn't mean we shouldn't encourage people\nto do so -- it just means we can't require it in HTTP/1.0.\n\n>   Chuck Sutton suggests:\n  >> IMHO, it should state, and CRLF should all be interpreted\n  >> equally as EOL when used as line ends. This avoids any problems with\n  >> machine dependent EOL symbols, and fairly represents the current practice.\n  >> (It also avoids forcing clients and especially servers to do line-by-line\n  >> translations of EOL for all outgoing response information, which is a BIG\n  >> performance hit.)\n> \n>   (Aside: Does somebody have benchmarks to establish the magnitude of\n>   this \"big performance hit\"?)\n\nYes, just turn on server parsing of HTML files -- the performance hit\nwas two-three orders of magnitude on my server (worse if you consider\nlogfile collisions important).\n\nIn general, it is much better for clients to use content heuristics\nthan it is for servers to deal with content canonicalization, because\nof the issues of scale.\n\n> - Passing thought:  If a request contains a Message-ID header, should\n>   the server include that message-ID in the response, maybe in an\n>   In-Reply-To: header?\n\nNope.  I misrepresented this in the spec -- Message-ID should only be\n\"strongly-recommended\" for PUT and POST requests, since that is the only\ntime when it is useful.  Is this acceptable?\n\nSimilarly, Date: should only be \"strongly recommended\" for response\nmessages and PUT/POST requests.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Regarding line-break treatment, I agree that it is possible to\nwrite a finite state machine which can usefully interpret\nCR, LF, CRLF (but not LFCR) as end-of-line breaks.\n\nI wrote such a program a year or two back to unscramble people's\nmistakes with FTP ;)\n\nHowever, I think this is too much of a burden to add to client\nwriters.\n\nI'd rather accept the mix of LF or CRLF as line breaks or\ntreat LF as a line break and CR as white-space. (Both which\ncoexist with Unix-centric practice.)\n\n(Even though I have a mac on my desk...)\n\nIf we were writing a standard from scratch, I'd insist on one\nline break.\n\nI think this issue has also wandered past in the HTML working group list\nlately, I'm having trouble keeping the two straight  ;)\n\nIt would be nice of HTTP and HTML standards agreed on the treatment\nof line breaks in text/html....\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Roy T. Fielding writes in <9412011758.aa00432@paris.ics.uci.edu>:\n> Marc VanHeyningen writes:\n>> (Actually, I wouldn't object to outright prohibiting any CTE other\n>> than clear ones like 7bit, 8bit and binary, but maybe there are\n>> reasons to allow q-p and base64.)\n>\n>Clients may wish to support others in order to post newsgroup messages\n>through a proxy, but that is the only case I can think of.\n\nOnce our newsfeed is up, we may well only provide a Web browser interface to \nnews so we don't have to support *yet another program!*.  This browser would \nbe talking to our proxy...\n======================================================================\nMark Fisher                            Thomson Consumer Electronics\nfisherm@indy.tce.com           Indianapolis, IN\n\n\"Just as you should not underestimate the bandwidth of a station wagon\ntraveling 65 mph filled with 8mm tapes, you should not overestimate\nthe bandwidth of FTP by mail.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "Mike Cowlishaw writes in <9412011958.AA00957@hplb.hpl.hp.com>:\n>I'm happy with the situation for responses (connection closes is quite\n>good enough), but still very unhappy with this definition for the data\n>(object body) for requests (PUT and POST).  The steps you just\n>outlined are not defined sufficiently well to be implementable (and\n>requiring every server to implement the rather gawky multi-part stuff\n>just in case data comes in that way seems unnecessary).  Is not\n>Content-Length essentially always present, in current practice, for\n>PUT and POST?  In which case, why require more that this, or\n>alternatives, unless truly necessary?\n\nContent-Length will only be essentially present when PUT and POST deal with \npre-existing forms and files.  Doing a PUT or POST with large amounts of \ndata created on-the-fly does not lend itself well to use of Content-Length \nas documented better by others than myself on this list.  I really hate to \nrestrict the possibilities for future Web applications by making the \nmulti-part stuff optional (or leaving it out entirely).\n\nWhat is so particularly gawky about the multi-part stuff?  From my \nviewpoint, you either have to deal with fragmentation (UDP) or with message \nboundaries (TCP with multi-part messages).  If you have a better idea on \nhandling multiple bodies of data, I for one would sure like to hear it.\n======================================================================\nMark Fisher                            Thomson Consumer Electronics\nfisherm@indy.tce.com           Indianapolis, IN\n\n\"Just as you should not underestimate the bandwidth of a station wagon\ntraveling 65 mph filled with 8mm tapes, you should not overestimate\nthe bandwidth of FTP by mail.\"\n\n\n\n"
        },
        {
            "subject": "Reading Request Object Dat",
            "content": "Roy Fielding writes:\n\n>>> It goes something like this:\n>>>\n>>>   a) If message includes Content-Length, use it.\n>>>\n>>>   b) If message uses an as-yet-undefined packetized Content-Transfer-Encodin\n>>>      then that encoding may define an EOF marker.\n>>>\n>>>   c) If message uses an as-yet-undefined packetized Content-Encoding,\n>>>      then that encoding may define an EOF marker.\n>>>\n>>>   d) If message is of type multipart/*, the effective object body ends\n>>>      when the boundary close-delimiter is reached.\n>>>\n>>>   e) If the connection closes, the object body has ended.\n>>>\n>>> Part (b) is along the lines of Dan Connolly's www-talk proposal of\n>>> 27 Sep 1994 (Message-Id: <9409271503.AA27488@austin2.hal.com>).\n>>\n>> I'm happy with the situation for responses (connection closes is quite\n>> good enough), but still very unhappy with this definition for the data\n>> (object body) for requests (PUT and POST).  The steps you just\n>> outlined are not defined sufficiently well to be implementable (and\n>> requiring every server to implement the rather gawky multi-part stuff\n>> just in case data comes in that way seems unnecessary).  Is not\n>> Content-Length essentially always present, in current practice, for\n>> PUT and POST?  In which case, why require more that this, or\n>> alternatives, unless truly necessary?\n>\n>The intention is to move away from \"connection closes is quite good enough\"\n>so that future versions of HTTP can support a connection keep-alive.\n>Multipart types have always been possible to implement -- its just that\n>server and client authors have neglected (in the past) to read the MIME\n>spec and thus understand that these things exist and how they are implemented.\n>Including these descriptions in the spec is a way to prod people into\n>implementing something that should have been supported long ago.\n>\n>Whether they stay in the spec, get moved to an appendix, or get shoved\n>off to HTTP/1.1 is a question for the group to decide.\n\nThanks for the response .. and I applaud the intent to move away from\n'connection closes is enough' in the future.\n\nHowever, you really haven't answered my questions for today that would\nenable me to implement a server from the HTTP 1.0 specification.  In\nparticular, what is a conforming HTTP 1.0 server required to implement\nin order to read the object data for PUT and POST?  Specifically:\n\n(a) Content-Length: in bytes [I assume this one is required]\n\n(b) Packetized C-T-E: might well be useful, but only if the CTEs are\n    defined.  Do I have to support PC-DOS ZIP?  Do I have to support\n    Unix GZIP?  If not defined and required, then clients cannot use\n    them, and hence they are not useful.  [Also, one would need a\n    statement about how (a) and (b) interact.]\n\n(c) C-E: Ditto -- only plain Binary is even suggested, at present, so\n    there isn't anything to implement, I think?\n\n(d) Multipart: Is this current practice?  If not, I trust that it's\n    not required in 1.0.\n\n(e) Closed connection: [Doesn't apply for Requests.]\n\nrom the above, my inference is that an HTTP 1.0 server need only\nimplement (a).  Any more would be wasteful processing.  Am I correct?\n\nThanks -- Mike\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "It's actually better to ignore the IMS header and have the cleint cancel \nif the date doesn't match. \ns\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "> It's actually better to ignore the IMS header and have the cleint cancel \n> if the date doesn't match. \n\nIT MOST CERTAINLY IS NOT BETTER TO DO THAT!!!!!!!!!!!!!\n\nThe point is to save network traffic, NOT make life slightly easier\non server implementors.  Supporting IMS is TRIVIAL and has already been \ndone on all major servers -- not supporting it is reprehensible and\ndeserving of public abuse.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Reading Request Object Dat",
            "content": "Mike Cowlishaw <mfc@vnet.ibm.com> writes:\n\n> However, you really haven't answered my questions for today that would\n> enable me to implement a server from the HTTP 1.0 specification.  In\n> particular, what is a conforming HTTP 1.0 server required to implement\n> in order to read the object data for PUT and POST?  Specifically:\n> \n> (a) Content-Length: in bytes [I assume this one is required]\n> \n> (b) Packetized C-T-E: might well be useful, but only if the CTEs are\n>     defined.  Do I have to support PC-DOS ZIP?  Do I have to support\n>     Unix GZIP?  If not defined and required, then clients cannot use\n>     them, and hence they are not useful.  [Also, one would need a\n>     statement about how (a) and (b) interact.]\n> \n> (c) C-E: Ditto -- only plain Binary is even suggested, at present, so\n>     there isn't anything to implement, I think?\n> \n> (d) Multipart: Is this current practice?  If not, I trust that it's\n>     not required in 1.0.\n> \n> (e) Closed connection: [Doesn't apply for Requests.]\n> \n> From the above, my inference is that an HTTP 1.0 server need only\n> implement (a).  Any more would be wasteful processing.  Am I correct?\n\nCurrent practice is (a) -- it just doesn't work any other way with\nexisting servers.\n\nUmmm, I think (b) and (c) are confused, but these are reasonable questions.\nThe problem is that the HTTP spec must define not only origin servers, but\nproxies, gateways, and client usage of HTTP as well.  We (as in the authors)\nwill have to clarify this in the next version.  Perhaps a section on\nminimal compliance is called for.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Reading Request Object Dat",
            "content": "Roy T. Fielding <fielding@avron.ICS.UCI.EDU> writes:\n\n> Current practice is (a) -- it just doesn't work any other way with\n> existing servers.\nOK, thanks.  Now I know what code to write, this weekend, and you know\nat least one sentence that needs to be added to the Internet Draft.\n\n> Ummm, I think (b) and (c) are confused, but these are reasonable questions.\n> The problem is that the HTTP spec must define not only origin servers, but\n> proxies, gateways, and client usage of HTTP as well.  We (as in the authors)\n> will have to clarify this in the next version.\nAgreed.  By the way, if there's something I can do to help, let me know.\n\n> Perhaps a section on minimal compliance is called for.\nIt's hard to find a 'good' standard that doesn't at least try and\nprovide one (and it's not always easy--a 'COPYFILE' command satisfies\nmany programming language standards, for example :-)\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "> > It's actually better to ignore the IMS header and have the cleint cancel \n> > if the date doesn't match. \n> \n> IT MOST CERTAINLY IS NOT BETTER TO DO THAT!!!!!!!!!!!!!\n\nMy words exactly since a boldface font is not available.  Servers are\nalready facing problems with TCP kernel bugs, and intentionally\ndropping connections from the client side would only make things\nworse.\n\n> The point is to save network traffic, NOT make life slightly easier\n> on server implementors.  Supporting IMS is TRIVIAL and has already been \n> done on all major servers -- not supporting it is reprehensible and\n> deserving of public abuse.\n\nYes; even proxies do it (at least both that I've written), and it was\ndoable even then, although slightly more complex (with all the\ncombinations of incoming and outbound requests).  But the benefits far\noutweigh the complexity.\n\nCheers,\n--\nAri Luotonenhttp://home.mcom.com/people/ari/\nNetscape Communications Corp.\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "Looking at IMS on a packet by packet basis, in the presence of possibly \nlarge headers greater than 512 bytes,  having the client  cancel will save\n2 packets in the best case, and be no worse in the worst case. I sent \na message on tis to www-talk during chicago with more analysis. my hands aren't up\nto recreating it at the moment, and I think the message dissapeared \nin transit. i'll see if i can find it again.\n\ns\n\nBOX-Line: From luotonen@neon.mcom.com Fri Dec  2 12:54:34 1994\nReceived: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Fri, 2 Dec 1994 20:52:46 GMT\nReceived: from http-wg (list exploder) by cuckoo.hpl.hp.com\n(1.37.109.8/15.6+ISC) id AA24980; Fri, 2 Dec 1994 20:53:52 GMT\nFrom: Ari Luotonen <luotonen@neon.mcom.com>\nMessage-Id: <9412022054.AA15651@neon.mcom.com>\nSubject: Re: Comments on HTTP draft [of 23 Nov 1994]\nTo: Simon E Spero <ses@tipper.oit.unc.edu>\nDate: Fri, 2 Dec 1994 12:54:15 -0800 (PST)\nCc: fielding@avron.ICS.UCI.EDU, cshotton@oac.hsc.uth.tmc.edu, \n    http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nIn-Reply-To: <9412022023.AA14914@tipper.oit.unc.edu> from \"Simon E Spero\" at Dec 2, 94 03:23:56 pm\nContent-Type: text/plain; charset=US-ASCII\nSender: http-wg-request@cuckoo.hpl.hp.com\n\n\n> Looking at IMS on a packet by packet basis, in the presence of\n> possibly large headers greater than 512 bytes, having the client\n> cancel will save 2 packets in the best case, and be no worse in the\n> worst case. I sent a message on tis to www-talk during chicago with\n> more analysis. my hands aren't up to recreating it at the moment,\n> and I think the message dissapeared in transit. i'll see if i can\n> find it again.\n\nWell, did you monitor the impact on the server's TCP kernel when that\nis constantly happening?  Basically what you're suggesting is very\nugly anyway, and I don't like it one bit even if it wasn't expensive\nfor the network.  Gee, it's a few lines of code to do it cleanly.\nIt's like smashing your car on a signpost because you don't feel like\nhitting the brakes.\n\n-- Cheers, Ari --\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "> Looking at IMS on a packet by packet basis, in the presence of possibly \n> large headers greater than 512 bytes,  having the client  cancel will save\n> 2 packets in the best case, and be no worse in the worst case. I sent \n> a message on tis to www-talk during chicago with more analysis. my hands\n> aren't up\n> to recreating it at the moment, and I think the message dissapeared \n> in transit. i'll see if i can find it again.\n\nGood luck.  Given that a typical IMS request message is\n\n     64 bytes + length(URL) + user-agent (if any)\n\nand a 304 Not Modified response is typically 98 bytes, I haven't got a\nclue as to how you could come up with such figures.\n\nThat means (aside from the connection packets) one packet for the request\nand one for the response.  In contrast, a normal request will generally \nbuffer 16 packets before the client even has a chance to read the response --\ncancelling at that point is useless.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP draft [of 23 Nov 1994",
            "content": "Roy - \n slow start buildup is exponential from 1. \ns\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">> Chuck Shotton writes:\n>>\n>> > IMHO, it should state that CR, LF, and CRLF should all be interpreted\n>> > equally as EOL when used as line ends. This avoids any problems with\n>> > machine dependent EOL symbols, and fairly represents the current practice.\n>> > (It also avoids forcing clients and especially servers to do line-by-line\n>> > translations of EOL for all outgoing response information, which is a BIG\n>> > performance hit.)\n>>\n>> Sounds reasonable to me.\n>\n>I object.\n>\n>So far, CRLF and LF have been understood as linebreaks.  In other\n>words, LF is a linebreak, with possibly a preceding CR.  This is fine,\n>even when used intermixed.\n>\n>If we change this in the proposed fashion, you will have ambiguity if\n>these are inconsistently used;  imagine a situation when you have a\n>file that begins with LF:\n>\n>...CRLF\n>Blaa: foobarCR\n>CR\n>LF\n>\n>This won't be ambiguous if you force the use of CRs and LFs to be\n>consistent, but I think it's better to allow LFs and CRLFs intermixed,\n>rather than allow CRs, LFs and CRLFs, but only one of them at a time.\n\nFirst, if you don't interpret CR, LF, and CRLF (when they appear as\nindependent tokens) equally as line ends, some servers are going to be\nforced to do an unnecessary amount of work.\n\nSecond, your example is flawed. There is NO ambiguity in this tiny EOL\ngrammar. A bare CR followed immediately by a LF is a CRLF \"token\" and\nrepresents a line end. As such, it is not ambiguous. So your last two lines\nin your example are not a problem.\n\nA CR followed by anything except a LF is considered a line end. A LF\nfollowed by ANYTHING is considered a line end. A CRLF followed by ANYTHING\nis considered a line end. This represents the line end logic that every\nclient understands, and it prevents EVERY server from having to parse text\nand modify line ends.\n\nThe logic for text file parsing becomes incredibly simple. If a CR is seen\nwith anything following it besides a LF, it's EOL. Otherwise, a LF always\nindicates EOL.\n\nI strongly suggest that the HTTP standard include a statement to the effect\nthat this is the approved way to tolerate cross-platform variations in text\nfile EOL tokens. Forcing a single platform's standards across all platforms\nis unacceptable, especially when all clients and servers cope with the\nproblem now as described above.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">> This won't be ambiguous if you force the use of CRs and LFs to be\n>> consistent, but I think it's better to allow LFs and CRLFs intermixed,\n>> rather than allow CRs, LFs and CRLFs, but only one of them at a time.\n>>\n>> -- Cheers, Ari --\n>\n>We are only talking about the object-body here -- the spec already\n>requires that headers be CRLF terminated (with LF-only tolerated).\n>\n>The suggestion up above is for clients to look for line breaks in the\n>object-body\n\nRIGHT! object-body only. Sorry if I was being confusing by not restating\nthis in my posts. Header fields are never a problem because they are\ngenerated and can always be generated \"correctly.\" On the other hand,\nconverting a 2 meg postscript file's line ends every time it is requested\nwill start to add up.\n\nI can't understand how it'd be \"easier\" to force some servers to do this\nwork rather than to accomodate it with a simple statement in the standard.\nDefining tolerant EOL handling has absolutely no effect on existing\nclients, since they support this already anyway. And it probably won't have\nANY effect on servers since it is primarily oriented towards object-bodies\nthat they return by snarfing data directly off the disk.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "At 11:35 PM 12/1/94, Albert Lunde wrote:\n>Regarding line-break treatment, I agree that it is possible to\n>write a finite state machine which can usefully interpret\n>CR, LF, CRLF (but not LFCR) as end-of-line breaks.\n>\n>I wrote such a program a year or two back to unscramble people's\n>mistakes with FTP ;)\n>\n>However, I think this is too much of a burden to add to client\n>writers.\n\nNot at all. Show me a single client that doesn't already do this. As Roy\nsays, it is also an issue of scale. A client can more effectively do this\ntranslation once for a single user than a server that must do it thousands\nof times an hour for all users.\n\n>If we were writing a standard from scratch, I'd insist on one\n>line break.\n\nWouldn't be a very efficient standard when implemented. Every compiler I'm\naware of supports multiple representations for EOL. Why shouldn't the\nparsers associated with HTTP and HTML be equally tolerant? HTML files are\n\"source code\" for the HTTP \"compiler.\" If you demand a standard EOL token,\nwhy don't we make sure that text starts in card column 6 while we're at it?\n:) Tolerance is the order of the day here. It makes things more efficient\nfor servers and MUCH easier for users.\n\n>It would be nice of HTTP and HTML standards agreed on the treatment\n>of line breaks in text/html....\n\nI agree... as long as it accomodates all the representations for EOL in\ncurrent practice. The current attitudes towards this seem to be very\nUnix-centric and this is very wrong. It won't be long before we see HTTP\nservers that have NOTHING to do with a local file system and reside on top\nof a DBMS or some other non-traditional object store. I'm not aware of ANY\ncommercial DBMS implementations that use LF as EOL. This diverges from the\ntopic a bit, but I'm trying to make a point that it is NOT sufficient to\naccomodate only a portion of the platforms in use (e.g., Unix) in the\nstandard as they will represent a decreasing proportion of Web platforms as\nthe Web grows.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Chuck Shotton said:\n>At 11:35 PM 12/1/94, Albert Lunde wrote:\n>>However, I think this is too much of a burden to add to client\n>>writers.\n\nIt might or might not be.  However, we do need to decide whether this\nbehavior should be described in an appendix on \"how to be tolerant of\nbad servers\" or it should be a required part of the spec.  If it\nstays where it is now, a \"non-bad server\" needs to be defined more\nclearly.\n\n(Note that this is also a burden on server authors, unless we require\nthat the \"request\" portion be in canonical form but allow the\n\"response\" portion not to be; this would be sort of strange.)\n\n>Not at all. Show me a single client that doesn't already do this. As Roy\n>says, it is also an issue of scale. A client can more effectively do this\n>translation once for a single user than a server that must do it thousands\n>of times an hour for all users.\n\nThis is assuming servers do this on the fly.  While this is one way\nthey might do this, it certainly is not the only way.  A clever server\nmight store the document in canonical form after converting it once,\nor cache frequently-requested documents after the conversion, or\nwhatever.  If it's important to do, it can be implemented efficiently;\nthe question is whether it's important.\n\nIn any case, if you are sending text with something other than CRLFs\nas line breaks, you are not sending text/plain; it's something else.\nThe definition of text/plain is very clear on this point.  I merely\nbelieve that, whatever it is, it should be clearly labeled.\n\nIf we want to invent a way to label it, that's OK; Content-Encoding is\na good way, and since it's not part of MIME we can do whatever we like\nwith it.\n\nThat reminds me, the current draft officially codifies \"x-compress\"\nand \"x-gzip\" as registered encodings. Since the x-token convention is\nnormally used to indicate unregistered stuff, I think this should\nchange to just \"compress\" and \"gzip\" [with compliance for the old\nlabels mentioned, since existing system will still use them.]\n\n(Actually, I think the encodings should be \"LZW\" and \"LZ77\", refering\nto the actual algorithms rather than the names commonly given to the\nUNIX implementations of those algorithms, but that would cause more\nconfusion than it's worth.)\n\n>Wouldn't be a very efficient standard when implemented. Every compiler I'm\n>aware of supports multiple representations for EOL. Why shouldn't the\n>parsers associated with HTTP and HTML be equally tolerant? HTML files are\n>\"source code\" for the HTTP \"compiler.\"\n\nHTML and HTTP are orthogonal.  And the issue of transmitting objects\nin canonical form is not just about text, though that's the most\nprominent example.\n\n>>It would be nice of HTTP and HTML standards agreed on the treatment\n>>of line breaks in text/html....\n\nIndeed it would be.  However, MIME-Version 1.0 requires that all\ntextual subtypes have line breaks represented as CRLFs, so the\ndecision is pretty easy unless we want to register it as\napplication/html.\n\n>I agree... as long as it accomodates all the representations for EOL in\n>current practice. The current attitudes towards this seem to be very\n>Unix-centric and this is very wrong.\n\nAnd requiring all clients in existence, regardless of what platform\nthey run on, to understand the UNIX conventions for line breaks in\ntext is not UNIX-centric?  Huh?\n\n>It won't be long before we see HTTP\n>servers that have NOTHING to do with a local file system and reside on top\n>of a DBMS or some other non-traditional object store. I'm not aware of ANY\n>commercial DBMS implementations that use LF as EOL. This diverges from the\n>topic a bit, but I'm trying to make a point that it is NOT sufficient to\n>accomodate only a portion of the platforms in use (e.g., Unix) in the\n>standard as they will represent a decreasing proportion of Web platforms as\n>the Web grows.\n\nI absolutely agree; over time, more and more different platforms will\nbe used in widely varying ways.  But I don't think this supports your\nposition; quite the opposite.  This is why I shy away from codifying\ninto the standard (is this intentended to be an Internet\nstandards-track protocol?) UNIX-centrisms or requirements that all\nimplementations understand the conventions used by every different\nsystem there is.\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: http://info.cern.ch/hypertext/WWW/Protocols/Overview.htm",
            "content": "Thus wrote: \"Roy T. Fielding\"\n>Larry Masinter wrote (on Monday):\n>>> 4.2 Content Types\n>> \n>> In what way is the HTTP content-type a superset of the MIME BNF?\n>> If you must repeat some BNF that occurs in another RFC, you must\n>> identify how this is either just a repeat, or a modification, and if\n>> it is a modification, how it is different from the source.\n>\n>It is a superset because it does not restrict itself to the official\n>MIME types and x-token types, as does the MIME spec.  Thus, the parsing\n>is the same but without restricting the token values.  This is equivalent\n>to the RFC 1590 decision to not constrain media types.\n\nI'm sorry, I don't see this difference is meaningful.  MIME defines\nsome types, a system for registering them, and says that unregistered\ntypes should have x- labeling.  1590 improves the registration process\nand changes the terminology to reduce email-centrisms, but changing\nthe grammar hardly seems necessary.  Are you saying HTTP should not\ncall for unregistered media types to employ the x-token convention?\n\nMind you, there are aspects of media type procedures I'd like to see\nchanged to increase HTTP-friendliness.  I'd like to see an easier way\nto revisit media type definitions, and I'd really like to see more\nliberal guidelines for optional parameters (since in HTTP, unlike\nEmail, you sometimes have the C-T without the object itself.)  But\nthat's for another day.\n\n>I'd rather include the generic syntax, just in case the MIME people\n>change their minds.  I am tempted to just remove MIME-Version altogether\n>(except for gateways), but that will have to be a group decision.\n\nGiven that we're picking and choosing which portions of MIME to\ninclude and which to ignore, tying to 1.0 seems as sensible as\nanything else.\n\n- Marc\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Where is the latest draf",
            "content": "I'm looking for the latest http and html drafts, can someone pointme at\nthem, they dont seem to be up to date on ds.internic.net,\n\n- Mitra\n\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">>Not at all. Show me a single client that doesn't already do this. As Roy\n>>says, it is also an issue of scale. A client can more effectively do this\n>>translation once for a single user than a server that must do it thousands\n>>of times an hour for all users.\n>\n>This is assuming servers do this on the fly.  While this is one way\n>they might do this, it certainly is not the only way.  A clever server\n>might store the document in canonical form after converting it once,\n>or cache frequently-requested documents after the conversion, or\n>whatever.  If it's important to do, it can be implemented efficiently;\n>the question is whether it's important.\n\nA clever server might do a lot of things that require a huge amount of\nextra programming for a diminishing return. I think requiring a server to\ndo something as dorky as translate line ends is ABSURD when a single line\nof code added to any client that doesn't already have it accomplishes the\nsame thing.\n\nI am really having a difficult time understanding the side of this argument\nthat says \"life is a linefeed.\" I don't think I am being unreasonable in\nthe points I am making, which advocate tolerance and flexibility. There\nseems to be a contingent opposed to flexibility in HTTP. This is inherently\na Bad Thing (tm). There are a LOT of ways to send a request to a HTTP\nserver besides Mosaic or NetScape, and there are a LOT of applications that\nmight respond to a HTTP request besides NCSA and CERN httpd. Accomodating a\nwide range potential input only makes the HTTP protocol stronger and of\nwider appeal.\n\nAnal retentive standards tweaking that legislates absolute, strict\nbehaviors guarantees that a lot of software will break or otherwise be\nincompatible. Defining a standard which accomodates standard practice and\nencourages support for the wide variety of clients, servers, and other\ntypes of applications that may participate in the WWW is going to mean a\nstandard that lasts a lot longer.\n\n>In any case, if you are sending text with something other than CRLFs\n>as line breaks, you are not sending text/plain; it's something else.\n>The definition of text/plain is very clear on this point.  I merely\n>believe that, whatever it is, it should be clearly labeled.\n\nI am not even talking about text/plain. I am discussing the majority of\ntext returns by HTTP servers, namely text/html. And, I think you will find\nthat text/plain has flexibility in its interpretation as well and is not as\nabsolute as you would like it.\n\n>>Wouldn't be a very efficient standard when implemented. Every compiler I'm\n>>aware of supports multiple representations for EOL. Why shouldn't the\n>>parsers associated with HTTP and HTML be equally tolerant? HTML files are\n>>\"source code\" for the HTTP \"compiler.\"\n>\n>HTML and HTTP are orthogonal.  And the issue of transmitting objects\n>in canonical form is not just about text, though that's the most\n>prominent example.\n\nWhat do you mean, \"orthogonal\"? HTML is a specific syntax which is a subset\nof the basic SGML standard. HTTP is a transport protocol for a myriad of\ndifferent data types used for requests and responses between clients and\nservers implementing the standard. You can easily argue that the two are\ncompletely unrelated. I'm sure that companies like Adobe and Frame could\ncare less about HTML but are avid supporters of HTTP. I suspect you'll find\nHTTP clients and servers long after HTML has fallen by the wayside.\nHowever, this entire discussion has nothing to do with reviewing the draft\nHTTP 1.0 standards document.\n\n>>>It would be nice of HTTP and HTML standards agreed on the treatment\n>>>of line breaks in text/html....\n>\n>Indeed it would be.  However, MIME-Version 1.0 requires that all\n>textual subtypes have line breaks represented as CRLFs, so the\n>decision is pretty easy unless we want to register it as\n>application/html.\n\nSo, if we adopt the most restrictive interpretation of the MIME standard\nfor text (which no HTTP clients or servers currently do) only the Windows\nservers and clients will have the remotest chance of reasonable\nperformance. Is this what you want? If so, WHY? WHY does it matter whether\nmultiple EOL tokens are supported. If it makes clients and servers more\nrobust to support all possible variations, and it's easier to implement and\nprovides higher performance, how can you argue that a standard that would\nrequire universal modification to all clients and servers and result in a\ncorresponding performance decrease is actually better?\n\n>>I agree... as long as it accomodates all the representations for EOL in\n>>current practice. The current attitudes towards this seem to be very\n>>Unix-centric and this is very wrong.\n>\n>And requiring all clients in existence, regardless of what platform\n>they run on, to understand the UNIX conventions for line breaks in\n>text is not UNIX-centric?  Huh?\n\nThat's not what I'm advocating at all. Perhaps you need to read over this\nthread again. I'm saying that any EOL token, CR, CRLF, or LF should be\nallowed. Only one of these accomodates Unix. One accomodates Macs, and one\naccomodates Windows/DOS. (Who can say what a standard text file is on a Vax\nor IBM mainframe?)\n\n>>It won't be long before we see HTTP\n>>servers that have NOTHING to do with a local file system and reside on top\n>>of a DBMS or some other non-traditional object store. I'm not aware of ANY\n>>commercial DBMS implementations that use LF as EOL. This diverges from the\n>>topic a bit, but I'm trying to make a point that it is NOT sufficient to\n>>accomodate only a portion of the platforms in use (e.g., Unix) in the\n>>standard as they will represent a decreasing proportion of Web platforms as\n>>the Web grows.\n>\n>I absolutely agree; over time, more and more different platforms will\n>be used in widely varying ways.  But I don't think this supports your\n>position; quite the opposite.  This is why I shy away from codifying\n>into the standard (is this intentended to be an Internet\n>standards-track protocol?) UNIX-centrisms or requirements that all\n>implementations understand the conventions used by every different\n>system there is.\n\nMarc, I don't know your background, and I am truly NOT throwing rocks. But\nhave you ever implemented a Web client or server? Are you aware of the\nperformance degradation that will be incurred by forcing all clients and\nall servers to parse every byte of text that passes through their buffers?\n\"Pre-compiling\" documents to a standard format is an unacceptable and\nunnecessary burden to place on users and will cause unending errors\neverytime a user forgets. Forcing servers to perform these text\ntransformations on the fly is an unnecessary burden on CPU resources and\ncontrary to what you may think, it is a non-trivial impact. Forcing clients\nto change is equally inconvenient. Things work fine now.\n\nAll I am advocating is that the status quo be documented as part of the\nstandard. I am under the impression that this is the goal of the HTTP 1.0\nstandard anyway. You are advocating something that is NOT current practice,\nwould require rework of ALL HTTP code in all clients and all servers, and\nwould result in decreased performance, all in the name of \"standards\npurity.\" I ask you to consider your position again in light of this.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Where is the latest draf",
            "content": "> I'm looking for the latest http and html drafts, can someone pointme at\n> them, they dont seem to be up to date on ds.internic.net,\n\nThey are both available at\n\n   http://www.ics.uci.edu/pub/ietf/http/\n   http://www.ics.uci.edu/pub/ietf/html/\n\nrespectively.  Replace the http: with ftp: if you prefer FTP.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Henrik Frystyk Nielsen: Hypertext Version ready of HTTP Spe",
            "content": "[ I think that this message was meant for the list proper, not the\nadministrative address for the list. -- ange ]\n\n------- Forwarded Message\n\nDate:    Fri, 02 Dec 1994 21:33:53 +0100\nFrom:    frystyk@ptsun00.cern.ch (Henrik Frystyk Nielsen)\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: Hypertext Version ready of HTTP Spec\n\n\n\nI have just installed a HTML version of the spec. It is available at\n\nhttp://info.cern.ch/hypertext/WWW/Protocols/Overview.html\n\nIt is generated using WEBmaker, but some of the alignments are not\nperfect. This would not have been a problem if more clients support\ntables ;-)\n\n- -- cheers --\n\nHenrik\n\nPS: Sorry for not beeing very active right now on the many questions. I\nhave been extremely busy preparing a code release before I go to San\nJose tomorrow morning. I have already promissed Roy a beer overthere.\n\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "For some reason, Chuck Shotton said:\n> Marc, I don't know your background, and I am truly NOT throwing rocks. But\n> have you ever implemented a Web client or server?\n\nYes, I have.  Are you happy now?\n\nClearly there is no point to continuing along these lines.  Good day.\n\n- Marc\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "At 12:07 AM 12/5/94, Larry Masinter wrote:\n>Why isn't this a problem for FTP servers returning ASCII data; they\n>translate native EOL convention to CRLF too...\n\nA FTP server only does this if the user requests ASCII mode. And then, it\nis doing it for a single file transfer. Not every single transfer of the\nsame text file for every user request. FTP servers also handle many orders\nof magnitude FEWER requests per hour than a Web server on the same host.\nFTP servers can afford to waste a little time as they are typically batch\noriented file transfer processes with no user interface to speak of. HTTP\nservers are part of a real-time interactive system where long transfer\ntimes and unnecessary delays are unacceptable.\n\nHave you ever tried using a set of Web pages being served by a FTP server?\nI think you'll understand why HTTP was developed as a more efficient\nalternative. Simply making HTTP as inefficient as FTP defeats its entire\npurpose.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "chuck-\n on sunsite, the volume of ftp requests was on a par with the number of \nhttp requests, and the average file size was larger- however, that isn't\nthe point here. EOL translation in message bodies is handled for HTML by\nthe SGML RE/RS mechanism, so the only real win would be for text/plain \n, so it's not really worth requiring such a major change. ch\n\nFTP efficincy  for web use is more a factor of poor\nimplementations than anything else- unless the control connection cache is \nenabled, libwww ftp really crawls. With the control connection cache turned\non, ftp and http are much of a muchness. newline translation is usually not\nthat significant over wide area networks ( especially if the ftpd has a \ndecently tweaked newline scanner)\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Okay, folks, no need to get personal -- please remember that this is\ne-mail and your remarks should be addressed to the content of the\nspec -- not to each other.\n\nMarc, I understand your concerns about MIME conformance and following\nthe norms of good network behavior.  However, HTTP is not Internet\nMail and thus its design constraints are quite different.  Although it\nmay at times cause confusion, I would still prefer to use 80% of MIME\nfor HTTP/1.0 instead of 0%  -- 100% is simply not an option.\n\nUse of \"x-token\" for unofficial MIME types will not be required by\nthe spec -- in fact, that was one of the main reasons for a separate\nBNF from that given in MIME.  For reasons that I have discussed on\nprior mailing lists (and don't have time to repeat right now),\nuse of x-tokens for anything but experiments is extremely bad\nengineering and not appropriate for systems that allow content\nnegotiation.  Besides, it is not current practice (even in Mail).\n\nThe HTTP/1.0 spec will not force any changes to existing clients and\nservers EXCEPT where those applications are known to be defficient due\nto bugs in their design (i.e. bad Content-Type parsing) or in their\nimplementation (i.e. XMosaic's useless Accept: headers).\n\nWe will add explicit mention of what it means to be \"conforming\"\nand what methods/headers/responses are appropriate in what contexts\nfor the next version of the spec.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "More followup",
            "content": "Two (hopefully) simple questions:\n\n1. In a HEAD response, what should Content-Length be set to?  The\n   length of the (non-existent) object body, or 0?\n\n2. Is there a view on how locally-time-stamped data should have their\n   Last-Modified GMT computed?  It's impractical to recreate the\n   true original GMT time-stamp (as the timezone and daylight savings\n   regime of the place of last modification is usually unknown).  Using\n   the current GMT offset will result in the timestamp of some data\n   jumping forwards or backwards an hour, twice a year, which could\n   affect caching.\n\nMike Cowlishaw\nIBM UK Laboratories\n\n\n\n"
        },
        {
            "subject": "401 Unauthorized  can I use it",
            "content": "Do you have to require that the 401 Unauthorized be tied to the Basic\nAuthentication Scheme? If so, will you be loosening it up in the\nfuture? I'd like to use that return code in our DCE Web work. However,\nthe client should not retry; we got all the information we needed to\nmake an authorization decision with the original client request. What\nstatus is sent when you use the basic authentication scheme and the\nclient is still unauthorized, even after she has sent a valid\npassword?\nMez\n\n\n\n"
        },
        {
            "subject": "Re: Where is the latest draf",
            "content": "Just before I left Friday night I sent a mail to announce the HTML version of the spec.\nSomehow it didn't get through, so I will do it again!\n\nThe HTML version is available from\n\nhttp://info.cern.ch/hypertext/WWW/Protocols/Overview.html\n\nIt has some few problems with alignment of the BFN parts but we are working on that!\n\n-- cheers --\n\n\n\n"
        },
        {
            "subject": "Re:  401 Unauthorized  can I use it",
            "content": "The 401 code is not tied to the basic  AA scheme. The WWW-Authenticate and WWW-Authorization\nheaders both are defined to contain extension tokens. HOwever, if you are sure that the \nserver is not going to send the object to the client and the client shouldn't try again\nthen the right code to use is `403 Forbidden'. If using the basic AA the server should repeat\nsending back a 401 code following the current spec.\n\nThough the server can switch to a 403 code if multiple attempts have been tried, but this \nrequires that the server keeps state of the connections whic his outside the scope of the\nspec.\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": "> 1. In a HEAD response, what should Content-Length be set to?  The\n>    length of the (non-existent) object body, or 0?\n\nThe length of the object-body that would have been returned had\nthe request been a GET.\n\n> 2. Is there a view on how locally-time-stamped data should have their\n>    Last-Modified GMT computed?  It's impractical to recreate the\n>    true original GMT time-stamp (as the timezone and daylight savings\n>    regime of the place of last modification is usually unknown).  Using\n>    the current GMT offset will result in the timestamp of some data\n>    jumping forwards or backwards an hour, twice a year, which could\n>    affect caching.\n\nThere are many views as to how this should be done, but none of them\nare within the realm of the HTTP protocol.  All that matters is that\nthe date used within the protocol is GMT (UT).  How the date is obtained\n(and, in fact, what it means to be \"modified\") is entirely up to the\napplication sending the object.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": "> From \"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU>\n> \n> > 1. In a HEAD response, what should Content-Length be set to?  The\n> >    length of the (non-existent) object body, or 0?\n> \n> The length of the object-body that would have been returned had\n> the request been a GET.\n\nThis is not very clear in the description of the Content-Length field\nthat it describes the length of the object referenced by the URI,\nwhich may or may not be present in the message.  Maybe HEAD should\nreturn the description of the object as the object body of the message\nso such ambiguities and overloading dont occur.\n\n> > 2. Is there a view on how locally-time-stamped data should have their\n> >    Last-Modified GMT computed?  It's impractical to recreate the\n> >    true original GMT time-stamp (as the timezone and daylight savings\n> >    regime of the place of last modification is usually unknown).  Using\n> >    the current GMT offset will result in the timestamp of some data\n> >    jumping forwards or backwards an hour, twice a year, which could\n> >    affect caching.\n> \n> There are many views as to how this should be done, but none of them\n> are within the realm of the HTTP protocol.  All that matters is that\n> the date used within the protocol is GMT (UT).  How the date is obtained\n> (and, in fact, what it means to be \"modified\") is entirely up to the\n> application sending the object.\n\nDoesnt the lack of a clear meaning of modification make this almost '\nuseless, except maybe for a matched pair of client and server that\nhave a common meaning.  It needs to reflect that either the content\nas returned is different then previous to the date.  I could also \nreflect metainformation change, such as expiration updated.  But, I\nthink that is a little too nebulous.\n\nEither way, some agreed meaning needs to be assigned to modified.\n\nKeith\n-----------------------------------------\nKeith Ball                       Unix/SMTP mail:  kball@novell.com\nBuilding 1                      MHS mail: KBALL@NOVELL\n2180 Fortune Drive\nSan Jose Fortune  (sjf.novell.com)\n(408) 577 8428\nFax: (408) 577 5855\n\nNovell, Inc.\n\n-- sent via the LAN WorkPlace Mailer\n\n\n\n"
        },
        {
            "subject": "Proposal for HTTP Extension Mechanis",
            "content": "At the WWW F'94 conference, I promised a proposal for an HTTP extensions\nmechanism.  Anticipating the HTTP BOF session at the IETF this Wednesday\nnight, I put forth the first version of the proposal, which is available\nat http://www.research.att.com/~dmk/extend.txt.\n\nDavid M. Kristol\nAT&T Bell Laboratories\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": "At 6:06 PM 12/5/94, Keith Ball wrote:\n>> From \"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU>\n>>\n>> > 1. In a HEAD response, what should Content-Length be set to?  The\n>> >    length of the (non-existent) object body, or 0?\n>>\n>> The length of the object-body that would have been returned had\n>> the request been a GET.\n\nNot to be a party pooper, but what about URIs that point to things like\nCGIs which generate content on the fly? Would it be appropriate in this\nsituation to not return a content-length header? Zero would be as wrong as\nany other number in this case.\n\n>> > 2. Is there a view on how locally-time-stamped data should have their\n>> >    Last-Modified GMT computed?  It's impractical to recreate the\n>> >    true original GMT time-stamp (as the timezone and daylight savings\n>> >    regime of the place of last modification is usually unknown).  Using\n>> >    the current GMT offset will result in the timestamp of some data\n>> >    jumping forwards or backwards an hour, twice a year, which could\n>> >    affect caching.\n>>\n>> There are many views as to how this should be done, but none of them\n>> are within the realm of the HTTP protocol.  All that matters is that\n>> the date used within the protocol is GMT (UT).  How the date is obtained\n>> (and, in fact, what it means to be \"modified\") is entirely up to the\n>> application sending the object.\n\nIn truth, it's possible that it doesn't even matter if the conversion to\nUTC is done or not, as long as the server is consistent. Simply appending a\nGMT to the local time can work, since the clients are asking for changes\nfrom a previously returned time. As long as the time is consistent (and\ndifferent), IMS will work. This breaks things like clients that\nautomatically expire, but is this a valid use of IMS anyway?\n\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "CHaracter representation negotiatio",
            "content": "As Roy pointed out, if one wants to, one can negotiate for different\ncharacer encodings for HTML with something like the following from a\nclient:\n\n   Accept: text/html; charset=unicode_1_1_utf_7\n\nHowever, very soon, we will be getting SGML aware browsers (and also\nbrowsers for other document formats). Now we could have a charset=\non each of these different MIME types, but I think we need to get a\nsingle HTTP field allocated for this. In addition, the following are\nprobably also needed.\n\n1) Either UTF-7 or UTF-8, or both, strongly recommended by both the\n   HTML and HTTP specs as the way to transmit multilingual documents.\n2) A definition of \"escape codes\" to be used to indicate language and\n   other such parameters to aid in display purposes. As I have said\n   elsewhere, such tagging would probably happen automatically, and so\n   not be visible to the end users.\n\nI think we should look upon thse as \"enabling technology\". They will\nnot be immediately used (or at least not widely), but eventually, as\nUnicode systems (browsers in particular) become available, they will\nbe increasingly important.\n\nOn top of this foundation, we can then build 2 libraries of great\nutility: \n\n1) A library for converting between various characer ancodings, and\n   the tagged UTF.\n2) A library for handling font display using Unicode. This is not\n   exceptionally difficult.\n\nWith these, multilingual browser become, while not trivial, at least\nnot much more difficult than roman only ones.\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": "Chuck Shotton writes:\n > At 6:06 PM 12/5/94, Keith Ball wrote:\n > >> From \"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU>\n > >>\n > >> > 1. In a HEAD response, what should Content-Length be set to?  The\n > >> >    length of the (non-existent) object body, or 0?\n > >>\n > >> The length of the object-body that would have been returned had\n > >> the request been a GET.\n > \n > Not to be a party pooper, but what about URIs that point to things like\n > CGIs which generate content on the fly? Would it be appropriate in this\n > situation to not return a content-length header? Zero would be as wrong as\n > any other number in this case.\n\nAs dynamic objects don't have a default Content-Length the right thing\nto do would be not to send the header field at all. This is one of the\nthe reasons why the field can not be mandatory.\n\n > >> > 2. Is there a view on how locally-time-stamped data should have their\n > >> >    Last-Modified GMT computed?  It's impractical to recreate the\n > >> >    true original GMT time-stamp (as the timezone and daylight savings\n > >> >    regime of the place of last modification is usually unknown).  Using\n > >> >    the current GMT offset will result in the timestamp of some data\n > >> >    jumping forwards or backwards an hour, twice a year, which could\n > >> >    affect caching.\n > >>\n > >> There are many views as to how this should be done, but none of them\n > >> are within the realm of the HTTP protocol.  All that matters is that\n > >> the date used within the protocol is GMT (UT).  How the date is obtained\n > >> (and, in fact, what it means to be \"modified\") is entirely up to the\n > >> application sending the object.\n > \n > In truth, it's possible that it doesn't even matter if the conversion to\n > UTC is done or not, as long as the server is consistent. Simply appending a\n > GMT to the local time can work, since the clients are asking for changes\n > from a previously returned time. As long as the time is consistent (and\n > different), IMS will work. This breaks things like clients that\n > automatically expire, but is this a valid use of IMS anyway?\n \nOhh - this would screw up a lot of cache managers which use the LM\n(which is the previously returned value) if the expired header is not\npresent. They don't like to get a future last modified date!\nI would not recommend this at all!\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": "Keith Ball writes:\n > > > 2. Is there a view on how locally-time-stamped data should have their\n > > >    Last-Modified GMT computed?  It's impractical to recreate the\n > > >    true original GMT time-stamp (as the timezone and daylight savings\n > > >    regime of the place of last modification is usually unknown).  Using\n > > >    the current GMT offset will result in the timestamp of some data\n > > >    jumping forwards or backwards an hour, twice a year, which could\n > > >    affect caching.\n > > \n > > There are many views as to how this should be done, but none of them\n > > are within the realm of the HTTP protocol.  All that matters is that\n > > the date used within the protocol is GMT (UT).  How the date is obtained\n > > (and, in fact, what it means to be \"modified\") is entirely up to the\n > > application sending the object.\n > \n > Doesnt the lack of a clear meaning of modification make this almost '\n > useless, except maybe for a matched pair of client and server that\n > have a common meaning.  It needs to reflect that either the content\n > as returned is different then previous to the date.  I could also \n > reflect metainformation change, such as expiration updated.  But, I\n > think that is a little too nebulous.\n \nNeither of the words `modified', `different', and `change' do define\nwhat is actually meant by this field. And this is how it should be! It\ndoes _not_ make it useless but merely take away the binding to the\ndate and time typically given by the file system. As somebody (sorry,\nbut I don't remember who) pointed out, a document can be generated\nautomatically on every request, however, this doesn't necessarily\nindicate that the LM field should change value.\n\n-- cheers --\n\nHenrik Frystyk\nfrystyk@W3.org\n+ 41 22 767 8265\nWorld-Wide Web Project,\nCERN, CH-1211 Geneva 23,\nSwitzerland\n\n\n\n"
        },
        {
            "subject": "Re: More followup",
            "content": ">> The length of the object-body that would have been returned had\n>> the request been a GET.\n> \n> This is not very clear in the description of the Content-Length field\n> that it describes the length of the object referenced by the URI,\n> which may or may not be present in the message.  Maybe HEAD should\n> return the description of the object as the object body of the message\n> so such ambiguities and overloading dont occur.\n\nI will clarify the description of HEAD.  The latter suggestion, though\nappealing  from a design point of view, is outside the scope of HTTP/1.0.\n\n> ...\n> Doesnt the lack of a clear meaning of modification make this almost '\n> useless, except maybe for a matched pair of client and server that\n> have a common meaning.  It needs to reflect that either the content\n> as returned is different then previous to the date.  I could also \n> reflect metainformation change, such as expiration updated.  But, I\n> think that is a little too nebulous.\n> \n> Either way, some agreed meaning needs to be assigned to modified.\n\nWell, there is an implied meaning as given by the name (i.e. when the\nobject was last modified).  What is not defined is how much (or how little)\nmodification needs to occur, and why.  All that matters is that the origin\nserver thinks that the object was last modified at the given time -- the\nclient is trusting that the server knows what it is talking about.\n\nPerhaps we could define it in terms of how it is used, i.e.\n\"If you have a copy of this object which is older than the given LM date,\n that copy should now be considered stale.\"\n\nMaybe.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "New specification document for SHTTP version 1.",
            "content": "I've sent a copy to the Internet-Drafts editor, too late for folks at the\nIETF meeting this week to get it this way. (I know, I should have gotten it\nout earlier.)\n\nWhile we're waiting for it to get into the drafts directory, you can get\nthe new spec from:\n        http://www.commerce.net/information/standards/drafts/shttp.txt\n\nI'll also arrange for some printed copies to be available at the IETF meeting.\n\nMajor changes from the June draft:\n        * Reorganized -- includes table of contents!\n        * More motivational & supporting material; 38 versus 23 pages [sorry].\n        * Changes to the protocol to support:\n                Manually distributed symmetric keys (e.g., passwords)\n                Kerberos\n                New mode for (non-signature) message authentication/integrity\n                Unified key naming.\n\nCheers,\n-Allan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Thus wrote: \"Roy T. Fielding\"\n>Marc, I understand your concerns about MIME conformance and following\n>the norms of good network behavior.  However, HTTP is not Internet\n>Mail and thus its design constraints are quite different.  Although it\n>may at times cause confusion, I would still prefer to use 80% of MIME\n>for HTTP/1.0 instead of 0%  -- 100% is simply not an option.\n\nWell, of the Internet standard protocols, HTTP is probably most like\nFTP in its design constraints, and FTP certainly does worry about\nnewline issues (though its approach is obviously a bit dated, and it\ndoesn't worry about other forms of canonicalization.)\n\nI think I understand your concerns, although there seem a few\ndifferent perspectives on why text canonicalization is inappropriate,\nand I'd like to at least have a clear idea of what the essential\nreason is.\n\n- Existing servers and clients don't do it.  (Well, except for the\n  demo prototype I hacked up myself. :-)\n\n  If this is an informational RFC, this reason is probably sufficient,\n  in which case the issue can be forgotten about and revisited when\n  a standards-track document for HTTP/1.1 or whatever gets discussed.\n\n- The performance hit is too great.\n\n  This may be a compelling reason, though my sense of this isn't as\n  strong as I'd like it to be.  Simon has hinted that he doesn't think\n  it's a big performance issue in most cases, and I certainly would\n  consider him *the* authority on performance issues.  It does mean\n  that canonical forms should be used for everything except textual\n  line breaks, presumably.\n\n- There's no reason to do it.\n\n  For me, the issue is whether there's sufficient reason not to; by\n  default, go where the specs have paved the way before.  I suspect\n  the main issue of contention is what constitutes \"sufficient.\" :-)\n\n>Use of \"x-token\" for unofficial MIME types will not be required by\n>the spec -- in fact, that was one of the main reasons for a separate\n>BNF from that given in MIME.  For reasons that I have discussed on\n>prior mailing lists (and don't have time to repeat right now),\n>use of x-tokens for anything but experiments is extremely bad\n>engineering and not appropriate for systems that allow content\n>negotiation.  Besides, it is not current practice (even in Mail).\n\nYour use of the phrase \"will not\" suggests the final decision has\nalready been made on the issue and it's pointless for anyone to\nsuggest otherwise.  Is that what you meant to say?\n\nIt's current practice in the messages I see, but I'm sure it gets\nignored.  It's certainly the case that X-tokens can get used far more\nwidely and non-experimentally than is really appropriate, as x-gzip\nand x-compress demonstrate clearly.\n\nI think the appropriate solution is to make the process for\nregistering types good and easy enough that there isn't really any\ngood excuse for not doing so, which I believe RFC 1590 does a decent\njob of, though there are some things we may wish to try to push for\nregarding the future of the Internet media-type registry system.\n\n>The HTTP/1.0 spec will not force any changes to existing clients and\n>servers EXCEPT where those applications are known to be defficient due\n>to bugs in their design (i.e. bad Content-Type parsing) or in their\n>implementation (i.e. XMosaic's useless Accept: headers).\n\nExtra Accept: headers seem mostly just sub-optimal and sort of a\nwaste; I don't think I'd call them \"broken\" the way content-type\nparsing is.  Or did you mean the way Mosaic claims it will handle\nanything for an inline image when it will only handle two types?\nThat's broken.\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "To sum up Marcs argument:\n\n1) The performance hit is not too great\n2) If there is no reason to do it and no reason not to then follow the spec.\n\nI do not want cannonicalisation under any circumstances. I have had my fill\nof systems that \"canonicalise\" trying to be \"clever\". Such systems break\nmuch much more than they mend. Like the FTP ASCII transfer mode which is\nenabled by default in most FTP clients (but not some of the more modern ones).\n\nIn most cases canonicalisation is simply impractical, if the message body is\ncompressed then canonicalisation is a loser.\n\nWhat the MIME specs state in this area is irrelevant. MIME is designed to\npass through mail gateways. HTTP is not. It is the 8 bit clean restriction\nthat is HTTPs main win over other protocols.\n\n\nThis is a character set issue, not a content type issue. If people want to\npropose that the default characterset interprets CRLF in this manner then\nfair enough. \n\nPhill.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "At 8:17 AM 12/7/94, hallam@alws.cern.ch wrote:\n>To sum up Marcs argument:\n>\n>1) The performance hit is not too great\n\nThis is a fallacy. Simply put, it comes down to whether or not every single\ncharacter in a text transfer must be examined. There are N comparisons that\nmust be performed to place text in a cannonical form (one for each byte in\nthe file) versus ZERO comparisons if clients and servers simply tolerate\nmultiple line ends. How anyone can say this doesn't impose a noticable\nimpact on a server is beyond me.\n\nTo carry it further, simply write a C program that compares execution times\nfor block oriented reads from disk with no conversions versus character by\ncharacter I/O with CRLF conversions. The performance difference is worse by\nalmost 200% for the latter. Try it yourself.\n\n>2) If there is no reason to do it and no reason not to then follow the spec.\n>\n>I do not want cannonicalisation under any circumstances. I have had my fill\n>of systems that \"canonicalise\" trying to be \"clever\". Such systems break\n>much much more than they mend.\n\nI agree 100%. My whole point in participating in this thread has been to\nadvocate adding some verbage to the standard that documents the current\npractice of tolerating mixed line ends as the prefered method. That's it.\nSomehow this \"cannonicalization\" side show crept in and stirred up a\nhornet's nest.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\\_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                             \\\nAssistant Director, Academic Computing     \\   \"Shut up and eat your\nU. of Texas Health Science Center Houston   \\    vegetables!!!\"\ncshotton@oac.hsc.uth.tmc.edu  (713) 794-5650 \\\n_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\\-_-_-_-_-_-_-_-_-_-_-_-_-\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Phillip said:\n> To sum up Marcs argument:\n> \n> 1) The performance hit is not too great\n\nI did not say this.  I am not convinced of this one way or the other.\nIf it is, the issue of what to do is still not crystal-clear.  Anyway,\nyour comments don't seem to be primarily performance-motivated, so never \nmind this one for now.  \"We should do it, but it's expensive\" probably \nleads us to a different place than \"We shouldn't do it, it's stupid.\"\n\n> 2) If there is no reason to do it and no reason not to then follow the spec.\n\nMore or less, yes.  I am skeptical of the extents to which:\n- HTTP is somehow radically different from everything else\n- The members of this group (including me.  Especially me.) are somehow\n  radically wiser than everyone else\n\nCall me a conservative in this area.  (Sorry if that's one of your dirty\nwords, Phillip. :-)\n\n> I do not want cannonicalisation under any circumstances. I have had my fill\n> of systems that \"canonicalise\" trying to be \"clever\". Such systems break\n> much much more than they mend. Like the FTP ASCII transfer mode which is\n> enabled by default in most FTP clients (but not some of the more modern ones).\n\nI have no idea what poorly-designed FTP clients have to do with this issue.\nIdeally, FTP could work such that the decision of whether ASCII or\nbinary mode would be employed was based on the specific object, and chosen\nby the server, which should know which is appropriate.\n\nCanonicalization is not \"clever\" at all.  Trying to guess which of various\ndifferent representations for line breaks is being employed is \"trying to be\nclever.\"  Personally, I think cleverness is good; but mandating it is\nsomething else.\n\nThis is the first time I've heard someone suggest that canonicalization would\nactually break something, as opposed to merely being a performance loss or\na pedantic irrelevancy.  Can you be more specific?\n\nFile-sharing mechanisms that don't concern themselves with this (say, NFS)\nend up pushing these problems off onto their applications and seriously \nrestrict their portability (if you assume NFS is worth anything even in\na homogenous environment. :-)\n\n> In most cases canonicalisation is simply impractical, if the message body is\n> compressed then canonicalisation is a loser.\n\nYes; obviously an object stored in a compressed non-canonical form would be\na big lose to convert in this fashion.  There need to be clear guidelines for\ndealing with such cases.\n\n> What the MIME specs state in this area is irrelevant. MIME is designed to\n> pass through mail gateways. HTTP is not. It is the 8 bit clean restriction\n> that is HTTPs main win over other protocols.\n\nNo way.  FTP is not 8 bit clean?  Finger is not 8 bit clean?\n\nIt is the uniform and portable representation of metadata (i.e. HTTP headers)\nthat is HTTP's main win over other protocols.  FTP could be nearly as good if\nthere were uniform ways to find out, rather than heuristically guess at,\nthings like the last modification time and content-type of files.\n\nHTTP mostly combines the headers and content-labeling of email/MIME, the\nfile-transfer of FTP, and the lightweight request-reply nature of finger.\nQuiz:  Which of these three protocols does not employ canonicalization?\n\n> This is a character set issue, not a content type issue. If people want to\n> propose that the default characterset interprets CRLF in this manner then\n> fair enough. \n\nHTTP supports different character sets? :-)\n\nAssuming MIME takes the direction it appears to be, it's the case in every\ncharset, not just US-ASCII.  (This does have the implication that Unicode\ncan't be a text/foo type but must be an application/foo type.)\n\n- Marc\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "From:ALWS::HALLAM        7-DEC-1994 19:29:33.47\nTo:DXMINT::\"mvanheyn@cs.indiana.edu\"\nCC:HALLAM\nSubj:Re: Comments on the HTTP/1.0 draft. \n\n\nMarc,\n\n>Call me a conservative in this area.  (Sorry if that's one of your dirty\n>words, Phillip. :-)\n\nI like it when you talkd dirty to me :-)\n\n>I have no idea what poorly-designed FTP clients have to do with this issue.\n\nThe problem is that whatever we do there will be a lot of non canonicalising\nservers arround so the clients have to cope anyway. So all the canonicalisation\nrequirement will mean is that documents will get incorrectly canonicalised \nwhen they should not.\n\n>HTTP supports different character sets? :-)\n\nYep, you can define the character set as part of the content type texp/plain;\ncharset=EBSIDIC or wotever.\n\nIt woiuld be easy enough to specify a MACversion of the ASCII charset.\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">> What the MIME specs state in this area is irrelevant. MIME is designed to\n>> pass through mail gateways. HTTP is not. It is the 8 bit clean restriction\n>> that is HTTPs main win over other protocols.\n>\n>No way.  FTP is not 8 bit clean?  Finger is not 8 bit clean?\n>\n>It is the uniform and portable representation of metadata (i.e. HTTP headers)\n>that is HTTP's main win over other protocols.  FTP could be nearly as good if\n>there were uniform ways to find out, rather than heuristically guess at,\n>things like the last modification time and content-type of files.\n>\n>HTTP mostly combines the headers and content-labeling of email/MIME, the\n>file-transfer of FTP, and the lightweight request-reply nature of finger.\n>Quiz:  Which of these three protocols does not employ canonicalization?\n\nMarc,\n  It's not clear that you're discussing the same thing as everyone else. No\none disputes the need for the request and response headers to use standard\nEOL encoding. That's a given and everyone understands that this is the\ncase. This is a no-brainer because all the headers are generated by clients\nand servers and can always be generated correctly.\n\nThis discussion is about object-bodies ONLY. Frankly, your continued\narguing for cannonicalization in this area is contrary to A) current\npractice, B) common sense, and C) any perceived need on anyone elses' part.\nI have yet to hear a factual, supported reason why this must be done or\nelse HTTP will fail. It isn't done now, everything works great, and nobody\nis forced to waste a bunch of CPU cycles to munge text files to keep a few\nstandards junkies happy.\n\nI apologize if my tone here is unprofessional, but continuing to throw up\nobstacles with respect to the subject of tolerant interpretation of line\nends without citing any rationale other than \"cannonicalization is a Good\nThing\" is wearing thin. Either cite some evidence as to why it's a good\nthing or leave it be. (and references to other protocols are of limited\nvalue, because most are batch oriented vs. interactive and none except\ngopher experience anywhere near the load of HTTP in terms of transactions\nper hour. Performance is THE major issue here, not squeaky clean, overly\nrestrictive standards definitions.)\n\nOpinions that cannonicalization has no impact on clients or servers and\nshould be done because it's \"cleaner\" aren't borne out by fact, at least as\nfar as the tests I have run are concerned or my experience with about 5000\ninstalled MacHTTP sites indicates. You are asking that current practice be\ndiscarded in favor of an idea that has not been proven to be of any use to\nthe HTTP community. Pardon me if I am skeptical.\n\n>> This is a character set issue, not a content type issue. If people want to\n>> propose that the default characterset interprets CRLF in this manner then\n>> fair enough.\n>\n>HTTP supports different character sets? :-)\n\nIt could support a million. It doesn't matter. The ideal situation is for\nHTTP servers to be able to completely ignore the data they are transporting\nin object-bodies. They can all do this now. Your approach means that EVERY\nserver except your Perl version will have to be recoded to examine EVERY\nbyte of text content they transmit. This is absurd.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\\_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                             \\\nAssistant Director, Academic Computing     \\   \"Shut up and eat your\nU. of Texas Health Science Center Houston   \\    vegetables!!!\"\ncshotton@oac.hsc.uth.tmc.edu  (713) 794-5650 \\\n_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\\-_-_-_-_-_-_-_-_-_-_-_-_-\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Marc VanHeningen writes:\n\n> Well, of the Internet standard protocols, HTTP is probably most like\n> FTP in its design constraints, and FTP certainly does worry about\n> newline issues (though its approach is obviously a bit dated, and it\n> doesn't worry about other forms of canonicalization.)\n\nFTP does not share the same design goals as HTTP.  In fact, HTTP0\nwas specifically created because of that difference in goals.\n\n> I think I understand your concerns, although there seem a few\n> different perspectives on why text canonicalization is inappropriate,\n> and I'd like to at least have a clear idea of what the essential\n> reason is.\n> \n> - Existing servers and clients don't do it.  (Well, except for the\n>   demo prototype I hacked up myself. :-)\n\nThat is the reason it is not in HTTP/1.0\n\n> - The performance hit is too great.\n\nThat is why it probably won't be in HTTP/1.1\n\n>   This may be a compelling reason, though my sense of this isn't as\n>   strong as I'd like it to be.  Simon has hinted that he doesn't think\n>   it's a big performance issue in most cases, and I certainly would\n>   consider him *the* authority on performance issues.  It does mean\n>   that canonical forms should be used for everything except textual\n>   line breaks, presumably.\n\nThe performance difference is easily measured -- I have already done so.\n\n>>the spec -- in fact, that was one of the main reasons for a separate\n>>BNF from that given in MIME.  For reasons that I have discussed on\n>>prior mailing lists (and don't have time to repeat right now),\n>>use of x-tokens for anything but experiments is extremely bad\n>>engineering and not appropriate for systems that allow content\n>>negotiation.  Besides, it is not current practice (even in Mail).\n> \n> Your use of the phrase \"will not\" suggests the final decision has\n> already been made on the issue and it's pointless for anyone to\n> suggest otherwise.  Is that what you meant to say?\n\nYes, for HTTP/1.0.  The spec describes how to parse media types -- NOT\nwhich ones are standard/experimental/valid.\n\n> It's current practice in the messages I see, but I'm sure it gets\n> ignored.  It's certainly the case that X-tokens can get used far more\n> widely and non-experimentally than is really appropriate, as x-gzip\n> and x-compress demonstrate clearly.\n\nAs they have clearly demonstrated, using \"x-\" types outside of a small\nexperiment is a bad idea.  Requiring that ALL types be standard or\nexperimental is just plain brain-damaged (and yes, that term applies\nequally well to those specs that have required it in the past).\nCurrent practice is to ignore this requirement -- I prefer not to issue\nit in the first place.\n\n> I think the appropriate solution is to make the process for\n> registering types good and easy enough that there isn't really any\n> good excuse for not doing so, which I believe RFC 1590 does a decent\n> job of, though there are some things we may wish to try to push for\n> regarding the future of the Internet media-type registry system.\n\nSaying something and doing it are entirely different things.  The spec\ndoes not prevent types from being registered -- it just doesn't require\nthat conforming implementations check the registry upon receipt of a\nnon-standard type.  The same is true in MIME -- they just lie in the RFC.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">   It's not clear that you're discussing the same thing as everyone else. No\n> one disputes the need for the request and response headers to use standard\n> EOL encoding. That's a given and everyone understands that this is the\n> case. This is a no-brainer because all the headers are generated by clients\n> and servers and can always be generated correctly.\n> \n> This discussion is about object-bodies ONLY. Frankly, your continued\n> arguing for cannonicalization in this area is contrary to A) current\n> practice, B) common sense, and C) any perceived need on anyone elses' part.\n> I have yet to hear a factual, supported reason why this must be done or\n> else HTTP will fail. It isn't done now, everything works great, and nobody\n> is forced to waste a bunch of CPU cycles to munge text files to keep a few\n> standards junkies happy.\n> \n> I apologize if my tone here is unprofessional, but continuing to throw up\n> obstacles with respect to the subject of tolerant interpretation of line\n> ends without citing any rationale other than \"cannonicalization is a Good\n> Thing\" is wearing thin. Either cite some evidence as to why it's a good\n> thing or leave it be. (and references to other protocols are of limited\n> value, because most are batch oriented vs. interactive and none except\n> gopher experience anywhere near the load of HTTP in terms of transactions\n> per hour. Performance is THE major issue here, not squeaky clean, overly\n> restrictive standards definitions.)\n\nI think there are two issues:\n\nWhat we should advocate, and how we should explain it.\n\nThe old spec i.e.\n<http://info.cern.ch/hypertext/WWW/Protocols/HTTP/HTTP2.html>\n\nreferences the RFC822 and MIME specs rather briefly, saying\n(in part):\n\nUnder The request:\n\"The request is sent with a first line containing the method to be \napplied to the object requested, the identifier of the object, \nand the protocol version in use, followed by further information \nencoded in the RFC822 header style.\"\n\nUnder Response Data it says:\n\"Additional information may follow, in the format of a MIME message body. \nThe significance of the data depends on the status code.\"\n\nUnder Object Contents it says:\n\"The data (if any) sent with an HTTP request or reply is in a format \nand encoding defined by the object header fields, the default \nbeing \"plain/text\" type with \"8bit\" encoding. Note that while all\nthe other information in the request (just as in the reply) \nis in ISO Latin1 with lines delimited by Carriage Return/Line \nFeed pairs, the data may contain 8-bit binary data.\"\n\nUnder Client tolerance of bad HTTP servers it says:\n\n\"Clients should be tolerant in parsing response status lines, in particular\nthey should accept any sequence of white space (SP and TAB) characters\nbetween fields.\n\nLines should be regarded as terminated by the Line Feed, and the\npreceeding Carriage Return character ignored.\"\n \nMy reading of this is that the old spec seemed to say that\nheaders and text object bodies should be sent with CR/LF\nas end of line. (just like MIME/RFC 822), but that clients\nshould tolerate other stuff from \"bad\" servers.\n\nNow, it seems like we are saying is that current practice\n(not just \"bad\" servers) is to treat EOL differently in\nthe object body for performance reasons.\n\nIn this, and in other ways, we are not just quoting the MIME\nspec, we are sort of rewriting it.\n\nWe need to be clear which parts of MIME we are reusing and\nwhich parts we are rewriting. (Are we redefining EOL\ntreatment for object bodies of type text/*? \n\nWe might do as well to redefine EOL for the whole message\nto remap CR LF or CRLF to EOL before doing any MIME header/body\nprocessing. It might make a spec easier to write.)\n\nThere is also the need to make this agree with the HTML spec\nwhich in the current draft is focused on defining HTML as\nSGML application, but talks as though it was referring to\nthe MIME network representation.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Phillip said:\n>The problem is that whatever we do there will be a lot of non canonicalising\n>servers arround so the clients have to cope anyway. So all the \ncanonicalisation\n>requirement will mean is that documents will get incorrectly canonicalised \n>when they should not.\n\nWhoa.  Are you saying a server that returns canonicalized text (as most MS-DOS\nbased servers presumably will) is broken, and clients will not be able to \nhandle the results?\n\nAny HTTP implementation should be able to deal with objects in canonical form,\nas well as possibly other forms.  One that can't do that is broken.  There is\nno case where objects must not get canonicalized; only possibly areas where\nthey *may* not.\n\n>>HTTP supports different character sets? :-)\n> \n>Yep, you can define the character set as part of the content type texp/plain;\n>charset=EBSIDIC or wotever.\n\nYes, I know; hence the smiley.  But in practice, client support is within\nepsilon of being nonexistent.  As is server support.  They can't even parse\nthe header right, let alone actually attempt to display the specified\ncharacter set.\n\n>It woiuld be easy enough to specify a MACversion of the ASCII charset.\n\nOne (more than one, if memory serves) already exists and is registered.\n\nChuck said:\n>  It's not clear that you're discussing the same thing as everyone else. No\n>one disputes the need for the request and response headers to use standard\n>EOL encoding. That's a given and everyone understands that this is the\n>case. This is a no-brainer because all the headers are generated by clients\n>and servers and can always be generated correctly.\n\n(Actually, I think your statement that all headers always are generated by\nclients and servers is a bit shortsighted, and I can envison instances in\nwhich HTTP headers are stored in files and shipped wholesale, but never mind\nthat now.)\n\n>This discussion is about object-bodies ONLY.\n\nNo, this discussion is about *textual* object-bodies ONLY.  I think everybody\nagrees that, in addition to headers, GIFs and audio files and MPEGs and\neverything else should be shipped around the network in their canonical form,\nrather than in some local form.  Luckily, most of the systems that people\nuse happen, by a convenient coincidence, to locally use the canonical form\nor something easily converted into canonical form, so that there isn't a\nrequirement for expensive conversion.\n\nThere are minor conversions; in a sense the Macintosh local form could be\nsaid to include the resource fork as well as the data fork, but I don't\nthink anybody thinks all clients should understand macbinary even though\nthis could be said to be the local form of Mac files.  Mac servers should\nconvert, say, GIF files stored on a Mac to canonical form by discarding\nthe resource fork and sending only the data fork.  Do we at least agree\non this point?\n\nWhat you are suggesting is that everything go in canonical form *except* text,\nwhich should be considered a special case because it's common, has varying\nlocal forms, but those local forms are not inordinately difficult to understand\nin a flexible fashion.  This may be a reasonable exception.  But it is an\nexception, an argument that textual object-bodies should be a special case.\n\n>I have yet to hear a factual, supported reason why this must be done or\n>else HTTP will fail.\n\nThat's OK, I have yet to hear a clear statement from you which of the two\npositions I mentioned in the first paragraph of my previous message is yours.\nDo you think that canonicalizing line breaks is technically fine but just too\nexpensive to implement, or that it's just plain dumb?  It does make a\ndifference.  If you think the former, then we agree about the important\nstuff.\n\n>You are asking that current practice be\n>discarded in favor of an idea that has not been proven to be of any use to\n>the HTTP community.\n\nNo, I am stating that I think existing standards and practices outside of\nHTTP are being dismissed without due consideration.  I fully expect us to\neventually get a reasonable approach which either tolerates or standardizes\nexisting practice with regard to the special treatment of textual objects.\nThe question is whether it should tolerate it, as the current spec appears\nto do, or standardize it, and exactly how.\n\nAlbert Lunde said:\n>Now, it seems like we are saying is that current practice\n>(not just \"bad\" servers) is to treat EOL differently in\n>the object body for performance reasons.\n>\n>In this, and in other ways, we are not just quoting the MIME\n>spec, we are sort of rewriting it.\n\nYes.  Absolutely.\n\n- Marc\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "At  3:22 PM 12/7/94 -0600, I  wrote:\n>We need to be clear which parts of MIME we are reusing and\n>which parts we are rewriting. (Are we redefining EOL\n>treatment for object bodies of type text/*?\n>\n>We might do as well to redefine EOL for the whole message\n>to remap CR LF or CRLF to EOL before doing any MIME header/body\n>processing. It might make a spec easier to write.)\n\nAs I looked at this more, and re-read the MIME spec, I realized this\nsuggestion makes no sense in the case that an HTTP message contains\narbitrary binary data types, since we clearly only want to cannonize EOL on\ntext data.\n\nThis led me back to asking: What are we doing (in a MIME framework) when we\nsay we tolerate or advocate a different or mixed EOL encoding than CRLF in\ntext bodies?\n\nIt seems to me that we are in effect defining a\nnew Content-Transfer-Encoding: which is not \"binary\" but something more\nlike\"8bit-sloppy-eol\" ;) ;) and saying that this new encoding shall be the\ndefault for text bodies.\n\nAlternately, we can say that text/html *is* treansferred as binary and push the\nquestion off onto the HTML WG (then fight over how to treat EOL in the other\ntext types.) ;)\n\nI'm a little worried about the more complex MIME constructs like multipart\ngoing off the deep end if we can't make the treatment of EOLs 100% clear.\n\nCan someone think of a nicer way to express the idea of mixing EOLs in MIME\nterms?\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">>This discussion is about object-bodies ONLY.\n>\n>No, this discussion is about *textual* object-bodies ONLY.  I think everybody\n>agrees that, in addition to headers, GIFs and audio files and MPEGs and\n>everything else should be shipped around the network in their canonical form,\n>rather than in some local form.  Luckily, most of the systems that people\n>use happen, by a convenient coincidence, to locally use the canonical form\n>or something easily converted into canonical form, so that there isn't a\n>requirement for expensive conversion.\n\nRight, textual object-bodies, the most important of which is text/html\n(because of the problems introduced with <pre> text within the HTML).\n\n>There are minor conversions; in a sense the Macintosh local form could be\n>said to include the resource fork as well as the data fork, but I don't\n>think anybody thinks all clients should understand macbinary even though\n>this could be said to be the local form of Mac files.  Mac servers should\n>convert, say, GIF files stored on a Mac to canonical form by discarding\n>the resource fork and sending only the data fork.  Do we at least agree\n>on this point?\n\nSure, but text files by their very nature are less precise than a GIF file.\nWe all recognize when a picture wasn't transfered properly, but in the case\nof HTML, white space is usually insignificant. In the specific case of\n<pre> text within HTML (or even text/plain for that matter), how to\ninterpret line ends is of critical importance. You can easily argue that\ncannonicalization of text solves this problem, and I would agree 100% if it\nweren't for the conversion problem.\n\nThe conversion process for turning a Mac GIF into a Unix GIF is a trivial\nexercise. Simply read blocks of data from the data fork and spew them out\nthe IP connection. No extra CPU power required. The process to convert text\nto cannonical form is not as simple, because there isn't a standard\ncross-platform representation for text data. In the simplest case, line\nends vary, to say nothing of character sets. This conversion process is CPU\nintensive and isn't required for the current suite of HTTP applications. My\nopposition to a cannonical text format requirement is based solely on the\nperformance hit. In truth, MacHTTP already does this conversion and I can\nsay with certainty that files sent without conversion transfer twice as\nfast as ones that must be parsed first.\n\n>What you are suggesting is that everything go in canonical form *except* text,\n>which should be considered a special case because it's common, has varying\n>local forms, but those local forms are not inordinately difficult to understand\n>in a flexible fashion.  This may be a reasonable exception.  But it is an\n>exception, an argument that textual object-bodies should be a special case.\n\nThat is somewhat close to what I'm saying. Text is a special case because\ntext is a special case. It isn't as clearly defined as any of the other\nstructured binary types. By its very nature, clients and servers must take\nspecial cares to deal with it. Because they already do, we are able to\ntransfer text files without line by line conversion\n\n>>I have yet to hear a factual, supported reason why this must be done or\n>>else HTTP will fail.\n>\n>That's OK, I have yet to hear a clear statement from you which of the two\n>positions I mentioned in the first paragraph of my previous message is yours.\n>Do you think that canonicalizing line breaks is technically fine but just too\n>expensive to implement, or that it's just plain dumb?\n\nIt's just plain dumb because it's too expensive to implement. ;) Seriously,\nforcing interpretation of text is a bad thing from a performance\nperspective, not because it doesn't make sense to do in an ideal world.\n\n>>You are asking that current practice be\n>>discarded in favor of an idea that has not been proven to be of any use to\n>>the HTTP community.\n>\n>No, I am stating that I think existing standards and practices outside of\n>HTTP are being dismissed without due consideration.  I fully expect us to\n>eventually get a reasonable approach which either tolerates or standardizes\n>existing practice with regard to the special treatment of textual objects.\n>The question is whether it should tolerate it, as the current spec appears\n>to do, or standardize it, and exactly how.\n\nI think it's beyond the scope of the HTTP standard to try and standardize\nthe representation of text information across all platforms.\n\n>Albert Lunde said:\n>>Now, it seems like we are saying is that current practice\n>>(not just \"bad\" servers) is to treat EOL differently in\n>>the object body for performance reasons.\n>>\n>>In this, and in other ways, we are not just quoting the MIME\n>>spec, we are sort of rewriting it.\n>\n>Yes.  Absolutely.\n\nNo, this is the HTTP spec! As Roy has said earlier, there are lots of\nthings that are MIME-like, and may even be considered MIME by some people,\nbut the HTTP standard doesn't say how to interpret the semantics of this\nMIME info. Rather, it says how to parse it. Interpretation is the MIME\nstandard's problem. The MIME standard says nothing about the \"proper\"\nformat for GIF files, XBMs, MPEGs, or any of the other content-types\n(except for text/plain, perhaps) used by HTTP. The HTTP standard doesn't\neither. So, why are we singling out text in object bodies?\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">agrees that, in addition to headers, GIFs and audio files and MPEGs and\n>everything else should be shipped around the network in their canonical form,\n>rather than in some local form.  Luckily, most of the systems that people\n>use happen, by a convenient coincidence, to locally use the canonical form\n>or something easily converted into canonical form, so that there isn't a\n>\n\nUm. Please define \"canonical text form\". There are far too many\nencodings for this to to exist. MPEG and GIF are shipped as a sequence\nof bytes.\n\nMy proposal for dealing with this in HTTP is to have a seperate field\nfor charset negotiation, and to ship Unicode (UTF) (marked up with\nsome languages/presentational tags that are autogenerated) as the\n\"canonical\" form into which everything can be converted into and from.\nMy proposal also allows clients to send any other commonly understood\ncharset (or rather encoding/charset pair, though MIME messed these two\ntogether... another area where \"it's broke\"). If MIME does not allow\nthis, it is hopelessly broken, and doomed to be changed at some point\nin the future. Hence, this CRLF issue should just be ignored as\n\"impractical\".\n \n\n\n\n"
        },
        {
            "subject": "i18n and the WW",
            "content": ">Gavin Nicol (gtn@ebt.com) recently proposed using Unicode as a kind of\n>character set \"bus\". Servers and clients would deal with their own, local\n\nYes, but a very important part of my proposal is the auto-tagging\nperformed in the conversion process. Simply using Unicode, with no\ndisambigutation tags, will cause information loss. In addition to the\nHan Unification problem, my proposal could probably be used to extend\nthe coverage for the languages that Unicode does not cover\ncompletely. In addition, I think it is very important to maintain the\npresentational differences of local encoding systems. For example, on\ncertain Japanese platforms, zenkaku and hankaku are used extensively,\nand in Japanese typography, the Kinsoku (rendering) rules are\ndifferent. In Unicode, they have the same code position I believe. As\nyou note toward the end of your document, such \"hints\" are not\ngenerally required to get legible renderings, but they are *vital* for\ngetting top quality output.\n\n>1. Since there is complete round-trip mapping between Unicode and the\n>character sets in use today.\nI understand that it does not cover Thai completely, and that it only\ncovers 12 of the 15 Indian languages. I may be wrong though.\n\n>The only disadvantage I see is how to enable such use of Unicode while\n>still supporting existing clients who don't understand it (the issue raised\n>by Sandra O'Donnell of OSF). In the absence of a character set negotiation\n>protocol in HTTP, it's not clear how to do this. It is certainly not the\nYes, and as I pointed out, the \"text/html; charset=xxxxx\" is not\nsufficient. We need a proper field in HTTP.\n\n>clients. It has the disadvantage that Asian characters take three bytes\n>each, a 50% increase in overhead over the encoding methods used now.\nAre all Asian encodings 2 bytes? I believe Thai is 4.... \n\n>Straight Unicode results in no overhead for Asian users, but users of\n>8859-x will pay double overhead. Since it is very easy to mechanically\n>convert between UTF-8 and Unicode, if there were a character set\n>negotiation protocol, you could select the appropriate encoding based on\n>client preferences.\n\nYes, and as another optimisation, 2 systems with the same local\ncharacter set and encoding could use that (SJIS->SJIS). In the case of\nSGML/HTML, this buys you little though: in the parser, it is simpler\nto have everything be 16 bits wide. That is, you have:\n\n   Machine #1         Machine #2           Parser\n   SJIS--------------->SJIS--(conv)------->Unicode/Wide Characters\n   SJIS----(conv)----->UTF---(conv)------->Unicode/Wide Characters\n\n>Another issue with using straight Unicode is that the MIME spec (at least,\n>the new draft version) specifies that all subtypes of the \"text\" content\n>type must use CRLF (0x0D 0x0A) line feed conventions, which rules out any\n>character set that does not use ASCII as a base, and certainly rules out a\n>16 bit character set like Unicode.\n\nSo this suggests UTF-8 or UTF-7, or a change in the MIME standard. \n\n>Also note that this only occurs when the recipient has a multilingual,\n>multi-character set operating system with the right fonts installed. This\n>capability is still a rarity. In all other cases, the language tagging\n>won't make any difference in the end result.\n\nPlanning for the future is never a bad thing. The\nlanguage/presentational hint auto-tagging can be ignored by many\ncurrent systems (and it *could* be part of format negotiation), but as\nUnicode aware systems become more popular, they will be useful.\n\n>In conclusion, I'm pretty familiar with Unicode but not very familiar\n>with HTTP and HTML. I am hoping to work together with the people on\n>this list to enable use of Unicode on WWW.\n\nNice to have you here! I am not a Unicode expert....\n\nAnyway, as I said in a recent posting, and again, above: just\n\"text/html; charset=xxx\" will not suffice for all the upcoming\ndocument formats, so we need a specific field in HTTP allocated for\ncharset negotiation, and I would like to see UTF-7 or UTF-8 strongly\nrecommended as the preferred encoding for documents in which the\ndefault MIME and HTML (US-ASCII and 8859-1) do not suffice. I will do\nthe necessary editorial work if need be.\n\nOf course, for quick acceptance, we also need *free* libraries (in the\nGNU/BSD sense) for Unicode conversion, and Unicode font mapping. I\nwould be *very* interested in hearing about peoples experiences\nimplementing Unicode based systems. My main experience is in a\nwindowing system I am writing for an experimental OS (shades of\nplan9?).\n\n---------------\nGavin Nicol                                         gtn@ebt.com\n*Not* speaking for EBT.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Concerning the many remarks about EOL encoding, here's a possible\n(somewhat ugly) solution:  clients can specify a header Accept-EOL\nwith values like CR, LF, CRLF.  The default would be\nAccept-EOL: CR;LF;CRLF\n\nA permissive client could specify all three encodings (the default),\nwhich would take the server off the hook for doing the transformation.\nHowever, if a Unix server (with EOL == LF) got only an Accept-Encoding:\nCR from the client, it would be obliged to do the translation, ugly\nthough that might be.  But at least this way the client says what it\nallows, and the server knows both what is expected and what it has (I\npresume).\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Roy said:\n> Marc VanHeningen writes:\n> > - Existing servers and clients don't do it.  (Well, except for the\n> >   demo prototype I hacked up myself. :-)\n> \n> That is the reason it is not in HTTP/1.0\n\nSeems a bit inconsistent, since there are plenty of other things that existing\nimplementations don't do but remain in the 1.0 spec, but OK.\n\n> > It's current practice in the messages I see, but I'm sure it gets\n> > ignored.  It's certainly the case that X-tokens can get used far more\n> > widely and non-experimentally than is really appropriate, as x-gzip\n> > and x-compress demonstrate clearly.\n> \n> As they have clearly demonstrated, using \"x-\" types outside of a small\n> experiment is a bad idea.  Requiring that ALL types be standard or\n> experimental is just plain brain-damaged (and yes, that term applies\n> equally well to those specs that have required it in the past).\n> Current practice is to ignore this requirement -- I prefer not to issue\n> it in the first place.\n\nThe only unregistered type I see employed widely without an x-prefix is\n\"text/html\", frankly.  \"Current practice\" is a nebulous thing.\n\nContent-Encoding: demonstrates that the HTTP community didn't do a very good\njob of actually formalizing its procedures, preferring instead to get something\njust good enough to kind of work for the immediate need and leaving it at that.\nThe problem was that there was no defined way to register these encodings, and\nso it didn't happen.  These concerns are not applicable to media-types.\n\nIt would indeed be nice, I guess, if there were more of a continuium between\n\"experimental\" and \"standard\", which effectively means a standards-track for\nmedia types.  Perhaps there will be one at some point.  But I don't think\nthat's what you really want.\n\n> > I think the appropriate solution is to make the process for\n> > registering types good and easy enough that there isn't really any\n> > good excuse for not doing so, which I believe RFC 1590 does a decent\n> > job of, though there are some things we may wish to try to push for\n> > regarding the future of the Internet media-type registry system.\n> \n> Saying something and doing it are entirely different things.  The spec\n> does not prevent types from being registered -- it just doesn't require\n> that conforming implementations check the registry upon receipt of a\n> non-standard type.  The same is true in MIME -- they just lie in the RFC.\n\nKindly show me where the MIME RFC requires conforming implementations to\ncheck the registry; I can't seem to find it in my copy.  (Actually, if the\nregistry were more machine-readable, one could imagine a registry that points\nto an implementation of a viewer for that media-type for your platform, and\na UA could automatically fetch it and use it, but that's probably a pipe\ndream.)\n\nI believe your interpretation of these documents is mistaken, and that 1590\nremoves the email-centrisms from media type registration procedures, which\nmakes the draft section:\n\n#   For mail applications, where there is\n#   no type negotiation between sender and receiver, it is reasonable\n#   to put strict limits on the set of allowed content types. With\n#   HTTP, however, user agents can identify acceptable media types as\n#   part of the connection, and thus are allowed more freedom in the\n#   use of non-registered types.\n\nat best redundant and at worst wrong; there are *not* strict limits on the set\nof allowed content-types for mail, either de facto or de jure.\n\nContent-type negotiation can only work if the client and the server both\nuse the same name for the same media-type, any associated parameters, etc.\nEncouraging use of unregistered types (which I believe the document as\nwritten does) merely increases the chance that either different groups will\nemploy different names for the same object or, much worse, use the same\nname for two totally different things.  As anybody who has tried to name\na product or file a trademark knows, there's almost certainly somebody out\nthere using the same name for something different.\n\nThe HTTP spec should, at bare minimum, mention these issues and encourage\nregistration of types that are employed.\n\n- Marc\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "> Um. Please define \"canonical text form\".\n\nI have already done so several times.  The canonical form for text is defined\nin RFC 1521 (which just cites 822, of course) as CRLF delimited for US-ASCII\nor ASCII-like things like 8859-1.  I do not know of a single Internet standard\nprotocol that does not employ this representation; let me know if you know\nof one.\n\nIf you want to say that using this form is unneeded, OK, but please don't\nsay there isn't one.\n\nUnicode, of course, is newer and doesn't have decades of developing canonical\nforms behind it, so things are less clear for such cases.  But we weren't\ntalking about Unicode, though obviously we don't want a solution that could\nscrew things up for it in the future.\n\n> My proposal for dealing with this in HTTP is to have a seperate field\n> for charset negotiation, and to ship Unicode (UTF) (marked up with\n> some languages/presentational tags that are autogenerated) as the\n> \"canonical\" form into which everything can be converted into and from.\n\nSounds interesting as a long-range approach, though I don't think it sounds\nsimple enough to just drop into place.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">> Um. Please define \"canonical text form\".\n>\n>I have already done so several times.  The canonical form for text is defined\n>in RFC 1521 (which just cites 822, of course) as CRLF delimited for US-ASCII\n>or ASCII-like things like 8859-1.  I do not know of a single Internet standard\n>protocol that does not employ this representation; let me know if you know\n>of one.\n>\n\nOur text, which art in transit,\nHallowed be thy representation,\nThy bytes arrive, in order derived,\nrom earth, not the RFC's\n\nYour \"canonical\" text representation does not work for at least half\nthe world's languages, and for more than half of the world's\npopulation. DOn't you think it's somewhat \"inconsiderate\" to force\nthese people to use non-standard kludges because people 10 years ago\nhad no idea at all about language encodings?\n\n\n\n"
        },
        {
            "subject": "Son of MIME (was comments",
            "content": "Chuck Shotton said:\n>Marc VanHeyningen said:\n>>Albert Lunde said:\n>>>Now, it seems like we are saying is that current practice\n>>>(not just \"bad\" servers) is to treat EOL differently in\n>>>the object body for performance reasons.\n>>>\n>>>In this, and in other ways, we are not just quoting the MIME\n>>>spec, we are sort of rewriting it.\n>>\n>>Yes.  Absolutely.\n>\n>No, this is the HTTP spec! As Roy has said earlier, there are lots of\n>things that are MIME-like, and may even be considered MIME by some people,\n>but the HTTP standard doesn't say how to interpret the semantics of this\n>MIME info. Rather, it says how to parse it. Interpretation is the MIME\n>standard's problem. The MIME standard says nothing about the \"proper\"\n>format for GIF files, XBMs, MPEGs, or any of the other content-types\n>(except for text/plain, perhaps) used by HTTP. The HTTP standard doesn't\n>either. So, why are we singling out text in object bodies?\n\nThe MIME standard does refer back a lot to RFC822 for text...\n\nBut the more I read these specs together, the more I seem to be\nconvincing myself that HTTP is not MIME, as the older spec\nI quoted earlier suggests, but \"MIME-like\" as the recent draft says.\n\n(Sort of a \"Son of MIME\" or \"MIME meets Mozilla\" ;)\n\nWe _are_ using a bunch of headers and the MIME/Internet media types\nbut the message structure is rather freely interpreted.\n\nThis leads me to some different thoughts:\n\n- We should omit MIME Version headers, since the body returned\nisn't really MIME-conforming.\n\n- We should put off multipart types to HTTP-NG or some such\nbecause 1) they aren't current practice 2) their syntax is\ncomplex and may be sensitive to what we do with EOL.\n\nWe still need to agree what to advocate for for treatment of EOL\nbut it may be easier to explain if it's not MIME...\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "(Your message reached me with an invalid To: line; something somewhere is\nbrok.)\n\n>>I have already done so several times.  The canonical form for text is defined\n>>in RFC 1521 (which just cites 822, of course) as CRLF delimited for US-ASCII\n>>or ASCII-like things like 8859-1.  I do not know of a single Internet \nstandard\n>>protocol that does not employ this representation; let me know if you know\n>>of one.\n> \n> Your \"canonical\" text representation does not work for at least half\n> the world's languages, and for more than half of the world's\n> population.\n\nOf course; it's a canonical form only for some things, which happen to be\nones that account for the vast majority of current usage (though that will\nprobably not remain the situation forever.)  It's a canonical representation\nfor ASCII and ASCII-like text; it says nothing about how one should represent\nnon-ASCII-like text.  Or GIFs or MPEGs.  Those things can be defined by others.\n\n> DOn't you think it's somewhat \"inconsiderate\" to force\n> these people to use non-standard kludges because people 10 years ago\n> had no idea at all about language encodings?\n\nHTTP is for transferring objects; I think it's way beyond the scope of \nthe HTTP spec to standardize an international character encoding.  HTTP\nshould be designed to that whatever method is desired can be dropped in\nand used.  In the long run maybe one method will be a clear victor.  Or\nmaybe not.  Certainly not today.\n\nCan you be more specific about what you are trying to say?  How exactly\nshould an ideal HTTP server send a document that is in plain, unformatted\nUS-ASCII text?  Using Unicode?\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">Can you be more specific about what you are trying to say?  How exactly\n>should an ideal HTTP server send a document that is in plain, unformatted\n>US-ASCII text?  Using Unicode?\n\nIf you read carefully. I am proposing UTF-7 or UTF-8. UTF-8 is\n\"ASCII compatible\", and I guess one could use CRLF as per MIME safely\nwith either. However, I think that text, in general, should just be\nsent as a stream of bytes (like GIF is), because there are too many\npossible representations in use today, and we should allow people to\ncontinue using  them. CLients should handle line break conversion.\n\nIn my proposal, if the client can accept US-ASCII (very high\nprobability), then the server would send US-ASCII provided the data\nwas US-ASCII, or easily converted into it. If the data is a mixed\nlanguage document containing Arabic, Kanji, Hiragana, English, and\nThai, and the client indicates that it can accept UTF, it would get\nUTF data (display is another matter).\n\nMy biggest problem with the MIME defintion of text/* is that it is\nhorribly skewed toward ASCII, which is *not* a canonical text format,\nbut rather a charset encoding. As such, I think text/* should be\nbasically a binary stream, and if we really want to use the current\nMIME stuff, it should be:\n\n   text/plain; charset=us-ascii\n\nand we can play all the games we like with CRLF within that.\nI would prefer a default charset of UTF-? for MIME, but *that* is not\nlikely to happen :-)\n\n \n\n\n\n"
        },
        {
            "subject": "Reference to 2nd WWW Conference paper on HTTP latenc",
            "content": "At the IETF BOF on Wednesday, I mentioned this paper, and was asked\nto post the URL on the mailing list.  Sorry, this is in HTML with\nGIF inlined images for the graphs; a Postscript version exists but\nI don't have it in a public place yet.\n\nNote that in spite of what it says in various places on the conference's\nWeb pages, Venkat is the primary author, not me.\n\n-Jeff\n\nhttp://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html\n\nImproving HTTP Latency\n\nVenkata N. Padmanabhan (University of California -- Berkeley) \n\nJeffrey C. Mogul (Digital Equipment Corporation Western Research Laboratory)\n\nAbstract\n\nThe HTTP protocol, as currently used in the World Wide Web, uses a\nseparate TCP connection for each file requested. This adds significant\nand unnecessary overhead, especially in the number of network round\ntrips required. We analyze the costs of this approach and propose\nsimple modifications to HTTP that, while interoperating with unmodified\nimplementations, avoid the unnecessary network costs.  We implemented\nour modifications, and our measurements show that they dramatically\nreduce latencies.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Marc VanHeyningen writes:\n\n> Kindly show me where the MIME RFC requires conforming implementations to\n> check the registry; I can't seem to find it in my copy.\n\nRFC 1521, Section 4:\n\n     content  :=   \"Content-Type\"  \":\"  type  \"/\"  subtype  *(\";\"\n     parameter)\n               ; case-insensitive matching of type and subtype\n\n     type :=          \"application\"     / \"audio\"\n               / \"image\"           / \"message\"\n               / \"multipart\"  / \"text\"\n               / \"video\"           / extension-token\n               ; All values case-insensitive\n\n     extension-token :=  x-token / iana-token\n\n     iana-token := <a publicly-defined extension token,\n               registered with IANA, as specified in\n               appendix E>\n\n     x-token := <The two characters \"X-\" or \"x-\" followed, with\n                 no intervening white space, by any token>\n\nA BNF defines how a conforming application should *parse* the input.\nRFC 1521 specifies that the parser should declare a syntax error if the\nMIME type is not registered with IANA.  RFC 1590 only changed the registration\nprocedure -- it did not change the MIME requirements.  Naturally, no mail\napplication I know of is stupid enough to implement this requirement --\nthey generally just check the ~/.mailcap for any token/token types.\n\nHTTP doesn't care what the actual content-type is of the object-body.\nAll it cares about is that the Content-Type header can be parsed, and thus\nit gives the only valid parsing rule possible for HTTP.\n\n> The HTTP spec should, at bare minimum, mention these issues and encourage\n> registration of types that are employed.\n\nIt already mentions these issues and includes explicit reference to how\nmedia types are registered.  I will attempt to elaborate on the finer details,\nbut the decision HAS been made that HTTP is not a MIME-conformant application.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "Thus wrote: \"Roy T. Fielding\"\n>Marc VanHeyningen writes:\n>\n>> Kindly show me where the MIME RFC requires conforming implementations to\n>> check the registry; I can't seem to find it in my copy.\n>\n>A BNF defines how a conforming application should *parse* the input.\n>RFC 1521 specifies that the parser should declare a syntax error if the\n>MIME type is not registered with IANA.\n\nI think I see what you're saying now; I see it as a normal distinction\nbetween spec (which is tight) and practice (where an implementation is\nwelcome, even encouraged, to accept a larger set of potential inputs\nthan the grammar specifies if such can be done cheaply and without\nintroducing ambiguity or other such problems.)\n\nBut, if you really think it belongs in the grammar, OK.  I just\ninstinctively shy away from creating new grammars unless it's\nabsolutely necessary; goodness knows there are enough of them in the\nworld.\n\n>It already mentions these issues and includes explicit reference to how\n>media types are registered.\n\nWell, the issue I take is with the statement that HTTP should be\n\"allowed more freedom in the use of non-registered types.\"  I believe\none way to interpret this is \"use whatever non-registered types you\nwant, as long as it's HTTP and not email that's no problem,\" and I'm\nnot sure that is what was meant.\n\n- Marc\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": ">>It already mentions these issues and includes explicit reference to how\n>>media types are registered.\n>\n>Well, the issue I take is with the statement that HTTP should be\n>\"allowed more freedom in the use of non-registered types.\"  I believe\n>one way to interpret this is \"use whatever non-registered types you\n>want, as long as it's HTTP and not email that's no problem,\" and I'm\n>not sure that is what was meant.\n\nWhat's wrong with interpreting it that way? HTTP doesn't do ANYTHING with\nthe MIME types AT ALL! You know this, I know this, and all the HTTP\nimplementors on the planet know this. The MIME types are strictly a\npass-through from the server's logical file system to the client, for the\nbenefit of the client and associated viewers. Whether the type is\nregistered or not has absolutely no bearing on HTTP, the interpretation,\nsemantics, and operation of the HTTP protocol, or its implementation.\nWhether MIME types are registered or not has nothing to do with the HTTP\nstandard.\n\nIt'd be like demanding that all of the words that appear in text\nobject-bodies be spelled correctly, or that the comments after a status\ncode in the response all be lower case. This is data that has no bearing on\nthe function of the protocol. It is data that is conveyed from point A to\npoint B in a standard slot in the HTTP protocol, just like the data in the\nContent-length field, the  Server field, etc. The syntax matters, but the\nsemantics don't. The client uses the MIME type to figure out how to display\nthe data, and the user on the other end of the pipe has made the join\nbetween the MIME type and the data, not the server or the HTTP protocol.\nSpecific MIME type info has no place in the HTTP standard. The syntax of a\nContent-type field needs to be there, but that's all.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "In the end, I think that we either change the definition of\n\"content-type\" to modify the handling of text/* types (such change\nbeing unlikely) or we register application/plaintext and\napplication/html to be \"just like text/plain and text/html\n(respectively), but \"charset\" defaults to ISO-8859-1 and the EOL\nconvention depends on the character set. For charset=US-ASCII and\nISO-8859-1, the EOL convention is that EOLs consist of either CR, LF,\nor CRLF, but uniformly through the body. We may want to restrict the\nvalue of the \"charset\" parameter to be those things which have\ncomplete registration information, including US-ASCII transliteration\n(<degree>, <a'>), mapping to other charactersets, etc.\n\nThis would allow current servers to continue to send what they are\nsending, except that they'd need to label it differently\n\nThe labelling can be upgraded gracefully (clients can be\nmodified to Accept: application/html & application/text, and then\nservers can send things that way if they're asked for.)\n\nThis also allows application/plaintext; charset=UNICODE, as well as\nallowing Japanese web servers to export JIS codes in a way that\nJIS-accepting browsers could read the data efficiently, and others\ncould still view the pages, albeit after translation (either by client\nor server).\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "At 11:06 PM 12/9/94, Larry Masinter wrote:\n>This would allow current servers to continue to send what they are\n>sending, except that they'd need to label it differently\n>\n>The labelling can be upgraded gracefully (clients can be\n>modified to Accept: application/html & application/text, and then\n>servers can send things that way if they're asked for.)\n\nHold it. MIME types are associated to particular file types by a HUMAN in\nsome sort of configuration file on the server. MIME types are mapped to a\ndisplay mechanism on the client by a HUMAN. HTTP is simply a pipe to convey\nthat mapping from one end of the connection to the other. The server has NO\ndecision in this process. It is simply using definitions that a HUMAN\npre-defined to map some suffix, file type, or creator code into a MIME\ntype. The server has no knowledge of the semantics of the MIME type. It is\nsimply matching a string of characters with no knowledge of what they mean.\n\nEveryone who is advocating the definition of a standard set of MIME types\nfor HTTP to use is missing the mark completely. HTTP doesn't care, doesn't\nneed to know, and doesn't need to manipulate MIME types. It simply needs to\nmatch them up and pass them in association with some data from server to\nclient or vice versa. We are confusing decisions that people make regarding\nassignment of MIME types to data types with a transaction-based protocol\nthat simply forwards data in a structured format from one computer to\nanother.\n\nIf HTTP queries and responses have improper MIME types being conveyed in\nthem, it is because people on both ends of the HTTP connection decided\nconsciously to do it incorrectly by defining improper types in their\nconfiguration files and mapping them to data types for transfer. That\ndoesn't make the protocol itself correct or incorrect. As long as the MIME\ntype matches the SYNTAX in the HTTP standard, why should a client or server\nthat implements the standard care one iota whether it is a registered type\nor not?\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Actual treatment of End-ofLine in different browser",
            "content": "(Note cross-posting between html-wg and http-wg, follow-ups should be\nredirected as you see fit.)\n\nOn both the HTTP and HTML working group lists, there has been some\ndiscussion of what to accept as end-of-line in text object bodies,\nespecially text/html.\n\nI thought it might be a good idea to do some tests with existing browsers,\nso I made some test files and served them up with the NCSA httpd (which\nseems to put the files out on the net with no attempt to change EOLs.)\n\nAmong the browsers I have handy, it looks like there are several different\nways used to \"tolerate\" different EOL representations. (This is just a\nquick eyeball evaluation of the results so I may miss some subtle issues.)\n\nThe test documents served up with NCSA httpd v 1.3 are at\nhttp://nuinfo.nwu.edu/world/testeol/\n\nEach is an html file with several lines of <PRE> text. Each document had\ndifferent End-of-Line strings.\n\nThe list below indicates the appearance of the results in different browsers.\n\nLynx version 2.1\n\nlfnormal\ncrlfnormal\ncrone line\nlf crnormal\ncrsplfextra blank lines at top\n\n(This looks like LF->EOL, CR-> ?? )\n\nMac Mosaic 1.0.3\n\nlfnormal\ncrlfdouble spaced\ncrnormal\nlf crdouble spaced\ncrsplfdouble spaced\n\n(This looks like LF->EOL, CR->EOL )\n\nMac Netscape 0.96\n\nlfnormal\ncrlfnormal\ncrnormal\nlf crdouble spaced\ncrsplfdouble spaced\n\n(This looks like LF->EOL, CR->EOL, CRLF->EOL)\n\nMacWeb 1.00A3\n\nlfnormal\ncrlfnormal\ncrnormal\nlf crdouble spaced\ncrsplfdouble spaced\n\n(This looks like LF->EOL, CR->EOL, CRLF->EOL)\n\nWin Mosaic 2.0 a5\n\nlfnormal\ncrlfnormal\ncrone line\nlf crnormal\ncrsplfnormal\n\n(This looks like LF-> EOL)\n\nWin Netscape 0.9 beta\n\nlfnormal\ncrlfnormal\ncrnormal\nlf crdouble spaced\ncrsplfdouble spaced\n\n(This looks like LF->EOL, CR->EOL, CRLF->EOL)\n\nCERN WWW linemode client ver 1.3c (WWWLib 1.1)\n\nlfnormal\ncrlfnormal\ncrone line\nlf crnormal\ncrsplfnormal\n\n(This looks like LF-> EOL)\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "HTTP Caching Model",
            "content": "Consider this scenario:\n\nOn a server S, a document D is available as plain text, HTML, or\npostscript.\n\nClient C1 is configured to only accept HTML. This client requests\ndocument http://S/D via proxy P as:\n\nGET http://S/D HTTP/1.0\nAccept: text/html\n\nProxy P connects to S, requests the document, caches it, and returns\nit to C1.\n\nClient C2 is configured to accept postscript and HTML, and to prefer\npostscript over HTML. It requests the same document through the same\nproxy:\n\nGET http://S/D HTTP/1.0\nAccept: text/html; q=0.5\nAccept: application/postscript; q=1.0\n\nProxy P receives the request, and notices that it has http://S/D in\nits cache, so it returns the cached copy.\n\nNote that had C2 requested the document straight from S, it would\nhave got postscript. But it got HTML from the proxy.\n\nTo me, this looks like the caching performed by P is not transparent,\nand hence violates the protocol.\n\nOK, ok, so currently nobody uses format negociation, and certainly\nnobody implements the q and c parameters on accept headers (except\nprobably the CERN linemode browser and server).\n\nBut some information providers are using, of all things, the\nUser-Agent field to customize their documents: they server up\ndifferent stuff for MacMosaic, WinMosaic, Netscape, etc.\n\nCertainly broken proxy caching is observable in these circumstances.\n(but in this case, I'd say the fault is at the informatino provider\nfor abusing User-Agent this way, not at the caching proxy.)\n\nOne way to correct the behaviour of proxy P above is to base the cache\non not tjust the URL in question, but also include all the request\nheaders in the cache key.\n\nBut clearly this is way too conservative.\n\nIt seems to me that the HTTP protocol spec should specify which\nrequest headers can affect the returned data, and which are just\n\"advisory.\" A correct cache would key on the URL plus all the request\nheaders which are allowed to affect the returned data.\n\nFor example, authentication headers shouldn't affect the returned\ndata.  User-Agent shouldn't affect the retuned data. (The fact that it\ndoes is a wart that we'll have to deal with somehow.)\n\nIt means that introducing new headers that can affect the returned\ndata (like the recently proposed Accept-Charset: header) can't be done\nwith correct backwards compatibility. It might be wise to say that all\nheaders matching Accept-*: are allowed to affect the returned data.\n\n\nAlso... I haven't carefully reviewed the latest HTTP/1.0 spec: does it\ninclude some specification of what is going on when a client requests\nftp://host/path or gopher://host/path via an HTTP proxy? Does it\ndiscuss correct vs. heuristic caching in these cases?\n\nFood for thought...\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "+--- On Mon, 12 Dec 1994, Daniel W. Connolly wrote:\n| [...] some information providers are using, of all things, the\n| User-Agent field to customize their documents: they server up\n| different stuff for MacMosaic, WinMosaic, Netscape, etc.\n[...]\n| User-Agent shouldn't affect the retuned data. (The fact that it\n| does is a wart that we'll have to deal with somehow.)\n| \n| It means that introducing new headers that can affect the returned\n| data (like the recently proposed Accept-Charset: header) can't be done\n| with correct backwards compatibility. It might be wise to say that all\n| headers matching Accept-*: are allowed to affect the returned data.\n+---\n\naargh.  I think you're placing undue blame on information providers for \nthis problem.  I agree that content should not be affected by the \nUser-Agent header; and that it should instead be affected by the Accept \nheader.\n\nAcknowledging that I am rather new to this working group and that I may \nbe missing some history that would make this more clear, I consider the \nfollowing line from the HTTP-1.0 draft to be at fault:\n\n-----included text follows-----\n\n5.5.8 ACCEPT\n\n[...] In order to save time, and also allow clients to receive content \ntypes of which they may not be aware, an asterisk \"*\" may be used in \nplace of either the type token and/or the subtype token. [...]\n\n-----included text ends-----\n\nFor an information provider, an Accept header of \"*/*\" is useless.  Other \nthan to save time (network time? implementation time?), I don't see why \nthis sentence couldn't be amended to restrict wildcarding to the subtype \ntoken only.  With such a restriction in place, a provider could search \nfor \"image/\" within the Accept header, and modify content based on its \npresence or absence; and a client sending \"image/*\" could still receive \nimage subtypes of which it may not be aware.\n\nPart of the problem also lies with the ALT value limitations of the <img> \ntag, which I understand are addressed by <fig>.  Since Doug Stevenson's \nmonthly browser survey indicates that plenty of people are using \ntext-only browsers, and since ALT and Accept are currently inadequate \nsolutions, what is there other than User-Agent?\n\nI agree with Dan that a specification for which request headers may affect \nreturned content would be helpful, and that the Accept-* headers are the \nobvious candidate.  I also think that this information belongs in the CGI \nspec, which information providers are more likely to consult.  (Is the CGI\nspecification handled by this working group, or some other?  Or none?)\n\n</marc>\n\n\n\n"
        },
        {
            "subject": "Formalizing Expires, LastModified, &quot;data object&quot",
            "content": "The current HTTP spec[1] is a tremendous improvement over previous\nversions. Great work!\n\nBut... :-)\n\nIt's unnecessarily informal in its use of the term \"data object\":\n\n   Full-Request and Full-Response use the generic message format of \n   RFC 822 [6] for transferring data objects. \n(replace \"data objects\" by \"body parts.\")\n\n   The data object (if any) sent with an HTTP/1.0 request or response \n   is in a format and encoding defined by the Object-Header fields, \n   the default being of type \"plain/text\" with \"binary\" encoding.\n(replace \"data object\" by \"body\")\n\n   The Last-Modified field indicates the date and time of when the \n   data object was last modified.\n(what? now a data object is modifiable? i.e. it has state?)\n\nThe term has no definition in the \"terminology\" section, nor anywhere\nelse that I can find.\n\nThe document correctly borrows the term \"body\" from the MIME spec. In\nthe MIME spec, a Body Part is a bunch of headers and a body. A body is\na sequence of octets. A message is a special kind of body part that\nhas all the right headers, like From:, To:, and Message-Id:. An HTTP\nmessage is a MIME body part, but it doesn't fit the definition of a\nMIME message. But all that makes sense. An HTTP message is different\nfrom a mail message, but an HTTP body is the same as a mail body, no?\n\nI suggest the term \"data object\" be replaced by \"entity,\" or \"body\" as\nappropriate. The MIME spec uses the term \"entity\" synonymously with\nbody part. I like entity better than body part. It's shorter, less\nconfusing with body, and it's conveniently analagous to the term\n\"entity\" in the SGML/HyTime specs.\n\n\nThen there's this stuff about Last-Modified, Expires, and \"data\nobjects\" that change over time.\n\nI suggest that the protocol be defined by the observable behaviour of\ncorrect servers (and clients). For example, the critical property of\nthe Last-Modified and Expires information for a URI is that the server\nstipulates that it will serve the same entity for that URI (given the\nsame Accept: headers) at all times between the Last-Modified time and\nthe Expires time.\n\nFor a formal treatment of HTTP expires, last-modified, and proxy\ncaching, please see:\n\nhttp://www.hal.com/%7Econnolly/drafts/formalism.html\n\n\nDan\n\n[1] HTTP Working Group, INTERNET-DRAFT, <draft-ietf-http-spec-00.txt>,\nExpires May 23, 1995\nhttp://info.cern.ch/hypertext/WWW/Protocols/HTTP1.0/HTTPDraft.html\n\n\n\n"
        },
        {
            "subject": "Re: Comments on the HTTP/1.0 draft",
            "content": "You know, Internet protocols don't define \"conforming implementations\"\nbut rather \"conforming behavior on the net\"; you might want to define\na \"conforming implementation\" as \"an implementation that never\nexhibits non-conforming behavior\", but of course, that seems to be a\nsite/configuration issue as much as anything.\n\nThe conformance requirement on data streams is that senders only send\nconforming data streams to recievers (unless somehow the protocol\nallows for the recievers to indicate that they're willing ot accept\nnon-conforming data streams.)\n\nWe should talk about senders and recievers rather than servers and\nclients, because when clients send data to servers, the roles are\nreversed and the conformance requirements reverse.\n\nNo sender should send an unregistered type to a reciever that hasn't\nindicated a willingness to accept it. The 'practice' that allows HTTP\nto be somehow more free with MIME types is that the protocol allows\nfor negotiation.\n\nThat \"text/html\" wasn't registered in the official MIME registry is\nsomewhat of a red herring: it was certainly registered in the web\ndocuments. \n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "At 11:37 AM 12/12/94, Marc H. wrote:\n>+--- On Mon, 12 Dec 1994, Daniel W. Connolly wrote:\n>| [...] some information providers are using, of all things, the\n>| User-Agent field to customize their documents: they server up\n>| different stuff for MacMosaic, WinMosaic, Netscape, etc.\n>[...]\n>| User-Agent shouldn't affect the retuned data. (The fact that it\n>| does is a wart that we'll have to deal with somehow.)\n>|\n>| It means that introducing new headers that can affect the returned\n>| data (like the recently proposed Accept-Charset: header) can't be done\n>| with correct backwards compatibility. It might be wise to say that all\n>| headers matching Accept-*: are allowed to affect the returned data.\n>+---\n\nThis doesnt surprise me at all, I've either used, or considered using this\nfield in the following ways.\n\nAt times there have been serious lags between browsers, so for example it\nwas neccessary to advise users of EINet's browser to upgrade to a browser\nthat supported authorisation, during the considerable period when their's\ndid not.\n\nWindows Mosaic has a serious bug related to caching images by relative URLs\n- same advise had to be given,\n\nSomething out there has a serious bug with truncating the last (or is it\nthe first) field of forms. If I'd ever tracked down which, then I'd have\nwritten code to put a dummy hidden field at the beginning and end of forms.\n\nThere were a number of cases during the development of the Techweb site\nwhere the best HTML for some browsers was dramatically different from that\nfor others - for example some browsers handle <br> correctly, while others\ntreat it as <p>\nideally I'd have presented this file in two different ways.\n\nNow, we have the situation where Netscape has a lot of really usefull\nfeatures, which the SGML-Purists dont want to put in HTML because they\nallow people to specify presentation (shock, horror). I can see lots of\ncases where the best presentation on a good browser is going to be\ndifferent that that for a lousy browser.\n\nOf course .... if none of the browsers had bugs, and all did a good job of\npresentation, then designers wouldnt need to server up multiple versions of\nfiles.\n\n- Mitra (raising flame-guard).\n\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <ab139bf30702100468c7@[192.190.111.98]>, Mitra writes:\n>At 11:37 AM 12/12/94, Marc H. wrote:\n>>+--- On Mon, 12 Dec 1994, Daniel W. Connolly wrote:\n>>[...]\n>>| User-Agent shouldn't affect the retuned data. (The fact that it\n>>| does is a wart that we'll have to deal with somehow.)\n>>|\n>>| It means that introducing new headers that can affect the returned\n>>| data (like the recently proposed Accept-Charset: header) can't be done\n>>| with correct backwards compatibility. It might be wise to say that all\n>>| headers matching Accept-*: are allowed to affect the returned data.\n>>+---\n>\n>This doesnt surprise me at all, I've either used, or considered using this\n>field in the following ways.\n>\n[creative hacks deleted...]\n>\n\nI'm not sure what you're suggesting here.\n\nI can see that real world nasty problems require real world nasty\nsolutions.\n\nBut as far as a spec, shouldn't we use the categorical imperative?\ni.e. what if everybody did that?\n\nIf everybody customized their documents on a per-user-agent basis, and\ncaching proxies don't take the User-Agent: header into account in\ntheir cache keys, then things will be broken.\n\nThe question is: where do we assign the fault?\n\nIf we say that the proxy was broken for not using User-Agent as a\ncache key, then we're saying that cached objects can never be shared\naccross clients with different user agents. I don't think we want\nthat.\n\nSo I'm suggesting that serving up different documents for different\nUser-Agents should be a protocol violation.\n\n>Of course .... if none of the browsers had bugs, and all did a good job of\n>presentation, then designers wouldnt need to server up multiple versions of\n>files.\n\nA spec tells you what happens when everybody plays by the rules. When\nthere are bugs, all bets are off, and you do what you must.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "On Tue, 13 Dec 1994, Daniel W. Connolly wrote, quoting Mitra:\n> >This doesnt surprise me at all, I've either used, or considered using this\n> >field in the following ways.\n\nI've had to do this too.  \"In an ideal world\" blah blah blah.... what \nmatters is that when people say \"you'll pry MacMosaic 1.0.3 from my cold \ndead fingers\", well, you have to shoot them.\n\n> I'm not sure what you're suggesting here.\n> \n> I can see that real world nasty problems require real world nasty\n> solutions.\n> \n> But as far as a spec, shouldn't we use the categorical imperative?\n> i.e. what if everybody did that?\n\nMitra and I use it to atone for bugs, er, \"incomplete implementations\".  \nOnce we have HTML and HTTP standards, and a \"www-user-agent\" standard, \nand people *stuck* to them when calling their browsers \"WWW Browser Level \nN\", *and* we have servers that do format and protocol negotiation \ncorrectly, we won't need to look at the User-Agent.  That's a hell of a \nlong list, one we're all striving to achieve but won't get there for awhile.\nIf use of User-Agent is mentioned in any spec, let it be allowed for use \nin accounting for incomplete implementations - serving out semantically \ndifferent documents should be the discouraged action.\n\n> If everybody customized their documents on a per-user-agent basis, and\n> caching proxies don't take the User-Agent: header into account in\n> their cache keys, then things will be broken.\n>\n> The question is: where do we assign the fault?\n\nProxies *must* be nearly invisible.  A proxy accessing a server must look\njust like a client accessing a server.  A client accessing a proxy must\nlook just like a client accessing a server.  If this is not a design goal\nof proxies, they will never become as ubiquitous as they need to be to\nallow the net to scale correctly. \n\nObviously there needs to be some way for the server to communicate bck to \nthe proxy \"this response is particular for *this* and *this* and *this* \nparameter, but this response is constant for all other parameters.\" How \nthe server determines this from the way the documents are created is up \nto the server and server maintainer, not an issue for HTTP.  A  \nheader could be returned in the response:\n\nCache-Key: User-Agent = WinMosaic\n\nwhich contains qualifications on various headers for caching.\n\n> >Of course .... if none of the browsers had bugs, and all did a good job of\n> >presentation, then designers wouldnt need to server up multiple versions of\n> >files.\n>\n> A spec tells you what happens when everybody plays by the rules. When\n> there are bugs, all bets are off, and you do what you must.\n\nThe suggestion above could be used for more than just User-Agent.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "You point out a problem with a naive approach to proxy (relay) caching:\n\n>On a server S, a document D is available as plain text, HTML, or\n>postscript.  Client C1 is configured to only accept HTML. This client\n>requests document http://S/D via proxy P as:\n>GET http://S/D HTTP/1.0\n>Accept: text/html\n>Proxy P connects to S, requests the document, caches it, and returns\n>it to C1.\n>\n>Client C2 is configured to accept postscript and HTML, and to prefer\n>postscript over HTML. It requests the same document through the same\n>proxy:\n>GET http://S/D HTTP/1.0\n>Accept: text/html; q=0.5\n>Accept: application/postscript; q=1.0\n>Proxy P receives the request, and notices that it has http://S/D in its\n>cache, so it returns the cached copy.  Note that had C2 requested the\n>document straight from S, it would have got postscript. But it got HTML\n>from the proxy.\n>\n>To me, this looks like the caching performed by P is not transparent,\n>and hence violates the protocol.\n\nThe problem here is that the \"cache tag\" kept by P is insufficient.\nYou can't just use the URL.  But if the \"tag\" includes an encoding\nof the Accept: headers, then P's caching could be \"correct\" in the\nsense that it never returns the wrong format of the document.  (We\nstill have to worry about returning the wrong version, but cache\nconsistency protocols are another can of worms entirely.)\n\nConsider what would happen in this case if P entered the document\nin its cache with this tag, constructed from C1's request:\nURL: http://S/D\nAccepts: text/html\nCached-format: text/html\nThen when C2 makes its GET request, P can see that the cached document\nmight not be sufficient, because P never asked S for a postscript\nfile, and C2 prefers postscript to text.\n\nNow, consider the scenario when the requests are done in the opposite\norder, and with S only holding the HTML form of the document.  C2 makes\nits request, S responds with html, and P caches the document with this tag:\nURL: http://S/D\nAccept: text/html; q=0.5\nAccept: application/postscript; q=1.0\nCached-format: text/html\nNow, if the next request is either from C1 with \"Accept: text/html\"\nor C2, P can tell from this tag that it already has what the client\nwants (provided that S has not subsequently added a postscript file\nto its repertoire; this is the cache-consistency problem again).  I.e.,\nP can see that because the \"strictest\" request asked for postscript\nin preference to HTML, and no postscript is cached, then it must not\nbe available.  P's behavior is entirely correct.\n\nFinally, consider the scenario when S has both HTML and Postscript,\nand C1 and C2 have both made requests.  P will end up with both forms\nin its cache, with this tag:\nURL: http://S/D\nAccept: text/html; q=0.5\nAccept: application/postscript; q=1.0\nCached-format: text/html\nCached-format: application/postscript\nrom this point, P can respond to requests for either Postscript or\nHTML, since it has them both cached.\n\nNow consider what happens when P has to remove some items from its\ncache, due to lack of space.  Suppose it removes the Postscript version.\nIt then has to change its tag either to say:\nURL: http://S/D\nAccept: text/html; q=0.5\nAccept: application/postscript; q=1.0\nCached-format: text/html\nPreviously-Cached-format: application/postscript\nor to say\nURL: http://S/D\nAccept: text/html; q=0.5\nCached-format: text/html\nThat is, either it must remember that it had a postscript version\ncached at one point, or it must forget that some client had asked it\nfor postscript.  I would suggest doing the former, because it preserves\nmore information.  For example, if S does not have the postscript file,\nC2 issues its usual request and P uses the former model, then it knows\nif postscript is available from S or not.  But if P uses the latter\nmodel, then it must re-request the postscript file from S, even if at\none point it know that S didn't have the postscript.\n\nAssuming that I haven't made some fatal logical error in this analysis,\nit should be possible for a caching proxy to\n(1) provide correct (transparent) behavior\n(2) do the minimal number of server accesses\nwithout any HTTP protocol changes, simply by recording as much information\nas possible about the requests that caused cache entries to be created.\n\nAlways ignoring the consistency problem, of course.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "At 12:32 PM 12/13/94, Daniel W. Connolly wrote:\n>I'm not sure what you're suggesting here.\n\nWhat I'm suggesting is that competent production people will work with\nwhatever the real world users have their.  Incompetent production people\nwill assume a perfect world where everyone has infinite bandwidth, and\nbug-free clients.\n\nWhat I'm suggesting is that the statement\n\n>>>| User-Agent shouldn't affect the retuned data. (The fact that it\n>>>| does is a wart that we'll have to deal with somehow.)\n\nis unreasonable given the lack of a perfect world.\n\nNow - if we had URCs and version control .... then clients could return\nexplicitly different URLs for different browsers, and have the caches do\nthe right thing, but I'm probably dreaming.\n\n>I can see that real world nasty problems require real world nasty\n>solutions.\n>But as far as a spec, shouldn't we use the categorical imperative?\n>i.e. what if everybody did that?\n\nIf everyone did that, then more documents would display reasonably on more\nbrowsers, but caching would be much harder.  I'll gamble on upgrading poor\ncaches more than I'll gamble on upgrading poor clients, because there are a\ncouple of orders of magnitude fewer.\n\n>If everybody customized their documents on a per-user-agent basis, and\n>caching proxies don't take the User-Agent: header into account in\n>their cache keys, then things will be broken.\n>The question is: where do we assign the fault?\n\nAssigning the fault doesnt help - once the bad clients are out there, it\ndoes a production group no good at all if they assume they dont exist.\n>\n>If we say that the proxy was broken for not using User-Agent as a\n>cache key, then we're saying that cached objects can never be shared\n>accross clients with different user agents. I don't think we want\n>that.\n\nJust an idea ..... Allow for a User-Agent field in the mime header of the\nreturned document. If a server wants to recognise the User-Agent field,\nthen it should return a User-Agent field to say this document is only valid\nfor that User-Agent.\n\n>So I'm suggesting that serving up different documents for different\n>User-Agents should be a protocol violation.\n>>Of course .... if none of the browsers had bugs, and all did a good job of\n>>presentation, then designers wouldnt need to server up multiple versions of\n>>files.\n>A spec tells you what happens when everybody plays by the rules. When\n>there are bugs, all bets are off, and you do what you must.\n\nAnd I'm saying that when the clients dont play by the rules, then the\nservers cant. So this particular rule should be modified.\n\n- Mitra\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "Dan pointed out a general problem with the current content-negotiation\nscheme within the presence of a caching proxy.  Paraphrasing, the problem\nis that a proxy cannot mirror the behavior of the origin server without\nknowing the same content availability information as the origin server. \nIn other words, the caching proxy needs to know what variants are available\nin order to determine what the origin server's response would be, and thus\nwhether that response has already been cached.\n\nAlthough it is possible to track cached responses via accept headers and\nuser-agents (as Dan and Jeff mentioned), that solution will not scale\nbecause there is a much higher variability in Accept* and User-Agent\nrequest headers than there is in URIs with negotiable content.\n\nSince, in reality, only a small portion of URI space is capable of\ngenerating variants, I think a more general solution would be to have\nthe server explicitly declare what variants are available (if any)\nas part of the response metainformation.  The proxy could then use\nthat information as the guide for determining the correct response.\nIn fact, separating the metainformation from the message entity will\nbe necessary in any case in order to cache multiple entities per URI.\n\nPeople who follow the URC discussions will probably notice that, in\nsending the information on variants, the server is essentially saying\nwhat subset of the URC for that resource is available from that server.\nIn fact, I am sure that some would argue that, when variants are present\nfor a specific URI, the server should just respond with a URC and allow\nclient redirection to find the specific non-variant URL desired.\n\nSo, the way I see it, we have several choices:\n\n  1) Keep the current model and just say it doesn't work with proxies\n\n  2) Patch the current model by adding a \"Variants:\" header like\n\n        Variants: text/html;qs=1;bs=9653;ua=\"Mozzilla\",\n                  text/html;qs=0.9;bs=8753,\n                  text/plain;qs=0.2;bs=7989,\n                  text/plain;qs=0.3;bs=8989;lang=\"en/gb\",\n                  application/postscript;qs=0.7;bs=90267\n\n     which would ONLY be sent when the given URI allows variants;\n\n  3) Implement a standard scheme for client-based redirection upon receipt\n     of variant metainformation (i.e. a URC).\n\n\nIf the answer is (1), then I think all mention of content-negotiation\nand the Accept* headers should be removed from HTTP/1.0.\n\n(2) is easy to implement (because only proxies and servers sending variants\nwould need to change), but it does not seem to be a good long-term solution.\n\n(3) is, surprisingly enough, relatively easy to implement -- all that would\nbe needed is a new 3xx response code and at least one format for enclosing\nURCs in the message body.  There are two that I can think of:\n\n      Content-type: text/iafa-template\n      Content-type: text/prdm\n\nThe latter is for the Partial Redundant Distributed Metalanguage\ndescribed at <http://www-pcd.stanford.edu/FRESCO/annotations.html>,\nthough it is used in their implementation for a different purpose.\nThis is a much better long-term solution (because it also enables\nURNs), but I think I'd have a hard time calling it HTTP/1.0.\n\nWhat do you think?\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "On Tue, 13 Dec 1994, Mitra wrote:\n\n> At 11:37 AM 12/12/94, Marc H. wrote:\n> >+--- On Mon, 12 Dec 1994, Daniel W. Connolly wrote:\n> >| [...] some information providers are using, of all things, the\n> >| User-Agent field to customize their documents: they server up\n> >| different stuff for MacMosaic, WinMosaic, Netscape, etc.\n> >[...]\n ...\n> This doesnt surprise me at all, I've either used, or considered using this\n> field in the following ways.\n \n ... \n> Of course .... if none of the browsers had bugs, and all did a good job of\n> presentation, then designers wouldnt need to server up multiple versions of\n> files.\n \nIn my experience, the glitches and bugs abound with varying degrees\nof impact when it comes to handling FORMs.  I considered asking the user\nwhich browser they were using and tailoring that way before I discovered\nthe User Agent field.  My client intends to offer my workproduct to their\ncustomers as a WWW interface to my clients data base application.  We\nintend group browsers based on the subset of FORMs support which works\ncorrectly and loudly tell users when their browser is less then\nsatisfactory.  Hope to offer a test panel users can select to evaluate\nunknown versions. Capture feedback and allow the admins to update their\nserver valid browser tables.  My problems are not related to netscape\nenhancements or lack there of.\n\nI am collecting a list of defects and then plan to attempt a very \nagressive stanse with all the publishers whose variations I have tested.\nI've tried reporting problems and get silence or automatic mail.  In any\ncase no sense that my issues will be resolved.  Perhaps having products\nsuch as mine blacklist user agents will improve the responsiveness\nof the authors.  Be happy to compare notes on problems with anyone\nbut I think I've said too much here.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "Haven't you left out the possibility that the proxy<->server protocol\nmight be differentt than the client<->server or client<->proxy\nprotocol? In particular, servers that support variants need not report\nthe variants back if they're being queried by an end-user agent, but\nmight if they're being queried by a caching server. After all, we did\nthis for if-modified-since...\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "Larry writes:\n\n> Haven't you left out the possibility that the proxy<->server protocol\n> might be differentt than the client<->server or client<->proxy\n> protocol? In particular, servers that support variants need not report\n> the variants back if they're being queried by an end-user agent, but\n> might if they're being queried by a caching server.\n\nWhat matters is whether or not the client (user agent or proxy) has\na shared cache.  There is no way for the server to know that, though it\ncould be added to the protocol.  However, I think information on variants\nmay be as useful to a user-agent as it is to a cache manager.\n\n> After all, we did this for if-modified-since...\n\nNo, we didn't.  If-modified-since is also used by user agents (like Netscape).\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "Netscape doesn't have a 'shared cache' in the sense that user agents\nwith different capabilities share the same cache.\n\nYes, you're right, it is more precise to suggest that the behavior of\na HTTP agent that caches documents for other HTTP agents should ask\nfor more complete information than a HTTP agent that is only asking\nfor itself.\n\n\n\n"
        },
        {
            "subject": "One paragraph summary of the HTTPBO",
            "content": "One paragraph summary of the HTTP-BOF, held Wednesday 7-Dec-94\nat the 31st IETF meeting, San Jose. Chaired by Dave Raggett\n<dsr@w3.org> +44 272 228046\n\nI tried to send this out from the IETF meeting, but somehow\nit screwed up ...\n\nThe full minutes will follow soon.\n\nThe HTTP-BOF was held to assess the need to set up a working\ngroup for the World Wide Web hypertext transfer protocol.\nThe suggested goals are to document current practise for\nHTTP 1.0 as a standards track RFC; to elicit needs for\nextensions to HTTP and to develop standards for these.\nRoy Fielding and Henrik Frystyk Nielsen reviewed the Internet\nDraft for HTTP 1.0 and suggested extensions for 1.1. This was\nfollowed by vigorous discussions on ideas for keeping connections\nopen and related ideas. Recent work on HTTPng was reported\nby Simon Spero. This work adds a session layer to tcp and uses\nASN.1 and PER for message encoding. Under congested conditions\nSimon reported dramatic improvements in performance. The BOF\nended by recommending setting up a working group.\n--\n Dave Raggett <dsr@w3.org> tel: +44 272 228046 fax: +44 272 228003\n  Hewlett Packard Laboratories, Filton Road, Bristol BS12 6QZ, United Kingdom\n\n\n\n"
        },
        {
            "subject": "Bugs in Forms (Was Re: HTTP Caching Model?",
            "content": "> I am collecting a list of defects and then plan to attempt a very \n> agressive stanse with all the publishers whose variations I have tested.\n> I've tried reporting problems and get silence or automatic mail.  In any\n> case no sense that my issues will be resolved.  Perhaps having products\n> such as mine blacklist user agents will improve the responsiveness\n> of the authors.  Be happy to compare notes on problems with anyone\n> but I think I've said too much here.\n\nI've already built a form testing suite.  Most browsers have been\ntested against it.  Have a look at\nhttp://www.research.digital.com/nsl/formtest/home.html\n\nI'm very interested in hearing about more form bugs to add to the suite.\n\nGlenn Trewitt\nNetwork Systems Laboratory\nDigital Equipment Corporation\nPalo Alto, California\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <9412132125.aa15482@paris.ics.uci.edu>, \"Roy T. Fielding\" writes:\n>\n>Since, in reality, only a small portion of URI space is capable of\n>generating variants, I think a more general solution would be to have\n>the server explicitly declare what variants are available (if any)\n>as part of the response metainformation.\n\nSo back to my question of \"where do we assign the fault?\" you're\nsaying we should assign it to the origin server for not telling the\nproxy that there were variants.\n\nIt seems like Mitra and Brian would agree.\n\nSeems like a workable strategy, now that I think about it:\n\nAnd I seem to recall that Tony Sanders gave an explanation* of the\nURI: (aka Location:) header a while back that would solve this problem.\n\n* http://www.research.digital.com/nsl/formtest/home.html\n\n>  2) Patch the current model by adding a \"Variants:\" header like\n>\n>        Variants: text/html;qs=1;bs=9653;ua=\"Mozzilla\",\n>                  text/html;qs=0.9;bs=8753,\n>                  text/plain;qs=0.2;bs=7989,\n>                  text/plain;qs=0.3;bs=8989;lang=\"en/gb\",\n>                  application/postscript;qs=0.7;bs=90267\n>\n>     which would ONLY be sent when the given URI allows variants;\n\n\nThis is kinda humorous... the answer has been in the spec all along:\n\n=======================\n7.9 URI Header\n\nThe URI-header field contains a URI by which the object may be\nfound. It should not be confused with the token in the Request-Line\ndescribed in Section 5.4. As for a normal request, there is no\nguarantee that the object can be retrieved using the URI\nspecified. The field is normally a part of a response having\nStatus-Code \"301 Moved Permanently\" or \"302 Moved Temporarily\".\n\nURI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\nvary    =        \"vary\" \"=\" <\"> 1#vary-param <\">\nvary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n        /       extension-vary\nextension-vary  =       token\n=======================\n\n\nWhen a server returns an entity in response to a GET query on a URI,\nit can give a URI header, which specifies the set of entities that\nrepresent the accessed resource (at that time, or between the\nLast-Modified and the Expires times).\n\nWe can describe the current practice by saying that when no URI:\nheader is given in response to GET xxx, the client should assume:\n\nURI: xxx\n\nThat is, the resource indicated by xxx has exactly one representation\n(at that time).\n\nIf a URI yyy has representations that vary based on preferred content\ntype, language, and user agent, the server should return:\n\nURI: yyy; vary=\"type,language,user-agent\"\n\nThe vary parameter tells a caching proxy exactly what headers it must\nkeep in its \"cache-key.\"\n\nThis meshes happily with current practice, to a large extent.\n\nThe only changes requred are that\n\n* when a server returns a representation of a resource, if\nit has other representations of that resource, it _must_ give\nan appropriate URI: header.\n\n* caching proxies _must_ take URI: headers into account when\nlooking for cache hits. (The quick and dirty implementation is\nto not cache any responses with URI: headers).\n\n\nHmmm... as I recall, TimBL or Tony S once explained that ideally, if\na server has a document in text, html, and postscript in english, french,\nand german, and it's returning the french postscript, it should return:\n\n200 Document follows\nDate: Wed Dec 14 15:34:19 CST 1994\nLast-Modified: Wed Dec 14 10:34:19 CST 1994\nURI: http://this.host/path/to/multi;\nvary=\"language,type\"\nURI: http://this.host/path/to/multi.ps;\nvary=\"language\"\nURI: http://this.host/path/to/multi-french;\nvary=\"type\"\nURI: http://this.host/path/to/multi-french.ps\nContent-Type: application/postscript\n\n%!PS-Adobe 2.0\n...\n\nSo the client user can make a link to:\n\n* the multiformat document\n* the postscript represenation (in any language)\n* the french representation (in any format)\n* the postscript, french version\n\nAnd the caching proxy would create 4 cache entries.\n\nThis is the ideal case. At a minimum, the server must return:\n\n200 Document follows\nURI: http://this.host/path/to/multi;\nvary=\"language,type\"\nContent-Type: application/postscript\n\n%!PS-Adobe 2.0\n...\n\nAnd the caching proxy could just not cache that response.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> 7.9 URI Header\n> \n> The URI-header field contains a URI by which the object may be\n> found. It should not be confused with the token in the Request-Line\n> described in Section 5.4. As for a normal request, there is no\n> guarantee that the object can be retrieved using the URI\n> specified. The field is normally a part of a response having\n> Status-Code \"301 Moved Permanently\" or \"302 Moved Temporarily\".\n> \n> URI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\n> vary    =        \"vary\" \"=\" <\"> 1#vary-param <\">\n> vary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n>         /       extension-vary\n> extension-vary  =       token\n\nA very good proposal -- definitely the best so far.  Thanks for\nbringing it to the daylight, Dan.  I knew it was there, but couldn't\nfind it anymore.  Somebody set this in stone and I for one wouldn't\nhave problems following it; the current situation is threateningly\nfragile.\n\nCheers,\n--\nAri LuotonenNetscape's Mr October 1995\nNetscape Communications Corp.http://home.mcom.com/people/ari/\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <9412142143.AA06326@ulua.hal.com>, \"Daniel W. Connolly\" writes:\n>\n>And I seem to recall that Tony Sanders gave an explanation* of the\n>URI: (aka Location:) header a while back that would solve this problem.\n>\n>* http://www.research.digital.com/nsl/formtest/home.html\n\nOops! I pasted the wrong URL! That should be:\n\nhttp://gummo.stanford.edu/html/hypermail/www-talk-1994q1.messages/955.html\n\nAlso, I've updated my caching formal model draft to put the onus\non original servers to advertise variants.\n\nPlease see \"An Example Scenario\" in:\n\nhttp://www.hal.com/%7Econnolly/drafts/formalism.html\n$Id: formalism.html,v 1.5 1994/12/15 01:34:39 connolly Exp $\n\nI still need to write up an example of the case where there are\nvariant representations and the proxy server is smart enough to be\nable to cache them...\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "I wrote:\n>>  2) Patch the current model by adding a \"Variants:\" header like\n>>\n>>        Variants: text/html;qs=1;bs=9653;ua=\"Mozzilla\",\n>>                  text/html;qs=0.9;bs=8753,\n>>                  text/plain;qs=0.2;bs=7989,\n>>                  text/plain;qs=0.3;bs=8989;lang=\"en/gb\",\n>>                  application/postscript;qs=0.7;bs=90267\n>>\n>>     which would ONLY be sent when the given URI allows variants;\n\nand Dan replied:\n\n> This is kinda humorous... the answer has been in the spec all along:\n> ...\n> \n> URI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\n> vary            =       \"vary\" \"=\" <\"> 1#vary-param <\">\n> vary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n>                 /       extension-vary\n> extension-vary  =       token\n> \n\nNo, that isn't enough information to allow a proxy to decide\nwhat is available.  For instance, let's say we have the same document\nin four natural languages: English (en), French (fr), Spanish (es),\nand Maori (mi).  Under the current scheme, the proxy would make its\nfirst request for Jacque (Accept-Language: fr) and get:\n\n    Content-Language: fr\n    URI: <http://www.roy.com/Maori/museum>;vary=\"language\"\n\n    Bonjour -- uma rapate, uma rapate, uma uma uma\n\nso the proxy could be smart enough to keep track that this is the\nFrench version.  Okay, so Charles requests the same document\nw/(Accept-Language: en).  Knowing that it only has a French version,\nthe proxy can pass the request on and get:\n\n    Content-Language: en\n    URI: <http://www.roy.com/Maori/museum>;vary=\"language\"\n\n    G'day mate, welcome to my museum of New Zealand native culture.\n\nNo problem so far -- we now have both the French and English versions\ncached.  Unfortunately (in more ways than one ;-), our next request comes\nfrom Hone Heke w/(Accept-Language: mi;q=1, en;q=0.2).  He wants to see the\nversion in Maori:\n\n    Content-Language: mi\n    URI: <http://www.roy.com/Maori/museum>;vary=\"language\"\n\n    Teenaa koutou katoa, Aotearoa haere mai!  Ke te pehea koe?\n    \nand he's going to be very upset when he gets the English version\ninstead.  But, as far as the proxy can tell, that is the only acceptable\nversion available.\n\nThe same problem is generic to all the negotiated dimensions.  There are\nonly three solutions that preserve transparent behavior:\n\n1) Have the proxy revert to the source if it receives a request which\n   includes a variant category that has not already been cached;\n\n2) Tell the proxy about all the possible variants so that it can make\n   the decision itself;\n\n3) Use URCs so that the decision can be made within the client and not\n   as part of the request.\n\nI think we can live with (1) for HTTP/1.0, but we should change to (3)\nfor the next version.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> URI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\n> vary            =       \"vary\" \"=\" <\"> 1#vary-param <\">\n> vary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n>                 /       extension-vary\n\nThe language parameter should not be part of the spec. This assumes\nthat a single document will contain a single language. In addition, I\ntend to think that any information that is available in multiple\nlanguages, will probably be stored as seperate files (or \"documents\"),\nand hence, would have different URI's. I do not think decisions can\nreasonably be made automatically at the protocol level. They are\nrather, a navigational problem (and this might also be better handled\nat the markup level via the LANG element).\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <199412151233.HAA16375@ebt-inc.ebt.com>, Gavin Nicol writes:\n>> URI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\n>> vary            =       \"vary\" \"=\" <\"> 1#vary-param <\">\n>> vary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n>>                 /       extension-vary\n>\n>The language parameter should not be part of the spec. This assumes\n>that a single document will contain a single language.\n\nHow so? I see no inconsistency between multi-language documents and a\nlanguage preference negociation.  Could you give an example of\nproblematic behaviour?\n\n> In addition, I\n>tend to think that any information that is available in multiple\n>languages, will probably be stored as seperate files (or \"documents\"),\n>and hence, would have different URI's. I do not think decisions can\n>reasonably be made automatically at the protocol level.\n\nWhy not?\n\nHere's an example: a certain European government agency is required\nby law to make all its regulations and forms available in English,\nFrench, and German.\n\nA client from France makes a request:\n\nGET /forms/tax-form-1 HTTP/1.0\nAccept-Language: fr\n\nAnd the server hands out the french version of the form.\nThe alternative is the dreaded:\n\n\"Click _here_ for English\"\n\"Click _here_ for French\"\n\"Click _here_ for Spanish\"\n\nAs this is not really current practice, I could live with leaving it\nout of the spec. But not because of the reasons you cite.\n\nIf there is some place in the spec that says that every entity has a\nwell-defined, unique Content-Language, that _is_ a bad thing and it\nshould be fixed. The spec doesn't say anything like \"The default\nContent-Language is English,\" does it?\n\nIt could perhaps be made clearer, by saying something in 7.6 like\n\nNOTE: In the absence of an explicit Content-Language header,\nthe client should make no assumptions about the language of\nthe returned entity, as it might be a language-neutral entity\nsuch as a graphic, or it might be a multi-language document.\n\nBut I don't see that having an Accept-Language negociation implies\nthat every entity has a unique Content-Language.\n\n\n> They are\n>rather, a navigational problem (and this might also be better handled\n>at the markup level via the LANG element).\n\nHow would the LANG element contribute to a solution?\n\n\nBy the way... is it a fault for a server to send an entity with a\nContent-Type not listed in the Accept: header of the request?\nHmmm... the \"406 None Acceptable\" header seems to imply that it is.\n\nNow that I think about it, it shouldn't be.\n\nIf a server has several representations of an object, it is bound to\nreturn one that minimizes the penalty function defined by the\nnegociation algorithm. There may be a unique representation that\nminimizes the penalty, or there may be two or more that are equally,\nbut minimally \"bad.\" In that case, the server may return either of the\nequally bad representations.\n\nIf a client accepts only HTML, and the server has only postscript, then\nthe postscript representation does in fact minimize the penalty function,\nand the server may return that representation, no?\n\nI believe this matches current practice. Has anybody done any testing\non this?\n\nPerhaps there should be a new header, say \"Max-Penalty.\" The\ninteraction is similar to the If-Modified-Since header: the client\ngives a maximum tolerable value for the penalty function. If all\nrepresentations exceed that maximum, then the server responds with:\n\n406 No suitable representation\n\nThis would require that the actual calculation of the penalty function\n-- rather than just its general properties -- be specified in the\nprotocol.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> The language parameter should not be part of the spec. This assumes\n> that a single document will contain a single language.\n\nNo it doesn't. The Content-Language header has explicitly been defined\nso that multiple natural languages can be specified. The\nAccept-Language header can also contain a list of languages which also\ncovers multi-linguistic documents.\n\n> In addition, I\n> tend to think that any information that is available in multiple\n> languages, will probably be stored as seperate files (or \"documents\"),\n> and hence, would have different URI's.\n\nThis is exactly what we want to avoid ;-)\n\n> I do not think decisions can\n> reasonably be made automatically at the protocol level. They are\n> rather, a navigational problem (and this might also be better handled\n> at the markup level via the LANG element).\n\nIf you already have the document and actually parsed it in order to find\nthe <LANG> element, then there is no idea in the language negotiation\nat all. It must be done at the protocol layer.\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> A client from France makes a request:\n> \n> GET /forms/tax-form-1 HTTP/1.0\n> Accept-Language: fr\n> \n> And the server hands out the french version of the form.\n> The alternative is the dreaded:\n> \n> \"Click _here_ for English\"\n> \"Click _here_ for French\"\n> \"Click _here_ for Spanish\"\n> \n> As this is not really current practice, I could live with leaving it\n> out of the spec. But not because of the reasons you cite.\n\nI would be very sad if we decide on taking it out. I have great plans\nof implementing it in the next release of the Library. Imagine the\npossibilities it gives when used in the line mode browser:\n\nwww -language dk\n\nIt is definitely needed in this version of HTTP!\n\n> If there is some place in the spec that says that every entity has a\n> well-defined, unique Content-Language, that _is_ a bad thing and it\n> should be fixed. The spec doesn't say anything like \"The default\n> Content-Language is English,\" does it?\n>\n> It could perhaps be made clearer, by saying something in 7.6 like\n> \n> NOTE: In the absence of an explicit Content-Language header,\n> the client should make no assumptions about the language of\n> the returned entity, as it might be a language-neutral entity\n> such as a graphic, or it might be a multi-language document.\n\nYou can have a list of multiple languages in the Content-Language for a\nmulti-linguistic document - or an audio file etc.\n\n> By the way... is it a fault for a server to send an entity with a\n> Content-Type not listed in the Accept: header of the request?\n> Hmmm... the \"406 None Acceptable\" header seems to imply that it is.\n> \n> Now that I think about it, it shouldn't be.\n\nThere are arguments for both views. If you have a dedicated client for\nonly a few content-types then you are not interested in anything you\ndon't ask for. However, general WWW clients might be able to handle\nmost formats so here it doesn't really matter.\n\n> If a server has several representations of an object, it is bound to\n> return one that minimizes the penalty function defined by the\n> negociation algorithm. There may be a unique representation that\n> minimizes the penalty, or there may be two or more that are equally,\n> but minimally \"bad.\" In that case, the server may return either of the\n> equally bad representations.\n>\n> If a client accepts only HTML, and the server has only postscript, then\n> the postscript representation does in fact minimize the penalty function,\n> and the server may return that representation, no?\n> \n> I believe this matches current practice. Has anybody done any testing\n> on this?\n\nThis _is_ current practice but the server _must_ have the possibility\nof not sending the object, that is \"406 None Acceptable\", as the client\nvery easy can specify that it accepts all formats by using \"Accept: */*\"\n\n> Perhaps there should be a new header, say \"Max-Penalty.\" The\n> interaction is similar to the If-Modified-Since header: the client\n> gives a maximum tolerable value for the penalty function. If all\n> representations exceed that maximum, then the server responds with:\n> \n> 406 No suitable representation\n> \n> This would require that the actual calculation of the penalty function\n> -- rather than just its general properties -- be specified in the\n> protocol.\n\nIn theory this is a good idea, but in practice it will never be\npossible to standardize the quality factor in terms of absolute values\nas the client does not know the server values. The q factores are only\nvalid as long as they are relative to each other.\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "HTTP: T-T-TTalking about MIME Generatio",
            "content": "HTTP: T-T-T-Talking about MIME Generation\n\n1.Much has been mentioned about the cumulative terror of the round trip time \n(RTT) penalty paid for many small HTTP requests, each of which requires its own \nTCP connection (and resultant RTTs) required to fetch and render an HTML \ndocument and its component objects.  Compounding the problem, some anti-social \nnetwork clients initiate multiple concurrent TCP connections in an attempt to \ncreate the illusion of a speed up by distracting from boredom by rendering all \nobjects as they arrive simultaneously.  Should the user choose to abort a \ndocument load conducted in this way, it aborts all in-progress TCP connections \nas well.  The network is then then saddled with what others have observed as\nthe substantial overhead of all the silent screams of those aborted connections.  \n2.  Packing all or part of an HTML document and its components into a multipart \nMIME message in response to an HTTP GET would increase the number of bytes sent \nalong a single TCP socket connection.  This will avoid some of the network \ncongestion observed with the multiple and/or simultaneous low content-length \nTCP sessions when clients build the document.  While valuable work is underway \nto provide binary-encoded and multiplexed transport-level solutions to the \nmultiple connect problem [ Spero, Raggett, HTTP-ng ], We need to look \nconcurrently at exploiting the transport encoding scheme upon which HTTP \ntheoretically rests to address this problem--MIME.  I do not believe that this \napproach conflicts with any of the work on HTTP-ng, and could envision a case \nwhere they could be used together transfer multiple HTML documents and their \ncomponents quite rapidly.  Most of all, we need to provide many options \nallowing for maximum tuning and customization on all sides.\n\n3.Operating under the assumption that at this time in the life cycle of the\nweb, most HTML documents contain components that reside on the same server as \nthe document itself, why not trade the multiple network connections from the \nclient to the same server for some up-front packaging by the server.  Instead \nof a server having to clone itself into as many instances as sub-objects it \nserves, it could perform a rudimentary (cached) parsing of the document\nto determine the appropriate sub-objects that it served as well, package \nthem up into MIME body parts sent along with the HTML document.  Accesses by \nthe server on which the components reside usually requires merely a simple \nsecondary storage access measured in terms of milliseconds which in the vast \nmajority of cases is significantly less than that of a TCP RTT which can run \ninto perceptable fractions of whole seconds.  As an option, the server could \nresolve any object references resident on other servers (caching the results \nin shared memory), and include them in the multipart data stream.  What follows\nis a draft protocol for encoding an HTML document and its components in\nmultupart MIME.\n\n4.The client would need to include multipart/mixed*  in its Accept: headers \nif it chose to accept this encoding.  A terminal-based browser such as lynx \nor LineMode would not send this in its Accept: headers and could thus opt out.\nIf a browser had the flushed the HTML document from its cache but had not \nflushed some of its inline images you would not want to include this header \nunless you wanted the larger, complete data stream.  The server builds a \nmultipart/mixed* message consisting of the HTML document and whatever \ncomponents were resolved.  The server could be configured either to resolve \nand cache or leave to the client any components that resided elsewhere.  \n\n* multipart/mixed might be better as application/http or multipart/http.\nI defer to the MIME dieties for a ruling.\n\n6. Interoperating efficiently with client cache management brings up some\ninteresting issues.  The ability to check the HTML document's requirements\nagainst the client-side cache before issuing a precisely tailored HTTP MGET \nrequest (which would be returned as multipart/mixed*).\n\n6. The HTML document is included as a body part of Content-Type:  text/html.\nThe outermost MIME headers (or the header associated with multipart/mixed*) \ncontain a Message-ID: field in the form:\n\nMessage-ID:  <URI>\n\nEach component object is identified by a Content-ID: field \n\nthat is in the form:\n\nContent-ID: <URI>\n\nIn this manner, each body part can be unpacked by the client, inserted into\nits cache data structure, any missing body parts could be acquired, and the\nmultipart images would be pulled out of the cache struture to render.  The\nmaster HTML document can be recognized as main body part by the URI common\nto both the Message-ID: field and that of the Content-ID: field in one of the \nContent-Type: text/html body parts.\n\n7.An instance of the proposed MIME encoding scheme for HTTP follows.  This \nis currently in the process of a feasibility study in METHADONE (Mime Encoding \nTHreAded Daemon Optimized for Network Efficiency), a caching, lightweight\nthreaded, MIME multipart encoding HTTP server for solaris 2.x (exploiting the \nrich base functionality of NCSA's httpd) currently under beta development at \nthe UCSF Library and Center for Knowedge Management.\n\nClient makes HTTP connection to host.domain:\n___\nGET /http_mime.html HTTP/1.0\n...\nLanguage: en\nAccept:  multipart/mixed, application/http\n...\n___\nServer responds:\n___\nHTTP/1.0 200 OK\nTitle: A Proposed MIME Encoding of an HTML Document and its Components \nDate: Thursday, 15-Dec-94 03:19:05 GMT\nServer: METHADONE 0.1\nCost: free!\nContent-Language: en\nAllowed: GET HEAD PUT\nAllowed: GET HEAD \nDate: Thu Dec 15 19:48:54 PST 1994\nLast-Modified: Wed Dec  7 14:54:48 1994\nMIME-version: 1.0\nMessage-ID: <http://host.domain/http_mime.html>\nVersion: beta\nContent-Type: multipart/mixed; \nboundary=__http__boundary__\n\n\n\n--__http__boundary__\n\nContent-type: text/html\nContent-Language: en\nContent-ID: <http://host.domain/path/http_mime.html>\nContent-Length:  193\n\n<HTML>\n<HEAD><TITLE>A Compound HTML Document</TITLE></HEAD>\n<BODY>\n<H1>Compound HTML Document</H1>\n<IMG SRC=\"http://host.domain/path/image.gif\">\n...\n<INC SRC=\"http://host.domain/path/frag.html\">\n...\n<IMG SRC=\"http://host.domain/path/image.xbm\">\n...\n</BODY></HTML>\n\n--__http__boundary__\n\nContent-Type: image/gif\nContent-ID: <http://host.domain/image.gif>\nContent-Transfer-Encoding: 8bit\nContent-Length: 1234\n\n<1234 bytes of GIF>\n\n--__http__boundary__\n\nContent-Type: text/html\nContent-ID: <http://host.domain/path/frag.html>\nContent-Transfer-Encoding: 7bit\nContent-Length:  799\n\n<HR>\n<UL>\n<LI>Client issues GET on HTML document.\n<LI>Server scans HTML document for any component objects (SRC=%URI;) upon \nreciept of GET.\n<LI>In most cases, the components are resident on the same server, so any\nGET's are to secondary storage instead of the network.  \n<LI>The server can perform GET on components resident elsewhere and cache or\ndefer that to the client.\n<LI>The server packs up the document with its components into a multipart MIME\nmessage.\n<LI>The data can be sent over an 8-bit HTTP socket, no content-transfer-encoding required.\n<LI>Content-ID: MIME Header contains URL of component.  \n<LI>Client parses multipart MIME message.\n<LI>Client adds components to image cache.\n<LI>Client performs GET on remaining images if not resolved by server.\n<LI>Client renders HTML.\n</UL>\n<HR>\n\n--__http__boundary__\n \nContent-Type: image/xbm\nContent-ID: <http://host.domain/path/image.xbm>\nContent-Transfer-Encoding: 8bit\nContent-Length: 2345\n\n<2345 bytes of XBM>\n\n--__http__boundary__\n\n--__http__boundary__\n\n\nTwo consecutive boundary strings indicate an EOF.\n\n8.  I plan to bring this up on the sgml-internet list as well, for a broader,\nmore general perspective.\n\n-marc\n--/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\\n// Marc Salomon    Software Engineer             e-mail: marc@ckm.ucsf.edu  // \n\\\\ Innovative Software Systems Group                                        \\\\\n// Center for Knowledge Management               phone :  415.476.9541      //\n\\\\ The University of California, San Fransisco                              \\\\\n// 530 Parnassus SF, CA 94143-0840               fax:    415.476.4653       //\n \\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <9412150046.aa28029@paris.ics.uci.edu>, \"Roy T. Fielding\" writes:\n>and Dan replied:\n>\n>> This is kinda humorous... the answer has been in the spec all along:\n>> ...\n>> \n>> URI-header      =       \"URI\" \":\" 1#( URI [\";\" vary] )\n>> vary            =       \"vary\" \"=\" <\"> 1#vary-param <\">\n>> vary-param      =       \"type\" / \"language\" / \"version\" / \"encoding\"\n>>                 /       extension-vary\n>> extension-vary  =       token\n>> \n>\n>No, that isn't enough information to allow a proxy to decide\n>what is available.\n\nWhy is that such a bad thing? The server can give a lot of information\nabout what's available by giving several URI: headers.\n\nPerhaps some way for the server to indicate \"these URI: headers\nspecify _all_ representations of this resource\" would be useful.\n\n>  For instance, let's say\n[ ... ]\n>and he's going to be very upset when he gets the English version\n>instead.  But, as far as the proxy can tell, that is the only acceptable\n>version available.\n\nThe proxy had better not jump to conclusions like this. It had better\ngo to the original server and get the real answer, if it doesn't have\nenough information locally.\n\n>  There are\n>only three solutions that preserve transparent behavior:\n>\n>1) Have the proxy revert to the source if it receives a request which\n>   includes a variant category that has not already been cached;\n>\n>2) Tell the proxy about all the possible variants so that it can make\n>   the decision itself;\n>\n>3) Use URCs so that the decision can be made within the client and not\n>   as part of the request.\n>\n>I think we can live with (1) for HTTP/1.0,\n\nBingo. I agree.\n\n> but we should change to (3)\n>for the next version.\n\nThe whole purpose of the HTTP format negociation algorithm is to so\nthat it only takes one round trip to get a document, even if there are\nvariants.\n\nHaving the server tell the client what all the options are, and then\nhaving the client choose is just like:\n\n\"Click _here_ for postscript\"\n\"Click _here_ for text\"\n\nThere's a time and a place for that, but we should avoid it whenever\npossible. The thing you call a URC can just be an HTML document that\nexplains in human-readable terms what the options are.\n\nOn the other hand, there seems to be a lot of interest in creating a\nmachine-readable representation of this information. The Harvest SOIF\nformat[1] is the best I've seen so far.\n\n\n[1] The Harvest Summary Object Interchange Format (SOIF)\nhttp://harvest.cs.colorado.edu/brokers/soifhelp.html\nFri Nov 25 17:34:20 1994\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "One thing to consider (not that i think this is a bad idea) is that often\nthe objects being sent along are images leading to two non-optimisations.\n\n1) Mime encoding is going to roughly double the number of bytes sent\n2) By explicitly sending them every time, the server wont allow the client\nto cache the images, so for example the stupid blue ball for a \"bullet\"\nwill get send a zillion times.\n\n- Mitra\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> Why is that such a bad thing? The server can give a lot of information\n> about what's available by giving several URI: headers.\n\nHmmm, I don't necessarily support this -- this wastes bandwidth.\nBesides, the set of available versions may vary, so the proxy can\nnever be sure if it has all the presentations/knowledge of them\nwithout going to the original server.\n\nThat's why:\n\n> It had better\n> go to the original server and get the real answer, if it doesn't have\n> enough information locally.\n\nYes!  If it doesn't find a perfect match locally it goes to the\nremote.  It may be that the remote tells to use the local copy, which\nis ok, and still saves a lot of bandwidth.\n\n> >1) Have the proxy revert to the source if it receives a request which\n> >   includes a variant category that has not already been cached;\n\nDefinitely.\n\n> >2) Tell the proxy about all the possible variants so that it can make\n> >   the decision itself;\n\nNo; the list can't be guaranteed to be exhaustive after some period of\ntime.  Rather than futher complicating things with life times for this\ninformation, have the proxy connect to remote every time in these\ncases to make a check.\n\n> >3) Use URCs so that the decision can be made within the client and not\n> >   as part of the request.\n\nMmmmm (the way Marge Simpson sez it).\n\n> >I think we can live with (1) for HTTP/1.0,\n> \n> Bingo. I agree.\n\nJackpot. Me too.\n\nCheers,\n--\nAri Luotonenhttp://home.mcom.com/people/ari/\nNetscape Communications Corp.\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "    2) By explicitly sending them every time, the server wont allow the client\n    to cache the images, so for example the stupid blue ball for a \"bullet\"\n    will get send a zillion times.\n    \nThis is why in our experiments we added a GETLIST method, so that the\nclient can explicitly request those images that it needs.  We also\nhad a GETALL method (\"give me the HTML and all the inlined images\")\nbut we had to do some tricky heuristics to decide when it was better\nto use this than to do a GET followed by a GETLIST.\n\nGETLIST, of course, can also be used to retrieve a set of HTML files,\nnot just a set of images.  And we actually implemented it as a series\nof GETS stuffed into the same pipe, rather than as a new method.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "In message <ab1685b2030210040a79@[192.190.111.98]>, Mitra writes:\n>One thing to consider (not that i think this is a bad idea) is that often\n>the objects being sent along are images leading to two non-optimisations.\n>\n>1) Mime encoding is going to roughly double the number of bytes sent\n\nHello? Mime encoding adds a few bytes between objects for the boundary.\n\nHTTP is 8-bit clean, after all. No base64 needed.\n\n>2) By explicitly sending them every time, the server wont allow the client\n>to cache the images, so for example the stupid blue ball for a \"bullet\"\n>will get send a zillion times.\n\nThis is a very good point.\n\nThe other point is that it is sufficiently expensive to deploy MGET or\nmultipart/* over HTTP at this point that we might as well bite the\nbullet and do something long-term like Session Control Protocol (which\nmeshes nicely with some security protocols like Secure Sockets Layer).\n\nDeploying MGET/multipart looks to me like:\n\n* Somebody hacks support into NCSA HTTPD (last I heard, that\nsource code is not to enhancement-friendly. No offense intended.)\n\n* Somebody figures out how to manage these multipart files: do\nyou cache them? create them with a cron job?\n\n* Bill Perry shows us how it's done with the emacs w3 browser.\n\n* Henric adds support in libwww 3.x (which is perhaps sufficiently\ndifferent from 2.x that the lynx/chimera guys can't upgrade without\na lot of pain and hassle?)\n\n* Somebody hacks support into NCSA Mosaic. (libwww in NCSA Mosaic\nbears little if any resemblance to CERN's libwww by now.)\n\n* A few information providers maybe start using it\n(It's 3 months into the future by now)\n\n* Other browser/server impelementors get pressured into adding\nsupport.\n\nMeanwhile, commercial folks are implementing HTTP-NG at lightning\nspeed. Six months from now, all the major vendors are doing\ninteroperable compression and encryption over something like SCP or\nSSL (not to mention strong authentication).\n\nIn short: 'taint worthit.\n\nI'd rather see more effort spent on\n\n(1) Specifying existing practice on HTTP/1.0, keeping an eye\non opportunities to gateway/cache/proxy out of HTTP and into\nnext-generation protocols.\n\n(2) Carefully crafting HTTP-NG using an interface definition\nlanguage like ASN/1 or OMG's IDL, so that it can be used\nwith various transports and protocols. (ILU! ILU! ILU! ahem...)\n\n(3) Investigating some name service and replication strategies\nso that links that we write today will continue to work tomorrow.\n\n(4) Coordinating implementation efforts, and working on\na common base of reusable code.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": ">In message <ab1685b2030210040a79@[192.190.111.98]>, Mitra writes:\n>>One thing to consider (not that i think this is a bad idea) is that often\n>>the objects being sent along are images leading to two non-optimisations.\n>>\n>>1) Mime encoding is going to roughly double the number of bytes sent\n\nAt 4:18 PM 12/15/94, Daniel W. Connolly wrote:\n>Hello? Mime encoding adds a few bytes between objects for the boundary.\n>HTTP is 8-bit clean, after all. No base64 needed.\n\nHmm - maybe I'm missing something, but I dont think you can put the file in\nWITHOUT encoding, if you are looking for a boundary, what if the file\ncontained the wrong bytes and got interpreted as the boundary.\n\n- Mitra\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "In message <ab169307050210042c52@[192.190.111.20]>, Mitra writes:\n>>In message <ab1685b2030210040a79@[192.190.111.98]>, Mitra writes:\n>>>One thing to consider (not that i think this is a bad idea) is that often\n>>>the objects being sent along are images leading to two non-optimisations.\n>>>\n>>>1) Mime encoding is going to roughly double the number of bytes sent\n>\n>At 4:18 PM 12/15/94, Daniel W. Connolly wrote:\n>>Hello? Mime encoding adds a few bytes between objects for the boundary.\n>>HTTP is 8-bit clean, after all. No base64 needed.\n>\n>Hmm - maybe I'm missing something, but I dont think you can put the file in\n>WITHOUT encoding, if you are looking for a boundary, what if the file\n>contained the wrong bytes and got interpreted as the boundary.\n\n\"Don't do that.\" :-)\n\nSeriously: you're supposed to pick a boundary so that this doesn't happen.\nI've seen two approaches: \n\n(1) scan the data to be sure your boundary doesn't appear in\nthe data\n\n(2) pick a true-random number of say, 64 bits and base64 encode\nthat as the boundary. Supposedly the chances of a collision\nusing this algorithm are less than the chance that a stray photon\nwill cause your computer to malfunction. I haven't boned up\non statistics and probability enough to follow the arguments\nexactly, though.\n\nThe boundary mechanism is compatible with all three\ncontent-transfer-encodings, not just 7-bit.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "In message <199412160001.QAA09663@neon.mcom.com>, Ari Luotonen writes:\n>\n>> Why is that such a bad thing? The server can give a lot of information\n>> about what's available by giving several URI: headers.\n>\n>Hmmm, I don't necessarily support this -- this wastes bandwidth.\n\nI agree. But I didn't say the server should or must; just that it may.\n\nThe only thing that it _must_ do is issue _some_ URI: header with \na vary parameter if it's got variants. That's enough to prevent\nproxies from jumping to conclusions.\n\n>Besides, the set of available versions may vary, so the proxy can\n>never be sure if it has all the presentations/knowledge of them\n>without going to the original server.\n\nIf the server gives an Expires: header, the proxy can conclude that\nthe variants are stable until then. Otherwise, the usual hueristics\napply.\n\nSee:\nhttp://www.hal.com/%7Econnolly/drafts/formalism.html\n\nfor details.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": ">You can have a list of multiple languages in the Content-Language for a\n>multi-linguistic document - or an audio file etc.\n\nProfuse apologies. On re-reading the spec, I see that this is indeed\nthe case, and that this field is intended to state preferences, not\ndecisions.\n\nThere should be a rule that one should never respond to mail/news at\n3am.... \n\n\n\n"
        },
        {
            "subject": "Re: HTTP Caching Model",
            "content": "> The only thing that it _must_ do is issue _some_ URI: header with \n> a vary parameter if it's got variants. That's enough to prevent\n> proxies from jumping to conclusions.\n\nOk, very good.\n\n> If the server gives an Expires: header, the proxy can conclude that\n> the variants are stable until then.\n\nNot necessarily -- take a Stephen King book \"Killer Web\".  It's first\npublished in English and put online.  It's static data and practically\nnever expires.  A month later they publish it in Finnish, and a month\nafter that in French.  This still shouldn't impose that the English\nversion expires once a month.  And you can't know of the translation\nschedules beforehand in the first place anyway.\n\nOk, so King probably would make this interactive book in which the\nplot changes as a function of time, number of people killed, and\nwhether IT really died in part two...\n\nCheers,\n--\nAri Luotonenhttp://home.mcom.com/people/ari/\nNetscape Communications Corp.\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> At 4:18 PM 12/15/94, Daniel W. Connolly wrote:\n> >Hello? Mime encoding adds a few bytes between objects for the boundary.\n> >HTTP is 8-bit clean, after all. No base64 needed.\n> \n> Hmm - maybe I'm missing something, but I dont think you can put the file in\n> WITHOUT encoding, if you are looking for a boundary, what if the file\n> contained the wrong bytes and got interpreted as the boundary.\n\nAs I said earlier (while many of you were off at the IETF?), I'm\nincreasingly convinced that HTTP messages are (as the recent spec\nsuggests) MIME-like, not MIME conforming. With so many other\ndeviations from MIME, I suggest we should drop the (rather complex)\nMIME multi-part structure based on boundaries, etc. and only allow\nmulti-part messages defined by a Content-Length byte count.\n\nWe'd still want to define conventions for how to count/treat EOL at\nthe start and end of bodies, and this would place some limits\non on-the-fly generation of multi-part types, but it would\nbe a lot easier to parse and would clarify the distinction between\nheader and body transport conventions. MIME is basically a\ntext based protocol. HTTP is a mixed text and binary protocol\nthat often looks like MIME, but isn't really.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: multiple-get (was: T-T-TTalking about MIME Generation",
            "content": "On Thu, 15 Dec 1994, Daniel W. Connolly wrote:\n\n> Ah... that reminds me: before we forget completely about HTTP/1.0, I\n> think the \"Pragma: keep-connection\" proposal (the idea; not\n> necessarily the syntax) is worth persuing in the short term.\n> \n> In other words: I agree that we should enhance clients and servers to\n> conduction multiple HTTP/1.0 transactions over the same connection.\n> \n> The Content-Type issues are a little sticky, but it's probably worth\n> persuing...\n\nIt was pointed out at the IETF HTTP-ng BOF that the keep session open\napproach (whatever the syntax) applied to the client/proxy and \nproxy/server connections independently.  As such the client/proxy could\nend up open a long time.  (I would suspect a high performance proxy \nserver could to virtual switching between client connections and\nidle server connections.) As I understood the discussion, multiple\ncomplete HTTP transactions would travel down the pipe so Content-Type\nissues should not vary from the current connection/transaction model.\n\nI think nothing I heard precludes a client from holding sessions open\nwith multiple hosts or multiple sessions with one host.  I for one\nfind the human factors of the NETSCAPE approach much more satisfactory\nwith early rendering of available information then sitting idle for\nlong intervals waiting for a stupid company logo to be transfered or\nwhat ever.  The SGML organization home page is particulariliy offensive\nas the first graphic fills the screen.  Infering from observed \nbehavior, I would speculate that the primary reason NETSCAPE opens\nmultiple connections is to learn the shape of graphics to be included\nthereby making the rendering more useful.\n\nIt might be nice in a future HTTP to consider restartable transactions\nsuch that a user interrupted transfer could be restarted at an \nintermediate point ... for that matter a long file transfer could\nrestart.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, Albert Lunde wrote:\n\n> As I said earlier (while many of you were off at the IETF?), I'm\n> increasingly convinced that HTTP messages are (as the recent spec\n> suggests) MIME-like, not MIME conforming. With so many other\n> deviations from MIME, I suggest we should drop the (rather complex)\n> MIME multi-part structure based on boundaries, etc. and only allow\n> multi-part messages defined by a Content-Length byte count.\n\nYeah!  Other than the fact that MIME existed as did tools for processing,\nI have never understood why an 8bit clean protocol like TCP/IP is\ncluttered with the syntax of mail/MIME (a comment was made at the IETF\nthat MIME semantics for multiple part content with a new binary syntax\nmight make sense).\n\nI haven't had time to read any drafts on HTTP-ng yet but I'm hoping that\nthe binary encoding I hear mentioned deals with eliminating \n(minimizing) ascill headers by using nice terse binary structures.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> On Fri, 16 Dec 1994, Albert Lunde wrote:\n> \n> > As I said earlier (while many of you were off at the IETF?), I'm\n> > increasingly convinced that HTTP messages are (as the recent spec\n> > suggests) MIME-like, not MIME conforming. With so many other\n> > deviations from MIME, I suggest we should drop the (rather complex)\n> > MIME multi-part structure based on boundaries, etc. and only allow\n> > multi-part messages defined by a Content-Length byte count.\n> \n> Yeah!  Other than the fact that MIME existed as did tools for processing,\n> I have never understood why an 8bit clean protocol like TCP/IP is\n> cluttered with the syntax of mail/MIME (a comment was made at the IETF\n> that MIME semantics for multiple part content with a new binary syntax\n> might make sense).\n\nExpanding on this idea a bit ... I can't find a good definition\nof Content-Length: at this hour (If it started out in the MIME\nspec it may have vanished in revisions) (so I may not follow\nprecident) ... but what I'd like to see is something like this:\n\nheaders     \nContent-Length: count<CR><LF>\ncounted bytes\ncounted bytes\ncounted bytes\n<CR><LF>\nmore headers\n\nThe final <CR><LF> would not be counted in the length (and is\npresent mainly to make life easier for line-oriented parsers.)\n\nIn the case of a single-part message, the final <CR><LF>\nwould be replaced with closing the connection. (Thus degenerating\nto current practice.)\n\n(Because I don't have a prior def handy I'm not sure\nhow to count or not count the initial <CR><LF>. I'd guess it\nis not counted.)\n\nWe'd still need mechanisms in the headers to distingush what\ninitial headers applied to the whole transaction, and what\nto a single body, and to indicate the presence of recursion\n(multi-part stuff in a body section) if we allow it.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> As I said earlier (while many of you were off at the IETF?), I'm\n> increasingly convinced that HTTP messages are (as the recent spec\n> suggests) MIME-like, not MIME conforming. With so many other\n> deviations from MIME, I suggest we should drop the (rather complex)\n> MIME multi-part structure based on boundaries, etc. and only allow\n> multi-part messages defined by a Content-Length byte count.\n\nI think we went through this with HTML; one might have said (many did)\nthat:\n\n\"... HTML files are (as the recent spec suggests) SGML-like, not SGML\nconforming. With so many other deviations from SGML....\"\n  \nI think the original *intent* was to be MIME conforming, and that it\nisn't *hard* to be MIME-conforming, and that there are *benefits* to\nbeing MIME-conforming.\n\nNow, we have to be careful to define what we mean by MIME-conforming,\nand, in particular, we may well want to register some new MIME-types\nor content-transfer-encodings. \n\nThe EOL convention issue is not a 'HTTP' issue but centers around\nwhether \"text/html\" is allowed to be more flexible about EOL\nconvention when transport in binary form than other text forms.\n\nMIME is not basically a text-based protocol, any more than HTTP is.\n\n\n \n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "\"Daniel W. Connolly\" <connolly@hal.com> wrote:\n\n  > Deploying MGET/multipart looks to me like:\n  [a list of steps for clients, servers, including...]\n  * A few information providers maybe start using it\n  (It's 3 months into the future by now)\n\n  > \n  > Meanwhile, commercial folks are implementing HTTP-NG at lightning\n  > speed. Six months from now, all the major vendors are doing\n  > interoperable compression and encryption over something like SCP or\n  > SSL (not to mention strong authentication).\n\nSorry, I'm skeptical about this statement.  At least some of the\nproposals for MGET/multipart and keep-alive are compatible with what\nexists now.  For example, a client could attempt to send an MGET to a\nserver.  If the server chokes, the client can revert to a series of\nregular GETs.\n\nMy contrast, deploying HTTP-NG would require significant changes to\nclients and servers both, and, despite the transition plan described by\nRaggett and Spero, I see HTTP 1.0 and HTTP-NG as fundamentally unable\nto interoperate.  (They propose using a proxy to translate.)\n\nSo, I think vendors are less likely to switch to HTTP-NG in three\nmonths, a protocol still being experimented with and IMO not quite\nready for prime time, than they are to adopt the MGET stuff.\n\nIt may well be that *some* vendors will have compression, encryption,\nand session control in six months (some do now, in limited ways), but\nI'm equally skeptical that \"all the major vendors\" will be doing so\n\"interoperabl[y]\" if there's as yet no agreed-to standard upon which to\ninteroperate.\n\nMy tastes (obviously) run to a more evolutionary approach for HTTP.  I'm\nunconvinced that the performance problems require a flash cut to a binary\nprotocol.  Spero has shown that doing multiple transactions over one\nconnection achieves signficant performance improvements.  His response is\nto change HTTP drastically.  Mine is to do so within the current overall\ndesign.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> > As I said earlier (while many of you were off at the IETF?), I'm\n> > increasingly convinced that HTTP messages are (as the recent spec\n> > suggests) MIME-like, not MIME conforming. With so many other\n> > deviations from MIME, I suggest we should drop the (rather complex)\n> > MIME multi-part structure based on boundaries, etc. and only allow\n> > multi-part messages defined by a Content-Length byte count.\n> \n> I think we went through this with HTML; one might have said (many did)\n> that:\n> \n> \"... HTML files are (as the recent spec suggests) SGML-like, not SGML\n> conforming. With so many other deviations from SGML....\"\n>   \n> I think the original *intent* was to be MIME conforming, and that it\n> isn't *hard* to be MIME-conforming, and that there are *benefits* to\n> being MIME-conforming.\n[...] \n> MIME is not basically a text-based protocol, any more than HTTP is.\n\nI'd be the first to agree that MIME is a fine example of protocol\ndesign. But it has different design objectives, the chief one of\nwhich is to stuff all sorts of data types into (possibly broken)\nimplementations of RFC822 messages and SMTP transport. These are\nclearly lines of text. The MIME standard does not really say much\nabout the \"binary\" content transfer encoding that we say we are\nusing, and says nothing about using it in the body of a message.\n\nMy view of \"MIME-Conforming\" is that \"you can parse it with\nthe MIME RFC\", not \"redefine half the protocol and call it MIME\".\nI don't see how a message with a body containing a un-encoded GIF\ncan be represented with the MIME RFC.\n\nWe use a bunch of headers not in MIME or RFC822 and we redefine\nsome headers found in MIME.\n\nThis notion of tolerating different EOL representations\nis a sticky one that can be pushed off on the HTTP or HTML specs.\n\nI get the impression some people think that (in particular)\nUnix HTTP servers are sending text with bare line feeds (and\nnot just text/html!) for reasons of efficency (which is\na design goal of HTTP) and not just because they are bad/non-\nconforming servers.\n\nWe clearly want to use MIME/\"Internet Media\" types -- I'd suggest\nthis is our strongest affinity to MIME. HTTP-NG sounds like\nit is becoming less RFC822-like.\n\nI think having a multi-part structure that logically maps onto MIME\nis a good idea, but we seem to have made and be making different\ndecisions about transport on a byte-by-byte level.\n\nSGML conformance clearly has particular benifits. What do you all\nthink are the payoffs (or downsides) to sticking close to the\nMIME spec?\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Statistics on reusing request headers in persistent connection",
            "content": "   The benefits of reusing request headers in persistent\n   ----------------------------------------------------\n   HTTP connections: A statistical analysis.\n   -----------------------------------------\n\n                                           Oct 31, 1995\n                                           Koen Holtman, koen@win.tue.nl\n\n 1. INTRODUCTION\n ---------------\n\nWhen sending HTTP request over a persistent (keep-alive) HTTP\nconnection, it would be possible to re-use request headers from\nearlier requests in subsequent requests.  For example, if the\nUser-agent header for requests n and n+1 are the same, there would be\nno need to send the header twice, a special request header (using less\nbytes) could indicate that the User-agent header is to be reused.\n\nRoy Fielding recently proposed a mechanism allowing such reuse.  The\nquestion is whether designing and implementing such a mechanism would\nbe a good move.\n\nFor:  - less HTTP traffic\n      - faster browsing response time\n\nAgainst: - more software complexity\n         - time spent in design and implementation cannot be\n           used for making other improvements\n\nI have made some statistics about the size of the gains.\n\n\n 2. CONCLUSION\n -------------\n\nMy conclusion is that the gains are too small to bother about request\nheader reuse at this point:\n      - HTTP traffic savings would be about 1.3%\n      - speedup of browsing response time would be minimal:\n        page+inline loading times would be noticeably faster in\n        about 17% of all cases.\n\nMuch higher gain/effort ratios can be had by focusing on other\ndesirable features of future HTTP software, for example\n\n - (general) support for `Content-Encoding: gzip'\n - support for sending .jpg inlines instead of .gif inlines to all\n   browsers that can handle .jpg\n - reducing the amount of Accept headers generated by some browsers\n   (my Mosaic for X browser sends 822 bytes of accept headers, most of\n   them for MIME types I can't even view!), maybe introducing a\n   mechanism for reactive content negotiation at the same time.\n - proxies that change multiple Accept headers in a request into one\n   big Accept header when relaying the request\n\nI therefore propose to drop the subject of request header reuse on\nhttp-wg.\n\nHeader reuse mechanisms would only get interesting again if we find\nsome good reason to make the average request message much larger (say\n500 bytes) than it needs to be now (200 bytes).\n\n(End of conclusions.)\n\nYes, you can stop reading now!\n\nYou can also page to Section 6, which contains some statistics about\nthe number of requests done over persistent connections.\n\n\n 3. HOW LARGE DO REQUEST MESSAGES NEED TO BE?\n --------------------------------------------\n\n 3.1 CURRENT ACCEPT HEADER PRACTICE\n -----------------------------------\n\nI captured the request headers sent by the three browsers present on\nmy Linux box. \n\nA typical Mozilla/1.12 (X11) GET request message for a normal URL:\n\n  ---------------------------------------------------\n  GET /blah/blebber/blex.html HTTP/1.0\n  User-Agent: Mozilla/1.12 (X11; I; Linux 1.2.9 i486)\n  Referer: http://localhost/blah/blebber/wuxta.html\n  Accept: */*\n  Accept: image/gif\n  Accept: image/x-xbitmap\n  Accept: image/jpeg\n\n  ---------------------------------------------------\n\nWhen GETting URL contents for inline images, Mozilla omits the\n`Accept: */*' header above.\n\nNote that the four Accept headers above could be combined into a\nsingle Accept header:\n\n  Accept: */* image/gif image/x-xbitmap image/jpeg .\n\nNone of the three browsers on my Linux system do such combining,\nthough it would make the request message shorter (see also the table\nbelow).  Is there some ancient HTTP server, not supporting\nmulti-element Accept headers, they want to stay compatible to?\n\nHere is a table of typical GET request message sizes for the browsers\non my Linux system:\n\n  -----------------------+---+---+-----+----\n  Browser                 Len Acc (Ac1) Rest\n  -----------------------+---+---+-----+----\n  NCSA Mosaic for X/2.2   995 882 (299) 113\n  Lynx/2.3 BETA           349 248 (100) 101\n  Mozilla/1.12 (normal)   207  73  (36) 134\n  Mozilla/1.12 (inline)   194  61  (34) 133\n  -----------------------+---+---+-----+----\n\n  Len  : #bytes in request message\n  Acc  : #bytes in the Accept headers\n  (Ac1): #bytes that would be in an equivalent single-line Accept header\n  Rest : #bytes in non-Accept headers and first line of request\n\n\n 3.2 LACK OF NEED FOR LARGE ACCEPT HEADERS\n -----------------------------------------\n\nIn current practice on the Web, 99% of all URLs (if not more) only\nhave one content variant, so the Accept headers contained in a request\nare almost never used.  It is unlikely that this will change in the\nfuture.\n\nThus, there is no good reason for tacking large Accept headers onto a\nrequest, now or in the future.  An accept header larger than\n\n  Accept: */* image/gif image/x-xbitmap image/jpeg\n\nis wasteful, the small number of cases case not covered by the header\nabove could be solved by reactive content negotiation (300 and 406\nresponses).  Note that, if a browser discovers it is doing a lot of\nreactive content negotiation to a site, it could dynamically make its\nAccept headers to that site larger to reduce future reactive\nnegotiation.  So sending large Accept headers may be efficient\nsometimes, but not by default.\n\nI see the large default Accept header problem as a problem that will\ndisappear with browser upgrades in the near future, after a reactive\nnegotiation mechanism has been defined.\n\n\n 4. STATISTICS\n -------------\n\nTo make the statistics below, I took a set of proxy<->server HTTP\ntransactions between the www.win.tue.nl proxy and off-campus servers\n(18 days worth of traffic, approximately 150Mb in 14501 HTTP\ntransactions), and calculated what would happen if these\ntransactions were all done over persistent HTTP connections.\n\nIf a simulated persistent connection has been idle for 10 minutes, it\nis closed.\n\n\n 4.1 HEADER SIZES\n ----------------\n\nWorking from the reasoning above, I take the following request\nmessage, generated by Mozilla, as typical.\n\n  ---------------------------------------------------\n  GET /blah/blebber/blex.html HTTP/1.0\n  User-Agent: Mozilla/1.12 (X11; I; Linux 1.2.9 i486)\n  Referer: http://localhost/blah/blebber/wuxta.html\n  Accept: */*\n  Accept: image/gif\n  Accept: image/x-xbitmap\n  Accept: image/jpeg\n\n  ---------------------------------------------------\n\nEvery header in this message could potentially be reused in future\nrequests.  Only the `GET' line will always be different.\n\nI will use the following figures in the statistics below:\n\n- Without header reuse, the average request size is 200 bytes\n\n- With header reuse, the average request size is\n    - 200 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\n- The average size of the response headers is always 180 bytes.\n\n\n 4.2 RESULTS\n -----------\n\n 4.2.1 Size of HTTP traffic transmitted.\n\n                           in response  in\n                           bodies       headers   total \n    ---------------------+------------+---------+----------------\n\n    Without header reuse:     145 Mb     5.3 Mb   150.3 Mb (100.0%)\n    With header reuse:        145 Mb     3.3 Mb   148.3 Mb ( 98.7%)\n\n    Reuse saves:                         2.0 Mb            (  1.3%)\n\nCompared to other possible savings, 1.3% is too little to care about.\n\nBut traffic size counts are dominated by very large requests: maybe we\ncan get a noticeably faster response time on small requests?\n\n 4.2.2. Response time\n\nI use the following approximations for getting response time results:\n\n - The sequence of requests done over each persistent HTTP connection\n   is divided into `wait chains'.\n\n - Each subsequent request in a `wait chain' is no more than 20\n   seconds apart.\n\n - the idea is that the user does not perceive the speedup of\n   individual HTTP transactions in a `wait chain', but only the\n   average transaction speedup for the whole `wait chain'.\n\n - We want to determine the percentage of wait chains that get\n   noticeably faster after the introduction of header reuse.\n\n - We assume that for a wait chain to get noticeably faster, the\n   HTTP traffic size generated in that wait chain must decrease\n   with at least 10%.\n\nAmount of wait chains with a certain percentage of traffic decrease:\n\n           decrease %   amount\n           -----------+-------------\n                   0     1069    24%\n                 1-4     1763    39%\n                 5-9      926    21%\n               10-19      396     9%\n               20-49      278     6%\n               50-         70     2%\n\nThus, request header reuse will lead to a noticeable speedup for\n17% of all wait chains.\n\n\n\n 5. ALTERNATIVE 500 BYTE SCENARIO\n --------------------------------\n\nThe above statistics assume that \n\n- Without header reuse, the average request size is 200 bytes\n\n- With header reuse, the average request size is\n    - 200 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\nThe reasons for these assumptions are given in Section 3.\n\nOne could imagine an alternative scenario, in which we have a good (or\nbad) reason to make the requests much larger.  To see if introducing\nheader reuse is a good idea under such a scenario, I made the above\nstatistics again with the following assumptions:\n\n- Without header reuse, the average request size is 500 bytes\n\n- With header reuse, the average request size is\n    - 500 bytes for the first request over a persistent connection\n    -  40 bytes for all subsequent requests over a persistent connection\n\nThis gets us:\n\n 5.1.1 Size of HTTP traffic transmitted in 500 byte scenario\n\n                           in response  in\n                           bodies       headers   total \n    ---------------------+------------+---------+----------------\n    Without header reuse:     145 Mb     9.4 Mb   154.4 Mb (100.0%)\n    With header reuse:        145 Mb     3.7 Mb   148.7 Mb ( 96.3%)\n\n    Reuse saves:                         5.7 Mb            (  3.7%)\n\n\n 5.1.2 Response time in 500 byte scenario\n\nAmount of wait chains with a certain percentage of traffic decrease:\n\n           decrease %   amount\n           -----------+------------\n                   0     809    18%\n                 1-4     884    20%\n                 5-9     749    17%\n               10-19     980    22%\n               20-49     718    16%\n               50-       362     8%\n\nThus, request header reuse will lead to a noticeable speedup for 46% of\nall wait chains.\n\nI conclude that header reuse becomes moderately interesting _IF_ we\nfind a good reason use request messages which contain a large (>460\nbytes) amount of reusable headers.\n\n\n 5.1.2 Comparison between Section 4 and 500 byte scenario\n ---------------------------------------------------------\n\nTraffic generated:\n                                    in response  in\n                                    bodies       headers   \n    -------------------------------+------------+---------\n\n    Section 4 without header reuse:  145 Mb       5.3 Mb \n    Section 4 with header reuse:     145 Mb       3.3 Mb\n    500 byte without header reuse:   145 Mb       9.4 Mb\n    500 byte with header reuse:      145 Mb       3.7 Mb \n\n\nAmount of wait chains with a certain percentage of traffic decrease,\nwhen going from Section 4 _without_ reuse to 500 byte _with_ reuse:\n\n           decrease %   amount\n           -----------+------------\n               - -21     121     3%\n           -20 - -11     189     4%\n           -10 -  -6     198     4%\n            -5 -  -1     442    10%\n             0 -   4    2088    46%\n             5 -   9     784    17%\n            10 -  19     356     8%\n            20 -         324     7%\n\n(7% of wait chains get noticeably slower, 15% get noticeably faster)\n\n\n 6. RANDOM STATISTICS\n -------------------\n\nThe statistics below are not very relevant for deciding about reuse,\nbut they are nice to have anyway.\n\nAmount of proxy<->server responses with a certain response body size:\n\n    body size (bytes)  amount cumulative amount\n    ------------------+------+-----------------\n                0-99     4%     4%\n             100-199     4%     8%\n             200-499     8%    16%\n             500-999     9%    25%\n           1000-1999    19%    44%\n           2000-4999    25%    69%\n           5000-9999    16%    85%\n         10000-19999     7%    92%\n         20000-49999     6%    97%\n         50000-99999     2%    99%\n        100000-          1%   100%\n\n\nAmount of persistent proxy<->server connections over which a certain\nnumber of HTTP transactions are made (the connections have a timeout\nof 10 minutes):\n\n- on average, one persistent connection gets 9.2 transactions.\n\n    # of transactions   amount     cumulative amount\n    ------------------+-----------+-----------------         \n                   1    415   26%     26%\n                   2    214   14%     40%\n                   3    169   11%     50%\n                   4    118    7%     58%\n                 5-6    148    9%     67%\n                 7-9    134    8%     76%\n               10-19    198   12%     88%\n               20-49    139    9%     97%\n               50-       49    3%    100%\n\n\n(End of document.)\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "That was a very nice study.  I'm sure people will find things to\nquibble about with your modelling of \"wait chains\", but it really\ndoes help to have some solid numbers.\n\nYou write:\n    Much higher gain/effort ratios can be had by focusing on other\n    desirable features of future HTTP software, for example\n    \n     [...]\n     - reducing the amount of Accept headers generated by some browsers\n       (my Mosaic for X browser sends 822 bytes of accept headers, most of\n       them for MIME types I can't even view!), maybe introducing a\n       mechanism for reactive content negotiation at the same time.\n\nI think Larry Masinter's hash-based approach still seems like the\nright one here.\n\nYou write:\n    Note that the four Accept headers above could be combined into a\n    single Accept header:\n    \n      Accept: */* image/gif image/x-xbitmap image/jpeg .\n\nI suggest that the HTTP 1.1 spec encourage this, changing the phrase\n\n The field may be folded onto several lines\n\nto\n\n The field SHOULD be folded onto several lines\n\nif that hasn't been done already.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Revised Charte",
            "content": "Its time to revise the charter for the HTTP working group, to bring it\ninto line with the group's planned activities.  The following reflects\nmy understanding, as HTTP-WG chairman, of our current work commitments.\nTo avoid us slipping into bottomless discussions about life, the universe\nand everything, can you please limit your comments on the charter revisions\nto things that the group has already discussed.\n\nThe existing (and sadly out of date) charter is at:\n\n   http://www.ietf.cnri.reston.va.us/html.charters/http-charter.html\n\nNow that HTTP 1.0 is done, We are currently focussed on two major areas: \nHTTP 1.x and HTTPng.  HTTP 1.x is work on extending the current MIME\nbased protocol, while HTTPng seeks to replace this with a much more\nefficient protocol, with a smooth transition between the two.\n\nCurrent Milestones:\n\n    o   HTTP 1.1 Internet Draft during November 1995\n    o   HTTPng Internet Draft + code and test data for March 1996\n\nAdditional milestones are needed for revising the Digest Access\nAuthentication proposal, and for work on HTTP 1.2 (see below).\n\nThe HTTP 1.1 work is limited to:\n\n   1) Persistent, stateless connections when client asks and server agrees\n      o Connection, Keep-Alive, Transfer-Encoding, valid Content-Length\n\n   2) Cache control and a consistent caching algorithm\n      o Cache-Control, URI, better descriptions\n\n   3) Preemptive content negotiation\n      o Accept, Accept-Encoding, Accept-Language, Accept-Charset\n\n   4) Reactive content negotiation\n      o 300, 406 responses\n\n   5) Protocol switch header\n      o Upgrade\n\n   6) Methods to support client-server capability negotiation\n\n   7) Methods to support user agent authoring and collaboration\n      o PUT, DELETE, LINK, UNLINK, LOCK, GETLOCK, UNLOCK, PATCH\n\n   8) A method to support message encapsulation\n      o WRAPPED\n\n   9) Digest Authentication - the Digest Access Authentication\n      proposal will be revised for submission as a candidate for\n      an experimental RFC. Third party digest authentication\n      will be deferred to future work\n\n  10) Proxy improvements\n      o Forwarded, Proxy-Authenticate and Proxy-Authorization\n      o 407 response, and a new response for \"Use Proxy\"\n\n  11) Defined semantics for metainformation\n      o Title, Link, Base, Content-MD5, Content-Language, etc.\n\n  12) Support for resource versioning\n      o Lock, Version, Derived-From, Unless (a general-purpose IMS)\n\nHTTP 1.1 is intended as a fast track specification. Any features\nthat fail to make the cut for 1.1 will be considered for an HTTP 1.2\nspecification. This is likely to include work on standard mechanisms\nfor extending and upgrading HTTP transactions.\n\nThe HTTPng work is initially limited to:\n\n   1) Fixing HTTP's poor performance over low bandwidth and/or\n      high latency connections\n\n   2) Providing greater control over delivery of information between\n      clients and servers, including out of order delivery of data and\n      much more efficient content negotiation facilities.\n\nHTTPng achieves this aim by a compact binary encoding and by encoding of\nheader information into contexts that are preserved across requests, and a\ndesign which allows for streaming of requests and responses.  Out of order\ndelivery and priority control is enabled by using a multiplexing transport,\nwith an initial implementation of the protocol over TCP.\n\n-- Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 8682\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/hypertext/WWW/People/Raggett\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Mitra writes:\n>Hmm - maybe I'm missing something, but I dont think you can put the file in\n>WITHOUT encoding, if you are looking for a boundary, what if the file\n>contained the wrong bytes and got interpreted as the boundary.\n\nActually, it should be easy to find the boundary if we make content-length a\nrequired field.  Since it's easy to tell where the content starts, you know\nthat any sequence of bytes before you've reached content-length isn't a real\nboundary.  (For that matter, you could probably make due without a boundary\nat all, by assuming that the next item began immediately after the first.)\n\n--\nJim Seidman\nSenior Software Engineer\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Agenda for Dallas IETF meetin",
            "content": "Please can you let me have your input for agenda items for the Dallas IETF\nmeeting as a matter of urgency.  So far I am intending to devote most\nof the time to discussing HTTP 1.1.  I would also like to include a status\nreport on HTTPng and plans to revise the Digest Access Authentication scheme.\n\nIt has been suggested that a single two hour slot on Monday morning won't\nbe sufficient.  There is a possibility of some additional time on Monday\nevening, but we need to pin down the agenda needs quickly if we are to be\ngranted this additional time.\n\n-- Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 8682\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/hypertext/WWW/People/Raggett\n\n\n\n"
        },
        {
            "subject": "Revised Charte",
            "content": "Dave Raggett writes:\n > \n > Its time to revise the charter for the HTTP working group, to bring it\n > into line with the group's planned activities.  The following reflects\n > my understanding, as HTTP-WG chairman, of our current work commitments.\n...\n\nThe list looks pretty good to me, but I'd also like to see the following\non it:\n\n- state management  (for example, the state-info proposal by Dave Kristol)\n\n- \"end-user\" extensibility -- there are inconsistencies that need resolving\n\n- alternative \"conditional GET\" mechanisms --\nfor example Jeff Mogul's \"modest proposal\".\n\nI also think this group shouldn't do one more single thing until\nall issues related to caching are completely nailed down.\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "Jeffrey Mogul writes:\n> That was a very nice study.  I'm sure people will find things to\n> quibble about with your modelling of \"wait chains\", but it really\n> does help to have some solid numbers.\nArgee with one exeption:\nKoen's modelling does not contain Accept-Language, which will be important\nin future, adding some bytes to headers.\nCurrently only Lynx (2.4.2) supports Accept-Language from browsers which I use.\n(I not tested the latest Mosaic for Linux, I not support ELF executables yet.)\nLynx adds \nAccept-Language: en; q=1\nAccept-Language: *; q=0.1\nwhich should be folded as:\nAccept-Language: en; q=1, *; q=0.1\nbut really I want to see\nAccept-Language: hu; q=1, en; q=0.75, ru; q=0.5, de;q=0.25\n(60 bytes, including CRLF)\nbut unfortunately Lynx doesn't support that. (NOTE: I don't know, what is\nthe proper language tag for russian, I assumed \"ru\" but it is only a (bad) guess.)\nOthers may have a significantly longer list of languages.\nWhile browsers and servers have only minimal support for Accept-Language, we\nshouldn't expect too much URIs having variants in many languages, but I expect\nmore and more in the future. If a software vendor supports multiple languages \nin its products, it will be willing to run a multi-language web server.\n> You write: [Koen Holtman]\n>     Much higher gain/effort ratios can be had by focusing on other\n>     desirable features of future HTTP software, for example\n>     \n>      [...]\n>      - reducing the amount of Accept headers generated by some browsers\n>        (my Mosaic for X browser sends 822 bytes of accept headers, most of\n>        them for MIME types I can't even view!), maybe introducing a\n>        mechanism for reactive content negotiation at the same time.\n> \n> I think Larry Masinter's hash-based approach still seems like the\n> right one here.\nAgree.\n> You write: [Koen Holtman]\n>     Note that the four Accept headers above could be combined into a\n>     single Accept header:\n>     \n>       Accept: */* image/gif image/x-xbitmap image/jpeg .\n> \n> I suggest that the HTTP 1.1 spec encourage this, changing the phrase\n> \n>  The field may be folded onto several lines\n> \n> to\n> \n>  The field SHOULD be folded onto several lines\n> \n> if that hasn't been done already.\nAgree.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "    >Date: Wed, 1 Nov 1995 03:18:08 +0100 (MET)\n    >From: \"Balint Nagy Endre\" <bne@bne.ind.eunet.hu>\n    >\n    >but unfortunately Lynx doesn't support that. (NOTE: I don't know, what is\n    >the proper language tag for russian, I assumed \"ru\" but it is only a\n    >(bad) guess.) Others may have a significantly longer list of languages.\n\nRFC 1766 (which specifies the structure of the value of ACCEPT-LANGUAGE\nand other headers which take language tag values) specifies that 2 character\nlanguage tags follow ISO 639 for which see:\n\n    http://www.stonehand.com/unicode/standard/iso639.html\n\nHowever, since ISO 639 contains only 136 tags, the authors of the HTML I18N\nI-D have specified an extension such that three character language tags\nfollow the language identifiers specified by the Ethnologue, 12th Edition,\nwhich contains 6790 3-character tags, for which, see:\n\n    http://www.stonehand.com/unicode/standard/ethn12.html\n\nFor more information on the Ethnologue, see:\n\n    gopher://sil.org:70/11/gopher_root/ethnologue/\n\nI would recommend that the HTTP specifications also be amended to specify\nthe same convention for using three character language identifiers since\nthis convention will be employed by HTML in specifying the value of the LANG\nattribute.\n\nRegards,\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re:  Revised Charte",
            "content": "\"Dave Raggett\" <dsr@w3.org> wrote:\n  [...]\n  > The HTTP 1.1 work is limited to:\n  [...]\n  >    9) Digest Authentication - the Digest Access Authentication\n  >       proposal will be revised for submission as a candidate for\n  >       an experimental RFC. Third party digest authentication\n  >       will be deferred to future work\nIs \"third-party digest...\" the same as \"mediated digest authentication\"?\nIf so, could we keep the naming consistent, please?\n\nI saw no reference to \"Host:\" or \"Orig-URI\" (or whatever the current\nconsensus name/function is).  I hope that was simply an omission.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "Balint Nagy Endre:\n>Koen's modelling does not contain Accept-Language, which will be important\n>in future, adding some bytes to headers.\n[...]\n>Accept-Language: hu; q=1, en; q=0.75, ru; q=0.5, de;q=0.25\n>(60 bytes, including CRLF)\n\nWell, 200 bytes plus 60 bytes for Accept-Language is still a lot lower than\nthe 500 byte request messages at which header reuse would get moderately\ninteresting.\n\nAnd of course, there is no need to ever send Accept-Language headers at all\nis we have a good(*) specification and implementation of reactive content\nnegotiation.\n\n(*) By my defition of good, that is: I feel that good reactive negotiation\nwill always give me a `multiple options' or `none acceptable' with the\nalternatives if my accept headers are missing or ambiguous.\n\nA browser should only send Accept-language headers if it suspects that doing\nso will prevent a reactive content negotiation cycle.  Thus, you start\nsending Accept-language if you got a `multiple options' response to an\nearlier request on that server, and it turned out that language was relevant\nin the options.\n\nAlso, in a hash-based content negotiation scheme, the Accept-language header\ncould be hashed along with the Accept header.\n\nBasically, everything I said about Accept: im my report also holds for\nAccept-language: .\n\n>[Jeffrey Mogul writes:]\n>> I think Larry Masinter's hash-based approach still seems like the\n>> right one here.\n\nBalint Nagy Endre:\n>Agree.\n\nI do not like the hash-based approach very much.  It seems to me that it is\na special case of reactive negotiation.\n\nOne of the problems I have with hash-based content selection is that it is\nthat it places a high penalty (one round trip time) on the first content\nselection by the server.  That means that putting up a home page with inline\nlinks that serve .gifs in the normal case, but .jpgs if the browser can\ndisplay them is less attractive.\n\nUner normal reactive negotiation, you could send either\n Accept: image/gif image/jpg\nor\n Accept: image/gif\nand no extra round trip would be required.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: Revised Charte",
            "content": "> From http-wg-request@cuckoo.hpl.hp.com Tue Oct 31 16:36:59 1995\n> Date: Tue, 31 Oct 1995 14:25:06 -0800\n> From: Shel Kaphan <sjk@amazon.com>\n> To: \"Dave Raggett\" <dsr@w3.org>\n> Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n> Subject: Revised Charter\n> In-Reply-To: <199510312200.AA232556813@w3.org>\n> References: <199510312200.AA232556813@w3.org>\n> Sender: http-wg-request@cuckoo.hpl.hp.com\n> \n> Dave Raggett writes:\n>  > \n>  > Its time to revise the charter for the HTTP working group, to bring it\n>  > into line with the group's planned activities.  The following reflects\n>  > my understanding, as HTTP-WG chairman, of our current work commitments.\n> ...\n> \n> The list looks pretty good to me, but I'd also like to see the following\n> on it:\n> \n> - state management  (for example, the state-info proposal by Dave Kristol)\n<Rest of Shel's message cut>\n\nI'd like to second the addition of state management.  Our initial development\nplans for sessions (or state) included support of Netscape's cookie, however\nI've been informed by someone who watches www-talk much more closely than I do,\nthat there is some debate over whether the Netscape implementation is the way\nit should be handled.\n\nIf the group can come to some agreement on how sessions and states should be\nhandled, we'll implement it (on the server side) early '96.\n\n-- \nElizabeth(Beth) Frank\nTechnical Manager\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "In a previous episode...Beth Frank said:\n-> > \n-> > Dave Raggett writes:\n-> >  > \n-> >  > Its time to revise the charter for the HTTP working group, to bring it\n-> >  > into line with the group's planned activities.  The following reflects\n-> >  > my understanding, as HTTP-WG chairman, of our current work commitments.\n-> > ...\n-> > \n-> > The list looks pretty good to me, but I'd also like to see the following\n-> > on it:\n-> > \n-> > - state management  (for example, the state-info proposal by Dave Kristol)\n-> <Rest of Shel's message cut>\n-> \n-> I'd like to second the addition of state management.  Our initial develop\n\nWhile extraneous 'me toos' are annoying I'm going to vocalize my\nsupport for including state identification in the 1.1 document. \nThere has been substantial discussion of this already and some Dave's\nproposals provided a good start for formalization, add that to NCSA's\ninterest and I think a 'the sooner the better' philosophy is in order.\n\n-Patrick McManus\nNYSERNet, Information Services\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "> \n> In a previous episode...Beth Frank said:\n> -> > \n> -> > Dave Raggett writes:\n> -> >  > \n> -> >  > Its time to revise the charter for the HTTP working group, to bring it\n> -> >  > into line with the group's planned activities.  The following reflects\n> -> >  > my understanding, as HTTP-WG chairman, of our current work commitments.\n> -> > ...\n> -> > \n> -> > The list looks pretty good to me, but I'd also like to see the following\n> -> > on it:\n> -> > \n> -> > - state management  (for example, the state-info proposal by Dave Kristol)\n> -> <Rest of Shel's message cut>\n> -> \n> -> I'd like to second the addition of state management.  Our initial develop\n> \n> While extraneous 'me toos' are annoying I'm going to vocalize my\n> support for including state identification in the 1.1 document. \n> There has been substantial discussion of this already and some Dave's\n> proposals provided a good start for formalization, add that to NCSA's\n> interest and I think a 'the sooner the better' philosophy is in order.\n> \n> -Patrick McManus\n> NYSERNet, Information Services\n> \n \nWhere would I find the past discussion of state management and Dave's\nproposals?  I'm going to writing up an internal discussion of our requirements\nfor managing state and sessions and I'd like review other relevant information.\n\nThanks in advance,\n\nBeth\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Dave Kristol:\n> \n> \"Daniel W. Connolly\" <connolly@hal.com> wrote:\n> \n>   > Deploying MGET/multipart looks to me like:\n>   [a list of steps for clients, servers, including...]\n>   * A few information providers maybe start using it\n>   (It's 3 months into the future by now)\n> \n>   > \n>   > Meanwhile, commercial folks are implementing HTTP-NG at lightning\n>   > speed. Six months from now, all the major vendors are doing\n>   > interoperable compression and encryption over something like SCP or\n>   > SSL (not to mention strong authentication).\n> \n> Sorry, I'm skeptical about this statement.  At least some of the\n> proposals for MGET/multipart and keep-alive are compatible with what\n> exists now.  For example, a client could attempt to send an MGET to a\n> server.  If the server chokes, the client can revert to a series of\n> regular GETs.\n> \n...\n> \n> So, I think vendors are less likely to switch to HTTP-NG in three\n> months, a protocol still being experimented with and IMO not quite\n> ready for prime time, than they are to adopt the MGET stuff.\n\nCorrect me if I am wrong, but I concluded from Spero's postings that\nnothing currently proposed including MGET, hold-open, or even HTTP-NG\nwould improve (or even match?) the user's perceived performance\ncurrently given by Netscape.  By this I mean the ellapsed time until\nthe user can start reading *all the text* and the ellapsed time until\nthe user can jump to a new link.\n\nIt seems to me that this \"user's perceived performance\" or UPP is going\nto be the dominant consideration for commercial client developers.  If\nthey can't match Netscape they simply won't be viable.  Accordingly I\nstrongly suspect that in six months all the major vendors will be doing\nwhat Netscape is doing today.  I don't see that they really have a choice.\n\nAnd I don't see this as really bad.  I know the Netscape technique will\nput a heavier load on network bandwidth, and maybe will stress some \nservers.  As Spero pointed out there are many aborted connections as\nusers jump to a new document without waiting for the current one to\ncompletely download.  But all that is the price we pay for quality service\n(from the user's point of view).\n\nI guess the bottom line is that there is not much point in changing\nHTTP unless the resulting protocol can (1) at least match the Netscape\nUPP, and (2) simultaneously significantly improve network efficiency.\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "RE: Revised Charte",
            "content": "At 2:06 PM 11/1/95, Beth Frank wrote (quoting Shel):\n>> The list looks pretty good to me, but I'd also like to see the following\n>> on it:\n>>\n>> - state management  (for example, the state-info proposal by Dave Kristol)\n><Rest of Shel's message cut>\n>\n>I'd like to second the addition of state management.  Our initial development\n>plans for sessions (or state) included support of Netscape's cookie, however\n>I've been informed by someone who watches www-talk much more closely than I do,\n>that there is some debate over whether the Netscape implementation is the way\n>it should be handled.\n\nI certainly agree that state-info should be in there, and I _very_ strongly\nagree that Dave Kristol's proposal is a far superior implementation to\ncookies.  Most of my complaints with the cookie proposal have to do with\nprivacy, which has been discussed at length on www-talk.  A number of\ncookie implementations have had serious privacy problems; and I have yet to\nsee an implementation that gives the user any clue as to what the hell it's\ndoing.\n\nSince Dave's proposal makes very reasonable efforts to reduce no-caching,\nand since the whole proposal makes session-munged URLs unnecessary, this\nissue falls well within the scope of Shel's closing statement:\n\n>I also think this group shouldn't do one more single thing until\n>all issues related to caching are completely nailed down.\n\n[Back to Beth:]\n>If the group can come to some agreement on how sessions and states should be\n>handled, we'll implement it (on the server side) early '96.\n\nThat would be great.  Any server supporting CGI can be made to use\nState-info through scripts, so if clients start recognizing it, the rest of\nthe implementation can be done pretty quickly.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "At 2:32 PM 11/1/95, Beth Frank wrote:\n>Where would I find the past discussion of state management and Dave's\n>proposals?  I'm going to writing up an internal discussion of our requirements\n>for managing state and sessions and I'd like review other relevant information.\n\nSee <URL:http://www.research.att.com/~dmk/session.html>.  Past discussion\nis scattered throughout the http-wg mailing list archives\n<URL:http://www.ics.uci.edu/pub/ietf/http/> and various www-* lists.  I\nthink Dave has done a good job of summarizing the issues in his draft --\nreading the mailing list archives might be more trouble than it's worth.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "M. Hedlund wrote:\n> \n> At 2:06 PM 11/1/95, Beth Frank wrote (quoting Shel):\n> >> The list looks pretty good to me, but I'd also like to see the following\n> >> on it:\n> >>\n> >> - state management  (for example, the state-info proposal by Dave Kristol)\n> ><Rest of Shel's message cut>\n> >\n> >I'd like to second the addition of state management.  Our initial development\n> >plans for sessions (or state) included support of Netscape's cookie, however\n> >I've been informed by someone who watches www-talk much more closely than I do,\n> >that there is some debate over whether the Netscape implementation is the way\n> >it should be handled.\n> \n> I certainly agree that state-info should be in there, and I _very_ strongly\n> agree that Dave Kristol's proposal is a far superior implementation to\n> cookies.  Most of my complaints with the cookie proposal have to do with\n> privacy, which has been discussed at length on www-talk.  A number of\n> cookie implementations have had serious privacy problems; and I have yet to\n> see an implementation that gives the user any clue as to what the hell it's\n> doing.\n> \n> Since Dave's proposal makes very reasonable efforts to reduce no-caching,\n> and since the whole proposal makes session-munged URLs unnecessary, this\n> issue falls well within the scope of Shel's closing statement:\n> \n\nDave's proposal does nothing to solve the privacy issues that\ncookies bring up.  But it does significantly reduce the capibilities.\nIn fact, it reduces the capibilities such that it is nearly\nunusable for large scale applications such as online shopping.\nA simple session ID requires the server to hold all the state\ninformation.  This leads to the need for large database lookup's\nand distributed database problems.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "  > From research!cuckoo.hpl.hp.com!http-wg-request  Wed Nov  1 16:44:42 1995\n  > Received: from research by allegra.tempo.att.com; id AA14310; Wed, 1 Nov 95 16:44:35 EST\n  > Received: by research.att.com; Wed Nov  1 16:40 EST 1995\n  > Received: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Wed, 1 Nov 1995 21:35:02 GMT\n  > Received: from http-wglistexploder by cuckoo.hpl.hp.com\n  > (1.37.109.16/15.6+ISC) id AA120731617; Wed, 1 Nov 1995 21:33:37 GMT\n  > From: efrank@ncsa.uiuc.edu (Beth Frank)\n  > Message-Id: <9511012132.AA08535@void.ncsa.uiuc.edu>\n  > Subject: Re: Revised Charter\n  > To: mcmanus@nysernet.org (Pat McManus)\n  > Date: Wed, 1 Nov 1995 15:32:29 -0600 (CST)\n  > Cc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\n  > In-Reply-To: <199511012120.QAA11914@pat.nyser.net> from \"Pat McManus\" at Nov 1, 95 04:20:13 pm\n  > X-Mailer: ELM [version 2.4 PL23]\n  > Content-Type: text/plain; charset=US-ASCII\n  > Sender: http-wg-request@cuckoo.hpl.hp.com\n  > Status: R\n  > \n  > > \n  > > In a previous episode...Beth Frank said:\n  > > -> > \n  > > -> > Dave Raggett writes:\n  > > -> >  > \n  > > -> >  > Its time to revise the charter for the HTTP working group, to bring it\n  > > -> >  > into line with the group's planned activities.  The following reflects\n  > > -> >  > my understanding, as HTTP-WG chairman, of our current work commitments.\n  > > -> > ...\n  > > -> > \n  > > -> > The list looks pretty good to me, but I'd also like to see the following\n  > > -> > on it:\n  > > -> > \n  > > -> > - state management  (for example, the state-info proposal by Dave Kristol)\n  > > -> <Rest of Shel's message cut>\n  > > -> \n  > > -> I'd like to second the addition of state management.  Our initial develop\n  > > \n  > > While extraneous 'me toos' are annoying I'm going to vocalize my\n  > > support for including state identification in the 1.1 document. \n  > > There has been substantial discussion of this already and some Dave's\n  > > proposals provided a good start for formalization, add that to NCSA's\n  > > interest and I think a 'the sooner the better' philosophy is in order.\n  > > \n  > > -Patrick McManus\n  > > NYSERNet, Information Services\n  > > \n  >  \nefrank@ncsa.uiuc.edu (Beth Frank) wrote:\n  > Where would I find the past discussion of state management and Dave's\n  > proposals?  I'm going to writing up an internal discussion of our requirements\n  > for managing state and sessions and I'd like review other relevant information.\n\nI found www-talk archived at\nhttp://www.eit.com/cgi-bin/textit/goodies/lists/www.lists/index.html\nMost of the action took place July-Sept 1995.\n\nYou can find information about the State-Info proposal at\nhttp://www.research.att.com/~dmk/session.html\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "Lou Montulli writes:\n...\n > \n > Dave's proposal does nothing to solve the privacy issues that\n > cookies bring up.\n\nDoes anyone else's?  \n\n  But it does significantly reduce the capibilities.\n > In fact, it reduces the capibilities such that it is nearly\n > unusable for large scale applications such as online shopping.\n\nI don't see how that is true.  It looks pretty usable to me.\nIts main limitation is that you've got only one session with a given\nserver per browser-window, and that actually may be a good limitation.\n\n > A simple session ID requires the server to hold all the state\n > information.  This leads to the need for large database lookup's\n > \n\nThat may be true, but it also isn't what Dave proposes.\n\n > and distributed database problems.\n\nNot necessarily.\n\n > :lou\n > -- \n > Lou Montulli                 http://www.netscape.com/people/montulli/\n >        Netscape Communications Corp.\n\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "Lou Montulli <montulli@mozilla.com> wrote:\n  > Dave's proposal does nothing to solve the privacy issues that\n  > cookies bring up.  But it does significantly reduce the capibilities.\n  > In fact, it reduces the capibilities such that it is nearly\n  > unusable for large scale applications such as online shopping.\n  > A simple session ID requires the server to hold all the state\n  > information.  This leads to the need for large database lookup's\n  > and distributed database problems.\n\nThe State-Info proposal is not the session ID proposal.  The purpose of\nState-Info is for the client and server to exchange stateful session\ninformation, very much like cookies.  In particular, the point is to\navoid requiring the server to keep any state.\n\nI've been waiting for Netscape to submit their Cookie scheme for\nstandardization.  Would you like to take the bait?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "> I do not like the hash-based approach very much.  It seems to me that it is\n> a special case of reactive negotiation.\n\nIt is.\n\n> One of the problems I have with hash-based content selection is that it is\n> that it places a high penalty (one round trip time) on the first content\n> selection by the server.  That means that putting up a home page with inline\n> links that serve .gifs in the normal case, but .jpgs if the browser can\n> display them is less attractive.\n\nDon't use it for embedded gifs and jpgs. In particular, if your accept\nheaders are short (as they are for image/gif image/jpg) the hash would\nbe longer than the material hashed, a bad idea.\n\nI only proposed this as an optimization in the case where the Accept*\nheaders are large.\n\n\n\n"
        },
        {
            "subject": "State Wars, part XI (was: Revised Charter",
            "content": "Before I get started, let me second Dave Kristol's call for the Netscape\nCookie proposal to be submitted as an Internet-Draft.  I don't mean any of\nthe following to be a bashing of Netscape as a whole; I recognize that\nthere are cookie capabilities that other proposals gloss over or omit\nentirely; and though I think we've talked most of these subjects to death,\nI'm more than happy to go another round on www-talk if that's necessary to\nmeet the needs cookies address.\n\nI wrote:\n>> I certainly agree that state-info should be in there, and I _very_ strongly\n>> agree that Dave Kristol's proposal is a far superior implementation to\n>> cookies.  Most of my complaints with the cookie proposal have to do with\n>> privacy, which has been discussed at length on www-talk.  A number of\n>> cookie implementations have had serious privacy problems; and I have yet to\n>> see an implementation that gives the user any clue as to what the hell it's\n>> doing.\n\nLou Montulli replied:\n>Dave's proposal does nothing to solve the privacy issues that\n>cookies bring up.\n\nIf you think Dave's measures are insufficient, fine, let's talk about it;\nbut he devotes Section 6 of his proposal to privacy issues, while the\ncookie proposal doesn't even contain the word \"privacy,\" much less do\nanything to protect it.  To say he has \"done nothing\" about privacy issues\naround cookies and state-tracking is incorrect.\n\n>But [Dave's proposal] does significantly reduce the capibilities.\n\nWell, yes, privacy protection does tend to reduce the utility of tracking\nschemes.  To the extent that you're talking about session-info retrieval\nand storage, I will address that after discussing privacy issues, below.\n\nThere are two privacy concerns here: what the proposals say and what the\ncookie implementators have done.\n\nFirst, what the proposals say.  The key capability ignored by the cookie\nproposal and addressed by Dave's proposal is (emphasis added):\n\n>  3.  Either the _user agent_ or the origin server may terminate a\n>      session.\n\nThe cookie proposal completely ignores the user's possible desire to\ndisable, suspend, terminate, or reset session-tracking at any point.\nFurthermore, it makes no recommendations to user-agent developers as to how\nthey might provide methods for the user to protect his or her privacy.\nFinally, it makes a poor attempt to isolate session information to\nparticular domains through \"tail matching.\"  _All_ of these issues are\nbetter addressed -- in some cases, extensively -- by Dave's state-info\nproposal.\n\nThe cookie proposal also makes greater attempts to preserve state\ninformation beyond the scope that might reasonably be expected,\nparticularly with its use of expiration dates.  Since the same cookie can\nbe transmitted to a site indefintely (with a very far distant expiration),\ncookie sessions have no definite end.  This is in contrast to Dave's\nproposal, which terminates state information when the browser's execution\nis stopped.\n\nThe one attempt the cookie proposal makes to preserve privacy -- \"tail\nmatching\" of server domain names to prevent universal cookies -- is\ninsufficient.  The cookie proposal states:\n\n>Only hosts within the specified domain can set a cookie for a domain\n>       and domains must have at least two (2) periods in them to prevent\n>       domains of the form: \".com\" and \".edu\".\n\nSince a trailing period in a domain name is allowed, '.com.' is a legal\ndomain setting for a cookie that any .com site could retrieve..  (Recent\nimplementations of Netscape have addressed this, but the language is still\ninsufficient.)  Even if a trailing period were not legal, the 'two period'\nrule is only sufficient in some regions -- '.ac.uk' has two periods in it,\nas does '.com.au'.  Furthermore, there's no reason to believe that two\nservers in the same domain should share cookie information -- for instance,\ntwo servers at an ISP might be completely unrelated.  (Dave's proposal is\nbetter on this count by restricting info-sharing to a single server, which\nis probably as good as it can get.)\n\nThose are some issues of proposal language where Dave's proposal is, as I\nsaid, \"far superior\" to the cookie proposal in privacy protections.  Beyond\nthe language of the proposal, however, cookie implementations have shown\nthe the lack of privacy recommendations leads directly to privacy abuses.\n\nHere are some questions to consider about Netscape's implementation of cookies:\n\n1. Why doesn't the word \"cookie\" appear in the Netscape Online Handbook?\n2. Why isn't the cookie specification URL given in any README or\nimplementation notes file?  How are users supposed to know what information\nis being kept about them, or for how long?\n3. Why can't the user turn off the 'Cookie:' header in a preferences pane?\n4. Other than throwing out the cookie file entirely, how would a user reset\na cookie for a particular site or all sites?\n5. Why is the user cautioned in the cookie file \"# This is a generated\nfile!  Do not edit.\"?\n6. Why is there no visual indicator of cookie transmission in the\nnavigator, similar to the \"secure key\" and blue security color bar?\n7. In the Mac version, why did the \"MagicCookie\" file change from filetype:\nTEXT (which  could be opened and read with any text editor) in 1.x to\nfiletype: COOK (which most editors don't recognize) in 2.x?  Do you realize\nthat this further hampers the users' ability to be aware of state\ninformation?\n\n(3) - (6) above, at the very least, are privacy conerns raised by the\ncookie proposal and addressed by Dave's proposal.  I think it unfair to say\nDave's proposal \"does nothing to solve the privacy issues that cookies\nbring up.\"  The cookie proposal is silent about these issues, and\nNetscape's implementation has shown that it should not be.\n\nTurning away from privacy, to Lou's other concerns:\n\n>In fact, [Dave's proposal] reduces the capibilities such that it is nearly\n>unusable for large scale applications such as online shopping.\n\nI disagree (as a programmer for an online shopping site).  I do agree that\nan extremely large online shopping site with many stores or large ordering\npossibilities might hit an upper limit with the state-info proposal if they\ntried to store all ordering info in the State-info: header.  This concern,\nhowever, is not pressing for the vast majority of ordering systems, which\naccumulate a small number of items in each session.  I would like to see\nthe State-info proposal incorporate the cookie proposal's concept of\n\"path\", which I think would solve this problem.\n\n>A simple session ID requires the server to hold all the state\n>information.  This leads to the need for large database lookup's\n>and distributed database problems.\n\nOnly if the ordering information would exceed a reasonable limit for the\nState-info header.  I disagree that this makes State-info \"nearly unusable\"\nfor shopping carts, and I think we could solve this by adding \"path\" to the\nState-info response header.\n\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Language tags (Re: Statistics on reusing request",
            "content": "Glenn,\nthanks for reminding me....\n\nI object strongly to the paragraph in I18N that says:\n\n\n   Two-letter primary-tags are reserved for ISO 639 language abbrevia-\n   tions [ISO-639], and three-letter primary-tags for the language\n   abbreviations of the \"Ethnologue\" [ETHNO] (the latter is in addition\n   to the requirements of RFC 1766). Any two-letter initial subtag is an\n   ISO 3166 country code [ISO-3166].\n\nThe reason is what I stated in RFC 1766:\n\n\n   The reason for reserving all other tags is to be open towards new\n   revisions of ISO 639; the use of \"i\" and \"x\" is the minimum we can do\n   here to be able to extend the mechanism to meet our requirements.\n\nIf you wish to register I-SIL-nnn as a standard for three-letter\nEthnologue-based tags, or even want to push for updating RFC 1766 to include S-nnn as a new category, I would not argue against that, but I would like to stick to the principle of using ISO standards for the basic namespace.\n\nOtherwise, we will end up with a really confusing situation once the\nISO 3166 three-letter project finishes (if it ever does); its tags are\nSURE to conflict with the SIL tag.\n\n         Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    From: Harald.T.Alvestrand@uninett.no\n    Date: Thu, 02 Nov 1995 11:33:53 +0100\n\n    If you wish to register I-SIL-nnn as a standard for three-letter\n    Ethnologue-based tags, or even want to push for updating RFC 1766 to\n    include S-nnn as a new category, I would not argue against that, but\n    I would like to stick to the principle of using ISO standards for\n    the basic namespace.\n\nSince IANA does not otherwise use such a principle I don't know why you\nwould adopt it here, let alone insist on it.\n\n    Otherwise, we will end up with a really confusing situation once the\n    ISO 3166 three-letter project finishes (if it ever does); its tags are\n    SURE to conflict with the SIL tag.\n\nEven more incongruent does your \"principle\" appear given the laggardness\nof this particular ISO work item.  It is extremely unlikely that ISO or\nanyone else for that matter will do as comprehensive a job as SIL has done\nin creating their language database.\n\nI was rather surprised to learn that you did not even know about the\nEthnologue database prior to writing your RFC.  Let's just forget about 3166\nand use what exists, namely 639 for 2 letter codes and the Ethnologue for\nthree letter codes.  Unless you can give a firm estimate of when (or if)\nISO is going to actually produce a revision to 639, then your objection\nsurely sounds quite empty to me.\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "Larry Masinter:\n\n[...hash based approach to content negotiation...]\n\n>Don't use it for embedded gifs and jpgs. In particular, if your accept\n>headers are short (as they are for image/gif image/jpg) the hash would\n>be longer than the material hashed, a bad idea.\n>\n>I only proposed this as an optimization in the case where the Accept*\n>headers are large.\n\nIf it is an optional optimization, I have no problems at all with it.\n\nI don't believe I ever saw a complete description of how hashing would\nfit in with the rest of the negotiation stuff.  Is there such a\ndescription, or do you plan to write one?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "John Franks writes:\n>Correct me if I am wrong, but I concluded from Spero's postings that\n>nothing currently proposed including MGET, hold-open, or even HTTP-NG\n>would improve (or even match?) the user's perceived performance\n>currently given by Netscape.  By this I mean the ellapsed time until\n>the user can start reading *all the text* and the ellapsed time until\n>the user can jump to a new link.\n\nI have to disagree here.  If the issue is how quickly all the text can be\ndownloaded, then the fastest way to do this is to send only the text and\nthen pass on the images later.  Especially for users on slower connections\n(such as the ever-mentioned 14.4 SLIP connection) there is a notable\ndegradation in getting all of the text if several images are being sent\nsimultaneously.\n\nThere are several other reasons why multiple simultaneous connections a bad\nidea.  As you pointed out, there are many aborted connections.  When\ndownloading one item at a time you are much more likely to have received\ncomplete items which you can cache.  (For example, if you are downloading a\npage with 15 images which are reused on other pages throughout the site, you\nmight at least get a few of them completely, rather than getting several\nuseless partial images which you'll then reload from scratch when they\nappear on the next page.)\n\nIn my mind the only defensible reason for simultaneous connections is to\nreduce the round trip time penalty for loading all the pieces of a document.\nHowever there are already several discussions taking place here about other\nways to minimize the RTT issues.  Even something as trivial as having a\nkeep-alive and allowing the client to buffer up requests for images as it's\ndownloading the text would provide a major improvement.\n\n--\nJim Seidman\nSenior Software Engineer\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "Dave Raggett writes:\n[...]\n >   11) Defined semantics for metainformation\n >       o Title, Link, Base, Content-MD5, Content-Language, etc.\n                              ***********\nSorry If I'm completly off base but could this be changed\ninto \"Content-Checksum:\". I'd rather not tie MD5 particular\n*algorithm* to the general 'checksum' *functionality*, (just in case\nMD6 pops out in few monthes for instance...) {Though I currently use\nMD5 as the algorithm currently}\n\n*** \"a la draft\" definition:\nContent-Checksum = \"Content-Checksum\" \":\" 1#(checksum)\n\nchecksum = checksum-algorithm \"=\" checksum-value\nchecksum-algorithm = \"MD5\" | extension-algo\n(you can define new checksums, like MD4, MD6 (!) unix sum,...)\nfor algorithm MD5 : checksum-value = 32 characters hex ascii coded MD5\nchecksum\n\nfor instance for a message content of \"this is a test\\n\"\nyou get :\n---------\nHTTP/1.0 200 Document follows\nServer: datasrv/dl2.6d99\nLast-Modified: Mon, 16 Oct 1995 15:42:06 GMT\nContent-Type: text/plain\nContent-Length: 15\nContent-Checksum: MD5=e19c1283c925b3206685ff522acfe3e6\n\nthis is a test\n---------\n***\n\nPs : yes I know there is an RFC about Content-MD5 Mime header, but it\nis wrong imo (because it is a too general thing to be tied to a\nparticular algorithm,...)\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\nSEAL Team 6 Greenpeace plutonium fissionable Chirac supercomputer\n Treasury\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": ">     If you wish to register I-SIL-nnn as a standard for three-letter\n>     Ethnologue-based tags, or even want to push for updating RFC 1766 to\n>     include S-nnn as a new category, I would not argue against that, but\n>     I would like to stick to the principle of using ISO standards for\n>     the basic namespace.\n\nIf there's even the slightest chance that multiple three letter code schemes\nwill exist in the future then the S-nnn approach seems like the way to go to\nme.\n\n> Since IANA does not otherwise use such a principle I don't know why you\n> would adopt it here, let alone insist on it.\n\nIncorrect. IANA does use such a principle elsewhere: In handling the\nregistration of top-level domains it follows ISO country codes. There was a lot\nof wailing about this policy at one point but as far as I know it has never\nbeen changed.\n\n>     Otherwise, we will end up with a really confusing situation once the\n>     ISO 3166 three-letter project finishes (if it ever does); its tags are\n>     SURE to conflict with the SIL tag.\n\n> Even more incongruent does your \"principle\" appear given the laggardness\n> of this particular ISO work item.  It is extremely unlikely that ISO or\n> anyone else for that matter will do as comprehensive a job as SIL has done\n> in creating their language database.\n\nThe problem as I see it is the adoption of a different set of criteria for\nlanguage tags here than what was used in the content-language work. I for one\ndon't especially care what we use as long as it is the same everywhere. I do\nNOT want to have two different language tag namespaces to contend with. In fact\nI'm not particularly happy with the notion of having overlapping subspaces, but\nI think that's inevitable.\n\n> I was rather surprised to learn that you did not even know about the\n> Ethnologue database prior to writing your RFC.  Let's just forget about 3166\n> and use what exists, namely 639 for 2 letter codes and the Ethnologue for\n> three letter codes.  Unless you can give a firm estimate of when (or if)\n> ISO is going to actually produce a revision to 639, then your objection\n> surely sounds quite empty to me.\n\nYou may not approve of it, but we have a proposed standard for language tags in\nplace -- RFC1766. And this is what HTTP needs to use. Defining another scheme\nspecifically for HTTP is not acceptable.\n\nNow, it may well be that RFC1766 is broken. If it is then we need to fix it.\nIts almost time for it to be up for review anyhow. It may be somewhat harder to\nfix RFC1766 than to invent an incompatible scheme for HTTP, but the difficulty\nof the task should not prevent us from doing the right thing.\n\nIn summary, the only thing here that \"sounds empty\" to me is the notion that\nits acceptable to have two different sets of language tags in different IETF\nwork items and that its acceptable avoid revising documents that need revision.\nGlenn may have an excellent case for putting the SIL codes in RFC1766. He may\neven have a case for putting the SIL codes in without an \"S-\" introducer and\nputting the introducer on the son-of-639 codes should they ever appear. If so,\nhe needs to bring this up with the WG that produced the content-language\nspecification -- the MAILEXT WG, I believe.\n\nIn fact it may even be possible to revise the tags specification and get a new\nversion of it out before the work on HTTP is finished. The MAILEXT WG has a\nproven track record of getting more work done faster than any other group I've\nbeen associated with.\n\nNed\n\n\n\n"
        },
        {
            "subject": "ContentMD",
            "content": "> Dave Raggett writes:\n> [...]\n >>   11) Defined semantics for metainformation\n >>       o Title, Link, Base, Content-MD5, Content-Language, etc.\n>                               ***********\n> Sorry If I'm completly off base but could this be changed\n> into \"Content-Checksum:\". I'd rather not tie MD5 particular\n> *algorithm* to the general 'checksum' *functionality*, (just in case\n> MD6 pops out in few monthes for instance...) {Though I currently use\n> MD5 as the algorithm currently}\n\nNo, we are not going to replace a well-defined proposed standard with\na newly-defined invention.  Using separate header fields for different\nalgorithms is just as applicable (and easier to parse) as any\nparameterized generic header field.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "Just a clarification.  HTTP is *not* going to use a language tag that\nis different than that defined by RFC 1766 (or it successors).  Any change\nproposed must first be made to that document, and that should be done\nwithin the MAILEXT WG (or through discussion with Harald, the author).\n\nIt is not a topic for discussion within the HTTP WG.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "> I don't believe I ever saw a complete description of how hashing would\n> fit in with the rest of the negotiation stuff.  Is there such a\n> description, or do you plan to write one?\n\nI'm not sure if content negotiation deserves a separate document from\nthe rest of HTTP, or whether all of the issues are independent. I\ndon't currently have any plans to write up this idea beyond what's\nbeen posted, unless there's a call for it.\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> > Dave Raggett writes:\n> > [...]\n>  >>   11) Defined semantics for metainformation\n>  >>       o Title, Link, Base, Content-MD5, Content-Language, etc.\n> >                               ***********\n> > Sorry If I'm completly off base but could this be changed\n> > into \"Content-Checksum:\". I'd rather not tie MD5 particular\n> > *algorithm* to the general 'checksum' *functionality*, (just in case\n> > MD6 pops out in few monthes for instance...) {Though I currently use\n> > MD5 as the algorithm currently}\n\n> No, we are not going to replace a well-defined proposed standard with\n> a newly-defined invention.  Using separate header fields for different\n> algorithms is just as applicable (and easier to parse) as any\n> parameterized generic header field.\n\nDraft standard, actually -- RFC1864.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "Glenn,\nwhat I desire is that we have a single decision that is valid\nevery time we want to indicate a language.\n\nIn this case, it means that if you think 1766 is broken, you should\nwork to change 1766, not introduce an incompatible naming scheme\nwithin the HTML I18N work.\n\n(You're not the only one - I also complained to the SRVLOC group\nabout their desire to use a fixed-length language field of FOUR\ncharacters. SIL codes would fit right in...)\n\nWhen we choose to rely on an outside source for names, we have to\nconsider:\n\n- Is the source available?\n- Is the source stable?\n- Is the source reliable?\n- Is the source policy acceptable to our members?\n\nIn the case of ISO, we might not like everything they are doing,\nbut its warts are something we have grown used to over the years.\n\nIn the case of SIL, I know that they chose to do their work for\na specific purpose (supporting and targeting the work of bible\ntranslation into new languages), but I do not know anything about\ntheir policies about changes to the language database, documentation\nof changes or reuse of deassigned identifiers.\n\nI don't say that these argue against SIL codes, just that we have\nto know what we are doing, and make sure we change the standard\nin the right place.\n\n               Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    Date: Thu, 02 Nov 1995 18:14:17 -0800\n    From: \"Roy T. Fielding\" <fielding@avron.ics.uci.edu>\n\n    Just a clarification.  HTTP is *not* going to use ...\n\nWow!  I wish I could say \"HTML is *not* going to use BLINK, FONT,\nmultiple BODY elements, ...\"  It would sure make life more easy.\n\nCome on Roy, whose crystal ball are you using?\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    Date: Thu, 02 Nov 1995 16:51:07 -0800 (PST)\n    From: Ned Freed <NED@innosoft.com>\n\n    Now, it may well be that RFC1766 is broken. If it is then we need to\n    fix it.\n\nI don't believe I said it was \"broken\".  I would simply say it is in\nneed of augmenting as described under the text for \"primary language tags\":\n\n  \"Other values cannot be assigned except by updating this standard.\"\n\n    In summary, the only thing here that \"sounds empty\" to me is the\n    notion that its acceptable to have two different sets of language tags\n    in different IETF work items and that its acceptable avoid\n    revising documents that need revision.\n\nOK, then lets update 1766.  I propose that 3 character primary language\ntags be those assigned by the Ethnologue, 12th edition, this list being\navailable here:\n\n    http://www.stonehand.com/unicode/standard/ethn12.html\n\nThis list contains 6790 language identifiers covering the entire known\nset of living and extinct languages.\n\nThe HTML I18N I-D has already proposed this extension due to the current\nlimited coverage of 2 character ISO 639 language tags.\n\nWould you like me to write up an I-D?\n\nRegards,\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    From: Harald.T.Alvestrand@uninett.no\n    Date: Fri, 03 Nov 1995 10:06:25 +0100\n\n    I don't say that these argue against SIL codes, just that we have\n    to know what we are doing, and make sure we change the standard\n    in the right place.\n\nWell, I don't know how pertinent it is to know about agendas here. We\ncould certainly question ISO in this regard also.  It seems to me that\nSIL's agenda here is quite irrelevant and that it is the end product of\ntheir agenda that is useful.  Namely, the language list and identifiers\nlist itself.  If we should make any judgements, it should be along the\nlines of comprehensiveness, which nobody can fault SIL for.\n\nI'd like to improve 1766 to make it more comprehensive.  You couldn't\nhelp it that 639 is so limited.  So I'm really not faulting 1766.  It\nseems it is important to recognize its limitations based on 639's limits\nand move on from there.  Given that the SIL list is available and a new\nlist from ISO is not forthcoming (its been stuck as a CD for over 5\nyears now with little activity), I think that we should move ahead and\nuse the SIL list.\n\nIf would be pleased to assist in improving 1766 so it can be a single,\ncomprehensive language tag standard.  Something I also want.\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> Correct me if I am wrong, but I concluded from Spero's postings that\n> nothing currently proposed including MGET, hold-open, or even HTTP-NG\n> would improve (or even match?) the user's perceived performance\n> currently given by Netscape.  By this I mean the ellapsed time until\n> the user can start reading *all the text* and the ellapsed time until\n> the user can jump to a new link.\n\nHTTPng will win over the multiple connections used by Netscape, as the\nlatter lead to congestion problems, since the connections don't share\ncongestion info. In addition, using a single connection gets around the\nslow start problem leading to better peformance. It will also be much\nnicer to servers!\n\nSimon explained all this in his notes.\n--\n Dave Raggett <dsr@w3.org> tel: +44 272 228046 fax: +44 272 228003\n  Hewlett Packard Laboratories, Filton Road, Bristol BS12 6QZ, United Kingdom\n\n\n\n"
        },
        {
            "subject": "Re: Statistics on reusing request headers in persistent connection",
            "content": "Larry Masinter:\n>\n>> I don't believe I ever saw a complete description of how hashing would\n>> fit in with the rest of the negotiation stuff.  Is there such a\n>> description, or do you plan to write one?\n>\n>I'm not sure if content negotiation deserves a separate document from\n>the rest of HTTP, or whether all of the issues are independent.\n\nOh, I was not thinking along the lines of a separate document, more\nalong the lines of a message on http-wg.  I just searched the http-wg\narchives, and the only thing I would find was a message from you\narchived as\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0674.html :\n\n|Did you miss the suggestion that clients hash all\n|'content-type-determining headers' and send them as a 'accept-hash:'\n|instead? I suppose I'm choosing Good and Fast, at the expense of a\n|little extra implementation complexity. One way to think of this is\n|that hashing is a kind of compression mechanism -- you can compress\n|any amount of data into 128 bits, but decompression can be very slow\n|and take a large amount of communication.\n\nI don't believe I ever saw the original suggestion referred to above,\nand I can't find it in the archive.  Perhaps it was sent in the\ntimeframe that the http-wg mail server was broken and only delivering\n50% of all messages on a good day?\n\nThings I would like to know is:\n \n- how does a server resolve a 'hash miss'?  \n\n- If a server gets\n  GET /blah/picture HTTP/1.0\n  Accept: image/gif;q=0.5\n  Accept-Hash: 4592462137846218\n\nand it has an image/gif and an image/jpg variant, must, may, or must\nit not resolve the hash to see if there is an image/jpg;q=1.0 in it?\n\n> I\n>don't currently have any plans to write up this idea beyond what's\n>been posted, unless there's a call for it.\n\nCould you check if your original accept-hash proposal is in the\nhttp-wg archives?  I could not find it, but that does not mean it is\nnot there.  If you can't find it either, I suggest you repost the\noriginal article (if you still have it) either now or at the time we\nstart discussing negotiation again.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "Roy T. Fielding writes:\n >  >>       o Title, Link, Base, Content-MD5, Content-Language, etc.\n > > Sorry If I'm completly off base but could this be changed\n > > into \"Content-Checksum:\". I'd rather not tie MD5 particular\n > > *algorithm* to the general 'checksum' *functionality*, (just in case\n > > MD6 pops out in few monthes for instance...) {Though I currently use\n > > MD5 as the algorithm currently}\nSo because there were a mere 70 lines document (headers dropped)\nwritten in 1993 (Rfc1544 (hmm, reissued with almost no changes as\nrfc1864 very recently)) *suggesting* the use of a *specific*\nalgorithm, for Email should imply that world stopped and that for a\nnew proposed protocol (http/1.1) you have to use the same buggy\nspecific definition ? No evolution possible ? No new ideas/fixes ever\nallowed ?\nAnd you make htpp/1.2 when MD6 or something else pops out ?\n > Using separate header fields for different\n > algorithms is just as applicable (and easier to parse) as any\n > parameterized generic header field.\nWhy not using Accept-Gif: Accept-jpeg:,... headers ? would be\neasier... \n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\n\n\n\n"
        },
        {
            "subject": "Re: State Wars, part XI (was: Revised Charter",
            "content": "M. Hedlund:\n>[Lou Montulli:]\n>>In fact, [Dave's proposal] reduces the capibilities such that it is nearly\n>>unusable for large scale applications such as online shopping.\n>\n>I disagree (as a programmer for an online shopping site).  I do agree that\n>an extremely large online shopping site with many stores or large ordering\n>possibilities might hit an upper limit with the state-info proposal if they\n>tried to store all ordering info in the State-info: header.  This concern,\n>however, is not pressing for the vast majority of ordering systems, which\n>accumulate a small number of items in each session.\n\nI agree.\n\n>  I would like to see\n>the State-info proposal incorporate the cookie proposal's concept of\n>\"path\", which I think would solve this problem.\n\n\"path\" may shift the upper limit to a higher value (but only if\nimplemented in a certain way), but it will not solve the problem of\nthere being an upper limit (determined by the browser, server, and\nproxy software) on the size of the State-Info header transmitted.\n\nIf your amount of state is larger than the upper limit, you will end\nup storing it in a server side database, using the contents of the\nState-Info header as a key into the database.\n\nOf course, this is not elegant.  A server cannot generally detect the\nend of a session, so it will have to use some timeout scheme to take\nno longer used key-state pairs out of the database after, say, 12\nhours.\n\nThe question is: is this solution to the large shopping bag problem\nfeasible?  I think the answer is yes.  The resource requirements for\nthis database solution are not higher than those of several things we\nalready have:\n\n1) Size of the database:\n\n- The database will always be very much smaller than the server access\n  log file, so that is not a problem.\n\n2) Time spent doing database lookups:\n\n- WWW servers already do (cached) database lookups for each request:\n  they map the IP address of the client to its internet host name for\n  use in the logfile.  Session state database lookups will have the\n  same time complexity and opportunities for caching.\n\n- WWW servers that use authentication also do database lookups for\n  each request:\n    - www.dds.nl has a database of 30,000 username/password\n      combinations accessed for each request.\n\n3) Time spent implementing the database\n\n- It was very easy to implement cached database lookups with timeouts \n  in my Futplex system.\n\n- If such databases turn out to be used often, I expect that either\n  someone will write a library, or that this functionality will become\n  a standard part of WWW servers.\n\n>M. Hedlund <hedlund@best.com>\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> So because there were a mere 70 lines document (headers dropped)\n> written in 1993 (Rfc1544 (hmm, reissued with almost no changes as\n> rfc1864 very recently)) *suggesting* the use of a *specific*\n> algorithm, for Email should imply that world stopped and that for a\n> new proposed protocol (http/1.1) you have to use the same buggy\n> specific definition ? No evolution possible ? No new ideas/fixes ever\n> allowed ?\n\nShort specs are better than long ones, and stable standards are better\nthan those which change frequently.  No bugs in that.\n\n> And you make htpp/1.2 when MD6 or something else pops out ?\n\nNo.  If you read the HTTP specifications, you will notice that the version\nnumber is not changed for additional message components which only add\nto extensible field values.  You will also notice that Entity-Header is\nan extensible field, and thus anyone may add Content-Fred at any time\nwithout changing the standard.\n\n >> Using separate header fields for different\n >> algorithms is just as applicable (and easier to parse) as any\n >> parameterized generic header field.\n> Why not using Accept-Gif: Accept-jpeg:,... headers ? would be\n> easier... \n\nUsing separate header fields for the *same* algorithm is a different matter.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "> OK, then lets update 1766.  I propose that 3 character primary language\n> tags be those assigned by the Ethnologue, 12th edition, this list being\n> available here:\n\n>     http://www.stonehand.com/unicode/standard/ethn12.html\n\n> This list contains 6790 language identifiers covering the entire known\n> set of living and extinct languages.\n\n> The HTML I18N I-D has already proposed this extension due to the current\n> limited coverage of 2 character ISO 639 language tags.\n\n> Would you like me to write up an I-D?\n\nNo. You need to bring this up in the proper WG (MAILEXT) as a suggested\nenhancement to RFC1766. There is no need for a separate, additional document.\nThis then needs to be discussed there and, if approved, I'm sure Harald will\nhave no problem with adding this to the next revision of RFC1766. As I said\nbefore, the mandatory six month review period for RFC1766 is nearly up so this\nis an ideal time to make such a change.\n\nThis isn't going to be totally trivial, however -- there are substantive\nquestions regarding the stability and update mechanisms for the SIL work that\nneed to be answered before its clear that the SIL codes can in fact be used. In\nparticular, a URL is *not* an adequate means of communicating such things --\nthe standards process currently requires either a publication to cite,\npublication of the list as an RFC (thus creating something that can be cited),\nor establishment of some form of registration authority along the lines of\nIANA. I do not believe a URL is an acceptable substitute.\n\nThese are all largely procedural questions, and it should be possible to deal\nwith them all, but they do have to be dealt with.\n\nNed\n\n\n\n"
        },
        {
            "subject": "CGI??",
            "content": "I've seen a number of requests for a more formal specification for CGI\nthan  <URL:http://hoohoo.ncsa.uiuc.edu/cgi/>.\n\nThis leads to the following questions:\n\n1) Is CGI appropriate for an RFC? If not, what standards group would\nit belong to?\n\n2) If it's appropriate for an RFC, is this this WG it would belong to?\nIf not, which does it belong to?\n\n3) Is anyone working on such a thing now?\n\nThanx,\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "I apologize for continuing this discussion on the HTTP list. It really needs to\nmove to the MAILEXT list ASAP.\n\n>     I don't say that these argue against SIL codes, just that we have\n>     to know what we are doing, and make sure we change the standard\n>     in the right place.\n\n> Well, I don't know how pertinent it is to know about agendas here. We\n> could certainly question ISO in this regard also.  It seems to me that\n> SIL's agenda here is quite irrelevant and that it is the end product of\n> their agenda that is useful.  Namely, the language list and identifiers\n> list itself.  If we should make any judgements, it should be along the\n> lines of comprehensiveness, which nobody can fault SIL for.\n\nI think you're missing the point Harald is trying to make here. The question\nisn't one of whether or not the codes are technically accurate, fairly\nassigned, or anything like that. The question is one of how stable the list is\nand what methods are used to update it. And this agenda *is* pertinent. In fact\nits absolutely crucial.\n\nYou've provided the list in the form of a URL that appears to be attached to\nyour site. Suppose we issue a document tomorrow containing the URL and you go\nout of business the day after that? The URL is no longer valid and the list can\nno longer be obtained. I suppose someone could call SIL and find a new\nlocation, assuming one exists, but that's really not acceptable.\n\nSuppose there's an academic revolt amongst the linguists at SIL and the list is\nradically revised. (You know as well as I do that such things do happen in\nacademia from time to time.) The document is then updated with the new list and\nmany old codes either vanish or are assigned to different things. Chaos insues.\n\nIn order to use the list as a standard we need some assurance that these sorts\nof things will not happen. We normally get that assurance by citing specific,\npublished entities. You may not like the ISO (I have a number of problems with\nthem myself), but their process insures that a citation to a given document\nwill always remain valid. Similarly, publication as an RFC provides a stable\nreference and also insures that updates will be made according to IETF\nguidelines. And these are not the only ways such assurances can be provided --\nthey simply examples of how it has been done in the past.\n\nThe simplest thing to do would be to publish the current list as an RFC.\nUpdates from the SIL would be possible and probably encouraged, but would be\nsubject to some review by the IETF to insure compatibility is maintained. (I\nsee absolutely no point in reviewing things as to technical linguistic accuracy\nin the IETF. The IETF lacks the expertise to make such judgements. On the other\nhand, the IETF does have the competence to assess compatibility issues\naccording to the installed base, which is something the SIL does not have.)\n\nJust because its the simplest thing to do doesn't make it the best thing,\nhowever. Other approaches are possible. This list may well have appeared in a\nbook or journal somewhere. If so, that could always be cited, and the citation\ncould be updated by updating RFC1766 as necessary.\n\n> I'd like to improve 1766 to make it more comprehensive.  You couldn't\n> help it that 639 is so limited.  So I'm really not faulting 1766.  It\n> seems it is important to recognize its limitations based on 639's limits\n> and move on from there.  Given that the SIL list is available and a new\n> list from ISO is not forthcoming (its been stuck as a CD for over 5\n> years now with little activity), I think that we should move ahead and\n> use the SIL list.\n\n> If would be pleased to assist in improving 1766 so it can be a single,\n> comprehensive language tag standard.  Something I also want.\n\nGood. Then let's get going on it in the MAILEXT WG.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": ">  >   11) Defined semantics for metainformation\n>  >       o Title, Link, Base, Content-MD5, Content-Language, etc.\n>                               ***********\n> Sorry If I'm completly off base but could this be changed\n> into \"Content-Checksum:\". I'd rather not tie MD5 particular\n> *algorithm* to the general 'checksum' *functionality*, (just in case\n> MD6 pops out in few monthes for instance...) {Though I currently use\n> MD5 as the algorithm currently}\n\nI would myself prefer \"Content-Digest\" as this is neutral with respect\nto new cryptographic algorithms for message digests that aren't based\non checksums.\n\n-- Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 8682\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/hypertext/WWW/People/Raggett\n\n\n\n"
        },
        {
            "subject": "Re: CGI??",
            "content": "At 10:33 AM 11/3/95, Mike Meyer wrote:\n>I've seen a number of requests for a more formal specification for CGI\n>than  <URL:http://hoohoo.ncsa.uiuc.edu/cgi/>.\n>\n>This leads to the following questions:\n>\n>1) Is CGI appropriate for an RFC? If not, what standards group would\n>        it belong to?\n\nSee <URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q3/0378.html>\n(in which Roy says no to your first question, and gives some explanation).\n\nI've said the W3C should probably be the standards group in charge, if any.\nIf you want to talk about it, www-talk is probably the best place.\nObviously, it's been discussed before to no end, but maybe some of the\nrecent discussion in c.i.www.a.cgi could be elided to a concrete proposal\n(something along the lines of BGI).  [I'd be willing to help if it gets to\nthat.]\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> > > As I said earlier (while many of you were off at the IETF?), I'm\n> > > increasingly convinced that HTTP messages are (as the recent spec\n> > > suggests) MIME-like, not MIME conforming. With so many other\n> > > deviations from MIME, I suggest we should drop the (rather complex)\n> > > MIME multi-part structure based on boundaries, etc. and only allow\n> > > multi-part messages defined by a Content-Length byte count.\n\nIt is true that multi-part structures in MIME are designed around constraints\nthat don't apply here, and changing or dropping them may be a reasonable\nthing to do.  Allowing specification by either content-length or by\nexisting boundaries may be a reasonable thing to do; since existing software\ndoesn't understand either method, there isn't really an \"installed base\" issue.\nI really would like to think that other, higher level constructs like HTTP-NG\nor something like it can make multiparts unneeded anyway.\n\n> > I think we went through this with HTML; one might have said (many did)\n> > that:\n> > \n> > \"... HTML files are (as the recent spec suggests) SGML-like, not SGML\n> > conforming. With so many other deviations from SGML....\"\n> >   \n> > I think the original *intent* was to be MIME conforming, and that it\n> > isn't *hard* to be MIME-conforming, and that there are *benefits* to\n> > being MIME-conforming.\n\nI certainly would agree that the original versions of the spec were quite\nclear about being MIME-conforming, and this was how server implementations\ndecided they needed to throw in things like \"Mime-Version\" headers.\n\n> > MIME is not basically a text-based protocol, any more than HTTP is.\n> \n> I'd be the first to agree that MIME is a fine example of protocol\n> design. But it has different design objectives, the chief one of\n> which is to stuff all sorts of data types into (possibly broken)\n> implementations of RFC822 messages and SMTP transport. These are\n> clearly lines of text. The MIME standard does not really say much\n> about the \"binary\" content transfer encoding that we say we are\n> using, and says nothing about using it in the body of a message.\n\nIt says that \"binary\" can contain any sequence of octets.  What more does it\nneed to say?\n\n> My view of \"MIME-Conforming\" is that \"you can parse it with\n> the MIME RFC\", not \"redefine half the protocol and call it MIME\".\n> I don't see how a message with a body containing a un-encoded GIF\n> can be represented with the MIME RFC.\n\nCTE: binary does it.  Of course, few if any ordinary MTAs can actually\ntransport such an object successfully, but HTTP can.\n\n> This notion of tolerating different EOL representations\n> is a sticky one that can be pushed off on the HTTP or HTML specs.\n\nIt has to be in HTTP, since it applies to all textual types, not just HTML.\n\n> SGML conformance clearly has particular benifits. What do you all\n> think are the payoffs (or downsides) to sticking close to the\n> MIME spec?\n\nIt means that, as new things are defined for MIME (like how to represent\nUnicode, or how to transport objects with PGP signatures, or how to handle\nX.400 stuff) HTTP gets it \"for free.\"  There does seem to be some effort to\nre-do some of that kind of work in HTTP and do it differently from MIME, in\nsome cases because of differences in design goals.\n\nI'm reminded of news, which borrowed a lot from mail.  News, therefore, gets\n\"for free\" the extensions of MIME rather than needing to re-invent them in\na fashion appropriate to its own domain.\n\nIt means that a WWW browser already has most of a functional MIME mail user\nagent \"for free\" (or vice-versa, if you prefer.)  The more seamless the\nintegration of mail, news, and HTTP the better.  It helps improve consistency\nand reduces the degree to which implementors must waste time on two separate\nbodies of code that do much the same thing with subtle differences.\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "Glenn Adams <glenn@stonehand.com> wrote in message\n<9511021047.AA02653@trubetzkoy.stonehand.com>:\n\n>     From: Harald.T.Alvestrand@uninett.no\n>     Date: Thu, 02 Nov 1995 11:33:53 +0100\n> \n>     If you wish to register I-SIL-nnn as a standard for three-letter\n>     Ethnologue-based tags, or even want to push for updating RFC 1766 to\n>     include S-nnn as a new category, I would not argue against that, but\n>     I would like to stick to the principle of using ISO standards for\n>     the basic namespace.\n> \n> Since IANA does not otherwise use such a principle I don't know why you\n> would adopt it here, let alone insist on it.\n\nOne thing that I don't understand is why you insist on 3-letter\ntags nnn instead of 7-letter tags I-S-nnn for Ethnologue-based\nlanguage labels. How can 4 bytes per tag make such a difference?\n_No_ changes of RFC 1766 at all are needed to register tags of\nthe form I-S-nnn.\n\n>     Otherwise, we will end up with a really confusing situation once the\n>     ISO 3166 three-letter project finishes (if it ever does); its tags are\n>     SURE to conflict with the SIL tag.\n> \n> Even more incongruent does your \"principle\" appear given the laggardness\n> of this particular ISO work item.\n\nYou will not have to wait for ISO to get its act together, if\nI-S-nnn tags are registered. _If_ a 3-letter extension of\nISO 639 is eventually adopted, the 3-letter langauge tag space\nshould still be available, should IETF find these codes useful.\n\n> It is extremely unlikely that ISO or\n> anyone else for that matter will do as comprehensive a job as SIL has done\n> in creating their language database.\n\nA possible future 3-letter ISO standard will probably have other\ndesirable properties:\n+  Wide acceptance in the bibliographic and lingusitic\n   communities that pioneered standardized language tagging.\n+  Extensive experience of a very similar language tagging\n   system (*).\n+  Interoperability with the internationalization work in\n   ISO/JTC1/SC22 (Posix etc.).\n+  Low frequency of errors and ambiguity thanks to careful\n   scrutiny by lingusitic experts.\n+  An open and international standardization process, minimizing\n   the risk for accusations of political bias and other\n   neonationalistic fuzz (**).\n+  The automatic authority possessed by ISO-stamped standards in\n   culturally and nationally sensitive fields.\n\n(*) The proposed 3-letter extension of ISO 639 was essentially\nthe Library of Congress language tagging system, with additions\nof alternative codes for some languages, thought to be more\nconsiderate and acceptable to the users of those languages.\n\n(**) I'm talking about _real_ politics here, not the toy\n\"politics\" of the fights between different networking standards.\n\nBut the important thing is that we don't have to choose between\nthese two systems, or try to predict the outcome and time frames\nof the ISO work: If a new ISO standard does come into existance,\nit can be added to the already registered tags, including\nI-S-nnn tags defined according to the Ethnologue project.\n\n/Olle\n\n--\nOlle Jarnefors, Royal Institute of Technology, Stockholm <ojarnef@admin.kth.se>\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "Ned Freed <NED@innosoft.com> wrote in message <01HX6C9585U29BVL0H@INNOSOFT.COM>:\n\n> If there's even the slightest chance that multiple three letter code schemes\n> will exist in the future then the S-nnn approach seems like the way to go to\n> me.\n\nThe I-S-nnn approach is fully consistent with the current RFC\n1766. Is a revision of the RFC justified, only to save 2 bytes?\n\n> In summary, the only thing here that \"sounds empty\" to me is the notion that\n> its acceptable to have two different sets of language tags in different IETF\n> work items and that its acceptable avoid revising documents that need revision.\n> Glenn may have an excellent case for putting the SIL codes in RFC1766. He may\n> even have a case for putting the SIL codes in without an \"S-\" introducer and\n> putting the introducer on the son-of-639 codes should they ever appear. If so,\n> he needs to bring this up with the WG that produced the content-language\n> !? specification -- the MAILEXT WG, I believe.\n\nDoes the MAILEXT WG still exist? The latest minutes seems to be\nmailext-minutes-95apr.txt, which says:\n\n: The working group should conclude its work by the Stockholm IETF. (Note:\n: the Area Directors would like to see the group conclude its work prior\n: to the IETF.)\n\nHow can one in general find out which IETF WGs are not yet\ndisbanded? Is there any always up-to-date database covering WGs?\nIs dissolution of WGs always announced on the ietf-announce\nlist, with some unique substring in the Subject header?\n\n/Olle\n\n--\nOlle Jarnefors, Royal Institute of Technology, Stockholm <ojarnef@admin.kth.se>\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "Glenn Adams <glenn@stonehand.com> wrote in message\n<9511031019.AA02999@trubetzkoy.stonehand.com>:\n\n> I'd like to improve 1766 to make it more comprehensive.  You couldn't\n> help it that 639 is so limited.  So I'm really not faulting 1766.  It\n> seems it is important to recognize its limitations based on 639's limits\n> and move on from there.\n\nThe way to do that is to use the registration procedure for\nlanguage tags defined in RFC 1766. This RFC was never meant to\ndefine a comprehensive set of langauge tags itself.\n\n> If would be pleased to assist in improving 1766 so it can be a single,\n> comprehensive language tag standard.  Something I also want.\n\nThe IANA language tag registry may become that comprehensive\nstandard. No revision of 1766 is needed to achieve that.\n\n/Olle\n\n--\nOlle Jarnefors, Royal Institute of Technology, Stockholm <ojarnef@admin.kth.se>\n\n\n\n"
        },
        {
            "subject": "Agenda for HTTPWG meeting at Dalla",
            "content": "The HTTP-WG meeting is scheduled for 0930-1130 MONDAY, December 4, 1995.\nIt was swapped with the HTML-WG meeting to give HTTP-WG mbone coverage.\n\nThe preliminary meeting agenda is:\n\n   o    5 minutes for intro and changes to the agenda\n\n   o    50 minutes on the status of current drafts:\n                                                       (submitted)\n        <draft-kristol-http-state-info-01.txt>          Sep 22nd\n        <draft-eastlake-internet-payment-00.txt>        Jul 7th\n        <draft-ietf-http-ses-ext-00.txt>                Jul 3rd\n        <draft-luotonen-http-url-byterange-00.txt>      Jun 21st\n        <draft-luotonen-ssl-tunneling-00.txt>           Jun 16th\n        \n   o    1 hour for HTTP 1.1 review\n                - Presentation by Roy Fielding\n                - Q & A\n\n   o    5 mins Wrap up\n\nIf the there is sufficient interest in particular topics, we may\nbe able to carry on discussions during Monday evening.\n\nPlease let me know if you would like to suggest changes to the\nagenda.  I am also looking for a volunteer to take the minutes!\n\n-- Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 8682\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/hypertext/WWW/People/Raggett\n\n\n\n"
        },
        {
            "subject": "ContentDigest proposal (was Re: Revised Charter",
            "content": "Ok, You're right !\nI'm for it, we are two\n[if it has any weight]\nwho else, who is against (and why?, besides Roy)\n\n(btw I've modified my implementations both clients and servers side \naccordingly, and it's nicer like that as It is shorter and it is even\nnow aligned with Content:Length :-)) \n\nDave Raggett writes:\n > >  >   11) Defined semantics for metainformation\n > >  >       o Title, Link, Base, Content-MD5, Content-Language, etc.\n > >                               ***********\n > > Sorry If I'm completly off base but could this be changed\n > > into \"Content-Checksum:\". I'd rather not tie MD5 particular\n > > *algorithm* to the general 'checksum' *functionality*, (just in case\n > > MD6 pops out in few monthes for instance...) {Though I currently use\n > > MD5 as the algorithm currently}\n > I would myself prefer \"Content-Digest\" as this is neutral with respect\n > to new cryptographic algorithms for message digests that aren't based\n > on checksums.\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\nPGP [Hello to all my fans in domestic surveillance] ammunition KGB\n strategic Uzi CIA\n\n\n\n"
        },
        {
            "subject": "Re: ContentDigest proposal (was Re: Revised Charter",
            "content": "> Ok, You're right !\n> I'm for it, we are two\n> [if it has any weight]\n> who else, who is against (and why?, besides Roy)\n\nRoy made valid points about this issue, and I give him my full\nsupport.  It should remain Content-MD5.\n\nHaving more generality just complicates things on both ends, because\nsupport for a new digest algorithms won't even appear simultaneously\non the server and client side.  Not that it's even important.  In\npractice, it's gonna remain MD5 for a real long time, if not forever\n(in terms of how long the protocol version itself is gonna live).  If\nyou do want to start using some other alg, you're more than welcome to\nuse some other Content-Foo-Bar: header.  The protocol spec has never\ndisallowed new headers from being added.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: ContentDigest proposal (was Re: Revised Charter",
            "content": "> \n> Having more generality just complicates things on both ends, because\n> support for a new digest algorithms won't even appear simultaneously\n> on the server and client side.  Not that it's even important.  In\n> practice, it's gonna remain MD5 for a real long time, if not forever\n\nPeople should be aware that many people consider MD5 to be insufficiently \nsecure to rely on it for long term use. If the header is using \nContent-MD5 as an insercure hash, then it's ok (in fact, using a weaker, \nfaster HASH such as MD4 may be better). If it's to be used for security \npurposes, then longer hashes are crucial. \n\nRemember, due to the Birthday Paradox, MD5 is breakable with effort\nO(2^64); the NSA recommends a minimum of 80 bits of security. \n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "I have two additions to the list:\n\n- Accurate statistics in spite of caching: schemes to let proxies\ncommunicate hit counts and other statistics for cached resources to\nthe origin servers.\n\n- Request-id (which is different from the State-Info proposal by Dave\nKristol).  Request-ID would be an optional header (or optional part of\nthe From header in some variants), send by clients, to allow accurate\nclicktrail analysis on server logs, even if the clients are behind\nproxies or firewalls.  I don't care much for this idea myself, but I\ncan't recall that we reached consensus in this discussion. (We did\nreach consensus on the need for Request-id and State-Info to be two\nseparate mechanisms.)\n\nThese things were discussed extensively on www-talk in July, for\nexample in the `3 Proposals: session ID, business-card auth, customer\nauth' thread.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: State Wars, part XI (was: Revised Charter",
            "content": "On Fri, 3 Nov 1995, Koen Holtman wrote:\n> M. Hedlund:\n> >[Lou Montulli:]\n> >>In fact, [Dave's proposal] reduces the capibilities such that it is nearly\n> >>unusable for large scale applications such as online shopping.\n> >\n> >I disagree (as a programmer for an online shopping site).  I do agree that\n> >an extremely large online shopping site with many stores or large ordering\n> >possibilities might hit an upper limit with the state-info proposal if they\n> >tried to store all ordering info in the State-info: header.  This concern,\n> >however, is not pressing for the vast majority of ordering systems, which\n> >accumulate a small number of items in each session.\n> \n> I agree.\n\nBesides, hopefully this \"extremely large shopping site\" will be sufficiently\nfunded and motivated to build a shopping cart Java applet which will store\neverything in a client-side database so none of this information needs to get\ntransmitted over the wire more than twice (once at browse-time, once at\norder-time) and there's no limit to what can go in your shopping cart.  Oh,\nthe Java applet API doesn't have persistant object support - oops!  Big\nmistake there.  Hopefully the API v2 will fix that... \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: State Wars, part XI (was: Revised Charter",
            "content": "Brian Behlendorf writes:\n > On Fri, 3 Nov 1995, Koen Holtman wrote:\n > > M. Hedlund:\n > > >[Lou Montulli:]\n > > >>In fact, [Dave's proposal] reduces the capibilities such that it is nearly\n > > >>unusable for large scale applications such as online shopping.\n...\n > > >\n > > >I disagree (as a programmer for an online shopping site).\n...\n > > I agree.\n...\n > Besides, hopefully this \"extremely large shopping site\" will be sufficiently\n > funded and motivated to build a shopping cart Java applet\n...\n\nI've been meaning to interject a slight note of pessimism into this\ndiscussion.  People building sites that require stateful interactions\nwill not be able to completely switch over to *any* new scheme for\ntransmitting state for a good long time.  The reason is that so long\nas a large fraction of the browsing public uses browsers that are\nincapable of dealing with whatever new scheme is used (Netscape\n\"cookies\", state-info, or whatever), purveyors of interactive services\nwill find it in their own interest to support the minimum feature set\npossessed by the maximum number of browsers.  This means that\nURL-encodings are here to stay for a while.\n\nThe next step beyond URL-encodings is a way to pass a session ID\naround without screwing with URLs, while leaving the server to manage\na database.  It will be fairly easy for server operators then to\nsupport hybrid schemes: browsers that support it can use a session ID,\nwhile those that don't can use URL encodings, while the server manages\nthe state-holding database in both cases. As long as a substantial\nfraction of possible users and customers uses browsers with no support for\nspecial state mechanisms, server operators will have to support server-\nside databases.  As long as they have to do this, only the most\nresource-rich server operators will be able to support *both*\nserver-side databases and whatever special state-handling mechanisms\nare built into different clients.  This will tend to slow progress.\n\nIf Netscape goes its way and the rest of the world goes another way (or\nmultiple other ways), it seems pretty clear to me that people who run\nservers requiring stateful interaction will just continue to do it the\nbad old way until the dust settles in ... a couple years ...?\n\nThe same goes for expecting special Java applets to magically solve this,\nsince it'll only work for people who have Java-capable browsers.\n\n--Shel Kaphan\n  sjk@amazon.com\n\n\n\n"
        },
        {
            "subject": "Re:  ContentDigest proposal (was Re: Revised Charter",
            "content": "There is a Content-MD5 RFC in common use already.\n\nOn the other hand, it seems clunky to have the hash mechanism specified\nin the header, as opposed to having a key/value list in a single\n\"Content-Hash\" header.  Years ago I started putting Snefru hashes on all\nmy posts to comp.sources.unix.  When that was upgraded to MD5 (after\nweaknesses in basic Snefru were found), it didn't feel right.\n\nSorry to be so indefinite, but speaking as an implementor \"it just didn't\nfeel right\" should carry some weight.\n/r$\n\n\n\n"
        },
        {
            "subject": "Original 'accepthash' post",
            "content": "Date: Mon, 10 Jul 95 14:01:24 EST\nTo: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nIn-reply-to: Alex Hopmann's message of Mon, 10 Jul 1995 13:45:11 -0700 <199507102045.NAA08852@holonet.net>\nSubject: Re: HTTP Session Extension draft \nFrom: Larry Masinter <masinter@parc.xerox.com>\n\nIt might improve net efficiency (and possibly allow servers to\nprecompute information or ignore these headers if they don't care) to\npackage together those things that are configuration specific (accept,\naccept-encoding, accept-charset, accept-language and user-agent:) and\nsend them by reference, e.g.,\n\nthe client sends:\n\naccept-hash: NNNNNNNNNNNNNNN\n\nwhere NNNNNNNNNNNNNN is the MD5 of the omitted headers; the server\nsends back an error return if it actually needs the fields.\n\nThis would be useful independent of whether the connection remains\nopen: even if the connection closes, the information might affect a\ncache choice; even if the connection remains open, a proxy might want\nto send different header information when proxying for different\nclients. \n\n(Clearly this would be in 1.1; if HTTP were recast as ILU or CORBA, it\nwould be done as a client object that the server could interrogate.)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Jim Seidman:\n> John Franks writes:\n> >Correct me if I am wrong, but I concluded from Spero's postings that\n> >nothing currently proposed including MGET, hold-open, or even HTTP-NG\n> >would improve (or even match?) the user's perceived performance\n> >currently given by Netscape.  By this I mean the ellapsed time until\n> >the user can start reading *all the text* and the ellapsed time until\n> >the user can jump to a new link.\n> \n> I have to disagree here.  If the issue is how quickly all the text can be\n> downloaded, then the fastest way to do this is to send only the text and\n> then pass on the images later.  Especially for users on slower connections\n> (such as the ever-mentioned 14.4 SLIP connection) there is a notable\n> degradation in getting all of the text if several images are being sent\n> simultaneously.\n> \n\nThe fastest way to get all the text is to send it first but it can't be\ndisplaye until layout information like the size and shape of all images is \nknown.  This is the point of the Netscape multiple connections.  They get\nthe first few bits of each image which contain the size information.\n\n> \n> In my mind the only defensible reason for simultaneous connections is to\n> reduce the round trip time penalty for loading all the pieces of a document.\n> However there are already several discussions taking place here about other\n> ways to minimize the RTT issues.  Even something as trivial as having a\n> keep-alive and allowing the client to buffer up requests for images as it's\n> downloading the text would provide a major improvement.\n> \n\nrom the *user's* perspective this would be *much slower*.  The user \ncouldn't read past the first image or jump to another link until everything\nhas been downloaded.\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Agenda for HTTPWG meeting at Dalla",
            "content": "I would like to request that we have a well-defined 30 minute\ndiscussion on content negotiation in the context of whether there is\npossibly a generic content negotiation mechanism that will work for\nmail agents as well as HTTP.  This is in lieu of a separate BOF on the\nthe topic. Since content negotation is part of HTTP 1.1, we could make\nit part of the HTTP 1.1 review.\n\nWhile HTTP 1.1 introduces content negotiation, it is a capability that\nis also needed in the mail community, and involving the other\nconstituencies for this discussion is a good idea.\n\nYou say:\n\n> If the there is sufficient interest in particular topics, we may\n> be able to carry on discussions during Monday evening.\n\nDo you have a room/time assigned for Monday evening? It may still\npossible to schedule a BOF Monday evening specifically on the topic.\n\n(From html-wg):\n> I think the lack of consensus was due to the fact that much of the\n> discussion requires some sort of content negotiation scheme, which is\n> commonly considered beyond the scope of the HTML working group. I think it\n> was Larry Masinter who informally proposed to create an IETF working group\n> on content negotiation, but that has yet to reach fruition.\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> Roy T. Fielding writes:\n\n>  > >       o Title, Link, Base, Content-MD5, Content-Language, etc.\n>  > > Sorry If I'm completly off base but could this be changed\n>  > > into \"Content-Checksum:\". I'd rather not tie MD5 particular\n>  > > *algorithm* to the general 'checksum' *functionality*, (just in case\n>  > > MD6 pops out in few monthes for instance...) {Though I currently use\n>  > > MD5 as the algorithm currently}\n\n> So because there were a mere 70 lines document (headers dropped)\n> written in 1993 (Rfc1544 (hmm, reissued with almost no changes as\n> rfc1864 very recently)) *suggesting* the use of a *specific*\n> algorithm, for Email should imply that world stopped and that for a\n> new proposed protocol (http/1.1) you have to use the same buggy\n> specific definition ? No evolution possible ? No new ideas/fixes ever\n> allowed ?\n\nOf course evolution is possible. Of course new ideas and fixes are allowed.\nHowever, just as in the case of language tags, you're attempting to engineer an\nHTTP-specific solution to a problem the IETF and IESG believes has been solved\nfor MIME objects already. If you think Content-MD5 is broken it must\nbe generally broken and any fix that's made should not be specific to HTTP.\n\nIf you disagree with Content-MD5 so much, why didn't you challenge its\nspecification during the last couple of months while it was up for review?\n\nI personally have never cared for the embedding of the algorithm in the  header\nname, and I pointed this out when the original proposal was made. I would have\npreferred a simple <algorithm-id>,<value> format.  But nobody else objected and\nit wasn't that important to me, so the proposal went through anyhow. And nobody\nobjected during the move to draft either, at least not that I recall.\n\nIts going to be tough to change it at this late date, but if you think you have\na case to make for an alternative by all means present it in the form of a new\nspecification. That's how it needs to be done. Having two different mechanisms\nfor mail and HTTP simply encourages everyone who comes along with the new\nprotocol that uses MIME to roll yet another one of these, and we really\ndon't need that.\n\n> And you make htpp/1.2 when MD6 or something else pops out ?\n\nOne of the advantage of having a separate specification for this is that\nyou can upgrade it without revising all of HTTP (or MIME for that matter).\n\nIn addition, an alternative has already presented itself: SHA.  A partial\nattack against MD5 has been presented (den Boer and Bosselaers), and while its\nnowhere near what's needed to break the algorithm it is nevertheless worrisome.\nSHA also provides a longer hash value than MD5, which is always nice.\n\nYou may think that the cryptographic quality of such things isn't an issue in\nthis application, but this actually isn't true. One possible use of content-md5\nis to checksum some external object and include that checksum within a signed\nMIME object. In this case the cryptographic strength of content-md5 is a\nsignificant issue.\n\nSHA is, on the other hand, quite a bit slower than MD5 -- less than half as\nfast, in fact.\n\nMy position at the present time is that the relative speed issues are more\nimportant than the minor observed weakness in MD5. This is especially true\ngiven the relative infrequency of use of content-md5 to actually provide some\nform of security, so RFC1874 is acceptable to me for the time being. But that's\njust my take on the situation.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "> > If there's even the slightest chance that multiple three letter code schemes\n> > will exist in the future then the S-nnn approach seems like the way to go to\n> > me.\n\n> The I-S-nnn approach is fully consistent with the current RFC\n> 1766. Is a revision of the RFC justified, only to save 2 bytes?\n\nIt isn't clear to me that this is actually allowed under the rules RFC1766 sets\nup for IANA registrations. There is nothing in there that seems to allow for\nregistration of an entire block to an agency outside of IANA. And even if there\nwere, the registration would still have to go through the registration process,\nat which time exactly the same set of issues are going to have to be evaluated\nand discussed. You gain nothing here except the time it takes to revise an RFC.\n\nAnd what's more, you lose a lot more than two bytes. You lose completeness\nof specification. A registration of this sort doesn't have to appear in an\nRFC, so when someone wonders what I-S-drofnats or whatever is they will have\nto:\n\n(1) Dig up the current RFC. Find the pointer in it to the current registration\n    policy.\n(2) Dig up the registration policy. Find the pointer in it to the registered\n    names table.\n(3) Look through the table and find the pointer to the allocated subspace.\n(4) Find the entry in the allocated subspace.\n\nThis seems like a lot more work than it should be. Why not simply have an RFC\nthat contains a pointer to the definitive table? Its not like all the\nindirection buys you anything.\n\n> > In summary, the only thing here that \"sounds empty\" to me is the notion that\n> > its acceptable to have two different sets of language tags in different IETF\n> > work items and that its acceptable avoid revising documents that need revision.\n> > Glenn may have an excellent case for putting the SIL codes in RFC1766. He may\n> > even have a case for putting the SIL codes in without an \"S-\" introducer and\n> > putting the introducer on the son-of-639 codes should they ever appear. If so,\n> > he needs to bring this up with the WG that produced the content-language\n> > !? specification -- the MAILEXT WG, I believe.\n\n> Does the MAILEXT WG still exist? The latest minutes seems to be\n> mailext-minutes-95apr.txt, which says:\n\n> : The working group should conclude its work by the Stockholm IETF. (Note:\n> : the Area Directors would like to see the group conclude its work prior\n> : to the IETF.)\n\nWorking groups exist by definition until the standards-track documents they are\nworking on reach the status of Standard or are removed from the standards\ntrack. Groups \"conclude their work\" all the time, only to be revived to handle\nthe transition from proposed to draft or draft to standard.\n\nIts also possible for documents to move along without WG reactivation. However,\nin the case of a poprosal to make a substantive change to a specification (and\nI don't see how you can view this in any other way), it seems inevitable that\nthe group will have to reactivate to consider it.\n\n> How can one in general find out which IETF WGs are not yet\n> disbanded? Is there any always up-to-date database covering WGs?\n> Is dissolution of WGs always announced on the ietf-announce\n> list, with some unique substring in the Subject header?\n\nAs far as I know the concept of \"disbanding\" a group doesn't exist. The mailing\nlist always continues to operate. Most groups eventually reach a state of\npermanent dormancy that they never escape from, and are eventually  rendered\ncompletely irrelevant by other work items that replace the specifications they\noriginally produced.\n\nned\n\n\n\n"
        },
        {
            "subject": "Re: State Wars, part XI (was: Revised Charter",
            "content": "On Fri, 3 Nov 1995, Shel Kaphan wrote:\n> I've been meaning to interject a slight note of pessimism into this\n> discussion.  People building sites that require stateful interactions\n> will not be able to completely switch over to *any* new scheme for\n> transmitting state for a good long time.  The reason is that so long\n> as a large fraction of the browsing public uses browsers that are\n> incapable of dealing with whatever new scheme is used (Netscape\n> \"cookies\", state-info, or whatever), purveyors of interactive services\n> will find it in their own interest to support the minimum feature set\n> possessed by the maximum number of browsers.  This means that\n> URL-encodings are here to stay for a while.\n\n<sermon>\nWith a graceful framework for dealing with clients of varying \ncapabilities, new capabilities can be introduced at any time without \ncausing problems for clients of lesser capabilities.  That graceful \nframework is content negotiation.  Until that's taken seriously, Doing \nCool New Things just gets more and more painful, and we get stuck in a \nmorass of chicken-and-egg-isms.\n</sermon>\n\n> The next step beyond URL-encodings is a way to pass a session ID\n> around without screwing with URLs, while leaving the server to manage\n> a database.  It will be fairly easy for server operators then to\n> support hybrid schemes: browsers that support it can use a session ID,\n> while those that don't can use URL encodings, while the server manages\n> the state-holding database in both cases. As long as a substantial\n> fraction of possible users and customers uses browsers with no support for\n> special state mechanisms, server operators will have to support server-\n> side databases.  As long as they have to do this, only the most\n> resource-rich server operators will be able to support *both*\n> server-side databases and whatever special state-handling mechanisms\n> are built into different clients.  This will tend to slow progress.\n\nThe goal here is not necessarily to reduce the workload of the server\noperators or server software authors - anecdotally and historically that's\nbeen a very abundant resource.  Particularly since once this type of multiple\nsupport is coded it doesn't have to be redone each time. The goal *should* be\nto provide the most graceful, flexible, lowest-bandwidth, secure, and\nprivacy-protecting protocols and systems.  \n\nI'm a pragmatist, too - if we were to implement a shopping cart, I would \nsupport what I had to to make it usable to the largest number of users, \nwhich would probably mean URL encoding as a base case.  My comment about \nJava apps was to try and give perspective to exactly what we want to be \nable to do with state maintenance.  Sending back and forth the list of the\n100 CD's in my shopping cart with each access seemes excessive - even if \nit were just a bitmap of that info.\n\n> If Netscape goes its way and the rest of the world goes another way (or\n> multiple other ways), it seems pretty clear to me that people who run\n> servers requiring stateful interaction will just continue to do it the\n> bad old way until the dust settles in ... a couple years ...?\n> \n> The same goes for expecting special Java applets to magically solve this,\n> since it'll only work for people who have Java-capable browsers.\n\nAgain, think: graceful framework.  Graceful framework.  \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": ">for MIME objects already. If you think Content-MD5 is broken it must\n>be generally broken and any fix that's made should not be specific to HTTP.\n\nI do think it is broken.  I exchanged a half-dozen messages with the\nauthors.  The history of this particular RFC is a little funny; it went\nthrough a long revision time because it was worded badly.  Content-MD5\nhas been around for years, the fact that the RFC is just now coming out\nis funny timing.  The phrase \"years of compliant behavior\" was quoted\nto me by Mr. Gardiner.\n\nJust as HTTP as extension headers, so does IETF email.  Just as HTTP\nis free to accept Content-MD5, the email-wg is free to accept any improved\nheader that we come up with here.\n\n>Its going to be tough to change it at this late date, but if you think you have\n>a case to make for an alternative by all means present it in the form of a new\n>specification. That's how it needs to be done. Having two different mechanisms\n>for mail and HTTP simply encourages everyone who comes along with the new\n>protocol that uses MIME to roll yet another one of these, and we really\n>don't need that.\n\nYes.  Has either \"side\" made a strong commitment to convergence?\n\n>My position at the present time is that the relative speed issues are more\n>important than the minor observed weakness in MD5. This is especially true\n>given the relative infrequency of use of content-md5 to actually provide some\n>form of security, so RFC1874 is acceptable to me for the time being. But that's\n>just my take on the situation.\n\nI disagree.  MD5 is routinely used in production RPC systems all the time.\nI expect that for some time most hashes (and signings) will be done off-line\nwhere speed is not an issue, and also verified while the document is being\ndisplayed where, again, speed is not an issue.\n/r$\n\n\n\n"
        },
        {
            "subject": "ContentDigest draft, request for ..",
            "content": "Request for comments, suggestions, etc...\nIs that the way to go,etc... can I submit that (with your inputs/\ncorrections,...) to someone else ?\n----------------8<-------------------\nHTTP Working Group                             Laurent Demailly\nINTERNET-DRAFT                                 Observatoire de Paris\n<draft-ietf-http-TBD(Content-Digest)-01.txt>                   \nExpires SIX MONTHS FROM--->                    Nov 4th 1995\n\n\nHTTP Content-Digest header\n\nStatus of this Memo\n\n   This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time. It is inappropriate to use Internet-\n\n   Drafts as reference material or to cite them other than as\n   \"work in progress.\"\n\n   To learn the current status of any Internet-Draft, please check\n   the \"1id-abstracts.txt\" listing contained in the Internet-\n   Drafts Shadow Directories on ds.internic.net (US East Coast),\n   nic.nordu.net (Europe), ftp.isi.edu (US West Coast), or\n   munnari.oz.au (Pacific Rim).\n\n   Distribution of this document is unlimited. Please send comments\n   to the HTTP working group at <http-wg@cuckoo.hpl.hp.com>.\n   Discussions of the working group are archived at\n   <URL:http://www.ics.uci.edu/pub/ietf/http/>. General discussions\n   about HTTP and the applications which use HTTP should take place\n   on the <www-talk@info.cern.ch> mailing list.\n\nAbstract\n\n   This extension suggest an additional header for HTTP/1.1:\n   An extensible entity body digest method.\n\n   Table of Contents\n\n   1.  Introduction\n       1.1  Purpose\n       1.2  Overall Operation\n       1.3  Definitions\n       1.4  Practical Considerations\n   2.  Examples\n   3.  Security Considerations\n   4.  Acknowledgments\n   5.  References\n   6.  Author's Address\n\n\n1. Introduction\n\n1.1  Purpose\n\n   The HyperText Transfer Protocol, HTTP [1] defines a Content-Length:\n   header that, when applicable, specify the length of the entity body\n   following. We define here a header that generally specify a digest\n   of the entity body. \n\n   A Content-MD5 [2] MIME [3] header is already defined, but it\n   is tied to a specific algorithm (MD5). Also it involves specific\n   considerations about \"canonical\" format that does not apply to HTTP.\n   Lastly the digest coding it suggests is not consistent with other\n   md5 digest producing tools like stand-alone md5/md5sum programs\n   and thus can not be used easily without dedicated user agent.\n\n   The purpose of this extension is to overcome the limitations of the\n   existing solution and to devise and extensible scheme for specific\n   inclusion in HTTP, this while nothing is currently defined for\n   message integrity verification with HTTP.\n\n1.2  Overall Operation\n\n   Each time an HTTP request is made where the Content-Length header\n   is present, a Content-Digest header can be present.\n\n1.3  Definitions\n\n   Using HTTP notation:\n\n   Content-Digest = \"Content-Digest\" \":\" 1#(digest)\n\n   digest = digest-algorithm \"=\" digest-value\n   digest-algorithm = \"MD4\" | \"MD5\" | \"SHA\" | extension-algo\n   extension-algo = any token identifying the algorithm\n   digest-value = string\n\n   The digest-algorithm states which algorithm was used,\n   proposed common keywords are RSA's  MD4 and MD5 [4],...\n\n   The minimal implementation should probably use MD5.\n   For MD5 digest the string coding should be the 32 characters long\n   hexadecimal representation of the 128 bits checksum (like\n   md5/md5sum stand-alone programs output)\n\n1.4  Practical Considerations\n\n   The header is not compulsory and can be ignored (specially for\n   performance considerations)\n\n   If a server uses the header to check incoming POST/PUT\n   entity, and the digest does not match it shall issue an 4xx error\n   (to be defined)\n\n   If a client uses the header and detects non matching digest\n   it shall warn the user explicitly.\n\n2.  Example\n\n    For a body content of \"this is a test\\n\"\n    you get :\n    ---------\n    HTTP/1.0 200 Document follows\n    Server: datasrv/dl2.6d99\n    Last-Modified: Mon, 16 Oct 1995 15:42:06 GMT\n    Content-Type: text/plain\n    Content-Length: 15\n    Content-Digest: MD5=e19c1283c925b3206685ff522acfe3e6\n    \n    this is a test\n    ---------\n\n\n3.  Security\n\n   The purpose of this extension is to improve integrity.\n   It does not imply the object has not been forged along with the\n   headers. The protection is against accidental modifications and\n   not malevolent ones. As there is no a strong cryptographic \n   need, if performance is an issue, it is suggested\n   to use MD4 as the digest algorithm to use, though MD5 is probably\n   currently the more common.\n\n4. Acknowledgments\n\n   Thanks to everybody\n\n5 References\n\n   [1]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen.\n        \"HyperText Transfer Protocol -- HTTP/1.0\"\n        Internet-Draft (work in progress), UC Irvine,\n        <URL:http://ds.internic.net/internet-drafts/\n        draft-ietf-http-v10-spec-00.txt>, March 1995.\n\n   [2] J. Myers and M. Rose, \"The Content-MD5 Header Field\"\n       RFC 1864 Oct 1995.\n   \n   [3] Borenstein, N., and N. Freed, \"MIME (Multipurpose Internet Mail\n       Extensions) Part One: Mechanisms for Specifying and Describing\n       the Format of Internet Message Bodies\", RFC 1521, Bellcore,\n       Innosoft, September 1993.\n\n   [4] Rivest, R., \"The MD5 Message-Digest Algorithm\", RFC 1321, MIT\n       Laboratory for Computer Science and RSA Data Security, Inc.,\n       April 1992.\n\n\n\n6 Author's Address\n\n   Laurent Demailly\n   dl@hplyot.obspm.fr\n   Observatoire de Paris\n   DESPA - Bat Lyot\n   5, pl J. Janssen\n   F-92190 Meudon\n   France\n\n----------------8<-------------------\n(I used  draft-ietf-http-ses-ext-00.txt as a canvas...)\n\nIt's a bit late here now (5am), so I hope I did not write too many\nthings making me look foolish... Ok it's no excuse aand all my fault\nanyway, feel free to flame ;-)\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> >for MIME objects already. If you think Content-MD5 is broken it must\n> >be generally broken and any fix that's made should not be specific to HTTP.\n\n> I do think it is broken.  I exchanged a half-dozen messages with the\n> authors.\n\nWhy didn't you bring it up on the IETF list or with the IESG? Sending cosmetic\nchanges to the authors is fine, but sending substantive issues to the authors\nis really a pointless waste of time, since the authors are constrained not to\ndo anything substantive to a document without IETF approval. The most they\ncould do would be to make the argument for you to the IESG, but it is not\nincumbent on them to do so and I for one think its a bit unreasonable to\nexpect them to.\n\n> The history of this particular RFC is a little funny; it went\n> through a long revision time because it was worded badly.  Content-MD5\n> has been around for years, the fact that the RFC is just now coming out\n> is funny timing.  The phrase \"years of compliant behavior\" was quoted\n> to me by Mr. Gardiner.\n\nI'm not sure what you mean. The original Content-MD5 RFC, RFC1544, is actually\npretty old -- it came out back in November, 1993. Implementations appeared soon\nafter that and I know of no reports of operational problems. The move to draft\noccurred last month, which is a longer wait than the six month required minumum\nbut not that unusual. This isn't a product of a working group, but not all\nstandards-track RFCs are.\n\n> Just as HTTP as extension headers, so does IETF email.  Just as HTTP\n> is free to accept Content-MD5, the email-wg is free to accept any improved\n> header that we come up with here.\n\nThis is only partially true. The IETF has a fairly firm policy of not\nduplicating work whenever it can be avoided. As such, given the existance of a\nstandardized, workable scheme that can already be used to perform this\nfunction, it is going to be difficult to obtain approval for another, duplicate\nmechanism. I for one would object to it in my capacity as a member of the\napplications area directorate.\n\nI don't think a case can be made for having two different mechanisms for email\nand HTTP because of some intrinsic difference between the two, so any new\nmechanism is basically going to have to replace the old one in order to be\nacceptable. And this in turn is going to require that someone come up with a\nbetter argument that the piddly \"I don't like its presentation\" stuff that\npeople, myself most definitely included, have given in the past.\n\n> > Its going to be tough to change it at this late date, but if you think you have\n> > a case to make for an alternative by all means present it in the form of a new\n> > specification. That's how it needs to be done. Having two different mechanisms\n> > for mail and HTTP simply encourages everyone who comes along with the new\n> > protocol that uses MIME to roll yet another one of these, and we really\n> > don't need that.\n\n> Yes.  Has either \"side\" made a strong commitment to convergence?\n\nNot as far as I can tell. However, this one of the reasons why we have an area\ndirectorate, an IESG, and an IAB -- these groups are supposed to check up on\nthings and make sure that different working groups don't go off in widely\ndiffering directions. These groups collectively have the power to override\nworking group decisions if need be in order to insure convergence where\nconvergence is appropriate.\n\n> > My position at the present time is that the relative speed issues are more\n> > important than the minor observed weakness in MD5. This is especially true\n> > given the relative infrequency of use of content-md5 to actually provide some\n> > form of security, so RFC1874 is acceptable to me for the time being. But that's\n> > just my take on the situation.\n\n> I disagree.  MD5 is routinely used in production RPC systems all the time.\n> I expect that for some time most hashes (and signings) will be done off-line\n> where speed is not an issue, and also verified while the document is being\n> displayed where, again, speed is not an issue.\n\nHmm. Well, what, exactly, do you disagree with? You say that MD5 is used all\nthe time in production RPC systems. (I'm well aware of this usage as well as\ndozens of other applications of MD5 -- its one of the most widely used digest\nalgorithms around.) This seems to imply that you are in favor of using MD5. But\nthen you go on to talk about how performance isn't really relevant. Yet\nperformance is one of the things that MD5 has going for it -- its a lot faster\nthan SHA and comparable in performance to HAVAL, which as far as I know is the\nonly other reasonable choice for a digest algorithm right now.\n\nSo which is it? Are you in favor of using MD5 or not? Or is this purely a\n\"packaging issue\" for you rather than an algorithm issue?\n\nNed\n\nP.S. Performance most certainly is an issue in some cases. It may not be for\nsome MIME-based applications, but as a matter of fact there is considerable\ndiscord in the community because of the use of MD5 in IP security. MD5 is seen\nby many as being much too slow, even though its performance is actually pretty\nreasonable compared to other digest algorithms. The problem is that the changes\nneeded to make MD5 perform better have not received anywhere near the public\nscrutiny that MD5 has, and thus aren't sufficient to base standards-track\nsecurity work on yet. See RFC1810 for details on performance and RFC1828 for\nalternatives to regular MD5. But if you're arguing that performance is not\nan issue, then there is no need to make provisions in MIME to allow for\nfaster and simpler digests.\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": ">Why didn't you bring it up on the IETF list or with the IESG?\n\nBecause this is a revision of 1544 which is basically two years old.\nWhile I think per-protocol headers are the wrong way to do things, it\nmakes sense to me that current practice should be standardized.\n\n>This is only partially true. The IETF has a fairly firm policy of not\n>duplicating work whenever it can be avoided. As such, given the existance of a\n>standardized, workable scheme that can already be used to perform this\n>function, it is going to be difficult to obtain approval for another, duplicate\n>mechanism. I for one would object to it in my capacity as a member of the\n>applications area directorate.\n\nThe IETF also has a fairly strong history of accepting duplication and letting\nthe market decide.  If someone can put forward a draft for a header that\nshows two hash mechanisms, and running code I can't imagine the IETF\nrejecting it.\n\n>> Yes.  Has either \"side\" made a strong commitment to convergence?\n>\n>Not as far as I can tell.\n\nPity.\n\nHowever, this one of the reasons why we have an area\ndirectorate, an IESG, and an IAB -- these groups are supposed to check up on\nthings and make sure that different working groups don't go off in widely\ndiffering directions. These groups collectively have the power to override\nworking group decisions if need be in order to insure convergence where\nconvergence is appropriate.\n\n> > My position at the present time is that the relative speed issues are more\n\nThis is what I disagree with.  I think it is (heck, they) both good enough,\nand it doesn't matter anyway since most use will be off-line.\n\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> > Why didn't you bring it up on the IETF list or with the IESG?\n\n> Because this is a revision of 1544 which is basically two years old.\n> While I think per-protocol headers are the wrong way to do things, it\n> makes sense to me that current practice should be standardized.\n\nAs I said before, the IETF generally doesn't standardize two different\nways to do the same thing.\n\n> > This is only partially true. The IETF has a fairly firm policy of not\n> > duplicating work whenever it can be avoided. As such, given the existance of a\n> > standardized, workable scheme that can already be used to perform this\n> > function, it is going to be difficult to obtain approval for another, duplicate\n> > mechanism. I for one would object to it in my capacity as a member of the\n> > applications area directorate.\n\n> The IETF also has a fairly strong history of accepting duplication and letting\n> the market decide.  If someone can put forward a draft for a header that\n> shows two hash mechanisms, and running code I can't imagine the IETF\n> rejecting it.\n\nI regard it as axiomatic that the IETF \"accepts duplication\". The networking\nworld duplicates functionality all the time, and as such the IETF has no choice\nbut to accept this practice.\n\nHowever, this attitude does not extend to work done within the IETF itself.\nDuplication of work within the IETF is actually quite rare, and when it happens\nits usually the result of duplication having occurred elsewhere or as a result\nof work having been done outside the IETF that the IETF isn't entirely\ncomfortable with. The acceptance of both ongoing work on Whois++ and on X.500\nis a good example of the latter, while acceptance of work on MIME<-->X.400\ninteroperation is a good example of the former.\n\nYou need to examine the record a lot more carefully if you think the IETF\nactually condones development and standardization of overlapping protocols.\nLook at the SNMPv2 mess, for example -- a possible solution is to have two\ndifferent security mechanisms available. This would break the deadlock and\nwould allow SNMPv2 to progress. Yet this alternative is not even being\nconsidered -- even when failure may in fact lead to the demise of SNMPv2\ncompletely.\n\nThere is even abundant evidence of this position in the work that's been done\non MIME. Proposals have been made to also standardize some alternatives to\nMIME. These were rejected out of hand on the basis that they duplicated\nstandards-track work. In addition, not many people are aware of the fact that\nMIME actually supercedes an extant, *standard* protocol -- RFC1049. This was in\nfact a substantive issue at one point, and will require taking action to retire\nRFC1049 before MIME progresses to standard.\n\nI could go on and on, but suffice it to say that objections to protocols on\nthe basis of duplication of function are taken very, very seriously, and\nsuch objections are going to be made should an attempt be made to standardize\nsome alternative to content-md5 without a clear understanding of how it\ninteracts with content-md5. I will object myself in my capacity as a member\nof the Applications Area Directorate if it comes down to it.\n\n> > > My position at the present time is that the relative speed issues are more\n\n> This is what I disagree with.  I think it is (heck, they) both good enough,\n> and it doesn't matter anyway since most use will be off-line.\n\nIf it doesn't matter then what are your objections to content-md5?\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Dave Raggett:\n> \n> > Correct me if I am wrong, but I concluded from Spero's postings that\n> > nothing currently proposed including MGET, hold-open, or even HTTP-NG\n> > would improve (or even match?) the user's perceived performance\n> > currently given by Netscape.  By this I mean the ellapsed time until\n> > the user can start reading *all the text* and the ellapsed time until\n> > the user can jump to a new link.\n> \n> HTTPng will win over the multiple connections used by Netscape, as the\n> latter lead to congestion problems, since the connections don't share\n> congestion info. In addition, using a single connection gets around the\n> slow start problem leading to better peformance. It will also be much\n> nicer to servers!\n> \n\nPerhaps.  But by now we have all had extensive experience with Netscape\nand while there have been numerous theoretical objections to multiple\nconnections I don't think servers have collapsed under the load.  I suspect\nthat most providers will simply see the increased load (and I don't think\nwe really know how much it is increased) as the price of supplying a\nbetter product (from the consumer's point of view).\n\nAs for the TCP slow start problem, I don't understand why a single\nslow start ala HTTP-NG is better for the UPP (user's perceived\nperformance) than multiple slow starts in parallel ala Netscape.\n\nEven if HTTPng will win over multiple connections, I think it will have\nto be a dramatic win before client vendors will completely switch.\n\n> Simon explained all this in his notes.\n> \n\nCould you give a reference?  Thanks.\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Revised Charte",
            "content": "Koen Holtman writes:\n> I have two additions to the list:\n> \n> - Accurate statistics in spite of caching: schemes to let proxies\n> communicate hit counts and other statistics for cached resources to\n> the origin servers.\nVery important question. I we omit this, lot of webadmins will stuff in\nCache-Control: no-cache into responses to have hit counts etc.\nThis may turn current internet brownouts into blackouts.\n> - Request-id ...\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": ">As I said before, the IETF generally doesn't standardize two different\n>ways to do the same thing.\n\nThere are enough exceptions -- OSPF/whatever-it-is, usenet/email,\npre-snmp/pre-whatever-it-was, character sets, -- that I am glad you picked\nthe word generally.  I'll claim you left enough open space by using that\nword that you did not say I am wrong and therefore while it may be\nan exception, it is possible.\n\n>I could go on and on, but suffice it to say that objections to protocols on\n>the basis of duplication of function are taken very, very seriously, and\n>such objections are going to be made should an attempt be made to standardize\n>some alternative to content-md5 without a clear understanding of how it\n>interacts with content-md5. I will object myself in my capacity as a member\n>of the Applications Area Directorate if it comes down to it.\n\nFine.  Glad I can count on your vote. :(.  When it comes time to use SHA\nor some other hash -- or multiple hashes -- you will vote against a single\ncombined header rather and would rather have two headers.  I think that\nis being stupid, but I will not write 2K of text trying to convince you\notherwise.  In fact, I hope to write no more after this note.\n\n>If it doesn't matter then what are your objections to content-md5?\n\nDid we really have that big a communication gap?  MD5 is a fine hash, it's\nperformant enough for now and seems solid.  (Was it MD4 or MD5 that just\nhad the collision?)  I would rather not have the hash name appear in the\nheader, based on my implementation experience.  (Arguably the first\nwidepsread use on Usenet, if not the Internet.)  After disucssion with\none of the RFC authors, I withdrew my objection.  There is nothing in\nthis paragraph that is new, I only wrote it to save you a trip to the\narchives.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> >As I said before, the IETF generally doesn't standardize two different\n> >ways to do the same thing.\n\n> There are enough exceptions -- OSPF/whatever-it-is,\n\nWhile this is not my area of expertise, I nevertheless don't think it is an\nexception -- there are simply a variety of different technologies providing\ndifferent services. I don't think any single technical niche is filled by two\ndifferent protocols that don't provide any other service.\n\n> usenet/email,\n\nFirst of all, UUCP mail formats are NOT standardized by the IETF. RFCs\ndescribing such formats are strictly informational in scope.  As far as I know\nthey are not standardized by anybody -- if they are I'd love to know who is\ndoing it because I have a lot of compliants to make...\n\nNEWS is another matter -- it absolutely supports my position here and not\nyours.\n\nNEWS is a different service from email in several significant ways, but they\nare nevertheless both built on top of RFC822, and considerable effort has been\nand is being expended to make sure that semantically identical services are\npresented using the same syntax everywhere. In fact some of us were asleep at\nthe switch and let some inconsistencies arise. This will be corrected on the\nemail side, even when it takes a recycle at proposed to accomplish it.\n\nThere are also work items to standardize the use of MIME in News, rather than\nuse something different.\n\n> pre-snmp/pre-whatever-it-was,\n\nI do not believe what came before SNMP (HEMS/RFC1076?) ever made it onto the\nstandards track. In fact I believe it was abandoned in favor of SNMP so as not\nto have two different standards track management services. (I don't think the\npeople behind it were very happy about this either.)\n\nIn any case, I haven't noticed any documents detailing the advancement of an\nalternative management protocol in the past 4-5 years. Have you?\n\n> character sets, \n\nThe IETF doesn't standardize character sets, period. It follows that there\ncannot be creation, let alone, duplication, of effort in this area.\n\n> -- that I am glad you picked\n> the word generally.  I'll claim you left enough open space by using that\n> word that you did not say I am wrong and therefore while it may be\n> an exception, it is possible.\n\nSure, anything is possible. Quantum mechanics allows for water to run uphill,\nbut I've never seen it happen...\n\n> > I could go on and on, but suffice it to say that objections to protocols on\n> > the basis of duplication of function are taken very, very seriously, and\n> > such objections are going to be made should an attempt be made to standardize\n> > some alternative to content-md5 without a clear understanding of how it\n> > interacts with content-md5. I will object myself in my capacity as a member\n> > of the Applications Area Directorate if it comes down to it.\n\n> Fine.  Glad I can count on your vote. :(.  When it comes time to use SHA\n> or some other hash -- or multiple hashes -- you will vote against a single\n> combined header rather and would rather have two headers.  I think that\n> is being stupid, but I will not write 2K of text trying to convince you\n> otherwise.  In fact, I hope to write no more after this note.\n\nFirst of all, I never said I would oppose the creation of a new digest field\nthat replaces content-md5. It would depend on what that proposal advocated and\nwhy. All I said was that I would oppose having different schemes for HTTP and\nemail. And I will indeed oppose this.\n\nIn other words, you don't care enough to bother to try and reconcile the\ntwo different schemes. That's fine with me, but surely you see it is this\nsort of attitude that has led to the present situation?\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "So much for hope.\n\nLook, Ned.  Without thinking much I came up with four examples where the\nIETF has RFCs on \"competing\" standards, and without thinking hard I named\nseveral.  Your followup was non-responsive.  One isn't your area of\nexpertise but you challenged me anyway, one you got wrong (Usenet, not\nUUCP mail), one you don't undersatnd the history of (SNMP), and one where\nI was sloppy in that I talked about character sets without enough context,\napparently, for you to see that I was talking about all the charset\ndefinitions and transport issues that are floating around.\n\nI have email from Scott Bradner that says the IETF has a history of\nadopting competing standards.\n\n>In other words, you don't care enough to bother to try and reconcile the\n>two different schemes. That's fine with me, but surely you see it is this\n>sort of attitude that has led to the present situation?\n\nI don't understand how you could come to such an understanding.  I know\nthat you saw my question on convergence statements and subsequent response.\n\nHTTP probably wants an extensible scheme that support multiple hashes in\na single header.  Email has Content-MD5 as existing practice.  I had\ndiscussions with one of the RFC authors and was convinced that better\nlanguage standardizing common practice was a good thing, in spite of the\nfact that my implementation experience (arguably the first in widespread\nInternet use) showed it to be less than optimal.  Again, this is just a\nsummary of my previous messages on this topic, easily verifiable from the\narchives.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "Can you  guys stop squabbling about history? If there is really\ngrounds for adding \n\ncontent-digest: MD5:xxxxxx\n\nas an alternative for\n\n       content-MD5: xxxxxxxxx\n\nso as to allow for \n\n       content-digest: SHA:yyyyyy\n\nand other hash algorithms, because these are needed both by HTTP and\nby mail, we could get this through as an experimental RFC and make\nprogress here. The draft should be reviewed by both mailext and by\nhttp-wg.\n\nDon't you agree that this would be a Good Thing?\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> Can you  guys stop squabbling about history?\n\nNot when its relevant to the task at hand. There is an important point to be\nmade here about the IETF's attitude towards having multiple conflicting\nstandards for doing the same thing. You may not think it is important, but I\ndo.\n\n> If there is really grounds for adding\n\n> content-digest: MD5:xxxxxx\n\n> as an alternative for\n\n>        content-MD5: xxxxxxxxx\n\n> so as to allow for\n\n>        content-digest: SHA:yyyyyy\n\n> and other hash algorithms, because these are needed both by HTTP and\n> by mail, we could get this through as an experimental RFC and make\n> progress here. The draft should be reviewed by both mailext and by\n> http-wg.\n\n> Don't you agree that this would be a Good Thing?\n\nLarry, this is exactly the point I have been trying to make, apparently with no\nsuccess whatsoever. No, I do not think this is a good thing. I think it is a\nvery bad thing. I want one scheme for both mail and HTTP and news and for\nwhatever else comes along that uses MIME. I don't want two or three or four or\nfive, not even one that's a standard and one or more that are experimental. Its\njust make-work for everyone to have to support all the variants.\n\nIf Content-MD5 is busted then fine, let's move it to historical ASAP and define\nsomething new. I never liked it much anyway. But if it isn't busted then let's\nsimply use it and be done with it. Either of these options is fine with me.\nHaving an ever-growing list of headers and algorithms and syntaxes that perform\nidentical functions is not.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> Look, Ned.  Without thinking much I came up with four examples where the\n> IETF has RFCs on \"competing\" standards, and without thinking hard I named\n> several.  Your followup was non-responsive.  One isn't your area of\n> expertise but you challenged me anyway, one you got wrong (Usenet, not\n> UUCP mail), one you don't undersatnd the history of (SNMP), and one where\n> I was sloppy in that I talked about character sets without enough context,\n> apparently, for you to see that I was talking about all the charset\n> definitions and transport issues that are floating around.\n\nI still disagree with your assessments on all counts here, but hey, since it\nseems like its vital for you to believe that the IETF endorses competing\nstandards all the time, then by all means have it your way.\n\nWhat matters now is the present proposal. All you have to do is finish it and\ntake it to the applications area directorate and ask to have it put on the\nstandards track. I'll object to it then on the grounds that it duplicates\nexisting functionality in MIME and we'll see how well having competing\nspecifications with identical capabilities sits with the area directors and\nthen with the IESG. Maybe you're right and I'm wrong and they will all just\nlove having two of these. Maybe they won't settle for two and will want five or\nsix. All you have to do is try it and see.\n\n> > In other words, you don't care enough to bother to try and reconcile the\n> > two different schemes. That's fine with me, but surely you see it is this\n> > sort of attitude that has led to the present situation?\n\n> I don't understand how you could come to such an understanding.  I know\n> that you saw my question on convergence statements and subsequent response.\n\nI come to such an understanding directly from statements you have made --\nincluding statements in your last message. You cared enough to bring this up in\nprivate with an RFC author, but not enough to bring it up with the IETF and\nIESG during not one but two last calls. This is why we have IETF last calls!\n\nI never said you didn't want convergence. I said that you don't want to\nactually have to do anything as tough as either using content-md5 in HTTP or\nelse moving content-md5 to historical to get it. It seems you'd rather have two\nparallel specifications than face either of these alternatives. This is my\nunderstanding, and everything you've said seems to support it.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "At the time Content-MD5 was described, we needed something to protect\nus against accidental mangling of E-mail.\n\nThe chances of something being mangled by accident in such a way that the\nContent-MD5 checksum remains valid is not well described by the word\n\"microscopic\"; it is too small. A new \"MD6\" algorithm won't change that.\n\nContent-MD5 is *NOT* a security feature; it is trivially easy to modify\nthe text of a message, recompute the MD5 checksum and insert that into\nthe headers.\n\nOne reason to choose Content-MD5 for the header name rather than a\nsyntax like \"content-checksum: alg=md5; zxclkjsakjfwe\" was exactly to\nPREVENT the adoption of MD2 or MD6 or SHA or the System V \"sum\".\nIn this case, one algorithm is (IMHO) better than two.\n\n               Harald A\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "Aside from the issues of duplication and the history of Content-MD5,\nnobody has presented a valid design reason for defining a generic\nheader field.\n\nHTTP (or Internet mail) header fields are designed to optimize the\nsemantics of <message-attribute>:<message-value>.  From a design\nperspective, the best way to process Internet messages is to read\nthem into a hash table with the field-name as the key.  Then, when\na particular system function needs a particular message attribute,\nit does a simple table look-up to obtain the value of that attribute\n(or nil if the attribute was undefined).\n\nContent-MD5 represents a message attribute.  Content-Digest does not.\nContent-MD5 allows the recipient's semantic parser (the step after\nmessage parsing) to determine if an MD5 has been calculated without\nparsing the contents of any particular field.  Content-Digest does not.\nContent-MD5 will make it easy for me to define a generic IMS construct of\n\n    GET / HTTP/1.1\n    Unless: Content-MD5 eq \"qyeur87168587646846109hgs==\"\n\nor  \n\n    PUT /happy HTTP/1.1\n    Unless: Last-Modified ne \"Mon, 06 Nov 1995 00:51:58 GMT\"\n\nContent-Digest would not.\n\nrom a perspective of having designed several HTTP systems, Content-MD5\n(and Content-SHA, Content-Sum, etc.) is a better design choice for the\nHTTP protocol.  Until it is proven otherwise, Content-MD5 will be the\npreferred choice for HTTP.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "> Content-MD5 represents a message attribute.  Content-Digest does not.\n> Content-MD5 allows the recipient's semantic parser (the step after\n> message parsing) to determine if an MD5 has been calculated without\n> parsing the contents of any particular field.  Content-Digest does not.\n> Content-MD5 will make it easy for me to define a generic IMS construct of\n\n>     GET / HTTP/1.1\n>     Unless: Content-MD5 eq \"qyeur87168587646846109hgs==\"\n\n> or\n\n>     PUT /happy HTTP/1.1\n>     Unless: Last-Modified ne \"Mon, 06 Nov 1995 00:51:58 GMT\"\n\n> Content-Digest would not.\n\n> From a perspective of having designed several HTTP systems, Content-MD5\n> (and Content-SHA, Content-Sum, etc.) is a better design choice for the\n> HTTP protocol.  Until it is proven otherwise, Content-MD5 will be the\n> preferred choice for HTTP.\n\nRoy, this is a very interesting point, and its one I admit I hadn't given\nadequate consideration to. In fact I think I finally see what John Myers, one\nof the coauthors of the content-md5 specification, was driving at in a\nconversation we had one time about this stuff. I believe his implementation \nuses rules similar to what you've described to convert MIME into an internal\nformat.\n\nThe issue I've always thought of in the past with having multiple algorithms is\nthat when it comes to adding a new digest scheme most of the work lies in\nadding the code to support the new digest calculation anyway, not in the rather\ntrivial changes needed processing another header field. And I still think this\nis the case. But as long as the MD5 value could only appear in one place I\ndidn't care so much where it was, and while I did favor the \"put it in the\nheader content\" approach I now see that there are actually some advantages to\nputting it in the field name.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Dave Kristol wrote:\n\n>My tastes (obviously) run to a more evolutionary approach for HTTP.  I'm\n>unconvinced that the performance problems require a flash cut to a binary\n>protocol.  Spero has shown that doing multiple transactions over one\n>connection achieves signficant performance improvements.  His response is\n>to change HTTP drastically.  Mine is to do so within the current overall\n>design.\n\nAgree.  It certainly does seem that most of the performance improvements\nmade by HTTP-NG could also be made in HTTP-TOS, with a simple modification\nto allow multiple objects to be retrieved/posted over a single TCP/IP\nconnection.\n\nAre we really willing to switch to something totally different and\nincompatible, just to eke that last 10% speed improvement?  I'm skeptical.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\"Can I get a direct flight back to reality, or do I have to change planes\nin Denver?\" - The Santa Clause\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "Roy T. Fielding writes:\n > HTTP (or Internet mail) header fields are designed to optimize the\n > semantics of <message-attribute>:<message-value>.  From a design\n > perspective, the best way to process Internet messages is to read\n > them into a hash table with the field-name as the key.  Then, when\n > a particular system function needs a particular message attribute,\n > it does a simple table look-up to obtain the value of that attribute\n > (or nil if the attribute was undefined).\nHttp, (maybe unlike MIME?) *do have* multi keywords headers already\n( Like for example Allow:, Accept:, Authorization:, Pragma:,\nWWW-Authenticate:, etc..). Having a library that can parse inside\nheaders does not seem to me a problem (as it has to be done for other\nheaders too anyway).\n > Content-MD5 represents a message attribute.  Content-Digest does not.\nIt represent the attributes that there is *a* Digest available.\n > Content-MD5 allows the recipient's semantic parser (the step after\n > message parsing) to determine if an MD5 has been calculated without\n > parsing the contents of any particular field.  Content-Digest does not.\n > Content-MD5 will make it easy for me to define a generic IMS construct of\n >     GET / HTTP/1.1\n >     Unless: Content-MD5 eq \"qyeur87168587646846109hgs==\"\n > or  \n >     PUT /happy HTTP/1.1\n >     Unless: Last-Modified ne \"Mon, 06 Nov 1995 00:51:58 GMT\"\n > Content-Digest would not.\nI don't know about IMS, but I expect any WWW lib to be able to write\nsame simple constructs even about multi-token headers\n( even simple regexp do the job\n  Unless /Content-Digest:.*MD5==18a34ec98aa84b961890470434f163fa/\n)\n\n > >From a perspective of having designed several HTTP systems, Content-MD5\n > (and Content-SHA, Content-Sum, etc.) is a better design choice for the\n > HTTP protocol.\nIt is *slightly* easier to process for the simplest client indeed,\nBut it looks very ugly, does not provide extensibility and flexibility\nthat changes in algorithm will demand,..etc...\nAlso the current Content-MD5 does not allow manual checking\n > Until it is proven otherwise, Content-MD5 will be the\n > preferred choice for HTTP.\nIs there any chance of changing your mind ?\nWhat do you think of the above, and of my proposal ?\n( http://hplyot.obspm.fr/~dl/draft-contentdigest.txt for current\nversion)\n\nThx,\nRegards\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\nTreasury cracking [Hello to all my fans in domestic surveillance]\n genetic Peking Mossad Castro\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> wrote:\n  > Aside from the issues of duplication and the history of Content-MD5,\n  > nobody has presented a valid design reason for defining a generic\n  > header field.\n  [...]\n\nI doubt this is an adequate reason, but let me identify an issue.  (BTW,\nI take no position regarding Content-Digest vs. Content-XYZ for XYZ in\n{MD5, SHA, ...}.)\n\nIf there's more than one digest header, we have to define what it means\nif a message contains more than one, and they disagree about the\nintegrity of the message.  Example:\n\nI have headers\nContent-MD5: xyz\nContent-SHA: qrs\n\nThe recipient computes the digests of the message and finds that the MD5\ndigest matches xyz, but the SHA digest does not match qrs.  Now what?\nI imagine we assume the integrity to be compromised.\n\nWith a single Content-Digest header, there's no ambiguity.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "Dave Kristol writes:\n > I have headers\n > Content-MD5: xyz\n > Content-SHA: qrs\n > The recipient computes the digests of the message and finds that the MD5\n > digest matches xyz, but the SHA digest does not match qrs.  Now what?\n > I imagine we assume the integrity to be compromised.\n > With a single Content-Digest header, there's no ambiguity.\nAhem, the mecanism I suggested does not state you have only one\nalgorithm key pair, you can have one or more (maybe that's not a good\nthing, and can be changed,... but..)\nIn your example it would be \"Content-Digest: MD5=xyz SHA=qrs\" {or\nmaybe with added \";\"} and as you suggested if they disagree you can\nprobably deduce there is a problem. So in this respects there are no\ndifferences.\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "At 9:21 AM 11/6/95, Laurent Demailly wrote (quoting Dave Kristol):\n> > I have headers\n> >      Content-MD5: xyz\n> >      Content-SHA: qrs\n> > The recipient computes the digests of the message and finds that the MD5\n> > digest matches xyz, but the SHA digest does not match qrs.  Now what?\n> > I imagine we assume the integrity to be compromised.\n> > With a single Content-Digest header, there's no ambiguity.\n>\n>Ahem, the mecanism I suggested does not state you have only one\n>algorithm key pair, you can have one or more (maybe that's not a good\n>thing, and can be changed,... but..)\n\nNo, you want to be able to have more than one digest.  From RFC 1810,\n\"Report on MD5 Performance,\" (last para. of \"Security Considerations\"):\n\n>It is important to the use of authentication in high-performance\n>systems that an alternative mechanism be available in IPv6 from the\n>outset.  This may require the specification of multiple \"required\"\n>authentication algorithms - one that's slower but believed strong,\n>and one that's faster but may inspire somewhat less confidence.\n\nIf we pretend for a moment that we really do care about speed issues, and\nif we agree with Harald's assertion that Content-MD5 is not intended to\nprovide security at all, then RFC1810 suggests that we will want some\ndigest _other_ than MD5 (perhaps the 16-way block chaining solution\nproposed in 1810).\n\nHowever, it is conceivable that someone, somewhere will want a digest that\ndoes provide some strength (perhaps while encapsulating HTTP, which could\nmake header forgery more difficult).  In that case, they would care very\nmuch about the strength of the digest, and less about speed.  Perhaps,\nthen, Content-MD5 is a good thing, were its strength not at issue, but\nSchneier[1] suspects its strength _is_ at issue; so Content-SHA might be a\nbetter choice for this purpose.\n\nThese considerations suggest duplication could be advantageous: one digest\nfor speed, one for potential security issues.  If those are the axes, then\nneither side should be arguing for MD5 as HTTP's digest of choice.\nContent-MD5 would only be the best choice if we wanted to fulfill both\nmoderate security needs and speed optomization in one digest, which we\ndon't; or if nothing faster than MD5 can be found.  Can anyone say what the\nIPv6 people have decided after RFC1810?  Later RFC's suggest they are still\nlooking.\n\nThis still says nothing about whether Content-MD5: xxx (or whatever) should\nbe preferred to Content-Digest: MD5=xxx.  I prefer the former.  If UA's\nnormally wanted to send both a digest for speed and one for security, then\nContent-Digest would make more sense.  However, that seems to be the less\nlikely possibility.  It is more likely that every UA will want to send the\nspeedy digest, and a few here and there will want to send the secure digest\nas well.\n\nM. Hedlund <hedlund@best.com>\n\n[1] Bruce Schneier, _Applied Cryptography, Second Edition_ (New York: John\nWiley & Sons, Inc., 1996), 441 (Section 18.5).\n\n\n\n"
        },
        {
            "subject": "Re: ContentMD",
            "content": "M. Hedlund writes:\n> At 9:21 AM 11/6/95, Laurent Demailly wrote (quoting Dave Kristol):\n[Dave Kristol]\n> > > I have headers\n> > >      Content-MD5: xyz\n> > >      Content-SHA: qrs\n> > > The recipient computes the digests of the message and finds that the MD5\n> > > digest matches xyz, but the SHA digest does not match qrs.  Now what?\n> > > I imagine we assume the integrity to be compromised.\n> > > With a single Content-Digest header, there's no ambiguity.\n[Laurent Demailly]\n> >Ahem, the mecanism I suggested does not state you have only one\n> >algorithm key pair, you can have one or more (maybe that's not a good\n> >thing, and can be changed,... but..)\n[M. Hedlund]\n> No, you want to be able to have more than one digest.  From RFC 1810,\n> \"Report on MD5 Performance,\" (last para. of \"Security Considerations\"):\n[Endre Balint Nagy]\nUntil we not specify some digest-negotiation scheme, servers will compute\nand send all digests they can compute in hope that the client undertands \nat least one of them.\nBTW. Using SHA is legal outside US?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: State Wars, part XI (was: Revised Charter",
            "content": "Brian Behlendorf writes:\n > On Fri, 3 Nov 1995, Koen Holtman wrote:\n > > M. Hedlund:\n > > >[Lou Montulli:]\n > > >>In fact, [Dave's proposal] reduces the capibilities such that it is nearly\n > > >>unusable for large scale applications such as online shopping.\n > > >\n > > >I disagree (as a programmer for an online shopping site).  I do agree that\n > > >an extremely large online shopping site with many stores or large ordering\n > > >possibilities might hit an upper limit with the state-info proposal if they\n > > >tried to store all ordering info in the State-info: header.  This concern,\n > > >however, is not pressing for the vast majority of ordering systems, which\n > > >accumulate a small number of items in each session.\n > > \n > > I agree.\n > \n > Besides, hopefully this \"extremely large shopping site\" will be sufficiently\n > funded and motivated to build a shopping cart Java applet which will store\n > everything in a client-side database so none of this information needs to get\n > transmitted over the wire more than twice (once at browse-time, once at\n > order-time) and there's no limit to what can go in your shopping cart.  Oh,\n > the Java applet API doesn't have persistant object support - oops!  Big\n > mistake there.  Hopefully the API v2 will fix that... \n\nThis doesn't seem to be a blocking problem. I have experienced this\nsolution with 1alpha3 API, and it seems doable, although the stuff\nyou'll find at ftp://koala.inria.fr/pub/abs/java/sb.tgz works just\nenough to say this ;-) \n\nBTW: the nicest thing about this scheme is that you don't need any\nmore proxy-defeating stuff in implementing shopping basket.\n\nAnselm.\n\n\n\n"
        },
        {
            "subject": "questions on httpdraft oct14 199",
            "content": "Hello\n\nI am looking at HTTP 1.0 draft (oct 14 1995 )and trying to implement HTTP\nserver. I got some questions\n\nWhy certain headers fields are missing\n\ne.g\n\naccept\nwithout this you don't know what type date\nclient can accept\n\naccept-charset\n\nwithout this you don't know what type charecter sets\nclient can accept\n\naccept-encoding\n\nwithout this you don't know whether client can perform\ndecode on data, If server sends encoded date\n\naccept-language\n\nThis never made any sense because IANA language\ncodes are not defined anywhere. I may be totally\nwrong. I saw only three language codes on ics\nweb site.\n\ncharge-to\n\n????\n\nhow come hopmann extension are not included\n\ne.g\n\nietf-draft on http-extension\n\nconnection: maintain\n\nWhy some methods are gone out of the draft\n\n\ndelete\npost\nlink\nunlink\n\nI never understood what link and unlink does \nIf any body has an example of link and unlink\nI would appreciate it.\n\nI need explanations from some body from the working group\nwhat impact will it have on new server implemetations\n\nThanks\nramana kovi\nramana@kovi.com\n\n\n\n"
        },
        {
            "subject": "Re: questions on httpdraft oct14 199",
            "content": "> Hello\n> I am looking at HTTP 1.0 draft (oct 14 1995 )and trying to implement HTTP\n> server. I got some questions\n> Why certain headers fields are missing [...]\n\nI think the changes you are referring to are the result of a recent\ndecision to write the first HTTP RFC as a \"best current practice\"\ndocument rather than a standards-track RFC. \n\nAs a result of this is it planned that the next spec written will\nbe called HTTP 1.1 and be standards-track, but will closely\nresemble what prior drafts called HTTP 1.0. (And what we were\npreviously planning to call HTTP 1.x may become HTTP 1.(x+1) etc.)\n\nSo the features you are looking for _are_ still part of the big picture.\n\nSee also:\n\nhttp://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/\n\nhttp://www.ics.uci.edu/pub/ietf/http/\n\n\n\n"
        },
        {
            "subject": "New Standard",
            "content": "Hello folks,\n\nWe have a new standard for extending HTML to include programming;  we\nare calling our language Meta-HTML.  Our documents get run through an\ninterpreter where HTML comes out like it went in and Meta-HTML gets\nexecuted and the results substituted inline.\n\nRecently however, I noticed there was some confusion about which\norganization to address for standards, so pointers would be\nappreciated.\n\nThanks,\nJoe\n\n----\nJoseph Arceneaux\nSamsara Partners\n\nhttp://www.samsara.com\njla@samsara.com\n+1 415 648 9988 (direct)\n+1 415 341 1395 (fax)\n+1 500 488 9308\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-luotonen-ssl-tunneling01.tx",
            "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts\ndirectories.\n\n       Title     : Tunneling SSL Through a WWW Proxy\n       Author(s) : A. Luotonen\n       Filename  : draft-luotonen-ssl-tunneling-01.txt\n       Pages     : 5\n       Date      : 11/06/1995\n\nThe wide success of SSL (Secure Sockets Layer from Netscape Communications\nCorporation) has made it vital that the current WWW proxy protocol be\nextended to allow an SSL client to open a secure tunnel through the proxy.\n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-luotonen-ssl-tunneling-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-luotonen-ssl-tunneling-01.txt\n \nInternet-Drafts directories are located at:\n                                                        \n     o  Africa\n        Address:  ftp.is.co.za (196.4.160.8)\n                                                        \n     o  Europe\n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                        \n     o  Pacific Rim\n        Address:  munnari.oz.au (128.250.1.21)\n                                                        \n     o  US East Coast\n        Address:  ds.internic.net (198.49.45.10)\n                                                        \n     o  US West Coast\n        Address:  ftp.isi.edu (128.9.0.32)\n                                                        \nInternet-Drafts are also available by mail.\n                                                        \nSend a message to:  mailserv@ds.internic.net. In the body type:\n     \"FILE /internet-drafts/draft-luotonen-ssl-tunneling-01.txt\".\n                                                        \nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n                                                        \nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "HTTP  why not multiple connections",
            "content": "The reason why Netscape's solution should be viewed as a short-term\nworkaround, rather than the correct design for some future version\nof HTTP, is that it starts from scratch with each document.  That is,\nthe use of parallel connections  does reduce the latency for inlined\nimages, but it can't reduce the latency for retrieving the enclosing\nHTML file.  Which is especially a problem if the HTML file has no\nimages (or if the client has them all cached).\n\nPersistent-connection HTTP, on the other hand, means that as long\nas a client is retrieving URLs from a given server (within a\nreasonable timeout period), the slow-start costs are paid once,\nand the inlined-image retrievals can be pipelined (thus parallelizing\nalmost every aspect of their retrieval as well as Netscape can do.\nBut see below for a slight wrinkle.)  Note that if you are at the end\nof a 56kbit/sec pipe, for example, retrieving images in parallel doesn't\ndo any better than retrieving them sequentially with a \"started-up\"\nTCP connection, and in fact might do a lot worse.\n\nI pointed this out at the BOF last week, but it bears repeating:\nbandwidth is improving, computers are improving, but the speed of\nlight is a constant (and the planet is not shrinking).  Netscape\nhas to pay at least one extra RTT for each HTML file, over what\npersistent-connection HTTP would pay (after the first one).\n\nAnd if bandwidth *is* a problem, Netscape's approach doesn't help,\nbecause it generates a number of extra packets (TCP SYNs and FINs)\nfor each image.  Also, the use of many short connections instead\nof one long connection is likely to foil attempts to improve\ncongestion control by assigning per-connection resources at the routers,\nsince the connections won't last long enough to make this pay off.\n\nFinally, multiple connections do impose real costs at the server.\nI'll send a separate message showing some preliminary simulation\nresults that underscore this point.\n\n    The fastest way to get all the text is to send it first but it\n    can't be displaye until layout information like the size and shape\n    of all images is known.  This is the point of the Netscape multiple\n    connections.  They get the first few bits of each image which\n    contain the size information.\n    \nThis is the one thing that Netscape's approach does help with.  I\nadmit that it's nice to see the text laid out before the images arrive.\nBut there is more than one way to solve this problem, and I think\nNetscape's solution causes more network-related trouble than necessary.\n\nFor example, as long as we are making minor changes to HTTP anyway,\nhow about definining a new method called \"GET_BOUNDING_BOXES\" (or\npick a shorter name).  This would take a list of URLs, and the server\nwould return the appropriate image attributes (height, width, maybe\na few other parameters) for each image.  If the server doesn't want\nto do this, or if one of the URLs is in an unrecognized format, no\nproblem; the client just has to wait until the real image is retrieved.\n\nIn this model, a typical interaction would be:\n\nClientServer\n\nsends GET\nsends HTML\nparses HTML\nsends GET_BOUNDING_BOXES\n  listing uncached images\nsends GETLIST\n  listing uncached images\nsends bounding boxes\nsends GIFs/JPEGs/whatever\n\nWe're still looking at only two \"necessary\" round-trip times, plus\nnetwork bandwidth latencies.  The server now has to \"understand\"\nthe image formats enough to extract the bounding boxes, and I'm\nsure some of you purists will scream bloody murder about that.\nBut look at how much code already exists in an HTTP server, and\nhow much would have to be added to do this, and then try to tell\nme with a straight face that it's too hard to do.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-luotonen-ssl-tunneling01.tx",
            "content": "Hi Ari,\nhi all,\nas I see, the draft is fine, and after assigning an RFC number we can\nincorporate it by reference into 1.1 or 1.2 HTTP protocol.\n\nI see only one unresolved case:\nhow to deal with multiple firewalls?\nConsider company.com connected to the internet trough its firewall\n'gatekeeper.company.com' and having an even more protected subnet\n'finance.company.com' connected to company backbone trough a second firewall.\nCurrently we have not that many systems configured this way, but this should\nnot imply that in future the situation remains the same.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: New Standard",
            "content": "In message <m0tDHyv-000A4WC@emptiness.samsara.com>, Joseph Arceneaux writes:\n>Recently however, I noticed there was some confusion about which\n>organization to address for standards, so pointers would be\n>appreciated.\n\nHmmm... you're not the first person to ask this. The info must be\na little hard to find. I'll update the HTML page to answer it:\n\nhttp://www.w3.org/pub/WWW/MarkUp/#discussion\n\n==================================\nHTML Groups, Discussion Forums, and\nArchives\n\ncomp.infosystems.www.authoring.html \n      A USENET newsgroup where HTML authoring issues are discussed.\n      \"How To ...\" questions should be addressed here. Note that many\n      issures related to forms and CGI, image maps, transparent gifs, etc.\n      are covered in the WWW FAQ. \nwww-html \n      A technical discussion list, with a hypertext archive (now searchable!\n      Thanks EIT guys!). \n\n      If you have a proposal for a change to HTML, you might start a\n      discussion here to see what other developers think of it. Always check\n      the archive first! \nHTML Working Group \n      W3C staff members edit and review HTML standards in the HTML\n      working group of the IETF. \n            WG mailing list archives \n            Internationalization of the Hypertext Markup Language, 15\n            August 1995 \n            forms-based file upload \n            client-side image maps \n\n      If you have researched a proposal thoroughly and at least started to\n      get some implementation experience, you may submit your proposal to\n      html-wg for standardization in the IETF. \n\n      If you are new to the IETF, you should probably do some background\n      reading. I recommend: \n            IETF Working Group Process \n            Guidelines to Draft Authors \n            The TAO of the IETF \n========================================\n\n\n>We have a new standard for extending HTML to include programming;  we\n>are calling our language Meta-HTML.  Our documents get run through an\n>interpreter where HTML comes out like it went in and Meta-HTML gets\n>executed and the results substituted inline.\n\nStandards are needed when large groups of folks need to agree on\nthings; e.g. the HTTP standard is necessary because every web browser\nand server have to agree.\n\nAn agreement about a server-side processing technique is between the\nimplementor/vendor, and the user/customer. So I'd agree that you need\ndocumentation/specification which might benefit from wide review, but\nI don't think you need a standard -- unless you expect to have lots\nof vendors producing products that need to agree on Meta-HTML (as is\nthe case with CGI). And even then, it's not clear that this is in\nthe scope of the charters of HTTP-WG nor HTML-WG.\n\nNote the Reply-To: www-html@w3.org.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-luotonen-ssl-tunneling01.tx",
            "content": "> I see only one unresolved case:\n> how to deal with multiple firewalls?\n\nThe SSL Tunneling I-D applies identically to connections between a\nclient and a proxy, and between two proxies.  The inner proxy then\nacts as a client to the outer proxy.\n\nThis functionality of going through multiple firewalls is actually\nalready available in Netscape Proxy Server.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: CGI??",
            "content": ">I've seen a number of requests for a more formal specification for CGI\n>than  <URL:http://hoohoo.ncsa.uiuc.edu/cgi/>.\n>\n>This leads to the following questions:\n>\n>1) Is CGI appropriate for an RFC? If not, what standards group would\n>        it belong to?\n\nThe W3C, probably, if they were interested.\n\n> 2) If it's appropriate for an RFC, is this this WG it would belong to?\n>        If not, which does it belong to?\n\nwww-talk, I suppose.\n\n> 3) Is anyone working on such a thing now?\n\nYes. I have written a fairly formal specification of CGI.\nSee <URL:http://www.ast.cam.ac.uk/~drtr/cgi.html>\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    Date: Fri, 03 Nov 1995 09:36:58 -0800 (PST)\n    From: Ned Freed <NED@innosoft.com>\n\n    You've provided the list in the form of a URL that appears to be\n    attached to your site.\n\nI wasn't making an effort to be formally complete.  So you shouldn't assume\nthat I cannot provide a more stable reference.  See:\n\nEthnologue, Languages of the World, Twelfth Edition, Barbara F. Grimes,\nEditor, 1992, SIL, Dallas TX. ISBN 0-88312-815-2.\n\nEthnologue Index, Twelfth Edition, Barbara F. Grimes, Editor, 1992, SIL,\nDallas TX. ISBN 0-88312-819-5.\n\nI'd be happy to move this discussion over to the MAILEXT WG, though I'm\nnot entirely convinced that is the pertinent forum for it to occur.  At\nleast in the sense that the issue of language tags extends to all application\nareas, not simply mail.\n\nRegards,\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "    Date: Fri, 3 Nov 95 21:02:21 +0100\n    From: Olle Jarnefors <ojarnef@admin.kth.se>\n\n    One thing that I don't understand is why you insist on 3-letter\n    tags nnn instead of 7-letter tags I-S-nnn for Ethnologue-based\n    language labels.\n\nI would accept the I-S-nnn form, and would encourage the folks working\non the new ISO 3 char standard to accept as an input document the existing\nEthnologue list.\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: Language tags (Re: Statistics on reusing request",
            "content": "> I'd be happy to move this discussion over to the MAILEXT WG, though I'm\n> not entirely convinced that is the pertinent forum for it to occur.  At\n> least in the sense that the issue of language tags extends to all application\n> areas, not simply mail.\n\nThe language tag document came out of that group. I believe that means that\ndiscussion of revisions to that document have to occur there unless you can\nconvince the area directors that a change of venue is appropriate. You're\nwelcome to try that if you like.\n\nNed\n\n\n\n"
        },
        {
            "subject": "Comments on Byte range draf",
            "content": "In the range specification section it says:\n\n* In the case that the second integer is smaller than the first\n  one, an empty range is returned.\n\nSince the specification says just above that point that:\n\n* The first integer must always be less than or equal to the second\n  one.\n\n\nI would suggest that in cases where the second integer is smaller than\nthe first, an error message indicating a malformed header should be\nreturned (rather than an empty range).  An empty range doesn't tell\nthe requester what went wrong; an error message might.\n\nIn the section on the return of multiple ranges as multipart MIME\nmessages, the draft says \"A server may send also a single byte range\nas a multipart message.\"  Why should a server send a single byte\nrange in a multipart message, and would that break any clients expecting,\nnot unreasonably, multiple parts to a multi-part message?\n\nI also note that the draft gives the response Range header as\nRange: bytes x-y/z .  Is z in use to handle situations where a\nnew version has occurred between the clients request of one byte\nrange and another, and, if so, isn't that already handled by the \nother headers (you note that the if-modified-since works with\nthis)?  I also assume that the Last-modified header and if-modified\nsince header will generally be applied to the document as a whole, \nnot to the individual byte-ranges, but I'm afraid I didn't find\nthe draft to be too clear on this point.\n\nRegards,\nTed Hardie\nNAIC\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-luotonen-ssl-tunneling01.tx",
            "content": "[Endre Balint Nagy]\n> > I see only one unresolved case:\n> > how to deal with multiple firewalls?\n[Ari Luotonen]\n> The SSL Tunneling I-D applies identically to connections between a\n> client and a proxy, and between two proxies.  The inner proxy then\n> acts as a client to the outer proxy.\n[Endre Balint Nagy]\nI read this as:\nmultiple firewall travelsal has no efect on the protocol, it's a proxy\nimplemenation issue.\nPartially agree.\nAs I see, the (HTTP) protocol has no features to report the server/proxy\nwhich generated the error response when something went wrong.\nClient      <->     firewall1           <->           firewall2 <-> server\n      connect ->\n      <- 407 proxy-authenticate\n      connect/proxy-authorisation ->\n      <- ???\n                   connect\n   407 proxy-authenticate\n      407 proxy-authenticate\n???\nIn some cases irrelevant, on which stage the problem occured, but in case of\nauthentication it is relevant.\nIf \"100 connecting to gatekeeper\" stays in the place of the first ???,\nthe client will know that the second 407 generated by gatekeeper.\n(Alternatively, the second 407 can be handled at the first proxy, depending\non authentication scheme.)\nAs far as I know, we have no standardised proxy-authenticate, only have\na placeholder. While it is a placeholder, the ssl-tunneling is fine, but\nwhen proxy-authenticate is elaborated in detail, some modifications will be\nneeded.\nOf course, this objection applies mostly to the 1.1 draft, not to ssl-tunneling.\n\n> This functionality of going through multiple firewalls is actually\n> already available in Netscape Proxy Server.\nWith autenthication on both firewalls?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: CGI??",
            "content": "> From: drtr1@cus.cam.ac.uk (David Robinson)\n> > 2) If it's appropriate for an RFC, is this this WG it would belong to?\n> >        If not, which does it belong to?\n> www-talk, I suppose.\n\nMakes sense. I've changed this to Cc: to the http-wg, and To:\nwww-talk. Would those replying please drop the http-wg from the Cc:\nlist?\n\n> > 3) Is anyone working on such a thing now?\n> Yes. I have written a fairly formal specification of CGI.\n> See <URL:http://www.ast.cam.ac.uk/~drtr/cgi.html>\n\nNice work. I wish I'd had when I was implementing mine. It's good to\nsee that W3C is pointing to that as the spec as well. However, I have\na few quibbles. Naturally, I can't seem to reach hoohoo.ncsa.uiuc to\ncheck some things on it, either. Grumble.\n\nOn to the quibbles.\n\n1) The AUTH_TYPE section mentions HHTP. Presumably this is a typo.\n \n2) I think it should be changed to allow multiple protocols in both\nGATEWAY_INTERFACE and SERVER_PROTOCOL. Follow the HTTP draft for the\nUser-Agent: field. This is a real change from current behavior, but I\nbelieve such an implementation would meet the NCSA spec. As an example:\n\nGATEWAY_INTERFACE = CGI/1.1 NCSA/1.4\n\nindicates that this server supports the extra variables from NCSA 1.4,\netc. SERVER_PROTOCOL already has an \"extension-version\" option, this\njust generalizes that.\n\n3) The CGI command line description has some serious errors. You don't\nget an argument only for ISINDEX query; not even according to this\nspec. An imagemap query meets the criteria as stated, and the NCSA 1.3\nimplementation certainly provided an argument (and the supplied\nimagemap program expected one). I also don't see any reason to\nrestrict the argument to a GET request, unless you want to actively\ndiscourage people from putting data in command line arguments.\n\n4) I think the reference to the \"array of strings\" in the CGI command\nline section is misleading.  Currently, only one is used, and it's\npassed to the script as if it were the first argument and the script\nhad been invoked from the command line. Better wording might be:\n\nSome platforms support a method for providing one or more\nargument strings to a script. This first argument is used in\nthe case of .... If there is no such argument, it should be\nthe empty string, and further arguments are reserved for\nserver-specific uses.\n\nSorry, I don't feel up to trying to rephrase the \"ISINDEX\" quote to\ncover the missing cases when it's not clear what those are yet.\n\n5) As to telling the difference between a missing body and a\nzero-length one, the presence (or absense) of Content-Type: can do the\ntrick. If it's not there, there's no body. If it is there, the body\nhas a length of zero.\n\n6) Under requirements for servers, you say:\n\nServers should reject with error 404 any requests that would\nresult in an encoded \"/\" being decoded into PATH_INFO or\nSCRIPT_NAME. \n\nNote: this should be made a requirement.\n\nWhy? Remember that the justification can't include the semantics of\nthe underlying file system.\n\n7) The \"Data input\", and \"Data output\" requirements should be\nappreciably tightened, to indicate that servers should tie the default\ninput and output streams to the incoming object and output to the\nclient. Since it may not be possible, we can't require it. But that's\nthe desired behavior if it's possible.\n\n8) You've made a quiet change to the behavior of parsed headers -\nrequiring that the CGI headers appear before the HTTP headers. A good\nidea, but this should be noted.\n\n9) It appears you've made another quiet change to the location:\nheader. I recall that NCSA required URL's that resolved to the server\nto be handled on the server, whereas you've chaged it to be only URL's\nwithout a scheme. I heartily approve, but think this should be noted\nas well.\n\n10) If the recommended changes to command line and I/O streams\nsections are made, then those parts of the system-specific information\nfor unix are redundant, as it complies with the recommended behavior.\n\n11) Amiga system-specific standards:\n\nNot a quibble, an addition to the system-specific information at the end:\n\nAmigaDOS\n\nEnvironment variables\n\nThese are accessed by the dos.library routine GetVar. The flags\nargument should be 0. Case is ignored, but upper case is\nrecommend for compatability with case-sensitive systems.\n\nThe command line and I/O streams follow the \"should\" recommendations,\nand aren't required in this one.\n\nThanx,\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: CGI??",
            "content": "According to Mike Meyer:\n> \n> 3) The CGI command line description has some serious errors. You don't\n> get an argument only for ISINDEX query; not even according to this\n> spec. An imagemap query meets the criteria as stated, and the NCSA 1.3\n> implementation certainly provided an argument (and the supplied\n> imagemap program expected one). I also don't see any reason to\n> restrict the argument to a GET request, unless you want to actively\n> discourage people from putting data in command line arguments.\n> \n\nMy recollection from the time the spec at NCSA was written is that\neveryone did want to discourage people from putting data in \ncommand line arguments.  Some people wanted to completely exclude\nit, but that was ruled out because of its existing use with ISINDEX.\nI think it was decided to allow it only in ISINDEX.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "HTTP POST streamin",
            "content": "I am interested in applications that can make use of having a\ncontinuous two-way connection to a web server and HTTP protocols that\nsupport it (and especially how HTTP proxies support it).\n\nRelevant is the IETF HTTP-WG Session Extension protocol, HTTP-NG,\nKeep-Alive, and perhaps server push, client pull protocols, although I\nam trying to sort them out in the context of my little test example below.\n\nHere is an example CGI script that is continuously reading data from\nthe client (in this example, telnet is used for the client) and\nresponding in a dialog fashion. This works using the NCSA httpd. (I\nsay Content-Length: -1 just to mean I don't know how much data I will\nsend). It doesn't work going through a CERN proxy, since the proxy\nseems to buffer up the POST data before sending it.\n\n------------------------------------------------------------------\n#!/share/bin/perl\n\n#  Server: NCSA httpd 1.4\n#  CGI:    /cgi-bin/nph-test-session.p\n#  Usage:\n#\n#  % telnet <host> <port>\n#  POST /cgi-bin/nph-test-session.p HTTP/1.0\n#  Content-Length: -1\n#  <empty line here>\n#  (type some non-empty lines with return and end with empty line)\n#\n\nrequire \"flush.pl\";\nprint \"HTTP/1.0 200\\n\";\nprint \"Content-type: multipart/x-mixed-replace;boundary=---ThisRandomString---\\n\\n\";\n&printflush(stdout,\"---ThisRandomString---\\n\");\nwhile (<STDIN>) {         # read POST data as it comes in\n    chop; chop;           # get rid of CRLF\n    if (length($_) < 1) { exit 0; }  # exit when blank line is sent\n    &printflush(stdout,\"Content-type: text/plain\\n\\n\");\n    printf(\"You said  %s   and transmitted %d chars\\n\",$_,length($_));\n    &printflush(stdout,\"---ThisRandomString---\\n\");\n}\n------------------------------------------------------------------------\n\nHere is an example use, where > indicates data typed by the client:\n\n------------------------------------------------------------------------\n> telnet gamera 8001\nTrying 157.170.34.48 ...\nConnected to gamera.\nEscape character is '^]'.\nPOST /cgi-bin/nph-test-session.p HTTP/1.0\nContent-Length: -1\n \nHTTP/1.0 200\nContent-type: multipart/x-mixed-replace;boundary=---ThisRandomString---\n \n---ThisRandomString---\n> hello\nContent-type: text/plain\n \nYou said  hello   and transmitted 5 chars\n---ThisRandomString---\n> there\nContent-type: text/plain\n \nYou said  there   and transmitted 5 chars\n---ThisRandomString---\n> \nConnection closed by foreign host.\n------------------------------------------------------------------------\n\nHere the idea is that POST data is continuously streaming to the\nserver and the server streams back (examples: continuously\nmoving in a VRML world while the other participants are moving around\nyou, moving a robot arm, bi-directional video communication, ...).\n\nI am interested in how \"POST streaming\" fits into the current\nsession ideas.\n\n                                      Philip Thrift\n\nNET: thrift@ti.com                    TI Corporate Software Laboratory\nTEL:(214) 995-7906                    P.O. Box  655474  M.S. 238\nFAX:(214) 995-0304                    Dallas, TX 75265\n\nURL: http://www.nowhere.net/~thrift/\nURL: http://mothra.csc.ti.com:8001/                  (TI Internal)\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-luotonen-ssl-tunneling01.tx",
            "content": "One important security note that should be added to this draft: any client\nthat supports this extension MUST perform some correlation between the\nrequested hostname and the certificate returned in the SSL connection -\notherwise the system becomes vulnerable to trivial Person-In-The-Middle\nattacks, with much less effort that conventional attacks. \n\nNo currently deployed systems perform this checking; the next release of \nthe Netscape 2 Beta does do the checking; previous versions are \nvulnerable. \n\nSimon\n\n\n----\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1) ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "You don't need content-length to search for the boundary. You can go\nahead and scan the binary data for <CR><LF>--boundary--<CR><LF>. Doing\nso either using a simple scan or using a more elaborate boyer-moore\nalgorithm should be computationally not significantly more expensive\nthan merely counting bytes. \n\nAt first glance, you might not think so, but both searching and\ncounting are order-N algorithms; some systems have optimized\nblock-move, but most C programmers don't think twice about using\nstrcpy, which is, after all, searching for a null byte. Using\nboyer-moore, searching for a long string is more efficient than\nsearching for a short one.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "I suggested before and will suggest again that byte ranges do not\nbelong in URLs, since the results are not in fact any of the media\ntypes that were identified in \"accept:\" or are likely to be reported\nby content-type, and that byte ranges are likely to become incorrect\nif the original data for which the range was valid is replaced with a\nnew version which does not exactly correspond byte-for-byte by the new\nstructure. \n\nIt is a category error. If you want to refer to a part of a structure\nby name or structural tag, that would at least be more robust (\"give\nme the catalog of this PDF document, or give me the data for page 1\").\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "> I suggested before and will suggest again that byte ranges do not\n> belong in URLs, since the results are not in fact any of the media\n> types that were identified in \"accept:\" or are likely to be reported\n> by content-type, and that byte ranges are likely to become incorrect\n> if the original data for which the range was valid is replaced with a\n> new version which does not exactly correspond byte-for-byte by the new\n> structure. \n> \n> It is a category error. If you want to refer to a part of a structure\n> by name or structural tag, that would at least be more robust (\"give\n> me the catalog of this PDF document, or give me the data for page 1\").\n\nI'll second this.  I tried to work this stuff into the framework of\nHTTP and it just doesn't fit as currently described.  My conclusion is\nthat there are only three ways to do what you want:\n\n   1) The Right Way\n\n      Treat all PDF files as a tree, containing a catalog of pages,\n      chapters, or whatever other components can be identified within\n      the PDF.  Assign each of these components a URL and make the server\n      smart enough to extract the right component.\n\n   2) The Unlikely Way\n\n      Redefine a network PDF which allows easy inclusion of component\n      parts by URL reference rather than the traditional monstrosity.\n\n   3) The Partial Way\n\n      Define a new method PART and use that for partial GET requests.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Roy T. Fielding wrote:\n> \n> > I suggested before and will suggest again that byte ranges do not\n> > belong in URLs, since the results are not in fact any of the media\n> > types that were identified in \"accept:\" or are likely to be reported\n> > by content-type, and that byte ranges are likely to become incorrect\n> > if the original data for which the range was valid is replaced with a\n> > new version which does not exactly correspond byte-for-byte by the new\n> > structure.\n> >\n> > It is a category error. If you want to refer to a part of a structure\n> > by name or structural tag, that would at least be more robust (\"give\n> > me the catalog of this PDF document, or give me the data for page 1\").\n> \n> I'll second this.  I tried to work this stuff into the framework of\n> HTTP and it just doesn't fit as currently described.  My conclusion is\n> that there are only three ways to do what you want:\n> \n>    1) The Right Way\n> \n>       Treat all PDF files as a tree, containing a catalog of pages,\n>       chapters, or whatever other components can be identified within\n>       the PDF.  Assign each of these components a URL and make the server\n>       smart enough to extract the right component.\n> \n\nYou and Larry are looking at this problem with blinders on.\nThere are many more uses for byterange URL's than simply\nPDF files.  For instance Netscape 2.0 uses byteranges to\nrequest parts of files that it didn't get the last time\nyou came to a page.  You can therefore interrupt a page\nduring download at any time and continue it exactly\nwhere you left off when you come back to the page.  This\nmakes cachine up to 50% more effective at saving bandwidth.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "> You and Larry are looking at this problem with blinders on.\n> There are many more uses for byterange URL's than simply\n> PDF files.  For instance Netscape 2.0 uses byteranges to\n> request parts of files that it didn't get the last time\n> you came to a page.  You can therefore interrupt a page\n> during download at any time and continue it exactly\n> where you left off when you come back to the page.  This\n> makes cachine up to 50% more effective at saving bandwidth.\n\nCheck out part (3) of what I sent -- asking for a partial retrieval\nof a resource demands another method, not a URL hack.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Header fields missin",
            "content": "Document: OCT 14 1995 IETF Draft Internet Draft 04 Best Current Practice\n\nI Need clear answer ? Are they are missed because of oversight\nand you guys trying make incompatable protocols\n\naccept\naccept-charset\naccept-encoding\naccept-transfer-encoding\naccept-language\ncharge-to\n\nThis are very important fields without them you really can't \nimplement a server.\n\n\n\nThanks\nramana kovi\nramana@kovi.com\n\n\n\n"
        },
        {
            "subject": "Methods missin",
            "content": "Document: OCT 14 1995 IETF Draft Internet Draft 04 Best Current Practice\n\nWhy Methods Post\n    Put\n    Delete\n    Link\n    Unlink\n\nare missing\n\n\nI Need clear explanation. Either they are not part HTTP 1.0 or not ?\n\n\nThanks\nramana kovi\nramana@kovi.com\n\n\n\n"
        },
        {
            "subject": "Connection Oriented HTTP conflic",
            "content": "I found descrepency in Hopmann specification about connection\noriented HTTP extension draft. I also confirmed the conflict \nwith Mr Hopmann. Iam attaching part of the mail message \nwith this note for other server developers.Please correct \nthe information in your downloaded version of the draft\n\n\n\n"
        },
        {
            "subject": "Re: ietf-draft on connectionoriented htt",
            "content": ">Hello Alex\n>\n>I think I found one conflict in your statement and example\n>       Can you have look at this and clarify if possible\nYou are correct. The example should have been Connection: maintain.\n>\n\n\n\n"
        },
        {
            "subject": "contentlanguag",
            "content": "test3.c\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "> You and Larry are looking at this problem with blinders on.\n> There are many more uses for byterange URL's than simply\n> PDF files.  For instance Netscape 2.0 uses byteranges to\n> request parts of files that it didn't get the last time\n> you came to a page.  You can therefore interrupt a page\n> during download at any time and continue it exactly\n> where you left off when you come back to the page.  This\n> makes cachine up to 50% more effective at saving bandwidth.\n\nYes, byte ranges are GREAT! They're wonderful. We should definitely\nhave byte ranges in HTTP! It's a wonderful addition. Honest!\n\nThey just don't belong at the end of arbitrary URLs. Maybe you want to\ndefine a new URL scheme that calls out a new extension to the HTTP\nprotocol?\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "In my last message, I wrote:\n    Finally, multiple connections do impose real costs at the server.\n    I'll send a separate message showing some preliminary simulation\n    results that underscore this point.\n    \nHere are those *preliminary* results.  Please do not quote/cite them\noutside this mailing list, since I haven't finished checking my\nsimulations for errors.\n\nThese simulations are driven by logs I made on the 1994 California\nElection server (http://www.election.ca.gov/, or if that doesn't work, try\nhttp://www.election.digital.com/).  This was actually a group of\nthree machines, and in this message I'm only going to talk about what\nhappened on one of them.  I'll be running simulations for the entire\ncluster later on.\n\nOur HTTP servers generated an extra set of log entries, with fine-grain\ntimestamps and connection durations; this allows me to simulate things\ndown to the millisecond.  Note that we ran a vanilla HTTP implementation;\nwe were not running any kind of persistent-connection protocol.  And the\nlogs obviously don't reflect connections that were attempted and\nrefused; I'll have to dig out the tcpdump traces and analyze them before\nI can tell what load was actually presented.\n\nIn this trace, over a period of a few days, we did 526788 connections.\n(Most of those were in a 48-hour period, Nov. 8-9).\n\nAnyway, my first simulation assumed \"vanilla\" HTTP and a limit of\n64 server processes.  At the peak, 27 connections were in progress\nat once.  The peak number of connections in \"TIME_WAIT\" state was\n2491, and the peak size of the PCB table was 2496 entries.\n\nThen I simulated persistent-connection HTTP, varying several paramters:\nthe maximum number of server processes, and the \"idle time limit\", the\nnumber of seconds after which the server would close an idle connection.\nThe server would also close an idle connection if it ran out of processes,\nusing an LRU scheme to choose the victim.\n\nI assumed that all requests from a given client IP address could be\ngrouped into one connection.  This is essentially true for workstations,\nPCs, and proxies; it isn't necessarily true for timesharing machines,\nbut we probably didn't see many of those.\n\nI assumed that clients never voluntarily closed their connections; in\nreal life, they probably would, and this would make my numbers generally\nmuch better.\n\nFor example, with 64 processes and a 60-second timeout, I got this\nresult:\n526788 requests\n66253 total connections opened\n726 PCB entries max\n662 TIME_WAIT entries max\n460535 requests \"hit\" already-open connections\nmaximum number of requests on a single connection: 1712\n23133 idle connections were closed due to lack of processes\nno connections were refused due to lack of processes\n43119 idle connections were timed out\nSo with these parameters, we get a factor of 4 fewer entries in\nthe PCB table, and a factor of ten fewer TCP connections.\n\nWith more generous parameters (a limit of 512 processes and a 10-minute\ntimeout), I got this result:\n526788 requests\nmaximum of 355 processes in use\n24182 total connections opened\n399 PCB entries max\n216 TIME_WAIT entries max\n502606 requests \"hit\" already-open connections\nmaximum number of requests on a single connection: 2754\nno idle connections were closed due to lack of processes\nno connections were refused due to lack of processes\n24180 idle connections were timed out\n\nWhile with more miserly parameters (32 processes and a 10-second timeout),\nI got:\n526788 requests\n152238 total connections opened\n1081 PCB entries max\n1050 TIME_WAIT entries max\n374550 requests \"hit\" already-open connections\nmaximum number of requests on a single connection: 738\n17885 idle connections were closed due to lack of processes\nno connections were refused due to lack of processes\n134352 idle connections were timed out\nSo even without significantly increasing the number of active\nconnections required, the persistent-HTTP model avoids most of\nthe TCP connections used by current HTTP, and shrinks the required\nPCB table size significantly.\n\nWe suspect that the reason why some connections were used to\nsatisfy so many requests is because they came from proxy servers.\n\nI also have simulated the number of \"near misses\": requests from\nclients that would be received just after the server closes the\nconnection.  Generally, the shorter the idle-timeout, the more\nof these I saw, and almost all came within the first second or\nso after the close.  So if you set the idle timeout to anything\nreasonable, you should see very few near misses.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Header fields missin",
            "content": "> Document: OCT 14 1995 IETF Draft Internet Draft 04 Best Current Practice\n> \n> I Need clear answer ? Are they are missed because of oversight\n> and you guys trying make incompatable protocols\n\nOne of the requirements of participating in an IETF working group is\nthat you make some attempt to read the mailing list archive first.\nIf you don't, you can post all you want but don't expect very many\nresponses.  It seems to me that you already received responses to these\nquestions from others.\n\n> accept\n> accept-charset\n> accept-encoding\n> accept-transfer-encoding\n> accept-language\n\nAre no longer defined as HTTP/1.0, though they may be implemented as\nextensions to the protocol (as they already have been on some servers\nand clients).\n\n> charge-to\n\nNo known implementation.\n\n> This are very important fields without them you really can't \n> implement a server.\n\nNone of them are necessary for a server.  Useful, yes.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Sat, 11 Nov 1995, Larry Masinter wrote:\n\n> Yes, byte ranges are GREAT! They're wonderful. We should definitely\n> have byte ranges in HTTP! It's a wonderful addition. Honest!\n\nAgreed...\n\n> They just don't belong at the end of arbitrary URLs. Maybe you want to\n> define a new URL scheme that calls out a new extension to the HTTP\n> protocol?\n\nOr a new HTTP method, as Roy Fielding suggested. But it seems to me that, \nlooking at how Netscape Navigator 2.0 actually uses said byte ranges, \nthere's a better solution. Namely, it uses them to get parts of PDF \nfiles, or to resume file transfers if disrupted. In each case, if the \nserver does not understand the byte range, the web browser will respond \nwith another request, one without the byte ranges. So why not cause this \nto be the default behavior, by adding a request header, something like \n\nRequest-Range: bytes=500-999\n\nThe server, if it understood it, would respond as per \ndraft-luotonen-http-url-byterange-00.txt (or whatever spec), and if not, \nit would just ignore it, being an unknown header, and simply return the \nentire document.\n\nIt seems that the only reason to put the byte ranges into the URL is to \nallow a stand-alone URL to contain partial file information. However, I \ncannot envision a situation where this would be more useful than the \nbrowser-generated variety of byte range requests, a la Netscape Navigator \n2.0.\n\nThoughts?\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: Methods missin",
            "content": "> Document: OCT 14 1995 IETF Draft Internet Draft 04 Best Current Practice\n> \n> Why Methods Post\n>     Put\n>     Delete\n>     Link\n>     Unlink\n> \n> are missing\n> \n> I Need clear explanation. Either they are not part HTTP 1.0 or not ?\n\nThey are not part of HTTP/1.0, though they may be implemented as extensions\nto HTTP/1.0.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Connection Oriented HTTP conflic",
            "content": "> I found descrepency in Hopmann specification about connection\n> oriented HTTP extension draft. I also confirmed the conflict \n> with Mr Hopmann. Iam attaching part of the mail message \n> with this note for other server developers.Please correct \n> the information in your downloaded version of the draft\n\nWe are not proceeding with the specification that Alex wrote.\nPersistent connections were already implemented by several vendors\nusing the notes I produced early this year, and which I reposted\nto the mailing list last month.  Please see\n\n     http://www.ics.uci.edu/pub/ietf/http/hypermail/\n\nfor more information.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "> Or a new HTTP method, as Roy Fielding suggested. But it seems to me that, \n> looking at how Netscape Navigator 2.0 actually uses said byte ranges, \n> there's a better solution. Namely, it uses them to get parts of PDF \n> files, or to resume file transfers if disrupted. In each case, if the \n> server does not understand the byte range, the web browser will respond \n> with another request, one without the byte ranges. So why not cause this \n> to be the default behavior, by adding a request header, something like \n> \n> Request-Range: bytes=500-999\n\nBecause it screws up caching by hierarchical proxies.\n\nAsking for a partial GET changes the meaning of the header fields returned\nin the response, which in turn requires that we either use a different\nmethod or a different status code.   Hmmmmm... what would be the effect\nof adding \"205 Partial Content\"?\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Sat, 11 Nov 1995, Roy T. Fielding wrote:\n\n> > Request-Range: bytes=500-999\n> \n> Because it screws up caching by hierarchical proxies.\n\nOh. Very good point. I hadn't thought of that.\n\n> Asking for a partial GET changes the meaning of the header fields returned\n> in the response, which in turn requires that we either use a different\n> method or a different status code.   Hmmmmm... what would be the effect\n> of adding \"205 Partial Content\"?\n\nWell, it would mean 205 couldn't be used for Reset Document, as was \ndiscussed a couple months ago on this list, and that most agreed was a \ngood idea (it would reset the document to how the browser originally \nreceived it, clearing forms and so forth, sort of like 204 No Content). \nExcept for that, a Partial Content response seems good. Satisfies my \nthought, which was that a non-byte range server should just return the \nfull document (with status 200).\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Alexei Kosut writes:\n> On Sat, 11 Nov 1995, Roy T. Fielding wrote:\n> > > Request-Range: bytes=500-999\n> > Because it screws up caching by hierarchical proxies.\n> Oh. Very good point. I hadn't thought of that.\n> > Asking for a partial GET changes the meaning of the header fields returned\n> > in the response, which in turn requires that we either use a different\n> > method or a different status code.   Hmmmmm... what would be the effect\n> > of adding \"205 Partial Content\"?\n> Well, it would mean 205 couldn't be used for Reset Document, as was \n> discussed a couple months ago on this list, and that most agreed was a \n> good idea (it would reset the document to how the browser originally \n> received it, clearing forms and so forth, sort of like 204 No Content). \n> Except for that, a Partial Content response seems good. Satisfies my \n> thought, which was that a non-byte range server should just return the \n> full document (with status 200).\nAgree. We need a new status code for the Partial content. If 205 is in use\nfor other reason, we can assign a different one.\nWho knows, what happened with Reset document?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">> Asking for a partial GET changes the meaning of the header fields returned\n>> in the response, which in turn requires that we either use a different\n>> method or a different status code.   Hmmmmm... what would be the effect\n>> of adding \"205 Partial Content\"?\n> \n> Well, it would mean 205 couldn't be used for Reset Document, as was \n> discussed a couple months ago on this list, and that most agreed was a \n> good idea (it would reset the document to how the browser originally \n> received it, clearing forms and so forth, sort of like 204 No Content). \n\nUh, right, I guess I forgot to write that down -- weird.  I do remember\nthe discussion, though, and will include it in the draft.\n\n> Except for that, a Partial Content response seems good. Satisfies my \n> thought, which was that a non-byte range server should just return the \n> full document (with status 200).\n\nI guess we'll make it \"206 Partial Content\" then.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Status of negotiation text in 1.1 draft",
            "content": "I recently re-read the text relating to content negotiation in\ndraft-ietf-http-v10-spec-01.txt, dated August 3, 1995.  This text was\ntaken out of subsequent HTTP 1.0 drafts, and should reappear in the\nupcoming HTTP 1.1 draft.\n\nIn my opinion, at least half of the v10-spec-01 content negotiation\ntext needs to be rewritten (both changes to the presentation and\nchanges to the semantics).  I am wondering how many rewriting has been\ndone already.\n\nMy question to the 1.1 draft authors is: will the new 1.1 content\nnegotiation text be very different from the old v10-spec-01 text?\nWill there be cleanups in the presentation?  Will things previously\nleft unspecified be specified?  Will there be changes to header\nsemantics?\n\nI'm trying to decide if I should postpone comments on content\nnegotiation mechanisms until after the new 1.1 draft has been\nreleased.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">   1) The Right Way\n> \n>      Treat all PDF files as a tree, containing a catalog of pages,\n>      chapters, or whatever other components can be identified within\n>      the PDF.  Assign each of these components a URL and make the server\n>      smart enough to extract the right component.\n\nIf you treat data as trees (and I'll agree that it's \"The Right Way\"),\nthen the TEI locators provide a very flexible and powerful addressing\nscheme based on occurence, typed occurence, and direct address. You\ncan browse the TEI guidelines at EBT's site.\n\nI also have a problem with byte ranges as they have absolutely no\nmeaning for many data types (HTML being one). If one wants to *really*\ndo this the right way, one should have a general/extensible addressing\nmechanism like that offered by HyTime (not that I am proposing it be\nadopted as is).\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">You and Larry are looking at this problem with blinders on.\n>There are many more uses for byterange URL's than simply\n>PDF files.  For instance Netscape 2.0 uses byteranges to\n>request parts of files that it didn't get the last time\n>you came to a page. \n\nAnd then you have to reparse the entire document again, and re-render\nit, possibly ignoring the errors caused by the file being\nincomplete. This is great for small pages, but if you try fetching\nsmall peices of a 5MB document, it makes no sense. \n\nByte ranges are a lazy replacement for a general naming mechanism.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Sun, 12 Nov 1995, Gavin Nicol wrote:\n\n> >You and Larry are looking at this problem with blinders on.\n> >There are many more uses for byterange URL's than simply\n> >PDF files.  For instance Netscape 2.0 uses byteranges to\n> >request parts of files that it didn't get the last time\n> >you came to a page. \n> \n> And then you have to reparse the entire document again, and re-render\n> it, possibly ignoring the errors caused by the file being\n> incomplete. This is great for small pages, but if you try fetching\n> small peices of a 5MB document, it makes no sense. \n> \n> Byte ranges are a lazy replacement for a general naming mechanism.\n\nYou still have those blinders on. The whole universe of documents is \nnot SGML/HTML/PDF/(favorite text markup language with naming mechanism). \nThe ability to restart an interrupted transfer is an item that naming \nmechanisms are insufficiently powerful to handle in the general case. \nByte ranges are not a 'lazy replacement' - they are the only general \nmechanism for restarting interrupted transfers of documents containing \narbitrary content.\n\nSuch as: you just fetched 100K of an inlined 160K GIF/JPG/PNG file but \ninterrupted the transfer by following a link before the transfer was \ncomplete. You then go back to that page. Now rather than spend a minute \nand a half getting the *same* information - you pick up where you left off.\n\nWhen the content of the interrupted transfer is arbitrary, \nyou have to use a recovery mechanism that *ignores* the content. I have \ncontinually puzzled why in this particular area (resumption of \ninterrupted transfers) the Internet implementation has lagged badly behind \nthe BBS community - which has had robust recovery mechanisms for interrupted \ntransfers for a number of years.\n\nThe question is not *if* they should be in the standard, but *how* they \nshould be implemented - in the URL or via Yet Another Header. In the \nURL seems to me to have the advantage of not breaking proxies.\n\nBenjamin Franz\n\n\"Heavily annoyed by beginining from 0 when D/Ling 2 meg reports that I \nalready D/led half of ten minutes ago.\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">On Sun, 12 Nov 1995, Gavin Nicol wrote:\n\n>> Byte ranges are a lazy replacement for a general naming mechanism.\n>\n>You still have those blinders on. The whole universe of documents is\n>not SGML/HTML/PDF/(favorite text markup language with naming mechanism).\n>The ability to restart an interrupted transfer is an item that naming\n>mechanisms are insufficiently powerful to handle in the general case.\n>Byte ranges are not a 'lazy replacement' - they are the only general\n>mechanism for restarting interrupted transfers of documents containing\n>arbitrary content.\n\nWe've had this discussion before but here we go again. Wanna see how\nscrewed up byte ranges can really be? OK, here's the prime example. Suppose\nyou're a HTTP client and you're half way through downloading a huge HTML\ndocument, the transfer terminates, and you decide to resume the transfer at\nthe \"appropriate\" byte offset. Fine concept on the surface, but an\nimplementation and performance nightmare for the following reason.\n\nOnly in the case of binary files does the byte stream transmitted by the\nserver stand a chance of being identical to what what it has stored locally\non disk. In the case of multi-fork or multi-part files, this won't be the\ncase. In the case of HTML files, for example, end of line termination ruins\nthe whole byte range theory. Many servers politely convert their machine\nspecific EOL sequence into a normalized version (LF or CR/LF) for the\ntransmission of text-only files. This means that there is a potential creep\nof at least 1 byte per EOL in the file. A client asking to resume a\ntransfer at byte 900,000 only has the data stream it was receiving to go\nby. This means that the server has to completely re-read the file,\ntranslating line ends and looking for the \"virtual\" 900,000th byte as\nrendered by the server. It's not simply a matter of jumping forward in the\nfile 900,000 bytes and resuming reading.\n\nI spent weeks of my time trying to negotiate a more reasonable solution\nwith the Adobe engineers regarding how to read portions of PDF files. If\nall the byte range proposal is really about is reading PDF (and it appears\nthat this is what is driving Netscape and Adobe), consider the following.\nFirst, the reason people seem to be trying to shove a change to the URL\nsyntax into place is because of the variety of syntax specifications for\nURLs (specifically, PATH arguments) on various servers. If everyone will\nremember, the entire portion of the URL after the host specification is\nSUPPOSED to be opaque to the client and the sole province of the server.\nClients have no business generating URLs like this. They are SUPPOSED to\nsimply return a URL originally provided by the server. That bit of\nphilosophy aside, here was the solution that was proposed to Adobe.\n\nRather than perturb the URL standard to meet the needs of 2 vendors,\nexisting mechanisms should be used. It is trivial to implement a CGI that\nwill return a range of bytes from a file specified as an argument to the\nCGI. This was Adobe's original approach. As a network saavy helper app,\nAcrobat Reader has the ability to generate and communicate URLs and\ntransmit them to servers. It can just as easily generate a URL that\nincludes a CGI name as it can generate a URL with byte range info. Since\nthe goal is to optimize the return of data for Acrobat, it makes more sense\nto have a CGI on the server that understands the intricacies of PDF files\nand can communicate more closely with the PDF reader than to warp the\nnon-conforming URL spec to Acrobat's needs.\n\nThe problem with this proposal on the surface is one of non-standard\ntechniques for passing arguments to CGIs as part of URLs. This was solved\nby proposing that at a minimum, the CGI reside at the same URL for all\nservers providing PDF content. This seems trivially possible for all\npopular WWW servers now. Variations in path argument syntax were resolved\nby making a call to the CGI with no parameters, with the reply from the\nserver (CGI) containing a format statement that specifies the syntax of\nfuture URLs with byte-range requests. This provides the client (Acrobat\nReader) with a template that could be used to insert byte range begin and\nend specifications into without having explicit knowledge of the actual URL\nsyntax (e.g., replace [START] with the starting byte, replace [END] with\nthe ending byte, and return the modified template to the server as a URL,\nnot changing the rest of the template text.) Simple solution, meets\nAcrobat's needs (and any other tool that decides to conform to the services\nof this CGI), and doesn't require lengthy standards harrangues.\n\nIn many cases, standardizing on a particular convention is preferable to\nshoving more baggage onto an existing standard that doesn't have very much\nto do with the problem. Especially when it is done without much forethought\nas to the implications. I'd like to encourage people to look at\nstandardizing server behavior rather than \"legislating\" it through\nmutations of the URL standard. If Adobe/Netscape chose to, they could\ndistribute a CGI for every server platform that would work with every\nversion of Acrobat reader, making both freely available to users on the\nnet. There would be no standards hassles, no mutation of the URL syntax,\nURL path information would remain private to servers, and all PDF\nusers/publishers would have timely and correctly implemented access to the\ntools necessary to efficiently serve PDF documents.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "MIME and binary transpor",
            "content": "Marc VanHeyningen <mvanheyn@cs.indiana.edu> (and others) write:\n> > > MIME is not basically a text-based protocol, any more than HTTP is.\n> > I'd be the first to agree that MIME is a fine example of protocol\n> > design. But it has different design objectives, the chief one of\n> > which is to stuff all sorts of data types into (possibly broken)\n> > implementations of RFC822 messages and SMTP transport. These are\n> > clearly lines of text. The MIME standard does not really say much\n> > about the \"binary\" content transfer encoding that we say we are\n> > using, and says nothing about using it in the body of a message.\n> It says that \"binary\" can contain any sequence of octets.  What more does it\n> need to say?\n\nIt does not say how to embed binary objects on the same TCP connection\nas the headers without applying an encoding. This is where HTTP seems\nto have made an innovation, using Content-Length: (The spec refers\nto Content-Length: as an optional header in MIME external bodies\nbut I haven't found this is in RFC 1521 or 1341.)\n\n>> SGML conformance clearly has particular benifits. What do you all\n>> think are the payoffs (or downsides) to sticking close to the\n>> MIME spec?\n>\n>It means that, as new things are defined for MIME (like how to represent\n>Unicode, or how to transport objects with PGP signatures, or how to handle\n>X.400 stuff) HTTP gets it \"for free.\"  There does seem to be some effort to\n>re-do some of that kind of work in HTTP and do it differently from MIME, in\n>some cases because of differences in design goals.\n\nWell, that's true, but a lot of that is localized in particular parts of MIME.\n\nIf we can both transport the same types of body parts and we can logically\nmap our multi-part structure onto theirs we ought to pick up most of the\nextensions.\n\nLarry Masinter <masinter@parc.xerox.com> writes:\n>You don't need content-length to search for the boundary. You can go\n>ahead and scan the binary data for <CR><LF>--boundary--<CR><LF>. Doing\n>so either using a simple scan or using a more elaborate boyer-moore\n>algorithm should be computationally not significantly more expensive\n>than merely counting bytes.\n\nYes, you can do this, but to make it work for arbitrary binary files, you\nhave to choose a boundary that is not in the file, either using a random\nstring and hoping, or scanning the whole file (which generally costs more\nthan just determining the size.)\n\nThe MIME boundary mechanism is a complex device that we don't need on an\n8-bit clean transport, and if implemented, could be sensitive to the EOL\ntreatment issues, which we are more sloppy about than MIME.\n\nI think we either need to move closer to MIME conformance or a bit farther\naway (and I'm leaning toward the latter).\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Sun, 12 Nov 1995, Chuck Shotton wrote:\n\n> >On Sun, 12 Nov 1995, Gavin Nicol wrote:\n> \n> >> Byte ranges are a lazy replacement for a general naming mechanism.\n> >\n> >You still have those blinders on. The whole universe of documents is\n> >not SGML/HTML/PDF/(favorite text markup language with naming mechanism).\n> >The ability to restart an interrupted transfer is an item that naming\n> >mechanisms are insufficiently powerful to handle in the general case.\n> >Byte ranges are not a 'lazy replacement' - they are the only general\n> >mechanism for restarting interrupted transfers of documents containing\n> >arbitrary content.\n> \n> We've had this discussion before but here we go again. Wanna see how\n> screwed up byte ranges can really be? OK, here's the prime example. Suppose\n> you're a HTTP client and you're half way through downloading a huge HTML\n> document, the transfer terminates, and you decide to resume the transfer at\n> the \"appropriate\" byte offset. Fine concept on the surface, but an\n> implementation and performance nightmare for the following reason.\n> \n> Only in the case of binary files does the byte stream transmitted by the\n> server stand a chance of being identical to what what it has stored locally\n> on disk. In the case of multi-fork or multi-part files, this won't be the\n> case. In the case of HTML files, for example, end of line termination ruins\n> the whole byte range theory. Many servers politely convert their machine\n> specific EOL sequence into a normalized version (LF or CR/LF) for the\n> transmission of text-only files.\n\nThis is a severely broken behavior by a server. And IRRELEVANT. Since \nbyte range requests are not recognized by todays servers - you obviously \ncannot break one by insisting the damn server keep its hands off the \ncontent if it support byte range requests. IOW: Find another red herring \nto drag.\n\n> This means that there is a potential creep\n> of at least 1 byte per EOL in the file. A client asking to resume a\n> transfer at byte 900,000 only has the data stream it was receiving to go\n> by. This means that the server has to completely re-read the file,\n> translating line ends and looking for the \"virtual\" 900,000th byte as\n> rendered by the server. It's not simply a matter of jumping forward in the\n> file 900,000 bytes and resuming reading.\n\nThat is the server's author's problem problem. Really. If the server \nauthor is stupid enough to try and *PARSE* a document when not \nexplicitly requested to so - they deserve all the headaches they bring on \nthemselves. They are clearly violating the intent of '8 bit clean' by \nsaying '8 bit clean, except if we think you messed up your end of lines \nwe are going to re-write them, so you can't reliably use \\x0a and \\x0d \nbecause we might change their number or order without warning you. Sorry \nabout that.'\n\n[deleted special purpose solution for PDF documents]\n\nPull the blinders back off. IGNORE PDF. There is a general problem with \nrestarting partially transmitted documents that that is just a special \ncase of. We need a method of saying *for any document what-so-ever*: \n\"Send me bytes 10000 through 20000\".\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "At 9:59 AM 11/12/95, Benjamin Franz wrote:\n>On Sun, 12 Nov 1995, Chuck Shotton wrote:\n>\n>> >On Sun, 12 Nov 1995, Gavin Nicol wrote:\n>> Only in the case of binary files does the byte stream transmitted by the\n>> server stand a chance of being identical to what what it has stored locally\n>> on disk. In the case of multi-fork or multi-part files, this won't be the\n>> case. In the case of HTML files, for example, end of line termination ruins\n>> the whole byte range theory. Many servers politely convert their machine\n>> specific EOL sequence into a normalized version (LF or CR/LF) for the\n>> transmission of text-only files.\n>\n>This is a severely broken behavior by a server. And IRRELEVANT. Since\n>byte range requests are not recognized by todays servers - you obviously\n>cannot break one by insisting the damn server keep its hands off the\n>content if it support byte range requests. IOW: Find another red herring\n>to drag.\n\nOh, servers that do ISO translations, Shift-JIS translations, or line end\ntranslations that conform to the MIME standard are \"severely broken?\"\nMethinks not. Perhaps you should study the available range of servers out\nthere before dismissing this particular issue as a \"red herring.\" Server\nside document translation is a HUGE issue, and many servers support it. The\npoint that the local storage representation of a document is not the same\nas the content transmitted to a client is a very real and valid one.\nOverlooking this issue or dismissing it shows a very narrow understanding\nof server implementation issues and the state of server development as it\nexists today. Please take a little more time to understand what is going on\nhere before you unilaterally decide that your position is correct.\n\n>> This means that there is a potential creep\n>> of at least 1 byte per EOL in the file. A client asking to resume a\n>> transfer at byte 900,000 only has the data stream it was receiving to go\n>> by. This means that the server has to completely re-read the file,\n>> translating line ends and looking for the \"virtual\" 900,000th byte as\n>> rendered by the server. It's not simply a matter of jumping forward in the\n>> file 900,000 bytes and resuming reading.\n>\n>That is the server's author's problem problem. Really. If the server\n>author is stupid enough to try and *PARSE* a document when not\n>explicitly requested to so - they deserve all the headaches they bring on\n>themselves. They are clearly violating the intent of '8 bit clean' by\n>saying '8 bit clean, except if we think you messed up your end of lines\n>we are going to re-write them, so you can't reliably use \\x0a and \\x0d\n>because we might change their number or order without warning you. Sorry\n>about that.'\n\nThere is NOTHING in any standard or convention that says a server cannot\nconvert content prior to sending it to a client. Especially when that\nclient is served out of something other than the cannonical Unix file\nsystem. One day everyone will realize that life on the Internet is NOT a\nUnix server, NOT a Unix file system, and certainly NOT a Unix client. Your\nassumption that the server just blindly tosses data with no interpretation\nto the client is naive at best and points out exactly why I am raising this\nas an issue.\n\nToo many people assume wrongly that the data served by a HTTP server is\nstored on the server's local storage medium in exactly the same\nbyte-for-byte format as it is transmitted to the client. This is an\nincredibly wrong assumption and it will become moreso as the complexity of\nserver-side objects increase and the amount of translated or generated\ncontent increases. The entire byte range concept is rooted (mired) in the\nconcept that WWW servers sit on top of (Unix) file systems. The simple fact\nis that they don't. And more and more the trend is away from file system\ndocument trees because of the semantically poor representation structure\nthey provide. Servers residing on databases will likely be the norm within\na year or so, and then what does a byte range URL get you?\n\n>[deleted special purpose solution for PDF documents]\n>\n>Pull the blinders back off. IGNORE PDF. There is a general problem with\n>restarting partially transmitted documents that that is just a special\n>case of. We need a method of saying *for any document what-so-ever*:\n>\"Send me bytes 10000 through 20000\".\n\nPerhaps you should take YOUR blinders off and realize that HTTP transmitted\ncontent is not a simply byte stream as far as the server is concerned. The\ndata objects are more complex than a simply binary file on disk, the\noperations and translations performed on them are more involved than\nblindly reading content off the disk and squirting it onto the net, and\nthat server implementors have a lot more to be concerned with than\nimplementing an inefficient, partial solution to the problem you describe.\n\nI would turn the problem around by asking you why clients only have partial\ndocuments to contend with? You seem to want to implement Zmodem file\ntransfers on top of HTTP, with the ability to resume an interrupted\ntransfer. The net is not a modem connection. TCP/IP is ostensibly a\nreliable delivery mechanism. You either get all the data or you don't. So,\nwhy are you getting partial files? Is your client broken? If so, that\ndoesn't seem to warrant a change to the URL standard to fix it. Is the\nserver broken? Ditto. Is your net connection flakey? Again, not a HTTP or\nURL problem.\n\nIf the issue is to deliver portions of an entire document because that\nportion is a recognizably distinct object that the browser can deal with, I\nsay let the server specify how those parts are to be requested and\ndelivered. This is a much more rational, useful reason for byte range\nextensions to exist. Trying to justify them with some specious argument\nabout resumed file transfers is perhaps the biggest red herring of all.\n\nAnd trying to anticipate every server-side representation scheme by\ngeneralizing everything to a byte stream takes us back to 1975 when Xmodem\nshowed up on the scene. LIFE IS NOT A BYTE STREAM. The Web already deals\nwith more complex objects than this. The semantics of these objects are\narguably outside the scope of the HTTP and URL standards and should be\nnegotiated between the applications that produce and consume these objects,\nnot the underlying transport protocol or the static addressing scheme for\nthese objects.\n\nLet's talk about matched sets of viewers/CGIs instead of warping every\nstandard under the sun and getting into endless academic standards\ndiscussions. The existing standards are already more than sufficient to\nsupport any type of client/server application you can think up.  But, you\nmust be willing to accept a paradigm shift away from trying to\nlegislate/mandate everything within the context of a narrow standard\ntowards a model where the application-level protocols that ride on top of\nthese standards cooperate to provide a more complicated exchange of info\nthan the underlying protocols can hope to represent.\n\nIt's fun to wave your hands and tell everyone what to do because some\noverengineered standard says so. It's a lot more fun to cooperate with\nothers on the net to build something bigger than a few paper documents by\nmaking APPLICATIONS cooperate. And it's a heck of a lot faster and easier\nto do, too. You, I, and everyone else on this list are not wise enough or\nexperienced enough to anticipate the possible future directions that\ninformation technologies will take even in the next year. Rather than\noverengineer a nice, simple standard like the HTTP or URL standard, why not\naccomplish the same thing within the context of the existing standard,\nsimply by build applications that cooperate within that context?\n\nWhat is your argument against a CGI/viewer solution? What is the argument\nFOR a generalized URL based solution? These are the questions that need to\nbe answered.\n\nI've presented a valid set of problems with the current byte range\nproposal. While you may have dismissed them, it doesn't diminish their\nmagnitude, nor do you provide a viable solution for those problems. Please\nprovide some constructive solutions instead of simply throwing rocks at a\nworkable alternative.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Effects of Persistent Connection",
            "content": "Well, it looks as though enough clients are around for this to get\ninteresting.  For example, my server is now showing around 1% more\nrequests than transactions; all of the excess must be requests on\npersistent connections.  I've also noticed a measureable increase in\nthroughput and an average response time drop.\n\nIt's interesting to note that the number of packets transmitted has\nincreased slightly (per request) from before; doubtless because\npersistent connections require that TCPIP_NODELAY be set.  Has anyone\ndone any simulation of this side-effect?\n\n(For live statistics on some of the above, try\nhttp://www2.hursley.ibm.com/!statistics)\n\nMike Cowlishaw\nIBM Fellow\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "According to Chuck Shotton:\n>  LIFE IS NOT A BYTE STREAM. \n\nThis is true.  But that is a very different statement than saying\n\"Nothing is a byte stream,\"  which seems to be what you are arguing.\nServers can offer byte ranges on those files that they choose.\nIt is perfectly fine for this to be the empty set.  Presumably no\nserver will offer byte ranges on CGI output.\n\nSaying that byte ranges won't work well on parsed documents is\nprobably true.  But that is not an argument that byte ranges should\nnever be allowed.  I suspect that image and sound files are never\nparsed, for example.\n\nParsed files really are a red herring in this discussion.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "I'm not going to say if I think byte ranges as presented here are A\nGood Thing or A Bad Thing (since there seem to be some valid arguments\non both sides), but I would like to suggest that it might be\nconsiderably safer if there was a standard way the client could\ninclude in the request the last-modified date, or a checksum of the\nwhole document, that it last received in connection with the original\nfragment of the document that it received.  Then servers too could\nhelp to keep things in sync, in particular by returning an error\nstatus if the request and some available version of the document don't\nmatch up.  Except for a request for an initial fragment of a\nnot-yet-seen document, clients should probably always include this\ninformation.\n\nRegarding certain other parts of this discussion I have not really\nread too thoroughly, the byte range should be defined in\nterms of the *server's* original byte numbering, before any\nend-of-line processing, double-byte character conversion, or whatever.\nAnd intermediates shouldn't do any such processing.  Or else this is doomed.\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Chuck Shotton writes:\n > \n > Perhaps you should take YOUR blinders off and realize that HTTP transmitted\n > content is not a simply byte stream as far as the server is concerned. The\n > data objects are more complex than a simply binary file on disk, the\n > operations and translations performed on them are more involved than\n > blindly reading content off the disk and squirting it onto the net, and\n > that server implementors have a lot more to be concerned with than\n > implementing an inefficient, partial solution to the problem you describe.\n > \n\nIf a server claims to support byte ranges, then it always has the\noption of implementing them trivially (but expensively) by simply\nediting its own output, either on the fly or out of a server-side\ncache.  If it has a cleverer algorithm for reverse-mapping from byte\nindexes to the position in its own internal representation, then so\nmuch the better, but that is in the realm of optimization.  There\nis always a way to do this, and it is usually going to be more\nefficient even to do the naive thing of generating the whole document as\nit will be transmitted, and throwing away all but a requested range of\nbytes, than it would be to generate the whole thing and then transmit\nthe whole thing.   Now, whether its a good idea is another question.\n\n...\n\n > And trying to anticipate every server-side representation scheme by\n > generalizing everything to a byte stream takes us back to 1975 when Xmodem\n > showed up on the scene.\n\n...\n\nDespite the fact that it's 1995, what goes over the net is still a\nbyte stream.  This isn't about internal representations, and servers\ndon't have to implement this.  (They better not have to...)\n\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Shel Kaphan writes:\n> \n> I'm not going to say if I think byte ranges as presented here are A\n> Good Thing or A Bad Thing (since there seem to be some valid arguments\n> on both sides), but I would like to suggest that it might be\n> considerably safer if there was a standard way the client could\n> include in the request the last-modified date, or a checksum of the\n> whole document, that it last received in connection with the original\n> fragment of the document that it received.  Then servers too could\n> help to keep things in sync, in particular by returning an error\n> status if the request and some available version of the document don't\n> match up.  Except for a request for an initial fragment of a\n> not-yet-seen document, clients should probably always include this\n> information.\nTo make this working, allways when a 'Partial content' response sent, we\nshould require sending checksums and digests for the whole url and the part\ntransmitted if the server supports any checksums/digests.\n> Regarding certain other parts of this discussion I have not really\n> read too thoroughly, the byte range should be defined in\n> terms of the *server's* original byte numbering, before any\n> end-of-line processing, double-byte character conversion, or whatever.\n> And intermediates shouldn't do any such processing.  Or else this is doomed.\nThe last 1.1 draft I read says something like:\nintermediaries and even clients should cache documents without any change in\ncontent, the only case when a client may and MUST apply conversions to local\nline-endings and other such stuff when it does a 'save as' operation.\nIf my memory is correct, then we have absolutely no problems with byte-ranges.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">> Byte ranges are a lazy replacement for a general naming mechanism.\n> \n>You still have those blinders on. The whole universe of documents is\n>not SGML/HTML/PDF/(favorite text markup language with naming mechanism).\n>The ability to restart an interrupted transfer is an item that naming\n>mechanisms are insufficiently powerful to handle in the general case.\n>Byte ranges are not a 'lazy replacement' - they are the only general\n>mechanism for restarting interrupted transfers of documents containing\n>arbitrary content.\n\nWell, let's agree to disagree on the \"only way\". I am not saying that\nbyte ranges should not be implemented, but that a general\nnaming/addressing scheme is needed, and byte range addressing should\nbe possible as part of it. \n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">Pull the blinders back off. IGNORE PDF. There is a general problem with\n>restarting partially transmitted documents that that is just a special\n>case of. We need a method of saying *for any document what-so-ever*:\n>\"Send me bytes 10000 through 20000\".\n\nWrong. We need a method of saying \"Send me pieces n to m\". \n\nBTW. Your argument that EOL conversion by servers is \"badly broken\" is\nalso wrong. I fully expect to see servers capable of coded character\nset and encoding translation appearing in the very near future. For\nsuch servers, byte ranges are simply BAD (Broken As Designed), because\na byte might no longer be the atomic unit of information, and because,\nas Chuck pointed out, the server would have to convert *everything*,\nand then select the piece requested.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Okay, so we have a suggested algorithm:\n\n1) Clients send the byte range as an HTTP header, something like \n   \"Request-Range: bytes=500-999\".\n\n2) Servers either\n\na) Respond with \"206 Partial Content\", with the same headers as what \n           the full object would get, the only difference being \n   \"Content-Length\".\n\nb) Respond with \"200 Ok\" if they don't support that header for that\n   object.  It's expected that some servers won't support the\n   header for a class of documents (CGI scripts, parsed documents, \n   other forms of dynamic documents) but that is an implementation \n           decision.  Note that this would be the default behavior for \n   servers which do not understand the new header.\n\n3) Proxies act like a client when pulling in data, and a server when \n   pushing out data.  Note that the default behavior of existing \n   proxies will be to ship around the full object, which is preferable\n   to failure, partial objects, or redundant data (i.e., having both\n   a full object and parts of the same object in the cache).\n\n\nCould we discuss the benefits/drawbacks of this algorithm? \n\n\nThe only thing which this document does not address is a syntax for encoding\nbyte ranges into URLs, with the expectation that its syntax would be\nstandardized and recognized by proxies/servers across the board for the\npurpose of optimization.  I would suggest that another mechanism be proposed\nfor query by structure for more complex data types.  The main purpose of \nthis algorithm is graceful recovery from aborted transfers. \n\nBrian\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Larry Masinter writes:\n>You don't need content-length to search for the boundary. You can go\n>ahead and scan the binary data for <CR><LF>--boundary--<CR><LF>.\n\nI was responding to Mitra's point that if you don't use some sort of\nencoding on the data, a file could conceivably contain\n<CR><LF>--boundary--<CR><LF> (or whatever else you wanted to use).  I can\nenvision a situation where you couldn't download httpd using http because\nthe file contained the boundary string.  Using content-length to determine\nthe end of a document would be 100% reliable, since the actual content\ncouldn't possibly conflict with the protocol information.\n\n--\nJim Seidman\nSenior Software Engineer\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Reopening RFC 1766  Language Tag",
            "content": "Hi,\nthis message is CCed to a lot of lists, and I'm asking people to spread\nit even further; copy it to any list you think should be invited to\nparticipate.\nBUT: PLEASE keep replies to me and the MAILEXT mailing list!\n\nThe language tags RFC, RFC 1766, is a Proposed Standard, and is now\nnine months old.\nIt has been very actively referenced in other Internet standards discussions,\nbut I don't know of any usage of it that can really be called\n\"implementations\".\n\nI also have had two change requests on the document:\n- To add a syntax for describing matching rules, so that, for instance,\n  one could ask for language \"en\" and have the tag \"en-cockney\" match\n- To add syntax for one or more larger language repertoires. In particular\n  the Summer Institute of Linguistics' Ethnologue 3-character codes has\n  been suggested.\n\nI would therefore ask community input on the following points:\n\n- Should the document be progressed to Draft Standard? This requires no\n  additional functionality, and at least some implementation experience\n- Should the document be revised, and recycled as Proposed?\n\nIf the second option is chosen, I would ask further:\n\n- Are there changes beyond the two I mentioned needed?\n- Do we want three-letter codes? If so, how?\n- Do we want matching rules? If so, how?\n\nPlease reply to me or to the mailext list (reply-to of this message is\nset to mailext).\nTo subscribe to mailext, I believe the correct invocation is a message\nto listproc@list.cren.net with \"subscribe mailext\" in the body.\n\n              Harald T. Alvestrand\n\n\n\n"
        },
        {
            "subject": "Re: Status of negotiation text in 1.1 draft",
            "content": "> I recently re-read the text relating to content negotiation in\n> draft-ietf-http-v10-spec-01.txt, dated August 3, 1995.  This text was\n> taken out of subsequent HTTP 1.0 drafts, and should reappear in the\n> upcoming HTTP 1.1 draft.\n\nAmongst other things, yes.\n\n> In my opinion, at least half of the v10-spec-01 content negotiation\n> text needs to be rewritten (both changes to the presentation and\n> changes to the semantics).  I am wondering how many rewriting has been\n> done already.\n\nIt's all in pieces right now (split amongst my notes) but I have not\nphysically changed that section of the FrameMaker document yet.\n\n> My question to the 1.1 draft authors is: will the new 1.1 content\n> negotiation text be very different from the old v10-spec-01 text?\n\nI am hoping to include reactive negotiation on an equal footing with the\ncurrent description of preemptive negotiation.\n\n> Will there be cleanups in the presentation?\n\nNone planned.\n\n> Will things previously left unspecified be specified? \n\nSome things, yes.  What did you have in mind?\n\n> Will there be changes to header semantics?\n\nYes, but perhaps not the ones you are thinking of.  It is difficult\nfor me to read minds, so I can't tell for sure.\n\n> I'm trying to decide if I should postpone comments on content\n> negotiation mechanisms until after the new 1.1 draft has been\n> released.\n\nAre they different than the comments already on the mailing list archive?\nIf so, then they may be useful, and I'd prefer to hear them before\ngenerating a new draft.  I can change my mind in a nanosecond, but it takes\nseveral days to generate a new draft.  I already have the old comments\nsaved and will review them again before finishing the section.\n\nI actually made a great deal of progress on it this weekend, so it should\nbe published next weekend (it must be published by Nov 22).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Sun, 12 Nov 1995, Gavin Nicol wrote:\n\n> >Pull the blinders back off. IGNORE PDF. There is a general problem with\n> >restarting partially transmitted documents that that is just a special\n> >case of. We need a method of saying *for any document what-so-ever*:\n> >\"Send me bytes 10000 through 20000\".\n> \n> Wrong. We need a method of saying \"Send me pieces n to m\". \n> \n> BTW. Your argument that EOL conversion by servers is \"badly broken\" is\n> also wrong. I fully expect to see servers capable of coded character\n> set and encoding translation appearing in the very near future. For\n> such servers, byte ranges are simply BAD (Broken As Designed), because\n> a byte might no longer be the atomic unit of information, and because,\n> as Chuck pointed out, the server would have to convert *everything*,\n> and then select the piece requested.\n\nOn consideration - the EOL conversion issue is just another red herring. \nIt simply doesn't matter to the question of byte ranges. Byte ranges \n*clearly* should apply to the transmitted byte stream - not the server side \nrepresentation of said byte stream. And one way or another the byte \nstream is *always* generated. And unless your conversion is \nnon-deterministic the byte-stream will always be the same from the same \nsource document for a given GET request. The byte is inherently the \natomic unit of information in HTTP. What connects a server to a client \n*is* a byte stream. End point representations of that byte stream \nshould be completely irrelevant to the issue of byte ranges.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "At 8:53 AM 11/13/95, Benjamin Franz wrote:\n>On Sun, 12 Nov 1995, Gavin Nicol wrote:\n\n>On consideration - the EOL conversion issue is just another red herring.\n\nLook, can we cut out the \"red herring\" red herrings? I want to see some\nconstructive discussion about why a CGI-based implementation of byte ranges\nis unacceptable. Nobody has presented a coherent argument against it. I\nwould like to see some rationale behind why the URL standard needs to be\nchanged to support an application-specific form of query for a specific\nsubset of data types. I'd like to see a well-reasoned discussion on your\npart about the valid concerns that server implementors have regarding the\nneed to re-render entire documents to serve you a byte range for low-value\nreasons like interrupted file transfers, etc. and why you think it is OK to\nignore these concerns.\n\n>It simply doesn't matter to the question of byte ranges. Byte ranges\n>*clearly* should apply to the transmitted byte stream - not the server side\n>representation of said byte stream. And one way or another the byte\n>stream is *always* generated. And unless your conversion is\n>non-deterministic the byte-stream will always be the same from the same\n>source document for a given GET request. The byte is inherently the\n>atomic unit of information in HTTP.\n\nThis is a trivial academic point. Please stop fixating on it. Of course all\ndata flows across a HTTP connection as a linear stream of bytes. That's the\nwhole legacy of a Von Neumann architecture. Now that we are past Data Comm\n101, let's talk about whether or not it makes sense to force byte stream\nmanipulations on applications that deal with pages, images, database\nrecords, complicated relational objects, real-time generated displays, etc.\n\n\nYou seem to want to trivialize the function of the Web and associated\napplications to the lowest common denominator of moving the contents of a\nfile from point A to point B, a byte at a time. This is an absurdly\nlow-level of abstraction. It's like talking about building a house by\naligning protein and sugar molecules so that they form 2x4s, lining up iron\natoms into nails, etc.\n\nThere is a LOT more to the problem space than simply bytes, and a\none-size-fits-all byte range proposal is unsuitable from an implementation\nperspective as well as a philosophical one. Clients have no business\nknowing about the internal representation of items stored at a given URL\nand allowing them to ask for specific chunks out of a file with something\nas coarse as a byte range violates the entire principle of a URL.\nSpecifically, a URL is supposed to be a client-opaque method for requesting\ninformation from a server. URLs are given to clients by servers or by\nhumans. The path information contained in a URL is supposed to be the\nprivate domain of the server. Allowing a client to generate or manipulate\nthis portion of a URL outside the bounds of the current hierarchy\nmanipulations allowed in the URL standard is WRONG.\n\nAs desirable as it may seem, there are serious integrity and consistency\nproblems that this opens up for the Web at large if this becomes an\nintegral part of the URL standard. On the otherhand, if it is a convention\nthat is adopted through the simple addition of a conforming CGI, the server\nretains the ability to tell the client how to access information it\ncontrols, the client gains the ability to access portions of documents, and\nthe overall model that governs how URLs are manipulated is maintained.\n\nThis is all jerk-off verbage and I apologize for contributing my own dreck\nto the torrent in this thread. However, there are some serious conceptual\nproblems and some difficult implementation problems raised by naively\nshoving byte ranges into the URL syntax, and I am concerned that these\nissues are being glossed over or ignored in a frenzy to get something\npasted into a standards document.\n\n> What connects a server to a client\n>*is* a byte stream. End point representations of that byte stream\n>should be completely irrelevant to the issue of byte ranges.\n\nWell, technically it's a bit stream. But even so, why are we continuing to\ntwaddle about in the low level bits and bytes of this problem space when we\ncould be building easier to use application level standards? If you are\nintent on forcing server implementors to pre-render all content so you can\nbe served a \"byte stream\" of particular offset and length, why not ask for\nsomething that makes a little more sense in the context of the WWW, like\npages, images, records, etc.? And what's more, why not allow servers to\ntell the client FIRST what representation is has chosen for partial\ndocument transfers (byte, page, record, etc.) and tell the client HOW to\nmake the request for these document portions? This last point is, I think,\nthe most important one to consider.\n\nAw heck, let's just replace URLs with SQL queries and be done with it.\nThat's where this whole discussion is heading anyway.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">Could we discuss the benefits/drawbacks of this algorithm?\n\nOh, Brian, there you go again actually creating solutions instead of\narguing about philosophy and who understands the problem better. :-)\n\nI'm no proxy maven, but this solution appears workable for both servers and\nend clients. Given that a client that wants to get more of an aborted\ndownload can just as easily create a \"Request-Range: bytes=\" as it can a\nURL as proposed, I believe that the optional HTTP header from the client is\nthe preferable solution than the overly-extended URL.\n\nBasically, users mess up URLs all the time, but I feel safe in letting\nclient software handle headers. If both schemes give the same result,\nheaders will be more reliable and less prone to user error.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "For reliability, shouldn't Request-Range requests be accompanied by a\n\"if-not-modified-since\", under the view that the range is likely\ninvalid if the resource has changed?\n\n\n\n"
        },
        {
            "subject": "Re: Effects of Persistent Connection",
            "content": "On Sun, 12 Nov 1995, Mike Cowlishaw wrote:\n> \n> It's interesting to note that the number of packets transmitted has\n> increased slightly (per request) from before; doubtless because\n> persistent connections require that TCPIP_NODELAY be set.  Has anyone\n> done any simulation of this side-effect?\n\n[So how long till you announce HotREXX? :-)]\n\nI don't really understand why TCPIP_NODELAY would need to be set for P-HTTP;\nalthough I haven't implemented the P-HTTP specifications, I have taken \nmeasurements for HTTP-NG operating in the closest available mode, using \nthe HTTP-TOS relay facility. Even though the implementation wasn't fully \ntuned to take best advantage, we found very little negative interaction\nwith the Nagle algorithm, and very few short packets. Indeed, even with \nthe simplest possible scheduling, we generally found that multiple \nrequests would be fitted into a single packet, *reducing* the average \npacket count per request. This effect was seen even without \nheader-caching enabled; header-caching gives better interactions.\n\n[details: The test was run using netscape browsers talking to the \nHP-proxy, which relayed HTTP/1.0 requests over HTTP-NG to other \nHP-proxies, which then connected to other HTTP/1.0 servers to perform the \nactual requests. Requests were processed in parallel - i.e. data from \nmultiple requests was interleaved over the HTTP-NG channel].\n\n\nThe pure-NG approach hasn't been tested yet, but hand coding gave some \nabsolutely ridiculous results :-) For example, cache-checking with  1.0\nIf-Modified for  a page with 20 images takes 6 packets per request \n(SYN->, <-SYN-ACK, REQ->, <-RESP+FIN, FIN+ACK->,<-ACK). 120 packets total.\n\nHTTP-NG can fit all the requests and responses in a single packet; if the \nconnection is closed, this takes 6 packets - a 20 fold saving. If the \nconnection is warm (as it would be if we're fetchig inline images), the \ntotal packet could would be 2, a 60 fold saving. \n\nIn the typical case, the packet savings are less- requests are co-aleced, \nbut responses typically need a whole packet or more. There are some extra\nsavings available; typically 1/2 MSS per response. These savings are in \naddition to the savings that come from avoiding the SYN/FIN/LAST-ACK \ntransactions documented in Jeff Mogul's SIGCOMM 95 paper.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Effects of Persistent Connection",
            "content": "TCP_NODELAY is useful if there are asynchronous packets being generated\n(as opposed to a request/response pair) over a long lived connection.\n\nIt was introduced for situations like mouse tracking, where\na server may generate packets on a connection without immediately\ngetting a response (which resets the delay in the OS).  The delayed\ntransmission of packets is for telnet like application behavior; if the\nnetwork is slow, the local system won't see a packet on the same connection, \nand then can coalesce many keystrokes into a packet.\n\nI suspect it may be useful for HTTP or HTTP-NG, but some measurements need\nto be made to confirm this.  Much more likely for NG, particularly when\nthe server starts sending content speculatively.\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, John Franks wrote:\n> Correct me if I am wrong, but I concluded from Spero's postings that\n> nothing currently proposed including MGET, hold-open, or even HTTP-NG\n> would improve (or even match?) the user's perceived performance\n> currently given by Netscape.  By this I mean the ellapsed time until\n> the user can start reading *all the text* and the ellapsed time until\n> the user can jump to a new link.\n\nActually I concluded that only -ng would give the same performance.  The\nsession layer would allow for parallel downloads over the same connection,\neffectively the same as the multiple-TCP-connections in NetScape.  In fact it\ncould provide a speed increase over NetScape in a couple ways - in addition\nto the benefit gained from not having to open all those TCP connections, the\nserver could also start sending inlined images before the client knows it\nwants them, giving the client the option of cleanly aborting them if it knows\nit already has them cached locally. \n\n> It seems to me that this \"user's perceived performance\" or UPP is going\n> to be the dominant consideration for commercial client developers.  If\n> they can't match Netscape they simply won't be viable.  Accordingly I\n> strongly suspect that in six months all the major vendors will be doing\n> what Netscape is doing today.  I don't see that they really have a choice.\n\nIf it's rendering-while-downloading, great, that needs to be done (the \nnew MacWeb does that, as do a few others).  If it's multiple connects, \nlet's hope not.\n\nIf a SESSION method is easy to implement for HTTP 1.1, let's do it and \nnot worry about MIME multipart messages (putting that effort instead \ntowards -NG).\n\nAs a content-provider that gets 300,000 hits a day, as soon as there is \nan implementation of SESSION or MGET for any of the full-feature servers \n(NCSA or WN) that is mirrored by a few browsers out there I'll install it.\n\n> And I don't see this as really bad.  I know the Netscape technique will\n> put a heavier load on network bandwidth, and maybe will stress some \n> servers.  As Spero pointed out there are many aborted connections as\n> users jump to a new document without waiting for the current one to\n> completely download.  But all that is the price we pay for quality service\n> (from the user's point of view).\n\nThe user isn't the one paying that price, though.  \n\n> I guess the bottom line is that there is not much point in changing\n> HTTP unless the resulting protocol can (1) at least match the Netscape\n> UPP, and (2) simultaneously significantly improve network efficiency.\n\nFor rendering-while-downloading browsers, MGET will improve performance \noverall - the UPP performance will only be improved if the person uses \nthe Netscape-only WIDTH and HEIGHT attributes to the IMG tag so the page \ncan be laid out before the latter image retreivals have even begun.  It's \nprecisely because NetScape can fetch the first few bytes of many inlined \nimages that it can calculate how to lay out the page without having to \nadjust layout later.  Since -NG will allow the layout-specific info in \nthe first few packets, it will improve both UPP and overall speed.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Chuck Shotton wrote:\n\n> At 8:53 AM 11/13/95, Benjamin Franz wrote:\n> >On Sun, 12 Nov 1995, Gavin Nicol wrote:\n> \n> >On consideration - the EOL conversion issue is just another red herring.\n> \n> Look, can we cut out the \"red herring\" red herrings? I want to see some\n> constructive discussion about why a CGI-based implementation of byte ranges\n> is unacceptable. Nobody has presented a coherent argument against it. I\n> would like to see some rationale behind why the URL standard needs to be\n> changed to support an application-specific form of query for a specific\n> subset of data types. I'd like to see a well-reasoned discussion on your\n> part about the valid concerns that server implementors have regarding the\n> need to re-render entire documents to serve you a byte range for low-value\n> reasons like interrupted file transfers, etc. and why you think it is OK to\n> ignore these concerns.\n\nA) The server has to re-render the document regardless.\n   What is being asked for here is that it only send *part* of that\n   rendered docuement. Something that should always take\n   less resources than retransmitting the *entire* document -\n   including the sections already received by the client.\n   Given that it only has to transmit part of the document,\n   a server *could* optimize its rendering in some cases for\n   partial documents as well. Clear win.\n\nB) CGI solutions are 'one off' solutions that fail to address the \n   *general* problem of restarting interrupted transfers.\n   Clear lose.\n\nC) CGI adds completely unnecessary overhead to server operation for what is\n   fundamentally a *simple* request of bytes X - Y of the rendered byte\n   stream. Clear lose.\n\nD) It *ISN'T* application specific. Byte ranges are a function of\n   the byte stream between the server and the client - not of the\n   servers or the clients local representation of that byte stream.\n   That byte stream is not dependant on the client/server *interpretation*\n   of it. It is an entity in its own right.\n\n> >It simply doesn't matter to the question of byte ranges. Byte ranges\n> >*clearly* should apply to the transmitted byte stream - not the server side\n> >representation of said byte stream. And one way or another the byte\n> >stream is *always* generated. And unless your conversion is\n> >non-deterministic the byte-stream will always be the same from the same\n> >source document for a given GET request. The byte is inherently the\n> >atomic unit of information in HTTP.\n> \n> This is a trivial academic point. Please stop fixating on it. \n\nOnly when you comprehend it. You are locked on the *meaning* of the byte \nstream - which is utterly irrelvant to a transport level issue. This is \nNOT about the meaning of the information - it is about *how* it is \ntransported. It is stupid to retransmit an entire document when you \nalready have part of it.\n\n> Of course all data flows across a HTTP connection as a linear stream of bytes.\n> That's the whole legacy of a Von Neumann architecture. Now that we are \n> past Data Comm 101, let's talk about whether or not it makes sense to \n> force byte stream manipulations on applications that deal with pages, \n> images, database records, complicated relational objects, real-time \n> generated displays, etc.\n\nAre you for real? You concede that what moves between the server and the \nclient is a byte stream and that a GET reguest should always return the \nsame byte stream for the same document - and then you dive right back \ninto what the *meaning* of the byte stream is? It does not matter. Bytes \n200-400 of the byte stream is bytes 200-400 of the byte stream whether \nthe byte stream is images, database search results or cookie recipies \nspoken aloud by Julia Childs.\n\nObviously a client should not make a request of a byte range on a \n*dynamic* document - that is implicit in the 'deterministic' condition. \nThe base request must give the same results every time or a byte range is \nmeaningless. But this is a non-issue! A dynamic document should be giving \na 'Pragma: no-cache' or Expires: <yesterday> header in the first place to \nindicate it is not safe to depend on the documents contents being \nunchanged between requests. At the last line - a server should ignore \nthe byte range request when clearly inappropriate (such as at the \ninvocation of a CGI script). This is no different that a server's handling \nof a Conditional GET. This is a *trivial* issue.\n\n> You seem to want to trivialize the function of the Web and associated\n> applications to the lowest common denominator of moving the contents of a\n> file from point A to point B, a byte at a time. This is an absurdly\n> low-level of abstraction. It's like talking about building a house by\n> aligning protein and sugar molecules so that they form 2x4s, lining up iron\n> atoms into nails, etc.\n\nBut it is the level we are *discussing* when talking about byte ranges.\n\n> There is a LOT more to the problem space than simply bytes, and a\n> one-size-fits-all byte range proposal is unsuitable from an implementation\n> perspective as well as a philosophical one. Clients have no business\n> knowing about the internal representation of items stored at a given URL\n> and allowing them to ask for specific chunks out of a file with something\n> as coarse as a byte range violates the entire principle of a URL.\n\nWhat are you talking about? One more time: BYTE RANGES \nSHOULD REFER TO POSITIONING WITHIN THE BYTE STREAM BETWEEN THE SERVER \nAND THE CLIENT NOT WITHIN THE SERVER'S OR CLIENT'S LOCAL REPRESENTATION OF \nTHAT STREAM. I have no idea how to make the statement any simpler. This \nis not about *what* the data is - it is about *how* the data is transported.\n\n> Specifically, a URL is supposed to be a client-opaque method for requesting\n> information from a server. URLs are given to clients by servers or by\n> humans. The path information contained in a URL is supposed to be the\n> private domain of the server. Allowing a client to generate or manipulate\n> this portion of a URL outside the bounds of the current hierarchy\n> manipulations allowed in the URL standard is WRONG.\n\nAnd is not what is being discussed. I have no idea why you are so locked \ninto the idea that byte range requests should to refer to anything other \nthan the byte stream.\n\n> > What connects a server to a client\n> >*is* a byte stream. End point representations of that byte stream\n> >should be completely irrelevant to the issue of byte ranges.\n> \n> Well, technically it's a bit stream. But even so, why are we continuing to\n> twaddle about in the low level bits and bytes of this problem space when we\n> could be building easier to use application level standards?\n\n> If you are\n> intent on forcing server implementors to pre-render all content so you can\n> be served a \"byte stream\" of particular offset and length, why not ask for\n> something that makes a little more sense in the context of the WWW, like\n> pages, images, records, etc.? And what's more, why not allow servers to\n> tell the client FIRST what representation is has chosen for partial\n> document transfers (byte, page, record, etc.) and tell the client HOW to\n> make the request for these document portions? This last point is, I think,\n> the most important one to consider.\n\nYou are off in the wild blue yonder again:\n\n   Questions of URL -> Dataspace representation are orthoganal to this\n   issue. You are considering a *different* problem than what is being\n   discussed here and one that the WG simply has no control over. That \n   issue is far beyond the scope of the HTTP group - which is *precisely* \n   concerned with the issues you dismiss as \"twaddling about in the low \n   level bits and bytes of this problem space.\" HTTP is a *transport* \n   mechanism - this proposal regards a *transport* issue: The problem of \n   restarting incomplete document transfers without having to re-transfer\n   the *entire* document again.\n\nAs presently done: The server must re-render the entire document and \nre-transmit the *entire* document. The proposed change would allow (not \nrequire) a server to partially render and/or partially transmit a document \nthat had been previously been sent but that did not finish transmission \nfor whatever reason. This clearly can not impose a higher work load on \na server (which *at worst* re-renders and re-sends the entire document - \nas it does right now) but has the potential in many cases to \n*decrease* the load on both the server and the network as information is \nnot redundantly sent to the client.\n\nThis has zero, nada, zilch to do with what that information represents.\nThose questions are simply irrelevant.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Effects of Persistent Connection",
            "content": "Simon beat me to it:\n   I don't really understand why TCPIP_NODELAY would need to be set for\n   P-HTTP; although I haven't implemented the P-HTTP specifications, I\n   have taken measurements for HTTP-NG operating in the closest\n   available mode, using the HTTP-TOS relay facility.\n\nWhile Simon then goes on to discuss why he doesn't think TCP_NODELAY\nis required for HTTP-NG, he does leave open the more immediate issue\nof whether it's required for persistent connections in HTTP V1.1.\n\nI would not be entirely surprised if TCP_NODELAY were useful in\nthe context of persistent connections, but it's Monday morning\nand I can't figure it out for myself.  So if someone could explain\nin some detail why it is required, that would be nice.\n\nThanks\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Chuck Shotton wrote:\n> I'd like to see a well-reasoned discussion on your\n> part about the valid concerns that server implementors have regarding the\n> need to re-render entire documents to serve you a byte range for low-value\n> reasons like interrupted file transfers, etc. and why you think it is OK to\n> ignore these concerns.\n\nPersonally, I don't consider anything that appears to give the user poor \nperformance and wastes network bandwidth to be a \"low-value reason\".  \nThere *are* servers out there in the Net today that do send back just a \nstream of bytes from a file to the user's browser _and_ there *are* \ninterrupted transfers over slow links.  The ability for a client do look \nat its local disc cache and say \"Oops, I only got the first 500K of this \n1M file last time, I'll try a byte-range for the last 500K\" is useful.  \nIf your server doesn't support that feature then fine, the user is no \nworse off than he is now.  If you do support that feature he gets his GIF \nfaster and uses less bandwidth.  Surely that's good news for everyone?\n\n> This is a trivial academic point. Please stop fixating on it. Of course all\n> data flows across a HTTP connection as a linear stream of bytes. That's the\n> whole legacy of a Von Neumann architecture. Now that we are past Data Comm\n> 101, let's talk about whether or not it makes sense to force byte stream\n> manipulations on applications that deal with pages, images, database\n> records, complicated relational objects, real-time generated displays, etc.\n\nHey, nobody's forcing you to use them in your server - if you don't \nsupport byte ranges (either at all or for a particular class of objects) \nthen you can send back a full response just as you do now.  No worries.  \nA byte-range aware browser (which is the only sort that could make the \nrequest) would be able to spot that you've sent the whole thing and act \naccordingly.\n\n> You seem to want to trivialize the function of the Web and associated\n> applications to the lowest common denominator of moving the contents of a\n> file from point A to point B, a byte at a time. This is an absurdly\n> low-level of abstraction. It's like talking about building a house by\n> aligning protein and sugar molecules so that they form 2x4s, lining up iron\n> atoms into nails, etc.\n\nThere's a lot of subatomic home construction going on in the Internet at \nthe moment then... :-)\n\n> Clients have no business\n> knowing about the internal representation of items stored at a given URL\n> and allowing them to ask for specific chunks out of a file with something\n> as coarse as a byte range violates the entire principle of a URL.\n\nAh, there's a difference between having a opaque string in the *URL* that \nis passed to the server that the client shouldn't interpret and having \nthe client be able to know about the internal representation of the \n*objects* that the URL points to.\n\n> Specifically, a URL is supposed to be a client-opaque method for requesting\n> information from a server. URLs are given to clients by servers or by\n> humans. The path information contained in a URL is supposed to be the\n> private domain of the server. Allowing a client to generate or manipulate\n> this portion of a URL outside the bounds of the current hierarchy\n> manipulations allowed in the URL standard is WRONG.\n\nIf the byte range is specified in the URL then one must assume that a \nhuman or server has given it to the client (and that it's opaque to the \nclient).  If the client is generating a Byte-Range: type header in its \nHTTP transaction becaused it received a partial object then it isn't \nlooking in the opaque part of the URL either - its looking in its local \nobject cache.\n\n> As desirable as it may seem, there are serious integrity and consistency\n> problems that this opens up for the Web at large if this becomes an\n> integral part of the URL standard. On the otherhand, if it is a convention\n> that is adopted through the simple addition of a conforming CGI, the server\n> retains the ability to tell the client how to access information it\n> controls, the client gains the ability to access portions of documents, and\n> the overall model that governs how URLs are manipulated is maintained.\n\nEr, by using a CGI script, isn't the client effectively mutilating the \nURL by changing its opaque part and doing exactly what you've been saying \nnot to do?\n\n> Well, technically it's a bit stream. But even so, why are we continuing to\n> twaddle about in the low level bits and bytes of this problem space when we\n> could be building easier to use application level standards?\n\nBecause if you don't consider whats floating along the wires you end up \nin OSI land.\n\n> If you are\n> intent on forcing server implementors to pre-render all content so you can\n> be served a \"byte stream\" of particular offset and length, why not ask for\n> something that makes a little more sense in the context of the WWW, like\n> pages, images, records, etc.?\n\nNobody is forcing the server implementors to do this (other than if all\ntheir users cry out for it to be supported but that's normal market forces\nat work on the Net).  If they all think its a crappy thing to do, then\nthey can just not support it and everything will carry on as it does at\nthe moment.  But if they *do* choose to support it, at least there'll be a\nstandard way of doing it.  Ditto for the client writers.\n\n> And what's more, why not allow servers to\n> tell the client FIRST what representation is has chosen for partial\n> document transfers (byte, page, record, etc.) and tell the client HOW to\n> make the request for these document portions? This last point is, I think,\n> the most important one to consider.\n\nDidn't we go through the byte/page/record options when byte-ranges were \nfirst proposed many millenium ago and have people raise similar \nideological objects to the others as to byte ranges?  Lets go with the \neasy one (byte ranges first) and then those of you how are more \nstructured than the rest of us can propose extensions for your favourite \nobjects.\n\n> Aw heck, let's just replace URLs with SQL queries and be done with it.\n> That's where this whole discussion is heading anyway.\n\nNow there's an idea... :-)\n\nJon\n\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nJon Knight, Researcher, Sysop and General Dogsbody, Department of Computer\nStudies, Loughborough University of Technology, Leics., ENGLAND.  LE11 3TU.\n* I've found I now dream in Perl.  More worryingly, I enjoy those dreams. *\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "If you want to encode byte ranges into URLs, you should also encode\nthe unique ID of the actual data stream.\n\nSo, if I start to GET  http://slow.host.dom/images/logo.gif HTTP/1.0\n\nand then cancel the transfer, and then later on want to get the rest\nof the data stream, I shouldn't use\n\n\"http://slow.host.dom/images/logo.gif;bytes=500-\" \n\nbut rather\n        \"http://slow.host.dom/cid:0102345@slow.host.dom#bytes=500-\" \n\nwhere \n0102345@slow.host.dom\n\nis the content-id URL for the content that was originally being\ndelivered with /images/logo.gif in the first place.\n\nThat is, the URL for partial content needs to identify the exact\ncontent from which the remaining bytes are to be extracted.\n\nNote that \"#\" is currently illegal in a URL as it is the separator\nbetween the URL and the client-side selector of the URI. In this case,\nyou could propose that when the server is willing to retrieve partial\ncontent that the server could perform what is logically a client-side\nextraction, it's allowed when applied to a uniquely identified\ncontent.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Gavin Nicol wrote:\n> \n> >You and Larry are looking at this problem with blinders on.\n> >There are many more uses for byterange URL's than simply\n> >PDF files.  For instance Netscape 2.0 uses byteranges to\n> >request parts of files that it didn't get the last time\n> >you came to a page.\n> \n> And then you have to reparse the entire document again, and re-render\n> it, possibly ignoring the errors caused by the file being\n> incomplete. This is great for small pages, but if you try fetching\n> small peices of a 5MB document, it makes no sense.\n\nWhat makes you think it makes no sense.  An If-modified-since\nrequest can guarantee that the object hasn't changed.  From\nthere it's just a simple matter of requesting the parts that\nare missing. \n\n> \n> Byte ranges are a lazy replacement for a general naming mechanism.\n\nWhat's your naming scheme for JPEG files?  How about AVI\nvideo streams?  Bytes are already a general purpose naming scheme,\nand they have been used for a number of years.  There is no\nneed to invent another one.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Chuck Shotton wrote:\n> >This is a severely broken behavior by a server. And IRRELEVANT. Since\n> >byte range requests are not recognized by todays servers - you obviously\n> >cannot break one by insisting the damn server keep its hands off the\n> >content if it support byte range requests. IOW: Find another red herring\n> >to drag.\n> \n> Oh, servers that do ISO translations, Shift-JIS translations, or line end\n> translations that conform to the MIME standard are \"severely broken?\"\n> Methinks not. Perhaps you should study the available range of servers out\n> there before dismissing this particular issue as a \"red herring.\" Server\n> side document translation is a HUGE issue, and many servers support it. The\n> point that the local storage representation of a document is not the same\n> as the content transmitted to a client is a very real and valid one.\n> Overlooking this issue or dismissing it shows a very narrow understanding\n> of server implementation issues and the state of server development as it\n> exists today. Please take a little more time to understand what is going on\n> here before you unilaterally decide that your position is correct.\n\nIf your server wishes to compute data on the fly then that's fine. \nAri's byterange proposal allows the server to specify explicitly \nwhich objects are byterange seekable and those that are not.\nSince your server is not smart enough to be able to compute a \nbyterange, you can simply keep byteranges off.\n\n> >[deleted special purpose solution for PDF documents]\n> >\n> >Pull the blinders back off. IGNORE PDF. There is a general problem with\n> >restarting partially transmitted documents that that is just a special\n> >case of. We need a method of saying *for any document what-so-ever*:\n> >\"Send me bytes 10000 through 20000\".\n> \n> I would turn the problem around by asking you why clients only have partial\n> documents to contend with? You seem to want to implement Zmodem file\n> transfers on top of HTTP, with the ability to resume an interrupted\n> transfer. The net is not a modem connection. TCP/IP is ostensibly a\n> reliable delivery mechanism. You either get all the data or you don't. So,\n> why are you getting partial files? Is your client broken? If so, that\n> doesn't seem to warrant a change to the URL standard to fix it. Is the\n> server broken? Ditto. Is your net connection flakey? Again, not a HTTP or\n> URL problem.\n> \n> If the issue is to deliver portions of an entire document because that\n> portion is a recognizably distinct object that the browser can deal with, I\n> say let the server specify how those parts are to be requested and\n> delivered. This is a much more rational, useful reason for byte range\n> extensions to exist. Trying to justify them with some specious argument\n> about resumed file transfers is perhaps the biggest red herring of all.\n\nYou seem to be content with ignoring the fact that users interrupt transfers\non nearly every heavily laden graphical page they visit.  This causes many\nmany valid partial transfers to occur.  If you want to sit around and ignore\nthe issues users are having trouble with that's fine, but don't try and\nblock a perfectly valid solution to a very serious problem.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Shel Kaphan wrote:\n> \n> I'm not going to say if I think byte ranges as presented here are A\n> Good Thing or A Bad Thing (since there seem to be some valid arguments\n> on both sides), but I would like to suggest that it might be\n> considerably safer if there was a standard way the client could\n> include in the request the last-modified date, or a checksum of the\n> whole document, that it last received in connection with the original\n> fragment of the document that it received.  Then servers too could\n> help to keep things in sync, in particular by returning an error\n> status if the request and some available version of the document don't\n> match up.  Except for a request for an initial fragment of a\n> not-yet-seen document, clients should probably always include this\n> information.\n\nNetscape always sends an If-modified-since request with a\ndate stamp and size checksum before attempting to use a\npartial cache file and request the missing end.\n\n> \n> Regarding certain other parts of this discussion I have not really\n> read too thoroughly, the byte range should be defined in\n> terms of the *server's* original byte numbering, before any\n> end-of-line processing, double-byte character conversion, or whatever.\n> And intermediates shouldn't do any such processing.  Or else this is doomed.\n\nThe byte range should be defined by what goes over the wire.\nIt should reflect any post processing that the server does but\nshould not reflect anything that the client changes.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Lou Montulli writes:\n...\n   An If-modified-since\n > request can guarantee that the object hasn't changed.  From\n > there it's just a simple matter of requesting the parts that\n > are missing. \n > \n\nThat makes this into a two-round-trip protocol to receive just\na part of the object (GET if-modified-since, 304, GET byte-range)\nunless you want to change the semantics of GET if-modified-since,\nwhich seems like barking up the wrong tree.  (What would it be?\nGET if-modified-since unless there's a byte-range in the URL, in which\ncase, instead of returning the 304, return the byte-range??? Yuck!)\nOrthogonality!  Orthogonality!!!\n\nIf we're actually going to *design* this part of the protocol, let's\ndesign into it that you can pass the last-modified date (at least) to\nthe server along with the request, in a header, so that you can safely get\nthe partial resource on one request, and not muck up the meaning of\nGET if-modified-since.  This also further suggests that the byte-range\nshould be in a header, or we should use some method other than GET,\nrather than putting it into the URL, which conflates two very\ndifferent things.\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Gavin Nicol wrote:\n> \n> >> Byte ranges are a lazy replacement for a general naming mechanism.\n> >\n> >You still have those blinders on. The whole universe of documents is\n> >not SGML/HTML/PDF/(favorite text markup language with naming mechanism).\n> >The ability to restart an interrupted transfer is an item that naming\n> >mechanisms are insufficiently powerful to handle in the general case.\n> >Byte ranges are not a 'lazy replacement' - they are the only general\n> >mechanism for restarting interrupted transfers of documents containing\n> >arbitrary content.\n> \n> Well, let's agree to disagree on the \"only way\". I am not saying that\n> byte ranges should not be implemented, but that a general\n> naming/addressing scheme is needed, and byte range addressing should\n> be possible as part of it.\n\nWell you should read ARI's draft again because it does specify\na general purpose mechanism of which \"bytes\" is simply the\nfirst to be used.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Some notes on content negotiatio",
            "content": "Roy T. Fielding:\n>[Koen Holtman:]\n>> I'm trying to decide if I should postpone comments on content\n>> negotiation mechanisms until after the new 1.1 draft has been\n>> released.\n>\n>Are they different than the comments already on the mailing list archive?\n\nI have not yet written out any comments, all I have now are notes, but\nyes, most comments would be different from the stuff already the\nmailing list archive.\n\n>If so, then they may be useful, and I'd prefer to hear them before\n>generating a new draft.\n\nOK, I'll write out some of the more `high level' notes.\n\nI tried to define some superstructure to the existing header\ndefinitions.  There are two reasons two have such a superstructure:\n  - it allows one to better state the definitions\n  - it allows one to see areas that are not yet defined.\n\nThis is what came out:\n\n`Negotiation port' model of content negotiation\n-----------------------------------------------\n\nFor various reasons, web software should distinguish between two kinds\nof URI types with respect to content negotiation:\n  1) normal URIs\n  2) negotiation ports\n\nA normal URI (e.g. http://blah.com/animals/cat.gif) is bound to one\nentity, which may be dynamic (the entity may change through time).\nThere is no content negotiation when accessing a normal URI.\n\nA negotiation port URI (e.g. http://blah.com/animals/cat) is not bound\nto an entity, but to a `variant set', which is a set of normal URIs,\ne.g\n\n { http://blah.com/animals/cat.gif , http://blah.com/animals/cat.jpg } \n\n, where each element in this set has certain properties known (at\nleast) to the server.  The variant set and variant properties can be\ndynamic, they can change through time.\n\nNormal and negotiation port URIs cannot be distinguished by their\nsyntax.  To determine the status of a certain URI, a request for that\nURI must be made: The URI is a negotiation port if and only if an URI:\nheader is present in the resulting response.\n\nWhen a user agent does a request to a negotiation port URI P, a\ncontent negotiation process is initiated, which results in either an\nerror message at some point, or the choosing of one normal URI U from\nthe variant set, after which the contents bound to U are displayed by\nthe user agent.\n\nThe content negotiation process can take multiple requests and\nresponses.\n\nIn the most simple case, with one request and one response, the\nrequest on the negotiation port P generates a 200 OK response, with\nthe response message containing:\n\n 1) in the response body:\n      the entity body bound to the chosen URI U\n 2) in the response headers:\n      2a) headers describing the variant set of the negotiation \n          port P (e.g. URI)\n      2b) the entity headers for the chosen URI U \n          (e.g. Content-type, Location, Last-modified)\n      2c) headers about the whole response\n          (e.g. Pragma, Date)\n\nOne of the unresolved issues is: for every response header, does it go\nunder 2a), 2b), or 2c)?\n\nOne particular sticky issue is the Expires header.  Does it apply to\nboth URI P and URI U?  Would two different expires headers (say\nExpires for 2b) and Port-Expires for 2a) ) be better?\n\n\nThere are a number of reasons for adopting the above model with its\nclear dichotomy between two types of URI:\n\n - Conceptual simplicity.  This is especially important because\n   the negotiation will be visible, on request, to the end user.  One\n   requirement for content negotiation is that the user can manually\n   request different variants of a content negotiated resource.  This\n   model\n    - has no recursive negotiation, which may be nice to have for\n      CS purists, but which also makes a clear presentation of \n      the available variants to the user a lot more difficult\n    - ensures that each variant has its own URI by which it can\n      be reached directly\n\n - I expect that having this model will make the semantics of strange\n   combinations of headers more easy to define\n\n - `emulation' of content negotiation by proxy caches can be\n   expensive.  Under this model, caches can always recognize normal\n   URIs (which make up at least 99% of all current URIs), and can\n   remember that they do not need to emulate negotiation for\n   them when serving them from the cache.\n\nSummary of caching related things that need to be defined:\n\n- how do we express the dynamism of\n   a) The variant set and variant properties bound to the negotiation\n      port P, and the property that P is a negotiation port\n   b) the entity bound to the result URI U\n\n- when may a conditional GET with date D on a negotiation port return\n  a `not modified' code?\n   i) if a) above was not modified since date D?\n   ii) if both a) and b) was not modified since date D?\n\nOK, that sums up the superstructure and the caching issues.  I have\nmore notes, some about the negotiation calculation itself and some\nabout a new option in reactive negotiation, but I don't have time to\nexpand them now.  I'll see if I can send them tomorrow.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Larry Masinter wrote:\n> \n> For reliability, shouldn't Request-Range requests be accompanied by a\n> \"if-not-modified-since\", under the view that the range is likely\n> invalid if the resource has changed?\n\nThere needs to be a method for using if-modified-since\nwith request-ranges but it shouldn't be required as it\nwon't always be necessary or desired.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "At 2:49 PM 11/13/95, Lou Montulli wrote:\n>If your server wishes to compute data on the fly then that's fine.\n>Ari's byterange proposal allows the server to specify explicitly\n>which objects are byterange seekable and those that are not.\n>Since your server is not smart enough to be able to compute a\n>byterange, you can simply keep byteranges off.\n\nIt's not a matter of being \"smart enough\". It's a matter of whether or not\nthere is a BETTER solution that doesn't impose additional computational\noverhead on the server side. For generated content, the content length\nisn't known so you are saying that in addition to the normal operations\nthat a server has to do, when a byte range request is made it also has to\ncount all the bytes that it intends to emit and only emit the ones that\nfall within a given range.\n\nThis whole proposal made great sense when it was motivated by serving\nunmodified binary PDF files out of a static file system. The server\nimplementation was trivial. But by trying to generalize this to a\nURL-oriented solution, we run across many content types where it would be\nnice for byte ranges to work, but the implementation of the currently\nproposed scheme is non-trivial, compute intensive, and of dubious value for\nanything other than static binary files.\n\n>> If the issue is to deliver portions of an entire document because that\n>> portion is a recognizably distinct object that the browser can deal with, I\n>> say let the server specify how those parts are to be requested and\n>> delivered. This is a much more rational, useful reason for byte range\n>> extensions to exist. Trying to justify them with some specious argument\n>> about resumed file transfers is perhaps the biggest red herring of all.\n>\n>You seem to be content with ignoring the fact that users interrupt transfers\n>on nearly every heavily laden graphical page they visit.  This causes many\n>many valid partial transfers to occur.  If you want to sit around and ignore\n>the issues users are having trouble with that's fine, but don't try and\n>block a perfectly valid solution to a very serious problem.\n\nFirst of all, I *don't* understand the \"seriousness\" of the problem. If a\nuser chooses to terminate a transfer, that was a conscious decision on\ntheir part.\nAs far as I've seen, the trouble with partial transfers is with certain Web\nclient applications that can't distinguish between a partial and complete\nfile transfer, resulting in corrupted client-side caches. I see no aspect\nof that problem that is solved by byte ranges.\n\nLet's get the goal straight once and for all. This whole byte range thing\noriginally came up last summer around the desire to serve portions of large\nPDF documents. Are you saying that the issue now is the ability to resume\ninterrupted file transfers? If so, then why are we dorking around with a\nURL oriented solution? This screams for a modification to the GET method.\n\nWhat you are saying is that you still want to retrieve a given URL (from\nthe client's perspective), but you'd like to GET only the portion you don't\nalready have. Rather like the if-modified-since header affects server\nresponses, a byte-range: header seems more appropriate than convoluting the\nURL itself. I guess it's really just a matter of semantics, but all the\nspecial punctuation, separators and other ca-ca that hang off the end URLs\ncould more easily be represented in many cases as header fields in a GET or\nPOST request.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: MIME and binary transpor",
            "content": "    Larry Masinter <masinter@parc.xerox.com> writes:\n    >You don't need content-length to search for the boundary. You can go\n    >ahead and scan the binary data for <CR><LF>--boundary--<CR><LF>. Doing\n    >so either using a simple scan or using a more elaborate boyer-moore\n    >algorithm should be computationally not significantly more expensive\n    >than merely counting bytes.\n\n    Yes, you can do this, but to make it work for arbitrary binary\n    files, you have to choose a boundary that is not in the file,\n    either using a random string and hoping, or scanning the whole file\n    (which generally costs more than just determining the size.)\n\nI used to think that boundary-scanning was expensive, but given my\nexperience that CPUs always get faster (I started my programming life\nusing a time-shared PDP-11/40, after all), I think I can see Larry's\npoint.\n\nA little experiment should help shed some light on this.  On a reasonably\nfast machine (a DEC3000/600), I did (after first arranging for /vmunix\nto be in the filesystem cache):\n    % time wc /vmunix \n 15102     97927   6351808 /vmunix\n    0.93u 0.14s 0:01 94% 0+1k 0+0io 15pf+0w\n    a% time ngrep masinter@parc.xerox.com /vmunix \n    0.27u 0.19s 0:00 74% 0+1k 4+3io 47pf+0w\n    % \nngrep uses a modified Boyer-Moore algorithm.  Larry is right;\nBoyer-Moore with a properly chosen search string is actually three\ntimes faster than simply counting bytes (although the byte count\nmight actually be available as a side-effect of some other operation,\n6351808 bytes in 0.27 seconds is 188 Mbits/sec., so it's not going\nto be the rate-limiting step.)\n\nI did a similar experiment on the slowest machine I could find,\nan old DECstation-3100 (about 11 SPECmark, I think).  This time,\n/vmunix didn't fit into the buffer cache.  Anyway, I got:\n    % time wc /vmunix\n8821   50904 2120392 /vmunix\n    3.8u 1.4s 0:09 56% 40+71k 263+2io 0pf+0w\n    % time ngrep masinter@parc.xerox.com /vmunix\n    0.3u 1.0s 0:02 67% 78+112k 16+0io 0pf+0w\n    %\nThat's \"only\" 56 Mbits/sec for Boyer-Moore.  But there is no way\nthat this little CPU is going to drive any network interface\nat that speed, anyway.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Chuck Shotton writes:\n...\n > \n > It's not a matter of being \"smart enough\". It's a matter of whether or not\n > there is a BETTER solution that doesn't impose additional computational\n > overhead on the server side. For generated content, the content length\n > isn't known so you are saying that in addition to the normal operations\n > that a server has to do, when a byte range request is made it also has to\n > count all the bytes that it intends to emit and only emit the ones that\n > fall within a given range.\n > \nThis is so cheap as to be, for all intents and purposes, free, and of\ncourse you only have to generate the document up to the end of the\nrequested range anyway.  The meaningful savings is of network\nbandwidth, which will way more than offset the cost of counting bytes.\n(by several orders of magnitude).\n\n > \n > First of all, I *don't* understand the \"seriousness\" of the problem. If a\n > user chooses to terminate a transfer, that was a conscious decision on\n > their part.\n\nSigh.  The \"problem\", and hence the savings, arise because often\npeople will go back to the interrupted transfer and re-request the\nsame things.  It then is much better to only have to ask for the part\nyou didn't get before.  It all boils down to being somewhat clever (and not\nterribly) in order to save some network bandwidth, and some time for users.\nThere's nothing more to it than that.\n\n...\n\n > Let's get the goal straight once and for all. This whole byte range thing\n > originally came up last summer around the desire to serve portions of large\n > PDF documents. Are you saying that the issue now is the ability to resume\n > interrupted file transfers?\n\nLet's *separate the issues* so you guys can stop flaming each other\nand get on with your lives.  There's room for more than one issue in\nthe world, wouldn't you agree?\n\nIt's perfectly reasonable to want to have some \"high-level\" mechanisms\nto request parts of structured objects.  Great.  But that is a\nseparate subject from this transport-level, efficiency-oriented\nquestion that we are pursuing here.  OK?\n\n If so, then why are we dorking around with a\n > URL oriented solution? This screams for a modification to the GET method.\n > \n > What you are saying is that you still want to retrieve a given URL (from\n > the client's perspective), but you'd like to GET only the portion you don't\n > already have. Rather like the if-modified-since header affects server\n > responses, a byte-range: header seems more appropriate than convoluting the\n > URL itself. I guess it's really just a matter of semantics, but all the\n > special punctuation, separators and other ca-ca that hang off the end URLs\n > could more easily be represented in many cases as header fields in a GET or\n > POST request.\n > \n...\n\nYes, on this, at least, I agree with you.\n\nShel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Shel Kaphan wrote:\n> \n> Lou Montulli writes:\n>         ...\n>    An If-modified-since\n>  > request can guarantee that the object hasn't changed.  From\n>  > there it's just a simple matter of requesting the parts that\n>  > are missing.\n>  >\n> \n> That makes this into a two-round-trip protocol to receive just\n> a part of the object (GET if-modified-since, 304, GET byte-range)\n> unless you want to change the semantics of GET if-modified-since,\n> which seems like barking up the wrong tree.  (What would it be?\n> GET if-modified-since unless there's a byte-range in the URL, in which\n> case, instead of returning the 304, return the byte-range??? Yuck!)\n> Orthogonality!  Orthogonality!!!\n\nI agree, a better solution would be nice.  I'm flexible, if\nwe can get most people to agree on a header solution that\nalso includes one trip if-modified-since support I think\nwe would all be better off. \n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Shel Kaphan wrote:\n> \n\n>  >\n>  > What you are saying is that you still want to retrieve a given URL (from\n>  > the client's perspective), but you'd like to GET only the portion you don't\n>  > already have. Rather like the if-modified-since header affects server\n>  > responses, a byte-range: header seems more appropriate than convoluting the\n>  > URL itself. I guess it's really just a matter of semantics, but all the\n>  > special punctuation, separators and other ca-ca that hang off the end URLs\n>  > could more easily be represented in many cases as header fields in a GET or\n>  > POST request.\n>  >\n>         ...\n> \n> Yes, on this, at least, I agree with you.\n> \n\nI would be happy with a good header proposal, but I wouldn't\nsupport a new method since byterange requests could apply\nto multiple methods.  Brian's header proposal looks like a\ngood start...\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Chuck Shotton wrote:\n> I want to see some\n> constructive discussion about why a CGI-based implementation of byte ranges\n> is unacceptable. \n\nTo use your Xmodem/Zmodem analogy, it's like suggesting that download\nparameters be tied into a name of an object to be transferred.  Using a CGI\nscript to deliver parts of files is definitely not \"wrong\".  However, writing\na PDF viewer which presumed any server serving PDF files to also have a\n\"/cgi-bin/pdfsplitter\" CGI script or something in its namespace is going\nbeyond the bounds of what a transport or communications protocol should\ndefine.  The point of trying to decide on a \"standard\" for this is not just\nso that server authors can agree to implement common support - proxy servers\nneed to know that a request for part of a thing can be immediately fulfilled\nif the whole thing is in the cache.  This is presumably why John and Ari\nbrought it to the table in the first place - so that Ari can support this\nkind of functionality in the netscape proxy.  A CGI-based implementation\nmakes this impossible, unless you suggest that the proxy recognize and\nintercept requests to /cgi-bin/pdfsplitter or whatever it is called. \n\nI believe what you are really asking for is a way for the client and \nserver to have something of a conversation about how to access sub-parts \nof content - i.e., frames of an movie, time ranges of audio, a page \nout of a PDF file.  Obviously byteranges are inadequate for these kind of \napplication-level queries.  Let's <em>separate</em> this from the \nbyterange proposal for the time being - there are definitely better \nanswers to this problem, Hytime is/was one solution, and I think it's \nill-advised to hold everything up until it's solved.  Focus on byte \nranges as being specifically for transmission recovery for incomplete \nfiles - the fact that some companies will be using them to access \nindividual pages out of a PDF file should just be considered an abuse.\n\n> I would like to see some rationale behind why the URL standard needs to be \n> changed to support an application-specific form of query for a specific\n> subset of data types. \n\nYou're right, it doesn't, I'm with you there.  \n\n> I'd like to see a well-reasoned discussion on your\n> part about the valid concerns that server implementors have regarding the\n> need to re-render entire documents to serve you a byte range for low-value\n> reasons like interrupted file transfers, etc. and why you think it is OK to\n> ignore these concerns.\n\nThe algorithm I posted yesterday allows for servers to return full \nobjects even when only a part is requested - the choice is up to them, \nsince that point may be hard to compute or dynamic or whatever.  So, I \nwould presume in your case the server would just always return full \nobjects.  No sweat - that's what the current netscape implementation \npresumes anyways.  Recognizing byte ranges is just a window for \noptimization.  If you don't want to do it, fine, you don't lose.  \n\n> You seem to want to trivialize the function of the Web and associated\n> applications to the lowest common denominator of moving the contents of a\n> file from point A to point B, a byte at a time. This is an absurdly\n> low-level of abstraction. It's like talking about building a house by\n> aligning protein and sugar molecules so that they form 2x4s, lining up iron\n> atoms into nails, etc.\n\nI hear Eric Drexler is making a comeback....\n\nBrian\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "At 3:55 PM 11/13/95, Lou Montulli wrote:\n>Shel Kaphan wrote:\n>> Yes, on this, at least, I agree with you.\n>>\n>\n>I would be happy with a good header proposal, but I wouldn't\n>support a new method since byterange requests could apply\n>to multiple methods.  Brian's header proposal looks like a\n>good start...\n\nI'm certainly not advocating a new request method by any stretch. A simple\nheader that describes the \"unit of measure\" (e.g., byte) and the offset and\nlength info is more than sufficient. For that matter, simply moving the\nproposed URL extensions into a header field would work. Anything in the\nheader has the added benefit of being non-trivial for browser users to\nmonkey with. I can think of lots of nasty scenarios where users misuse byte\nrange additions on URLs.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Lou Montulli writes:\n > \n > I would be happy with a good header proposal, but I wouldn't\n > support a new method since byterange requests could apply\n > to multiple methods.  Brian's header proposal looks like a\n > good start...\n > \n > :lou\n > -- \n > Lou Montulli                 http://www.netscape.com/people/montulli/\n >        Netscape Communications Corp.\n\nI'd also prefer a header proposal.\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Just a few points:\n\nIf the byte range is carried in the URL, and is generated by the client, \nthen the client needs to know whether or not the server suports byte \nranges before generating the request- otherwise the request will be \nrejected. If the information is stored in a header, then servers that \ndon't understand the header will just send the entire object. \n\nAlso, a general subdocument addressing facility must necessarily be more \ncomplicated than a simple range, as it must be able to cope with \ndiscontiguous areas. This sort of ability isn't need for restarting HTTP \nrequests, but is essential for caching proxies that are using lossy media \nspecific transfer protocols, where the initial fetch may be performed in \nreal-time, with losses, but which needs to be completed in non-real-time \nbefore being used to serve other requests (see some of the RODEO work for \nwhy lossy transports will become important).\n\nAll that's needed at the moment for this limited sub-problem is a quick \nhack; since sticking stuff on the end of URLS takes more work than \nsticking in an extra header in the request, and another in the response, \nI'd go for the later approach. Add nocache to make sure that proxies \nwhich don't understand byte-ranges don't cache it, and add another pragma \nto reenable caching for proxies which do understand byte-ranges.\n\n\nRequest-\nX-Byte-Range: [start]-[finish] \n\nResponse-\nPragma: no-cache, cache-if-you-understand-byte-range\nX-Byte-Range: [start]-[finish]\n\n\n\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1) ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "Re: Connection Oriented HTTP conflic",
            "content": "Roy T. Fielding wrote:\n> \n> > I found descrepency in Hopmann specification about connection\n> > oriented HTTP extension draft. I also confirmed the conflict\n> > with Mr Hopmann. Iam attaching part of the mail message\n> > with this note for other server developers.Please correct\n> > the information in your downloaded version of the draft\n> \n> We are not proceeding with the specification that Alex wrote.\n> Persistent connections were already implemented by several vendors\n> using the notes I produced early this year, and which I reposted\n> to the mailing list last month.  Please see\n> \n>      http://www.ics.uci.edu/pub/ietf/http/hypermail/\n> \n\nI couldn't find your notes, and I have a question.\n\nDid your proposal include multipart/mixed responses for\nkeep-alive cgi scripts?  That seems to be what everyone\nis implementing.  I'm not happy with using the \"multipart/mixed\"\nname.  I would prefer \"multipart/x-http-response\" or something\nlike it so that we don't have name space collision with email\nmessages.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Lou Montulli wrote:\n\n> Simon Spero wrote:\n> \n> We don't need a hack here.  Using a 205 response to signify a \n> partial document is being returned seems far better than \n> the \"no-cache\" nonsense.  The 205 response is also necessary\n> for the client to tell the difference between a full document \n> and a partial document response.\n\nLou- the 205 response is not part of http/1.0 as described, and this can \nlead to some caching confusion; if a 1.1 client is talking through a 1.0 \nproxy to a 1.1 client, and a partial fetch is done, it's possible to the \n1.0 proxy to keep a copy of the partial contents, yet not be aware that \nthe contents are bogus- the next client to do a fetch could end up with \npartial data, yet not be aware of it.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Simon Spero writes:\n...\n > \n > All that's needed at the moment for this limited sub-problem is a quick \n > hack; since sticking stuff on the end of URLS takes more work than \n > sticking in an extra header in the request, and another in the response, \n > I'd go for the later approach. Add nocache to make sure that proxies \n > which don't understand byte-ranges don't cache it, and add another pragma \n > to reenable caching for proxies which do understand byte-ranges.\n > \n\nSince not all proxies understand nocache, a different response\ncode for partial returns (eg 206) would be more backward compatible.\nIt's imperative that these ranges never get inadvertently cached as\nif they were the whole document.\n\n(I hope no proxies or clients ignore the return code and cache anyway).\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: MIME and binary transpor",
            "content": "I used to be on the other side of the fence (I thought scanning for\nboundaries was going to be too expensive), but Ned Freed convinced me.\nNow I'm a convert. Boundary scanning is also more *reliable* than\ncontent-length, because length calculations are unreliable.\n\n> I was responding to Mitra's point that if you don't use some sort of\n> encoding on the data, a file could conceivably contain\n> <CR><LF>--boundary--<CR><LF> (or whatever else you wanted to use).  I can\n> envision a situation where you couldn't download httpd using http because\n> the file contained the boundary string.  Using content-length to determine\n> the end of a document would be 100% reliable, since the actual content\n> couldn't possibly conflict with the protocol information.\n\nI only require 99.99999% reliability. (Besides, I'm in the market for\na Pentium.)\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Simon Spero wrote:\n> \n> On Mon, 13 Nov 1995, Lou Montulli wrote:\n> \n> > Simon Spero wrote:\n> >\n> > We don't need a hack here.  Using a 205 response to signify a\n> > partial document is being returned seems far better than\n> > the \"no-cache\" nonsense.  The 205 response is also necessary\n> > for the client to tell the difference between a full document\n> > and a partial document response.\n> \n> Lou- the 205 response is not part of http/1.0 as described, and this can\n> lead to some caching confusion; if a 1.1 client is talking through a 1.0\n> proxy to a 1.1 client, and a partial fetch is done, it's possible to the\n> 1.0 proxy to keep a copy of the partial contents, yet not be aware that\n> the contents are bogus- the next client to do a fetch could end up with\n> partial data, yet not be aware of it.\n\nI see, so we are back full circle to the reason why byte ranges were\noriginally proposed as part of the URL.  \n\nBut your concerns are not valid.  Neither of the two most used proxy\nservers (CERN and Netscape) will cache objects that are not returned\nwith a 200 status.  If there are any other proxy servers that do\ncache non-200 requests then they are clearly in violation of the\nHTTP spec.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Simon Spero wrote:\n> \n> Just a few points:\n> \n> If the byte range is carried in the URL, and is generated by the client,\n> then the client needs to know whether or not the server suports byte\n> ranges before generating the request- otherwise the request will be\n> rejected. If the information is stored in a header, then servers that\n> don't understand the header will just send the entire object.\n> \n...\n> I'd go for the later approach. Add nocache to make sure that proxies\n> which don't understand byte-ranges don't cache it, and add another pragma\n> to reenable caching for proxies which do understand byte-ranges.\n> \n> Request-\n> X-Byte-Range: [start]-[finish]\n> \n> Response-\n> Pragma: no-cache, cache-if-you-understand-byte-range\n> X-Byte-Range: [start]-[finish]\n> \n\nWe don't need a hack here.  Using a 205 response to signify a \npartial document is being returned seems far better than \nthe \"no-cache\" nonsense.  The 205 response is also necessary\nfor the client to tell the difference between a full document \nand a partial document response.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Lou Montulli wrote:\n\n> \n> But your concerns are not valid.  Neither of the two most used proxy\n> servers (CERN and Netscape) will cache objects that are not returned\n> with a 200 status.  If there are any other proxy servers that do\n> cache non-200 requests then they are clearly in violation of the\n> HTTP spec.\n> \nUnfortunately, this isn't true. The rule is: if you see a response code \nthat you don't understand, you must treat it as if the code was <N>00, \nwhere <N> is the first character of the response code. Thus, a conformant \nHTTP/1.0 system should treat a code '205' as code '200'. \n\nIt's in the last paragraph of section 6.1:\n---\nHTTP status codes are extensible, but the above codes are the only\n   ones generally recognized in current practice. HTTP applications are\n   not required to understand the meaning of all registered status codes,\n   though such understanding is obviously desirable. However,\n   applications must understand the class of any status code, as\n   indicated by the first digit, and treat any unknown response as being\n   equivalent to the x00 status code of that class. For example, if an\n---\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "> Unfortunately, this isn't true. The rule is: if you see a response code \n> that you don't understand, you must treat it as if the code was <N>00, \n> where <N> is the first character of the response code. Thus, a conformant \n> HTTP/1.0 system should treat a code '205' as code '200'. \n\nNevertheless, the current practice in proxies is to be safe and not\ncache anything that isn't 200.  So it will be backwords compatible\nwith existing implementations.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Byteranges with 206 partial conten",
            "content": "Based on the discussion during the past hours, it appears that a\nbetter way to do byte ranges is indeed via an additional header, and\nwith a 206 partial content response code.\n\nDoing it via a header will still make it work through existing\nproxies, and 206 status code will prevent them from caching it, unless\nthey understand what's going on.\n\nRoy, could you allocate 206 (or whatever) for Partial Content in HTTP,\nplease?\n\nAn additional feature is to say \"give me a range if the document\nhasn't changed, but if it has, send me the entire document\".  Similar\nto If-modified-since, but still quite different...  What would you\ncall such a header?\n\nI will re-vamp a new version of the byterange draft reflecting these\nchanges, and will submit it for review in http-wg shortly.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Simon Spero writes (in reply to Lou Montulli):\n> \n> Lou- the 205 response is not part of http/1.0 as described, and this can \n> lead to some caching confusion; if a 1.1 client is talking through a 1.0 \n> proxy to a 1.1 client, and a partial fetch is done, it's possible to the \n> 1.0 proxy to keep a copy of the partial contents, yet not be aware that \n> the contents are bogus- the next client to do a fetch could end up with \n> partial data, yet not be aware of it.\nAs fas as I know, 1.0 proxies think that only 200 responses are cachable.\nAre there bogus proxies caching responses with any or unknown response codes?\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Simon said:\n\n> Lou- the 205 response is not part of http/1.0 as described, and this can \n> lead to some caching confusion; if a 1.1 client is talking through a 1.0 \n> proxy to a 1.1 client, and a partial fetch is done, it's possible to the \n> 1.0 proxy to keep a copy of the partial contents, yet not be aware that \n> the contents are bogus- the next client to do a fetch could end up with \n> partial data, yet not be aware of it.\n\nThat is not true of correct caching implementations.  A cache cannot\nstore responses with a status code other than 200 unless the response\ninclude an Expires header (or, now, a Cache-Control header) which\nindicates that the response is cachable.\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Byteranges with 206 partial conten",
            "content": "Ari Luotonen writes:\n...\n > An additional feature is to say \"give me a range if the document\n > hasn't changed, but if it has, send me the entire document\".  Similar\n > to If-modified-since, but still quite different...  What would you\n > call such a header?\n > \n\nI'd use \"last-modified\", since what you're essentially saying is\n\"the document I received part of claimed to have been last-modified on <date>,\nand now I want another piece of this document, unless it has changed since\nthat date.\"   Since we want some specific behavior in both cases -- \nin one case the whole document is returned, and in the other case only\na range -- it isn't quite right to use \"if-not-modified-since.\n(i.e. something happens if it *is* modified, too).  Last-modified would\nreasonably encapsulate everything you need to say.\n\nBut I'd like to request that we break with tradition slightly, and as\nan option allow something other than (or in addition to) last-modified\nhere: a checksum field, the rule being that you return in the request\nthe checksum received with the original fragment of the document.\n\n --Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "On Mon, 13 Nov 1995, Ari Luotonen wrote:\n\n> An additional feature is to say \"give me a range if the document\n> hasn't changed, but if it has, send me the entire document\".  Similar\n> to If-modified-since, but still quite different...  What would you\n> call such a header?\n\nYou wouldn't want to use If-modified-since itself, since that would cause\nincorrect behavior with non-byte range compatible servers. Me, I would\nincorporate it into the range request header, since it's part of that\nfunction. Something like: \n\nRequest-Range: Sun, 06 Nov 1994 08:49:37 GMT; bytes=0-499\n\nThat would seem to make sense to me, since you're saying \"send me the \nfirst 500 bytes of the document if it hasn't been modified since this \ndate, otherwise send it all.\"Of course, you'd still want to be able to \nsay\n\nRequest-Range: bytes=0-499\n\nFor those cases where you really just want a certain byte range from a\ndocument, even if it's changed (or if you've never seen it before). I'd\nassume servers are smart enough to tell the difference. \n\n> I will re-vamp a new version of the byterange draft reflecting these\n> changes, and will submit it for review in http-wg shortly.\n\nGreat!\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Wait wait, I keep on saying \"if-not-modified-since\" and you keep on\nreplying \"if-modified-since\". \n\nNormally, GET with caching uses 'if-modified-since', i.e., give me the\ndata if it's newer than/different from what I already have.\n\nHowever, GET of a byte range need the converse. It's \"give me this\nbyte range of this object, UNLESS the object is newer than/different\nfrom what I already have\".\n\nThe sense is different. HTTP doesn't have a \"if-not-modified-since\",\nand would need it if you want byte ranges to actually work with data\nthat might change.\n\n  \n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Brian Behlendorf:\n> \n> For rendering-while-downloading browsers, MGET will improve performance \n> overall - the UPP performance will only be improved if the person uses \n> the Netscape-only WIDTH and HEIGHT attributes to the IMG tag so the page \n> can be laid out before the latter image retreivals have even begun.  It's \n> precisely because NetScape can fetch the first few bytes of many inlined \n> images that it can calculate how to lay out the page without having to \n> adjust layout later.  Since -NG will allow the layout-specific info in \n> the first few packets, it will improve both UPP and overall speed.\n> \n\nBrian makes a very good point that I wasn't fully aware of.  It bears\nrepeating. If documents containing in-line images all specify the\nWIDTH and HEIGHT of those images then an MGET can be nearly as good as\nmultiple connections in user preceived performance.\n\nOf course, there is the political problem of getting providers to include\nthis information in their documents.  There is no mention of these parameters\nin the IMG tag in the HTML 2.0 specification.  I don't know about their\nstatus for 3.0.\n\n> \n> As a content-provider that gets 300,000 hits a day, as soon as there is \n> an implementation of SESSION or MGET for any of the full-feature servers \n> (NCSA or WN) that is mirrored by a few browsers out there I'll install it.\n> \n> \n\nThere is a little bit of a chicken and egg problem here.  I would happily\nput MGET in WN if there were any browsers supporting it.  I would probably\neven put it in if there were some consensus on a standard for doing it,\nassuming it was kept simple.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Ari Luotonen wrote:\n\n> \n> > Unfortunately, this isn't true. The rule is: if you see a response code \n> > that you don't understand, you must treat it as if the code was <N>00, \n> > where <N> is the first character of the response code. Thus, a conformant \n> > HTTP/1.0 system should treat a code '205' as code '200'. \n> \n> Nevertheless, the current practice in proxies is to be safe and not\n> cache anything that isn't 200.  So it will be backwords compatible\n> with existing implementations.\n\nIn that case, the HTTP/1.0 spec is incorrect, and must be changed, either \nto put in a special case for 2XX series results; the general case is \nstill needed as it has been used in several places. \n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">Benjamin Franz wrote:\n>\n>What are you talking about? One more time: BYTE RANGES\n>SHOULD REFER TO POSITIONING WITHIN THE BYTE STREAM BETWEEN THE SERVER\n>AND THE CLIENT NOT WITHIN THE SERVER'S OR CLIENT'S LOCAL REPRESENTATION OF\n>THAT STREAM. I have no idea how to make the statement any simpler. This\n>is not about *what* the data is - it is about *how* the data is transported.\n\nIf byte ranges do not address an object on the server, or a part thereof\n(and part needs to be defined in that case), then they do not belong\nin the URL space.\n\nI have nothing against byte ranges, or partial transfers in general,\nbut limiting oneself to byte ranges, and adding the syntax to the URL\nspace, is not the way to do it.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Larry Masinter wrote:\n> \n> Wait wait, I keep on saying \"if-not-modified-since\" and you keep on\n> replying \"if-modified-since\".\n> \n> Normally, GET with caching uses 'if-modified-since', i.e., give me the\n> data if it's newer than/different from what I already have.\n> \n> However, GET of a byte range need the converse. It's \"give me this\n> byte range of this object, UNLESS the object is newer than/different\n> from what I already have\".\n> \n> The sense is different. HTTP doesn't have a \"if-not-modified-since\",\n> and would need it if you want byte ranges to actually work with data\n> that might change.\n\nI agree with you.  I think that \"if-not-modified-since\" would\nbe a great addition and would work well for byte-ranges.\nIt should be tied into the header some how though.\n\nMaybe we should make \"if-not-modified-since\" an optional\nattribute to the request-range header.\n\nTherefore a byte range request would look like:\n\nGET /byterange-capable-document HTTP/1.0\nRequest-Range: bytes=500-999; if-not-modified-since=\"DATE\"\n\nWe should also be able to send size checksums, so we\ncould add 'if-size-equal=\"LENGTH\"' as well.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": ">> incomplete. This is great for small pages, but if you try fetching\n>> small peices of a 5MB document, it makes no sense.\n> \n>What makes you think it makes no sense. \n\nGive a 5MB HTML document to Netscape Navigator. For certain media\ntypes, and HTML is one, you can request peices (not byte ranges), and\ndo not need to reparse the data. For example, it makes more sense to\nask for single elements from HTML, than a byte range.\n\n>> Byte ranges are a lazy replacement for a general naming mechanism.\n> \n>What's your naming scheme for JPEG files?  How about AVI\n>video streams?  Bytes are already a general purpose naming scheme,\n>and they have been used for a number of years.  There is no\n>need to invent another one.\n\nByte ranges are not sufficient for many data types, and indeed, are\npotentially harmful. A general naming mechanism would allow byte range\nnaming, as well as things like asking for elements m to n in an HTML\ndocument. \n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "> Based on the discussion during the past hours, it appears that a\n> better way to do byte ranges is indeed via an additional header, and\n> with a 206 partial content response code.\n> \n> Doing it via a header will still make it work through existing\n> proxies, and 206 status code will prevent them from caching it, unless\n> they understand what's going on.\n> \n> Roy, could you allocate 206 (or whatever) for Partial Content in HTTP,\n> please?\n\nI did this yesterday.  I have also defined\n\n    Request-Range:  for specifying the desired range on a request\n\nand\n\n    Range: for specifying the actual range returned on the 206 response\n\nBoth headers use the syntax for Range given in Ari and John's current draft:\n\n   The following HTTP response header is sent back to provide\n   verification and information about the range and total size of the\n   document:\n\n        Range: bytes X-Y/Z\n\n   where:\n\n      X      is the number of the first byte returned (the first byte is\n             byte number zero).\n\n      Y      is the number of the last byte returned (in case of the end of\n             the document this is one smaller than the size of the document\n             in bytes).\n\n      Z      is the total size of the document in bytes.\n\nBut, I haven't included the text yet, so now would be a good time to say\nwhether or not that syntax is acceptable.\n\n> An additional feature is to say \"give me a range if the document\n> hasn't changed, but if it has, send me the entire document\".  Similar\n> to If-modified-since, but still quite different...  What would you\n> call such a header?\n\nIf-Modified-Since   (for the current case)\nUnless              (for the generic case)\n\n> I will re-vamp a new version of the byterange draft reflecting these\n> changes, and will submit it for review in http-wg shortly.\n\nI am willing to proceed with this in the main HTTP/1.1 spec, since the\nchanges required are interwoven with the description of GET and caching.\n\nHowever, I am not willing to support multiple ranges within a single\nrequest at the current time, so no multipart/x-byteranges.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "According to Ari Luotonen:\n> \n> \n> Based on the discussion during the past hours, it appears that a\n> better way to do byte ranges is indeed via an additional header, and\n> with a 206 partial content response code.\n> \n> Doing it via a header will still make it work through existing\n> proxies, and 206 status code will prevent them from caching it, unless\n> they understand what's going on.\n> \n\nI would agree with this.\n\n> An additional feature is to say \"give me a range if the document\n> hasn't changed, but if it has, send me the entire document\".  Similar\n> to If-modified-since, but still quite different...  What would you\n> call such a header?\n> \n\n\"Unless-modified-since\"  (send the byte range)\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "At 4:51 PM 11/13/95, Ari Luotonen wrote:\n>Based on the discussion during the past hours,\n\nDiscussion? Usually people have to listen to each other and value each\nother's opinions for a discussion. Or have I just brought up another red\nherring? :-)  Glad to see that some movement was made.\n\n>An additional feature is to say \"give me a range if the document\n>hasn't changed, but if it has, send me the entire document\".  Similar\n>to If-modified-since, but still quite different...  What would you\n>call such a header?\n\nTwo things here:\n\n- I question the value of a \"Byterange-even-if-changed\" header or action.\nIt seems too dangerous and unlikely to be used by anyone if there is a\nsafer alternative (which there will certainly be).\n\n- It seems to me that we need two headers to work: \"Byterange\" coupled with\n\"Unless-modified-since\". There may be a more elegant solution using\n\"If-modified-since\", but I'd hate to make that header try to act like both\n\"if\" and \"unless\" just to prevent creating one more header. Let's remember\nthat these headers will only be seen by <.1% of users, and clarity of\nwording will help server, client, and proxy implementors.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "    Wait wait, I keep on saying \"if-not-modified-since\" and you keep on\n    replying \"if-modified-since\". \n    \nI wish that the mailer at cuckoo.hpl.hp.com would simply reject\nall future mail containing the string \"if-modified-since\".\n\nI've argued before that what we really want in HTTP is an\nopaque cache-validator string that is provided by the server\nwith an object, should be kept with any cached copy of an\nobject, and can be presented to the server to say \"is the\nobject associated with this validator still valid?\"\n\nIf the server implementor is foolish enough to use a date\nfor this string, that's not the problem of the HTTP working\ngroup.  \n\nSo let me rewrite Larry's message, which he sent as this:\n\n    Normally, GET with caching uses 'if-modified-since', i.e., give me\n    the data if it's newer than/different from what I already have.\n\n    However, GET of a byte range need the converse. It's \"give me this\n    byte range of this object, UNLESS the object is newer\n    than/different from what I already have\".\n\n    The sense is different. HTTP doesn't have a\n    \"if-not-modified-since\", and would need it if you want byte ranges\n    to actually work with data that might change.\n\nusing my terminology:\n\n    Normally, GET with caching uses 'Cache-validator: XXXX', i.e., give me\n    the data if it's different from what I already have.\n\n    However, GET of a byte range need the converse. It's \"give me this\n    byte range of this object, UNLESS the object is diffferent from\n    what I already have\".\n\nWhich implies that the byte-range header should be named\n\nRequest-byte-range-if-cache-valid-else-transmit-whole-file: 1-3\n\nwhen presented along with a Cache-validator: header.  Which is to\nsay, the meaning of the Cache-validator field itself DOES NOT HAVE\nTO CHANGE.  The only thing we need to do is to be clear about\nwhat \"Request-byte-range\" actually means (even if we don't actually\ncall it by a 60-character name!).\n\nAny use of terms such as \"newer\" or \"since\" are just going to confuse\nthings.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "On Mon, 13 Nov 1995, Roy T. Fielding wrote:\n\n> > An additional feature is to say \"give me a range if the document\n> > hasn't changed, but if it has, send me the entire document\".  Similar\n> > to If-modified-since, but still quite different...  What would you\n> > call such a header?\n> \n> If-Modified-Since   (for the current case)\n> Unless              (for the generic case)\n\nUnless could work, if it's implemented in the same spec as byte ranges. \nIf-Modified-Since will not work, though, because it's already in use for \nsomething different. Namely, one of the points of using a header instead \nof a URL extension or new HTTP method is so that non-byte range serverss \nwill ignore it and send a 200 response and the entire document. Using \nIf-Modified-Since defeats this, since non-byte range servers will return \na 304.\n\nI think either as I said in a previous message, or as Lou Montulli said, \nputting it in with the Request-Range header, would be best, because the \nbehavior with respect to byte ranges really is quite different than the \nbehavior with respect to entire documnets, in that if it has changed, it \nshould send a 200 and the entire document, as opposed to the normal case, \nwhere a 304 is sent with no content.\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: Connection Oriented HTTP conflic",
            "content": "Roy T. Fielding wrote:\n> \n> > Did your proposal include multipart/mixed responses for\n> > keep-alive cgi scripts?  That seems to be what everyone\n> > is implementing.\n> \n> No, and I'm not aware of *any* implementation that does this.\n> Is somebody holding out on me?\n> \n> Right now, CGI scripts just ignore keep-alive and close the connection.\n> For the \"official\" HTTP/1.1, they can use the chunked transfer encoding\n> instead of a multipart, if desired.\n\nNetscape navigator and the netscape server both use \n\"multipart/mixed\" as per Alex's proposal.  I'm not\naware of any server implementer's even wanting to\nsupport chunked transfer encoding.  I for one\nwill strongly fight against adding yet another \nencoding form.\n\n> \n> > I'm not happy with using the \"multipart/mixed\"\n> > name.  I would prefer \"multipart/x-http-response\" or something\n> > like it so that we don't have name space collision with email\n> > messages.\n> \n> That isn't a namespace collision -- multiparts are just multiparts:\n> a mechanism for sending multiple bodies in a single message.  The UA\n> should treat them identically no matter where they come from.\n> \n\nThere is a namespace collision.  the user representation of\na multipart/mixed message is well specified and does not\nconform to the usage in Alex's proposal.  \"Multipart\"\nencoding is the right thing for HTTP but it should use\na different sub name.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "On Fri, 16 Dec 1994, Jeffrey Mogul wrote:\n>     The fastest way to get all the text is to send it first but it\n>     can't be displaye until layout information like the size and shape\n>     of all images is known.  This is the point of the Netscape multiple\n>     connections.  They get the first few bits of each image which\n>     contain the size information.\n>     \n> This is the one thing that Netscape's approach does help with.  I\n> admit that it's nice to see the text laid out before the images arrive.\n> But there is more than one way to solve this problem, and I think\n> Netscape's solution causes more network-related trouble than necessary.\n>\n> For example, as long as we are making minor changes to HTTP anyway,\n> how about definining a new method called \"GET_BOUNDING_BOXES\" (or\n> pick a shorter name).  This would take a list of URLs, and the server\n> would return the appropriate image attributes (height, width, maybe\n> a few other parameters) for each image.  If the server doesn't want\n> to do this, or if one of the URLs is in an unrecognized format, no\n> problem; the client just has to wait until the real image is retrieved.\n\nNetScape themselves provide a solution in the form of WIDTH and HEIGHT\nattributes to the IMG tag - if used everywhere the document can be laid out\ncompletely before any images retreived.  I'd be against having authoring\ntools or the author themselves put this information in, but if we go down the\npath of having servers \"do something\" to the HTML they serve before sending\nit to the client (a la server-side includes, something we use a lot here)\nthen having servers add those attributes to those tags isn't such a bad idea\n(servers can cache these translations anyways).  \n\nI guess the big question is - do we solve this problem in HTML or HTTP?\nOr both?\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "At 5:32 PM 11/13/95, Lou Montulli wrote:\n>Larry Masinter wrote:\n>Maybe we should make \"if-not-modified-since\" an optional\n>attribute to the request-range header.\n>\n>Therefore a byte range request would look like:\n>\n>GET /byterange-capable-document HTTP/1.0\n>Request-Range: bytes=500-999; if-not-modified-since=\"DATE\"\n\nI assume the semantics of this would mean that if the if-not-modified-since\ninformation is present, it must be honored by the server in that it must be\nused to determine if the identical byte range can be served. If this is the\ncase, what is acceptable behavior for a server that receives this header\nbut cannot determine the date associated with the requested byte range?\n(e.g., the content was generated on the fly and has no modification date,\nper se, but may be reproducable)\n\n>We should also be able to send size checksums, so we\n>could add 'if-size-equal=\"LENGTH\"' as well.\n\nHow would this work? I assume this is for documents where the entire\ndocument has already been received, or at least a size checksum has already\nbeen transmitted to the client as part of a previous response. I assume it\nhas no bearing on the partial file transfer problem? What is the added\nvalue of this option?\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Chuck Shotton wrote:\n> \n> At 5:32 PM 11/13/95, Lou Montulli wrote:\n> >Larry Masinter wrote:\n> >Maybe we should make \"if-not-modified-since\" an optional\n> >attribute to the request-range header.\n> >\n> >Therefore a byte range request would look like:\n> >\n> >GET /byterange-capable-document HTTP/1.0\n> >Request-Range: bytes=500-999; if-not-modified-since=\"DATE\"\n> \n> I assume the semantics of this would mean that if the if-not-modified-since\n> information is present, it must be honored by the server in that it must be\n> used to determine if the identical byte range can be served. If this is the\n> case, what is acceptable behavior for a server that receives this header\n> but cannot determine the date associated with the requested byte range?\n> (e.g., the content was generated on the fly and has no modification date,\n> per se, but may be reproducable)\n\nIf the results are reproducable than the server can produce any date\nthat it wishes to use as a time checksum.  I have written such scripts\nin the past and I usually use the most recent modification date of\nall the dependancies for the data.\n\n> \n> >We should also be able to send size checksums, so we\n> >could add 'if-size-equal=\"LENGTH\"' as well.\n> \n> How would this work? I assume this is for documents where the entire\n> document has already been received, or at least a size checksum has already\n> been transmitted to the client as part of a previous response. I assume it\n> has no bearing on the partial file transfer problem? What is the added\n> value of this option?\n\nIn most cases the server returns a content-length for the object.  The\ncontent length would be returned in the if-size-equal header.\n\nThe added value of the option is to give an additional checksum to guarentee\naccuracy.  A generalized checksum method would also be desirable.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Gavin Nicol wrote:\n> \n> >> incomplete. This is great for small pages, but if you try fetching\n> >> small peices of a 5MB document, it makes no sense.\n> >\n> >What makes you think it makes no sense.\n> \n> Give a 5MB HTML document to Netscape Navigator. For certain media\n> types, and HTML is one, you can request peices (not byte ranges), and\n> do not need to reparse the data. For example, it makes more sense to\n> ask for single elements from HTML, than a byte range.\n\nI'm not disagreeing with your application, but I am saying\nthat byteranges can make sense for any object, as demonstrated\nby partial caching.\n\n> \n> >> Byte ranges are a lazy replacement for a general naming mechanism.\n> >\n> >What's your naming scheme for JPEG files?  How about AVI\n> >video streams?  Bytes are already a general purpose naming scheme,\n> >and they have been used for a number of years.  There is no\n> >need to invent another one.\n> \n> Byte ranges are not sufficient for many data types, and indeed, are\n> potentially harmful. A general naming mechanism would allow byte range\n> naming, as well as things like asking for elements m to n in an HTML\n> document.\n\nAri's proposal allows for generalizable requests.  Bytes is simply\none of the range request methods.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Jeffrey Mogul writes:\n...\n > \n > Which implies that the byte-range header should be named\n > \n > Request-byte-range-if-cache-valid-else-transmit-whole-file: 1-3\n > \n > when presented along with a Cache-validator: header.\n\nI am with you in spirit, as I *much* prefer this kind of validation\nto date comparisons,  but there's something wrong with these names\nfor this purpose.  (Something besides the length, that is).\nThere is no \"cache\" to be valid, so perhaps what you really mean is\n\nRequest-byte-range-if-cache-validator-would-still-apply-to-the-requested-document-if-it-were-cached:\n\nBut seriously, I doubt http-wg is going to get consensus on this,\ngiven the lack of discussion last time these issues were brought up,\nwhich is why I wanted just the *option* of a \"checksum\" somewhere in\nthis mechanism.  But it's true that calling it a checksum means it\nshould be ... a checksum ... so calling it something more general like\n\"validator\" would be better.\n\nI like the parameterized version of this:\n\nRequest-range:bytes=X-Y; validator=<xxxxx>; other-parameter=<yyyy>\n\nI propose, then, that we choose a set of parameters that can be in this\nheader, and\n\n(1) that each parameter name be the same as the name of some header\nactually received from the server when the initial document fragment was\nfetched (without byterange request), and\n\n(2) that these parameters be allowed to include (without limitation)\nlast-modified, content-MD5, and content-length, the intent being that\nany parameter in this header is intended to help the server assess the\nvalidity of the request w.r.t. the available document, and\n\n(optional) (3) we extend the set of headers to include the\ngeneralization \"validator\" which might be used to contain (e.g.) a\ndate, filesize, hash, checksum, or random number chosen by the server\nas pertaining to a unique version of the document, and allow\n\"validator=<xxxxx>\" to appear as a header in server responses as well\nas in this header.\n\nIf we can agree to (3) then maybe later we can also think about using\nthis in other appropriate places.\n\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "On Mon, 13 Nov 1995, Gavin Nicol wrote:\n\n> >Benjamin Franz wrote:\n> >\n> >What are you talking about? One more time: BYTE RANGES\n> >SHOULD REFER TO POSITIONING WITHIN THE BYTE STREAM BETWEEN THE SERVER\n> >AND THE CLIENT NOT WITHIN THE SERVER'S OR CLIENT'S LOCAL REPRESENTATION OF\n> >THAT STREAM. I have no idea how to make the statement any simpler. This\n> >is not about *what* the data is - it is about *how* the data is transported.\n> \n> If byte ranges do not address an object on the server, or a part thereof\n> (and part needs to be defined in that case), then they do not belong\n> in the URL space.\n> \n> I have nothing against byte ranges, or partial transfers in general,\n> but limiting oneself to byte ranges, and adding the syntax to the URL\n> space, is not the way to do it.\n\nI have no problem with that. My initial leaning towards putting it in the \nURL for the sake of not breaking proxies was more than adequately handled by \nthe header proposal. I have a deep suspicion that this is a case of \nviolent agreement. I was concerned about partial caching (something I \nthink is *badly* needed) while you were concerned about the byte range \nbeing part of the URL (something you are massively opposed to). We were \nsimply not talking about the same thing.\n\n;-)\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Connection Oriented HTTP conflic",
            "content": "Lou Montulli <montulli@mozilla.com> wrote:\n  > Roy T. Fielding wrote:\n  > > \n  > > > Did your proposal include multipart/mixed responses for\n  > > > keep-alive cgi scripts?  That seems to be what everyone\n  > > > is implementing.\n  > > \n  > > No, and I'm not aware of *any* implementation that does this.\n  > > Is somebody holding out on me?\n  > > \n  > > Right now, CGI scripts just ignore keep-alive and close the connection.\n  > > For the \"official\" HTTP/1.1, they can use the chunked transfer encoding\n  > > instead of a multipart, if desired.\n  > \n  > Netscape navigator and the netscape server both use \n  > \"multipart/mixed\" as per Alex's proposal.  I'm not\n  > aware of any server implementer's even wanting to\n  > support chunked transfer encoding.  I for one\n  > will strongly fight against adding yet another \n  > encoding form.\n\nFWIW, my (non-product) server supports chunking.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "If we introduce an Unless-modified-since header, it may be necessary,\nin the interest of completeness, to state what happens if both\nUnless-modified-since and If-modified-since headers are present in a\nrequest.  Also is Unless-modified-since honored in the absence of\nByte-range?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "    (optional) (3) we extend the set of headers to include the\n    generalization \"validator\" which might be used to contain (e.g.) a\n    date, filesize, hash, checksum, or random number chosen by the server\n    as pertaining to a unique version of the document, and allow\n    \"validator=<xxxxx>\" to appear as a header in server responses as well\n    as in this header.\n\nAs much as it pains me to disagree with someone who is trying to\nagree with me ...\n\nThe most important feature of a cache-validator is that it is \n100.00000000% opaque to clients and caches.  Checksums aren't\nopaque; dates aren't opaque; hashes aren't opaque.\n\nIf the client or cache is able to do anything with the validator\nbesides send it back to the server, then it won't work.  Which\nis to say that some creative but misguided programmer will try\nto use it in a way that will, sooner or later, make us all unhappy.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Jeffrey Mogul writes:\n >     (optional) (3) we extend the set of headers to include the\n >     generalization \"validator\" which might be used to contain (e.g.) a\n >     date, filesize, hash, checksum, or random number chosen by the server\n >     as pertaining to a unique version of the document, and allow\n >     \"validator=<xxxxx>\" to appear as a header in server responses as well\n >     as in this header.\n > \n > As much as it pains me to disagree with someone who is trying to\n > agree with me ...\n > \n > The most important feature of a cache-validator is that it is \n > 100.00000000% opaque to clients and caches.  Checksums aren't\n > opaque; dates aren't opaque; hashes aren't opaque.\n > \n > If the client or cache is able to do anything with the validator\n > besides send it back to the server, then it won't work.  Which\n > is to say that some creative but misguided programmer will try\n > to use it in a way that will, sooner or later, make us all unhappy.\n > \n > -Jeff\n\nMuch as I hate to agree with someone who is trying to disagree with\nme, I hate to disappoint you, but I think we're still agreeing.  My\nsuggestions were intended as things the server could use in computing\nthe validator, not things the client would *know* were in the\nvalidator.  I certainly DID NOT mean to imply that strings such as\n\n\"Validator:date=01-March-1995\" or \"Validator:checksum=19283719827\"\n\nwould appear.  I meant we'd see\n\n\"Validator:8923984792\"  and that the server would be\n\nresponsible for generating and understanding this.  Sorry for being,\nuh, opaque about this.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "I agree that there should be some *opaque* string used to select if the\nobject is the same or not (string which could be for instance a last\nmodified date, an MD5 digest,... whatever the server wants)\nAnd the client could for instance blindy use what the server sent as\nContent-Digest: (for instance, but we could use a different name if that\none is 'burned' ;-) )\n\nGET /someurl HTTP/1.x\nserver answers:\nHTTP/1.0 200 Document follows\n...\nContent-Digest: DATE=\"Nov 14 10:26:03 1995\"; MD5=0906bdfddce0964b42ad656 \nContent-Lenght: 12000\n\nLater client requests\nGET /someurl HTTP/1.x\nRequest-Range: 6000-12000\nPartial-If-Digest: DATE=\"Nov 14 10:26:03 1995\"; MD5=0906bdfddce0964b42ad656\n  ^^^^^^^^^^^^^^\n( that name is also open to any better suggestion )\n\n2 main cases now,\n1) server does not understand /deals with Request-Range:\nHTTP/1.0 200 Document follows\nfull object again\n2) server do understand, 2 sub cases:\n   a) digest,... is the same, ok\n      HTTP/1.0 206 Partial Content\n      Range: 6000-12000\n   b) digest,... is NOT the same, there we have two options again\n        i) send the whole thing again (to save \n           a request in the most probable case that the client would\n           request it anyway)\n        ii) send some error like some HTTP/1.0 3XX Changed  (or 4XX ?)\n           thus saving a bit of bandwith in the case the client\n           would throw it away\n\nMy preference goes to \"i)\" but maybe both way be valid ?\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\n\n\n"
        },
        {
            "subject": "Re: Connection Oriented HTTP conflic",
            "content": "dmk@allegra.att.com (Dave Kristol) wrote:\n\n> FWIW, my (non-product) server supports chunking.\n\nIs there an HTTP (firewall) proxy server implementation that supports\nthis?\n\nrom what I understand in the Keep Alive notes\n(<URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0063.html>)\nthis form of persistent connection can be used for a single\nbut unbounded request (as opposed to multiple requests)\n(<URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0237.html>).\n\nPhilip Thrift\nthrift@ti.com\n\n\n\n"
        },
        {
            "subject": "using opaque strings to determine uniquenes",
            "content": "On Tue, 14 Nov 1995, Laurent Demailly wrote:\n> I agree that there should be some *opaque* string used to select if the\n> object is the same or not (string which could be for instance a last\n> modified date, an MD5 digest,... whatever the server wants)\n\nWhile this sounds good in theory, I believe there are situations where \nthis breaks down. \n\nTime    Action\n\nT+0:    client A connects through proxy1.bigISP.com to a server which \n        contains a document which changes hourly, and has a CID of \"X\".\nT+1h:   client B connects through proxy2.bigISP.com to the same server,\n        and gets the hourly-changing document which now has a CID of \"Y\".\nT+1h1m: client B \"refreshes\" the page by doing an IMS request, but this\n        time goes through proxy1.bigISP.com, either because of round-robin\n        DNS or some sort of client load-balancing[*].\n        client B says \"send me the document, unless it has the CID of 'Y'\"\nproxy1.bigISP.com sees its CID is \"X\", not \"Y\", and sends the OLD\nDOCUMENT.\n\n[*] - consider the 20+ proxies that AOL uses, sometimes one user can go \nthrough multiple proxies in the same session to a server!\n\nNow, you can say that the server is negligent for not adding \"Expires\" \nheaders, but at any rate this works now with true If-Modified-Since.  And \nmy general gut feeling is that when you design protocols they should map \nto users and application builder's metaphors as closely as possible - \njust asking \"is this document different\" is *much* different than asking \n\"is this document more recent\".\n\nThat said, I think what we need for doing conditional requests is a \ngeneral grammar to which we can apply file attributes.  Something like\n\n  Send-If: \"Last-Modified\" > \"Nov 14 10:26:03 1995\" &&\n    \"Content-Digest\" != \"MD5=0906bdfddce0964b42ad656\"\n\nBy allowing equalities, inequalities, greater-than/less-than, and \nconjunctions, we remove the need for \"Send-Unless\", for \n\"In-Not-Modified-Since\", and lots of other things I'm sure we'll find.\n\nSince the \"fallback\" for conditional requests is to get the entire \ndocument, if the server can't determine enough attributes about the \nrequested object to answer the conditional query, it can just send the \ndocument.\n\nSimon, does HTTP-NG have something along these lines?\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "I am at home with the flu, logged in over a serial line, so here's responses\nto many comments.\n\nFrom: Albert-Lunde@nwu.edu (Albert Lunde)\n\n|With so many other deviations from MIME, I suggest we should drop the (rather\n|complex) MIME multi-part structure based on boundaries, etc. and only allow\n|multi-part messages defined by a Content-Length byte count.\n\nRather complex?  Hmmm.  One of the big wins from MIME is that it is trivial \nto encode and decode, especially over an 8-bit pipe, where it is a matter of\nwrapping and unwrapping.  I don't see, aside from the remote possibility\nI don't see, aside from the remote possibility that an intelligently-generated \nboundary-marker could appear in the data.\n\nWorking code already exists to perform MIME encoding and decoding, the main\nproblems in implementing multipart for the web is designing a scheme where we\ncan pack and unpack the parts and preserve the structure required for this \napplication of multipart.\n\n|We'd still need mechanisms in the headers to distingush what\n|initial headers applied to the whole transaction, and what\n|to a single body, and to indicate the presence of recursion\n|(multi-part stuff in a body section) if we allow it.\n\nAssume inheritance from top-level headers unless overridden by body-part\nheaders.\n\nFrom: dmk@allegra.att.com (Dave Kristol)\n|>\"Daniel W. Connolly\" <connolly@hal.com> wrote:\n|\n|  > Meanwhile, commercial folks are implementing HTTP-NG at lightning\n|  > speed. Six months from now, all the major vendors are doing\n|  > interoperable compression and encryption over something like SCP or\n|  > SSL (not to mention strong authentication).\n|\n|\n|Sorry, I'm skeptical about this statement.\n\nYeah, I have a touch of the flu, and it is especially effecting my sarcasm\nmeter, so I couldn't tell if Dan was serious about the 6mo time frame or not.\nI think the odds of that happening are between that of a photon spooking my \nCPU or finding a random 64-bit base 64 encoded number in random binary data.\n\nFrom: Albert-Lunde@nwu.edu (Albert Lunde)\n\n|I don't see how a message with a body containing a un-encoded GIF\n|can be represented with the MIME RFC.\n\nEasily.  I believe that it is properly demonstrated in my example with the\nContent-Transfer-Encoding: 8bit.  MIME was tweaked on 7-bit transport, so we\nare used to having to decode base64 data because we mostly see MIME in mail \n(or don't, as the case may be), but it is designed to work as well on an 8-bit \nchannel.  Seems like people are seeing a problem here where none exists.  \n\nThe MIME spec provides for the unencoded transfer of 8-bit data, HTTP operates\nover an 8-bit channel.  What's the problem?\n\n|I think having a multi-part structure that logically maps onto MIME\n|is a good idea, but we seem to have made and be making different\n|decisions about transport on a byte-by-byte level.\n\nAs long as you leave an encoding trail in your normalization scheme for CR/LF,\nyou should be able to revert a known state by traversing that trail in reverse.\n\nFrom: john@math.nwu.edu (John Franks)\n\n|It seems to me that this \"user's perceived performance\" or UPP is going\n|to be the dominant consideration for commercial client developers.  If\n|they can't match Netscape they simply won't be viable.\n\nUntil Netscape's net.chickens come home to net.roost, that is.\n\nNetscape's UPP win comes as it GET's the HTML doc, initiates as many TCP \nconnections as images it lacks in its cache.  Then, as soon as it has sizing\ninfo, renders the document with proper-sized holes for the images.  As the \ndata arrive for each image, it simultaneously renders each image until the \ntransmissions close.  \n\nUnder a multipart solution, what is to stop the client from creating a thread \nto conduct the multipart get, and as soon as the HTML doc arrives (transferring \nit as the first body part would be a smart thing), rendering it with 'n' holes, \nand then resizing and filling in the holes as their data arrive--all with a \nsingle TCP connection, or assuming residency of some inlines on the other \nservers, fewer than 'n' connections.\n\nDo browsers have to know the size of the image before you create a spot for it\non the page at render time?  Couldn't you render the HTML as soon as it\narrives enabling anchors and leaving a standard sized hole, like the [S] icon \nthat xmosaic uses, and resize the hole as the image data arrive?  It'd be a \nbit jumpy...unacceptably so?\n\n|I guess the bottom line is that there is not much point in changing\n|HTTP unless the resulting protocol can (1) at least match the Netscape\n|UPP, and (2) simultaneously significantly improve network efficiency.\n\nFrom: jim@spyglass.com (Jim Seidman)\n\n|I have to disagree here.  If the issue is how quickly all the text can be\n|downloaded, then the fastest way to do this is to send only the text and\n|then pass on the images later.\n\nGiven the scenario above, sounds like multipart/mixed gets an A in net.\ncitizenship (points off for sending all those nasty ascii headers) and perhaps \na A- in UPP (since the images do not all arrive simultaneously), although \nsomeone has sketched a scheme for multiplexing several images into a \nMIME stream earlier in the year on www-talk, I think, which would do just that.\n\n|In my mind the only defensible reason for simultaneous connections is to\n|reduce the round trip time penalty for loading all the pieces of a document.\n\nIt's to make the RTT's happen at the same time by parallelizing them instead\nof sequentializing them.\n\nDo any of you all out there with gig and gig of log files have any data on \nwhat percentage of requests for HTML docs come from Netscape?\n\nrom the HTTP perspective, multipart/mixed for representing HTML, is probably\na bridge solution to realize a limited performance gain until we can see \nwidespread deployment of next-generation and binary protocols.  But for the \nweb as a worldwide information system, there are long-term benefits to \nextending the interchange of HTML and therefore the web beyond just HTTP, but \nto other MIME-compliant systems like NNTP and SMTP.  \n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "    I agree that there should be some *opaque* string used to select if the\n    object is the same or not (string which could be for instance a last\n    modified date, an MD5 digest,... whatever the server wants)\n    And the client could for instance blindy use what the server sent as\n    Content-Digest: (for instance, but we could use a different name if that\n    one is 'burned' ;-) )\n    \nI would recommend against using the Content-Digest as the\ncache-authenticator.  This means that if a server wants to control\ncaching, it is forced to generate a Content-Digest that is also\nsemantically correct for whatever other uses a Content-Digest\nis useful for.\n\nIn short, Content-Digest is NOT opaque to the client.\n\nIt also means that the server must either re-digestify the object\nto do a validity check, or keep a database of Digest values.\n\nIf the server wants to use a simpler scheme (such as file modification\ntime) to generate a validator, it should be able to do so.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: using opaque strings to determine uniquenes",
            "content": "Brian Behlendorf writes:\n    While this sounds good in theory, I believe there are situations where \n    this breaks down. \n    \n    Time    Action\n    \n    T+0:    client A connects through proxy1.bigISP.com to a server which \n    contains a document which changes hourly, and has a CID of \"X\".\n    T+1h:   client B connects through proxy2.bigISP.com to the same server,\n    and gets the hourly-changing document which now has a CID of \"Y\".\n    T+1h1m: client B \"refreshes\" the page by doing an IMS request, but this\n    time goes through proxy1.bigISP.com, either because of round-robin\n    DNS or some sort of client load-balancing[*].\n    client B says \"send me the document, unless it has the CID of 'Y'\"\n    proxy1.bigISP.com sees its CID is \"X\", not \"Y\", and sends the OLD\n    DOCUMENT.\n\n    Now, you can say that the server is negligent for not adding \"Expires\" \n    headers, but at any rate this works now with true If-Modified-Since.\n\nHey folks, we're writing the spec, we should be able to require servers\n(and proxies) to play by the rules.\n\nIf you recall, I suggested that an object without an explicit Expires:\nheader attached must always be validated by a proxy.  There are three\ncases:\nExpires: missing\nvalidation required on every fetch from any cache\nExpires: \"never\"\nvalidation never required (immutable documents)\nExpires: <some timestamp>\nvalidation not required until <timestamp>, but\nalways required after that.\n\nThe failure in your example could only happen if either\n(1) The server sets an explicit expiration date that is\ntoo far in the future\nor\n(2) the caches are not obeying the spec.\n\nI should also point out that the client should NOT be phrasing\nits request as:\n\"send me the document, unless it has the CID of 'Y'\"\nbut rather it should be saying (i.e., the protocol should define\nthe requst as meaning):\nsend me the document UNLESS my copy with CID=Y is valid\nthis puts the burden on the proxy to validate the client's copy,\nrather than on the client to know by magic if CID=Y is still valid.\n\n    And my general gut feeling is that when you design protocols they\n    should map to users and application builder's metaphors as closely\n    as possible - just asking \"is this document different\" is *much*\n    different than asking \"is this document more recent\".\n\nMy feeling is that when we design protocols, they ought to\noperate correctly.  If our users are naive about the foundations\nof caching in distributed systems (something that even computer\nscientists took several decades to understand), we should make\nthe caches transparent to them, not try to adopt some metaphor\nthat doesn't actually work.  Remember the solar system prior\nto Copernicus?  It got extremely complicated to do astronomy\nbecause the \"users\" were stuck in a geo-centric metaphor.\n\nSo users should not normally be placed in the position of having\nto ask for \"more recent documents\".  The HTTP protocol, servers,\nproxies, and clients should present a valid view of an object\nwhenever a user asks to see it.  Of course, we know that implementors\nare only human, and implementations will have bugs.  That's what\nthe \"reload\" key is for, but it shouldn't be used nearly as often\nas we do today.  That's definitely a bug.\n    \n    That said, I think what we need for doing conditional requests is a \n    general grammar to which we can apply file attributes.  Something like\n    \nBut this implies that the clients and servers share a deep understanding\nof the attributes of the objects.  What about objects that don't have\n\"modification times\"?  What other attributes could be both useful and\ngenerally supported?  I'm as much of a fan of file attributes as anyone\n(after all, I did my PhD dissertation on the topic), but I don't think\nwe should be pushing HTTP in that direction.  HTTP is not a file\naccess protocol.\n\nAnd if someone does come up with another use for attributes, are these\nreally the right thing to use for cache validation?  I think not.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Comments on Byte range draf",
            "content": "Jeffrey Mogul writes:\n       [me:]\n >     I agree that there should be some *opaque* string used to select if the\n >     object is the same or not (string which could be for instance a last\n >     modified date, an MD5 digest,... whatever the server wants)\n >     And the client could for instance blindy use what the server sent as\n >     Content-Digest: (for instance, but we could use a different name if that\n >     one is 'burned' ;-) )\n[rest of my suggestion, which gave more information about how\n\"flexibility\" of the digest can be used to make the thing really\nopaque was deleted by Jeffrey]\n > I would recommend against using the Content-Digest as the\n > cache-authenticator.  This means that if a server wants to control\n > caching, it is forced to generate a Content-Digest that is also\n > semantically correct for whatever other uses a Content-Digest\n > is useful for.\n > In short, Content-Digest is NOT opaque to the client.\nAs I said, It can be made, just a matter of defining the correct\nbehaviour, see below. (as content-digest: does not yet exists, it is\nstill possible to state precisely several cooperating utilisations)\n > It also means that the server must either re-digestify the object\n > to do a validity check, or keep a database of Digest values.\nThis is not a problem, as the \"digest\" can be anything server wants \n > If the server wants to use a simpler scheme (such as file modification\n > time) to generate a validator, it should be able to do so.\nIt can, As I gave in exemple (did you read?) there is no problem using\nContent-Digest: WHATEVERKEYWORD=WHATEVERVALUE\nwhich couple is opaque\nie it is ok to have (again from my exemple):\nContent-Digest: date=\"Tue Nov 14 21:18:28 MET 1995\"\nor whatever the server wants\neven\nContent-Digest: private=\"816380285\"\nwhich is faster to compare, or whetever...\n\nanyway, again, if you can't bare the name, its no problem for me to\nuse another one \n(though suggesting server implementation based on digest is probably\nthe safest (compared to date stuff))\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\n\n\n"
        },
        {
            "subject": "using opaque strings to determine uniquenes",
            "content": "Brian Behlendorf writes:\n > On Tue, 14 Nov 1995, Laurent Demailly wrote:\n > > I agree that there should be some *opaque* string used to select if the\n > > object is the same or not (string which could be for instance a last\n > > modified date, an MD5 digest,... whatever the server wants)\n > \n > While this sounds good in theory, I believe there are situations where \n > this breaks down. \n > \n > Time    Action\n > \n > T+0:    client A connects through proxy1.bigISP.com to a server which \n >         contains a document which changes hourly, and has a CID of \"X\".\n > T+1h:   client B connects through proxy2.bigISP.com to the same server,\n >         and gets the hourly-changing document which now has a CID of \"Y\".\n > T+1h1m: client B \"refreshes\" the page by doing an IMS request, but this\n >         time goes through proxy1.bigISP.com, either because of round-robin\n >         DNS or some sort of client load-balancing[*].\n >         client B says \"send me the document, unless it has the CID of 'Y'\"\n > proxy1.bigISP.com sees its CID is \"X\", not \"Y\", and sends the OLD\n > DOCUMENT.\n\nGood point, let's think it through.\n\nThe rule for a proxy performing this kind of comparison must be different\nfrom the current rule for GET if-modified-since.\n\nIn the case of GET if-modified-since, the proxy is allowed to service\nthe request out of the cache if the last-modified date of its\n(non-expired) copy of the document is after the request's if-modified-since\ndate, otherwise it has to pass on the request.  It can return 304 if\nit holds a non-expired document that was modified <= the request's\nif-modified-since time.\n\nIn the case of an opaque string match, let's consider two versions of\na document, served by the server at times T and T+n.  At T, the\nserver provides a document with an opaque string \"X\".  At time T+n\nthe server begins serving a new version of that document with opaque\nstring \"Y\".\n\nA proxy can only service the request for `unless \"X\"' out of its cache,\nwhen it contains a copy of the document, under these conditions:\n\n(a) If the proxy holds no document matching \"X\":\n\nIf the proxy has records of having received the document version \"X\"\nand knows that it received its version, \"Y\", more recently than it\nreceived version \"X\", it can return \"Y\".\n\nIf it is unaware of version \"X\", it cannot serve version\n\"Y\" out of its cache, even if it turns out that \"Y\" is newer than \"X\".\n*** This is the condition under which this scheme differs most from\nthe if-modified-since scheme ***.\n\n(b) If the proxy contains a cached copy matching \"X\":\n\nIf the proxy has received a version of the document more recently than\nit received one with \"X\", it returns the more recently received one.\n\nIf it hasn't received a copy more recently, the proxy can follow the\nsame rules as GET if-modified-since, using the expiration date to\ndecide if it must forward the request, or can return a 304.\n\nSo using opaque strings results in some more cache misses when proxies\nhave only a sparse history of the versions of a document.  The\nadditional \"hair\" in proxies is that to make this work optimally, they\nhave to keep a history of the versions of a document they have\nreceived, and when they have received them.\n\nI'm sure if I made an error in this, someone will point it out...\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: using opaque strings to determine uniquenes",
            "content": "Jeffrey Mogul writes:\n > \n > Hey folks, we're writing the spec, we should be able to require servers\n > (and proxies) to play by the rules.\n > \n > If you recall, I suggested that an object without an explicit Expires:\n > header attached must always be validated by a proxy.  There are three\n > cases:\n > Expires: missing\n > validation required on every fetch from any cache\n > Expires: \"never\"\n > validation never required (immutable documents)\n > Expires: <some timestamp>\n > validation not required until <timestamp>, but\n > always required after that.\n > \n\nBut do you really want to just ignore the case where the server has made an\nincorrect estimate about the expires date and issues a new version of\na document before that date?\n\nThe typical case is that, while a file may change at any time, we\nstill want caches to cache it.  Yet we generally do not want users to\nreceive out of date, or more to the point, \"previous\" versions of\nthings to the versions they have already got.  These goals are\nsomewhat in contradition.\n\nBrian's example could happen, and even if it is \"valid\" for it to\nhappen, it isn't \"nice\".  The point is that proxies can make *some*\nefforts to prevent such things from happening, with some additional\nbookkeeping, and with some (I claim not very much) less effective caching.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: using opaque strings to determine uniquenes",
            "content": "On Tue, 14 Nov 1995, Jeffrey Mogul wrote:\n> If you recall, I suggested that an object without an explicit Expires:\n> header attached must always be validated by a proxy.  There are three\n> cases:\n> Expires: missing\n> validation required on every fetch from any cache\n> Expires: \"never\"\n> validation never required (immutable documents)\n> Expires: <some timestamp>\n> validation not required until <timestamp>, but\n> always required after that.\n\nOkay, this would seem to solve the problem conditions I can think of.\n\n>     That said, I think what we need for doing conditional requests is a \n>     general grammar to which we can apply file attributes.  Something like\n[my proposal for Send-If, which I was informed was proposed at one point \nfor HTTP as \"Unless:\", and will hopefully be in 1.1 - cool]\n\n> But this implies that the clients and servers share a deep understanding\n> of the attributes of the objects.  What about objects that don't have\n> \"modification times\"?  What other attributes could be both useful and\n> generally supported?  I'm as much of a fan of file attributes as anyone\n> (after all, I did my PhD dissertation on the topic), but I don't think\n> we should be pushing HTTP in that direction.  HTTP is not a file\n> access protocol.\n\nMy mistake for calling them file attributes rather than object \nattributes - while you might refrain from calling it a \"file access \nprotocol\", I think we could certainly agree to call it an \"object request \nprotocol\".  To some extent, \"Send Unless\" and conditional GETs are just \nan optimization over doing a HEAD request on an object before fetching \nit, so i'm not going to proclaim that the WWW will fall apart if it's not \nimplemented.\n\n> And if someone does come up with another use for attributes, are these\n> really the right thing to use for cache validation?  I think not.\n\nI will defer to the cache validation experts.  Of which there don't seem \nto be many :)\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Byteranges with 206 partial conten",
            "content": "> > An additional feature is to say \"give me a range if the document\n> > hasn't changed, but if it has, send me the entire document\".  Similar\n> > to If-modified-since, but still quite different...  What would you\n> > call such a header?\n> \n> If-Modified-Since   (for the current case)\n> Unless              (for the generic case)\n> \n> > I will re-vamp a new version of the byterange draft reflecting these\n> > changes, and will submit it for review in http-wg shortly.\n> \n> I am willing to proceed with this in the main HTTP/1.1 spec, since the\n> changes required are interwoven with the description of GET and caching.\n> \n> However, I am not willing to support multiple ranges within a single\n> request at the current time, so no multipart/x-byteranges.\n> \n>  ...Roy T. Fielding\n>     Department of Information & Computer Science    (fielding@ics.uci.edu)\n>     University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n>     http://www.ics.uci.edu/~fielding/\n> \n\nCould we use extra parameters in Request-Range as Shel Kaphan suggested instead\nof having interactions between headers?  \n\nrom Shel:\n\nRequest-range:  bytes=X-Y; validator=<xxxxx>; other-parameter=<yyyy>\n\nI think it provides a cleaner implementation.  The value of the validator\n(or whatever we call it) can be interpreted by the server returning the\nobject.  One thing I'm not clear on is which header this identifier would be\npassed in when delivering the original object?  One downside to this would be\nthat mirrored objects would have to have consistent values for the validator.  \nWhen using URNs in something like the path scheme there is no guarantee that\nthe request will return to the original server.  So servers would have to\ncooperate when mirrorring documents.\n\nAnother thing I like about Shel's suggestion is the idea of being able to use\nan opaque string to uniquely identify an object which could be used instead of\nthe last modification date and URL. We are looking at the possibility of using\ninternal identifiers for each object for other reasons, but they could also be\nused for the opaque string.  It would rapidily indicate to our server if the\nrequested URL and the previously identified object are identical or not.  Again,\nwe'd have to keep track of which documents are mirrored and treat the validator\ndifferently for mirrorred documents and non-mirrored documents.\n\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Re: using opaque strings to determine uniquenes",
            "content": "> Jeffrey Mogul writes:\n>  > Hey folks, we're writing the spec, we should be able to require servers\n>  > (and proxies) to play by the rules.\n>  > \n>  > If you recall, I suggested that an object without an explicit Expires:\n>  > header attached must always be validated by a proxy.  There are three\n>  > cases:\n>  > Expires: missing\n>  > validation required on every fetch from any cache\n>  > Expires: \"never\"\n>  > validation never required (immutable documents)\n>  > Expires: <some timestamp>\n>  > validation not required until <timestamp>, but\n>  > always required after that.\n>  > \nHmm. Expires: \"never\" currently should be interpreted as 'expired in past',\nas \"never\" isn't a valid date. Is it not too late to change the spec defining\n\"never\" as the +infinity date? Perhaps \"cache-control: max-age=infinity\" is\na better alternative than \"Expires: never\" to mean immutable document.\nWe have one open question:\nwhat is the meaning, when Expires and cache-contol: max-age both present\nin a response?\nI proposed, that before the expiry date max-age has no effect, and after \nthe expiry date max-age takes over.\n(For me Expires: <now+1week> isn't the same as cache-control: max-age=<1week>.\nStarting at the second week, the first requires check on every request, while\nthe second requires weekly check if the document requested enough frequently.)\nShel Kaphan replies:\n> But do you really want to just ignore the case where the server has made an\n> incorrect estimate about the expires date and issues a new version of\n> a document before that date?\nIn most cases humans will estimate the expires date, not servers. Using AI\nservers can make estimations, but till now I not heared about servers featuring\nthis. Independently of the source, estimates allways have some errors.\nOne way of extending the protocol may be the inclusion of (estimates of)\nthose errors.\n> The typical case is that, while a file may change at any time, we\n> still want caches to cache it.  Yet we generally do not want users to\n> receive out of date, or more to the point, \"previous\" versions of\n> things to the versions they have already got.  These goals are\n> somewhat in contradition.\nHTTP caches are free in doing extra checks. (The protocol specifies when\ncaches MUST check the origin server - in fact the next-hop cache.)\nThose extra checks are cache implementation issues too.\nIn contrary, assigning expiry dates for documents, not having Expires or\ncache-control: max-age is a protocol violaton in some sense. \n(The alternative interpretation of no expires as expires: \"don't care\"\nhowever legalises that.)\n> Brian's example could happen, and even if it is \"valid\" for it to\n> happen, it isn't \"nice\".  The point is that proxies can make *some*\n> efforts to prevent such things from happening, with some additional\n> bookkeeping, and with some (I claim not very much) less effective caching.\nHaving extra cache validators like checksums and digests in addition to\ncontent-length and last-modified can help in verifying Location and URI\nheaders pointing to other servers by requesting a HEAD from the other server.\n(Think of mirroring widely used in ftp world and sometimes (mostly trough ftp\nmirroring) in www world.)\nNOTE: Beth Frank's notes on opaque validators are serious when talking about\nmirrored resources.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "ContentDigest heade",
            "content": "I believe the proposal draft-demailly-cd-header-00 is extremely\ncounter-productive.\n\nMy concerns:\n\n1) Creating a new mechanism to provide an existing service (Content-MD5)\nwould cause user agents to be more complex with no gain in functionality.\n\n2) Allowing more than one digest algorithm is harmful to interoperability.\n\n3) Content-MD5 is a widely deployed Draft Standard protocol.  It works well\nin practice.  It is unlikely the IETF will permit an incompatible method for\ndoing the same thing to be standardized, and I will certainly encourage them\nto oppose this proposal.\n\nTo address the complaints with Content-MD5:\n\n1) You have to digest _something_ and it is important that that something is\nwell specified.  Thus canonical format discussions are absolutely necessary.\nThe canonical format for http is whatever data is sent over the wire\nas the \"body\".\n\n2) What's the big deal of converting between base64 and hex coding?  It's\na few lines of code, and is only an aesthetic consideration.\n\n3) If MD5 is too computationally intensive for some situation, then there\nprobably shouldn't be any digest in that situation to get reasonable\nperformance.\n-----\nChris Newman <chrisn+@cmu.edu>, http://www.contrib.andrew.cmu.edu/~nifty/\nThe worst thing about censorship is: [censored]\n\n\n\n"
        },
        {
            "subject": "comments on Robinson's CGI spec",
            "content": "Here are some more comments on David Robinson's (Oct 16 and Nov 15,\n1995) CGI 1.1 specification.\n\nPATH_INFO\n    I think it's important for the CGI to be able to reconstruct the\n    original request URI.  With NCSA's server, the original request is\n    http://$SERVER_NAME$SERVER_PORT$SCRIPT_NAME$PATH_INFO{?$QUERY_STRING}\n\n    I would prefer how PATH_INFO is set to be spelled out, not be left\n    so vague as to be useless for a specification.  However, let me\n    suggest we take a page from the ANSI C standard and create\n    \"implementation-defined\" behavior.  An implementation would be\n    required to document what its PATH_INFO (and other environment\n    variable) behavior is.  More clearly than is now the case!\n\nPATH_TRANSLATED\n    I think the description is wrong.  enc-path conventionally encodes\n    both the name of the script and PATH_INFO, not just PATH_INFO.  I\n    agree that PATH_TRANSLATED is a translation of the PATH_INFO.\n\n    I think \"enc-path\" could never be null as shown in the example,\n    although it can be null after the script name is stripped from\n    enc-path.  (A client must specify some character to take the place\n    of URL in the request line.  That character would logically be a\n    part of PATH_INFO as currently described.)\n\nDefining a script URI\n    The information here, particularly script-uri, appears to conflict\n    with the information about PATH_INFO and PATH_TRANSLATED.  Here we\n    see enc-script called out explicitly, which is what I would expect.\n\nData output from the CGI script: Content-Type\n    To answer the questions in the Note:  Content-Type should be mandatory\n    only when there's a message body.  That gets said under Parsed Header\n    Output.\n\nData output from the CGI script: Status\n    I believe Status: can have an optional comment:\n    Status = \"Status\" \":\" 3digit *<not NL> NL\n\nData output from the CGI script: HTTP headers\n    I don't understand what this section is trying to say.  For which\n    HTTP headers does the CGI header syntax differ?\n\nRecommendations for scripts\n    Why shouldn't the CGI script expect the server to fold PATH_INFO\n    (PATH_TRANSLATED) information that contains \".\" or \"..\" the same\n    way that the server handles them in a URL?  In that case, the CGI\n    would never actually see such stuff.  Likewise for \"//\".\n\nSystem-specific standards\n    I would like to see another \"implementation-defined\" item:  for\n    Unix systems (at least) what is \".\" when the script runs?  NCSA\n    httpd, for example, sets \".\" to the directory of the script.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: ContentDigest heade",
            "content": "Chris Newman writes:\n > I believe the proposal draft-demailly-cd-header-00 is extremely\n > counter-productive.\n > \n > My concerns:\n > \n > 1) Creating a new mechanism to provide an existing service (Content-MD5)\n > would cause user agents to be more complex with no gain in functionality.\nNo major http UA use Content-MD5\nand no server at all produce them\n > 2) Allowing more than one digest algorithm is harmful to interoperability.\nSome user *will* need other algorithm\nIf not today, in few monthes or years, I'll bet on that,\nyou need to think a bit for the future and standardize will its not\ntoo late\n > 3) Content-MD5 is a widely deployed Draft Standard protocol.  It works well\n > in practice.  It is unlikely the IETF will permit an incompatible method for\n > doing the same thing to be standardized, and I will certainly encourage them\n > to oppose this proposal.\nMy bike works well for some stuff, but I won't use it to go 100 miles\naway\n > To address the complaints with Content-MD5:\n > 1) You have to digest _something_ and it is important that that something is\n > well specified.  Thus canonical format discussions are absolutely necessary.\n > The canonical format for http is whatever data is sent over the wire\n > as the \"body\".\nThis has to be defined, indeed\n > 2) What's the big deal of converting between base64 and hex coding?  It's\n > a few lines of code, and is only an aesthetic consideration.\nIt is no big deal except you can use it as hex with 0 lines of new\nsoftware... \n > 3) If MD5 is too computationally intensive for some situation, then there\n > probably shouldn't be any digest in that situation to get reasonable\n > performance.\nThat's untrue\nsome people could be happy with unix sums, or any private digest\nalgorthm,\nAlso the Digest could be used as an opaque string by the client to do\nconditional requests...etc...\n\ndl\n--\nLaurent Demailly * http://hplyot.obspm.fr/~dl/ * Linux|PGP|Gnu|Tcl|...  Freedom\nPrime#1: cent cinq mille cent cinq milliards cent cinq mille cent soixante sept\n\n\n\n"
        },
        {
            "subject": "More notes on content negotiatio",
            "content": "As promised, here are some more notes on content negotiation.  The\ntext below assumes that you know what a `negotiation port' and\n`variant set' is, see my previous message.\n\nReactive negotiation\n--------------------\n\nThere are two possible forms of two-round-trip reactive negotiation.\nThey start out the same:\n\n1) Client sends a request with some small set of accept headers X\n   to a negotiation port on URI P\n\n2) Server calculates the quality values for its variants, but has\n   insufficient information in the headers X to determine which\n   variant would match best to the client capabilities.  Server\n   responds with a 300 Multiple Choices or 406 None Acceptable\n   response containing an encoding of the variant set V bound to P,\n   also listing some of the properties of the variants.\n\nFirst form: early reactive negotiation (as in the v10-spec-01 text)\n\n3A) Client chooses the URI U with the highest quality from the variant\n    set V and issues a request for the contents of U\n\n4A) Server sends back the contents of U\n\nI call this _early_ reactive negotiation because the variant selection\nis done early, in step 3A.\n\nSecond form: late reactive negotiation (new, proposed)\n\n3B) Client sends a new request on the negotiation port P with\n    _different_ accept headers Y. The headers Y are \n      i) for naive clients: headers containing all configured MIME\n         types, languages, etc.\n     ii) for sophisticated clients: headers containing a subset of the\n         configured MIME types, languages, etc. constructed by looking\n         at the variant properties in the variant set returned in \n         step 2): the headers Y would allow the server to\n         unambiguously select the variant with the best quality.\n    The client also sends a special `you-choose-one' flag that will\n    force the server to choose and send back one of the variants in\n    its variant set.  If the `you-choose-one' flag is present, the\n    server may not send a 300 Multiple Choices or 406 None Acceptable\n    response, even if the accept headers still give an ambiguous match\n    to the variant set.\n\n4B) The server selects the response with the highest quality based on\n    the accept headers Y and sends it back.\n\nI call this _late_ reactive negotiation because the variant selection\nis done late, in step 4B.\n\nThe only thing that is needed to add late reactive negotiation to the\nprotocol would be the introduction of the `you-choose-one' flag.  We\nmay want to introduce such a flag anyway to make 1.0->1.1 request\ntranslation in proxies easier.  I propose to introduce such a flag.\n\nUsing the `vary' fields in the URI header, proxies can safely cache\nall transactions in the two kinds of reactive negotiation described\nabove.\n\nIf late reactive negotiation is added to the protocol, making a\nminimal implementation of a negotiation conformant client is very\nsimple, because the client would not have to have a `variant selection\nengine' that matches up a user agent profile with a variant set to\ndetermine the best variant.\n\nAlso, if early reactive negotiation is taken out of the protocol,\n`variant selection engines' are only needed in proxies and origin\nservers, not in user agents.  That could make it a lot less painful to\nintroduce new variant selection engine functionality in subsequent\nprotocol versions.\n\nInstead of taking early reactive negotiation out completely, we could\nalso define circumstances under which a client side `variant selection\nengine' would produce a `cannot determine which variant is best'\nresult, which would cause late reactive negotiation to be used.  One\nof those circumstances could be that the client side `variant\nselection engine' has a lower `engine version number' than the\nrequired version number attached to the variant set.  Again, this\nwould allow smooth upgrades to higher levels of engine complexity.\n\nPerhaps even more importantly, having a safe way to disable the\nclient/proxy side `variant selection engine' would allow service\nproviders to use special purpose variant selection algorithms that\ncannot be `run' on client-side `variant selection engines'.  One\nexample of such a special purpose algorithm is negotiation around\nknown bugs in user agents based on the user-agent header.\n\n\nUser-agent header based negotiation\n-----------------------------------\n\nOne of the requirements I have for HTTP/1.1 content negotiation is\nthat variant selection based on the contents of the User-Agent header\nsent by the user agent is efficient.  In my opinion, the v10-spec-01\nnegotiation text does not satisfy this requirement.\n\nIn a typical example, the origin server has 3 different versions of a\nHTML page under negotiation port URI /some/report:\n\n  /some/report.html.1 : HTML page without tables\n  /some/report.html.2 : HTML page with tables and tables-in-tables\n  /some/report.html.3 : HTML page with tables but no tables-in-tables, \n                 because they would trigger a bug in user agent X.\n  /some/report.dvi    : document in dvi file format\n\nThe origin server would use some specialized variant selection rules,\ne.g. based on a database of client capabilities at the server side, to\nselect the proper version of the HTML page if the client has no dvi\nviewer capabilities (with a high enough quality).  Proxy caches and\nuser agents have no hope of doing this negotiation on behalf of the\norigin server.\n\nIn the case of user agents, this shows that `variant selection\nengines' in user agents _must_ have the option of saying `cannot\ndetermine which variant is best' and delegate the decision to the\norigin server (as in late reactive negotiation).  The alternative\nwould be for origin servers to 1) never combine user-agent negotiation\nwith other negotiation, so that it can always be done preemptively or\n2) tailor variant sets sent in URI headers for each user agent, so\nthat the client side variant selection engines can never select the\nwrong variant. But then you get into problems with proxy caches that\nserve more than one kind of user agent, so you would have to disallow\ncaching.\n\nSuppose that a proxy cache serving a large number of different user\nagents already has the /some/report variants /some/report.html.1 and\n/some/report.html.2 already in cache.\n\nNow, suppose that the proxy gets a request for /some/report from a user\nagent with a user-agent string X it has not yet seen in earlier\nrequests for /some/report. It will then need to relay the request to the\norigin server, and the origin server will (either immediately or in a\nreactive negotiation response) send a reply with the correct variant,\nwhich happens to be variant 2, to the proxy.  But this is wasteful,\nthe proxy already had variant 2 in cache memory.\n\nI therefore propose a new GET request header:\n\n  Send-no-body-for: <list of variant URIs> ,\n\nwhich would cause a server serving a GET request on a negotiation port\nto omit sending back the body of the selected variant if the\n(Location) URI of that variant is in the list.\n\nFor the example above, the GET request would look like:\n\n GET /some/report HTTP/1.1\n User-Agent: X\n Accept: <something indicating that the user agent cannot\n          handle dvi files>\n Send-no-body-for: /some/report.html.1 /some/report.html.2\n ....\n\nand the response would be\n\n 20x No Body\n Location: http://blah.com/some/report.html.2\n ....\n (no response body here)\n\nIf we have this header, it is not that bad if the `variant selection\nengine' in the proxy is unable to select the appropriate variant for a\npreviously unseen combination of varying request headers.  Being\nunable to select would cost time because the origin server has to be\ncontacted, but it would hardly cost any bandwidth.\n\nThus, if we have this header, there is a lot less pressure on `variant\nselection engines' in proxies and user agents to always produce a\ndefinite result.  Producing a `cannot determine which variant is best'\nwould only cause an overhead comparable to a conditional GET request.\nThis lack of pressure would allow the `variant selection engine' we\nneed to standardize in the HTTP protocol to be simple, which is a good\nthing.\n\nWe could even decide to do away with `variant selection engines' in\nproxies and user agents altogether, and use late reactive negotiation\nonly.\n\n\nSummary of requirements mentioned in these notes\n------------------------------------------------\n\n- We need a `superstructure' to the content negotiation headers to\nallow these headers to be discussed and defined more easily.\n\n- The content negotiation mechanism should allow a clear presentation\nof all available variants to the end user.\n\n- The mechanism should allow negotiation based on the User-Agent\nheader to be efficient.\n\n- The mechanism should not put too much pressure on client side\n`variant selection engines' to always produce a definite result.\n\n- There should be a way to bypass client side `variant selection\nengines' to allow use of variant selection algorithms not (yet)\nsupported by those engines.\n\n\nSummary of proposals in these notes\n-----------------------------------\n\nFor meeting these requirements in HTTP/1.1, I propose that\n\n1) there should be a negotiation model like the `negotiation port\nmodel'.\n\n2) for each response header in a negotiated response, it should be\ndefined whether it applies to the the variant set bound to the\nnegotiation port, the variant chosen, or the response as a whole.\n\n3) The exact semantics of Expires headers and conditional GETs for\nnegotiated URIs need to be defined.\n\n4) there should be a `you-choose-one' flag in requests to force a\nserver to select one variant.  This allows for safe late reactive\ncontent negotiation and may also be nice for 1.0->1.1 gateways.\n\n5) there should be a Send-no-body-for: <list of variant URIs> request\nheader to make content negotiation more efficient and take some\npressure off the requirements for client-side `variant selection\nengines'.\n\n6) Client side `variant selection engines' must have the option of\nsaying `cannot determine which variant is best' and leaving the\nvariant selection to the server.\n\n7) To allow service providers to bypass client side `variant selection\nengines' when implementing a negotiation algorithm that cannot be\n`run' on such engines, the protocol must define at least one situation\nin which the client side `variant selection engine' must take itself\nout of the loop and leave the variant selection to the server.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "New byterange",
            "content": "Here's a new revision of the byterange draft.  In short, what changed\nis now we have Request-range request header, and the response comes as\n206 Partial Content.  There's an additional Unless-modified-since to\ncause a 200 response if things have changed.\n\nThe word \"url\" should probably be dropped from the draft filename.\n\nI urge someone to write up a separate Internet Draft for cache updates\nand validators.  If I had time I would gladly do it myself.  I'm all\nfor them, but I will not even try to tackle that area in this I-D.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n----------------------------------------------------------------------------\n\nINTERNET-DRAFT                                              Ari Luotonen\nExpires: May XX, 1996                Netscape Communications Corporation\n                                                             John Franks\n                                                 Northwestern University\n<draft-luotonen-http-url-byterange-XX.txt>             November XX, 1995\n\n\n                      Byte Range Extension to HTTP\n\n\nSTATUS OF THIS MEMO\n\n   XXX\n\n\nTABLE OF CONTENTS\n\n   1.      Overview ................................................. X\n\n   2.      Allow-Ranges HTTP response header ........................ X\n\n   3.      Byte range HTTP request .................................. X\n   3.1.    Request-Range HTTP request header ........................ X\n   3.1.1.  Multiple ranges .......................................... X\n   3.1.2.  Examples ................................................. X\n   3.2.    Unless-Modified-Since HTTP request header................. X\n\n   4.      Byte range HTTP response ................................. X\n   4.1.    206 Partial Content ...................................... X\n   4.2.    Range HTTP response header ............................... X\n   4.2.1.  Examples ................................................. X\n   4.3.    Multiple ranges as multipart MIME messages ............... X\n   4.4.    Caching issues ........................................... X\n\n\n\n\nLuotonen, Franks                                                [Page 1]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   5.      Future considerations .................................... X\n   5.1.    Extending Allow-Ranges, Request-Range and Range headers .. X\n   5.2.    Other possible ranges .................................... X\n\n   6.      References ............................................... X\n\n   7.      Authors' Addresses ....................................... X\n\n\n1. OVERVIEW\n\n   There are a number of Web applications that would benefit from being\n   able to request the server to give a byte range of a document. As an\n   example an Adobe PDF viewer needs to be able to access individual\n   pages by byte range; the table that defines those ranges is located\n   at the end of the PDF file.\n\n   An additional, equally important benefit would be for clients to\n   retrieve the rest of a partially retrieved document or image, in the\n   case that the user initially interrupted the connection, but later\n   resumed.\n\n   Setting this standard will promote interoperability between clients,\n   servers and intermediate proxy servers, make (partial) caching\n   effective, and save bandwidth.\n\n   This specification defines only the byte ranges.  It shows other\n   types of ranges as an example of how this specification could be\n   extended, as proof of its generality.  Those examples should not be\n   viewed as their definition.\n\n   This specification is simple enough to be adopted quickly by the\n   server authors/vendors, and be quickly and easily exploited on the\n   client side.  The proposed solution will be backward compatible with\n   existing proxy servers, and once this specification becomes official\n   it will actually be possible to support this in a smart way in proxy\n   servers.\n\n   This specification can be applied to document types for which byte\n   ranges make sense; there are types for which they don't, and this\n   specification is not trying to enforce semantics for byte ranges for\n   them.  In practice most of the data in the Web is represented as a\n   byte stream, and can be addressed with a byte range to retrieve a\n   desired portion of it.  This is especially useful when there is a\n   partial copy of the document, the transfer of which was interrupted\n   by the user, but later resumed, in which case only the missing\n   portion needs to be transferred.\n\n\n\n\nLuotonen, Franks                                                [Page 2]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   Byte range requests are typically generated by software, not written\n   by humans.\n\n\n2. ALLOW-RANGES HTTP RESPONSE HEADER\n\n   The server needs to let the client know that it can support byte\n   ranges.  This is done through the Allow-Ranges HTTP header when a\n   server is returning a document that supports byte ranges:\n\n        Allow-Ranges: bytes\n\n   The server will send this header only for documents for which it will\n   be able to satisfy the byte range request, e.g. for PDF documents, or\n   images, which can be partially reloaded if the user interrupts the\n   page load, and image gets only partially cached.\n\n   Because of the way the byte range request and response are\n   architected, the client is not limited to attempting to use byte\n   ranges only when this header is present.  The Request-Range header is\n   simply ignored by a server that does not support it, and it will send\n   the entire document as a response.\n\n\n3. BYTE RANGE REQUEST\n\n   Byte range request is made like any other HTTP request, with the\n   addition of the Request-Range HTTP Request header.\n\n\n3.1. Request-Range HTTP Request Header\n\n   The client requests a byte range via the Request-Range HTTP header:\n\n        Request-Range: bytes=0-500,5000-\n\n\nThe Request-Range header is defined extensibly so that it can take a\ngeneric parameter specifying the type of range.  The parameter name for\nbyte ranges is \"bytes\".  The syntax of this parameter is described\nbelow.\n\n   The name of the byte range parameter is bytes. It is passed to the\n   server in the Request-Range HTTP request header, followed by an equel\n   sign and the byte range specification.  (In an earlier version of\n   this draft, it was passed to the server appended to the end of the\n   path part of the URL, separated by a semicolon).\n  CGI Applications\n\n\n\nLuotonen, Franks                                                [Page 3]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   As defined by the CGI/1.1 specification, the value of the Request-\n   Range header will be passed to CGI scripts in the HTTP_REQUEST_RANGE\n   environment variable.  The CGI applications can choose to support it\n   if they so desire, and if it is possible.  If the CGI applications do\n   not support it, or if the content they return changes from call to\n   call, they simply ignore the presence of that header, and return the\n   entire document.\n\n\nEach range consists of one or two non-negative integers, separated by a\nhyphen.  The first integer must always be less than or equal to the\nsecond one. One of these integers may be missing, but not both at the\nsame time.  The hyphen is always there, so it is possible to tell which\nnumber is missing.\n\n   If the first number is missing, it means to return the n last bytes\n   of the document, where n is the second number. If n is equal to, or\n   larger than, the size of the document minus one, then the entire file\n   is returned.\n\n   If the second number is missing, it means the end of document.  That\n   is, all the bytes starting from byte n until the end of the document,\n   where n is the first number.\n\n   The first byte in a document is byte number 0.\n\n   If the second number is larger than the size of the document minus\n   one, it is taken to mean the size of the document minus one (that is,\n   the end of the document).\n\n   The range is inclusive; as an example, the range 500-1000 includes\n   bytes from 500 to 1000, including 500 and 1000.\n\n   There may be multiple ranges, separated by a comma. The order of the\n   ranges is the preferred order in which the ranges should be returned.\n\n   In the case that the second integer is smaller than the first one,\n   that particular range is tagged as invalid, and ignored.  If it was\n   the only requested byte range, the entire document is returned.\n   Otherwise the remaining valid ranges will be returned.\n\n   The byte ranges refer to ranges in data as they are transferred over\n   the network (and retrieved by the client). E.g. if in an imaginary\n   system the server stores all lines terminated by CR LF, but turns\n   them into a single LF before sending the data, then byte ranges refer\n   to ranges inside this modified data (the one with single LF line\n   separators). That is, the ranges refer to the data that the client\n   would see.\n\n\n\nLuotonen, Franks                                                [Page 4]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   The byte ranges apply to the \"raw\" data, that is, the data encoded by\n   Content-encoding; but not to the \"armored\" data, that is, the data\n   encoded by content-transfer-encoding.\n\n\n3.1.3. Examples of the Request-Range with the bytes parameter\n\n   The first 500 bytes:\n\n        Request-Range: bytes=0-499\n\n   The second 500 bytes:\n\n        Request-Range: bytes=500-999\n\n   All bytes except for the first 500 until the end of document:\n\n        Request-Range: bytes=500-\n\n   The last 500 bytes of the document:\n\n        Request-Range: bytes=-500\n\n   Two separate ranges:\n\n        Request-Range: bytes=50-99,200-249\n\n   The first 100 bytes, 1000 bytes starting from the byte number 500,\n   and the remainder of the document starting from byte number 4000\n   (byte numbering starts from zero):\n\n        Request-Range: bytes=0-99,500-1499,4000-\n\n   The first 100 bytes, 1000 bytes starting from the byte number 500,\n   and the last 200 bytes of the document:\n\n        Request-Range: bytes=0-99,500-1499,-200\n\n\n3.2. Unless-Modified-Since HTTP request header\n\n   Guaranteeing that individual parts are all up-to-date and in sync\n   with each other is crucial.  This can be made easier by providing a\n   way to tell the server to send the byte range only, if it hasn't\n   changed since the time of the retrieval of the other ranges.  If it\n   has, the entire document is transferred instead.\n\n   The Unless-Modified-Since header will be sent by the client to the\n\n\n\nLuotonen, Franks                                                [Page 5]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   server (or the proxy), carrying the date and time received in the\n   Last-Modified header from the previously received parts.  If at any\n   point the last-modified date or time mismatch is detected by the\n   client, the older parts should be discarded.\n\n   The server will send the requested byte range (as a 206 Partical\n   Content response, as described below) if and only if the document has\n   not changed since that date and time.  If it has, the server will\n   send the entire document to the client instead (as a normal 200\n   response).\n\n   Example:\n\n        Unless-Modified-Since: Wednesday, 15-Nov-95 06:25:15 GMT\n\n\n4. BYTE RANGE HTTP RESPONSE\n\n   The byte range response uses the 206 Partial Content HTTP response\n   status.  Servers and CGI applications not supporting byte ranges will\n   simply ignore the Request-Range header in the request, and return the\n   entire document in a 200 response.\n\n   Existing proxy servers only cache 200 Ok responses.  This way\n   intermediate proxy servers will not mistakenly cache a partial\n   document as if it was the entire document.\n\n   If the request includes multiple ranges, the response is sent back as\n   a multipart MIME message, with content-type multipart/x-byteranges.\n   A server may, but is not required to, send also a single byte range\n   as a multipart message.\n\n   If there are overlapping ranges the behaviour for each range doesn't\n   change. That is, a range will not be truncated, merged, or left out,\n   just because there is an overlap.\n\n   If there was an Unless-Modified-Since header in the request, and the\n   document was modified since that time, the server will send a normal\n   200 Ok response, and transfer the entire document instead.\n\n\n4.2 The Range HTTP Response Header\n\n   The Range HTTP response header is sent back to provide verification\n   and information about the range and total size of the document.  This\n   header can be used by the client to determine which one of the\n   requested ranges is in question.  Syntax:\n\n\n\n\nLuotonen, Franks                                                [Page 6]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n        Range: bytes X-Y/Z\n\n   where:\n\n      X      is the number of the first byte returned (the first byte is\n             byte number zero).\n\n      Y      is the number of the last byte returned (in case of the end of\n             the document this is one smaller than the size of the document\n             in bytes).\n\n      Z      is the total size of the document in bytes.\n\n\nExamples of the Range HTTP Response Header\n\n   The first 500 bytes of a 1234 byte document:\n\n        Range: bytes 0-499/1234\n\n   The second 500 bytes of the same document:\n\n        Range: bytes 500-999/1234\n\n   All bytes until the end of document, except for the first 500 bytes:\n\n        Range: bytes 500-1233/1234\n\n   The last 500 bytes of the same document:\n\n        Range: bytes 734-1233/1234\n\n\nExample of a response:\n\n   HTTP/1.0 206 Partial content\n   Server: Netscape-Communications/2.0\n   Date: Wednesday, 15-Nov-95 06:25:24 GMT\n   Last-modified: Wednesday, 15-Nov-95 04:58:08 GMT\n   Range: 21010-47021/47022\n   Content-length: 26011\n   Content-type: image/gif\n\n\n\n4.3. Multiple Ranges as Multipart MIME Messages\n\n   Multipart MIME is defined in [RFC-1521].  With byteranges, the\n\n\n\nLuotonen, Franks                                                [Page 7]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n   multipart MIME message uses content-type multipart/x-byteranges, with\n   a boundary parameter.\n\n   Example:\n\n       Content-type: multipart/x-byteranges; boundary=THIS_STRING_SEPARATES\n\n       --THIS_STRING_SEPARATES\n       Content-type: application/x-pdf\n       Range: bytes 500-999/8000\n\n       ...the first range...\n       --THIS_STRING_SEPARATES\n       Content-type: application/x-pdf\n       Range: bytes 7000-7999/8000\n\n       ...the second range...\n       --THIS_STRING_SEPARATES--\n\n\n4.4. Caching\n\n   The server must give Last-modified headers for each range request\n   whenever possible, and the client side must take care of having all\n   the fragments in sync. Conditional GET (the GET request with the If-\n   modified-since header) works as expected with byte ranges.\n\n   Ranges can be cached, and if the Last-modified header matches they\n   can be combined.  If a received Last-modified date at any time\n   differs from the ones in the cache, all the cached ranges will be\n   discarded.\n\n   The client side should monitor the Last-modified header value\n   returned by the server, and make sure that all of its individual\n   fragments are in sync. If there are older ones they should be\n   immediately discarded and re-retrieved.\n\n\n5. Future Considerations\n\n\n5.1. Extending the Request-Range and Range headers\n\n   If at some point there will be additional parameters for the\n   Request-Range header, they should be separated by the semicolon\n   character.\n\n   Example:\n\n\n\nLuotonen, Franks                                                [Page 8]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n        Request-Range: param1=bar; param2=xyzzy\n\n   This specification does not define semantics for cases with multiple\n   Request-Range parameters. Future specifications should define\n   semantics for these. Until then, Request-Range headers with\n   parameters that cannot be understood should be ignored.\n\n\n5.2. Other Possible Ranges\n\n   There are other kinds of ranges that can be addressed in a similar\n   fashion; this document does not define them, but both the Request-\n   Range HTTP request header and the Range: HTTP header are defined so\n   that it is possible to extend them.\n\n   As an example, there might be a \"lines\" parameter, with the same kind\n   of range specification, and the Range: header would then specify the\n   numbers in lines. Example:\n\n        GET /dir/foo HTTP/1.0\n        Request-Range: lines=20-30\n\n   The response from a 123 line document would be:\n\n        HTTP/1.0 206 Partial Content\n        Range: lines 20-30/123\n        Last-Modified: ...\n        Content-Length: 773\n        Content-Type: text/plain\n\n   This could be useful for such things as structured text files like\n   address lists or digests of mail and news, but isn't meaningful to\n   such document types as GIF or PDF.\n\n   Other examples might be document format specific ranges, such as\n   chapters:\n\n        GET /dir/foo HTTP/1.0\n        Request-Range: chapters=6-9\n\n        206 Partial Content\n        Range: chapters 6-9/12\n        Last-Modified: ...\n        Content-Length: 36023\n        Content-Type: application/x-book-type\n\n\n\n\n\n\nLuotonen, Franks                                                [Page 9]\n\nBYTE RANGE EXTENSION TO HTTP INTERNET-DRAFT                November 1995\n\n\n6. References\n\n   [RFC-1521] N. Borenstein, N. Freed, \"MIME (Multipurpose Internet Mail\n              Extensions), Part One: Mechanisms for Specifying and\n              Describing the Format of Internet Message Bodies\",\n              RFC 1521, Bellcore, Innosoft, September 1993\n\n   [HTTP]     T. Berners-Lee, R. Fielding, H. Frystyk, \"Hypertext\n              Transfer Protocol -- HTTP/1.0\",\n              draft-ietf-http-v10-spec-04.html, October 14, 1995.\n\n   [CGI]      R. McCool et al, \"Common Gateway Interface -- CGI/1.1\",\n              http://hoohoo.ncsa.uiuc.edu/cgi/, NCSA, 1994.\n\n\n7. Authors' Addresses:\n\n   Ari Luotonen                                       <ari@netscape.com>\n   Netscape Communications Corporation\n   501 E. Middlefield Road\n   Mountain View, CA 94043\n   USA\n\n   John Franks                                       <john@math.nwu.edu>\n   Department of Mathematics\n   Northwestern University\n   Evanston, IL 60208-2730\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuotonen, Franks                                               [Page 10]\n\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "----------\n| From: Larry Masinter  <masinter@parc.xerox.com>\n| Date: Friday, December 16, 1994 10:54AM\n|\n| You don't need content-length to search for the boundary. You can go\n| ahead and scan the binary data for <CR><LF>--boundary--<CR><LF>. Doing\n| so either using a simple scan or using a more elaborate boyer-moore\n| algorithm should be computationally not significantly more expensive\n| than merely counting bytes.\n\nI disagree.  In the case of HTTP, byte counting is *not* doing a \nstrncpy of bytes.  When receiving a chunk of data, you generally give \nthe buffer directly to the network layer which fills in the buffer.  To \nthen have to scan this buffer *does* significantly add a performance \nhit to the server.\n\nSomebody on the www-talk list posted a comparison of a server doing \nsimple document scan before sending a file and just sending a file \n(analagous to the receive case we are talking about).  The server was \ntwo to three times slower when scanning the data.\n\nA server should always provide a Content-length if possible (a wrong \ncontent length is a bug in the server).  The primary case the multipart \nbecomes interesting is when dealing with a streaming gateway (aka, CGI) \nwhere buffering the data can be too expensive.  I think this is the \nonly case doing a boundary scan should be used (I think I'd prefer a \npacket approach with byte count personally, but whatever).\n\nByte counts are good.  Real protocols use byte counts.  Let's make sure \nwe move in that direction.\n\nJohn\n\n\n\n"
        },
        {
            "subject": "Re:  More notes on content negotiatio",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  [a long proposal on content negotiation]\n  [...]\n  > User-agent header based negotiation\n  > -----------------------------------\n  > \n  > One of the requirements I have for HTTP/1.1 content negotiation is\n  > that variant selection based on the contents of the User-Agent header\n  > sent by the user agent is efficient.  In my opinion, the v10-spec-01\n  > negotiation text does not satisfy this requirement.\n  [...]\n  > - The mechanism should allow negotiation based on the User-Agent\n  > header to be efficient.\n  [...]\n\n<purist> I really despise content negotiation based on User-Agent. </purist>\n\nOne problem with such negotiation is it precludes a user's\nenabling/disabling features on the User-Agent.  Suppose someone decided\n(Jeez, I don't know why) to disable tables in tables.  If my User-Agent\nfield says Browser/Version-with-tables-in-tables, this negotiation very\nlikely may decide to send a tables-in-tables version of a resource.\nWhat then?\n\nOTOH, I sat in a meeting this week with a content vendor that uses\nUser-Agent precisely to choose which content to send to a browser.  Not\njust because of incompatibilities, but because there are users with\nolder versions of once-hot browsers that now don't have the latest\nfeatures.  The content vendor wants to send the best possible content\nfor each.  Makes sense from that point of view.\n\nSo, while I dislike the idea of some kind of feature profile, be it a\nbitmap or whatever, that would seem preferable to just going on\nUser-Agent.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: More notes on content negotiatio",
            "content": "Dave Kristol:\n>koen@win.tue.nl (Koen Holtman) wrote:\n>  [a long proposal on content negotiation]\n>  [...]\n>  > - The mechanism should allow negotiation based on the User-Agent\n>  > header to be efficient.\n>  [...]\n>\n><purist> I really despise content negotiation based on\n>User-Agent. </purist>\n\n<me-too>I don't really like it either</me-too>, but I don't think we\ncan live without it: it is the only way to negotiate around user agent\nbugs.\n\n>One problem with such negotiation is it precludes a user's\n>enabling/disabling features on the User-Agent. \n\nNo it does not, it only does if used incorrectly.\n\n> Suppose someone decided\n>(Jeez, I don't know why) to disable tables in tables.  If my User-Agent\n>field says Browser/Version-with-tables-in-tables, this negotiation very\n>likely may decide to send a tables-in-tables version of a resource.\n>What then?\n\nThen the service provider used User-Agent based negotiation when it\nwas not appropriate.\n\nUsing the User-Agent header to determine the value of a user-settable\npart of the user agent profile is of course not possible: User-Agent\nbased negotiation should always be limited to properties that cannot\nbe changed by the user.\n\nIf the user can disable tables-in-tables, we'd better hope that the\n`tables-in-tables enabled' flag can be accessed by the negotiation\nprocess in some way, e.g. because it is in a bitmap of\nuser-configurable browser capabilities sent in an accept header.\n\n>So, while I dislike the idea of some kind of feature profile, be it a\n>bitmap or whatever, that would seem preferable to just going on\n>User-Agent.\n\nProfiles listing some user agent features that cannot be changed by\nusers may reduce the number of cases in which User-Agent based\nnegotiation is needed, they cannot eliminate those cases.\n\n(OK, we could have browsers that automatically download the latest bug\nlist for inclusion in the profile, but that would be even more\nhorrible than User-Agent based negotiation.)\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: New byterange",
            "content": "I've never been able to figure out how the client knows what byte\nrange to ask for in a PDF document. This is just a mystery part of the\nproposal. (Is it some trade secret? My PDF book doesn't say anything\nabout byte ranges that I can find.)\n\nShould the response to a 'range:' request still include a\ncontent-type: header? It better be the same as the content-type of the\nprevious range request, no? In fact, maybe instead of a new 'range:'\nresponse header, you should just say\n\n content-type: application/range-response; range=\"chapters 6-9/12\"\n\nor something, since the content of the actual body of the message\nisn't the type of the whole.\n\n\n>   the fragments in sync. Conditional GET (the GET request with the If-\n>   modified-since header) works as expected with byte ranges.\n\nI think we've established that 'as expected' varies wildly from one\nperson to another. Could you please explain in this document exactly\nhow YOU expect conditional GET to work with byte ranges?\n\n\n\n"
        },
        {
            "subject": "Re: New byterange",
            "content": "On Thu, 16 Nov 1995, Larry Masinter wrote:\n> I've never been able to figure out how the client knows what byte\n> range to ask for in a PDF document. This is just a mystery part of the\n> proposal. (Is it some trade secret? My PDF book doesn't say anything\n> about byte ranges that I can find.)\n\nI suggest that mention of specific media types be stricken from the ID to \navoid confusion.  Larry - I think we can address the general issue of \nsubaddressing at a different time, perhaps in the URI group instead since \nit's more an issue for naming and resolution than it is for HTTP.  How \ndoes an HTTP server know what \"chapters 5-7\" of a PDF file is?  Or \n\"frames 300-350\" of an MPEG movie?  This is a bigger issue.\n\nRight now the biggest win the byte range proposal can give is for \nresumption of interrupted transmissions.  Let's define it with that \ncontext, and the fact that it will be used as a way to obtain a \nparticular part of a larger object concerns me about as much as the fact \nthat people use tables for layout rather than just tabular data.  \n\nWhen we nail the subaddressing problem, I'm sure the PDF folk will be \nvery happy.  \n\n> Should the response to a 'range:' request still include a\n> content-type: header? It better be the same as the content-type of the\n> previous range request, no? In fact, maybe instead of a new 'range:'\n> response header, you should just say\n> \n>  content-type: application/range-response; range=\"chapters 6-9/12\"\n> \n> or something, since the content of the actual body of the message\n> isn't the type of the whole.\n\nOr, we could define that the Content-type in a 406 response maps to the \nContent-type of the whole document the response was extracted from.\n\n> >   the fragments in sync. Conditional GET (the GET request with the If-\n> >   modified-since header) works as expected with byte ranges.\n> \n> I think we've established that 'as expected' varies wildly from one\n> person to another. Could you please explain in this document exactly\n> how YOU expect conditional GET to work with byte ranges?\n\nI would presume he means that the defined semantics for a conditional GET \ndo not change whether a Range: is specified or not.  I would consider it \nhighly unlikely that a conditional GET would include a range, but that \nbehavior (which is the default behavior for servers unaware of the Range: \nheader anyways) seems perfectly valid.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: New byterange",
            "content": "Larry Masinter writes:\n > I've never been able to figure out how the client knows what byte\n > range to ask for in a PDF document. This is just a mystery part of the\n > proposal. (Is it some trade secret? My PDF book doesn't say anything\n > about byte ranges that I can find.)\n > \n\nIt seemed to me that there should be no reference to PDF files in the\nbyte range document if it is going to become part of a standard, as it\nis clear from the discussion on this list that many people don't\nbelieve this is an appropriate use of the the byte-range\nfunctionality.  (Nothing's going to stop anyone from using it this way\n-- it just shouldn't be stated as a reason for the existence of this\npart of the protocol).\n\n\n\n"
        },
        {
            "subject": "keepalives and proxies: a request and a proposa",
            "content": "Hi,\n\nI have a question and a proposal about keepalives.\n\nFrom what I have seen, a client (C) connection to a server(S) sends\n\n    Connection: Keep-Alive\n\nto ask the server to keep the connection alive. When connecting to a\nproxy(P), it sends instead\n\n    Proxy-Connection: Keep-Alive\n\nso that P can either understand it and transform it to\n\n    Connection: Keep-Alive\n\nwhen talking to S, or leave it untouched, in which case S can understand\nthat P cannot handle keepalives. This scheme does not cover the case of\na proxy talking to another proxy, e.g.\n\n    C -------->  P1 ---->  P2  ------> S\n\nPeeking at the list archives I have not seen a solution to the problem.\nHas this already been discussed and/or solved ?\n\nOtherwise, how about the following (which should also make the\ndistinction between \"Connection:\" and \"Proxy-Connection:\" redundant).\n\n    A node (be it a client or a proxy) which likes the connection to\n    stay up, will send\n\nConnection: Keep-Alive my-name\n\n    my-name should be a unique identifier of the requesting node,\n    e.g. the fully qualified hostname or the IP address.  The only\n    requirement, it must be easily verifiable by other nodes (thus\n    it cannot be the Ethernet address).\n\n    A proxy receiving the \"\"Connection:\" header as above can:\n\n* ignore it, or pass it unchanged to the next node;\n* replace \"my-name\" with its own name before sending it\n  to the next node (proxy or server);\n\n    When the reply is sent up to the requesting node, each node in\n    the chain which understands the \"Connection:\" header field will\n    compare the name received as a parameter with the actual name\n    of the previous node (e.g. derived by asking the peer's IP\n    address of the connection). If they match, then the previous\n    node wants the connection to stay up. If they don't, then the\n    connection must be closed.\n\n    This should work for arbitrarily long chains of nodes.\n\nBTW, such an approach can be of general use whenever we want to\nadd backward-compatible options using proxies: compliant nodes just\nreplace the name field, non-compliant ones leave it untouched.\n\nI would appreciate your comments on this solution.\n\nThanks\nLorenzo\n\n<|--------------------------------------------------------------------------|>\n | Lorenzo Vicisano                     | http://www.iet.unipi.it/~vicisano |\n | Dip. di Ingegneria dell'Informazione | e-mail vicisano@iet.unipi.it      |\n | Universita' di Pisa                  | Phone  +39-50-568654              |\n | Via Diotisalvi, 2 56100 PISA, ITALY  | Fax    +39-50-568522              |\n<|--------------------------------------------------------------------------|>\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "I'm all for addition of Proxy-Connection: keep-alive to the spec; more\nspecifically:\n\n - if there are no proxies involved, everything's normal\n - if the client talks to a proxy, it *never* sends a Connection: keep-alive\n - if the client talks to a proxy and wants a persistent connection,\n   it will send Proxy-Connection: keep-alive; the proxy can still\n   respond with Connection: keep-alive\n - a proxy can now independently support Connection: keep-alive\n   between itself and the servers, and itself and the client\n - the proxy will intercept a Connection: keep-alive received from a\n   remote, as it is a reaction to the Connection: keep-alive sent by\n   itself, and should not be forwarded to the client\n - a proxy will generate a new Connection: keep-alive to the client as\n   a response to Proxy-Connection: keep-alive\n\nThis way, there is never a case where a client talking to an old proxy\nserver would fool it to forward the Connection: keep-alive to the\nremote, and then the remote would respond with it, but the proxy would\nclose the connection in between.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "> \n> I'm all for addition of Proxy-Connection: keep-alive to the spec; more\n> specifically:\n> \n>  - if there are no proxies involved, everything's normal\n>  - if the client talks to a proxy, it *never* sends a Connection: keep-alive\n>  - if the client talks to a proxy and wants a persistent connection,\n>    it will send Proxy-Connection: keep-alive; the proxy can still\n>    respond with Connection: keep-alive\n>  - a proxy can now independently support Connection: keep-alive\n>    between itself and the servers, and itself and the client\n>  - the proxy will intercept a Connection: keep-alive received from a\n>    remote, as it is a reaction to the Connection: keep-alive sent by\n>    itself, and should not be forwarded to the client\n>  - a proxy will generate a new Connection: keep-alive to the client as\n>    a response to Proxy-Connection: keep-alive\n> \n> This way, there is never a case where a client talking to an old proxy\n> server would fool it to forward the Connection: keep-alive to the\n> remote, and then the remote would respond with it, but the proxy would\n> close the connection in between.\n\nIt still does not work with the following (chained-Proxies) configuration: \n\n     (C) <--------> (P1) <--------> (P2) <--------> (S)\n      ^     co1      ^      co2      ^      co3      ^\n      |              |               |               |\n     NEW            OLD             NEW              ?\n\n - `C' sends `Proxy-Connection: keep-alive' to `P1' \n - `P1' is not compliant and ignores that header field forwarding\n    it to `P2'\n - `P2', receiving `Proxy-Connection: keep-alive', supposes to talk\n    to a compliant client and decides to keep alive `co2' connection,\n    whereas `P1' is waiting P2 to close connection after its response.\n\nThe `Connection: Keep-Alive my-name' solution allows each compliant\nproxy to realize whether is talking to a compliant proxy/client or not.\n\n\n   Cheers,\n   Lorenzo\n\n\n<|--------------------------------------------------------------------------|>\n | Lorenzo Vicisano                     | http://www.iet.unipi.it/~vicisano |\n | Dip. di Ingegneria dell'Informazione | e-mail vicisano@iet.unipi.it      |\n | Universita' di Pisa                  | Phone  +39-50-568654              |\n | Via Diotisalvi, 2 56100 PISA, ITALY  | Fax    +39-50-568522              |\n<|--------------------------------------------------------------------------|>\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "> It still does not work with the following (chained-Proxies) configuration: \n> \n>      (C) <--------> (P1) <--------> (P2) <--------> (S)\n>       ^     co1      ^      co2      ^      co3      ^\n>       |              |               |               |\n>      NEW            OLD             NEW              ?\n\nI think we can reasonably make it a requirement that proxies in the\nupstream are same revision or newer, or at least it will be easier to\nupgrade those few.  Typical case is a company's/country's main proxy\nvs departmental proxies.  This'll keep the protocol cleaner.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": ">Of course, there is the political problem of getting providers to include\n>this information in their documents.  There is no mention of these parameters\n>in the IMG tag in the HTML 2.0 specification.  I don't know about their\n>status for 3.0.\n\nSpeaking as the HTML WG chair:\nThey're not in the 2.0 spec since they are not really widely used yet.\n\nSpeaking as a commercial browser implementor:\nWe have contacts with a lot of content providers, and all have expressed a\ngreat willingness to include HEIGHT and WIDTH attributes in their content.\n\n--\n\nI've heard another option suggested:  Have the inline image sizes returned\nby the HTTP server as headers when the base document is retrieved.  These\nhints could look something like this:\n\nImage size: myfile.gif  image/gif  400x300  49152\n\nWhere the four components on the line are the [partial] URL, the MIME type,\nthe image dimensions, and the size of the image file in bytes.  Current\nbrowsers could ignore them.  Browsers who make use of them will have to be\nable to cope with their absence.  Cool servers will keep a database of this\ninformation around so that it doesn't have to be looked up on the fly.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\"Can I get a direct flight back to reality, or do I have to change planes\nin Denver?\" - The Santa Clause\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "> I'm all for addition of Proxy-Connection: keep-alive to the spec; more\n> specifically:\n\nIn a word, NO.  It does not adequately take into account the presence\nof hierarchical proxies, nor does it help with gateways.  The only\nway to differentiate communication capabilities is through the \nprotocol name + version number, since that is the only feature\nthat cannot be passed on by a proxy.  Things will improve with HTTP/1.1\nin terms of communication rules, but we are not going to add another\nproxy header hack when it won't be sufficient anyway.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "    The `Connection: Keep-Alive my-name' solution allows each compliant\n    proxy to realize whether is talking to a compliant proxy/client or not.\n\nOne quibble: we should not use a host name here; we should use\nthe IP address of the host (and on a multi-homed host or proxy,\nthe IP address that is actually assigned to the connection).  This\navoid the overhead (and possibly the failure) of an extra DNS lookup\nat each proxy and server.\n\nAside from that, I agree; this is 100% foolproof because every link in\nthe chain must prove that it understands this header.  I don't\nunderstand Roy's comment that it does not adequately account for\nhierarchical proxies or gateways.\n\nRoy sez:\n    The only way to differentiate communication capabilities is through\n    the protocol name + version number, since that is the only feature\n    that cannot be passed on by a proxy.\nThe implication is that the 1.1 protocol spec will say something\nlike \"a client or proxy must not send or forward a Keepalive:\nheader to a proxy [or server?] with a version number below 1.1\".\nIs that what you are planning, Roy?\n\nOne problem that the \"Keepalive: myaddress\" approach seems to\nsolve better than the version-number based approach is that it\nallows a 1.1 proxy to not implement persistent connections.\nThat is, under Roy's approach, persistent connection support\nwould have to be 100% mandatory for all 1.1 (and later) proxies.\nUnder the alternate approach, any proxy not wanting to support\npersistent connections (either for implementation reasons or\nfor policy reasons) could simply drop the \"Keepalive: myaddress\"\nheader.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Philip Thrift: chunked transfer of video data in KeepAlive connection",
            "content": "[ This was probably meant for the list proper -- ange ]\n\n------- Forwarded Message\n\nDate:    Fri, 17 Nov 1995 15:15:08 -0600\nFrom:    Philip Thrift <thrift@osage.csc.ti.com>\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: chunked transfer of video data in Keep-Alive connections\n\n\n\nRegarding the Transfer-Encoding: chunked mechanism, I would like\nto find out if there has been any use of this for transfer of\nvideo streams from the client to the server.\n\nPhilip Thrift\nthrift@ti.com\n\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": ">     The `Connection: Keep-Alive my-name' solution allows each compliant\n>     proxy to realize whether is talking to a compliant proxy/client or not.\n> \n> One quibble: we should not use a host name here; we should use\n> the IP address of the host (and on a multi-homed host or proxy,\n> the IP address that is actually assigned to the connection).  This\n> avoid the overhead (and possibly the failure) of an extra DNS lookup\n> at each proxy and server.\n\nThat won't work either (see below).\n\n> Aside from that, I agree; this is 100% foolproof because every link in\n> the chain must prove that it understands this header.  I don't\n> understand Roy's comment that it does not adequately account for\n> hierarchical proxies or gateways.\n\nI was referring to the Proxy-Connection thingy in Alex's draft, which\nwill not be in HTTP/1.1.  The IP thingy has other problems.\n\n> Roy sez:\n>     The only way to differentiate communication capabilities is through\n>     the protocol name + version number, since that is the only feature\n>     that cannot be passed on by a proxy.\n> The implication is that the 1.1 protocol spec will say something\n> like \"a client or proxy must not send or forward a Keepalive:\n> header to a proxy [or server?] with a version number below 1.1\".\n> Is that what you are planning, Roy?\n\nNo, it says a recipient cannot trust a Connection header received from\nan HTTP/1.0 message.  This can't be replaced by including the IP number\nin the header field because the IP is changed when sent through a tunnel.\nThe only general way to solve this problem is to change to HTTP/1.1.\n\n> One problem that the \"Keepalive: myaddress\" approach seems to\n> solve better than the version-number based approach is that it\n> allows a 1.1 proxy to not implement persistent connections.\n> That is, under Roy's approach, persistent connection support\n> would have to be 100% mandatory for all 1.1 (and later) proxies.\n\nNope -- removal/replacement of the Connection header, and any header fields\nnamed within the Connection header, will be mandatory.\n\n> Under the alternate approach, any proxy not wanting to support\n> persistent connections (either for implementation reasons or\n> for policy reasons) could simply drop the \"Keepalive: myaddress\"\n> header.\n\nPersistent connections are identified by the keep-alive keyword in\nthe Connection header -- the Keep-Alive header is just for optional\nparameters describing the keep-alive.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "Hi,\n\n>     The `Connection: Keep-Alive my-name' solution allows each compliant\n>     proxy to realize whether is talking to a compliant proxy/client or not.\n> \n> One quibble: we should not use a host name here; we should use\n> the IP address of the host (and on a multi-homed host or proxy,\n> the IP address that is actually assigned to the connection).  This\n> avoid the overhead (and possibly the failure) of an extra DNS lookup\n> at each proxy and server.\n\nWe discussed this with Lorenzo. The IP address is probably better, but\nit's just a matter of efficiency. Also,\n   i) the DNS mapping is likely to be already cached locally;\n  ii) all we need is a verifiable identifier which can be valid between\n      the two parties. An address (IP, Decnet, whatever) is better\n      than a name, but it should not be mandatory. Especially, not\n      limited to the IP address family.\n\n> Aside from that, I agree; this is 100% foolproof because every link in\n> the chain must prove that it understands this header.  I don't\n> understand Roy's comment that it does not adequately account for\n> hierarchical proxies or gateways.\n\nNeither do I. The only setting where this cannot work is where the two\nproxies are connected through a tunnel, e.g.\n\nC --- P1 ---- T ---- P2 --- S\n\nand T is passing bidirectional data without interpreting them. How\ncommon is this setting, I don't know. Especially, I don't know if\nit can be useful.\n\n> Roy sez:\n>     The only way to differentiate communication capabilities is through\n>     the protocol name + version number, since that is the only feature\n>     that cannot be passed on by a proxy.\n> The implication is that the 1.1 protocol spec will say something\n> like \"a client or proxy must not send or forward a Keepalive:\n> header to a proxy [or server?] with a version number below 1.1\".\n> Is that what you are planning, Roy?\n> \n> One problem that the \"Keepalive: myaddress\" approach seems to\n> solve better than the version-number based approach is that it\n> allows a 1.1 proxy to not implement persistent connections.\n> That is, under Roy's approach, persistent connection support\n> would have to be 100% mandatory for all 1.1 (and later) proxies.\n> Under the alternate approach, any proxy not wanting to support\n> persistent connections (either for implementation reasons or\n> for policy reasons) could simply drop the \"Keepalive: myaddress\"\n> header.\n\nNot even necessary to drop it. Just pass it downstream, the next\nnode will notice the difference between the addresses and will\ndisable the feature.\n\nAri Luotonen says:\n\n> I think we can reasonably make it a requirement that proxies in the  \n> upstream are same revision or newer, or at least it will be easier to\n> upgrade those few.  Typical case is a company's/country's main proxy\n> vs departmental proxies.  This'll keep the protocol cleaner.\n\nNo, the assumption is completely unreasonable (not only for this\nparticular topic, but every change in the protocol). Each organisation\nuses its own product, and it is hard to force anybody to upgrade.\n\nAs for the cleannes of the protocol: Lorenzo's proposal uses just\na single header field as opposed to the two that are currently\nused.  And the client has to send the same header both to a Server\nand to a Proxy.\n\nAlso I would like to emphasize Lorenzo's last sentence:\n\n> BTW, such an approach can be of general use whenever we want to\n> add backward-compatible options using proxies: compliant nodes just\n> replace the name field, non-compliant ones leave it untouched.\n\nThe idea of adding, next to a header field, the identifier of the node \nasking for that particular feature, is of very general applicability,\nand should be kept in mind when adding new features.\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Dilbert, the Next Generatio",
            "content": "Scott Adams just held a signing session at Kepler's in Menlo Park; this \nbeing Menlo Park, I brought along a copy of the new specs, and as a \nresult, the HTTP-NG architecture spec now sports its own Dogbert \ndedicated to the HTTP working group. Does that mean I have to give Scott \nco-authorship? \n\nSimon\n----\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1) ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "Re: Dilbert, the Next Generatio",
            "content": "On Fri, 17 Nov 1995, Larry Masinter wrote:\n\n> gif! gif!\n\nhttp://sunsite.unc.edu/ses/dogbert.gif\n\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1)  ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "HTTP and Media Servin",
            "content": "Hello All:\n\nI have been following the byte range discussions with much interest.\nWhat I propose is that instead of byte ranges, that a concept like\ntime code be utilized to control media on the web. Traditional time code\nhas the form:\n\n      hours:minutes:seconds:frames\n\nI propose that this be augmented with, for example, the following \ninformation:\n\n      URL:filename:hours:minutes:seconds:frames\n\nThis would allow the HTML creator to determine what frames\nto be played from a video or audio stream. As for text information\nthe author could determine that each page is a frame for example. The\ntext creator would then determine the range of pages to send.\n\nI am proposing the use of time code because I feel the HTTP server\nshould be viewed as a media server as opposed to a byte stream server.\nIf it is considered in this fashion then the HTTP media server coupled \nwith a time code based access mechanism could become much integrated\nwith content creation. For example, content creators could go straight\nfrom non-linear edit systems to the web.\n\nComments ?\n\nWest Suhanic\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "On Fri, 17 Nov 1995, Roy T. Fielding wrote:\n\n> I was referring to the Proxy-Connection thingy in Alex's draft, which\n> will not be in HTTP/1.1.  The IP thingy has other problems.\n> \n> > Roy sez:\n> >     The only way to differentiate communication capabilities is through\n> >     the protocol name + version number, since that is the only feature\n> >     that cannot be passed on by a proxy.\n> > The implication is that the 1.1 protocol spec will say something\n> > like \"a client or proxy must not send or forward a Keepalive:\n> > header to a proxy [or server?] with a version number below 1.1\".\n> > Is that what you are planning, Roy?\n> \n> No, it says a recipient cannot trust a Connection header received from\n> an HTTP/1.0 message.  This can't be replaced by including the IP number\n> in the header field because the IP is changed when sent through a tunnel.\n> The only general way to solve this problem is to change to HTTP/1.1.\n\nI don't know what is the exact definition of a tunnel, but with the\nmost intuitive one (an object which passes data bidirectionally,\nand closes the connection when either side closes), a tunnel is \nintrinsically \"not compliant\" with any protocol. Thus, any technique to \ndistinguish between a tunnel and a proxy running an old version of\nthe protocol can only try to exploit some feature of the old protocol.\n\nAlso, with reference to my previous posting:\n \n> Neither do I. The only setting where this cannot work is where the two\n> proxies are connected through a tunnel, e.g.\n> \n>         C --- P1 ---- T ---- P2 --- S\n>\n> and T is passing bidirectional data without interpreting them. How\n> common is this setting, I don't know. Especially, I don't know if\n> it can be useful.\n\nIt is not that this does not work, it merely cause P1 and P2 to close the \nconnection as if the other side were not compliant (provided that there \nis not another way to tell that there is a tunnel in between).\nIt's just a matter of efficiency, but it does work.\n\n> > One problem that the \"Keepalive: myaddress\" approach seems to\n> > solve better than the version-number based approach is that it\n> > allows a 1.1 proxy to not implement persistent connections.\n> > That is, under Roy's approach, persistent connection support\n> > would have to be 100% mandatory for all 1.1 (and later) proxies.\n> \n> Nope -- removal/replacement of the Connection header, and any header fields\n> named within the Connection header, will be mandatory.\n> \n> > Under the alternate approach, any proxy not wanting to support\n> > persistent connections (either for implementation reasons or\n> > for policy reasons) could simply drop the \"Keepalive: myaddress\"\n> > header.\n> \n> Persistent connections are identified by the keep-alive keyword in\n> the Connection header -- the Keep-Alive header is just for optional\n> parameters describing the keep-alive.\n\nI am under the impression that Jeff uses \"Keepalive: myaddress\" as a \nsynonym to \"Connection: Keep-alive my-addr\" header, which should be \ninterpreted as \"keep-alive the connection to my-addr, if you can\" \n\nLuigi\n\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": ">>>>> \"Dave\" == Dave Kristol <dmk@allegra.att.com> writes:\n\n    Dave> My tastes (obviously) run to a more evolutionary approach\n    Dave> for HTTP.  I'm unconvinced that the performance problems\n    Dave> require a flash cut to a binary protocol.  Spero has shown\n    Dave> that doing multiple transactions over one connection\n    Dave> achieves signficant performance improvements.  His response\n    Dave> is to change HTTP drastically.  Mine is to do so within the\n    Dave> current overall design.\n\nWell, there's always my other response :-) One of the things I talked about\nwith Alex at the IETF was making a slight change to the SESSION proposal. \nThe idea is to use the SESSION method to switch the connection over to \nrunning SCP, and then use SCP to manage multiple sequential HTTP 1.0 \ntransactions. It turns out that this technique allows even more code to be\nreused than using MIME multipart, and provides a very obvious transition\npath to full HTTP-NG. \n\nOne other thing that was discussed was the relative advantages of using a \nsession method vs. an ignorable header. It turns out that there is a problem\nwith using ignorable headers when proxies are used - if a proxy which doesn't\ninterpret the header is used to talk to a server which does handle the header,\nthe connection can become deadlocked (the end server things that the \nproxy doesn't want it to drop the connection, whilst the proxy is sitting there\nwaiting for the connection to drop). \n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": ">I have been following the byte range discussions with much interest.\n>What I propose is that instead of byte ranges, that a concept like\n>time code be utilized to control media on the web. Traditional time code\n>has the form:\n\nWhat we really need (to repeat), is a general addressing mechanism\nsimilar to that offered by HyTime. If we continually reinvent\naddressing schemes with different syntaxes, it makes supporting all\nthe various syntaxes a real headache. One general (and extensible)\nsyntax is far more preferrable. \n\nNot that I am arguing for adoption of the syntax... \n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": ">> No, it says a recipient cannot trust a Connection header received from\n>> an HTTP/1.0 message.  This can't be replaced by including the IP number\n>> in the header field because the IP is changed when sent through a tunnel.\n>> The only general way to solve this problem is to change to HTTP/1.1.\n> \n> I don't know what is the exact definition of a tunnel, but with the\n> most intuitive one (an object which passes data bidirectionally,\n> and closes the connection when either side closes), a tunnel is \n> intrinsically \"not compliant\" with any protocol. Thus, any technique to \n> distinguish between a tunnel and a proxy running an old version of\n> the protocol can only try to exploit some feature of the old protocol.\n\nTunnel is defined in the lastest HTTP/1.0 specification (it is one of\nthe many definitions I added to explain some of the characteristics\nof HTTP communication that is often ignored by implementors).\nUse of a tunnel is compliant with HTTP, but requires that the HTTP\nsemantics not be changed by the presence of a tunnel.\n\nSuffice it to say that IP addresses exist at (or below) the transport\nlevel, and HTTP exists at the application level.  Using IP numbers\n(or even hostnames) to define application-level behavior is wrong\nbecause it won't work when the transport layer changes.  Therefore,\nI try to avoid including them in the protocol when possible.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "On Sat, 18 Nov 1995, Roy T. Fielding wrote:\n\n> Tunnel is defined in the lastest HTTP/1.0 specification (it is one of\n> the many definitions I added to explain some of the characteristics\n> of HTTP communication that is often ignored by implementors).\n> Use of a tunnel is compliant with HTTP, but requires that the HTTP\n> semantics not be changed by the presence of a tunnel.\n\nI'll look at the definition of tunnels before commenting more on this.\n\n> Suffice it to say that IP addresses exist at (or below) the transport\n> level, and HTTP exists at the application level.  Using IP numbers\n> (or even hostnames) to define application-level behavior is wrong\n> because it won't work when the transport layer changes.  Therefore,\n\nIn the original message, and in further postings, Lorenzo and I\ntalked about \"unique identifiers\" of the node (client, proxy or\nserver), not IP addresses. I agree that addresses exist at the\ntransport level, but there must exist some other form of unique\nidentifier that is understood at the application level. Just use\nthat.\n\n> I try to avoid including them in the protocol when possible.\n\nI fully agree on this point; this is something that probably hasn't\nreceived much attention in the past.  Note, though, that at times the\ndistinction between application and transport is very difficult. As an\nexample, URLs can include IP addresses, such as\n\n    http://131.114.9.237/~luigi/\n\nand all browsers will happily find the requested info.\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "Luigi Rizzo writes:\n> On Sat, 18 Nov 1995, Roy T. Fielding wrote:\n> \n> > Tunnel is defined in the lastest HTTP/1.0 specification (it is one of\n> > the many definitions I added to explain some of the characteristics\n> > of HTTP communication that is often ignored by implementors).\n> > Use of a tunnel is compliant with HTTP, but requires that the HTTP\n> > semantics not be changed by the presence of a tunnel.\nHmm. The word tunnel mean for me IP tunneled trough IPX or vice versa.\n(One transport protocol tunneled trough another.) Not better to name\napplication level bridge?\n> I'll look at the definition of tunnels before commenting more on this.\n> \n> > Suffice it to say that IP addresses exist at (or below) the transport\n> > level, and HTTP exists at the application level.  Using IP numbers\n> > (or even hostnames) to define application-level behavior is wrong\n> > because it won't work when the transport layer changes.  Therefore,\n> \n> In the original message, and in further postings, Lorenzo and I\n> talked about \"unique identifiers\" of the node (client, proxy or\n> server), not IP addresses. I agree that addresses exist at the\n> transport level, but there must exist some other form of unique\n> identifier that is understood at the application level. Just use\n> that.\nI propose using {transport-protocol,node-name or address} pairs as unique\nidentifiers.\nI guess Novell later or sooner will define http over SPX as did with\nsmtp/spx and telnet/spx.\nAs I am informed, Novell operates an IPX network number registry, which\ncan be extended (on the technical side with some appropriate extensions\nto NDS) to operate like internet address registries.\nAn other argument that Novell plans (or already does) world-wide scalability\ntests of NDS.\nAnd the HTTP draft speaks about TCP/IP implementations of HTTP, but not\nexcludes implementations over other transport protocols.\nAs I see, support for other protocols needs some change in URL syntax too.\n(the \"//\" host [:port] part should be renamed to network part and extended to\nsupport multiple transport protocols having multiple node naming schemes).\nI don't know yet, how the IPv4 -> IPv6 transition will be made, but that may\nneed some distiction, because the first needs a name to IN A lookup while the\nsecond needs name to IN AAAA lookup.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "Gavin Nicol writes:\n > >I have been following the byte range discussions with much interest.\n > >What I propose is that instead of byte ranges, that a concept like\n > >time code be utilized to control media on the web. Traditional time code\n > >has the form:\n > \n > What we really need (to repeat), is a general addressing mechanism\n > similar to that offered by HyTime. If we continually reinvent\n > addressing schemes with different syntaxes, it makes supporting all\n > the various syntaxes a real headache. One general (and extensible)\n > syntax is far more preferrable. \n > \n > Not that I am arguing for adoption of the syntax... \n\n\nIt seems there at least a couple of questions that can be addressed separately:\n\n1. Should there be one uniform syntax for addressing sub-parts of\n\"structured resources\" independently of the content-type?  \n\n2. Should this addressing scheme go into URLs or elsewhere?\n\nOn (1), I find it hard to imagine that it is possible to predict in\nadvance what kinds of parameters, ranges, subspaces, sets, regions,\nchapters, verses, etc. are going to be needed to describe this in a\nworld of increasing numbers of content-types, so unless someone can\ndemonstrate a sufficiently general design, I suspect this is\nimpractical.\n\nOn (2), since there is little guarantee of an association between a\nURL and any particular data type, and decreasingly so a\ncontent-negotiated world, I think it is a mistake to tie sub-part\naddressing into URLs.  It would seem that requests for sub-part\naddressing would have to be tied to specific content-types in the\nrequest.\n\n\n\n"
        },
        {
            "subject": "Re: keepalives and proxies: a request and a proposa",
            "content": "> Tunnel is defined in the lastest HTTP/1.0 specification (it is one of\n> the many definitions I added to explain some of the characteristics\n> of HTTP communication that is often ignored by implementors).\n> Use of a tunnel is compliant with HTTP, but requires that the HTTP\n> semantics not be changed by the presence of a tunnel.\n\nrom your document\n\nhttp://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v10-spec-04.ps.gz\n\n(btw, on http://www.ics.uci.edu/pub/ietf/http/, the link to HTTP/1.1\nspec points to an older version than those of HTTP/1.1)\n\ntunnel\n\nA tunnel is an intermediary program which is acting as a\nblind relay between two connections. Once active, a tunnel\nis not considered a party to the HTTP communication, though\nthe tunnel may have been initiated by an HTTP request. A\ntunnel is closed when both ends of the relayed connections\nare closed. Tunnels are used when a portal is necessary\nand the intermediary cannot, or should not, interpret the\nrelayed communication.\n\nI think the sentence \"A tunnel is closed...\" is ambiguous. When\neither side of the tunnel closes the connection, I expect the tunnel\nto close the connection on the other side to propagate the change\nof state. I would change the sentence it to something like\n\nWhen the connection on either side of the tunnel is closed, the\ntunnel closes the connection on the other side. The tunnel\nceases to exist when both ends of the relayed connections\nare closed.\n\nWith this interpretation, and given that the tunnel is a blind relay,\na party cannot tell the presence of a tunnel unless the other party\nclearly identifies itself in a way that is distinguishable (e.g. by\nspecifying the transport address they use for the connection).\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": ">1. Should there be one uniform syntax for addressing sub-parts of\n>\"structured resources\" independently of the content-type?\n \nYes, which is precisely what HyTime does. I can do the same thing in\nLISP. I cannot imagine a way to define a general adressing scheme\nwithout also making the addresses almost impossible to remember\nthough... in an ideal world, one would not see most of these links \"in\nthe raw\" anyway.\n\n>On (2), since there is little guarantee of an association between a\n>URL and any particular data type, and decreasingly so a\n>content-negotiated world, I think it is a mistake to tie sub-part\n>addressing into URLs. \n\nI tend to agree. \n\n\n\n"
        },
        {
            "subject": "ReallyFrom: allowing for proxying problems",
            "content": "Hi All,\n\nHere's a suggestion to help out the problem with proxy accesses all coming\nfrom the same address?\n\nIf the request included an Entity-Header, Really-From, then the proxy\ncould pass this information on to the server which would be more meaningful\nthan the proxy's own address. So:\n\nIf \"Really-From: machine.name\" exists then \"machine.name\" is used in preference\nto \"proxy.machine\". Admittedly this allows unscrupulous individuals to spoof\nwhere they have come from so perhaps the header should be \"Supposedly-From\".\n\nWhat do you think?\n\nLee\n\n------------------------------------\nLee M J Stephens - lee@info.bt.co.uk\nBT Laboratories  - (01473) 641653\n------------------------------------\n\n\n\n"
        },
        {
            "subject": "no &quot;conforming&quot; HTTP/1.",
            "content": "Now, after all the hard work that has gone into an HTTP/1.0\nspecification, it occurs to me, ironically, that it will never matter\nwhether there are \"conforming\" HTTP/1.0 specifications.  (BTW, in no\nway do I mean to denigrate all the work!)  The reason is that a program\ncould never tell whether it's talking to a conforming implementation,\nbecause all the non-conforming implementations we've been using over\nthe past few years claim to be HTTP/1.0.\n\nOn to HTTP/1.1!\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: ReallyFrom: allowing for proxying problems",
            "content": "On Tue, 21 Nov 1995, Lee Stephens wrote:\n\n> Hi All,\n> \n> Here's a suggestion to help out the problem with proxy accesses all coming\n> from the same address?\n> \n> If the request included an Entity-Header, Really-From, then the proxy\n> could pass this information on to the server which would be more meaningful\n> than the proxy's own address. So:\n> \n> If \"Really-From: machine.name\" exists then \"machine.name\" is used in preference\n> to \"proxy.machine\". Admittedly this allows unscrupulous individuals to spoof\n> where they have come from so perhaps the header should be \"Supposedly-From\".\n> \n> What do you think?\n\nMmmm...\"Apparently-From\" might sound a little less sarcastic :)\n\nLiam\n--\n Liam Relihan,             PGP aware - see homepage     Voice: +353-61-335322\n Piercom Ltd., International   [     space    ]           Fax: +353-61-335051\n Business Centre, Plassey      [    for rent  ]    E-mail: liam.relihan@ul.ie\n Tech. Park, Limerick, Ireland   http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html\n\n\n\n"
        },
        {
            "subject": "Re: New byterange",
            "content": "For historic/political/MIME reasons, I would like to replace the names\n\n    Request-Range:     with   Range:          (on the request)\n\nand\n\n    Range:             with   Content-Range:  (on the response)\n\nPlease let me know ASAP if this will cause any brains to explode.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": ">>>>> \"John\" == John Franks <john@math.nwu.edu> writes:\n\n\n    John> Correct me if I am wrong, but I concluded from Spero's\n    John> postings that nothing currently proposed including MGET,\n    John> hold-open, or even HTTP-NG would improve (or even match?)\n    John> the user's perceived performance currently given by\n    John> Netscape.  By this I mean the ellapsed time until the user\n    John> can start reading *all the text* and the ellapsed time until\n    John> the user can jump to a new link.\n\nThe perceived performance of Netscape is merely a function of their\nrendering order. Netscape renders GIFS as they come in. THis is completely\nindependent of the protocol. The new parser for Arena is based on a \nrestartable model- it will render everything that it possibly can as it \ncomes in - I haven't seen Dave's latest design, but I think it can even start\nrendering GIFS before it's finished retrieving the text. \n\nThe netscape approach of stealing other peoples bandwidth is\nultimately a negative sum game. If more and more people start using\nthis approach, not only will the netscape user not see any benefit,\nbut *EVERYBODY* will lose as more and more bandwidth becomes lost to\nretransmission.\n\nIf I wanted to get better performance by stealing bandwidth, I'd just\nwrite my own transport protocol.  Just run blast datagrams out as UDP\nas fast as your interface can handle it, and ack every single\nmessage. Hell, use a regenerative protocol and send everything\nmultiple times. You'll clear every TCP user off the routers in your\npath and get the backbone for yourself. \n\nSimon\n\n\n\n"
        },
        {
            "subject": "Keepalives and proxies (hopefully solved",
            "content": "Probably we have found a solution to the keepalive problem in\npresence of cascaded proxies and tunnels. From his messages, it is\nvery likely that Roy already had this solution in mind (or has\npossibly posted it); we cannot check because are currently disconnected\nfrom both his mind and the mailing list archive.\n\n** in the Request-Line header a node specifies the protocol it is\n   speaking. The receiving node (unless it is a tunnel) is always able\n   to understand it and must change the \"HTTP-Version\" according to\n   what it can understand. [we haven't checked if pre-1.0 proxies\n   actually behave this way]\n\n   Thus, any node knows the protocol spoken by the previous one in the\n   request chain.\n\n** a \"Connection: keepalive\" (possibly with other flags, but\n   *without* the node ID) can only be generated by nodes speaking\n   1.1 and greater versions of the protocol.\n   Nodes speaking 1.0 or earlier versions of http *should not*\n   generate such an header, as it is not in the spec.\n   Unfortunately some do...\n   following node must not honor the request).\n\n** A 1.0 node receiving the \"Connection: ...\" header should just ignore\n   it and pass it down the link.\n\n** A 1.1 node receiving the \"Connection: ...\" header should parse it,\n   determine if it comes from a 1.1-capable node, and possibly\n   honor it (it should not be mandatory, though).\n\n   If the header comes from a 1.0 node, the request *must not* be\n   honored.\n\n** A 1.1 node can autonomously decide to pass, insert or delete a\n   \"Connection: ...\" header according to its own policies.\n\n** A tunnel receiving the \"Connection:... \" header, just passes it down\n   the link unchanged.\n\nNote the difference between this approach and the \"Connection:\nkeepalive my-id\" one. The former only works because of a difference\nin the protocol version, the latter can be used to negotiate options\neven without changing the protocol version, but is not applicable in\npresence of tunnels.\n\nNote also that the two approaches can coexist, i.e. 1.0 nodes could\nsend a \"Connection: keepalive my-id\", and 1.1 nodes could try to\nhonor the request if the peer matches the id specified in the\nrequest. In the spirit of the \"robustness principle\" (RFC793,\nsec.2.10):\n\n    be conservative in what you do, be liberal in what you accept\n    from others\n\nit wouldn't be a bad idea to include an optional id field in the\n\"Connection: header\".\n\nLorenzo and Luigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Which Doc is for which HTTP version",
            "content": "  I went out to http://www.ics.uci.edu/pub/ietf/http/ and downloaded the\n  postscript versions of the HTTP protocol documents. The link under the\n  \"Hypertext Transfer Protocol -- HTTP/1.0\" is to draft-ietf-http-v10-spec-04\n  and the one under \"Hypertext Transfer Protocol -- HTTP/1.1\" points at\n  draft-ietf-http-v10-spec-01.\n\n  I don't want to understand how that naming convention works :-) but I \n  do want to know:\n    1. Why both documents are titled HTTP/1.0?\n    2. Why the document, supposedly for HTTP/1.1, is dated August\n       and the HTTP/1.0 is dated October?\n    3. Are these the correct documents?\n    4. Is PUT part of the HTTP/1.1 spec?\n\n  I'm just confused,\n  Brad\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 draft 00 is availabl",
            "content": "The text and postscript versions are now available at\n\n     http://www.ics.uci.edu/pub/ietf/http/\n      ftp://www.ics.uci.edu/pub/ietf/http/\n\nand have been submitted to the IETF.  Egads, 80 pages in postscript.\n\nI'm going to sleep now -- I'll create an HTML version on Friday.\n\nI am disappointed with the amount of stuff I didn't get finished,\nbut at least it doesn't contain so many contradictions any more and\nmost of the ideas are in place.  Koen -- I do intend to improve the\ncontent negotiation sections, but I ran out of time.\n\nI don't recommend reading it on a full stomach (or an empty one),\nso us Yanks may want to wait until Monday anyway.\n\nCheers,\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Proxies and loop",
            "content": "Proxies share many problems with routers. Among these, a catastrophic\nevent is the creation of a loop of proxies, originating from the\nmisconfiguration of one of more of the members of the loop.\n\nWhile the configuration of proxies does not really belong in the\nHTTP protocol, it helps a lot having a way to detect the existence\nof loops, or at least minimising its bad effects.\n\nIn this particular case, the ability to identify and break loops\nis of fundamental importance because even a single misconfigured\nproxy can create large loops and completely disrupt service on the\nproxies which are part of the loop.\n\nNote that it is very easy to break service on a given proxy: the\nmisconfigured proxy, A, only needs to send a request to a proxy\n(P) specifying an URI on A itself. With the complicity of an\nintermediate node A' on the loop P can loose any possibility of\ndetecting the existence of the loop.\n\n           +--> A --(A')---> P ----+\n   |                       |\n   +------------<----------+\n\nNow, while it is true that a program can easily be written to induce\nsuch a behaviour, my point is that the above scenario can be\nintroduced too easily because of misconfiguration of a node running\nstandard code.\n\nInvoking the \"robustness principle\", and similarly to IP, I propose\nthe addition of a Time-To-Live field somewhere in the *compulsory*\nheaders for the request. A node receiving a request must decrement\nthe TTL and can only forward it if the TTL is >0. If the TTL goes to 0,\nthe node should reply with an error message to help the location of the\nproblem.\n\nThe TTL can either be a separate, compulsory header field, or appear\nas an additional parameter in the Request-Line, perhaps right after\nthe protocol version.\n\nA note: a malicious node A, can create an artificial loop even with\na TTL field, and even if an authentication mechanism is used to\nallow a proxy to accept and forward requests (although, in this\ncase the malicious process must first authenticate itself). Actually,\nit is not even necessary that data are actually flowing on the\nconnection, it suffices that enough requests are sent to P while\nkeeping it waiting for responses: the node running P will quickly\nhit against some limit (file descriptors, process table entries,\nmax concurrent requests...). An alternative approach, namely the\naddition, in the request, of a \"history\" of already visited nodes,\ndoes not help either.\n\nA \"robust\" proxy should certainly check for requests coming from\na single node, and avoid allocating a significant fraction of its\nresource to it.\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> While the configuration of proxies does not really belong in the\n> HTTP protocol, it helps a lot having a way to detect the existence\n> of loops, or at least minimising its bad effects.\n\nThe Forwarded header field was created for this purpose.\nIs it insufficient?\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 draft 00 : chunked transfer and proxie",
            "content": "Regarding the use of the \"chunked\" coding for persistent connections,\nI am confused how this will work if the proxy must remove this\nencoding according to the language below.\n\nrom the draft:\n> C.5  Introduction of Transfer-Encoding\n> \n>    HTTP/1.1 introduces the Transfer-Encoding header field \n>    (Section 10.39). Proxies/gateways must remove any transfer coding \n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>    prior to forwarding a message via a MIME-compliant protocol. The \n>    process for decoding the \"chunked\" transfer coding (Section 3.6) \n>    can be represented in pseudo-code as:\n\nHere the chunked transfer represents a single, potentially unbounded\nPOST request and the server can be concurrently transmitting response\ndata during the request.\n\n\nPhilip Thrift\nthrift@ti.com\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 draft 00 : chunked transfer and proxie",
            "content": "On Sat, 25 Nov 1995, Philip Thrift wrote:\n\n> Regarding the use of the \"chunked\" coding for persistent connections,\n> I am confused how this will work if the proxy must remove this\n> encoding according to the language below.\n\nI think you're misreading the line following the one you indicated:\n\n> >    (Section 10.39). Proxies/gateways must remove any transfer coding \n>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> >    prior to forwarding a message via a MIME-compliant protocol. The \n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRemember, HTTP is not a MIME-compliant protocol. But if you have a gateway\nthat's translating HTTP to, say, email, you need to \"remove any transfer\ncoding\", because MIME email agents won't know what to do with it.  That's\nall that line is saying. If you've an HTTP proxy through and through, \nthen there's no need whatsoever to remove the encoding.\n\nOf course, I may be wrong. But that's my take on that sentance.\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 draft 00 : chunked transfer and proxie",
            "content": "> I think you're misreading the line following the one you indicated:\n> \n>> >    (Section 10.39). Proxies/gateways must remove any transfer coding \n>>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n>> >    prior to forwarding a message via a MIME-compliant protocol. The \n>                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> \n> Remember, HTTP is not a MIME-compliant protocol. But if you have a gateway\n> that's translating HTTP to, say, email, you need to \"remove any transfer\n> coding\", because MIME email agents won't know what to do with it.  That's\n> all that line is saying. If you've an HTTP proxy through and through, \n> then there's no need whatsoever to remove the encoding.\n> \n> Of course, I may be wrong. But that's my take on that sentance.\n\nAnd it is exactly the right take -- that is what Appendix C is all about.\n\n......Roy\n\n\n\n"
        },
        {
            "subject": "gag me with a spoo",
            "content": "Well, first our server crashed for two days (hard disk meltdown)\nand then I had the wrong links at the archive page for the 1.1 draft...\nthis just isn't my week!  Fortunately, it is over.\n\n......Roy  (mmmmm, time for a second thanksgiving dinner ...\n            more pumpkin pie for dessert tonight)\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> > While the configuration of proxies does not really belong in the\n> > HTTP protocol, it helps a lot having a way to detect the existence\n> > of loops, or at least minimising its bad effects.\n> \n> The Forwarded header field was created for this purpose.\n> Is it insufficient?\n\nFinally I could get a copy of the 1.1 draft.\n\nThe Forwarded: header can actually be used to detect loops. However,\n\n* it is not compulsory (although a node willing to avoid loops will\n  certainly insert it and detect the loop);\n* in principle, it does not prevent the occurrence of arbitrarily\n  long paths (although a node can decide not to pass a request which\n  has been \"Forwarded:\" too many times);\n\nand these reasons make me like better the use of a *compulsory*\nTTL field as a loop detector (which is also simpler to manage).\n\nThe \"Forwarded:\" header was inspired from email, but email loops\nare different from http loops. The former are built one step at a\ntime, in a store-and-forward fashion:  it might take hours before\na loop is complete and detected, during this time the system does\nnot allocate resources other than the storage for a single copy of\nthe message.  But http allocates resources on all nodes, so loops\nare more expensive for the network.\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "request identifiers in HTT",
            "content": "One thing that I'd like to see in the protocol is a request identifier\nin the response headers, that would allow to match responses with\nrequests.\n\nThis would have many applications, such as enable a client to send\na batch of requests to a server, and receive the responses out of\norder (in order to minimize the overall response time). Ultimately,\nthis would even allow to separate requests and responses, or (in\nmany cases) use a connectionless transport for HTTP.\n\nHas this subject been discussed before ?\n\n        Luigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "Hi Again:\n\nI think the central issue here is where the concept of media\nis handled. If the HTTP server is considered as a media server\nthen it must have the facilities to deal with media. Time code\nis the language of the professional media creators. Therefore \ngiven the web's ever increasing consumption of media it would\nhave to deal with time code. However if the server is still\nviewed as a byte stream server then this would push the\nresponsibilty for dealing with  media out to the clients; ie,\nbrowsers and any tools they use.\n\nEither way there has to be a richer media control mechanism\nthan byte ranges. I think time code is it. For me the important\nissue becomes which end handles it.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "    NetScape themselves provide a solution in the form of WIDTH and\n    HEIGHT attributes to the IMG tag - if used everywhere the document\n    can be laid out completely before any images retreived.  I'd be\n    against having authoring tools or the author themselves put this\n    information in, but if we go down the path of having servers \"do\n    something\" to the HTML they serve before sending it to the client\n    (a la server-side includes, something we use a lot here) then\n    having servers add those attributes to those tags isn't such a bad\n    idea (servers can cache these translations anyways).\n\n    I guess the big question is - do we solve this problem in HTML or\n    HTTP?  Or both?\n\nWell, I assume you are proposing (hypothetically, anyway) that the \nserver rewrite the HTML it's transmitting to add the WIDTH and HEIGHT\nattributes.  This means that the server has to not only know how to\nparse the image files to discover their bounding boxes (which it would\nalso have to do with my proposed \"GET_BOUNDING_BOXES\" method), it has\nto know how to rewrite the HTML.\n\nTo me, this seems like a lot more effort, since it means\n(1) Parsing every HTML file (at least once), even though\nmany do not contain images.\n(2) Rewriting the HTML file on every retrieval (or keeping\na separate cached copy), even though the client may already\nhave the images cached, and even though the client may not\nbe able to use the bounding box info.\n(2) parsing the entire HTML file, instead of just the\nfirst few bytes of the image files, since the images may\nappear anywhere in an HTML file.\n(2) getting the parse right.  For example, an HTML file\ndescribing HTML might have the \"IMG\" token appear as text,\nrather than a tag.\n\nI would rather see the clients drive the choice of when to do work\nto get the bounding boxes, since the clients know which ones they\nneed and when.  It also avoids the need for the servers to understand\nHTML (unless they implement the GETALL method we played with this\nsummer).\n\nOn the other hand, if Netscape succeeds in selling everyone clients\nand servers that understand and generate HEIGHT and WIDTH tags, then\nthis may all be moot.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": ">        Either way there has to be a richer media control mechanism\n>        than byte ranges. I think time code is it. For me the important\n>        issue becomes which end handles it.\n\nThe only problem with having it in the client is that it requires\nevery client to understand the data format. What happens when a new\ndata format is invented, or when a client wants frame 1 of an MPEG\nmovie in GIF form, because it doesn't have the code locally for\nhandling MPEG?\n\n\n\n"
        },
        {
            "subject": "ANNC: Draft of &quot;PEP: An Extension Mechanism for HTTP&quot; avail",
            "content": "Hello --\n\n\"PEP: An Extension Mechanism for HTTP\" was submitted to the Internet-Drafts\neditor last week for discussion in Dallas.\n\nIt is also available as a W3C Working Draft:\nhttp://www.w3.org/pub/WWW/TR/WD-http-pep.html\n\nComments either to the author (khare@w3.org) or to this list.\n\nThanks,\nRohit Khare\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v11-spec00.txt, .p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.1                 \n       Author(s) : R. Fielding, H. Nielsen, T. Berners-Lee\n       Filename  : draft-ietf-http-v11-spec-00.txt, .ps\n       Pages     : 75\n       Date      : 11/22/1995\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol for\ndistributed, collaborative, hypermedia information systems. It is a \ngeneric, stateless, object-oriented protocol which can be used for many \ntasks, such as name servers and distributed object management systems, \nthrough extension of its request methods (commands). A feature of HTTP is \nthe typing and negotiation of data representation, allowing systems to be \nbuilt independently of the data being transferred.       \n                  \nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification defines the protocol referred to as \n\"HTTP/1.1\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v11-spec-00.txt\".\n Or \n     \"get draft-ietf-http-v11-spec-00.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v11-spec-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-00.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v11-spec-00.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19951122190819.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-v11-spec-00.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-v11-spec-00.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19951122190819.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "lists\nhelp\nend\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "I think the central issue here is where the concept of media\nis handled. If the HTTP server is considered as a media server\nthen it must have the facilities to deal with media.\n\nWe've had this discussion at least once before.  I firmly believe\nthat HTTP should NOT be used for real-time continuous media.  The\nURL mechanism allows us to include multiple transport protocols\n(e.g., HTTP, FTP, Gopher) in the web, and if we want to include\nreal-time continuous, then we should use a protocol optimized for\nthat.  We should not try to turn HTTP into a kitchen-sink protocol,\nmaking it into a second-rate media protocol while also making it\nharder to implement.\n\nAs a purely practical matter, this working group is chartered to\nwork on IETF standards, which normally require \"rough consensus\nand working code\" to progress.  We would be in relatively uncharted\nterritory when it comes to real-time continuous media, which is\nstill the subject of active research and debate.   It would be\nquite premature to try to standardize this kind of thing, especially\nin the context of the most heavily-used protocol protocol in today's\nInternet.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "    The Forwarded: header can actually be used to detect loops. However,\n    \n    * it is not compulsory (although a node willing to avoid loops will\n      certainly insert it and detect the loop);\n    * in principle, it does not prevent the occurrence of arbitrarily\n      long paths (although a node can decide not to pass a request which\n      has been \"Forwarded:\" too many times);\n    \n    and these reasons make me like better the use of a *compulsory*\n    TTL field as a loop detector (which is also simpler to manage).\n\nWhen I first read your proposal for a TTL field, I had the same\nreaction that Roy seems to have had: this isn't really necessary.\nThe Forwarded: header allows any proxy to detect loops through\nitself, and can support that task without being mandatory.\n\nAnd do we have any evidence that forwarding loops are a real problem?\nRouting loops occur because our routing protocols are automatic and\ndynamic, and so can do stupid things rapidly and for transient periods.\nHTTP forwarding loops, on the other hand, would be created by humans\nand should thus be both less frequent and less transient.\n\nI'm also not that worried about arbitrarily long forwarding paths\n(what's the harm?), and I would be worried about\n    o   the danger that some browser would set the TTL too short,\nthus causing hard-to-debug service problems\n    othe overhead of managing yet another mandatory header field on\n        every request, just to avoid a very rare problem.\n\nHowever: after I thought about this some more, I realized that there\nis an entirely different reason to include a TTL header in HTTP.\n\nThere are two main uses for the TTL field in IP headers: first,\nto avoid routing loops and long-delayed packets.  Second, to make\n\"traceroute\" work.  I would bet that most TTL \"failures\" in today's\nInternet are from traceroute users, not routing loops.\n\nTraceroute has proved to be an essential tool in debugging IP-level\nproblems.  We ought to be thinking about providing analogous debugging\ntools at the HTTP level.  For example, \"how come the users on my\nLAN are having trouble reaching server www.xxx.com?\"  It would be\nreally nice to have a \"trace-http-path\" program, like traceroute\nbut displaying the HTTP-level forwarding path instead of the IP-level\nrouting path.\n\nA TTL header in HTTP would make that quite easy.  I would NOT make\nthis a mandatory header; that is, it would not be set on most requests.\nThus we would avoid imposing additional overhead on normal requests.\nHowever, I would make it mandatory for proxies and servers to\nhonor and process it.  (Since HTTP 1.0 proxies would not understand it,\nthey would not appear in trace-http-path displays, but HTTP 1.0 servers\nwould respond to the attached GET method and so one could see the end\npoint of any path.)\n\ntrace-http-path would work like traceroute: send a series of requests\nwith TTLs starting at 1 and increasing.  A proxy seeing a TTL header\nwould be required to decrement it by 1 before forwarding.  If the\nTTL reaches zero, the proxy would be required to NOT forward the\nrequest, and to return a distinctive status code meaning \"TTL expired\".\n(Should this be 5xx status code, or should we invent a 6xx series\nfor proxy-specific errors?)  Along with the error code, the proxy\nwould return a short text message that includes the URI of the\nproxy (as in the Forwarded: header) and perhaps optional information\nabout the proxy's configuration and/or status (such as load average\nor cache hit rate).\n\nNote that trace-http-path need not be a separate program; it could\nbe a function built into a browser client.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "Jeffrey Mogul <mogul@pa.dec.com> wrote:\n> ...  I firmly believe\n> that HTTP should NOT be used for real-time continuous media.  The\n> URL mechanism allows us to include multiple transport protocols\n> (e.g., HTTP, FTP, Gopher) in the web, and if we want to include\n> real-time continuous, then we should use a protocol optimized for\n> that.  We should not try to turn HTTP into a kitchen-sink protocol,\n> making it into a second-rate media protocol while also making it\n> harder to implement.\n\nOne major point in making HTTP more supportive of media communication\nsuch as this is that a significant number of users will be utilitizing\nproxies. Each new protocol must be handled by the proxy, and if you\nonly have an HTTP proxy, you are out of luck.\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> When I first read your proposal for a TTL field, I had the same\n> reaction that Roy seems to have had: this isn't really necessary.\n> The Forwarded: header allows any proxy to detect loops through\n> itself, and can support that task without being mandatory.\n>\n> And do we have any evidence that forwarding loops are a real problem?\n> Routing loops occur because our routing protocols are automatic and\n> dynamic, and so can do stupid things rapidly and for transient periods.\n> HTTP forwarding loops, on the other hand, would be created by humans\n> and should thus be both less frequent and less transient.\n\nAt the moment this is true, but one might try to use some protocol\nfor the automatic configuration of proxies -- actually, caches (as\na matter of fact, I am investigating on this subject).  It might\nbe a bad idea or not, I cannot say at the moment.\n\nWhat I know for sure is that it is very easy *now* to point a proxy\nto another proxy, (e.g. in CERN code) without even thinking of the\nproblem of loops. You might not necessarily see the loop immediately,\nperhaps much later when a particular request triggers the loop.\n\nCertainly loops would be less frequent and less transient. The\nlatter makes the problem perhaps easier to trace, but harder to\nremove on saturday nights. Especially on our side of the ocean, I\nbelieve.\n\n> I'm also not that worried about arbitrarily long forwarding paths\n> (what's the harm?), and I would be worried about\n>     o   the danger that some browser would set the TTL too short,\n> thus causing hard-to-debug service problems\n\n:) this makes me remember our microvax 3500 running an old version of\nUltrix (2.1 ?) where the TTL was limited to some low value, and it\ncouldn't communicate with some nodes...\n\n>     othe overhead of managing yet another mandatory header field on\n>         every request, just to avoid a very rare problem.\n\nactually I thought of including the TTL as an additional parameter\nin the request header, right after the protocol.\n\n> However: after I thought about this some more, I realized that there\n> is an entirely different reason to include a TTL header in HTTP.\n\nTwo motivations are better than one...\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "On Mon, 27 Nov 1995, Jeffrey Mogul wrote:\n\n> \n> We've had this discussion at least once before.  I firmly believe\n> that HTTP should NOT be used for real-time continuous media.  The\n> URL mechanism allows us to include multiple transport protocols\n> (e.g., HTTP, FTP, Gopher) in the web, and if we want to include\n> real-time continuous, then we should use a protocol optimized for\n> that.  We should not try to turn HTTP into a kitchen-sink protocol,\n> making it into a second-rate media protocol while also making it\n> harder to implement.\n> \n\nThe media would not have to be served in real-time. Rather\nI envision the first pass providing the ability to provide\nusers with richer access to media via mechanisms like time\ncode. I don't think including a more media friendly mechansim\nwould make http a kitchen-sink protocol. It could make it\nmore powerful to work with media which would ultimately permit\na richer communications environment.\n\n- West\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "There are also reasons why the Forwarded header can lead to incorrect \nbehaviour in a network of proxies interlinked by long-lived connections.\nThere are plenty of cases where forwarding a query on through a proxy \nthough which it has already passed is in fact correct behaviour. \n\nFor example, Suppose we have a network of servers in the following topology\n\n\n    1    2    3    4    5\n  A----B----C----D----E---F\n   \\     /\n    \\-----G-------------/\n      6        7\n\n\nNow, lets assume we want to sent a query from A to E. The network \ntopology tells us that we should use link 6 and send the query via G and F\n\nSuppose however, that in the meantime  F crashes, bringing down links 5 \nand 7.  When the query arrives at G, the routing tables will have changed \nto make the correct next hop A again (A will have updated its tables to \nroute queries to E via B).\n\nWith a Forwarded header, A would be forced to discard the message, even \nthough a path exists between A and E. \n\n\n\n"
        },
        {
            "subject": "Re: HTTP and Media Servin",
            "content": "> I think the central issue here is where the concept of media\n> is handled. If the HTTP server is considered as a media server\n> then it must have the facilities to deal with media.\n> \n> We've had this discussion at least once before.  I firmly believe\n> that HTTP should NOT be used for real-time continuous media.  The\n> URL mechanism allows us to include multiple transport protocols\n> (e.g., HTTP, FTP, Gopher) in the web, and if we want to include\n> real-time continuous, then we should use a protocol optimized for\n> that.  We should not try to turn HTTP into a kitchen-sink protocol,\n> making it into a second-rate media protocol while also making it\n> harder to implement.\n> \n> As a purely practical matter, this working group is chartered to\n> work on IETF standards, which normally require \"rough consensus\n> and working code\" to progress.  We would be in relatively uncharted\n> territory when it comes to real-time continuous media, which is\n> still the subject of active research and debate.   It would be\n> quite premature to try to standardize this kind of thing, especially\n> in the context of the most heavily-used protocol protocol in today's\n> Internet.\n\nI'll second this, and point out that the Upgrade header field in the\ndraft of HTTP/1.1 is designed to allow changes in application protocol\nwhen the server wants to send a resource with these characteristics.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, Marc Salomon wrote:\n> Do browsers have to know the size of the image before you create a spot for it\n> on the page at render time?  Couldn't you render the HTML as soon as it\n> arrives enabling anchors and leaving a standard sized hole, like the [S] icon \n> that xmosaic uses, and resize the hole as the image data arrive?  It'd be a \n> bit jumpy...unacceptably so?\n\nToo jumpy, yes.  Look at MacWeb and MacMosaic - both leave a small icon \nfor the image, redrawing the screen when it starts rendering the image.  \nIf I am reading something below that, I completely lose my place when it \ngets resized.  \n\n> Given the scenario above, sounds like multipart/mixed gets an A in net.\n> citizenship (points off for sending all those nasty ascii headers) and perhaps \n> a A- in UPP (since the images do not all arrive simultaneously), although \n> someone has sketched a scheme for multiplexing several images into a \n> MIME stream earlier in the year on www-talk, I think, which would do just that.\n\nBut aren't you 75% of the way towards -NG when multiplexing MIME?\n\n> Do any of you all out there with gig and gig of log files have any data on \n> what percentage of requests for HTML docs come from Netscape?\n\nFor the last week, NetScape (any platform) accounted for 65% of hits to \nour home page.\n\n> >From the HTTP perspective, multipart/mixed for representing HTML, is probably\n> a bridge solution to realize a limited performance gain until we can see \n> widespread deployment of next-generation and binary protocols.  But for the \n> web as a worldwide information system, there are long-term benefits to \n> extending the interchange of HTML and therefore the web beyond just HTTP, but \n> to other MIME-compliant systems like NNTP and SMTP.  \n\nDefinitely - I'd like to be able to send an HTML document + inlined \nimages as a multipart mail message or posted to a newsgroup.  I can do \nthat now, it's just hardly anyone has a MIME news reader and there's no \n100% reliable way to build an HREF (that I know of) from one part to \nanother.  Likewise I'd like to see HTTP kept separate from HTML so that \nits benefits extend to other media types too.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> Suppose however, that in the meantime  F crashes, bringing down links 5 \n> and 7.  When the query arrives at G, the routing tables will have changed \n> to make the correct next hop A again (A will have updated its tables to \n> route queries to E via B).\n> \n> With a Forwarded header, A would be forced to discard the message, even \n> though a path exists between A and E. \n\nIn this case, A should recognize that there is a loop (which is exactly\nwhat this scenario describes), cancel the original request, and re-route\nit to the new destination.  There is no better way to handle such a request.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> There are two main uses for the TTL field in IP headers: first,\n> to avoid routing loops and long-delayed packets.  Second, to make\n> \"traceroute\" work.  I would bet that most TTL \"failures\" in today's\n> Internet are from traceroute users, not routing loops.\n\nUmmm, I thought traceroute just used this hack (useful as it is) because\nthere was no way to change IP to include a \"traceroute\" command sending\na response from each recipient.\n\n> Traceroute has proved to be an essential tool in debugging IP-level\n> problems.  We ought to be thinking about providing analogous debugging\n> tools at the HTTP level.  For example, \"how come the users on my\n> LAN are having trouble reaching server www.xxx.com?\"  It would be\n> really nice to have a \"trace-http-path\" program, like traceroute\n> but displaying the HTTP-level forwarding path instead of the IP-level\n> routing path.\n\nThat is why I put TRACE in the HTTP/1.1 specification as a new method.\nThe only problem is that you won't get any response back from an unbounded\ncircular route.  I suppose we could add a Max-Forwards request header,\nbut I'd prefer to have an application test it out first.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "    > There are two main uses for the TTL field in IP headers: first,\n    > to avoid routing loops and long-delayed packets.  Second, to make\n    > \"traceroute\" work.  I would bet that most TTL \"failures\" in today's\n    > Internet are from traceroute users, not routing loops.\n    \n    Ummm, I thought traceroute just used this hack (useful as it is)\n    because there was no way to change IP to include a \"traceroute\"\n    command sending a response from each recipient.\n    \nI didn't say that this was the original intent of the TTL field,\njust that it might well be the main use.  Evolution works like that :-)\n\nBut if you were going to design things from scratch to support\ntraceroute, would you come up with a radically different mechanism?\nThe \"record route\" IP header option isn't all that useful.  Aside\nfrom its inability to deal with paths longer than 6 or 7 hops, it\nonly works if the entire path is functioning, and hence cannot\nbe used to find out why a path is not functioning.\n\n    > Traceroute has proved to be an essential tool in debugging IP-level\n    > problems.  We ought to be thinking about providing analogous debugging\n    > tools at the HTTP level.  For example, \"how come the users on my\n    > LAN are having trouble reaching server www.xxx.com?\"  It would be\n    > really nice to have a \"trace-http-path\" program, like traceroute\n    > but displaying the HTTP-level forwarding path instead of the IP-level\n    > routing path.\n\n    That is why I put TRACE in the HTTP/1.1 specification as a new\n    method.  The only problem is that you won't get any response back\n    from an unbounded circular route.\n\nTRACE suffers from the same problem as the IP \"record route\" option:\nit's useless if the path is broken.  (Loops are not the only way\nto break a forwarding path; it's probably even easier to do that\nby misconfiguring a \"proxy\" pointer to point at a black hole.)\nAccording to the current 1.1 draft,\n\n   The TRACE method requests that the server identified by the \n   Request-URI reflect whatever is received back to the client as the \n   entity body of the response.\n\nThis means that (1) the originator of the TRACE method must know\nthe URI of the system that is being asked to reflect the header,\nand (2) that system must be up and reachable.  Neither of these\nthings is generally true in the case of a faulty path!\n\nTRACE seems to be useful for debugging proxies and perhaps clients,\nbut not for determining where a path is entirely broken.\n\n    I suppose we could add a Max-Forwards request header, but I'd\n    prefer to have an application test it out first.\n\nI think this is an appropriate thing to put into a Proposed Standard.\nWe know from experience with traceroute that the basic algorithm\nis sound and useful.  On the other hand, we aren't likely to be\nable to do any realistic testing of the mechanism unless it is\nsupported by popular proxies and perhaps servers.  And because it\nis not normal for clients to send it, if it turns out to be\na bad idea, we won't have saddled ourselves with a lot of extra\nbaggage.\n\nIn summary, I think we are probably best served by a combination\nof the proposed mechanisms:\n\nTRACE\nmethod, useful for debugging implementations,\nvaguely analogous to ICMP Echo as used in \"ping\"\n\nMax-Forwards: (or TTL:)\nheader, useful for debugging path problems,\nanalogous to IP TTL as used in \"traceroute\"\n\nForwarded:\nheader, for stopping loops, also useful in conjunction\nwith TRACE, analogous to SMTP \"Received:\" header\n\nNormal requests would carry Forwarded: but not TRACE or Max-Fowards:\nAll servers SHOULD implement TRACE.  All proxies and servers SHOULD\nimplement Max-Forwards:.  All proxies SHOULD implement Forwarded:.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Administrivia: List unsupervised for 2 week",
            "content": "Avid readers,\n\nThe http-wg mailing list will be left to run on its own for 2 weeks as I fly\nfrom England to attend the IETF meeting in Dallas and WWW4 in Boston.  (Hope\nto see some of you folks there.)\n\nSince the list is manually administered, all subscription and unsubscription\nrequests will be queued until 16/12/95.\n\nIf you see irate people posting to the list, can you warn them.  ;-)\n\nThanks!\n-- ange -- <><\n\nange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Logic Bag concern",
            "content": "I hate to lob this into the discussion so shortly before IETF, and\nparticularly when I haven't had time yet to finish looking at HTTP/1.1,\nbut....\n\nI'm troubled by Logic Bags (3.11), which apparently have been added to\nsupport the Unless header and byte ranges.  My particular concern is\nhow to implement the relational operators, such as \"gt\".  It seems to\nme that their implementation is header-dependent.  \"gt\" for\nLast-Modified is different from \"gt\" for Content-Length and from \"gt\"\nfor \"Content-Version\".  Now, it's easy to state, on a case by case\nbasis, what the correct behavior should be.  But what should a server\ndo in the face of unfamiliar headers?  Example:\n\nI get header Foobar: 100.  My bag says {gt {Foobar \"AX\"}}.  How do I\ncompare them?  Lexicographically?  Numerically?  (What base?)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "twophase send concern",
            "content": "I also hate to lob this into the discussion so shortly before IETF, and\nparticularly when I haven't had time yet to finish looking at HTTP/1.1,\nbut....\n\nSeveral HTTP/1.1 methods (POST, e.g.) propose a two-phase send, first\nheaders, then body.  A server can respond with 100 Continue after\nreceiving the headers.\n\nMy concern:  I don't think what's described can be implemented.  The\nspec. calls for a five-second timeout.  In the face of a chain of\nproxies, early agents in the chain are likely to timeout and proceed,\nand the body is likely to cross paths with the 100 response.  In fact,\nno one timeout is ever likely to be correct for all circumstances.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Logic Bag concern",
            "content": "Another extremely small point about this stuff:\nIf there's already a NOT operator in the expression syntax, why do we\nhave to start off with a negative in the header (\"UNLESS:\") itself? \nI don't think it won't make the headers more readable if we don't keep it\nthis way.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "    I'm troubled by Logic Bags (3.11), which apparently have been added\n    to support the Unless header and byte ranges.\n\nPeople are beginning to find specific problems with Logic Bags\nand Unless.  I'd like to raise a more general question: why do\nwe need this stuff at all?\n\nI can see specific justifications for byte ranges, cache validators,\nand (weakly justified, but required for compatibility) if-modified-since.\nI don't see any rationale for a more general mechanism, but I do\nsee a lot of implementation pain.\n\nI'd vote to remove Unless and Logic Bags from the 1.1 spec, unless\nor until we have a concensus that these are necessary.  And I still\nthink we ought to be using a Cache-validator: header (syntax and\nexact name to-be-determined, don't flame me about this!) with a 100%\nopaque validator, rather than fooling around with elaborate schemes\nthat nobody seems to have any explicit need for.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "According to Jeffrey Mogul:\n> \n> I'd vote to remove Unless and Logic Bags from the 1.1 spec, unless\n> or until we have a concensus that these are necessary.  And I still\n> think we ought to be using a Cache-validator: header (syntax and\n> exact name to-be-determined, don't flame me about this!) with a 100%\n> opaque validator, rather than fooling around with elaborate schemes\n> that nobody seems to have any explicit need for.\n> \n\nI would also vote for an opaque cache validator.  We would need an\n\"If-cache-valid: <validator> \" and an \"If-cache-stale: <validator>\"\nand we would need to specify their *semantics*.  E.g. for a Range:\nrequest with an If-cache-valid header send the range if valid, else send\nentire document. \n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "    I would also vote for an opaque cache validator.  We would need an\n    \"If-cache-valid: <validator> \" and an \"If-cache-stale: <validator>\"\n    and we would need to specify their *semantics*.  E.g. for a Range:\n    request with an If-cache-valid header send the range if valid, else\n    send entire document.\n    \nCan you explain why we need both\nIf-cache-valid: <validator>\nand\nIf-cache-stale: <validator>\ninstead of simply\nCache-validator: <validator>\nalong with a set of rules that explain how it is supposed to\nbe interpreted?\n\nE.g., for\nGET\nRange: 3-8\nCache-validator: XYZZY\nI would expect the semantics to be\nif the validator of the actual object is XYZZY then\nreturn range 3-8, else return the whole thing\n\nYou might argue that one could use:\nGET\nRange: 3-8\nIf-cache-stale: XYZZY\nbut this seems to mean\nif the validator of the actual object is NOT XYZZY then\nreturn range 3-8, else return nothing\nbut that doesn't make a lot of sense to me, because I don't think\nit is rational to obtain a range of bytes if your cached copy\nof the entire document is known to be invalid.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "According to Jeffrey Mogul:\n>     \n> Can you explain why we need both\n> If-cache-valid: <validator>\n> and\n> If-cache-stale: <validator>\n> instead of simply\n> Cache-validator: <validator>\n> along with a set of rules that explain how it is supposed to\n> be interpreted?\n> \n> E.g., for\n> GET\n> Range: 3-8\n> Cache-validator: XYZZY\n> I would expect the semantics to be\n> if the validator of the actual object is XYZZY then\n> return range 3-8, else return the whole thing\n> \n\nI could live with this.  But I worrly a little about the clarity.\nCompare\n\nGET /foo\nRange: bytes=30-80\nCache-validator: XYZZY\nand\nGET /foo\nCache-validator: XYZZY\n\nIn the first case the server honors the the (range) request only if\nthe validator is XYZZY; in the second it honors the request only if\nthe validator is NOT XYZZY.\n\nI would prefer a little more redundancy for clarity.  E.g.\n\nGET /foo\nRange: bytes=30-80\nIf-Cache-Valid: XYZZY\nand\nGET /foo\nIf-Cache-Stale: XYZZY\n\nYou are correct that this isn't strictly necessary.  But remember it\nis only the validator we want to be opaque, not the header :)\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "logic ba",
            "content": "For a contrarian view on logic bags:\n\nI don't know what the motivation for the logic-bag is. \n\nHowever, folks like us who are involved in implementing complex, cursored\ndata transfers within HTTP can use it. I don't think we should toss the idea\nof logic bag out of the window. I am sure other people have seen this, but\nthe combination of KEEP-ALIVE header field, UNLESS header field, logic-bag,\nLINK header field, and chunked transfer can be used to support very powerful\nquery and update facilities within HTTP.\n\nI do agree however that it makes server implementers life harder.  \nSankar Virdhagriswaran                         Phone: (508) 287 4511\nCrystaliz Inc.                                 Fax:   (508) 287 4512\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "Jeff - \n\nI could work it out, but i'm feeling kind of lazy :-)\n\nWhat value were you using for 2MSL in the traces? Also, did you assume that \nthe effective bandwidth for a given path remained the same between the \ntraces and the simulated run? \n\nSimon\n\n\n\n"
        },
        {
            "subject": "Byte ranges (was Re: Logic Bag concerns",
            "content": "    I could live with this.  But I worrly a little about the clarity.\n    Compare\n    \n    GET /foo\n    Range: bytes=30-80\n    Cache-validator: XYZZY\n    and\n    GET /foo\n    Cache-validator: XYZZY\n    \n    In the first case the server honors the the (range) request only if\n    the validator is XYZZY; in the second it honors the request only if\n    the validator is NOT XYZZY.\n\nDear me.  I believe there is still some confusion, implied by\nyour use of the phrase \"honors the request\" for both \"GET\" and\n\"Range:\".  Range: is not a request; it is a modifier to the GET\nrequest (method).\n\nConsider the case when the Range: header is NOT included.  Then\nGET /foo\nCache-validator: XYZZY\nmeans\nif the cache-validator for /foo is XYZZY\nthen\n    send me nothing/* Valid case */\nelse\n    send me /foo/* Invalid case */\n\nNow, what does \"Range:\" mean?  \nGET /foo\nRange: bytes=30-80\nCache-validator: XYZZY\nmeans\nif the cache-validator for /foo is XYZZY\nthen\n    send me bytes 30-80/* Valid case */\nelse\n    send me /foo/* Invalid case */\n\nIn other words, IN ALL CASES what the cache-validator means is\nthat if it doesn't match, send the whole object.  What Range:\nmodifies is the behavior of GET when the cache-validator does\nmatch.  And (if you like) you can think of a missing Range:\nheader as implying \"Range: bytes=0-0\", so there is really only\none meaning.\n\nMaybe it's simpler to think about this if you pronounce\n\"GET\" as \"Make sure my cache for this object is up to date\".\n\nThen Range: doesn't affect whether the server honors this\nrequest or not, it simply tells the server \"Oh, by the\nway, I know that this part of my cached copy is missing.\"\n\nYour proposal for having separate \"If-Cache-Valid:\" and \"If-Cache-Stale:\"\nheaders worries me.  For example, this would be syntactically valid:\nGET /foo\nRange: bytes=30-80\nIf-Cache-Stale: XYZZY\nbut semantically it seems like a bad idea: \"if my cached copy is\nbad, then send me only part of a replacement\".  I sure hope nobody\nthinks this is a good idea!\n\n    You are correct that this isn't strictly necessary.  But remember it\n    is only the validator we want to be opaque, not the header :)\n    \nHTTP headers are not meant to be read by humans, I hope.  (Well,\nOK, I suppose I consider server and browser programmers human;\nat least some of them have cute little baby daughters.)  So clarity\nis important for the specification, but isn't really important\nfor the header syntax.  What matters instead is how complicated\nit is to implement software that uses the syntax.\n\nThis is a lesson that CPU architects learned in the 1980s.  Don't\ndesign the instruction set to make it easy to write assembly code\nby hand; design it to make it easy to implement fast CPUs.  There\nwill always be a few maniacs who are willing to write compilers\nfor the rest of us to use.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: logic ba",
            "content": "    However, folks like us who are involved in implementing complex,\n    cursored data transfers within HTTP can use it. I don't think we\n    should toss the idea of logic bag out of the window. I am sure\n    other people have seen this, but the combination of KEEP-ALIVE\n    header field, UNLESS header field, logic-bag, LINK header field,\n    and chunked transfer can be used to support very powerful query and\n    update facilities within HTTP.\n\nHow about at least one specific example?  Including some analysis\nof how much harder it would be to implement the same functionality\nwithout Logic Bag?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "Jeffrey Mogul writes:\n >     I'm troubled by Logic Bags (3.11), which apparently have been added\n >     to support the Unless header and byte ranges.\n > \n > People are beginning to find specific problems with Logic Bags\n > and Unless.  I'd like to raise a more general question: why do\n > we need this stuff at all?\n > \n > I can see specific justifications for byte ranges, cache validators,\n > and (weakly justified, but required for compatibility) if-modified-since.\n > I don't see any rationale for a more general mechanism, but I do\n > see a lot of implementation pain.\n > \n > I'd vote to remove Unless and Logic Bags from the 1.1 spec, unless\n > or until we have a concensus that these are necessary.  And I still\n > think we ought to be using a Cache-validator: header (syntax and\n > exact name to-be-determined, don't flame me about this!) with a 100%\n > opaque validator, rather than fooling around with elaborate schemes\n > that nobody seems to have any explicit need for.\n > \n > -Jeff\n\nBefore I cast a vote to get rid of ``logic \"bags\"(?)''  I'd like to\nhear from Roy and anyone else who likes this stuff as to (a)\nspecifically why they think it is justified, and (b) why worrying\nabout it is more pressing than some of the other things that need to\nget done.  Though, truth be told, without some evidence as to its\nutility I'm inclined to agree that it should not be included.  I'd\nhate to see the baby thrown out with the bathwater though, so I'd like\nto second (or third) Jeff's motion to do something rational about\nopaque validators if we do take this out.   The problem is that we've\ngot two quite different mechanisms conflated here -- support for\nvalidators and support for the first 2% of Common Lisp....and the rest\ncan't be far behind :^].\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": ">Before I cast a vote to get rid of ``logic \"bags\"(?)''  I'd like to\n>hear from Roy and anyone else who likes this stuff as to (a)\n>specifically why they think it is justified, and (b) why worrying\n>about it is more pressing than some of the other things that need to\n>get done.  \n\nI would too. I wonder what the original reasons for it were. Since I spoke\nup, here are some example areas where I would like to apply it.\n\nThe simplest ones include testing on content type or content length to\nprovide some hints to the user (build rover the dog within WWW browsers !!).\nMore complex ones include:\n\nBuilding iterators. The general idea is to use the \"unless\" test (testing\nfor leaf or empty nodes) combined with the \"link\" header to traverse a tree\nof related objects. Using this sort of functionality, I can implement a\ncontainer like concept. This of course means that the client is holding\nstate between a series of get or put or link methods, but such is life in\nthe HTTP land.\n\nProtocol upgrade. Another place where I would like to use it is to upgrade\nto a different (possibly private) protocol based on \"unless\" tests on\ncontent type,  and content length.\n\nTransactions or Reliability. Let us say that I have an update which consists\nof many puts, each of which are actually saved to a transactional db. The\nglobal transaction is not known to the transactional db. In order to make\nsure that all my puts have been committed by the transactional database, I\nwould like to use a callback from the transactional db to set an extension\nheader (defined by myself). The puts will check against this tocken to see\nif the previous put(s) has been committed. If any of them fail (determined\nby a test with unless and logic bags) I would then use some other mechanism\nto inform the client that the global transaction did not committ.  Notice\nthat this sort of thing would come in very, very handy in HTTP-NG timeframe\nwhen request and responses may span connections. Even with HTTP 1.1, since\nwe have the keep-alive notion that spans requests and responses, if one is\nconcerned about transactional capabilities, one needs to perform such tests.\n\nBegin SoapBox: \n\nThe notion of per method dynamic binding of services is very powerful. At\nOMG we are struggling with those concepts just now. Let us not do the same\nmistake at WWW. If HTTP systems are to perform as object oriented systems in\nthe WAN, per method dyanmic binding of services is going to be very useful.\nCurrently, the HTTP headers donot have all the needed headers with\nappropriate semantics to do very useful things. But, unless and logic bags\nwill encourage experimentation in this space and we will be able to learn\nfrom those experiences for the next rev.\n\nEnd SoapBox \nSankar Virdhagriswaran                         Phone: (508) 287 4511\nCrystaliz Inc.                                 Fax:   (508) 287 4512\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "There are times when it is easier to implement an abstraction that covers\na number of related problems than it is to implement each solution\none by one.  I realize that some folks have only seen the need for a\ncache-validator, and therefore don't understand why I chose to design\na more general mechanism.  I am not surprised by that at all, and fully\nexpect people to question it (that is, after all, why we are all here).\n\nOn the other hand, it is a very new construct and I'm not going to throw\nit away until everyone gets a chance to think about how it might be used,\nand implementors get a chance to examine how it might be implemented on\ntheir systems.  I also need to prepare three presentations for the next\ntwo weeks of travelling, so I'd much rather talk about it at the Dallas\nIETF and Boston WWW4 than try to defend it on a mailing list (there just\nisn't enough bandwidth).\n\nHere is the problem statement:\n\n   How do you allow implementors to define a precondition on any method\n   (not just GET) without requiring a change to the protocol version\n   with every new precondition?\n\nand the question is\n\n   What is the actual difference in development and run-time expense\n   between implementing a general mechanism for preconditions such as\n\n       IF: {eq {Content-MD5 \"89787jhlkr8r87y98437==\"}}\n   or\n       IF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 GMT\"}}\n\n   over implementing a special-purpose mechanism for cache validation\n\n       Content-Validator: \"89787jhlkr8r87y98437==\"\n   and\n       If-Validator: \"89787jhlkr8r87y98437==\"\n\n   keeping in mind that no existing server currently provides a\n   Content-Validator header field and that the field, whatever it\n   contains, must be duplicating some other entity-header.\n\nSo, I weighed the issues, looked at the implementations, and decided that\nthe long-term benefits of a general-purpose solution was not only far\nsuperior to the single-case, but was no more difficult to introduce to\nthe existing implementations.  In other words, I made a design choice.\n\nNote that I use \"IF\" instead of \"Unless\" above -- I am not devoted to\neither form -- just to the notion that we shouldn't need to change the\nprotocol every time someone thinks of a new precondition.\n\nFor example, let's consider payment.  Although many payment transactions\nmay be considered a complex interaction of exchanging offers and needs,\nsome are as simple as buying the morning paper.  What, then, if someone\nin the future wants to state the condition\n\n   GET this unless the price is more than I usually pay\n\nSounds reasonable to me (I do not claim to be an expert on payment\nissues, by any stretch of the imagination).  To solve this person's\nrequest, do we need to create a new protocol?  Or, do we allow those\npeople who are experts in the payment area define what \"Cost\" means,\nhow to compare two \"Cost\" values, and simply use\n\n   GET this HTTP/1.1\n   Unless: {gt {Cost \"US$0.25\"}}\n\nThis is the design tradeoff that I had in mind when I created the\nLogic Bag syntax and the Unless (or IF, if you prefer) header field.\n\n> validators and support for the first 2% of Common Lisp....and the rest\n> can't be far behind :^].\n\nPerhaps.  Dan Connolly forced me to look hard at the S-expression syntax\nversus the ;parameter=value syntax, and I found the S-expressions to be\nmuch better when the value could be a set, list, or structure.\nI would not be surprised if the syntax, being simple and expressive,\nturns out to be useful for things far beyond what can be anticipated\ntoday.  At the least, I'll give it a couple months.\n\nIt should make for an interesting discussion on Monday.  See y'all\nin Dallas.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "> I'm troubled by Logic Bags (3.11), which apparently have been added to\n> support the Unless header and byte ranges.  My particular concern is\n> how to implement the relational operators, such as \"gt\".  It seems to\n> me that their implementation is header-dependent.  \"gt\" for\n> Last-Modified is different from \"gt\" for Content-Length and from \"gt\"\n> for \"Content-Version\".  Now, it's easy to state, on a case by case\n> basis, what the correct behavior should be.  But what should a server\n> do in the face of unfamiliar headers? \n\nrom Section 3.11:\n\n   A field-tuple consists of a field-name (assumed to be an HTTP\n   header field name, though not constrained to those defined by this\n   specification) and the field-value component which is to be\n   compared against the resource's field value. The actual method of\n   comparison (e.g., byte equivalence, substring matching, numeric\n   order, substructure containment, etc.) is defined by the logical\n   definition of the operator and the type of field-value allowed for\n   that field-name. Server implementors must use an appropriate\n   comparison function for each type of field-value given in this\n   specification. The default functions for unrecognized fields are\n   numeric comparison (for values consisting of 1*DIGIT) and lexical\n   comparison (for all others).\n\n   Except for \"ne\", any comparison to a field not defined by the\n   resource evaluates to false.\n\n> Example:\n> \n> I get header Foobar: 100.  My bag says {gt {Foobar \"AX\"}}.  How do I\n> compare them?  Lexicographically?  Numerically?  (What base?)\n\nLexicographically [is that a word?]\n\nI do anticipate that this might cause some problems during the immediate\nperiod of introducing a new header field for which the default comparison\ndoes not apply.  There are ways to solve this [such as a protocol registry],\nbut I did not want to add too much too soon.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "> Another extremely small point about this stuff:\n> If there's already a NOT operator in the expression syntax, why do we\n> have to start off with a negative in the header (\"UNLESS:\") itself? \n> I don't think it won't make the headers more readable if we don't keep it\n> this way.\n\nI must have been in a negative mood at the time.          ;-)\n\nActually, I usually think of preconditions as negative expressions,\nwhich is why I picked Unless.  IF is fine too (it even saves 4 whole bytes).\n\n....Roy\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "> Several HTTP/1.1 methods (POST, e.g.) propose a two-phase send, first\n> headers, then body.  A server can respond with 100 Continue after\n> receiving the headers.\n> \n> My concern:  I don't think what's described can be implemented.  The\n> spec. calls for a five-second timeout.  In the face of a chain of\n> proxies, early agents in the chain are likely to timeout and proceed,\n> and the body is likely to cross paths with the 100 response.  In fact,\n> no one timeout is ever likely to be correct for all circumstances.\n\nIt doesn't have to be correct -- it exists as a safety-valve, allowing\nalmost all servers (even with multiple levels of persistent-connection\nproxies) to send back a terminate response before the user agent has \nsent a large portion of the request data.  If it misses, that's okay --\nat least it tried.\n\nPerhaps more importantly, it forces the implementor to think about\nthese issues.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: request identifiers in HTT",
            "content": "Luig said:\n\n> One thing that I'd like to see in the protocol is a request identifier\n> in the response headers, that would allow to match responses with\n> requests.\n> \n> This would have many applications, such as enable a client to send\n> a batch of requests to a server, and receive the responses out of\n> order (in order to minimize the overall response time). Ultimately,\n> this would even allow to separate requests and responses, or (in\n> many cases) use a connectionless transport for HTTP.\n> \n> Has this subject been discussed before ?\n\nYes, but it was decided that such issues were beyond the scope of\nHTTP/1.x and better left to HTTP/2.0.  The reason being that it would\ntake the same time to explore those issues in HTTP/1.x as it would\nto make them obsolete with HTTP/2.0 (where multiple interleaved\nrequests are possible).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "    One other thing that was discussed was the relative advantages of using\n    a session method vs. an ignorable header. It turns out that there is a\n    problem with using ignorable headers when proxies are used - if a proxy\n    which doesn't interpret the header is used to talk to a server which\n    does handle the header, the connection can become deadlocked (the end\n    server things that the proxy doesn't want it to drop the connection,\n    whilst the proxy is sitting there waiting for the connection to drop).\n    \nThis is a good point.  However, the situation is not actually a \"deadlock\",\nsince the end server will presumably time out the connection after a\nwhile.\n\nThis could lead to some longish delays, of course, but if the end server\nvaries the idle timeout according to the observed number of reuses\nof a connection (i.e., start with a short timeout, and increase it if\nif the client is observed to reused the connection) then the situation\nwill probably converge nicely.\n\nEven with a 1-second idle timer, my simulations show about half of\nthe TCP connections are avoided.  I have not simulated an adaptive\ntimer, but I'll put that on my list of things to do.  I would hazard\na guess that over a reasonable interval, this would do almost as well\nas the optimal case.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concerns (with some proposals",
            "content": "Roy T. Fielding:\n[About Unless and logic bags:]\n>On the other hand, it is a very new construct and I'm not going to throw\n>it away until everyone gets a chance to think about how it might be used,\n\nSome remarks on uses of this construct:\n\n1) To make continuing interrupted GETs using the Range header\npossible, just defining logic bags and Unless: in HTTP/1.1 is not\nsufficient.\n\nHTTP/1.1 should also put a restriction on the behavior of servers that\nwould give clients a 100% reliable way of ensuring that the entity\nbody bound to a resource has not changed since the previous request.\n\nBasically, this means defining at least one header value or\ncharacteristic derived from a number of headers that, if present in\nthe response, is _guaranteed_ to change if the entity body changes.\n<draft-luotonen-http-url-byterange-XX.txt> defines Last-modified as\nsuch a guaranteed-to-change header, but this may not be the best\nsolution.\n\nWithout such 100% guarantees defined in HTTP/1.1, continuation of\ninterrupted GETs cannot be implemented reliably.  The ability to test\nlots of response headers with Unless (as in the logic bag example in\nSection 4.2 of the 1.1 draft) may be impressive, it is not sufficient.\n\n\n2) One nice use of Unless I just thought up: it allows for a \n`delay image loading for images larger than XX bytes' option in\nbrowsers.  This could be implemented with minimal overhead by sending\n  Unless: {gt {Content-length XX}} \nin inline picture requests.\n\n\n3) I agree with Dave Kristol that the relational operators like 'gt'\nin logic bags have big problems when used with undefined headers.\nYour default does not make these problems go away.  There are three\nimportant ways of comparing:\n\n  numeric  (2 < 10)\n  lexical  (ba < c)\n  date     (Mon, 04 Dec 1995 01:23:45 GMT < Mon, 04 Dec 1995 01:23:55 GMT)\n\nand the defined default would break on undefined headers that carry a\ndate.  I therefore propose to define three `greater than` operators:\n\n  gtn   greater than, numeric\n  gtl   greater than, lexical\n  gtd   greater than, date.\n\nand maybe, but not required:\n\n  gt   greater than, with field-name dependent function (use only with\n       field-names defined in HTTP/1.1).\n\n(of course, the same for ge, lt, le).  Another question: what is the\n`appropriate comparison function' server authors are supposed to use\nfor `Content-Version'?  Do we really want to open this can of worms?\n\n4) Implementation and portability considerations will make the Unless\nheader useless for some applications, including the `Transactions or\nReliability' application described by Sankar Virdhagriswaran earlier\nin this thread.  See 5) for an explanation.\n\n>and implementors get a chance to examine how it might be implemented on\n>their systems. \n\n5) There is a big problem here. Consider the following request:\n\n  POST /bin/send_mail HTTP/1.1\n  Unless: {gt {Content-length 123}}\n  Accept: */*\n  <more request headers and the encoding of a form containing a mail\n  message follow>\n\nNow, the 1.1 draft says that `the server must abort the request' if\nthe Content-length of the response would be bigger than 123.  The\nproblem is: abort when?  Before or after the send_mail CGI script\nsends the mail?\n\nImplementation and portability considerations dictate that the abort\ncan only happen _after_ the send_mail script has sent the mail.  Thus,\nI propose that the text\n\n  `the server must abort the request and respond\n   with the 412 (unless true) status code.'\n\nin Section 10.40 be replaced with \n\n   `the server must not send the response resulting from the normal\n    semantics of the request, but respond with a 412 (unless true)\n    status code.'\n\nand that the following text is added at the end of the Section:\n\n    `If a request has the significance of taking an action other than\n    retrieval, the truth of the expression in the Unless field may,\n    but is not required to, prevent the action from being taken.'\n\nWithout this change, we must basically put a logic bag parser and,\nworse, speculative execution or `undo' code in each existing and\nfuture CGI script that takes an action other than retrieval.  This is\nnot an option, especially not for CGI scripts with functionality like\n`delete some files and report the cpu time used'.\n\nMy conclusions about implementability: having Unless will only be\nviable if Unless processing can be done transparently to CGI scripts.\nWhat you can implement under my proposed Unless semantics is the\nfollowing sequence of server actions:\n\n  1. Decode request\n  2. Run CGI script\n  3. Prepare response headers using CGI script output\n  4. Calculate value of logic bag in `unless' header\n  5a. If true, send a 412 (unless true) response\n  5b. If false, send a 200 (or whatever the CGI script produced), \n     response headers and entity body.\n\n6) I observe that Unless is basically a bandwidth-saving device, not a\ndevice for preventing actions like sending mail from happening.  We\nmay want to define another logic-bag based request header (for example\n`No-action-if') for this latter purpose.  The difference with Unless\nwould be that a `No-action-if' header used on a standard CGI script\nwould cause the server to immediately send a `not implemented'\nresponse.  Only special CGI scripts registered as capable of\n`No-action-if' processing would be run by the server.\n\n> I also need to prepare three presentations for the next\n>two weeks of travelling, so I'd much rather talk about it at the Dallas\n>IETF and Boston WWW4 than try to defend it on a mailing list (there just\n>isn't enough bandwidth).\n\nIn that case, I will not expect an immediate answer to my concerns\nabove.  I won't be at the IETF or the WWW4, but I can wait for you to\nintroduce mutated versions of Unless and logic bags after the WWW4.\n\n[...]\n>   GET this HTTP/1.1\n>   Unless: {gt {Cost \"US$0.25\"}}\n\nOops: as Cost is no 1.1 header, a 1.1 server would be allowed to send\nyou a response and charge you \"US $ 640000.25\" for it.  After all,\nthis amount is lexically smaller than \"US$0.25\".\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concerns (with some proposals",
            "content": "Koen Holtman writes:\n...\n >  Another question: what is the\n > `appropriate comparison function' server authors are supposed to use\n > for `Content-Version'?  Do we really want to open this can of worms?\n > \n\nIf Content-Version is going to be used to hold an opaque identifier,\nthen the only appropriate comparison operators are `eq' and `ne'.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Byte ranges (was Re: Logic Bag concerns",
            "content": "Jeffrey Mogul wrote:\n> In other words, IN ALL CASES what the cache-validator means is\n> that if it doesn't match, send the whole object.  What Range:\n> modifies is the behavior of GET when the cache-validator does\n> match.  And (if you like) you can think of a missing Range:\n> header as implying \"Range: bytes=0-0\", so there is really only\n> one meaning.\n\n(I assume you meant \"Range: bytes=0-\")\n\n\n> Maybe it's simpler to think about this if you pronounce\n> \"GET\" as \"Make sure my cache for this object is up to date\".\n\nWell, yes, but an empty cache is just as valid as one full of\nup-to-date objects :)\n\nI hate the notion of automatically dumping the whole file onto the net.\nFirst of all, it is not intuitive to an ex-OS hacker like myself that\nif something in the cache goes stale, I reload the whole (file,...)\ninstead of simply flushing the object from the cache.  More\npragmatically, we are working on displaying potentially huge SGML\ndocuments via HTTP, and if a 20 MB file that I am pulling subtrees\nout of gets changed, I don't want to trigger a send of the whole file.\nPerhaps Ari can tell me how they plan for this to work in the PDF\ncase -- I may be missing something obvious here!\n\nWhat would make sense to me is to define something like a\n\"305 Modified\" response that passes me back the new cache-validator.\nThen _I_ can decide whether to fetch the whole file, or an index or\nwhatever. What are the drawbacks to such a scheme?\n\nMike Braca\nWasabe Software, Inc.\n(Currently at mb@ebt.com)\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concerns (with some proposals",
            "content": "Shel Kaphan:\n>\n>Koen Holtman writes:\n>...\n> >  Another question: what is the\n> > `appropriate comparison function' server authors are supposed to use\n> > for `Content-Version'?  Do we really want to open this can of worms?\n>\n>If Content-Version is going to be used to hold an opaque identifier,\n>then the only appropriate comparison operators are `eq' and `ne'.\n\nI agree that the only approprate version comparison operators are `eq'\nand `ne'.  My point was that, in my reading of the 1.1 spec, server\nauthors would also be required to think up an `appropriate' `gt'\ncomparison function.\n\n>--Shel\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "I thought I started to understand this issue in the working group\nmeeting, that the 'five seconds' was an arbitrary timeout, and that if\nthe timeout were 'zero seconds' that this would also continue to work:\nsenders should read if sending would block, and recievers should send\nabort messages as soon as they can, and NOT CLOSE THE CONNECTION if\nthey care whether the sender actually gets the abort.\n\nDid I get this right?\n\n\n\n"
        },
        {
            "subject": "Re: Byte ranges (was Re: Logic Bag concerns",
            "content": "According to Mike Braca:\n\n> Jeffrey Mogul wrote:\n> > In other words, IN ALL CASES what the cache-validator means is\n> > that if it doesn't match, send the whole object.  What Range:\n> > modifies is the behavior of GET when the cache-validator does\n> > match.  And (if you like) you can think of a missing Range:\n> > header as implying \"Range: bytes=0-0\", so there is really only\n> > one meaning.\n> \n> (I assume you meant \"Range: bytes=0-\")\n> \n\nNo, I think he meant an implicit Range: 0-0 with the intent that\nnothing be sent (but actually it would send one byte).  I think\nthis illustrates the potential for confusion that could be eliminated\nwith an If-cache-valid: <authenticator> and If-cache-stale: <authenticator>.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n>I thought I started to understand this issue in the working group\n>meeting, that the 'five seconds' was an arbitrary timeout, and that if\n>the timeout were 'zero seconds' that this would also continue to work:\n>senders should read if sending would block, and recievers should send\n>abort messages as soon as they can, and NOT CLOSE THE CONNECTION if\n>they care whether the sender actually gets the abort.\n>\n>Did I get this right?\n\nI asked Dave Clark about the issue of abort and closing connections,\nbecause I didn't understand the problem.  Dave (Clark) says a correct TCP\nimplementation should ensure than all bytes sent actually get to the\nreceiving end.  It's up to the receiver to actually look at them.\n\nSo, to return to Larry's question, I think the timeout could indeed be\nreduced to zero.  However, apart from closing a connection correctly, I\ndon't think there's anything special a receiver (to be clear:  a server)\nneeds to do to ensure the sender (client) gets an abort message's bytes.\nBut the sender (client) does have to look for them!\n\nRecall, though, that the point of the timeout was to give the server a\nchance to avoid getting deluged by bytes.  So reducing the timeout to zero\nis not necessarily wise.  But everything should indeed work.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "> I thought I started to understand this issue in the working group\n> meeting, that the 'five seconds' was an arbitrary timeout, and that if\n> the timeout were 'zero seconds' that this would also continue to work:\n> senders should read if sending would block, and recievers should send\n> abort messages as soon as they can, and NOT CLOSE THE CONNECTION if\n> they care whether the sender actually gets the abort.\n> \n> Did I get this right?\n\nYep, that's right.  The 5 seconds is arbitrary and only serves as\na forcing function.\n\n......Roy\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": ">>>>> Dave Kristol writes:\n\ndmk> Larry Masinter <masinter@parc.xerox.com> wrote:\n>> should send abort messages as soon as they can, and NOT CLOSE THE\n>> CONNECTION if they care whether the sender actually gets the abort.\n\ndmk> So, to return to Larry's question, I think the timeout could\ndmk> indeed be reduced to zero.  However, apart from closing a\ndmk> connection correctly, I don't think there's anything special a\ndmk> receiver (to be clear: a server) needs to do to ensure the sender\ndmk> (client) gets an abort message's bytes.  But the sender (client)\ndmk> does have to look for them!\n\nI'd be interested in talking to anyone who has actually implemented\nPUT.  My Unix implementation works fine via SIGPIPE signal handling\nwith no delay, but the port of the client to Windows (Winsock TCP\nstack) has lots of problems with this.\n\nrom my experience, it seems like Larry is correct; the server -does-\nneed to hold the connection open.  Otherwise, (and I'm guessing this\nis due to a poor TCP implementation) as soon as the client sends a\nsingle byte on a closed connection, a TCP reset occurs, and the abort\ndata is -lost- to the client, even though it physically went out on\nthe wire.  I've watched this happen with a sniffer.\n\nThe big problem from my perspective is that the abort data is often\nquite important.  In particular, the place where I see this happen is\nin a PUT to a location that is unauthorized.  The -only- way I've\ngotten this to work is for the server to hold the connection open\nuntil the client notices the data to read and closes.\n\n-Roger\n\nRoger Gonzalez                    NetCentric Corporation\nrg@server.net                     56 Rogers Street\nhome   (617) 646-0028             Cambridge, MA 02142\nmobile (617) 755-0635             work (617) 868-8600\n\n\n\n60 09 3A EE FE 6A 1E CC   -pgp-   B7 F7 6B 0F 00 1D 01 C7 \n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Simon E Spero:\n> The perceived performance of Netscape is merely a function of their\n> rendering order. Netscape renders GIFS as they come in. THis is completely\n> independent of the protocol. \n\nI think the perceived performance of Netscape is a function of doing\nthe layout and displaying the text *before* the images are downloaded.\nThis is not completely independent of the protocol.  It is also a function\nof being able to jump to a new link before all the downloads are finished\n(and hence abort all those downloads).\n\n> \n> The netscape approach of stealing other peoples bandwidth is\n> ultimately a negative sum game. If more and more people start using\n> this approach, not only will the netscape user not see any benefit,\n> but *EVERYBODY* will lose as more and more bandwidth becomes lost to\n> retransmission.\n> \n> If I wanted to get better performance by stealing bandwidth, I'd just\n> write my own transport protocol.  Just run blast datagrams out as UDP\n> as fast as your interface can handle it, and ack every single\n> message. Hell, use a regenerative protocol and send everything\n> multiple times. You'll clear every TCP user off the routers in your\n> path and get the backbone for yourself. \n> \n\nI was, by chance, just looking a Eric Bina's home page where he discusses\ngetting the FLAME OF THE MONTH award.  I fear Simon may be in the competition\nthis month.  I am not sure what he means by \"stealing other people's \nbandwidth.\"  If I only use lynx and my viewing is slowed because I share the\nbandwidth with people who have a lot of images, are they stealing my\nbandwidth?\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "Maybe the reciever should wait \"5 seconds\" (i.e., 'an arbitrary time\nperiod') before closing the connection on an abort. That is, if we're\ngoing to stick random delays into our protocol in order to protect\nreal applications against bugs in the protocol stacks on popular\nplatforms, we're better off putting those random delays in for the\ncase where the transaction is being aborted than in the normal course\nof events.\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "rg@server.net (Roger Gonzalez) wrote:\n[...]\n>>From my experience, it seems like Larry is correct; the server -does-\n>need to hold the connection open.  Otherwise, (and I'm guessing this\n>is due to a poor TCP implementation) as soon as the client sends a\n>single byte on a closed connection, a TCP reset occurs, and the abort\n>data is -lost- to the client, even though it physically went out on\n>the wire.  I've watched this happen with a sniffer.\n\nI can believe this happens.  It's an interesting question what a TCP\nimplementation should do when a program tries to send to a half-closed\nconnection.\n>\n>The big problem from my perspective is that the abort data is often\n>quite important.  In particular, the place where I see this happen is\n>in a PUT to a location that is unauthorized.  The -only- way I've\n>gotten this to work is for the server to hold the connection open\n>until the client notices the data to read and closes.\n\nHow does the server know the client has noticed the data and closed the\nconnection?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "On Tue, 5 Dec 1995, Roy T. Fielding wrote:\n\n> \n> Yep, that's right.  The 5 seconds is arbitrary and only serves as\n> a forcing function.\n\nI too want to understand the question Alex raised during the WG meeting\nre. why slow start and transmit windows don't prevent this from being\na non-concern?  Isn't the client naturally restricted from sending\na huge number of bytes before the server starts receiving some of the\nbytes?  If there needs to be a timeout to protect poor implementations,\nmight it not be better to restrict the server from closing the connection\nfor some interval after sending the abort. The server can leave the\ndata unreceived or otherwise close the receive window but leave the\nconnection open.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "HTTP 1.1 Unless heade",
            "content": "Hi...\n\nReading over draft-ietf-http-v11-spec-00.txt, which in general looks \nrather good, I have become confused about a certain part. In section \n10.40, regarding the Unless header, it says:\n\n   When a request containing an Unless header field is received, the \n   server must evaluate the expression defined by the listed \n   logic-bags (Section 3.11). If the expression evaluates to false, \n   then no change is made to the semantics of the request. If it \n   evaluates true and the request is not a conditional GET \n   (If-Modified-Since, Section 10.23) or a partial GET (Range, \n   Section 10.33), then the server must abort the request and respond \n   with the 412 (unless true) status code. If the request is a \n   conditional GET, then the server must disregard the \n   If-Modified-Since value and respond as it would for a normal GET. \n   Similarly, if the request is a partial GET, then the server must \n   disregard the Range value and respond as it would for a normal GET.\n\nThis handing of conditional GETs seems to me completely wrong. If I read \nthe above correctly, here's what happens:\n\n* No If-Modified-Since, but an Unless:\n- Unless true: 412 response\n- Unless false: send the document\n\n* If-Modified-Since, no Unless:\n- Not modified since: 304 response\n- Modified since: send the document\n\nAll well and good so far, but...\n\n* If-Modified-Since, and an Unless:\n- Unless true and not modified since: send document\n- Unless false and not modified since: 304 response\n- Unless true and modified since: send document\n- Unless false and modified since: send document\n\nThe first two are backwards! The first one will send me the document, even\nthough it's matched all the paramters I told it to match. For example if I\nsaid \"Unless: {eq {Content-MD5 \"whatever\"}}, but also threw in an\nIf-Modified-Since, I'd get a 304 if the MD5 *didn't* match. In other\nwords, it's behaving more as an If:  header than an Unless: header. This\nseems rather contradictory. It seems to me the only change to Unless\nsemantics when an If-Modified-Since header is present should be to replace\nthe 412 with a 304 if it's found true, and add an {le {Modified-Since\n\"date\"}} to the internal parsing of the Unless header.\n\nIt seems to me that more often then not, browsers will send Unless and \nIf-Modified-Since at the same time, to maintain compatibility with HTTP \n1.0 servers. However, the current draft's wording on this is very oblique \nand hard to interpret. The above is what I think it says, but I have to \nadmit I'm really not sure. Is it possible to get some clarification on \nthis point?\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "> I too want to understand the question Alex raised during the WG meeting\n> re. why slow start and transmit windows don't prevent this from being\n> a non-concern?  Isn't the client naturally restricted from sending\n> a huge number of bytes before the server starts receiving some of the\n> bytes?  If there needs to be a timeout to protect poor implementations,\n> might it not be better to restrict the server from closing the connection\n> for some interval after sending the abort. The server can leave the\n> data unreceived or otherwise close the receive window but leave the\n> connection open.\n\nIf someone would test that theory on real systems and see if it works,\nthat would be a big help.  I specified the only known solution to the stated\nproblem -- not the only possible solution.\n\n......Roy\n\n\n\n"
        },
        {
            "subject": "Re: HTTP 1.1 Unless heade",
            "content": "> It seems to me that more often then not, browsers will send Unless and \n> If-Modified-Since at the same time, to maintain compatibility with HTTP \n> 1.0 servers. However, the current draft's wording on this is very oblique \n> and hard to interpret. The above is what I think it says, but I have to \n> admit I'm really not sure. Is it possible to get some clarification on \n> this point?\n\nI think this will be cleared up in general by switching to IF instead\nof Unless.  The purpose should be straightforward and should indeed\nmaintain compatibility with HTTP/1.0 behavior -- I probably just messed\nup a few words in the wee hours before the deadline.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "HTTP BOF at WWW4: Wednesday 7p",
            "content": "Hi all,\n\nI will be running an HTTP Futures birds of a feather session at the WWW4\nconference next week.  At that time, I would like to have the various\ngroups working on bits of HTTP/1.1 to explain to people (i.e., me)\nhow they intend to proceed, maintain communication with the other groups,\nand feed the results back into the main draft(s).\n\nSpeaking of which, this would be a good time for our new co-Chair\nLarry Masinter to explain what on earth I am babbling about. ;-)\n\n......Roy\n\n\n\n"
        },
        {
            "subject": "Grievances  Wid",
            "content": "I have a few concerns I'd like to address about the current state of the\nHTTP working group.  I tried posting these Tuesday, but by mailing software\nis malfunctioning.\n\nFirst I'd like to plead for someone to give an overview of what happened at\nthe night meeting of the HTTP WG.  Unfortunately, I had to leave early, and\nthe idea of waiting two months for minutes is unpleasant.  As such, I have\nsome feelings on the last minute agenda change at the Dallas IETF meeting.\n1) I think the addition was ill-timed.  The entire Spyglass group had plane\nreservations and hotel arrangements that required we leave in the late\nafternoon.  This is a small gripe, but we at Spyglass, myself in\nparticular, really, really wanted to participate, mainly because of the\nissues I'm going to address later. 2) The structure of the Agenda that\npushed the bulk of the HTTP 1.1 to the 2nd session was horrible!  No way\nshould we have spent the 1st session discussing the\ntunneling/session-ext/payment/etc drafts.  The HTTP 1.1 spec is THE issue\nof this group.\n\nThe next thing basically is a process issue.  Some of the informal hallway\nconversations I had confirmed my suspicions that the current progress of\nthe group is insufficient.  As I see it there are three things working\ntogether to impede our progress.  If only one of these things existed, it\nwould be perfectly acceptable, but together they spell doom:  1) the\naddition by the editor of large surprises to the HTTP 1.1 draft without any\ndiscussion in the working group, 2) the very infrequent revisions of the\nspec, 3) the monolithic-kitchen sink nature of the document.\n\nNow before you say: \"Putting things in the I-D is the way to\nsubmit an idea for consideration by the group!\", I must reiterate: that\nworks fine if you have frequent revisions on a small doc and no one is\nactually using the draft spec.\n\nPeople are trying to code to these documents.  The HTTP WG is not\nresponding to the needs of the marketplace.  The addition of Logic Bags,\nPATCH methods, two-stage responses and the like without both the input of\nthe group, and constant revisions of our \"main product\" is just not\nworkable.\n\nI am calling for a rough consensus from this working group that the\nbest way to progress on HTTP 1.1 is to a) require any large additions\nto the spec be discussed in this group - as a separate Internet Draft\nif convenient - before any introduction, and b) we commit ourselves\nto a somewhat only-what's-necessary Draft of 1.1 to get some kind of\nconsensus in a reasonable amount of time.  There was also a sentiment in\nDallas that not only should big chunks not be added, but that the HTTP spec\nshould be broken up, and the more I think about it, the more I have to\nconcur.\n\nI think I'll separate individual gripes about particular aspects of the\nspec to a separate message, and I'll propose some potential break-outs\nthere.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Grievances  Focuse",
            "content": "1)\nFirst things first: the most concrete complaint I have about the latest\nhttp draft is that we're still defaulting to qe=1 and qc=1 for encodings\nand charset when the headers are missing.  The way I see it, this isn't\njust my opinion, but rather, this is just broken.  A 1.0 user agent could\nask for a document, not be aware of encodings or charsets, click on stuff,\nand according to the current spec, receive \"stuff.txt.zip\".  1) The User\ngets garbage on their screen, and 2) they might not even have the\nopportunity to do a successful Save As, as a browser might have done\nend-of-line translations or high-bit stripping.  The default encoding and\ncharset during lack of headers should be 0.001.  This has been discussed\nbefore.  If logic bags can go in, certainly this change can be made.\n\n2)\nPersonally, I don't understand the existence of the version control\nmethods in the current spec.  This is a glaring example of something that\ncan easily be moved out to a separate draft.  The main implementors of\nthe HTTP spec in the marketplace are not even using PUT: PATCH and MOVE\nare really fringe niche interests.  I'd like to know which of the 42\nservers in Paul Hoffman's list support the PUT method.  2?  5 maybe?  I'm\nmaking a generous guess here.  Not even the File Upload RFC uses the PUT\nmethod.  These may have been in the interest of the original CERN uses of\nHTTP, but they are not to the HTTP implementors at large.  I have nothing\nagainst extension methods, PUT or others, but we need to shrink the HTTP\nspec if we are to progress successfully, and these could be removed.\n\n3)\nI heard a number of people express the feeling that Content negotiation is\na large, independent module that could be removed from HTTP for faster\nconsensus, and also something that should be accessible to other protocols.\nI think the example of enhanced SMTP was given.  Let's see that happen!\n\nI could go on but I think this is more than enough for the WG to digest for\nnow.  I encourage others to debate these points, and devise the details of\nother breakups that will allow individual drafts to come to a speedier\nrough consensus.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "Since I was unable to attend the IETF meeting (to be honest,\nI was also unwilling to go to Dallas), I am only vaguely\naware of the problem that the two-phase approach is trying\nto solve.\n\nApparently, the problem is that a server may want to refuse\nto accept a large entity body from a client, to avoid being\n\"deluged by bytes.\"  Is this the correct (and only) problem\nthat two-phase send is trying to solve?  If not, would someone\nplease define the problem?  Anyway, I'll assume that I have\nthe right definition.\n\nIn other contexts, people make the distinction between \"optimistic\"\nand \"pessimistic\" approaches.  (For example, concurrency control,\nparallel simulation event processing, etc.)  Optimistic approaches\nassume that everything will go well, and then if things don't go\nwell, you pay something to fix the damage.  Pessimistic approaches\nassume that things will go badly, and always pay something to\nprotect against this ever happening.  Usually, pessimistic\napproaches pay less overhead for \"good events\" than optimistic\napproaches pay for \"fixing up after bad events\", but if bad\nevents are rare, then the optimistic approach is still a net win.\n\nRoy has taken a pessimistic approach to the byte-deluge problem.\nIn particular, every use of every method with a two-phase requirement\nmust pay either the arbitrary timeout period OR at least one\nround-trip time.  In other words, by taking this approach, we\nbuild in \"extra\" delay to every POST (etc.) invocation for millions\nof users for many years.  Hmm.\n\nThe optimistic approach would be more along the line of \"if the\nserver decides that the entity body is too large, it can do something\nfairly expensive to solve the problem\".  Ideally, the \"solution\"\nwould include making sure that the client does not continually\nretry its large POST (or whatever).\n\nThe optimistic approach makes sense if we expect that most of the\ntime, servers will accept the POST (or whatever), and so removing\nthe extra RTT is a net win, even if it makes someone's life harder\nin those few cases where a POST is large *and* the server doesn't\nwant to see it.  Since we presumably want to avoid congestive collapse\nof the server, we ought to look for a solution that forces the costs\nof this case onto the client.\n\nDavid Morris suggests (apparently following a suggestion from \"Alex\"):\n> If there needs to be a timeout to protect poor implementations,\n> might it not be better to restrict the server from closing the connection\n> for some interval after sending the abort. The server can leave the\n> data unreceived or otherwise close the receive window but leave the\n> connection open.\n\nAnd Roy responds:\n    If someone would test that theory on real systems and see if it\n    works, that would be a big help.\n\nOne part of the theory, at least, is true by the definition of TCP\n(and all other reliable transport protocols that I know about):\nthe receiver can avoid being deluged by data simply by not accepting\nanything, since the flow-control mechanisms are specifically designed\nfor this purpose.  Some possible ways to accomplish this are:\n(1) use a small or zero receive window\n(2) don't do any read()s from the input stream\n(3) close or reset the connection.\n\nSo we have a way to avoid the deluge for the current connection.\nWe need two other things:\n(A) some way to ensure that the client doesn't simply\ntry again with a large POST.\n(B) some way to indicate to the user that the operation\nisn't allowed.\n\nUnfortunately, I don't expect that the other part of the theory is\ncorrect.  I.e., that it would work reliably to simply keep the\nconnection open for some arbitrary timeout, after sending an abort.\nThis is because I suspect that some clients which simply write() the\nentire POST body before bothering to read() any response will simply\nblock.  No matter how long the server holds the connection open, the\nclient will never read the response.  (We could try to have the spec\ninsist that a conforming 1.1 client must always be looking for\nasynchronous responses, but I doubt this would fly.)\n\nSo here's an alternate suggestion (mostly as a way to start people\nthinking about additional suggestions, although I think it would\nwork).\n\nIf the server doesn't want to receive the large body,\nit immediately replies with its 4xx or 5xx response,\nand immediately closes (not resets) the connection.\n\nIf the client manages to read the 4xx or 5xx response,\nit must honor it and should reflect it to the user.\n\nIf the client simply sees the transport connection\ndisappear, then the language in section 1.4, \"Overall\nOperation\",\n   Both clients and servers must be capable of handling cases\n   where either party closes the connection prematurely, due to\n   user action, automated time-out, or program failure. In any\n   case, the closing of the connection by either or both\n   parties always terminates the current request, regardless of\n   its status.\napplies with one variation:\n   if the aborted request is a POST, PUT, PATCH, etc. (i.e.,\n   any request with a non-empty entity body) then if the\n   client repeats the aborted request, it MUST include\n   the request-header:\nConnection: two-phase\n   and then follow Roy's two-phase protocol: wait for\n   a response or for an specified timeout period before\n   proceeding.\n\nA client MAY use\nConnection: two-phase\nat other times, on its own initiative, but MUST use it\nwhen repeating a request because of a prematurely-closed\nconnection.\n\nWe might also want to do this:\nServers rejecting an entity-body for reasons of size\nmust respond with \n413 Request too large\n\nOnce a client has received a 413 response from a server,\nit SHOULD always use\nConnection: two-phase\nand the two-phase protocol with that server in the future.\nThis is actually not a big cost, because we know that\na server which has sent us a 413 response is an HTTP 1.1\nserver, and so the client should (almost?) never need to wait\nmuch longer than one RTT for the two-phase mechanism\nto complete.\n\nSeveral people have pointed out that 5 seconds seems like a\npoor choice for a time.  It appears to be a compromise between\n\"give the server enough time to respond\" and \"don't delay too\nlong waiting for HTTP 1.0 servers, which won't send a Continue\nresponse.\"  But in the optimistic approach, we only invoke the\nN-second wait in the (hopefully) rare case of a prematurely\nclosed connection.  So N could be somewhat larger than 5 (say,\n10 or 20) without impacting the normal case.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "|Well, I assume you are proposing (hypothetically, anyway) that the\n|server rewrite the HTML it's transmitting to add the WIDTH and HEIGHT\n|attributes.\n\nIf the server would have to open each image in any event in order to pack it \ninto a multipart message.  It could have access to the image size parameters\nand include them in the a MIME header.\n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "In message <9512071918.AA08028@rafiki.spyglass.com>, Daniel DuBois writes:\n>\n>The next thing basically is a process issue.  Some of the informal hallway\n>conversations I had confirmed my suspicions that the current progress of\n>the group is insufficient.  As I see it there are \n\nAre you volunteering some resources?\n\nSome venting in \"post mortem\" mode is healthy, but let's keep in mind\nthat this is a volunteer organization.\n\nSome of your comments directly addressed works-in-progress, and I\nappreciate those comments (I disagree with some of them -- in\nparticular, the notion that content negotiation is subordinate to the\nmain focus of the HTTP spec...)\n\nBut the general exhortations to the group aren't really all that\nuseful, are they?\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "    Date: Thu, 07 Dec 1995 15:25:10 -0500\n    From: \"Daniel W. Connolly\" <connolly@beach.w3.org>\n\n    But the general exhortations to the group aren't really all that\n    useful, are they?\n\nI think they are.  It is quite a different thing to develop new standards\nthan to label as standard an existing practice.  The former requires much\nmore work, organization, and a strong voice every now and then to bring the\nflock back in from its wanderings.\n\nIt seems to me that both the HTTP and HTML WGs have yet to prove themselves\nin terms of developing extensions to defacto standards that actually get\nwidely implemented.  Thus, I think Daniel's remarks are quite apropos.\n\nRegards,\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "Daniel W. Connolly writes:\n > In message <9512071918.AA08028@rafiki.spyglass.com>, Daniel DuBois writes:\n > >\n > >The next thing basically is a process issue.  Some of the informal hallway\n > >conversations I had confirmed my suspicions that the current progress of\n > >the group is insufficient.  As I see it there are \n > \n > Are you volunteering some resources?\n > \n > Some venting in \"post mortem\" mode is healthy, but let's keep in mind\n > that this is a volunteer organization.\n > \n > Some of your comments directly addressed works-in-progress, and I\n > appreciate those comments (I disagree with some of them -- in\n > particular, the notion that content negotiation is subordinate to the\n > main focus of the HTTP spec...)\n > \n > But the general exhortations to the group aren't really all that\n > useful, are they?\n > \n > Dan\n > \n\nI think they're somewhat useful, in that:\n (a) the big changes to the spec do not seem to mirror the conversation on the\nworking group mailing list, or even to follow the process described\nby Roy.  (Are there working implementations of Unless and\nlogic bags that I don't know about?  What about the new methods?)\n (b) issues that have been brought up repeatedly in the working group\nare not resolved by the spec.  As the perpetrator of some of those\nissues I am not going to harp on them more right now, except\nto say that I think issues that could be resolved rather\neasily are too often left unresolved.\n (c) by ignoring things like Netscape's cookies, the working group\nstands in danger of simply being bypassed by the market.  If \nthe working group continues not to respond to such issues\n(whether they've been submitted as I-D's or not) then it will simply\nbecome irrelevant over time. I think it would be unfortunate\nto abdicate control of this protocol to any single commercial entity\nby such obvious inaction.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Byte ranges (was Re: Logic Bag concerns",
            "content": "    According to Mike Braca:\n    > Jeffrey Mogul wrote:\n    > > In other words, IN ALL CASES what the cache-validator means is\n    > > that if it doesn't match, send the whole object.  What Range:\n    > > modifies is the behavior of GET when the cache-validator does\n    > > match.  And (if you like) you can think of a missing Range:\n    > > header as implying \"Range: bytes=0-0\", so there is really only\n    > > one meaning.\n    > \n    > (I assume you meant \"Range: bytes=0-\")\n    \n    No, I think he meant an implicit Range: 0-0 with the intent that\n    nothing be sent (but actually it would send one byte).\n\nCorrect.  I should have written that a missing Range: header implies\n\"Range: bytes=1-0\" ... or more properly, I should have simply written\nthat a missing Range: header implies \"the empty set.\"\n\n    I think this illustrates the potential for confusion that could be\n    eliminated with an If-cache-valid: <authenticator> and\n    If-cache-stale: <authenticator>.\n\nNo, it illustrates the potential for confusion when I don't think\nhard enough about what I am typing :-).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "I am not quite as aggrieved as some people appear to be, but\nI think there are several procedural issues that we need to\naddress.  In some cases, even if we don't all agree that the\ncurrent procedures are broken, I think we should all agree\nthat as long as a substantial number of people are unhappy\nabout the procedures, that in itself is a problem.\n\nShel lists these issues:\n    (a) the big changes to the spec do not seem to mirror the\n    conversation on the working group mailing list, or even to follow\n    the process described by Roy.\n\n    (b) issues that have been brought up repeatedly in the working\n    group are not resolved by the spec.\n\n    (c) by ignoring things like Netscape's cookies, the working group\n    stands in danger of simply being bypassed by the market.\n\nDaniel DuBois listed others, including (if I may paraphrase)\n    (d) it may be a mistake to introduce a large set of new\n    issues in the form of a monolithic I-D for the whole spec.\n\nI would translate these concerns into specific recommendations\n(\"constructive criticism\"):\n\n(1) Like it or not, some of us cannot attend IETF or WWW meetings in\nperson.  The mailing list MUST remain the document of record.  This\nmeans that, while it makes a lot of sense to discuss things in\nface-to-face meetings, those \"owning\" a specific and significant aspect\nof the spec are duty-bound to summarize the discussions for the mailing\nlist in a timely manner.\n\nWhat's the point of building the global Internet if we don't use it to\nget our own work done?\n\n(2) Someone in the \"working group management\" MUST maintain a list of\nunresolved issues, preferrably as Web page so that people can check the\ncurrent status.  I'm willing to let the WG chair(s) decide whether an\nissue is resolved or not; that's necessary for forward progress as long\nas they don't abuse this power.\n\n(3) Major additions and major changes to the spec MUST be discussed\nindividually.  In some cases, a separate I-D might be the best approach\n(we can combine several I-Ds into a complete spec later on).  In other\ncases, simply describing the addition/change in a distinct email\nmessage to the mailing list should suffice.\n\n(4) To the extent that the spec involves both principles (such as\ncaching or anti-byte-deluge, for example) and specific implementations\nof these principles, we SHOULD agree on the principles BEFORE\nfighting over the specifics.\n\n(5) Proposed additions and changes to the spec MUST be accompanied by a\nwritten rationale, and SHOULD be accompanied by at least an attempt to\nanalyze the proposal with respect to possible alternatives.  I would\nlike to see brief summaries of these rationales and analyses included\nin the spec itself, since this would aid implementors in understanding\nthe spec, and would probably avoid some future arguments.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "OK, I'm nearly convinced that an extension mechanism such as\nIf: <logic-bag>\ncould be a good thing to have in the HTTP 1.1 spec.\n\nI'm less convinced that we have actually found useful purposes\nfor it, or that we have a clean semantics for it.  The suggestions\nmade by Sankar Virdhagriswaran (iterators, upgrades, and transactions)\nare all (in my opinion) things that are either possible to do without\nIf:, or things speculative enough that they should probably wait until\nsome later revision of the spec.  (Getting \"transactions\", in the\nformal sense of the term, right almost certainly requires more support\nthan If: provides).\n\nMeanwhile, Koen Holtman has pointed out that the current specification\nhas some potential problems (e.g., doing lexicographic comparisons\non dollar-denominated amounts or dates; defining at what point in\nthe process of serving a request the conditional should be applied).\n\nKoen also pointed out that If: is (in some applications) \"basically a\nbandwidth-saving device\" (although if it could be used to implement\ntransactions, that would be different).  I think this observation\nmight lead us to a clearer set of design principles:\n\nSince If: is being proposed primarily as an extension mechanism,\nand we don't quite know how to make it work yet, how about making\nit optional?  That is,\n(1) servers need not support If:\n(2) If: should only be used (in HTTP 1.1) as a technique\nfor optimizing performance, and never for ensuring\ncorrectness.\n(3) Features that apply to all or many requests\nshould be implemented as specific headers, rather than\nIf: extensions, since this allows more efficient\nimplementation and probably more efficient on-the-wire\nencodings.\n\nTo my mind, this means that cache validation and byte ranges\nought to be standalone headers, and not implemented using If:\nWe know we need them, and they are simple enough to implement\nwithout If:\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "In message <9512072137.AA01343@acetes.pa.dec.com>, Jeffrey Mogul writes:\n>\n>(2) Someone in the \"working group management\" MUST maintain a list of\n>unresolved issues, preferrably as Web page so that people can check the\n>current status.\n\nThis is a great idea, but I've tried to do it, and found it to be\nexceedingly difficult. I expect some suitably easy-to-use issue\ntracking tools to hit the web any time now, what with all the\ncollaboration experiments and Java applets wizzing around.\n\nIf anybody has a solution they really like, please let me know!\n\n>(5) Proposed additions and changes to the spec MUST be accompanied by a\n>written rationale, and SHOULD be accompanied by at least an attempt to\n>analyze the proposal with respect to possible alternatives.  I would\n>like to see brief summaries of these rationales and analyses included\n>in the spec itself, since this would aid implementors in understanding\n>the spec, and would probably avoid some future arguments.\n\n\nThis is another good idea. Again, I've found that it's a LOT of work.\nI think you'd need another editor to take on the work of doing\na rationale. Any volunteers?\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": ">>>>> Jeffrey Mogul writes:\n\nJeff> If the server doesn't want to receive the large body, it\nJeff> immediately replies with its 4xx or 5xx response, and\nJeff> immediately closes (not resets) the connection.\n\nJeff> If the client manages to read the 4xx or 5xx response, it must\nJeff> honor it and should reflect it to the user.\n\nI really don't like the \"If\".  401 (not to mention 3xx) -require- the\nclient to read the response, because they aren't fatal, and they\ncontain critical information to making the transaction succeed!\nIf there is any doubt that the client might not read the response,\nthen we are doing something wrong.\n\nThe protocol should be deterministic in any case where the physical\nconnection hasn't been screwed up.  I'm all for fault tolerance and\nfallback mechanisms, but they should not be the \"standard\" way of\nmoving data around.\n\nAnyway, everyone keeps talking about the 4xx and 5xx -fatal- cases.\nThese are the easy ones.  In any scenario, please think in terms of a\nharder case, such as the client wants to PUT a giant chunk of data to\na location that requires authentication, or tries to POST to a\nresource that has moved, etc.\n\n-Roger\n\nRoger Gonzalez                    NetCentric Corporation\nrg@server.net                     56 Rogers Street\nhome   (617) 646-0028             Cambridge, MA 02142\nmobile (617) 755-0635             work (617) 868-8600\n\n\n\n60 09 3A EE FE 6A 1E CC   -pgp-   B7 F7 6B 0F 00 1D 01 C7 \n\n\n\n"
        },
        {
            "subject": "Confusion over caching (was Re: Logic Bag concerns",
            "content": "Roy writes:\n   What is the actual difference in development and run-time expense\n   between implementing a general mechanism for preconditions such as\n       IF: {eq {Content-MD5 \"89787jhlkr8r87y98437==\"}}\n   or\n       IF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 GMT\"}}\n   over implementing a special-purpose mechanism for cache validation\n       Content-Validator: \"89787jhlkr8r87y98437==\"\n   and\n       If-Validator: \"89787jhlkr8r87y98437==\"\n\nThis entirely misses the point I've been trying to make over and\nover and over again.  If you accept the principle that\ncache validation ought to be done using an opaque validator,\nand that the design of caching is *central* to the operation of\nHTTP, then this is hardly a \"special-purpose mechanism for cache\nvalidation.\"  (This is why I wrote in another message today\nthat agreeing on principles should be done before fighting about\nsyntax.)\n\nRemember that (in the opaque-validator model) the server *could* use\nan MD5 hash or a modification date as the cache-validator.  And\nthis means that the decision about how to define cache validity\nrests with the origin-server, which knows the semantics of the\ndata, rather than with the client.\n\n   keeping in mind that no existing server currently provides a\n   Content-Validator header field and that the field, whatever it\n   contains, must be duplicating some other entity-header.\n\nI think you are confused.  It *may* duplicate some other entity-header,\nbut it might not.  For example, the server may choose to encode\nthe file modification date in (seconds.fractional_seconds) as\na string of 9 or 10 hex digits, which is both shorter than the\n29 bytes of an HTTP date, and also more precise.  Or the server\nmay use a proprietary hashing function (i.e., not MD5) which might\nbe more efficient for the purpose.  Or the server might use some\ncombination of its internal index for an item in a database,\nplus a database update-version number, if it isn't storing\nmodification timestamps for each database entry.\n\nMy point is that we should not restrict the cache-validator\nto something that can be expressed using just the other\nstandard header fields, both because these fields may not\ninclude the requisite information, and because doing so\nwould require the *client* to decide how to determine if\na cached object is valid, when this is most certainly the\nrole of the origin-server.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "    What value were you using for 2MSL in the traces?\n    \nThe \"legal\" value, 240 seconds.  Although my simulator allows me\nto change this.  And (shhh, don't tell anyone) I think DEC OSF/1,\nwhich was what we were actually running, uses 120 seconds.\n\nIf you want a few simulations done with different values, let me\nknow.  I suspect the main difference is that the number of TIME_WAIT\nentries depends approximately linearly on this value.\n\n    Also, did you assume that the effective bandwidth for a given path\n    remained the same between the traces and the simulated run?\n    \nI assumed that the \"request\" durations would be the same.  Since\nthe server CPU and elapsed times were a tiny component of the overall\nelapsed times, server loading should not matter.  On the other hand,\nyou're right to point out (I assume this is what you mean) that the\nactual durations would likely be shorter, since the requests would\ngo faster without the cost of connection setup or slow-start.\n\nShorter connections should generally mean fewer active ones at once,\nwhich is \"good\".  So my failure to account for this effect is a\nconservative error, given that my goal is to argue in favor of\npersistent connections.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Byte ranges (was Re: Logic Bag concerns",
            "content": "> Maybe it's simpler to think about this if you pronounce\n> \"GET\" as \"Make sure my cache for this object is up to date\".\n\n    Well, yes, but an empty cache is just as valid as one full of\n    up-to-date objects :)\n\n    I hate the notion of automatically dumping the whole file onto the\n    net.  First of all, it is not intuitive to an ex-OS hacker like\n    myself that if something in the cache goes stale, I reload the\n    whole (file,...) instead of simply flushing the object from the\n    cache.\n\nAs a current OS hacker, I can see the point you are trying to make,\nbut I think you are drawing a false analogy.\n\nMost caches used in CPU hardware and operating systems are not\nexplicitly checked for validity on each use, against a \"master\"\ncopy of the data.  This means that if the hardware or OS suspects\nthat a cached item is not valid, it must either remove it from\nthe cache or mark it with a valid bit.\n\nAlso, in the case of a CPU or OS, the invalidation of an item\nis \"local\" to the system, and so can normally be detected immediately.\n\nTo be more concrete: when a CPU loads a line from its cache, it\ndoesn't have the time to ask the main memory controller to tell it\nif the cache line is still valid.  It has to have a fast, local\ncheck (i.e., a valid bit) or the cache would be pretty useless.\nAlso, since (especially in a uniprocessor) the CPU itself is\nresponsible for most changes to memory locations, it can easily\nclear the valid bit synchronously on a write.  The situation becomes\nmore complex with DMA and/or multiple processors, of course.\n\nIn our situation, on the other hand, the system containing the\ncache (client or proxy) is quite remote from the system that\nmodifies objects (the origin server) and so it's not at all\nreasonable to expect the client to know when to flush the cache.\nThis is why we have already (implicitly) agreed on a caching\nmodel for HTTP in which each cache entry is validated on demand,\nrather than when the original object is changed.  This is a\nform of \"late binding\" or \"lazy evaluation\": don't do work up\nfront if you might not need to do it at all.\n\n    More pragmatically, we are working on displaying potentially huge\n    SGML documents via HTTP, and if a 20 MB file that I am pulling\n    subtrees out of gets changed, I don't want to trigger a send of the\n    whole file.  Perhaps Ari can tell me how they plan for this to work\n    in the PDF case -- I may be missing something obvious here!\n\n    What would make sense to me is to define something like a \"305\n    Modified\" response that passes me back the new cache-validator.\n    Then _I_ can decide whether to fetch the whole file, or an index or\n    whatever. What are the drawbacks to such a scheme?\n    \nDoesn't the HEAD method do what you want?  That is, if you really\nwant to cheat on the principle that retrieved byte ranges should\nbe consistent with the version in the client's cache, then you\ncould do\nHEAD <url>\nwhich should return exactly the same entity-headers as GET (according\nto the current draft of the spec), including the Cache-validator.\n\nOnce you have the Cache-validator, you could then use the\nGET method with Range: and Cache-validator: headers to load\nspecific parts of the 20 MB file.\n\nI would make one concession to make this work a little better.\nIf the underlying 20 MB file changes between retrievals of\nseveral Ranges, you probably want to know that (right?) but\nyou clearly don't want to get back the entire document in\nresponse to Get+Range+Cache-validator request.  Therefore,\nI propose a modification of the Range: header to be\nRange = \"Range\" \":\" byte-range [\"unconditional\"]\nActually, the word \"unconditional\" is too long; \"U\" would\nsuffice.\n\nSo\nGET url\nRange: 0-60\nCache-validator: XYZZY\nwould still mean\nif (cache-validator == \"XYZZY\")then\n    send bytes 0-60\nelse\n    send whole file\n\nbut\nGET url\nRange: 0-60 U\nCache-validator: XYZZY\nwould still mean\nif (cache-validator == \"XYZZY\")then\n    send bytes 0-60\nelse\n    send 305 Modified\n\nFinally (and this is the original reason I was going to propose\nthe unconditional Range: header, but your message arrived first),\nthis\nGET url\nRange: 0-60 U\n(i.e., no Cache validator supplied by the client) would simply\nmean\nsend bytes 0-60\nI think this would be a great solution to the problem of\nhow to implement Netscape's early rendering of bounding boxes\nwith the single-persistent-TCP-connection model.\n\nHere is how that would work (assuming that the TCP connection\nis already open)\nStep 1:\n    do\nGET file.html\n    then parse out a list of image files.\n\nStep 2:\n    for each GIF file do\nGET fileN.gif\nRange 0-<size of GIF header> U\n    for each JPEG file do\nGET fileN.jpeg\nRange 0-<size of JPEG header> U\nonce you have all the responses to step 2, you can render *all*\nthe image bounding boxes, not just the first 4.\n\nStep 3:\nCompare the Cache-Validators returned with the responses from\nstep 2 to the cache-validators stored with the cached images,\nif any.  Since these are opaque values (in my model), this is a\nsimple equality check.  You now not only know the correct bounding \nboxes for each image file, you also know which files are valid\nin your cache.  (Note that if you have cached image files with\nfuture Expires: dates, then you don't need to include them in\nsteps 2 or 4.)\n\nStep 4:\n    for each GIF file do\nGET fileN.gif\nRange <size of GIF header>- U\n    for each JPEG file do\nGET fileN.jpeg\nRange <size of JPEG header>- U\nNote that you can issue the GETs for step 4 before receiving\nthe responses from step 2.\n\nThis has these advantages:\n(1) works fine with just one persistent TCP connection\n(2) renders any number of bounding boxes, not just 4\n(3) does not require the HTTP server to understand\nimage formats, and does not require the creator of the\nHTML file to know what the image sizes are.\n(4) does not cost very much, since each image byte\nis only retrieved once, and no unnecessary round trips\nare required.\n\n-Jeff\n\nP.S.: If people don't like the assymetry between the semantics\nof the unconditional Range header with and without the Cache\nvalidator, then here's a somewhat clearer proposal:\n\nRange = \"Range\" \":\" byte-range [\"U\" | \"V\"]\n\nU = unconditional, V = unconditional if valid\n\nSemantics:\nGET url\nRange: 0-60\nCache-validator: XYZZY\nwould still mean\nif (cache-validator == \"XYZZY\")then\n    send bytes 0-60\nelse\n    send whole file\n\nGET url\nRange: 0-60 V\nCache-validator: XYZZY\nwould mean\nif (cache-validator == \"XYZZY\")then\n    send bytes 0-60\nelse\n    send 305 Modified\n\nGET url\nRange: 0-60 U\nwould ignore the cache validator and would simply mean\nsend bytes 0-60\n\nCombining these all into one piece of server pseudo-code:\n\nGET url\n[Range: range-value [range-modifier]]\n[Cache-validator: validator-value]\n\nwould mean\n/* set up defaults */\nif (Range: not present) {\nrange-value = empty set;\nrange-modifier = \"None\";\n}\nif (Range: present but range-modifier not present)\nrange-modifier = \"None\";\nif (Cache-validator: not present) {\nvalidator-value = null value;\n}\n\n/* compute what to send */\nif (range-modifier == \"U\" \n     or\nactual-cache-validator == validator-value) {\n    send bytes described by range-value;\n}\nelse {\n    if (range-modifier == \"V\")\nsend 305 modified;\n    else\nsend whole file;\n}\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "I'm sorry that the two-day delay in getting the minutes out for the\nafternoon session have caused you some concern, but I promise I'll be\ndoing so soon.\n\nI do think there've been concerns about the progress of the group so\nfar, but I hope that we've established a mechanism to make more rapid\nprogress. More news when the minutes are out.\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "    Jeff> If the server doesn't want to receive the large body, it\n    Jeff> immediately replies with its 4xx or 5xx response, and\n    Jeff> immediately closes (not resets) the connection.\n    \n    Jeff> If the client manages to read the 4xx or 5xx response, it must\n    Jeff> honor it and should reflect it to the user.\n    \n    I really don't like the \"If\".  401 (not to mention 3xx) -require- the\n    client to read the response, because they aren't fatal, and they\n    contain critical information to making the transaction succeed!\n    If there is any doubt that the client might not read the response,\n    then we are doing something wrong.\n    \nGood point, although this is actually the result of an ambiguity\nin what I wrote.\n\nWhen I wrote \"4xx or 5xx response\", I was essentially qouting from\nRoy's draft.  It occurred to me while I was writing that message\nthat the two-phase mechanism really needed an explicit status\ncode for use here, which is why I ended up including this\nbit:\n    We might also want to do this:\n        Servers rejecting an entity-body for reasons of size\n        must respond with\n                413 Request too large\nSo I would restate the phrases you object to as:\n\n    If the server doesn't want to receive the large body, it\n    immediately replies with a 413 response, and\n    immediately closes (not resets) the connection.\n    \n    If the client manages to read the 413 response, it must\n    honor it and should reflect it to the user.\n\nIn other words, the server should only prematurely close the\nconnection for this specific purposes (request too large)\nand SHOULD NOT prematurely close the connection for other\n4xx and 5xx statuses (although the server is always allowed\nto prematurely close the connection if it has to ... for example,\nif it crashes and reboots!)\n\n    The protocol should be deterministic in any case where the physical\n    connection hasn't been screwed up.  I'm all for fault tolerance and\n    fallback mechanisms, but they should not be the \"standard\" way of\n    moving data around.\n    \nI think my proposal is fully deterministic if no networking\nfailures occur, although it does require at least one retry\nin some rare cases.\n\n    Anyway, everyone keeps talking about the 4xx and 5xx -fatal- cases.\n    These are the easy ones.  In any scenario, please think in terms of a\n    harder case, such as the client wants to PUT a giant chunk of data to\n    a location that requires authentication, or tries to POST to a\n    resource that has moved, etc.\n\nThe real question here is \"if there are two or more reasons why\na request would fail, which one is reported in the status code?\"\n(this is because each response carries exactly one status code).\n\nOne answer would be \"it doesn't matter\" (more precisely, \"it's\nup to the server implementer\").  Take your first example: if the\nclient wants to PUT a zillion bytes to a location that requires\nauthentication, then does it really matter why it fails?  Either\nway, it can't be done.\n\nWe could take that approach, but I think it might be too crude.\nFor example, suppose that a server will accept a 1 KB chunk\nfrom anyone, but wants authentication before it will accept\na 20MB chunk.  If a 20 MB Put arrives without authentication,\nit would be more useful to respond with 401 Unauthorized than\n413 Request too large, since if the client sees the 401 it\ncould presumably retry after authentication, whereas if it\nsimply sees the 413, it has no clue.\n\nEven in this scenario, it may be the case that the server\nimplementor might have some reason for sending the 413\nresponse in preference to the 401 response (e.g., an organizational\nsecurity policy that tries to obscure the reasons for\na failure), so I think the HTTP spec could recommend a\nset of status-code precedences, but not require it.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "At 03:54 PM 12/7/95 PST, Larry Masinter wrote:\n>I'm sorry that the two-day delay in getting the minutes out for the\n>afternoon session have caused you some concern, but I promise I'll be\n>doing so soon.\n\nI was of the impression that the standard time for minutes to be released\nwas around 6-8 weeks.  I in no way meant to insinuate that someone was\nmoving slowly - I just thought there was some outside-the-WG IETF\nbureaucracy behind the scenes with regard to minutes' distribution.  I must\nbe working from 2nd or 3rd hand information.\n\nSorry, no offense intended.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "    >(2) Someone in the \"working group management\" MUST maintain a list of\n    >unresolved issues, preferrably as Web page so that people can check the\n    >current status.\n    \n    This is a great idea, but I've tried to do it, and found it to be\n    exceedingly difficult. I expect some suitably easy-to-use issue\n    tracking tools to hit the web any time now, what with all the\n    collaboration experiments and Java applets wizzing around.\n    \nI'm not looking for any fancy tracking tools.  I'm looking for a list\nthat is kept reasonably up-to-date by a human.\n    \nIt doesn't even have to be in HTML format.\n\nI think this is part of the task of the working group chair(s).  It\nsets the agenda for discussion, and (as I wrote in my original message)\nsomeone has to make decisions about what goes on and comes off.\nIf anyone volunteered to maintain this list, they would effectively\nbe running the working group.\n\n    >(5) Proposed additions and changes to the spec MUST be accompanied by a\n    >written rationale, and SHOULD be accompanied by at least an attempt to\n    >analyze the proposal with respect to possible alternatives.  I would\n    >like to see brief summaries of these rationales and analyses included\n    >in the spec itself, since this would aid implementors in understanding\n    >the spec, and would probably avoid some future arguments.\n    \n    This is another good idea. Again, I've found that it's a LOT of work.\n    I think you'd need another editor to take on the work of doing\n    a rationale. Any volunteers?\n\nNonsense (with all due respect!).  Anyone who writes a specification\nshould be expected to justify that specification.  If the rationale\ndoesn't appear in the spec per se, it will be dragged out of the\nspec writer on the mailing list (or perhaps at a meeting, but then\nthe whole issue will be re-opened on the mailing list by those of\nus not lucky enough to attend).\n\nI'm not arguing for a separate rationale document, I'm arguing that\nall non-obvious parts of the spec should include some rationale\ninline.  Not a PhD dissertation; just something explicit.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "HTTP WG meeting in Dallas: Summar",
            "content": "The HTTP working group met twice at the Dallas IETF. This is the\nsummary of the WG meeting; minutes will be available tomorrow. (Dave\nRaggett has produced them from notes taken by the notes-takers, but I\npromised to review them before sending them out.)\n\nA revised set of milestones for the charter will be submitted\nimmediately.\n\n- The HTTP/1.0 draft had been rejected as a 'best current practice'.\n  The HTTP/1.0 draft will be revised to become an 'informational RFC'\n  which describes common current practice.\n\n- The HTTP/1.1 draft will be reviewed independently by separate\n  sub-groups. The sub-groups are chartered to review the HTTP/1.1\n  draft for text related to their issue, and propose changes to the\n  HTTP/1.1 draft that consist of either wording changes, or movement\n  of major chunks of HTTP/1.1 to separate documents, as appropriate.\n\n  The issues are:\n\n   * Persistent connections\n      (this contains all of the 1.1 proposals for keep-alive,\n       and maintaining connections to avoid TCP startup costs.)\n   * Cache-control and proxy behavior\n   * Content negotiation\n   * Authentication\n   * State management\n   * Range retrievals\n   * Extension mechanisms\n   * other new methods and header features\n\nThe volunteers for these activities, and details of the subgroup tasks\nwill be elaborated in the minutes.\n\n- Subgroups should conclude their work by Jan 96, in time to publish\n  their conclusions (or lack thereof) to the rest of the WG so that\n  new internet draft(s) for HTTP/1.1 will be ready in March 96 and\n  ready for Proposed RFC status by June 96.\n\n- Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n  consensus will revert to HTTP/1.0 status in 1.1 and be considered\n  for inclusion in HTTP/1.2.\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "On Thu, 7 Dec 1995, Jeffrey Mogul wrote:\n\n> One answer would be \"it doesn't matter\" (more precisely, \"it's\n> up to the server implementer\").  Take your first example: if the\n> client wants to PUT a zillion bytes to a location that requires\n> authentication, then does it really matter why it fails?  Either\n> way, it can't be done.\n\nWell there is the interesting situation where the server requires\nauthentication for a PUT AND also can't accept 1meg of data.\nThe way WWW/HTTP authentication works, the client may not know that\nauthentication is required until attempting the PUT and getting the\n401 unauthorized response.  HENCE, we have a situation where the\nclient could be presented two possible errors, one which it can possibly\ncorrect and one which it can't ever correct. In normal transaction flow\nauthentication would be verified and the semantics of the request would\nbe verified.  YET, what is the point of a server issuing an unauthorized\nresponse when it could/should/might know it's is never going to process\nthe transaction anyway.  As an end user, I'd be mightly annoyed if I\ndid what I believed was valid, was delayed for the password prompt in\nresponse to the 401 UNAUTHORIZED and possibly some substantial delay\nbecause of the bytes in the pipe the first time only to be then told that\nthe size of the data to PUT was too big.\n\nrom a protocol perspective it needs to be valid to issue the failures\nin either order because the size failure may be detected only when a\nprocessing 'CGI' program responds while the server detects the unauthorized\nsituation. It would be good however to encourage presentation of fatal\n(can't be overcome) errors before correctable failures, when possib\n\nTo backup .... as I understand the discussion, the purpose of the 2 phase\nsend was to increase the probablility that the client has a chance to\nreceive the response. The impression was that some/many TCP implementations\nmay close a connection in such a way that the response is flushed and\nnever seen from the client.  The second lesser concern was that by\nusing a short delay, there was less waste of bandwidth in the failure case.\n\nI like your (Jeffrey Mogul) suggestion for requiring the explict two phase\nrequest from the client in the quiet close case.  I would further\nextend your suggestion to get rid of the timeout or make it quite large\nand only allow the client to continue when the response arrives.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Wid",
            "content": "On Thu, 7 Dec 1995, Daniel W. Connolly wrote:\n\n> >(5) Proposed additions and changes to the spec MUST be accompanied by a\n> >written rationale, and SHOULD be accompanied by at least an attempt to\n> >analyze the proposal with respect to possible alternatives.  I would\n> >like to see brief summaries of these rationales and analyses included\n> >in the spec itself, since this would aid implementors in understanding\n> >the spec, and would probably avoid some future arguments.\n> \n> \n> This is another good idea. Again, I've found that it's a LOT of work.\n> I think you'd need another editor to take on the work of doing\n> a rationale. Any volunteers?\n\nI believe there is a fundamental difficulty where one person is both\nan important design idea contributor and the editor of the consensus\ndesign document. There is certainly a strong temptation to economize\non effort and record design ideas in the form of new text in the\ndesign document with the idea of editing the stuff out later. Particularily\nwhere it seems like the design represented in the document is broken.\n\nThe problem is however that this substantially increases the effort \nrequired of all the other designers who must review the document to\nlearn about changes as opposed discussing the ideas, reaching concenus\nand then only needing to review the document to insure it represents\nconcensus and looking for other issues of course.\n\nI think many of us believe the draft is a group effort with an editor, \nnot design proposals from a designer. That I think is the basic conflict.\nOne which I don't have the resource to resolve. But unless some organization\ncan relieve Roy of part of the responsiblity or otherwise help him have more\ntime to devote to the process, I think we must attempt to be tolerant of\nRoy's attempts to work efficiently AND perhaps Roy can help us feel more\nlike participants by making sure new ideas are presented to the list for\ndiscussion. It is generally accepted that discussion will flow best if\neach post consists of a single main topic so my suggestion would be that\nconcurrent with revealing a draft which includes new ideas, Roy would\nprovide a set of summary postings which briefly outline the new idea,\napproach, etc along with page references or whatever in the draft.\nIf there is rationale or background or a problem statement not appropriate\n(or simply not yet included) for the draft, that could be in the post.\n\nThen the draft doesn't contain suprises and the discussion can be much\nmore focused. In turn those of us on the list can focus back on content\nand not process and work effectively with Roy's efficiencies.\n\nJust a 'long' idea...\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: HTTP BOF at WWW4: Wednesday 7p",
            "content": "Excuse me!  I think it is quite unreasonable to expect that an unofficial\ngathering of folks interested in HTTP expect to hear from 'various groups\nworking on bits of HTTP/1.1' about how they plan to proceed. First of all,\nthe groups aren't formed yet and except for those of us who were in Dallas\nat the 2nd HTTP session, the group proposal is still unknown and possibly\neven rejected. Perhaps this should have been private mail to Larry Masinter\nasking if he would be available to introduce the concept and poposed \ntime-line.\n\nDave Morris\n\n\nOn Wed, 6 Dec 1995, Roy T. Fielding wrote:\n\n> Hi all,\n> \n> I will be running an HTTP Futures birds of a feather session at the WWW4\n> conference next week.  At that time, I would like to have the various\n> groups working on bits of HTTP/1.1 to explain to people (i.e., me)\n> how they intend to proceed, maintain communication with the other groups,\n> and feed the results back into the main draft(s).\n> \n> Speaking of which, this would be a good time for our new co-Chair\n> Larry Masinter to explain what on earth I am babbling about. ;-)\n> \n> ......Roy\n> \n\n\n\n"
        },
        {
            "subject": "WIDTH and HEIGH",
            "content": "According to Eric W. Sink:\n> --\n> \n> I've heard another option suggested:  Have the inline image sizes returned\n> by the HTTP server as headers when the base document is retrieved.  These\n> hints could look something like this:\n> \n> Image size: myfile.gif  image/gif  400x300  49152\n> \n> Where the four components on the line are the [partial] URL, the MIME type,\n> the image dimensions, and the size of the image file in bytes.  Current\n> browsers could ignore them.  Browsers who make use of them will have to be\n> able to cope with their absence.  Cool servers will keep a database of this\n> information around so that it doesn't have to be looked up on the fly.\n> \n> \n\nThis is a very good suggestion.  A simple MGET proposal which included \nheaders like this would be easy from the server side.  I'm not a client\nwriter, but I think it should not be too difficult there either.  One\ncould probably get user perceived performance similar to that of Netscape.\nI think it would be cleaner than multiple connections and would it should\ndecrease bandwidth use slightly.  What Netscape has that this wouldn't\nprovide is the ability to draw all the images simultaneously rather than\nsequentially, but I am not sure how important that is.\n\nThis proposal removes the onus of getting the image size from the author\nand puts it on the server which is probably a good thing.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "(hopefully) last message on WG proces",
            "content": "Now that people have vented about the WG process, I'd like to declare\nthe subject of 'Working Group' process off-limits, unless you've\ndiscussed the WG process with the WG chairs (myself and Dave Raggett)\nand still feel that you haven't gotten satisfaction. It's a matter of\nsaving the mailing list for technical issues.\n\n>   (a) the big changes to the spec do not seem to mirror the\n>   conversation on the working group mailing list, or even to follow\n>   the process described by Roy.\n\nIt has been acknowledged that this is annoying, and I hope to make\nsure that people won't feel left out of the process again. Still,\nwe're going to take the latest 1.1 draft as baseline, and propose\nchanges to it, even if those proposals mean winding back to 1.1.\n\n>    (b) issues that have been brought up repeatedly in the working\n>    group are not resolved by the spec.\n\nI'd like to hear (privately) about any issues that aren't the subject\nof discussion of any of the subgroups.\n\n>    (c) by ignoring things like Netscape's cookies, the working group\n>    stands in danger of simply being bypassed by the market.\n\nwe're not ignoring it.\n\n>    (d) it may be a mistake to introduce a large set of new\n>    issues in the form of a monolithic I-D for the whole spec.\n\noh, it's certainly confusing, but I think 'mistake' is too strong.\nLet's just agree that the current 1.1 draft is just a 'stake in the\nground' and that no one should attempt to code to it at the moment.\n\n> (1) Like it or not, some of us cannot attend IETF or WWW meetings in\n> person.  The mailing list MUST remain the document of record. \n\nAgreed, 100%\n\n> (2) Someone in the \"working group management\" MUST maintain a list of\n> unresolved issues, preferrably as Web page so that people can check the\n> current status.\n\nHowever, for the most part I'm hoping we can handle this by\nsub-groups. I'll keep a list of other issues, but I'll probably just\nsend it to the list from time to time. Send me (privately) any issues\nthat you think aren't handled by any of the subgroups (after the\nminutes get out.)\n\n> (3) Major additions and major changes to the spec MUST be discussed\n> individually.  In some cases, a separate I-D might be the best approach\n> (we can combine several I-Ds into a complete spec later on).  In other\n> cases, simply describing the addition/change in a distinct email\n> message to the mailing list should suffice.\n\nI got in trouble before having too many I-Ds in the URI group, so I'm\nwary of encouraging them. (Please ask before authoring a 'working\ngroup' Internet Draft.)\n\n> (4) To the extent that the spec involves both principles (such as\n> caching or anti-byte-deluge, for example) and specific implementations\n> of these principles, we SHOULD agree on the principles BEFORE\n> fighting over the specifics.\n\npeople have trouble sometimes talking about principles in the absence\nof implementation details. This sounds like a good idea but in\npractice it's hard. Have some sympathy.\n\n> (5) Proposed additions and changes to the spec MUST be accompanied by a\n> written rationale, and SHOULD be accompanied by at least an attempt to\n> analyze the proposal with respect to possible alternatives.  I would\n> like to see brief summaries of these rationales and analyses included\n> in the spec itself, since this would aid implementors in understanding\n> the spec, and would probably avoid some future arguments.\n\nThis is a nice idea, but we're often not up to it. If you have some\nspecific suggestions or questions on rationale that you think need to\nbe in the spec, please send them on as appropriate.\n\n\n\n"
        },
        {
            "subject": "DRAFT Minutes, HTTPW",
            "content": "This is a draft of the minutes for the HTTP-WG sessions at the Dallas\nIETF. My apologies for any errors, and thanks to Phill Hallam-Baker\nand Rohit Khare for taking notes, and Dave Raggett for putting the\nfirst draft of the minutes together.\n\nPlease send me corrections, updates, additions. (Especially if you\ngave a talk, send your slides or a summary of them.) Please send your\ncomments ASAP so that we can send this on as official minutes.\n\n================================================================\nThe meeting was held as two sessions, one in the morning with around\n100 people and one in the evening from 1930-2200. Minutes for the\nmorning session were taken by Phill Hallam-Baker and for the evening\nsession by Rohit Khare.\n\nMorning session:\n\nLarry Masinter has become co-chair of the WG along with Dave Raggett.\n\nRoy Fielding first presented, and we discussed, the status and plans\nof the working group.\n\nHTTP/1.0 was proposed as Best Current Practice but rejected by the\nIESG because didn't describe people's view of what was 'best', and\nthus it was an inappropriate status.\n\nRoy described his proposal that HTTP 1.1 be 'fast track' and that HTTP\n1.2 contain the extension mechanism and those bits that didn't make\nHTTP 1.1. He explained that he wrote things into the HTTP 1.1\nspecification even though they might be controversial, because it was\neasier to take things out than to add them in.\n\n* After some back and forth about a variety of options, the desire for\n  a stable core, and so on, the discussion led to the proposal that\n  HTTP/1.0 be re-written as an Informational RFC describing current\n  practice.\n\n  However, 'current practice' does not mean that we should document\n  all 50 versions of content negotiation as practices, merely use the\n  core of the 1.0 document as it stands for the parts that are\n  specified, and note that other features are not implemented\n  consistently. \n\nSome other points raised:\n* 1.0 is not actually much simpler than 1.1. 1.0 ignores proxies and\n  gateways, however.\n* If 1.0 isn't 'current practice', then all current implementations\n  will be non-conforming, since they all identify themselves as 1.0.\n  This is a bad situation.\n* The 1.0 specification has been useful as it is, because it is stable\n  and consistent even if not complete.\n* It's a bad idea to make something a standards track document and\n  then obsolete it immediately. 1.0 should not be a proposed standard;\n  1.1 should be.\n================================================================\nDave Kristol discussed his draft:\n   draft-kristol-http-state-info-01.txt\n\nIdea to support state full sessions in a stateless protocol, Similar\nto netscape cookies. Examples include shopping basket, subscription\nlibrary system (remembers what has been looked at).\n\nDefine new header State-info that carries state information between\nuser agent and origin server.\n\nRequirements\n    o Cache friendly\n    o simple to implement\n    o simple to use\n    o can be deployed quickly\n    o downward compatibility\n    o reliable\n    o protect privacy\n    o support complex dialogues\n    o enough cache control possible\n    o minimal risks when used with non conforming caches\n\nHe went through the proposal. There is some belief that this may\n'compete' with the Netscape 'cookies' method, but that is claimed not\nto be sufficiently documented. Comments during the meeting:\n\nLarry Masinter:\n> The privacy concern is not that the site that initiated the session\n> might know things about the user, but that other non-related sites\n> might be able to find out things about your sessions.\n\nPhill Hallam-Baker:\n> The security concern of allowing server to store data on the clients\n> disk should be addressed explicitly.\n(Ed: this was a subtle point)\n\nAlex Hopman:\n> There is an issue with regard to servers with multiple CGI services\n> which do not wish to share state information.\n\n(Dave Kristol responded that this is a server implementation issue.)\n================================================================\nRohit Kahre gave an overview of his PEP proposal\n draft-khare-http-pep-00.txt.\n\nComments at the meeting were:\n\nJohn Klensin (speaking personally and not as A.D.):\n> The trend in the IETF has been to strict versions something that\n> people can read on the back of a box. Once an extension list is in a\n> protocol we have a checklist. We have never handled a transition from\n> an extensions mechanism to a real verb well. PEP should consider\n> migration to real verb properly.\n\nDave Crocker:\n> Typically the view is that an extension mechanisms allow negotiation\n> of optional features.  A different view is to say we want to move from\n> here to this one place, and that the negotiation mechanism allows\n> movement of a functional core to a new base.  This is Dave's view of\n> the ESMTP work. Negotiation for combinatorial choices does not seem to\n> work but as a migration strategy it does.\n\n================================================================\nDon Eastlake described his Internet Draft:\ndraft-eastlake-internet-payment-00.txt\n\nHe's looked at the problem of doing payments on the internet taking a\npretty general view. Need a mechanism for specifying payment systems\nso that once there is an idea of prices can decide what the system to\nbe used will actually be. Does not attempt to constrain payments\nsystem.  Draft allows a payment or receipt to be put into header.\nAllows payments to be done over internet in a common framework.\nCurrent plan is to recast in terms of PEP.\n\nLarry Masinter:\n> In order to do reliable payments you need real (ACID) transactions,\n> but HTTP does not seem to have any transaction mechanisms; e.g., you\n> do not have ability to find out where an aborted transaction got to.\n> Would expect to provide a transaction mechanism on top of HTTP to\n> permit this to be used.\n\nDon Eastlake replied:\n> Would expect such matters to be handled by payment system rather\n> than http layer, e.g. server allows interogation of server to find\n> out where transaction got. Some lightweight payment systems will not\n> provide such guarantees, 2 cents may just be lost. Attempted to\n> restrict draft to just messages and receipts.\n\nDave Kristol expressed reservations about costs being embedded in\ndocuments and URLs.\n\nWe deferred further discussion to the Internet Payment BOF.\n\n================================================================\nAlex Hopman was scheduled to discuss draft-ietf-http-ses-ext-00.txt.\nHowever, half of that internet-draft is now in HTTP/1.1.\n================================================================\nAri Luotonen said only a few words about\ndraft-luotonen-ssl-tunneling-00.txt. It's been out for half a year.\n\nThe WG did not come to any conclusions on this issue.\n================================================================\nWe then talked about draft-luotonen-http-url-byterange-00.txt.\n\nThis draft inspired the work in the HTTP/1.1 draft on ranges. The\nURL-method for byte ranges will be abandoned, although it may go\nthrough an interim release by Adobe/Netscape.\n\nThe core HTTP/1.1 draft need not specify this if the methods and\nadditions can be done in a separate document and linked to 1.1 by\nreference; this is still an option. \n\nOne motivation for this feature was partial retrieval of PDF files;\nhowever, PDF as currently defined in application/pdf does not have\nbinary offsets or byte ranges. Apparently this is a feature of a new\nversion of to-be-released PDF.\n\nJohn Klensin pointed out that the byte range protocol should look at\ncheckpoint and restart experience.  It was likely that the real\nproblem is that document wants to be returned by parts, and that we\nneed to devise a way inside the document format to refer to parts and\nuse that for references.\n\n================================================================\nRoy Fielding then began a discussion of the HTTP 1.1 draft.\n\nComments in the first section included:\n\nLarry Masinter:\n\n> thought that caching was meant to be a transparent optimisation\n> technique. Nice simple semantics of http is being interfered with by\n> the clutter of discussing this intermediary optimisation. The\n> problem is that there are lots of different type of caches and hence\n> different types of optimisation between server and intermediaries.\n> We're describing cache control headers in terms of operational\n> effect rather than semantic differentiation; this does not\n> anticipate future cache techniques.\n\nRoy wasn't sure, these were just the header names, and the semantics\nwere described that way.\n\nSomeone suggested putting content negotation outside in a separate\ndraft.\n================================================================\nSession 2 ran 19:30 to 23:30\n\nSimon Spero discussed HTTP-NG.\n\nHTTP-NG is the first protocol endorsed by Dogbert. \"Resist and you\nwill be shot.\" (Scott Adams cartooned a Dogbert on the HTTP-NG draft.)\n\nHis presentation was a shortened version of the one to be presented at\nthe WWW4 Conference in Boston.\n\nThe primary modifications have been:\n\nSplit the document into Architecture and Basics documents.\n\nThe basic purpose is: negotiation, meta-information, and control.\n\nHighlights:\n\n-Uses SCP to multiplex sessions.\n-Transition strategies using DNS CNAME to indicate NG support.\n-Not a superset of HTTP/1.x\n-Just forwarding HTTP1.0 through ng encoding reduces packet count by 50 and speed by 180%\n-negotiation and profiles. Dictionaries on either side constitute\nshared state, and profiles are predefined dictionaries of prefernces.\nDictionary structure patterns can be reused in different exchanges.\n-Security Key exchange\n-reinventing several secrecy nd signature mechanisms\n-Get put and metainformation\n-HTTP/1.x metainfo + US MARC records.\n-can request metainfo for included or linked objects.\n-speculative sending of data can be enabled, experiments can reduce latency to 1/5th\n\nDave Kristol asks, why the radical departure from current practice?\n\nA: latency, latency, latency. Optimazation for very fast and very\nslow networks. Reducing number of active connections.Http/1.1 can do\npersistent connections, but can't do multiplex (which increases user\npercieved speed).\n\nKristol: you propose speculative transmission, which uses bandwidth.\nSecond, how many of the benefits can be captured by HTTP alone?\n\nA: That's what these numbers are from: using 1.x messages encapsulated\n   in SCP.\n\nAlex Hopmann:So why use [your testbed], just SCP'd HTTP/1.1?\n\nSimon: the compactnss of the records: we can put 20 cache updates in a\nsingle packets.\n\nTed Hardie: you're using CNAME, which is not neccesarily easy, since\nsome server operators may not have admin access to modify DNS service\nrecords. DNS community is also trying to explicitly deprecate using\nDNS as a directry service.\n\nSimon: it's a easy hack. All other schemes needed extra RTT. This\nworks, and cuts that out.\n\nTed: you may find politically, it's not as free as you think.\n\nQ; Upgrade path: are these competitors?\n\nA: No these are complementary. 1.x is very simple to do a quick hack\njob of. At the highest levels of performance it starts breaking down.\nIt's not so ideal for 1M hits/hr.\n\n================================================================\nAndy Norman then reported on the NG Prototype\n\n- SLIP over 19.2 k 115% to 140% throughput\n- UK to HP-India: 140% to 160%\n- US to UK through SOCKS (130%-500%)\n- Compared against browser using 4 simultaneous connections w/o keep alive\n- This is using straight 1.0 over SCP, without \"header reuse\"\n\n================================================================\nWe then discussed the role of NG in this working group.\n\nShould this be in the charter, or wait for a formalized proposal?\n\nDSR: Comments on removing NG from the charter.\n\nPaul Hoffman: Sounds from this presentation, this should be completely\nseparate track until its clearer.\n\nMasinter: many feature have emerged in 1.1 learned from NG. We should\nlook to NG for experiences. Doesn't mean this should be our work item.\nSo far most of the work has been done outside our WG. We should\nexplicitly refer in the charter to paying attention to the NG work. I\nwouldn't have a lot of confidence in the milestone dates and work plan\nfor NG given its speculative features. That's my uneasiness.\n\n================================================================\nWe then returned to the discussion of 1.1:\n\nYes, 'host:' is required in 1.1.\n\nFowarded:\nQ:Is there experience with loops occuring, is forwarded useful if some\ngateways strip them?\nA:Forwarded is still useful as a diagnostic.\nComment: if proxies remove forward headers for security reasons, then\nthe fact that this has been done should be indicated.\nPhill: Secrecy can be obtained by using a secret hashed token: no hist\ninfo needs to be revealed.\n\nDiscussion of the 5-second arbitrary timing and partial response.\n\n> Jim Gettys: This problem is due to buggy clients not reading while\nspewing data. The answer is to warn clients to do so.\n> Roy: if the data is sent out faster than the TCP reset.  \n> Ted: this contradicts your goal of engineering a solution to some\nparticular transport problem. \n> Alex: We must come up with a solution that works for TCP.\n> Larry: You can put a multipart with unknown content-length. too! So\nyou'll still need the general fix. \n\nAlex: why can't you use full url when talking to the origin server?\nA: Why try? why send url to a server that may reject it, but does nothing different?\n\nDiscussion of new methods:\n* only applies to the new methods : OPTIONS, TRACE, and WRAPPED.\nLarry: This is long list of new methods. I've seen lots of distributed\nfile systems. Sees like a random selection of methods from filesystems\nfrom the past. If you want a kitechen sink, you're missing a lot. If\nnot, what's the rationale? HTTP doesn't have the power of ACLs,\netc. It's not good enough to put placeholders in a protocol\nspecification.\nPhill: following Larry's points on Portmanteau toolsets: remember,\nthere are an incredible number of methods being implemented by\nCGI-bins; only OPTIONS and Trace can be done that way. Sounds like\npatch & friends can be done that way...\nRoy: We should provide a consistent way for everyone to do it so that\neveryone doesn't supply their own form to do these operations.\nPhill: when I did CI/CO, I found what Larry found: you'll need a lot\nmore data to specify the total operation. \n\nQuestions on PATCH:\nIt was intended that PATCH be an arbitrary data object, e.g.,\nreplacing a b&w film with a colorized film could be a patch? \n\nOther comments:\n\nDave Kristol: Why do we have two mechanisms for streaming: multipart\nand chunked TE? [Notetaker disagreed -- it's easier to use CTE -- real\nmime boundarys are hard-- not amenable to fast processing]\n\nDMK: regarding TRACE, is it allowed to have a body? A: I can't\nremember. DMK: if there is a body, is it exactly the same body\nreferenced, or perhaped PEPped, etc.\n\nHarald: HTTP 1.1 looks like a case of second-system syndrome:\nexcessive backwards-compatiblity, and kitchen-sinking.\n\nRoy confessed to having thrown out 4 new methods already.\n\nHarald: I would suggest a very narrow focus on writing a basic\nplatform to negotiate upgrades.\n\nComment: note that locking/checkin/checkout is logically impossible.\n================================================================\nSteven Zilles made a quick announcement:\n\nADOBE will have a PDF demo this week that will use byte ranges via\nNetscape 2.0b3 using URLs today, but partial get next year. Feb '96\nrelease of PDF will also reveal the \"hint\" table. Also, a CGI script\nto serve byte ranges.\n\nRoy: Why is the rationale to modify the protocol, rather than define a\nnet-pdf -type?\n\nSteve: the tools for PDF are already in place ...\n================================================================\nAfter these discussions, the chairs presented the results of their\ndinner planning session:\n\n- The HTTP/1.0 draft will be revised to become an 'informational RFC'\n  which describes common current practice. Paul Hoffman (maintainer of\n  the list of HTTP servers and their features) will help.\n\n- The HTTP/1.1 draft will be reviewed independently by separate\n  sub-groups. The sub-groups are chartered to review the HTTP/1.1\n  draft for text related to their issue, and propose changes to the\n  HTTP/1.1 draft that consist of either wording changes, or movement\n  of major chunks of HTTP/1.1 to separate documents, as appropriate.\n\n  The issues are:\n\n   * Persistent connections\n      (this contains all of the 1.1 proposals for keep-alive,\n       and maintaining connections to avoid TCP startup costs.)\n       Alex Hoppman, Simon Spero, Mike Cowlishaw, Andy Norman,\n       Scott Powers, Brian Swetlund volunteered.\n\n   * Cache-control and proxy behavior\n       Ari Luotonen, David Morris, Jim Gettys volunteered.\n\n   * Content negotiation\n       Larry Masinter, Simon Spero volunteered.\n\n   * Authentication\n       Phill Hallam-Baker, Alex Hopman, John Marchinoi,\n       Stefek Zaba, Scott Powers volunteered.\n       (This issue must be coordinated with WTS working\n       group drafts.)\n\n   * State management\n       Dave Kristol, Rohit Khare, Scott Powers volunteered.\n\n   * Range retrievals\n       Stephen Zilles, Ari Luotonen volunteered.\n\n   * Extension mechanisms\n       Paul Hoffman, Rohit Khare, Daniel LaLiberte,\n       Simon Spero, Phill Hallam-Baker volunteered.\n\n   * other new methods and header features\n       (no list of volunteers for this was gathered)\n\n- Subgroups should conclude their work by Jan 96, in time to publish\n  their conclusions (or lack thereof) to the rest of the WG by\n  February 96, so that new internet draft(s) for HTTP/1.1 will be\n  ready in March 96 and ready for Proposed Standard RFC status by June\n  96.\n\n- Subgroups should document meetings, progress, etc. and are\n  encouraged to meet regularly, by conference, phone, etc.\n\n- Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n  consensus will revert to HTTP/1.0 status in 1.1 and be considered\n  for inclusion in HTTP/1.2.\n\nAlex wondered where 'logic bags' fit, and Larry suggested it should be\nhandled by the cache control/proxy group.\n\nThe meeting ajourned.\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "Mostly reflects what I recall after allowing for possible naps when I may\nhave missed a whole discussion fragment I don't recognize ;-:). See one\ncomment below.\n\nOn Fri, 8 Dec 1995, Larry Masinter wrote:\n\n>[...]\n> HTTP/1.0 was proposed as Best Current Practice but rejected by the\n> IESG because didn't describe people's view of what was 'best', and\n> thus it was an inappropriate status.\n> \n>[...]\n> * After some back and forth about a variety of options, the desire for\n>   a stable core, and so on, the discussion led to the proposal that\n>   HTTP/1.0 be re-written as an Informational RFC describing current\n>   practice.\n> \n>   However, 'current practice' does not mean that we should document\n>   all 50 versions of content negotiation as practices, merely use the\n>   core of the 1.0 document as it stands for the parts that are\n>   specified, and note that other features are not implemented\n>   consistently. \n>[...]\n> ================================================================\n> After these discussions, the chairs presented the results of their\n> dinner planning session:\n> \n> - The HTTP/1.0 draft will be revised to become an 'informational RFC'\n>   which describes common current practice. Paul Hoffman (maintainer of\n>   the list of HTTP servers and their features) will help.\n\nSomehow I was under the impression that changes to the current 1.0 document\nwere to be largely cosmetic to change the boiler plate to align with the\ninformational status. What I see recorded here sounds like a major rewrite\nand that seems like an awful lot of effort for all of us. Find some\nverbage to describe it as commonly available interoperable current practice\nand perhaps allow some focused comment from current server/UA authors who\nfeel they have a commonly used feature which has been missed. Then lets\nmove on to 1.1 quickly!\n\nMy sense is that 1.0 should be a document from which a workable server could\nbe implemented and exoect to interoperate with user agents which existed\ncirca Mar 1995 (or pick another date). Secondly, it will describe current\nusage for problem determination.  \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "making progress on StateInf",
            "content": "As Larry Masinter's draft IETF meeting notes said, HTTP/1.1 will\nprogress with separate pieces that will be brought together in time\nfor the next IETF meeting.  I agreed to try to make progress on\nState-Info (http://www.research.att.com/~dmk/session.html).  (The\nactual topic area was \"state management\", and right now there's only\nthe one draft proposal.)\n\nI claimed at the meeting, on the basis of http-wg mailing list\nactivity, that there was \"rough consensus\" for the I-D I wrote,\ndraft-kristol-http-state-info-00.txt.  I was advised that I had better\nconfirm that here.  So, are there dissenting voices?  (I'm really\nhesitant to do this just before WWW4!  How about taking time to read\nthe draft, then responding late next week....)\n\nWhether or not there's \"rough consensus\", there's another important\nissue:  will the browser vendors actually implement it?  Absent their\nacceptance, this whole exercise is academic.  What say ye, Netscape\n(Lou) and Spyglass (Eric)?  Others?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "> Roy writes:\n>    What is the actual difference in development and run-time expense\n>    between implementing a general mechanism for preconditions such as\n>        IF: {eq {Content-MD5 \"89787jhlkr8r87y98437==\"}}\n>    or\n>        IF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 GMT\"}}\n>    over implementing a special-purpose mechanism for cache validation\n>        Content-Validator: \"89787jhlkr8r87y98437==\"\n>    and\n>        If-Validator: \"89787jhlkr8r87y98437==\"\n> \n> This entirely misses the point I've been trying to make over and\n> over and over again.  If you accept the principle that\n> cache validation ought to be done using an opaque validator,\n\nShould, yes, but I cannot accept that condition as a MUST.\nI simply cannot do that given the presence of existing resources on existing\nservers for which no Cache-Validator header field is present.\n\n> and that the design of caching is *central* to the operation of\n> HTTP, then this is hardly a \"special-purpose mechanism for cache\n> validation.\"  (This is why I wrote in another message today\n> that agreeing on principles should be done before fighting about\n> syntax.)\n\nSorry, but if the only thing it can be used for is cache validation,\nthen it is a special-purpose mechanism for cache validation.  There\nis no point in arguing that it isn't.\n\n> Remember that (in the opaque-validator model) the server *could* use\n> an MD5 hash or a modification date as the cache-validator.  And\n> this means that the decision about how to define cache validity\n> rests with the origin-server, which knows the semantics of the\n> data, rather than with the client.\n\nYes, which is why I used that exact example above.\n\n>    keeping in mind that no existing server currently provides a\n>    Content-Validator header field and that the field, whatever it\n>    contains, must be duplicating some other entity-header.\n> \n> I think you are confused.  It *may* duplicate some other entity-header,\n> but it might not.  For example, the server may choose to encode\n> the file modification date in (seconds.fractional_seconds) as\n> a string of 9 or 10 hex digits, which is both shorter than the\n> 29 bytes of an HTTP date, and also more precise.  Or the server\n> may use a proprietary hashing function (i.e., not MD5) which might\n> be more efficient for the purpose.  Or the server might use some\n> combination of its internal index for an item in a database,\n> plus a database update-version number, if it isn't storing\n> modification timestamps for each database entry.\n\nAll of which are duplicating an entity header field -- the server may\nnot be sending that information in a header field now, but there is\nno reason why it can't do so.\n\n> My point is that we should not restrict the cache-validator\n> to something that can be expressed using just the other\n> standard header fields, both because these fields may not\n> include the requisite information, and because doing so\n> would require the *client* to decide how to determine if\n> a cached object is valid, when this is most certainly the\n> role of the origin-server.\n\nIf the server is capable of generating an opaque validation string,\nthen it is capable of sending that string as the value of Content-Version,\nin which case the client is capable of using Content-Version within\nthe request precondition to serve as the opaque validator.\n\nIt is reasonable to require that the client use Content-Version in\npreference to other comparators.  It is not reasonable to require\nall resources to use the same comparator.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Dave Kristol:\n>As Larry Masinter's draft IETF meeting notes said, HTTP/1.1 will\n>progress with separate pieces that will be brought together in time\n>for the next IETF meeting.  I agreed to try to make progress on\n>State-Info (http://www.research.att.com/~dmk/session.html).  (The\n>actual topic area was \"state management\", and right now there's only\n>the one draft proposal.)\n>\n>I claimed at the meeting, on the basis of http-wg mailing list\n>activity, that there was \"rough consensus\" for the I-D I wrote,\n>draft-kristol-http-state-info-00.txt.  I was advised that I had better\n>confirm that here.  So, are there dissenting voices?\n\nI too remember there being a rough consensus for incorporating\ndraft-kristol-http-state-info-00.txt into HTTP/1.1.\n\nLet me try to break this consensus down into three separate issues:\n\n1) Some web services need to maintain session state (where a session\nis a series of requests and responses between one server and one user\nagent).\n\nI don't think anyone on this list would disagree with 1).  Several\nexisting services (like some games, some cooperative systems, some\ntele shopping services, some library-like systems) already maintain\nstate, either through ugly, not completely reliable, and\ncache-unfriendly hacks or by requiring user authentication (which is\nreliable, but both cache-unfriendly and user-unfriendly).\n\n2) HTTP/1.1 needs to introduce reliable and cache-friendly mechanisms\nto support session state.\n\nI also believe there to be a rough consensus on this.  I remember\nsomeone saying that Java would, among other things, solve the session\nstate problem so why bother also solving it in HTTP, but many people\nhad objections to this reliance on Java.\n\n3) draft-kristol-http-state-info-00.txt is internally consistent and\nwill offer good session state support.\n\nI believe there is consensus on draft-kristol-http-state-info-00.txt\nbeing robust, cache-friendly, and easy to implement.\n\nThere recently was some discussion on\ndraft-kristol-http-state-info-00.txt not addressing its stated privacy\nconcerns, but consensus was reached that\ndraft-kristol-http-state-info-00.txt does indeed provide adequate\nprivacy.\n\nThere has also been some discussion on scalability: some people think\nthat draft-kristol-http-state-info-00.txt will not scale to services\nwith a very large number of concurrent sessions and a very large\namount of session state.  I believe there to be at least consensus\nthat, if these large cases exist at all, they will be very rare.\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP BOF at WWW4: Wednesday 7p",
            "content": "> Excuse me!  I think it is quite unreasonable to expect that an unofficial\n> gathering of folks interested in HTTP expect to hear from 'various groups\n> working on bits of HTTP/1.1' about how they plan to proceed.\n\nI don't care if it's reasonable or not -- it is simply how I would like\nto run the BOF.  It is my BOF, my time, and my effort -- not a WG meeting --\nand I would prefer to have the discussion reflect the actual plans of the\nWG.  Why is that bad?  Would you prefer that I just say the HTTP WG\ndoesn't know what it is doing?\n\n> First of all,\n> the groups aren't formed yet and except for those of us who were in Dallas\n> at the 2nd HTTP session, the group proposal is still unknown and possibly\n> even rejected. Perhaps this should have been private mail to Larry Masinter\n> asking if he would be available to introduce the concept and poposed \n> time-line.\n\nNo, it was a reminder to the WG Chairs that they needed to get the information\non the list NOW (which Larry and Dave promptly accomplished) before I gave\ntwo separate presentations on the topic next week.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re:  [more about IMG attributes for size",
            "content": "Regarding new attributes for the IMG tag, to specify width and height:\n\nKeep in mind that the person/program that creates the IMG reference may\nnot actually have an image in hand -- it may be on another server.\nThat's one problem with the proposals to have a server bundle images\nalong with the base document:  the server may not actually have them.\n\nIt's also a problem for whoever creates the base document:  they may\nnot know its size, so they can't edit the IMG tag.  Oh, they could\npeek, but since the image is elsewhere, the owner of the image could\nchange what the image URL refers to, and then the information in the\nIMG tag would be wrong.  (No flames about URNs, please!)\n\nI'm nervous about solutions that would require the server to scan\ndocuments (even if they cache the result) to locate IMG requests.  On\nthe other hand, if the image information were part of the server's\ndatabase (using the term loosely), the whole affair would \"fail-soft\".\nFor example, with the scheme Eric Sink described, if a client didn't\nget response headers that describe the images in the base document, it\nwould simply make a best effort to render the page.  But if the server\nhelped the client by providing the correct (would be nice!) size\ninformation, the client could do a nice rendering job as it receives\nthe base document.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "I think this may perhaps be an instance of \"consensus by lack of discussion\"\nfor which this group ought to be famous.\n\nWhether there is consensus (of any texture) for it or not, I think the\nreasonable thing to do under the circumstances is to compare it\ncritically to the Netscape Cookie Proposal, since that is the main\ncompetitor for this ecological niche (aside from the actual dominant\nspecies, which is URL hacks in all their glory).\n\nI personally think that the Kristol proposal, to which several members\nof this group have contributed, is a reasonable and simple solution to\nthe problem, although it does have certain limitations, and we should\nbe clear on the acceptability of those limitations before OKing it.\n\nI'll try to address some of these issues later -- gotta get some work\ndone today.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "> I too remember there being a rough consensus for incorporating\n> draft-kristol-http-state-info-00.txt into HTTP/1.1.\n\nWhat?  No.  \"Rough consensus\" can only be tested by polling the list.\nTo date, the only piece of HTTP that has ever gained \"rough consensus\"\nfor includion in HTTP/1.1 is the Host header field and those features\nwhich already exist in HTTP/1.0 (by definition).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "At 4:49 PM 12/8/95, Roy T. Fielding wrote:\n>\"Rough consensus\" can only be tested by polling the list.\n\nWhat?  If Dave Kristol announces a new draft to this list and no one raises\nobjections, that sure sounds like \"rough consensus\" to me.\n\nIf we poll list members for their opinions, we discover the majority\nopinion, which is more precise than consensus.  I agree that the \"Host\"\npoll was a useful way to resolve a contentious issue.  This is simply not\nsuch a case.\n\nLou is the only person I recall objecting to the most recent state-info\ndraft, and he has not backed up his objections with a counter-proposal nor\nwith a submission of the Netscape cookie proposal (either of which I would\nlike to see happen).\n\nIf I am somehow just wrong that tacit approval equals rough consensus, and\nif Roy is right that polling is the only accepted consensus-building\nmethod, then _it needs to happen more often_.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "At 4:52 PM 12/8/95, Shel Kaphan wrote:\n>Whether there is consensus (of any texture) for it or not, I think the\n>reasonable thing to do under the circumstances is to compare it\n>critically to the Netscape Cookie Proposal\n\nSee <URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0161.html>\nfor a comparison on privacy issues.  I don't think there's any valid\ncomparison between the two other than: 'state-info:' addresses privacy, and\n'cookie:' does not.[1]\n\nLou's objection was that it would be impractical to store all shopping\nbasket information in one header.  I disagreed (as an implementor of\nseveral shopping-basket type applications) and others did as well.  I\nsuggested that if Lou felt this was a serious concern, perhaps we could add\nthe cookie proposal's concept of \"path\" to state-info.  Koen objected that\n\"path\" was only a minor improvement.  Lou, would the addition of \"path\"\novercome your objections to 'state-info:'?  If not, what would?\n\nM. Hedlund <hedlund@best.com>\n\n[1]  My favorite quote on the issue:\n        \"Currently, when you first use the Netscape program,\n        a message is sent to our server in Mountain View, and\n        we retrieve what we call the Magic Cookie, and this\n        Magic Cookie uniquely identifies you.  The second\n        time you run our program, our central system checks\n        if we have received that Magic Cookie before.\"\n                -Jim Clark, _The Red Herring_, Nov. 95, p. 70\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "M. Hedlund wrote:\n> \n> Lou is the only person I recall objecting to the most recent state-info\n> draft, and he has not backed up his objections with a counter-proposal nor\n> with a submission of the Netscape cookie proposal (either of which I would\n> like to see happen).\n> \n> If I am somehow just wrong that tacit approval equals rough consensus, and\n> if Roy is right that polling is the only accepted consensus-building\n> method, then _it needs to happen more often_.\n> \n> M. Hedlund <hedlund@best.com>\n\nI still have objections to the state-info draft, but I admit I have dropped\nthe ball on getting a timely counter proposal back to this group.  We have\nbeen in the midst of hiring a technical writer to turn our cookie proposal\ninto an internet draft so that it could be submitted to this group for\nimprovements.   \n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "In the IETF, the WG chairs seem to have the perogative to declaring\nwhether or not the WG has reached 'rouch consensus'. If you disagree\nwith the WG chairs on this issue, you may appeal this decision to the\narea directorate if you can't come to some resolution with the WG\nchairs first.\n\nAs far as I am concerned, there is no 'rough consensus' on any of the\nissues for which we have constituted subgroups; this includes 'state\ninfo'.\n\nI would appreciate it if WG members would avoid declaring or denying\nthe existance of 'consensus' on any of these issues.\n\nThe members of the subgroup on 'state management' are:\n\nDave Kristol, Rohit Khare, Scott Powers\n\nIf you would like to join this subgroup, please contact those people.\nIf the subgroup wishes to report on their meetings, conversations,\ndiscussions, and the progress or lack thereof, please do so.\nIf there are technical issues on which you cannot make progress and\nwould like to consult the rest of the group, please let us know.\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "> Somehow I was under the impression that changes to the current 1.0 document\n> were to be largely cosmetic to change the boiler plate to align with the\n> informational status. What I see recorded here sounds like a major rewrite\n> and that seems like an awful lot of effort for all of us. Find some\n> verbage to describe it as commonly available interoperable current practice\n> and perhaps allow some focused comment from current server/UA authors who\n> feel they have a commonly used feature which has been missed. Then lets\n> move on to 1.1 quickly!\n\nI will try to fix the minutes so that it doesn't sound like this is\nintended to be a lot of work.  I'm not expecting a major rewrite of\nthe 1.0 draft, just enough so that we don't get any more \"where's\nAccept: ?\" queries.\n\nI suggest we give Paul Hoffman and Roy a couple of weeks to post new\ndraft with these changes before starting to worry that 1.0 will be\nbogged down.\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "> Since If: is being proposed primarily as an extension mechanism,\n> and we don't quite know how to make it work yet, how about making\n> it optional?  That is,\n> (1) servers need not support If:\n> (2) If: should only be used (in HTTP 1.1) as a technique\n> for optimizing performance, and never for ensuring\n> correctness.\n> (3) Features that apply to all or many requests\n> should be implemented as specific headers, rather than\n> If: extensions, since this allows more efficient\n> implementation and probably more efficient on-the-wire\n> encodings.\n> \n> To my mind, this means that cache validation and byte ranges\n> ought to be standalone headers, and not implemented using If:\n> We know we need them, and they are simple enough to implement\n> without If:\n\nI strongly disagree with all three points, as already indicated\nand explained, and have no intention of changing the syntax as it\ncurrently stands without an explicit directive by the WG as a whole\nAND proof that such a scheme can be implemented within the constraints\nof the existing HTTP applications.\n\nI am, however, willing to accept explicit restrictions on the set\nof precondition expressions allowed in HTTP/1.1 and some ordering\nof their preference.  Therefore, if you will only accept preconditions\nfor cache validation using an opaque value, then I will only accept\n\n    IF: {eq {Content-Version \"...\"}}\n\nto represent the syntax for cache validation using an opaque value.\nIf we also wish to allow cache validation if no opaque value is available,\nthen\n\n    IF: {eq {Content-MD5 \"...\"}}\nand\n    IF: {eq {Last-Modified \"Thu, 07 Dec 1995 14:08:05 GMT\"}}\n\nwould also be acceptable (in order, with preference implied).\n\nIn order to work at all, preconditions must be a required feature\nof HTTP/1.1 or removed entirely -- making them optional is not an option.\nUsing multiple header fields for preconditions is not acceptable to me\nbecause they make it impossible to extend the conditions without an\nexponential increase in feature interaction and a corresponding change\nto the standard.  By the time HTTP/1.x is finished, it should be possible\nto extend all extensible features without going through this WG.  If not,\nthen this WG is not working on HTTP.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": " Simon E Spero <ses@tipper.oit.unc.edu> says:\n  > Well, there's always my other response :-) One of the things I talked about\n  > with Alex at the IETF was making a slight change to the SESSION proposal. \n  > The idea is to use the SESSION method to switch the connection over to \n  > running SCP, and then use SCP to manage multiple sequential HTTP 1.0 \n  > transactions. It turns out that this technique allows even more code to be\n  > reused than using MIME multipart, and provides a very obvious transition\n  > path to full HTTP-NG. \n\nDo you (or Alex -- Alex, are you out there?) have a reference for the SCP\nproposal?  Somehow I failed to get a reference or paper at the IETF.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "I would like to ask that an additional subgroup be created for\ncoming up with a registry (and forms for registration) for HTTP\nalong the lines of RFC 1590 (actually, the more recent MIME drafts\nare a better guide, but I don't have the names handy).  That is\nsomething I've meant to do but didn't have the stomach to tackle\nwhile other things were still undone.\n\n.....Roy\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "Dave and I chose areas for subgroups where there had been discussion\non the list, some disagreement, and no convergence. This isn't the\ncase for 'registration'. \n\nCould you elaborate what it is that you think needs to be registered?\nI'm wary about trying to invent adminstrative procedures in an\notherwise technical standard.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Larry Masinter:\n>In the IETF, the WG chairs seem to have the perogative to declaring\n>whether or not the WG has reached 'rouch consensus'. If you disagree\n>with the WG chairs on this issue, you may appeal this decision to the\n>area directorate if you can't come to some resolution with the WG\n>chairs first.\n\nI have no problems with the WG chairs claiming the right to declare\nrough consensus, _if they indeed plan to regularly exercise that\nright_.\n\nIn at least the last half year, I have never seen the WG chairs take a\nleading role in the consensus game, which resulted in the WG reverting\nto \"consensus by lack of discussion\" mode as Shel Kaphan described.\n\nIf the chairs want to take the effort to let this WG function as\ndescribed in the IETF rulebooks again, they have my full support.\n\nI have reported to you that we did indeed reach rough \"consensus by\nlack of discussion\" on the State-Info draft.  I even included a\nsummary of relevant issues.  The whole story can be found in the\nhttp-wg and www-talk archives.\n\nWhile I think that the State-Info draft could be decided on now,\nwaiting until the end of January 96 is acceptable.\n\n>As far as I am concerned, there is no 'rough consensus' on any of the\n>issues for which we have constituted subgroups; this includes 'state\n>info'.\n\nIf you mean by this that the WG chairs never declared consensus, I\nagree.\n\n>I would appreciate it if WG members would avoid declaring or denying\n>the existance of 'consensus' on any of these issues.\n\nI could live with that.  Please tell me how to refer to the \"lack of\ndiscussion\" type of consensus we have had in the past.  I am not\nprepared to have the whole privacy discussion we had on www-talk and\nhtml-wg last summer again, this time in a more formal setting.\n\n>The members of the subgroup on 'state management' are:\n>\n>Dave Kristol, Rohit Khare, Scott Powers\n>\n>If you would like to join this subgroup, please contact those people.\n\nThank you, I will.\n\n\nOne other thing: Roy Fielding recently said on this list:\n\n| \"Rough consensus\" can only be tested by polling the list.\n|To date, the only piece of HTTP that has ever gained \"rough consensus\"\n|for includion in HTTP/1.1 is the Host header field and those features\n|which already exist in HTTP/1.0 (by definition).\n\nWould you (the WG chairs) please either confirm or deny this?\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Seems like the state-info proposal covers what one would need for the\nmoment. While it pushes the really difficult aspect of implementing\ntransactional concepts to CGI bin scripts, one could argue that is how it\nshould be (since the notion of transaction varies in different application\nareas). \n\nRegarding Koen's proposal about a path name as a value for state-info (in\norder to accomodate Netscape cookies) I actually prefer leaving the\nstate-info: header value un-specified for the moment. If I understand the\nuse of path one could use it to store and look up state maintained on the\nserver side (correct me if I am wrong). But, that again gets into one of the\nmodels for implementing state management on the server side. There are other\nequally viable ones (shared memory with a flat look up is another). Also, if\nwe leave it unspecified then I could think of using IF (the renamed unless)\nto implement comparator functions on types of state-info information. I\nbelieve that the combination of IF and state-info  is a more extensible way\nof implementing different types of session management strategies by CGI bin\nscripts. \n\nSankar Virdhagriswaran                         Phone: (508) 287 4511\nCrystaliz Inc.                                 Fax:   (508) 287 4512\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Coming to agreement on a solution to a problem requires agreement on\ntwo things: 1: the problem, 2: the solution. We have to agree that\nthere's a problem, and the nature of the problem, and we have to agree\non the solution, that it really solves the problem, that if we have\nmore than one potential solution to the problem, that we've picked the\nbest one.\n\nI think in the case of state-info there's agreement that there's a\nproblem, and there's mainly agreement that draft-kristol-etc probably\nsolves the problem, but less agreement that it is a better solution\nthan Netscape's cookies. \n\nIt's easy to confuse \"agreement that there's a problem\" with\n\"agreement on the solution\", and it appears you have done so here. \n\n> One other thing: Roy Fielding recently said on this list:\n\n> | \"Rough consensus\" can only be tested by polling the list.\n> |To date, the only piece of HTTP that has ever gained \"rough consensus\"\n> |for includion in HTTP/1.1 is the Host header field and those features\n> |which already exist in HTTP/1.0 (by definition).\n\n> Would you (the WG chairs) please either confirm or deny this?\n\nI deny both uses of the word 'only'.\n\nI dislike spending http-wg bandwidth on 'process' questions; if you\nhave further comments on process, please mail me directly.\n\nSince we've constituted subgroups, let's try to let them work.\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "\"Registration\" issues are involved with PEP (registering protocols, headers,  \ncontent-encodings). I've chatted with Jon Postel about these issues, and I  \nthink that, for the time being, registration discussions should be delegated  \nto the Extensions group.\n\nThanks,\nRohit Khare\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Sankar Virdhagriswaran, Crystaliz Inc.:\n>\n>Regarding Koen's proposal about a path name as a value for state-info (in\n>order to accomodate Netscape cookies) \n\nI think there is some confusion here.  The discussion was on whether\nthe state-info mechanism proposed by Dave Kristol should be extended\nwith a `path=' attribute as in Netscape cookies.  Someone suggested\nthat this might allow for more session state to be kept without\nresorting to server side databases, if I remember correctly.  I\nremarked that I failed to see how that `path=' would allow this.\n\n>Sankar Virdhagriswaran                         Phone: (508) 287 4511\n>Crystaliz Inc.                                 Fax:   (508) 287 4512\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": ">I think there is some confusion here.  The discussion was on whether\n>the state-info mechanism proposed by Dave Kristol should be extended\n>with a `path=' attribute as in Netscape cookies.  Someone suggested\n>that this might allow for more session state to be kept without\n>resorting to server side databases, if I remember correctly.  I\n>remarked that I failed to see how that `path=' would allow this.\n>\n\nSorry about the confusion. I agree with what Koen has to say. Also, servers\n(or atleast CGI bin scripts) may choose to keep state. For certain class of\napplications that is the right thing to do. The current proposal, as I see\nit does not preclude it. In fact, Dave in his section on implementation,\nattempts to bring up the issue and does not explicitly forbid it. He does\nthe mention the problems of some CGI bin script maintaining state between\nsessions. Well, such is life in the state oriented, transaction world. They\nall eventually become hairy beasts to maintain.\n\n \nSankar Virdhagriswaran                         Phone: (508) 287 4511\nCrystaliz Inc.                                 Fax:   (508) 287 4512\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "At 4:10 PM 12/9/95, Koen Holtman wrote:\n>The discussion was on whether\n>the state-info mechanism proposed by Dave Kristol should be extended\n>with a `path=' attribute as in Netscape cookies.  Someone suggested\n>that this might allow for more session state to be kept without\n>resorting to server side databases, if I remember correctly.\n\nYes, I proposed the addition of \"path\" to try to address Lou's concerns\nabout the usability of state-info.\n\n>I remarked that I failed to see how that `path=' would allow this.\n\nImagine an extended newspaper archive search done through a Web browser.\nSay the archive-server would like to avoid showing you articles you have\nalready reviewed; and say the server stores articles by week.  As you read\nan article, the server responds with a state-info header that says \"within\npath /week1, send state-info token w1a1.\"  The browser flits around the\narchive and returns to /week1.  Rather than having to send one token for\nevery article reviewed (w1a1, w3a4, w5a7 ....), it just returns the\ntoken(s) for week1.  The server modifies the week's index accordingly.\n\nThe advantage is that you can maintain several state-registers particular\nto parts of (or the whole) server without returning a full session history\nwith each request; the token-database is client-side.  The disadvantage is\nthat you blow away Dave's provision for an unspecified header-content.  A\nside effect would be the need to return more than one state-info token in\nthe same request.\n\nI'm not saying this is optimal -- I'm happy with Dave's proposal as it\nstands.  I am suggesting a potential middle ground.\n\nM. Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": ">The advantage is that you can maintain several state-registers particular\n>to parts of (or the whole) server without returning a full session history\n>with each request; the token-database is client-side.  The disadvantage is\n>that you blow away Dave's provision for an unspecified header-content.  A\n>side effect would be the need to return more than one state-info token in\n>the same request.\n\nSure. This will work, but, as I said, this is only one way. It will also\nlimit the implementation to using hierarchical organizations of state\ninformation. That sort of organization is not optimal for flat look up.\nEither the HTTP protocol provide complete support for various mechanisms for\nimplementing state or it punts it to outside programs, essentially acting as\na communication mechanism for state information between clients and servers.\nObviously, the first is not what we want to do. In that case, we are left to\ndo the second. \n\nMuch like Roy argues for opaque cache identifiers (and individual servers\nimplementing their own mechanisms for comparisons), I am arguing for opaque\nsession information. Let server writers or CGI bin script writers implement\ndifferent quality of service features that they want, for the type of\nsession management they want to do.\n\nSankar Virdhagriswaran                         Phone: (508) 287 4511\nCrystaliz Inc.                                 Fax:   (508) 287 4512\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "Thus wrote: Marc Salomon\n>|Well, I assume you are proposing (hypothetically, anyway) that the\n>|server rewrite the HTML it's transmitting to add the WIDTH and HEIGHT\n>|attributes.\n>\n>If the server would have to open each image in any event in order to pack it \n>into a multipart message.  It could have access to the image size parameters\n>and include them in the a MIME header.\n\nThis, by the way, is an excellent example of the kind of ways in which\nInternet media-types might be refined to make things work better for\nour purposes.  The content-type image/gif (or possibly all image\nsubtypes) should have optional parameters for width and height (and\npossibly colors, and maybe other things I am not thinking of.)  This\nallows that information to be passed around along with the\ncontent-type.  It wasn't included originally because it's trivial to\ndetermine from the file itself -- given the email centric assumption\nthat you have the file to look at.\n\nUnfortunately, media type registration issues like this aren't made as\nclear as they could be.  I'd like to see this group get a better idea\nof the kind of things we need from media types and try to add them to\nthe registry (as opposed to doing something incredibly stupid, like\nestablishing our own parallel registry or just using the labels without\nbothering to register them.)\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "\"Sankar Virdhagriswaran, Crystaliz Inc.\" <sankar@fcrao1.phast.umass.edu> wrote:\n  > \n  > Seems like the state-info proposal covers what one would need for the\n  > moment. While it pushes the really difficult aspect of implementing\n  > transactional concepts to CGI bin scripts, one could argue that is how it\n  > should be (since the notion of transaction varies in different application\n  > areas). \n\nLet me try to clarify something.  The value for the State-Info header\nis opaque.  But just because the specification says it's opaque does\nnot mean a server can't structure the information in some way specific\nto it, such as making the first part a path.  The server could further\nassume that State-Info headers that return to it have that structure\nand use the path information to control behavior.\n\nThere's a false presumption that a CGI-like mechanism must be used for\nall implementations of State-Info.  The mechanism is general and can be\nimplemented and used many ways.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "> Let me try to clarify something.  The value for the State-Info header\n> is opaque.  But just because the specification says it's opaque does\n> not mean a server can't structure the information in some way specific\n> to it, such as making the first part a path.\n\nThis is imposing a complex infrastructure for state-info maintenance\nin the server side, in the state info identifier allocation process.\nA CGI application no longer could simply generate a Cookie: header,\nbut would have to ask the server to allocate it, so that it falls into\nthe server's unique state identifier space.  The same identifier would\nbe shared between different applications on the same server, which may\nnot always be desirable.\n\nFurthermore, a shortcoming in the state-info spec as opposed to the\nCookie spec is that state identifiers cannot be shared between hosts\n(unless yet some more infrastructure is laid down on the server\ncluster side).  Not all systems are self-contained in a single host,\nand therefore there is value in being able to share cookies accross\ndifferent hosts.\n\nIn order for the state-info to be useful, it has to be usable.  In\ntheory it may seem great, but the moment you apply it in practice\nyou'll bump into problems.  Those problems have for the most part\nalready been thought of and solved in Netscape's Magic Cookies, and\nthey've been proven to work in real-life applications.  I don't claim\nthat there isn't be room for improvement, but I think time would be\nbetter spent in finding those improvements and adding them to cookies,\nrather than taking a step back and starting from scratch again with\nsomething that isn't all that much better.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Ari Luotonen <luotonen@netscape.com> wrote:\n>> [DMK wrote:]\n>> Let me try to clarify something.  The value for the State-Info header\n>> is opaque.  But just because the specification says it's opaque does\n>> not mean a server can't structure the information in some way specific\n>> to it, such as making the first part a path.\n>\n>This is imposing a complex infrastructure for state-info maintenance\n>in the server side, in the state info identifier allocation process.\n>A CGI application no longer could simply generate a Cookie: header,\n>but would have to ask the server to allocate it, so that it falls into\n>the server's unique state identifier space.  The same identifier would\n>be shared between different applications on the same server, which may\n>not always be desirable.\n\nWhoa!  I don't think I *impose* anything.  I pointed out that a server\nimplementation could choose to structure its State-Info, or not.  How the\nserver implements them will naturally have an impact on how CGIs are\nimplemented.\n>\n>Furthermore, a shortcoming in the state-info spec as opposed to the\n>Cookie spec is that state identifiers cannot be shared between hosts\n>(unless yet some more infrastructure is laid down on the server\n>cluster side).  Not all systems are self-contained in a single host,\n>and therefore there is value in being able to share cookies accross\n>different hosts.\n\nThis is definitely true.\n>\n>In order for the state-info to be useful, it has to be usable.  In\n>theory it may seem great, but the moment you apply it in practice\n>you'll bump into problems.  Those problems have for the most part\n>already been thought of and solved in Netscape's Magic Cookies, and\n>they've been proven to work in real-life applications.  I don't claim\n>that there isn't be room for improvement, but I think time would be\n>better spent in finding those improvements and adding them to cookies,\n>rather than taking a step back and starting from scratch again with\n>something that isn't all that much better.\n\n[Sorry, Larry M.]  Yes, I agree that Cookies are used and work for\nreal-life applications.  I welcome the possibility to improve them.  Many\nof us await a more detailed specification of the current Cookie mechanism\nso we can do just that.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Grievances  Focuse",
            "content": "> 1)\n> First things first: the most concrete complaint I have about the latest\n> http draft is that we're still defaulting to qe=1 and qc=1 for encodings\n> and charset when the headers are missing.  The way I see it, this isn't\n> just my opinion, but rather, this is just broken.  A 1.0 user agent could\n> ask for a document, not be aware of encodings or charsets, click on stuff,\n> and according to the current spec, receive \"stuff.txt.zip\".\n\nYes, and that exactly matches current practice.\n\n> 1) The User\n> gets garbage on their screen, and 2) they might not even have the\n> opportunity to do a successful Save As, as a browser might have done\n> end-of-line translations or high-bit stripping.  The default encoding and\n> charset during lack of headers should be 0.001.  This has been discussed\n> before.\n\nYes, this was discussed before, and the decision made on this list was\nthat lack of headers meant qe=1 and qc=1 and that if the client wished\nto exclude other encodings and charsets, then\n\n     Accept-Encoding:\n     Accept-Charset: iso-8859-1\n\nis what the client must send.\n\n> 2)\n> Personally, I don't understand the existence of the version control\n> methods in the current spec.\n\nThey are there to support collaborative systems.  They are also optional.\n\n> This is a glaring example of something that\n> can easily be moved out to a separate draft.  The main implementors of\n> the HTTP spec in the marketplace are not even using PUT: PATCH and MOVE\n> are really fringe niche interests.\n\nAhem. May I remind you that one person's fringe interests are another\nperson's requirements.  The fact that the industry has only implemented\nHTTP/1.0 is irrelevent to what goes in the initial draft of HTTP/1.1.\nIf the industry does not implement these features *by the time HTTP/1.1\nis ready to proceed outside the WG*, then they will not be in the final\ndocument(s).  However, if two implementations of these features do\nexist and are interoperable, and the features are optional, then they will\nbe represented in the final standard (perhaps as a separate document,\nbut that is a non-issue).\n\n> 3)\n> I heard a number of people express the feeling that Content negotiation is\n> a large, independent module that could be removed from HTTP for faster\n> consensus, and also something that should be accessible to other protocols.\n> I think the example of enhanced SMTP was given.  Let's see that happen!\n\nWe cannot discuss hierarchical caching without also discussing content\nnegotiation and its effect on the request/response semantics.  Since\nHTTP/1.1 will not progress without caching, content negotiation cannot\nbe removed from the HTTP/1.1 standard.  One way or another, this WG must\ndeal with the questions at hand regardless of whether it is all done\nin one document or in multiple documents.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "> Dave and I chose areas for subgroups where there had been discussion\n> on the list, some disagreement, and no convergence. This isn't the\n> case for 'registration'. \n> \n> Could you elaborate what it is that you think needs to be registered?\n> I'm wary about trying to invent adminstrative procedures in an\n> otherwise technical standard.\n\nHTTP allows a number of fields to be extended independent of the\nprotocol version.  Some are already covered by the MIME registry at IANA.\nOthers exist with no defined mechanism for notifying other implementors\nof commonly defined extensions.  These include:\n\n    Methods\n    Status codes\n    Entity Header Fields\n    Content codings\n    Transfer codings\n\nNote that this list does not include things like PEP and Link Relationships,\nwhich also need a registry, but not necessarily the same registry.\n\nThis task needs to be done before any HTTP document becomes standards-track,\nso I do think that it justifies a subgroup and a separate specification\n(just as MIME did it).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Logic Bag concern",
            "content": "(personal opinion)\n\nOne of the reasons i don't like 'if' is that the body of it has\ncomplex parsing rules. I'm thinking ahead to binary-encoded HTTP,\nwhere the headers themselves are sent in binary. Yet, with something\nlike 'if', you're still stuck with parsing the things.\n\nIt's another reason why I'd prefer specific mechanisms (validator)\nover a generalized one (if).\n\n\n\n"
        },
        {
            "subject": "Re: DRAFT Minutes, HTTPW",
            "content": "At 11:43 AM 12/8/95, David W. Morris wrote:\n>Somehow I was under the impression that changes to the current 1.0 document\n>were to be largely cosmetic to change the boiler plate to align with the\n>informational status.\n\nIt was a bit more than that, but still easy to do. My plans (to be made\nreal in the next week, jury duty notwithstanding) are to change the\nintroduction in the BCP document describing \"informational\" and what it\nmeans, leave everything else in the document alone, and add an appendix of\nthings that are common for some mid-1995 HTTP servers but not standardized.\nThe appendix will mostly/completely come from what Roy took out before the\nBCP draft. The wording at the beginning of the appendix will be a bit\ntricky, but I think doable.\n\n>Then lets\n>move on to 1.1 quickly!\n\n1.0 work should *not* get in the way of any 1.1 work: it is independent.\nKeep working on 1.1!\n\n--Paul Hoffman\n\n\n\n"
        },
        {
            "subject": "Registration (was Re: DRAFT Minutes, HTTPWG",
            "content": "We've gotten some warnings from those folks who have been involved in\nthe message-header registration wars to tread carefully here. There's\na quite analogous situation between HTTP headers and message headers,\nand quite a bit of experience in the IETF about what can go wrong.\n\nFor example, if you merely allow anyone to register anything, then you\nget poorly specified headers, conflict over interpretation of\nregistered headers, vanity registration, trademark conflicts, etc.\n\nOn the other hand, if you require standards-track RFCs describing the\nheader before the header becomes registered, you wind up with a\ncumbersome process which either interferes with experimentation, or in\nwhich the registration mechanism is ignored, the experiment proceeds,\nwhich then interferes with registration once the experiment is\nsuccessful.\n\nRegistration mechanisms are constrained by market, financial,\npolitical, legal issues far more then technical ones.\n\nThe default registration procedure for anything that doesn't otherwise\nhave one is 'create a standards-track RFC describing the item'.\nI think this currently applies to\n\n    Methods\n    Status codes\n    Entity Header Fields\n    Content codings\n    Transfer codings\n\nalthough it doesn't seem to apply to content-type. In lieu of any\nother registration mechanism, it currently applies to URL schemes.\n\nIt's not that I don't think this issue needs to be addressed; I think\nregistration is very important. It's not unreasonable also to ask that\nIANA would maintain an authoritative version of such a registry, even\nif the only way that items could be added to the registry was via a\n'standards-track document'.\n\nOne way to make better progress in HTTP-WG is to avoid 'ratholes', I\nthink this is one. I would like to make the issue of devising new\nregistration procedures 'out of scope' for HTTP-WG.\n\nIs there is a group of individuals who are knowledgable about the\nhistory, status, and difficulties with the registration procedures for\nprotocol extensions in other Internet protocols (SMTP, SNMP, media\ntypes, Telnet options come to mind) as well as HTTP and who are\nwilling to work on this issue, I might feel differently.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "At 6:52 PM 12/9/95, Ari Luotonen wrote:\n>In order for the state-info to be useful, it has to be usable.  In\n>theory it may seem great, but the moment you apply it in practice\n>you'll bump into problems.  Those problems have for the most part\n>already been thought of and solved in Netscape's Magic Cookies, and\n>they've been proven to work in real-life applications.\n\nI accept that cookies work (for providers, not for users) in real-life\nsituations.  I am not, however, convinced that all of the features\nspecified in the cookie proposal are necessary for state-maintenance to be\nsuccessful.\n\n>I don't claim\n>that there isn't be room for improvement, but I think time would be\n>better spent in finding those improvements and adding them to cookies,\n>rather than taking a step back and starting from scratch again with\n>something that isn't all that much better.\n\nI disagree that state-info \"isn't all that much better.\"  Koen Holtman has\ndescribed a list of state-info proposal qualities that distinguish one from\nanother, and Dave Kristol has added that list to his proposal.  Two of the\nqualities listed are:\n\n>   + amount of privacy protection\n>\n>   + maximum complexity of stateful dialogs supported\n\nAs I and others have elaborated, cookies offer far less than state-info in\nregards to privacy.  You are saying that cookies offer far more in the way\nof complexity.  Fine, I agree with that statement, but:\n        (1) that doesn't mean state-info \"isn't all that much better\"\n            than cookies, it just has different strengths; and\n        (2) I have yet to be convinced that cookies' complexity is\n            necessary -- and certainly, you have a long way to go before\n            convincing me that cookies' complexity is worth the\n            trade-off in privacy protection that currently exists.\n\nAlso, I don't agree that state-info is \"starting from scratch.\"  Dave's\nproposal grew out of extensive conversations on this list, and incorporated\na number of ideas from Koen's parallel proposal.\n\nAll of that said, I am completely amenable to adding state-info concepts to\nthe cookie proposal rather than the other way around, if we can find common\nground between the two strengths described above.\n\n>[...] a shortcoming in the state-info spec as opposed to the\n>Cookie spec is that state identifiers cannot be shared between hosts\n>(unless yet some more infrastructure is laid down on the server\n>cluster side).\n\nYou consider this a shortcoming; I consider it a strength of state-info.\nThe tail-matching scheme proposed for sharing cookies between hosts is\ninsufficient.[1]  As a result, the cookie proposal opens the door for\nsharing of cookies between arbitrary hosts -- in other words, the sharing\nof information about a user between sites.[2]  If you feel that cookie\nsharing between servers at one site is essential, you need to come up with\nbetter language to ensure that information is contained to one site.\n\nTwo other areas I'd like to see addressed in regards to the cookie proposal:\n\n        (1) Why is an expiration date necessary?  Why is it essential to\nmaintain\n            state beyond the current execution of the user-agent?\n\n        (2) How should the user agent indicate its actions to the user?  The\n            state-info proposal has some specific recommendations -- would\n            you be willing to add them to the cookie proposal?  Further, are\n            the authors of the cookie proposal opposed to giving the user the\n            ability to reset cookies or ignore set-cookie responses?\n\nMarc Hedlund, Organic Online <marc@organic.com>\n\n[1] Again, see\n<URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0161.html>.\nThis is not just idle speculation -- all cookie-supporting versions of\nNetscape before 2.0 implement the tail-matching problem I describe.  It\nlooks like the spec langauge on this point has changed recently, but it is\nstill insufficient: '.com.' is a legal domain setting under the language in\nplace.\n\n[2] See\n<URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0469.html>,\n\"DRAFT Minutes, HTTP-WG,\" for more comments on this issue.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "M. Hedlund:\n>\n>Imagine an extended newspaper archive search done through a Web browser.\n>Say the archive-server would like to avoid showing you articles you have\n>already reviewed; and say the server stores articles by week.  As you read\n>an article, the server responds with a state-info header that says \"within\n>path /week1, send state-info token w1a1.\"  The browser flits around the\n>archive and returns to /week1.  Rather than having to send one token for\n>every article reviewed (w1a1, w3a4, w5a7 ....), it just returns the\n>token(s) for week1.  The server modifies the week's index accordingly.\n\nThanks for the example, in this case, having `path=' indeed saves you\nfrom building a server side database.\n\nI believe my remark on `path=' at the time had to do with the `large\nshopping bag problem'.  In a tele shopping application, you could, by\nusing `path=', divide the shopping bag into multiple sections, each\none holding products from a different part of the shop.  But a request\non the `cash register' or `I want to order and pay for the contents of\nmy basket' URL would have to have contain the shopping bag information\nfrom all bag sections at once, which may lead to a Cookie request\nheader large enough to overrun a buffer in client, server, or proxy.\n\nSo for shopping applications, `path=' doesn't buy you that much.\nBeyond a certain bag size, you will either have to give a `your bag is\ntoo full to hold any more products' message, or switch to a backup\nscheme that stores the bag contents a server side database and uses\nthe cookie header to hold the database key.\n\n>The advantage is that you can maintain several state-registers particular\n>to parts of (or the whole) server without returning a full session history\n>with each request; the token-database is client-side.  The disadvantage is\n>that you blow away Dave's provision for an unspecified header-content.\n\nI don't quite see what you mean here.  The NAME=VALUE pairs which\ncarry the actual state in the cookie headers are just as opaque as the\nstate information in Dave's state-info header would be.\n\n>  A\n>side effect would be the need to return more than one state-info token in\n>the same request.\n\nYes.  In general, compared to state-info, Netscape cookies put greater\ncomplexity at the browser side.  With cookies, it is the browser that\nhas to keep the state for different applications on the same server\napart: with state-info, the complexity is at the server end.\n\n>I'm not saying this is optimal -- I'm happy with Dave's proposal as it\n>stands.  I am suggesting a potential middle ground.\n\nI don't think that `path=' is that controversial a feature: it is one\nof the few parts where privacy problems are almost absent.  If browser\nauthors express that they are willing to implement the associated\ncomplexity, I see no reason for not putting it in the proposed\nstandard.\n\n>M. Hedlund <hedlund@best.com>\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP  why not multiple connections",
            "content": "I think there's some interest in forming a new 'media type' working\ngroup that concentrates on Internet media-type registration issues to\nsatisfy the needs of multiple protocols, including MIME, HTTP, news,\netc., and could take on some of the issues of attribute registration,\nimage description tags, manditory auxliary information, character set\nregistration, etc. that keep coming up.\n\nIt might be appropriate to do this after the mail-extension RFCs get\npassed as proposed and that WG shuts down.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "Dave Kristol:\n>\n>Ari Luotonen <luotonen@netscape.com> wrote:\n>>Furthermore, a shortcoming in the state-info spec as opposed to the\n>>Cookie spec is that state identifiers cannot be shared between hosts\n>>(unless yet some more infrastructure is laid down on the server\n>>cluster side).  Not all systems are self-contained in a single host,\n>>and therefore there is value in being able to share cookies accross\n>>different hosts.\n>\n>This is definitely true.\n\nIt is also definitely true that the `domain=' cookie feature as\nproposed by Netscape has some serious worst case privacy problems. A\ndiscussion of this, using co.uk domains as an example I believe, was\nposted a while ago.\n\nI think a middle ground can be found here: we could either make use of\nlarge domains (like co.uk) more visible to the user, or strengthen the\nrestrictions on the `domain=' header from the current\n\n|         Only hosts within the specified domain can set a cookie for a\n|         domain and domains must have at least two (2) periods in them\n|         to prevent domains of the form: \".com\" and \".edu\".\n\nto something like\n\n          A host can only set cookies for a domain DOMAIN_NAME if its\n          host name has the form HOST.DOMAIN_NAME where HOST may not\n          contain any periods.  Also, domains must have at least two\n          (2) periods in them to prevent domains of the form: \".com\"\n          and \".edu\".\n\nWhen writing the above, I went over the domain restrictions in the\ncookie proposal again, and it occurs to me that there is also a\n_security_ problem involved.  Take for example a shopping server named\n\n www.acme.co.uk\n\nthat uses cookies like CUSTOMER=WILE_E_COYOTE to identify customers.\nNow, a malicious site\n\n www.roadrunner.co.uk\n\ncould send out the following header in its responses:\n\n Set-Cookie: CUSTOMER=BILL_GATES; path=/; domain=co.uk;\n        expires=Wednesday, 09-Nov-99 23:12:40 GMT\n\nIf WILE_E_COYOTE first goes to www.roadrunner.co.uk and then to\nwww.acme.co.uk, the acme server will get\n\n Cookie: CUSTOMER=BILL_GATES\n\nwithout any hint of the fact that this cookie was not set by the acme\nserver itself.  This will presumably drastically reduce\nWILE_E_COYOTE's chances of getting acme to send him a\nROCKET_LAUNCHER_0001.  And if the real BILL_GATES happens to be using\nacme.co.uk at the same time, lots of additional interesting things\nwill happen.\n\nSo basically, with this domain feature, you cannot trust any cookie\nvalue you get.  A malicious site could flood your server with 500\nusers all called BILL_GATES.  The problem gets even bigger if you want\nto use cookies as anonymous, but unique, user identifiers.\n\nHm.  It seems that to completely solve this cookie spoofing problem,\neven my strengthening for the domain restriction above will not be\nsufficient.  There are a number of ISPs that offer host names like\ncompanya.isp.net and companyb.isp.net to competing companies.  Maybe\nthe hostname of the host that last changed the cookie will have to be\nsent along with any Cookie that is set for a domain?\n\n[...]\n>[Sorry, Larry M.]  Yes, I agree that Cookies are used and work for\n>real-life applications.\n\nFor the record, Lou Montulli recently sent me mail saying that he was\ntold that the number of sites using cookies is in the hundreds.\n\n>  I welcome the possibility to improve them.  Many\n>of us await a more detailed specification of the current Cookie mechanism\n>so we can do just that.\n\nCount me among the waiting.  Netscape has been very silent about\ncookies on this list in the past.  Am I the first one to think up the\nabove security problem, or have things like this already been found\nand solved within Netscape without us knowing about it?\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "I think the issue between opaque validators vs. header-based\nvalidators is one of where you expect the additional implementation\nwork to be if a different value is chosen as validator.\n\nIF: {eq {Content-MD5 \"blah\"}}\n\nor\n\ncontent-validator: md5:blah\n\n====\n\nIF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 gMT\"}}\n\nor\n\ncontent-validator: lastmodified:19951201141306\n\nThese are logically equivalent in terms of protocol, but using IF the\ncache has to decide which field is relevant for determining cache\nvalidity, while in the latter case, the server decides which field is\nvalid.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "At 2:13 PM 12/10/95, Koen Holtman wrote:\n>I believe my remark on `path=' at the time had to do with the `large\n>shopping bag problem'.  In a tele shopping application, you could, by\n>using `path=', divide the shopping bag into multiple sections, each\n>one holding products from a different part of the shop.  But a request\n>on the `cash register' or `I want to order and pay for the contents of\n>my basket' URL would have to have contain the shopping bag information\n>from all bag sections at once, which may lead to a Cookie request\n>header large enough to overrun a buffer in client, server, or proxy.\n>\n>So for shopping applications, `path=' doesn't buy you that much.\n\nYou're assuming one 'checkout' operation for the whole site.  At a\n\"mall\"-type site, each store could have its own directory and checkout\noperation.  In this case, 'path' would prevent the browser from sending\nevery cookie for the whole mall with every request.\n\n>>The disadvantage [of path] is\n>>that you blow away Dave's provision for an unspecified header-content.\n>\n>I don't quite see what you mean here.  The NAME=VALUE pairs which\n>carry the actual state in the cookie headers are just as opaque as the\n>state information in Dave's state-info header would be.\n\nYes, you're right.  I meant only that the inclusion of path would shift\naway from the whole header value being opaque, to one attribute of the\nvalue being opaque.  Minor point.\n\nMarc Hedlund, Organic Online <marc@organic.com>\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "> I think the issue between opaque validators vs. header-based\n> validators is one of where you expect the additional implementation\n> work to be if a different value is chosen as validator.\n> \n> IF: {eq {Content-MD5 \"blah\"}}\n> \n> or\n> \n> content-validator: md5:blah\n> \n> ====\n> \n> IF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 gMT\"}}\n> \n> or\n> \n> content-validator: lastmodified:19951201141306\n> \n> These are logically equivalent in terms of protocol, but using IF the\n> cache has to decide which field is relevant for determining cache\n> validity, while in the latter case, the server decides which field is\n> valid.\n\nThere is more to it than that.  A separate validator field would have\nto be generated by all servers for all cachable resources, consisting\nof an opaque value which is only usable for metadata comparison (i.e.,\nit does nothing to ensure that the entity received is the same as that\nsent by the origin server).  It requires that the server be capable and\nwilling to generate this opaque validator even when the entity is\nnot directly controlled by the server.\n\nIn contrast, IF does not make any assumptions or special requirements\non the information being compared.  If an opaque value is available,\nthen it can be compared.  If an MD5 is available, then it can be\nused as both an MD5 checksum and for cache validation.  If any\nuseful metainformation (as judged by the client) is available, then\nit can be used within a comparison.\n\nMost importantly, we don't have to specify the interaction between\nN types of preconditions if we only use one precondition field.\n\nBTW, the logic bag syntax was designed to be tokenized within a\nbinary HTTP.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: logic bag synta",
            "content": ">BTW, the logic bag syntax was designed to be tokenized within a\n>binary HTTP.\n\nTurning an expression into a tree or quads or some form of binary is\npretty common knowledge, or readily available for the cost of the new\nDragon book.  Nontheless, if \"If:\" survives I'd like to see the syntax\nlook more like something already known, such as C, Lisp, etc.  The semi-\nTCL-syntax and the FortranIV operators make for a funky mix.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: twophase send concern",
            "content": "> Roy has taken a pessimistic approach to the byte-deluge problem.\n> In particular, every use of every method with a two-phase requirement\n> must pay either the arbitrary timeout period OR at least one\n> round-trip time.  In other words, by taking this approach, we\n> build in \"extra\" delay to every POST (etc.) invocation for millions\n> of users for many years.  Hmm.\n\nYes, this is an excellent analysis of the tradeoffs.  However, it fails\nto consider that all of the two-phase methods are not speed-dependent:\nit is far more important to get it right the first time (before data\nis sent across the wire, if possible) then it is to save one or two\nround-trips.  Given that, it is okay to be pessimistic and\npay for that round-trip every time.\n\n> So here's an alternate suggestion (mostly as a way to start people\n> thinking about additional suggestions, although I think it would\n> work).\n> \n> If the server doesn't want to receive the large body,\n> it immediately replies with its 4xx or 5xx response,\n> and immediately closes (not resets) the connection.\n> \n> If the client manages to read the 4xx or 5xx response,\n> it must honor it and should reflect it to the user.\n> \n> If the client simply sees the transport connection\n> disappear, then the language in section 1.4, \"Overall\n> Operation\",\n>    Both clients and servers must be capable of handling cases\n>    where either party closes the connection prematurely, due to\n>    user action, automated time-out, or program failure. In any\n>    case, the closing of the connection by either or both\n>    parties always terminates the current request, regardless of\n>    its status.\n> applies with one variation:\n>    if the aborted request is a POST, PUT, PATCH, etc. (i.e.,\n>    any request with a non-empty entity body) then if the\n>    client repeats the aborted request, it MUST include\n>    the request-header:\n> Connection: two-phase\n>    and then follow Roy's two-phase protocol: wait for\n>    a response or for an specified timeout period before\n>    proceeding.\n> \n> A client MAY use\n> Connection: two-phase\n> at other times, on its own initiative, but MUST use it\n> when repeating a request because of a prematurely-closed\n> connection.\n\nYes, this might work also -- it depends on whether or not we are willing\nto send a bunch of data downstream before getting the response.  Slow start\nmay not apply here, since this request may be in the middle of a persistent\nconnection.\n\n> We might also want to do this:\n> Servers rejecting an entity-body for reasons of size\n> must respond with \n> 413 Request too large\n> \n> Once a client has received a 413 response from a server,\n> it SHOULD always use\n> Connection: two-phase\n> and the two-phase protocol with that server in the future.\n> This is actually not a big cost, because we know that\n> a server which has sent us a 413 response is an HTTP 1.1\n> server, and so the client should (almost?) never need to wait\n> much longer than one RTT for the two-phase mechanism\n> to complete.\n\nYep, we should assign 413 for that error, and use Retry-After for\ncases when it is a temporary injunction -- both good ideas.\n\n> Several people have pointed out that 5 seconds seems like a\n> poor choice for a time.  It appears to be a compromise between\n> \"give the server enough time to respond\" and \"don't delay too\n> long waiting for HTTP 1.0 servers, which won't send a Continue\n> response.\"  But in the optimistic approach, we only invoke the\n> N-second wait in the (hopefully) rare case of a prematurely\n> closed connection.  So N could be somewhat larger than 5 (say,\n> 10 or 20) without impacting the normal case.\n\nI don't think \"poor choice\" is the word -- it is an arbitrary choice\nbased on typical overall request timeouts (30 seconds) and user\nannoyability (anything less than 10 seconds is probably okay) and the\n\"normal\" round-trip time of poor network connections over long\ndistances.\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "Roy T. Fielding writes:\n > > I think the issue between opaque validators vs. header-based\n > > validators is one of where you expect the additional implementation\n > > work to be if a different value is chosen as validator.\n > > \n > > IF: {eq {Content-MD5 \"blah\"}}\n > > \n > > or\n > > \n > > content-validator: md5:blah\n > > \n > > ====\n > > \n > > IF: {eq {Last-Modified \"Fri, 01 Dec 1995 14:13:06 gMT\"}}\n > > \n > > or\n > > \n > > content-validator: lastmodified:19951201141306\n > > \n > > These are logically equivalent in terms of protocol, but using IF the\n > > cache has to decide which field is relevant for determining cache\n > > validity, while in the latter case, the server decides which field is\n > > valid.\n > \n > There is more to it than that.\n\nAnd less:  the content-validator header should NOT specify\n\"lastmodified\", \"md5\", or any other specific algorithm.  If it did, it\nwould hardly be opaque, would it?\n\n  A separate validator field would have\n > to be generated by all servers for all cachable resources, consisting\n > of an opaque value which is only usable for metadata comparison (i.e.,\n > it does nothing to ensure that the entity received is the same as that\n > sent by the origin server).  It requires that the server be capable and\n > willing to generate this opaque validator even when the entity is\n > not directly controlled by the server.\n > \n\nI think it would be helpful if you would explain these claims rather\nthan just claiming them.  Yes, the header would need to be present for\nany cachable resource (except for backwards compatibility with 1.0).\nBut why do you say it is only usable for metadata comparison?  If a\npart of a server is configured to use algorithm X to determine its own\nstated content-validator, then that part of the server must be able to\nrespond to requests that use content-validators as generated by\nalgorithm X, no?  And isn't it only the origin server that has to\nworry about generating these headers?  \n\n\n > In contrast, IF does not make any assumptions or special requirements\n > on the information being compared.  If an opaque value is available,\n > then it can be compared.  If an MD5 is available, then it can be\n > used as both an MD5 checksum and for cache validation.  If any\n > useful metainformation (as judged by the client) is available, then\n > it can be used within a comparison.\n > \n\nThe point of the opaque validator is to remove the smarts from the\nclient side.  It really seems like there are multiple issues being\ndiscussed at once, which should be being discussed separately:\n\n1. What are the foreseeable \"high-level\" reasons for doing conditional\nrequests, and how should those conditional requests be encoded in the\nprotocol?  We have yet to see a plausible scenario that demonstrates\nthis need.  Without stated requirements this seems like an exercise in\nfutility.\n\n2. Is there a requirement or benefit of having a general case solution\nto this that outweighs its complexity and the difficulty of specifying\nthe semantics exactly?  General case solutions are nice, where there\nis a general case problem to be solved, but the added hair of having\nto put an expression parser in at this level seems quite questionable\nwithout a definite need.\n\n3. Should we mix the high level mechanism with the low-level\ncache-integrity mechanisms?  What are the benefits/costs of that?\nIt seems to me that cache-integrity concerns are a low level and\nencapsulatable piece of the puzzle, and that trying to mix it with\na condition about, say, the price of some product seems like \nmixing levels in a most unfortunate way.   (Don't cross the beams!)\nThe cache integrity problem is, or at least should be, well defined,\nand should not require an interpreter to solve.\n\n > Most importantly, we don't have to specify the interaction between\n > N types of preconditions if we only use one precondition field.\n > \n\nDoesn't backwards compatiblity already imply that this is required?\n\n\n > BTW, the logic bag syntax was designed to be tokenized within a\n > binary HTTP.\n > \n > \n >  ...Roy T. Fielding\n >     Department of Information & Computer Science    (fielding@ics.uci.edu)\n >     University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n >     http://www.ics.uci.edu/~fielding/\n\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: logic bag synta",
            "content": ">>BTW, the logic bag syntax was designed to be tokenized within a\n>>binary HTTP.\n> \n> Turning an expression into a tree or quads or some form of binary is\n> pretty common knowledge, or readily available for the cost of the new\n> Dragon book.  Nontheless, if \"If:\" survives I'd like to see the syntax\n> look more like something already known, such as C, Lisp, etc.  The semi-\n> TCL-syntax and the FortranIV operators make for a funky mix.\n\n:)\n\nDan and I had a lengthy discussion on that topic when we hashed out the\nsyntax.  Use of parentheses \"()\" would seem better, but they are already\nused as the notation for comments in RFC 822 and HTTP.  The operators\nare token names because it reduces parsing problems and provides a\nconsistent hook name for object-oriented systems.  Overall, the syntax\nis intended to be embeddable in many different contexts.\n\nThe reason an existing language was not used is that:\n\n  1) security concerns forbid using an existing language interpreter anyway;\n  2) I didn't want to start a language war;\n  3) I wanted to restrict it to tree-based S-expressions.\n\nTCL had no influence on this design -- Perl is a more likely candidate,\nthough the only real concern is that the syntax be self-contained and\nself-describing (i.e., you don't need to know the operator in order to\ncorrectly build the expression tree).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "contentlength vs. boundary marker",
            "content": "> John Ludeman <johnl@microsoft.com> writes:\n\n> I disagree.  In the case of HTTP, byte counting is *not* doing a \n> strncpy of bytes.  When receiving a chunk of data, you generally give \n> the buffer directly to the network layer which fills in the buffer.  To \n> then have to scan this buffer *does* significantly add a performance \n> hit to the server.\n\nUh, the server doesn't have to scan the data. It just sends the data\nwith a random boundary marker. The client has to scan the data, but\nthe client's scanning the data anyway.\n\nContent-length is either impractical or inefficient when the data is\ncomputed or being translated on-the-fly, and unreliable when serving\nfiles for which there might be asynchronous updates. \n\n> Byte counts are good.  Real protocols use byte counts. Let's make sure \n> we move in that direction.\n\nAs far as I know, real protocols only use small byte counts. 128.\n1024. Not 1023523.\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "On Sun, 10 Dec 1995, Koen Holtman wrote:\n\n>           host name has the form HOST.DOMAIN_NAME where HOST may not\n>           contain any periods.  Also, domains must have at least two\n>           (2) periods in them to prevent domains of the form: \".com\"\n>           and \".edu\".\n\nWell, your proposed rule would, if I understand it, preclude my host\nand root domain:\n   xpasc.com\nfrom using the facility.  To preclude domains such as '.com' two periods\nisn't the answer.  One needs at least two non-null name parts separated by\nperiods.\n\nIn any case, if security is an issue, surely your hypothetical user will\nuse SSL or SHTTP? Perhaps the protection should be based on some aspect\nof the SSL / SHTTP key certificate. \n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": ">   A separate validator field would have\n >> to be generated by all servers for all cachable resources, consisting\n >> of an opaque value which is only usable for metadata comparison (i.e.,\n >> it does nothing to ensure that the entity received is the same as that\n >> sent by the origin server).  It requires that the server be capable and\n >> willing to generate this opaque validator even when the entity is\n >> not directly controlled by the server.\n >> \n> \n> I think it would be helpful if you would explain these claims rather\n> than just claiming them.  Yes, the header would need to be present for\n> any cachable resource (except for backwards compatibility with 1.0).\n\nWhich means that no 1.0 resource (or script designed for 1.0) can\ngenerate something useful for cache validation.  Given the presence of\nhierarchical caching, this is sufficient to reject the special-purpose\ncase as not fulfilling the requirements for HTTP/1.1.\n\n> But why do you say it is only usable for metadata comparison?  If a\n> part of a server is configured to use algorithm X to determine its own\n> stated content-validator, then that part of the server must be able to\n> respond to requests that use content-validators as generated by\n> algorithm X, no?  And isn't it only the origin server that has to\n> worry about generating these headers?  \n\nNo and no.  Only the recipient can test for message integrity of the\nmessage received, and to do so they need to know the algorithm used\nto generate the validator.  If the validator is something useful, like\nContent-MD5 or Content-SHA or Content-Checksum or even Content-Length,\nthen it can be used for both message integrity checks AND validation,\nwhich means you don't duplicate information supplied for the special case.\n\n >> In contrast, IF does not make any assumptions or special requirements\n >> on the information being compared.  If an opaque value is available,\n >> then it can be compared.  If an MD5 is available, then it can be\n >> used as both an MD5 checksum and for cache validation.  If any\n >> useful metainformation (as judged by the client) is available, then\n >> it can be used within a comparison.\n >> \n> \n> The point of the opaque validator is to remove the smarts from the\n> client side.\n\nNo.  The point is to provide reliable validation.  There is no reason\nwhy this cannot be done just as easily and just as reliably within an\nextensible syntax, and with whatever validation-capable metainfo is\npresent in any given cachable entity, as it would be to do so for just\na special case.  Therefore, the special case loses.\n\n> It really seems like there are multiple issues being\n> discussed at once, which should be being discussed separately:\n> \n> 1. What are the foreseeable \"high-level\" reasons for doing conditional\n> requests, and how should those conditional requests be encoded in the\n> protocol?  We have yet to see a plausible scenario that demonstrates\n> this need.  Without stated requirements this seems like an exercise in\n> futility.\n\nI have already provided several.  As far as I am concerned, you must\nprove that they are not plausible, since the solution provided does\nsatisfy the needs of opaque validation.  Your requirements are fulfilled\nby a general syntax, my requirements are not fulfilled by a special-case\nsyntax, and therefore the only reasonable design is the general case.\n\n> 2. Is there a requirement or benefit of having a general case solution\n> to this that outweighs its complexity and the difficulty of specifying\n> the semantics exactly?  General case solutions are nice, where there\n> is a general case problem to be solved, but the added hair of having\n> to put an expression parser in at this level seems quite questionable\n> without a definite need.\n\nI have already answered this question twice.  There is no semantic\nambiguity and no additional complexity if reasonable constraints are\nplaced on the set of required expressions.\n\n> 3. Should we mix the high level mechanism with the low-level\n> cache-integrity mechanisms?  What are the benefits/costs of that?\n\nIrrelevant -- both represent the same semantics for interpreting\nthe request, and therefore are at the same level within HTTP.\n\n >> Most importantly, we don't have to specify the interaction between\n >> N types of preconditions if we only use one precondition field.\n> \n> Doesn't backwards compatiblity already imply that this is required?\n\nNo, it doesn't -- allowing additional expressions does not change\nthe semantics of IF.  Using separate header fields for every precondition\ndoes change the semantics of interpreting the request for each additional\nfield.  I KNOW THIS to be true because I've written and rewritten the HTTP\nspecification over 60 times now and can see this effect every time a new\nrequest header field is added.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "I'm becoming more and more convenced that single-exchange PUT is\nunworkable. That is, PUT might have to happen in two steps:\n\n\n\n1. client: wanna PUT this file of this size of this media type \n     in this location with these PERMS with\n   this validator \n2. server: OK, send file (or permission denied)\n3. client: OK, here's the file.\n4. server: OK, got the file\n\nStep 1 might offer more than one media type with step 2 accepting a\nparticular one or a subset. Step 1 might say that it doesn't know the\nfile size, for example.\n\nOpinions?\n\n\n\n"
        },
        {
            "subject": "Re: making progress on StateInf",
            "content": "David W. Morris:\n>\n>On Sun, 10 Dec 1995, Koen Holtman wrote:\n>\n>>           host name has the form HOST.DOMAIN_NAME where HOST may not\n>>           contain any periods.  Also, domains must have at least two\n>>           (2) periods in them to prevent domains of the form: \".com\"\n>>           and \".edu\".\n>\n>Well, your proposed rule would, if I understand it, preclude my host\n>and root domain:\n>   xpasc.com\n>from using the facility.\n\nIf you have only one host, you do not need to share cookies between\nhosts, so you have no use for the facility anyway.  Or do you also\nhave hosts like www1.xpasc.com and www2.xpasc.com?\n\nI did not intend my proposed rule to be a 100% solution for this\nproblem: I think no rule that only uses simple minded matching on host\nand domain names can be.\n\n[...]\n>In any case, if security is an issue, surely your hypothetical user will\n>use SSL or SHTTP? Perhaps the protection should be based on some aspect\n>of the SSL / SHTTP key certificate. \n\nMy hypothetical user is not a computer expert: he will in general not\neven be aware of the possibility that the web can be insecure, nor\nwill he know that he really ought to use secure protocols for some\napplications.  Also, there is a 30% chance that the software used by\nmy hypothetical user does not support any form of encryption.\n\nMy hypothetical user will simply expect all shopping sites to work\nwith his software.  If acme.co.uk thinks he is called BILL_GATES, it\nwill be acme.co.uk that he will blame.  On broadcast TV.\n\nAs usual, it is the responsibility of the service provider to\nimplement a working site.  If the only way to prevent cookie spoofing\nis to require your users to use a secure protocol (as seems to be the\ncase now), cookies become a lot less usable.\n\nThis brings the number of cookie disaster scenarios waiting to happen\nto 2:\n\n1) If less ethical service providers implement a large scale user\ntracking scheme using persistent cookies, the discovery of that\nscheme, and subsequent media coverage, may cause a crisis in user\ntrust that will impact on the acceptance of all web applications.\n\n2) Sites may loose their reputation as a result of cookie spoofing\nattacks.\n\n>Dave Morris\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Larry Masinter <masinter@parc.xerox.com> wrote:\n> I'm becoming more and more convenced that single-exchange PUT is\n> unworkable. That is, PUT might have to happen in two steps:\n\n> 1. client: wanna PUT this file, various parameters ...\n> 2. server: OK, send file (or permission denied)\n> 3. client: OK, here's the file.\n> 4. server: OK, got the file\n\nThis seems logical to me.  Neither FTP (which uses a second connection)\nnor SMTP (which uses a two-step request reply on a single connection),\nthe two (IMHO) most widely implemented high-level protocols around, can\ndo this in one step.  So you've got a good precedent for the two-step\napproach\n\n\n   o _ |_ __  +  |_  _  _ |/    Software Developer / Web Wizard\n   |(_)| || | (_ |_)(/_(_ |\\    Pangaea Reference Systems\n  \\/  ___________________)      http://www.reference.com/~jbeck\n     (______________            jbeck@reference.com\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns  ",
            "content": "For what it's worth, latency for round-trips are currently the a major\nlimiting factor in large-scale NNTP these days.  I'd be wary of forcing\na two-step method, particularly if you ever want to do Usenet-scale over\nHTTP.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "    I'm becoming more and more convenced that single-exchange PUT is\n    unworkable. That is, PUT might have to happen in two steps:\n    \n    1. client: wanna PUT this file of this size of this media type \n       in this location with these PERMS with\n       this validator \n    2. server: OK, send file (or permission denied)\n    3. client: OK, here's the file.\n    4. server: OK, got the file\n\nTwo questions:\n\n(1) What is the purpose of the validator in this example?  If you\nmean \"cache validator\", then presumably this is a token that the\nclient has obtained from the server which can be used to decide\nif the client's cached version of the file is still valid.  This\nmight be useful if we want to ensure that some other client doesn't\ncome along midway through this exchange (i.e., between steps 2 and\n3) and update the same file.  But if we want to make this work,\nthen we need to be honest about supporting atomic transactions.\n\nFor example, in this case if the server \"locks\" the file against\nupdates from other clients, what happens if the client then crashes\nbefore step 3?  How long does the lock persist?\n\nIf we are going to introduce transaction semantics, let's do that\ndirectly, not with some half-way approach.\n\n(2) Someone else (sorry, I forgot who) has already pointed out\nthat this adds at least one RTT of latency.  As with my comments\non the two-phase send mechanism, I'd ask: what is the common\ncase, and can we optimize for it?\n\nI would assume that the common case is that the server accepts\nthe PUT (otherwise, people would soon stop doing it).  So to me\nthis implies that the protocol should avoid extra RTTs if the\nPUT succeeds, even if that means a certain amount of messy\nstate reconstruction if the PUT fails.  I think this is another\nargument in favor of the \"optimistic two-phase\" approach that\nI outlined last week: try doing the PUT in a single step, and\nthen if it fails for any reason (e.g., the server closes the\nconnection prematurely), try again with the cautious two-phase\nmethod.\n\nOf course, a client that manages to read the server's error\ncode before the first connection is aborted can optimize\nout the second attempt, if the error code is definitive\n(e.g., permission denied).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Another major limitation of the PUT (or PATCH) transaction is that the client  \nnever knows which version of the data is about to be overwritten. This note  \nis a heads-up to the list that I and few colleagues are designing (& proving)  \na resource-leasing technique that may be the answer to both the two-phase and  \nlocking problems of PUT.\n\nAn excerpt from the document:\n\n>  The Web, in its guise as a document publishing\n>  system, has always been envisioned as an editable\n>  medium. To that end, the HTTP standard includes\n>  specification not only of the common GET and POST\n>  methods, but also PUT and DELETE. However, PUT\n>  blindly replaces the old data for a resource with\n>  the contents of the new message -- the client\n>  can never be sure of what data is being\n>  overwritten. This is the \"lost-update\" problem.\n>\n>  First, we can extend HTTP to detect lost-updates:\n>  each PUT request  and reply includes the\n>  Content-Version of the data being modified\n>  (Content-Version is some opaque  identifier, like\n>  an MD5 hash). If the two differ, some update has\n>  been lost.\n>\n>  Second, we can prevent  lost-updates through an\n>  HTTP mechanism for preconditiong  actions. We\n>  can send a header that only enables a \"conditional\n>  PUT\" iff the current Content-Version at the\n>  server's end matches the client's expectations.\n>  However, a particular client may never succeed\n>  at updating the resource, since other processes\n>  may modify the Content-Version before it can post\n>  its update.\n>\n>  Third, we can guarantee progress by explictly\n>  granting clients exclusive update privileges for\n>  a fixed time interval. That is, clients may\n>  request leases on resources frm servers, and be\n>  guaranteed a window to return with updates.\n>\n>  Fourth, we can plan a fair lease-granting system\n>  that guarantees that each contending process will\n>  indeed receive a lease in finite time. We propose\n>  a new techniques, based on Lamport's Bakery\n>  algorithm, called Auctions, which is specifically\n>  adapted for HTTP.\n\nBasically, you probe (in GET or HEAD) to get a lease; if you do get a lease,  \nfor the next delta-t seconds, you're entitled to use that lease in a  \nAuthorization: header (using the LEASE scheme) to execute an exclusive PUT or  \nPOST or PATCH.\n\nRohit Khare\nkhare@w3.org\n\n\n\n"
        },
        {
            "subject": "Connection Heade",
            "content": "I knew I should have replied to these messages yesterday,\nbut the need for sleep got the better of me.  Enabling multiple\nrequests on a single connection will be the primary goal for HTTP/1.1.\nHowever, before that happens, we needed a solid basis for HTTP/1.0 --\none which is compatible with correct current practice and yet does\nnot prevent future extensibility.  Thus, in the process of writing\nHTTP/1.0, Henrik and I have been figuring out what needs to be done\nfor HTTP/1.1.  In this way, we have identified several aspects of\n\"current practice\" which must be fixed before standardizing 1.0.\nMost of these we have already talked about (i.e. being able to parse\nmedia types correctly so that parameters can be used, enabling\nsomething other than a closed connection to mean end-of-body, etc.).\n\nSimon briefly mentioned another problem that we have yet to discuss\non the mailing list:\n\n> One other thing that was discussed was the relative advantages of using a \n> session method vs. an ignorable header. It turns out that there is a problem\n> with using ignorable headers when proxies are used - if a proxy which doesn't\n> interpret the header is used to talk to a server which does handle the header,\n> the connection can become deadlocked (the end server things that the \n> proxy doesn't want it to drop the connection, whilst the proxy is sitting\n> there waiting for the connection to drop). \n\nAs always, all the real work at the IETF was done in the hallways, so\nmany of you at the BOF may not have heard about this either.  The essential\nproblem is that HTTP intermediaries (proxies) treat everything they\ndon't understand as being something they should pass on to the destination\nhost.  Unfortunately, that means they would pass on stuff that was only\nintended for them (like connection setup information), thus fooling the\nend-server into thinking that the proxy wants that connection setup applied.\nWorse, I believe this applies equally for both ignorable headers and\nunknown methods!\n\nThus, we need a way for all proxies/servers, both HTTP/1.0 and 1.1, to be\nable to identify information which must not be passed downstream.  Henrik\nand I (with help from many others, some of whom have asked to remain\nanonymous ;-) worked out a possible solution in two parts:\n\n==========================================================================\n\n1) a SESSION method\n\n   Like what was discussed at the BOF, except current proxies would\n   be changed such that no SESSION requests were passed-on -- in effect,\n   this would be the same as a NOP command, but allow connection info\n   to be passed in the headers.\n\n2) a Connection header\n\n   The Connection header is used to specify the parameters (desired or actual)\n   of the current connection. Clients can use this header to indicate\n   their desire to use a set of connection options. Servers can use this\n   header to indicate what options are actually being applied.  This\n   field applies only to the current connection -- receivers should not\n   cache or otherwise save the connection information after the\n   connection is closed. Proxies must not forward this header, though\n   they may generate a separate Connection header for their own\n   connections.\n\n      Connection      = \"Connection\" \":\" 1#connect-option\n      connect-option  = token [ \"=\" word ]\n\n   Although HTTP/1.0 clients and servers do not make use of the\n   Connection header outside of experiments, this field will be necessary\n   to enable future extensibility of connection-specific behavior. Most\n   importantly, HTTP/1.0 proxies need to know that they must not forward\n   this header even when they do not understand or make use of its\n   contents. For example, an experimental client may send:\n\n      Connection: keep-alive\n\n   to indicate that it desires to keep the connection open for multiple\n   requests. The server may then respond with a message containing:\n\n      Connection: keep-alive, timeout=10, maxreq=5\n   \n   to indicate that the connection will be kept open for a maximum of 5\n   requests, but will timeout if the next request is not received within\n   10 seconds. Note that the semantics of these options are not defined\n   for HTTP/1.0, though similar options may be defined by future versions\n   of HTTP.\n\n==========================================================================\n\nNote that the above two are independent -- a Connection header could\nbe applied to a GET, HEAD, PUT, POST, etc.  It does require a change to\ncurrent practice, but only for proxies.  Personally, I can't think of any\nother way of doing it without disallowing proxies altogether.\n\nThe above is what I plan on putting in the next draft of the HTTP/1.0 spec.\nNow would be a good time to hack it to pieces, if you are so inclined.\n\nOne idea that has been mentioned is that the Connection header should\nbe a list of other header-names, thus allowing complete flexibility.\nI do not favor that approach, however, because \n\n   a) it adds more overhead to the request\n   b) it would require more work on the part of proxies, and\n   c) it would require that Connection be the first header received\n      (of those named).\n\nComments, please?\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "HELP\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "Roy T. Fielding writes:\n > >   A separate validator field would have\n >  >> to be generated by all servers for all cachable resources, consisting\n >  >> of an opaque value which is only usable for metadata comparison (i.e.,\n >  >> it does nothing to ensure that the entity received is the same as that\n >  >> sent by the origin server).  It requires that the server be capable and\n >  >> willing to generate this opaque validator even when the entity is\n >  >> not directly controlled by the server.\n >  >> \n > > \n > > I think it would be helpful if you would explain these claims rather\n > > than just claiming them.  Yes, the header would need to be present for\n > > any cachable resource (except for backwards compatibility with 1.0).\n > \n > Which means that no 1.0 resource (or script designed for 1.0) can\n > generate something useful for cache validation.  Given the presence of\n > hierarchical caching, this is sufficient to reject the special-purpose\n > case as not fulfilling the requirements for HTTP/1.1.\n > \n\nWhether any new mechanism for validation is handled as a special case \nheader or part of a general case expression is orthogonal to whether backward\ncompatibility has to be supported.\n\nIf more than one kind of header (e.g. content-MD5, content-length,\nlast-modified) is supported for validation, either for backward\ncompatibility or as a continuing design feature, it seems to me that\nthe set of possible logical expressions is highly constrained.  It\nseems improbable (and incorrect) for clients to do anything except for\nstrict equality tests for MD5 or content-length, or GT tests for\nmodification-date, for instance.  And clients can determine the best\nheader to include in a request by virtue of which header was\npreviously received in association with the requested resource.\n\n   \n > > But why do you say it is only usable for metadata comparison?  If a\n > > part of a server is configured to use algorithm X to determine its own\n > > stated content-validator, then that part of the server must be able to\n > > respond to requests that use content-validators as generated by\n > > algorithm X, no?  And isn't it only the origin server that has to\n > > worry about generating these headers?  \n > \n > No and no.  Only the recipient can test for message integrity of the\n > message received, and to do so they need to know the algorithm used\n > to generate the validator.\n\nAha!  Now we're exposing the difference of various people's models.\nIn Jeff's \"opaque validator\" model, the client never uses the\nvalidator except as a token to pass back to the server. You're using\nthe term in a different way.  Of all the possible headers that the\nclient even *could* use to support your meaning of the term, only\nContent-Length and Content-MD5 would be meaningful.  (or maybe the\nothers you mention below, which don't seem to be spec'ed yet).  Clearly your\nusage is also useful (don't get me wrong), but it is a completely\ndifferent meaning of the term.  Whether we should allow \"punning\" --\nusing the same header for content-verification and for\ncache-validation -- is a reasonable question to ask.  Clearly the\nopaque validator model disallows that.\n\n  If the validator is something useful, like\n > Content-MD5 or Content-SHA or Content-Checksum or even Content-Length,\n > then it can be used for both message integrity checks AND validation,\n > which means you don't duplicate information supplied for the special case.\n > \n\nRight -- using the opaque validator model, the server would have to\nsend a separate header if it wanted to provide for clients checksumming the\ndata received.  (But is that *bad*?)\n\n >  >> In contrast, IF does not make any assumptions or special requirements\n >  >> on the information being compared.  If an opaque value is available,\n >  >> then it can be compared.  If an MD5 is available, then it can be\n >  >> used as both an MD5 checksum and for cache validation.  If any\n >  >> useful metainformation (as judged by the client) is available, then\n >  >> it can be used within a comparison.\n >  >> \n\nThis is orthogonal to whether an IF expression is used or\nwhether separate request headers are used.  In either case,\nan *opaque* token can't be interpreted by the client.\n\nI think it is an open issue whether, in HTTP 1.1, we should also allow for\nusing non-opaque headers for cache validation except for backward\ncompatibility.  I continue to have the feeling that that is what the\ndiscussion about IF is actually about, and separating these issues is important.\n\n > > \n > > The point of the opaque validator is to remove the smarts from the\n > > client side.\n > \n > No.  The point is to provide reliable validation.\n\nEither you already know what I meant, in which case it is pointless to argue,\nor you don't, in which case it is also pointless to argue.\n\n  There is no reason\n > why this cannot be done just as easily and just as reliably within an\n > extensible syntax,\n\nI agree -- the syntax is not the most important issue.\n\n  and with whatever validation-capable metainfo is\n > present in any given cachable entity,\n\nIf we're talking about pure 1.1 <=> 1.1 communication, I disagree.  We\nhave the opportunity to define it -- we don't have to support multiple\nmechanisms with widely different (and antagonistic) underlying\nphilosophies, and if we do, then (I claim) it will be a strong sign of\ndesign by committee.\n\n as it would be to do so for just\n > a special case.  Therefore, the special case loses.\n > \n\nI disagree.  Again, we're confusing discussion of validation\nalgorithms with discussion of syntax.  The fact that the IF syntax\nallows for more general manipulations does not get down to the brass\ntacks of the correctness of the basic validation algorithm.  If this\ndiscussion is actually about a lack of consensus on what the basic\nvalidation algorithm should be, let's work on those issues, and not\ntry to push it off to the indefinite future by providing a general\nmechanism whose primary function seems to be to allow us to avoid\nworking through the hard issues of how caching should actually work.\n\n\n > > It really seems like there are multiple issues being\n > > discussed at once, which should be being discussed separately:\n > > \n > > 1. What are the foreseeable \"high-level\" reasons for doing conditional\n > > requests, and how should those conditional requests be encoded in the\n > > protocol?  We have yet to see a plausible scenario that demonstrates\n > > this need.  Without stated requirements this seems like an exercise in\n > > futility.\n > \n > I have already provided several.  As far as I am concerned, you must\n > prove that they are not plausible, since the solution provided does\n > satisfy the needs of opaque validation.  Your requirements are fulfilled\n > by a general syntax, my requirements are not fulfilled by a special-case\n > syntax, and therefore the only reasonable design is the general case.\n > \n\nI looked back at previous posts, and the closest thing I could find to\na requirement statement was that future extensions for preconditions\nshould not change the protocol.  I agree that this is a highly desirable\ngoal.  What I find questionable is whether this extension mechanism\nneeds to be the same mechanism as is used for cache validation.\n\n > > 2. Is there a requirement or benefit of having a general case solution\n > > to this that outweighs its complexity and the difficulty of specifying\n > > the semantics exactly?  General case solutions are nice, where there\n > > is a general case problem to be solved, but the added hair of having\n > > to put an expression parser in at this level seems quite questionable\n > > without a definite need.\n > \n > I have already answered this question twice.  There is no semantic\n > ambiguity and no additional complexity if reasonable constraints are\n > placed on the set of required expressions.\n > \nSee Koen's response to your \"price\" example.\n\n\n > > 3. Should we mix the high level mechanism with the low-level\n > > cache-integrity mechanisms?  What are the benefits/costs of that?\n > \n > Irrelevant -- both represent the same semantics for interpreting\n > the request, and therefore are at the same level within HTTP.\n > \n\nRelevant -- I'd rather not have it be possible for goofy logical\nexpressions to mess up caches, while I'm perfectly happy for them to \nbe used for arbitrarily complex conditional gets based on things\nnobody has thought of yet.  As such, I'd rather view it as an optional\nextension mechanism. \n\n >  >> Most importantly, we don't have to specify the interaction between\n >  >> N types of preconditions if we only use one precondition field.\n > > \n > > Doesn't backwards compatiblity already imply that this is required?\n > \n > No, it doesn't -- allowing additional expressions does not change\n > the semantics of IF.\n\nBut IF already must interact with other headers, which are still in\nthe spec, my only point above.\n\n  Using separate header fields for every precondition\n > does change the semantics of interpreting the request for each additional\n > field.  I KNOW THIS to be true because I've written and rewritten the HTTP\n > specification over 60 times now and can see this effect every time a new\n > request header field is added.\n > \n\nYes, I cannot argue with that.  But I'm coming to my own conclusion\nthat IF, if it stays in the spec, should be optional, and a\nCache-Validator header should become the fundamental means of cache\nvalidation.\n\n\n >  ...Roy T. Fielding\n >     Department of Information & Computer Science    (fielding@ics.uci.edu)\n >     University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n >     http://www.ics.uci.edu/~fielding/\n\n\nAnd now, back to the salt mines.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Confusion over caching (was Re: Logic Bag concerns",
            "content": "According to Shel Kaphan:\n> \n> Yes, I cannot argue with that.  But I'm coming to my own conclusion\n> that IF, if it stays in the spec, should be optional, and a\n> Cache-Validator header should become the fundamental means of cache\n> validation.\n> \n\nI agree.  And I would like to see the Cache-Validator be opaque.  This\nseems simple and robust.  It places all the cache validation responsibility\nwith the origin server which is where it belongs.  Trying to share this\nresponsibility between the client/cache and the origin server may offer\nsome benefits but I have yet to see a persuasive case made for it.\n\nConditional gets may or may not be a good idea, but they should be \na separate issue from cache validation.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Formation of persistent connections subgrou",
            "content": "Larry Masinter has asked me to lead the \"persistent connections\"\nsubgroup.  After spending a few days recovering (almost) from the\nflu, I'm trying to get things underway.\n\nAccording to the Dallas minutes, the volunteers were\n    Alex Hoppman\n    Simon S\n    Mike cowlishaw\n    Andy Norman\n    Scott Powers\n    Brian Swetlund\n    \nI do not have email addresses for Scott Powers or Brian Swetlund,\nso would you each please send me your address?  (If someone out\nthere has a copy of the roster from the WG meeting last week,\nperhaps you could send me their addresses?)\n\nIf other people *really* want to join this subgroup, please\nsend me a message.  I don't want to leave out anyone who has\nsomething important to contribute, but I also want to keep\nit small enough that we can reach an actual consensus by\nthe \"January 1996\" deadline.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "New response cod",
            "content": "It would be really nice if there were a response code (say, 405) for\n\"robot forbidden that URL.\"  Technically, \"forbidden\" is already covered\nthrough 403, but it would still be nice to have something more\ndescriptive.\n\n[Note that I'm not subscribed to the list, so if this is deemed worthy of \na reply, please mail me directly or CC: me.]\n\n  Mordechai T. Abzug\nhttp://umbc.edu/~mabzug1   mabzug1@umbc.edu   finger -l mabzug1@gl.umbc.edu\nAs easy as 3.14159265358979323846264338327950288419716. . .\n\n\n\n"
        },
        {
            "subject": "Paul Leach: RE: Formation of persistent connections subgrou",
            "content": "[ This was probably meant for the list -- ange ]\n\n------- Forwarded Message\n\nDate:    Mon, 11 Dec 1995 14:37:34 -0800\nFrom:    Paul Leach <paulle@microsoft.com>\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: RE: Formation of persistent connections subgroup\n\nMessage-ID: red-16-msg951211223400MTP[01.51.00]000000c4-41516\n\nI'm interested -- would have raised my hand at the meeting, but I was \nat SOSP....\n- ----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] To:  <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Cc: Simon Spero  <ses@tipper.oit.unc.edu>; Alex Hopmann  \n<hopmann@holonet.net>;\n] Mike Cowlishaw  <mfc@VNET.IBM.COM>; Andy Norman\n] <ange@hplb.hpl.hp.com>\n] Subject: Formation of persistent connections subgroup\n] Date: Monday, December 11, 1995 2:17PM\n]\n] Larry Masinter has asked me to lead the \"persistent connections\"\n] subgroup.  After spending a few days recovering (almost) from the\n] flu, I'm trying to get things underway.\n]\n] According to the Dallas minutes, the volunteers were\n]     Alex Hoppman\n]     Simon S\n]     Mike cowlishaw\n]     Andy Norman\n]     Scott Powers\n]     Brian Swetlund\n]\n] I do not have email addresses for Scott Powers or Brian Swetlund,\n] so would you each please send me your address?  (If someone out\n] there has a copy of the roster from the WG meeting last week,\n] perhaps you could send me their addresses?)\n]\n] If other people *really* want to join this subgroup, please\n] send me a message.  I don't want to leave out anyone who has\n] something important to contribute, but I also want to keep\n] it small enough that we can reach an actual consensus by\n] the \"January 1996\" deadline.\n]\n] -Jeff\n] \n\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: Registration (was Re: DRAFT Minutes, HTTPWG",
            "content": "On Sun, 10 Dec 1995, Larry Masinter wrote:\n\n> We've gotten some warnings from those folks who have been involved in\n> the message-header registration wars to tread carefully here. There's\n> a quite analogous situation between HTTP headers and message headers,\n> and quite a bit of experience in the IETF about what can go wrong.\n> \n> For example, if you merely allow anyone to register anything, then you\n> get poorly specified headers, conflict over interpretation of\n> registered headers, vanity registration, trademark conflicts, etc.\n> \n> On the other hand, if you require standards-track RFCs describing the\n> header before the header becomes registered, you wind up with a\n> cumbersome process which either interferes with experimentation, or in\n> which the registration mechanism is ignored, the experiment proceeds,\n> which then interferes with registration once the experiment is\n> successful.\n...\n> The default registration procedure for anything that doesn't otherwise\n> have one is 'create a standards-track RFC describing the item'.\n> I think this currently applies to\n> \n>     Methods\n>     Status codes\n>     Entity Header Fields\n>     Content codings\n>     Transfer codings\n> \n> although it doesn't seem to apply to content-type. In lieu of any\n> other registration mechanism, it currently applies to URL schemes.\n\nThere is a well established system for content-type which involves posting\ninformation to the ietf-types mailing list.  This gives the interested\ncommunity a chance to comment on it and it can get registered after\ncomments have been incorporated or died down unless there seems to be\na consensus against it.\n\n> It's not that I don't think this issue needs to be addressed; I think\n> registration is very important. It's not unreasonable also to ask that\n> IANA would maintain an authoritative version of such a registry, even\n> if the only way that items could be added to the registry was via a\n> 'standards-track document'.\n> \n> One way to make better progress in HTTP-WG is to avoid 'ratholes', I\n> think this is one. I would like to make the issue of devising new\n> registration procedures 'out of scope' for HTTP-WG.\n> \n> Is there is a group of individuals who are knowledgable about the\n> history, status, and difficulties with the registration procedures for\n> protocol extensions in other Internet protocols (SMTP, SNMP, media\n> types, Telnet options come to mind) as well as HTTP and who are\n> willing to work on this issue, I might feel differently.\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\n\n\n\n"
        },
        {
            "subject": "Re: Registration (was Re: DRAFT Minutes, HTTPWG",
            "content": "> There is a well established system for content-type which involves posting\n> information to the ietf-types mailing list.  This gives the interested\n> community a chance to comment on it and it can get registered after\n> comments have been incorporated or died down unless there seems to be\n> a consensus against it.\n\nYou must be unaware of the controversy that has and continues to\nsurround the media type registration mechanism. To pick just one of a\ndozen controversial cases, there is a group that wishes to define a\nnew top-level type \"chemical\" so that they can define a large number\nof \"chemical/*\" types.\n\nI might feel differently if there were a group of individuals willing\nto work on this issue and also knowledgable both about HTTP and about\nthe history, status, and difficulties with the registration procedures\nfor protocol extensions in other Internet protocols (SMTP, SNMP, media\ntypes, Telnet options, ...).\n\n\n\n"
        },
        {
            "subject": "minutes, HTTP Working Grou",
            "content": "Reported by Phill Hallam-Baker and Rohit Khare, notes edited by Dave\nRaggett and Larry Masinter.\n\nThe meeting was held as two sessions, one in the morning with around\n100 people and one in the evening from 1930-2200. Minutes for the\nmorning session were taken by Phill Hallam-Baker and for the evening\nsession by Rohit Khare.\n\nMorning session:\n\nLarry Masinter has become co-chair of the WG along with Dave Raggett.\n\nRoy Fielding first presented, and we discussed, the status and plans\nof the working group.\n\nHTTP/1.0 was proposed as Best Current Practice but rejected by the\nIESG because didn't describe people's view of what was 'best', and\nthus it was an inappropriate status.\n\nRoy described his proposal that HTTP 1.1 be 'fast track' and that HTTP\n1.2 contain the extension mechanism and those bits that didn't make\nHTTP 1.1. He explained that he wrote things into the HTTP 1.1\nspecification even though they might be controversial, because it was\neasier to take things out than to add them in.\n\n* After some back and forth about a variety of options, the desire for\n  a stable core, and so on, the discussion led to the proposal that\n  HTTP/1.0 be re-written as an Informational RFC describing current\n  practice.\n\n  However, 'current practice' does not mean that we should document\n  all 50 versions of content negotiation as practices, merely use the\n  core of the 1.0 document as it stands for the parts that are\n  specified, and note that other features are not implemented\n  consistently. \n\nSome other points raised:\n* 1.0 is not actually much simpler than 1.1. 1.0 ignores proxies and\n  gateways, however.\n* all current clients/servers/proxies claim HTTP/1.0 in their headers.\n  If we were to create a 'best current practice' document for\n  HTTP/1.0, there becomes something for clients/servers proxies\n  to conform to, but then there's no way for an agent to tell whether\n  it is talking to a peer that claims conformance.\n* The 1.0 specification has been useful as it is, because it is stable\n  and consistent even if not complete.\n* It's a bad idea to make something a standards track document and\n  then obsolete it immediately. 1.0 should not be a proposed standard;\n  1.1 should be.\n================================================================\nDave Kristol discussed his draft:\n   draft-kristol-http-state-info-01.txt\n\nIdea to support state full sessions in a stateless protocol, Similar\nto netscape cookies. Examples include shopping basket, subscription\nlibrary system (remembers what has been looked at).\n\nDefine new header State-info that carries state information between\nuser agent and origin server.\n\nRequirements\n    o Cache friendly\n    o simple to implement\n    o simple to use\n    o can be deployed quickly\n    o downward compatibility\n    o reliable\n    o protect privacy\n    o support complex dialogues\n    o enough cache control possible\n    o minimal risks when used with non conforming caches\n\nHe went through the proposal. There is some belief that this may\n'compete' with the Netscape 'cookies' method, but that is claimed not\nto be sufficiently documented. Comments during the meeting:\n\n(Larry Masinter) The privacy concern is not that the site that\ninitiated the session might know things about the user, but that other\nnon-related sites might be able to find out things about your\nsessions.\n(Phill Hallam-Baker) The security concern of allowing server to store\ndata on the clients disk should be addressed explicitly.  (Ed: this\nwas a subtle point)\n(Alex Hopman) There is an issue with regard to servers with multiple\nCGI services which do not wish to share state information.\n(Dave Kristol) This is a server implementation issue.\n================================================================\nRohit Khare gave an overview of his PEP proposal\n draft-khare-http-pep-00.txt.\n\n--\n1. History\n   \"Motivation; Existing Practice of Adding Headers; Extension/Negotiation\n   experience in other IETF work areas\"\n\n2. How \"extensions\" work\n- feature present\n \"Modes activated by mere presence of a header: e.g. keep-alive, etc\"\n- reprocess body\n \"Filter message through program with <args>: e.g. encryption\"\n\n3. PEP features\n- naming\n \"Packaging a standard into a single name; a la ESMTP\"\n- addressing\n \"Explaining which HTTP agents need to act on extensions; a la IPv6\n option faulting and scoping\"\n- negotiation\n \"Advertising which extensions (at which settings) HTTP agents will\n accept; lessons from TELNET Option Negotiation\"\n\n- processing\n \"Order-of-application; reuse of Content-Encoding header to form pipe\"\n\n4. Directions\n   \"W3C is developing this for security, payments application; PEP will be HTTP \n   1.2; May form the basis for integrating work of HTTP sub-working-groups\"\n\nComments:\n\n(John Klensin, speaking personally and not as A.D.)  The trend in the\nIETF has been to strict versions something that people can read on the\nback of a box. Once an extension list is in a protocol we have a\nchecklist. We have never handled a transition from an extensions\nmechanism to a real verb well. PEP should consider migration to real\nverb properly.\n\n(Dave Crocker) Typically the view is that an extension mechanisms\nallow negotiation of optional features.  A different view is to say we\nwant to move from here to this one place, and that the negotiation\nmechanism allows movement of a functional core to a new base.  This is\nDave's view of the ESMTP work. Negotiation for combinatorial choices\ndoes not seem to work but as a migration strategy it does.\n\n================================================================\nDon Eastlake described his Internet Draft:\ndraft-eastlake-internet-payment-00.txt\n\nHe's looked at the problem of doing payments on the internet taking a\npretty general view. Need a mechanism for specifying payment systems\nso that once there is an idea of prices can decide what the system to\nbe used will actually be. Does not attempt to constrain payments\nsystem.  Draft allows a payment or receipt to be put into header.\nAllows payments to be done over internet in a common framework.\nCurrent plan is to recast in terms of PEP.\n\n(Larry Masinter) In order to do reliable payments you need real (ACID)\ntransactions, but HTTP does not seem to have any transaction\nmechanisms; e.g., you do not have ability to find out where an aborted\ntransaction got to.  We might expect to provide a transaction\nmechanism on top of HTTP to permit this to be used.\n\n(Don Eastlake) We should expect such matters to be handled by payment\nsystem rather than http layer, e.g. server allows interogation of\nserver to find out where transaction got. Some lightweight payment\nsystems will not provide such guarantees, your 2 cents may just be\nlost. Don attempted to restrict draft to just messages and receipts.\n\nDave Kristol expressed reservations about costs being embedded in\ndocuments and URLs.\n\nWe deferred further discussion to the Internet Payment BOF.\n\n================================================================\nAlex Hopman was scheduled to discuss draft-ietf-http-ses-ext-00.txt.\nHowever, half of that internet-draft is now in HTTP/1.1.\n================================================================\nAri Luotonen said only a few words about\ndraft-luotonen-ssl-tunneling-00.txt. It's been out for half a year.\n\nThe WG did not come to any conclusions on this issue.\n================================================================\nWe then talked about draft-luotonen-http-url-byterange-00.txt.\n\nThis draft inspired the work in the HTTP/1.1 draft on ranges. The\nURL-method for byte ranges will be abandoned, although it may go\nthrough an interim release by Adobe/Netscape.\n\nThe core HTTP/1.1 draft need not specify this if the methods and\nadditions can be done in a separate document and linked to 1.1 by\nreference; this is still an option. \n\nOne motivation for this feature was partial retrieval of PDF files;\nhowever, PDF as currently defined in application/pdf does not have\nbinary offsets or byte ranges. Apparently this is a feature of a new\nversion of to-be-released PDF.\n\nJohn Klensin pointed out that the byte range protocol should look at\ncheckpoint and restart experience.  It was likely that the real\nproblem is that document wants to be returned by parts, and that we\nneed to devise a way inside the document format to refer to parts and\nuse that for references.\n\n================================================================\nRoy Fielding then began a discussion of the HTTP 1.1 draft.\n\nComments in the first section included:\n\n(Larry Masinter) He thought that caching was meant to be a transparent\noptimisation technique. Nice simple semantics of http is being\ninterfered with by the clutter of discussing this intermediary\noptimisation. The problem is that there are lots of different type of\ncaches and hence different types of optimisation between server and\nintermediaries.  We're describing cache control headers in terms of\noperational effect rather than semantic differentiation; this does not\nanticipate future cache techniques.\n\nRoy wasn't sure, these were just the header names, and the semantics\nwere described that way.\n\nSomeone suggested putting content negotation outside in a separate\ndraft.\n================================================================\nSession 2 ran 19:30 to 23:30\n\nSimon Spero discussed HTTP-NG.\n\nHTTP-NG is the first protocol endorsed by Dogbert. \"Resist and you\nwill be shot.\" (Scott Adams cartooned a Dogbert on the HTTP-NG draft.)\n\nHis presentation was a shortened version of the one to be presented at\nthe WWW4 Conference in Boston.\n\nThe primary modifications have been:\n\nSplit the document into Architecture and Basics documents.\n\nThe basic purpose is: negotiation, meta-information, and control.\n\nHighlights:\n\n-Uses SCP to multiplex sessions.\n-Transition strategies using DNS CNAME to indicate NG support.\n-Not a superset of HTTP/1.x\n-Just forwarding HTTP1.0 through ng encoding reduces packet count by 50 and speed by 180%\n-negotiation and profiles. Dictionaries on either side constitute\nshared state, and profiles are predefined dictionaries of prefernces.\nDictionary structure patterns can be reused in different exchanges.\n-Security Key exchange\n-reinventing several secrecy nd signature mechanisms\n-Get put and metainformation\n-HTTP/1.x metainfo + US MARC records.\n-can request metainfo for included or linked objects.\n-speculative sending of data can be enabled, experiments can reduce latency to 1/5th\n\nDave Kristol asks, why the radical departure from current practice?\n\nA: latency, latency, latency. Optimazation for very fast and very\nslow networks. Reducing number of active connections.Http/1.1 can do\npersistent connections, but can't do multiplex (which increases user\npercieved speed).\n\nKristol: you propose speculative transmission, which uses bandwidth.\nSecond, how many of the benefits can be captured by HTTP alone?\n\nA: That's what these numbers are from: using 1.x messages encapsulated\n   in SCP.\n\nAlex Hopmann:So why use [your testbed], just SCP'd HTTP/1.1?\n\nSimon: the compactnss of the records: we can put 20 cache updates in a\nsingle packets.\n\nTed Hardie pointed that using DNS CNAMEs to indicate which hosts could\nhandle HTTPng would not work where HTTPng servers were being run by\nthose without the right to change DNS entries (which happens\nfrequently with HTTP).  It also uses the DNS as a directory service,\nwhich the DNS community is trying to avoid.\n\nSimon: it's a easy hack. All other schemes needed extra RTT. This\nworks, and cuts that out.\n\nQ: Upgrade path: are these competitors?\n\nA: No these are complementary. 1.x is very simple to do a quick hack\njob of. At the highest levels of performance it starts breaking down.\nIt's not so ideal for 1M hits/hr.\n\n================================================================\nAndy Norman then reported on the NG Prototype\n\n- SLIP over 19.2 k 115% to 140% throughput\n- UK to HP-India: 140% to 160%\n- US to UK through SOCKS (130%-500%)\n- Compared against browser using 4 simultaneous connections w/o keep alive\n- This is using straight 1.0 over SCP, without \"header reuse\"\n\n================================================================\nWe then discussed the role of NG in this working group.\n\nShould this be in the charter, or wait for a formalized proposal?\n\nDSR: Comments on removing NG from the charter.\n\nPaul Hoffman: Sounds from this presentation, this should be completely\nseparate track until its clearer.\n\nMasinter: many feature have emerged in 1.1 learned from NG. We should\nlook to NG for experiences. Doesn't mean this should be our work item.\nSo far most of the work has been done outside our WG. We should\nexplicitly refer in the charter to paying attention to the NG work. I\nwouldn't have a lot of confidence in the milestone dates and work plan\nfor NG given its speculative features. That's my uneasiness.\n\n================================================================\nWe then returned to the discussion of 1.1:\n\nYes, 'host:' is required in 1.1.\n\nFowarded:\nQ:Is there experience with loops occuring, is forwarded useful if some\ngateways strip them?\nA:Forwarded is still useful as a diagnostic.\nComment: if proxies remove forward headers for security reasons, then\nthe fact that this has been done should be indicated.\nPhill: Secrecy can be obtained by using a secret hashed token: no hist\ninfo needs to be revealed.\n\nDiscussion of the 5-second arbitrary timing and partial response.\n\n> Jim Gettys: This problem is due to buggy clients not reading while\nspewing data. The answer is to warn clients to do so.\n> Roy: if the data is sent out faster than the TCP reset.  \n> Ted: Using a hack like this to solve the problem contradicts a\nstated design goal, making HTTP implementable over multiple transport\nprotocols, by building into HTTP a solution to buggy implemtations of\nTCP.\n> Alex: We must come up with a solution that works for TCP.\n> Larry: You can put a multipart with unknown content-length. too! So\nyou'll still need the general fix. \n\nAlex: why can't you use full url when talking to the origin server?\nA: Why try? why send url to a server that may reject it, but does nothing different?\n\nDiscussion of new methods:\n* only applies to the new methods : OPTIONS, TRACE, and WRAPPED.\nLarry: This is long list of new methods. I've seen lots of distributed\nfile systems. Sees like a random selection of methods from filesystems\nfrom the past. If you want a kitechen sink, you're missing a lot. If\nnot, what's the rationale? HTTP doesn't have the power of ACLs,\netc. It's not good enough to put placeholders in a protocol\nspecification.\nPhill: following Larry's points on Portmanteau toolsets: remember,\nthere are an incredible number of methods being implemented by\nCGI-bins; only OPTIONS and Trace can be done that way. Sounds like\npatch & friends can be done that way...\nRoy: We should provide a consistent way for everyone to do it so that\neveryone doesn't supply their own form to do these operations.\nPhill: when I did CI/CO, I found what Larry found: you'll need a lot\nmore data to specify the total operation. \n\nQuestions on PATCH:\nIt was intended that PATCH be an arbitrary data object, e.g.,\nreplacing a b&w film with a colorized film could be a patch? \n\nOther comments:\n\nDave Kristol: Why do we have two mechanisms for streaming: multipart\nand chunked TE? [Notetaker disagreed -- it's easier to use CTE -- real\nmime boundarys are hard-- not amenable to fast processing]\n\nDMK: regarding TRACE, is it allowed to have a body? A: I can't\nremember. DMK: if there is a body, is it exactly the same body\nreferenced, or perhaped PEPped, etc.\n\nHarald: HTTP 1.1 looks like a case of second-system syndrome:\nexcessive backwards-compatiblity, and kitchen-sinking.\n\nRoy confessed to having thrown out 4 new methods already.\n\nHarald: I would suggest a very narrow focus on writing a basic\nplatform to negotiate upgrades.\n\nComment: note that locking/checkin/checkout is logically impossible.\n================================================================\nSteven Zilles made a quick announcement:\n\nADOBE will have a PDF demo this week that will use byte ranges via\nNetscape 2.0b3 using URLs today, but partial get next year. Feb '96\nrelease of PDF will also reveal the \"hint\" table. Also, a CGI script\nto serve byte ranges.\n\nRoy: Why is the rationale to modify the protocol, rather than define a\nnet-pdf -type?\n\nSteve: the tools for PDF are already in place ...\n================================================================\nAfter these discussions, the chairs presented the results of their\ndinner planning session:\n\n- The HTTP/1.0 draft will be revised to become an 'informational RFC'\n  which describes common current practice. Paul Hoffman (maintainer of\n  the list of HTTP servers and their features) will help.\n  This is primarily an editorial task, although additional material\n  may be added to list features that are widely but inconsistently\n  implemented (e.g., accept: headers).\n\n- The HTTP/1.1 draft will be reviewed independently by separate\n  sub-groups. The sub-groups are chartered to review the HTTP/1.1\n  draft for text related to their issue, and propose changes to the\n  HTTP/1.1 draft that consist of either wording changes, or movement\n  of major chunks of HTTP/1.1 to separate documents, as appropriate.\n\n  The issues are:\n\n   * Persistent connections\n      (this contains all of the 1.1 proposals for keep-alive,\n       and maintaining connections to avoid TCP startup costs.)\n       Alex Hoppman, Simon Spero, Mike Cowlishaw, Andy Norman,\n       Scott Powers, Brian Swetlund volunteered.\n\n   * Cache-control and proxy behavior\n       Ari Luotonen, David Morris, Jim Gettys volunteered.\n\n   * Content negotiation\n       Larry Masinter, Simon Spero volunteered.\n\n   * Authentication\n       Phill Hallam-Baker, Alex Hopman, John Marchinoi,\n       Stefek Zaba, Scott Powers volunteered.\n       (This issue must be coordinated with WTS working\n       group drafts.)\n\n   * State management\n       Dave Kristol, Rohit Khare, Scott Powers volunteered.\n\n   * Range retrievals\n       Stephen Zilles, Ari Luotonen volunteered.\n\n   * Extension mechanisms\n       Paul Hoffman, Rohit Khare, Daniel LaLiberte,\n       Simon Spero, Phill Hallam-Baker volunteered.\n\n   * other new methods and header features\n       (no list of volunteers for this was gathered)\n\n- Subgroups should conclude their work by Jan 96, in time to publish\n  their conclusions (or lack thereof) to the rest of the WG by\n  February 96, so that new internet draft(s) for HTTP/1.1 will be\n  ready in March 96 and ready for Proposed Standard RFC status by June\n  96.\n\n- Subgroups should document meetings, progress, etc. and are\n  encouraged to meet regularly, by conference, phone, etc.\n\n- Any proposed HTTP/1.1 features not in HTTP/1.0 for which there is no\n  consensus will revert to HTTP/1.0 status in 1.1 and be considered\n  for inclusion in HTTP/1.2.\n\nAlex wondered where 'logic bags' fit, and Larry suggested it should be\nhandled by the cache control/proxy group.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "What do you do when the lease expires just when the client does the\nPUT?\n\nHow do you know how long to lease the document for? Why not always ask\nfor more than you need? For infinite leases?\n\nI've seen a lot of version management systems around, and I've never\nseen any with leases. Why do you imagine this is EVER a good idea?\n\n\n\n"
        },
        {
            "subject": "Re: Registration (was Re: DRAFT Minutes, HTTPWG",
            "content": "On Wed, 13 Dec 1995, Larry Masinter wrote:\n\n> > There is a well established system for content-type which involves posting\n> > information to the ietf-types mailing list.  This gives the interested\n> > community a chance to comment on it and it can get registered after\n> > comments have been incorporated or died down unless there seems to be\n> > a consensus against it.\n> \n> You must be unaware of the controversy that has and continues to\n> surround the media type registration mechanism. To pick just one of a\n\nI'm aware there is controversy but despite that, it seems to work\npretty well.  I would say the IETF is full of controvery and some\nthink the output is bad but it seems to be better than all other\nstandards bodies in its area.\n\n> dozen controversial cases, there is a group that wishes to define a\n> new top-level type \"chemical\" so that they can define a large number\n> of \"chemical/*\" types.\n\nYou must be unaware that there was a strong consensus that adding new\ntop level type was a big deal and the ietf-types list was really set\nup for registering subtypes.  The chemical/* type seems to have been\nmerged with a number of other ideas and subsumed into a model/* top\nlevel type which is more likely to succeed.\n\n> I might feel differently if there were a group of individuals willing\n> to work on this issue and also knowledgable both about HTTP and about\n> the history, status, and difficulties with the registration procedures\n> for protocol extensions in other Internet protocols (SMTP, SNMP, media\n> types, Telnet options, ...).\n\nSeems to me that you believe unless there are exact and precise rules\nfor everything, then there is a \"problem\".  I don't see it that way.\n\nUnless there are rules to the contrary, IANA is in charge of allocating\n/ registering most everything in IETF standards.\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> (1) What is the purpose of the validator in this example?  If you\n> mean \"cache validator\", then presumably this is a token that the\n> client has obtained from the server which can be used to decide\n> if the client's cached version of the file is still valid.  This\n> might be useful if we want to ensure that some other client doesn't\n> come along midway through this exchange (i.e., between steps 2 and\n> 3) and update the same file.  But if we want to make this work,\n> then we need to be honest about supporting atomic transactions.\n\nYes, I wanted this to be a way of telling whether some other client\nhas already updated the same data. It doesn't give you locking, but\nkeeps the second user from stepping on the first.\n\nBut you're right, the two steps don't do much other than add a RTT and\na little reassurance for early validation. On the other hand, perhaps\nmany 'first' calls to PUT will fail for authentication errors (the get\nis not authenticated but a PUT is) so maybe this *is* a good case to\noptimize.\n\n\n\n"
        },
        {
            "subject": "rethinking cachin",
            "content": "When you do an HTTP method for a URL, the results may depend on things\nother than the method name and the path of the URL.  To control\ncaching, the source has to tell any client and proxy what things the\nresults depended on other than the URI. Sometimes those things are the\nclient's headers (accept:, accept-language:, authentication, state)\nand sometimes those things are external to what the client sent (date,\nclient IP address, etc., whether the client has a license)\n\nA client/proxy that has a cached resource for a URL may want to invoke\nthe same method for the same URL but with different parameters, or in\na different context, e.g., when the date has changed, or a helper app\nhas been added and the accept headers might be different, etc.\n\nIn those cases, the client/proxy needs to ask the server for the\nresults of applying the method to the URL, but also indicate to the\nserver which objects it DOES have cached for that URL.\n\nIf a site at Canada were to run a proxy cache for the site, it might\nhave cached documents for both the French and English versions of a\nURL. Different clients should be able to retrieve the French and\nEnglish versions from the cache without clearing or 'invalidating' the\ncache for the other clients who want the other language; modifying the\nEnglish version shouldn't cause the French version to get dropped from\nthe cache.\n\nMost of the current proposals for headers back and forth don't handle\nthis situation correctly. Yet a straight-forward enumeration of\n'depends-on' in the return from the server to the proxy, along with\nproxy services that preserve that information and also all headers\nthat comprise the things the value returned depended upon, would be a\ngood first step. For some headers, (accept, for example), you need\nmore than merely knowing what it depended upon, but also the WAY in\nwhich it depended upon the original data, so that the proxy itself can\ndecide whether a cached item is appropriate.\n\n\n \n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Marc Salomon writes:\n\n> 3. Operating under the assumption that at this time in the life cycle of the\n> web, most HTML documents contain components that reside on the same server as \n> the document itself, why not trade the multiple network connections from the \n> client to the same server for some up-front packaging by the server.\n\nBecause it defeats caching.  Besides, some people may want to use MIME\nmultipart types on HTTP for other reasons -- why should the server treat\nthem any differently than other content types?  [just server here -- I know\nwhy clients would want to do so]\n\n> 6. Interoperating efficiently with client cache management brings up some\n> interesting issues.  The ability to check the HTML document's requirements\n> against the client-side cache before issuing a precisely tailored HTTP MGET \n> request (which would be returned as multipart/mixed*).\n\nIt's much easier to just keep the connection open (with possibly a very\nshort timeout as defined by the server) and respond with multiple responses\n(each one of which may be a multipart entity).\n\n> 7. An instance of the proposed MIME encoding scheme for HTTP follows.  This \n> is currently in the process of a feasibility study in METHADONE (Mime\n> Encoding \n> THreAded Daemon Optimized for Network Efficiency), a caching, lightweight\n> threaded, MIME multipart encoding HTTP server for solaris 2.x (exploiting the \n> rich base functionality of NCSA's httpd) currently under beta development at \n> the UCSF Library and Center for Knowedge Management.\n\nSounds like fun, but that's not the hard part.  Serving up MIME multipart\nis not the problem.  What you need to do is get clients to understand\n(and parse) MIME multiparts.  And that means ALL clients.\n\n> Message-ID: <http://host.domain/http_mime.html>\n\nThis is an invalid use of Message-ID -- it must be unique.\n\n> Content-ID: <http://host.domain/path/http_mime.html>\n\nThis is an invalid use of Content-ID -- it must be unique.\n\nAnd what do you do about Content-Encoding?  It's not valid MIME at all.\n\n> Two consecutive boundary strings indicate an EOF.\n\nWhat?  See RFC 1521.  Or, for that matter, read the HTTP/1.0 draft -- it\nalready defines EObody-part and EOentity for Multipart entities.\n\n> 8.  I plan to bring this up on the sgml-internet list as well, for a broader,\n> more general perspective.\n\nThey already want to ship SGML as a multipart/mixed -- adding a bunch of\nHTTP-specific controls to it just mucks-up things for the mail people.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Formation of caching subgrou",
            "content": "I seem to have ended up as the leader of the caching subgroup\n(as well as of the persistent connections subgroup), mostly\nsince nobody else has volunteered.\n\nAnyway, if you want to participate in the design of the HTTP\ncaching model, please let me know ASAP.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Rohit Khare wrote:\n>Basically, you probe (in GET or HEAD) to get a lease; if you do get a lease,  \n>for the next delta-t seconds, you're entitled to use that lease in a  \n>Authorization: header (using the LEASE scheme) to execute an exclusive PUT or  \n>POST or PATCH.\nWell I don't know about the \"lease\" idea, but I definatelly think you are on\nthe right track with the \"you probe\" idea. One of the beauties of HTTP is\nthat there is a very clear model:\n\nStep 1: Client makes a request. Server just receives.\nStep 2: Server sends a response. Client just receives.\n\nThe two-phase send changes some of the clarity of this model. However it is\npossible to accomplish much the same thing (At least the goals as Roy\nexpressed them in Dallas), by doing a probe:\n\nTransaction #1 \"The Probe\"\nClient asks the server if the following send is acceptable, using a GET,\nHEAD, or some new method. Server responds.\n\nTransaction #2 \"The Send\"\nIf the server answered transaction #1 in the affirmative, client sends the data.\n\nNow a couple of notes:\n1) Don't pay too much attention to the word transaction above. I know it\nimplies more than I mean, but I can't think of a better word at this minute.\n2) The above only makes sense (in terms of performance) in the context of\npersistant connections.\n3) In situations where clients either don't care, or are sure that their\n\"send\" is acceptable, they can skip the \"probe\".\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Alex Hopmann:\n>\n>Rohit Khare wrote:\n>>Basically, you probe (in GET or HEAD) to get a lease; if you do get a lease,  \n>>for the next delta-t seconds, you're entitled to use that lease in a  \n>>Authorization: header (using the LEASE scheme) to execute an exclusive PUT or  \n>>POST or PATCH.\n>Well I don't know about the \"lease\" idea, but I definatelly think you are on\n>the right track with the \"you probe\" idea. One of the beauties of HTTP is\n>that there is a very clear model:\n>\n>Step 1: Client makes a request. Server just receives.\n>Step 2: Server sends a response. Client just receives.\n>\n>The two-phase send changes some of the clarity of this model.\n\nYes, and this changing of clarity comes at a price: the CGI interface\nwill need to be extended to handle reciept of two-phase sends.  This\nwill impact on http server CGI handling code and on CGI library code.\n\nI'm not yet sure we should be willing to pay that price.  The\ntwo-transaction model you bescribe below seems like an equaly good\nsolution to me.\n\n> However it is\n>possible to accomplish much the same thing (At least the goals as Roy\n>expressed them in Dallas), by doing a probe:\n>\n>Transaction #1 \"The Probe\"\n>Client asks the server if the following send is acceptable, using a GET,\n>HEAD, or some new method. Server responds.\n>\n>Transaction #2 \"The Send\"\n>If the server answered transaction #1 in the affirmative, client sends the data.\n>\n>Now a couple of notes:\n>1) Don't pay too much attention to the word transaction above. I know it\n>implies more than I mean, but I can't think of a better word at this minute.\n>2) The above only makes sense (in terms of performance) in the context of\n>persistant connections.\n>3) In situations where clients either don't care, or are sure that their\n>\"send\" is acceptable, they can skip the \"probe\".\n\nMore notes:\n4) The server should _still_ have the option to refuse to actually\nperform the request in transaction #2 above, even if it has said to go\nahead in the probe transaction.\n5) A client sending a probe could be said to be asking the\nquestion: 'would it make sense for me to send this request with a very\nlarge request body?'.\n6) Probes (and two-phase sends) are really a device to save bandwidth\nby sometimes preventing the useless sending of large requests.\nExtending their semanctics to be also a device for temporary locking\ndoes not buy us anything.\n7) Clients can skip sending a probe if the request body is small anyway.\n\n>Alex Hopmann\n>ResNova Software, Inc.\n>hopmann@holonet.net\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Larry Masinter:\n>\n>When you do an HTTP method for a URL, the results may depend on things\n>other than the method name and the path of the URL.\n\nLarry, most of the problems you write about are addressed to some\nextent by the 1.1 content negotiation mechanism, not the caching\nmechanism.  You may want to read my 'notes on content negotiation', I\nbelieve they address some of your concerns about things currently\nmissing from 1.1.  The notes are in\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0289.html\n\nand\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0347.html .\n\n[...]\n>In those cases, the client/proxy needs to ask the server for the\n>results of applying the method to the URL, but also indicate to the\n>server which objects it DOES have cached for that URL.\n\nIn my notes on content negotiation, I proposed a new header\n\n  Send-no-body-for: <list of variant URIs> \n\nfor the purpose of indicating the variants the cache DOES have cached\nfor a negotiated URL (called a negotiation port in my notes).  \n\nIt is interesting to note that the IF: (Unless) header allows one to\nimplement the functionality of my proposed Send-no-body-for\nheader. Roy's\n\n  If: {ne {Location \"http://blah.com/doc.french.html\"}}\n\nwould have about the same semantics as my\n\n  Send-no-body-for: http://blah.com/doc.french.html .\n\nReading your message, it seems to me that we have both independently\nconcluded that `Send-no-body-for' functionality is needed.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "> If a site at Canada were to run a proxy cache for the site, it might\n> have cached documents for both the French and English versions of a\n> URL. Different clients should be able to retrieve the French and\n> English versions from the cache without clearing or 'invalidating' the\n> cache for the other clients who want the other language; modifying the\n> English version shouldn't cause the French version to get dropped from\n> the cache.\n\nwhat i'm wondering is why content negotiation can't just return\nthe actual URL of the negotiated resource, using the new\n\"Content-Location\".  so if you requested /foo/bar/constitution.txt\nand preferred english, you'd get /foo/bar/constitution-english.txt\n(and would be silently redirected there), and the proxy would cache\nit under that name.  similarly, the french request would get cached\nunder the other name.\n\ngranted, this would mean a \"reload\" would go to the specific one\n(and pre-empt negotiation).\n\nsorry if this is an obvious question, but just how *is* cacheing\nsupposed to work with content negotiation?\n\n-=- sfw, canadian and former lurker\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "sfwhite@incontext.ca writes:\n > \n > what i'm wondering is why content negotiation can't just return\n > the actual URL of the negotiated resource, using the new\n > \"Content-Location\".  so if you requested /foo/bar/constitution.txt\n > and preferred english, you'd get /foo/bar/constitution-english.txt\n > (and would be silently redirected there), and the proxy would cache\n > it under that name.  similarly, the french request would get cached\n > under the other name.\n > \n\nAt least one issue is the spoofing issue: If clients use the\n\"Location\" header as the cache key for the resource, then it is easy\nfor a server to claim to be sending any old URI, and for that resource\nto get lodged in a cache under the false URI.  This might not be *so*\nterrible for end-user caches, but it could cause some real trouble if\nit happened in a more public intermediate proxy cache.  I don't recall\nanyone having mentioned a good workaround for this either.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Shel Kaphan:\n>\n>sfwhite@incontext.ca writes:\n> > \n> > what i'm wondering is why content negotiation can't just return\n> > the actual URL of the negotiated resource, using the new\n> > \"Content-Location\".\n\nIt _can_ return the actual URL.  From the 1.1 draft:\n\n|10.27  Location\n|\n|   The Location response-header field defines the exact location of \n|   the resource that was identified by the Request-URI. For 2xx \n|   responses, if the Request-URI corresponds to a negotiable set of \n|   variants and the response includes one of those variants, then the \n|   response must also include a Location header field containing the \n|   exact location of the chosen variant.\n\n[Shel gives a reason why Location cannot work this way:]\n>At least one issue is the spoofing issue: If clients use the\n>\"Location\" header as the cache key for the resource, then it is easy\n>for a server to claim to be sending any old URI, and for that resource\n>to get lodged in a cache under the false URI.\n\nEek.  I had completely forgotten the spoofing issue, thanks for\nreminding me.  The solution to this spoofing issue is simple, and I\nbelieve it has been discussed before.  Include the following rule:\n\n  Clients (including caching proxies) should disregard Location\n  headers in 2xx responses if they do not point to the same server\n  that generated the response.\n\nThis restriction still leaves you with a negotiation mechanism\npowerful enough to handle the French/English example.\n\nNote that the above solution assumes that either the content providers\non the same server either trust each other not to spoof, or that the\nserver has some Location response header filtering mechanism that\nexcludes spoofing.\n\nThis spoofing issue is not addressed in the current draft 1.1 spec.\nI'm in the content negotiation subgroup, I'll make sure we will\naddress it there.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "On Sat, 16 Dec 1995, Koen Holtman wrote:\n\n> \n>   Clients (including caching proxies) should disregard Location\n>   headers in 2xx responses if they do not point to the same server\n>   that generated the response.\n> \n> This restriction still leaves you with a negotiation mechanism\n> powerful enough to handle the French/English example.\n> \n> Note that the above solution assumes that either the content providers\n> on the same server either trust each other not to spoof, or that the\n> server has some Location response header filtering mechanism that\n> excludes spoofing.\n> \n\nThis is not a safe assumption. Numerous providers sell space to many \nindependent people on single servers. For example: www.xmission.com \nserves on the order of 1000 independent entities, including many \nbusinesses and people, and allows CGI to be owned by the individuals. \nClearly there is the opportunity for someone to spoof there under the \nrule. It is not significantly safer than unrestricted redirections when \nmany (most?) people share common servers.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Benjamin Franz writes:\n > On Sat, 16 Dec 1995, Koen Holtman wrote:\n > \n > > \n > >   Clients (including caching proxies) should disregard Location\n > >   headers in 2xx responses if they do not point to the same server\n > >   that generated the response.\n > > \n > > This restriction still leaves you with a negotiation mechanism\n > > powerful enough to handle the French/English example.\n > > \n > > Note that the above solution assumes that either the content providers\n > > on the same server either trust each other not to spoof, or that the\n > > server has some Location response header filtering mechanism that\n > > excludes spoofing.\n > > \n > \n > This is not a safe assumption. Numerous providers sell space to many \n > independent people on single servers. For example: www.xmission.com \n > serves on the order of 1000 independent entities, including many \n > businesses and people, and allows CGI to be owned by the individuals. \n > Clearly there is the opportunity for someone to spoof there under the \n > rule. It is not significantly safer than unrestricted redirections when \n > many (most?) people share common servers.\n > \n > -- \n > Benjamin Franz\n\nIf the cache key for the returned document were essentially\nthe _pair_ (request-URI, location-URI), then this would be safe.\n\nThen even if a server returned a completely bogus Location URI, it\nwould still be associated with the request URI, and so could not\n\"escape\" to spoof other requests where the request URI was the same\nas that Location URI.\n\nFurthermore, associated with this pair (and the object) could be the\ncontent-negotiation information that caused that request-URI to yield\nthat particular Location-URI.\n\nHowever, in order to avoid duplicate objects in the cache, I think\nthat an object returned with a given Location-URI must flush any other\nobjects from the cache with that URI as either the request-URI or the\nlocation-URI.  This is \"spoofable\" in a sense, but can only affect\nperformance, and not correctness.\n\nBut this should probably wait for the caching subgroup.... (sorry, Jeff)\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "First, I apologize for jumping into the middle of this thread.  I'd like\nto make a couple of comments about how TTL and Forwarded are relevant\nto the Harvest cache.\n\n>    The Forwarded: header can actually be used to detect loops. However,\n>    \n>    * it is not compulsory (although a node willing to avoid loops will\n>      certainly insert it and detect the loop);\n>    * in principle, it does not prevent the occurrence of arbitrarily\n>      long paths (although a node can decide not to pass a request which\n>      has been \"Forwarded:\" too many times);\n>    \n>    and these reasons make me like better the use of a *compulsory*\n>    TTL field as a loop detector (which is also simpler to manage).\n>\n>When I first read your proposal for a TTL field, I had the same\n>reaction that Roy seems to have had: this isn't really necessary.\n>The Forwarded: header allows any proxy to detect loops through\n>itself, and can support that task without being mandatory.\n>\n>And do we have any evidence that forwarding loops are a real problem?\n>Routing loops occur because our routing protocols are automatic and\n>dynamic, and so can do stupid things rapidly and for transient periods.\n>HTTP forwarding loops, on the other hand, would be created by humans\n>and should thus be both less frequent and less transient.\n\nI think that HTTP forwarding loops may become a real problem.  As\nwe have been getting our six national caches (http://www.nlanr.net/Cache/)\noperational the past few weeks, forwarding loops were unintentionally\ncreated due to a bug in the software.  But I doubt that such bugs\nwill often be the cause of forwarding loops.  Rather they will be due\nto configuration errors.\n\nFor now HTTP cache configuration is a fairly manual process, but I \ndoubt it will remain so.  I can already see the need for a cache\nto have a default \"next hop\" with the ability to switch to an\nalternate next hop if the default becomes unavailable.\n\nA TTL header would not help Harvest to eliminate loops.  When the\nloop is completed and the cache gets a second request for the same URL,\nthis second client-side request becomes \"attached\" to the first request\n(on the server-side).  The caches will deadlock until one of them times\nout.  The forwarded header would solve this problem quite easily\n(except for the hassle of parsing it).\n\n\n>However: after I thought about this some more, I realized that there\n>is an entirely different reason to include a TTL header in HTTP.\n>\n>There are two main uses for the TTL field in IP headers: first,\n>to avoid routing loops and long-delayed packets.  Second, to make\n>\"traceroute\" work.  I would bet that most TTL \"failures\" in today's\n>Internet are from traceroute users, not routing loops.\n>\n>Traceroute has proved to be an essential tool in debugging IP-level\n>problems.  We ought to be thinking about providing analogous debugging\n>tools at the HTTP level.  For example, \"how come the users on my\n\nI totally agree that a HTTP-traceroute will be very useful.  However, I\ndon't think we need TTL to do it.  If I understand correctly,\ntraceroute uses increasing TTLs because the IP LSRR option is/was not\nalways implemented, and the fixed IP header size limit only allowed\nsomething like nine addresses to be recorded.  Since HTTP doesn't\nsuffer from either of those problems, wouldn't it be better to just\nhave the Forwarded lines included in the response header?  Then you\nonly need to make one request.\n\nDuane W.\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "> First, I apologize for jumping into the middle of this thread.  I'd like\n> to make a couple of comments about how TTL and Forwarded are relevant\n> to the Harvest cache.\n> \n> A TTL header would not help Harvest to eliminate loops.  When the\n> loop is completed and the cache gets a second request for the same URL,\n> this second client-side request becomes \"attached\" to the first request\n> (on the server-side).  The caches will deadlock until one of them times\n> out.  The forwarded header would solve this problem quite easily\n> (except for the hassle of parsing it).\n\nAt first I thougt that deadlocks would only occur when a node in the\nloop finishes it resources (be them connections, processes, whatever).\nI thought that there were *many* of them, so a suitable TTL would avoid\ndeadlocks. This looks like the same problem, except that there is\nexactly *one* resource on each node.\n\n> I totally agree that a HTTP-traceroute will be very useful.  However, I\n> don't think we need TTL to do it.  If I understand correctly,\n> traceroute uses increasing TTLs because the IP LSRR option is/was not\n> always implemented, and the fixed IP header size limit only allowed\n> something like nine addresses to be recorded.  Since HTTP doesn't\n> suffer from either of those problems, wouldn't it be better to just\n> have the Forwarded lines included in the response header?  Then you\n> only need to make one request.\n\nConvincing arguments!\n\nLuigi\n====================================================================\nLuigi Rizzo                     Dip. di Ingegneria dell'Informazione\nemail: luigi@iet.unipi.it       Universita' di Pisa\ntel: +39-50-568533              via Diotisalvi 2, 56126 PISA (Italy)\nfax: +39-50-568522              http://www.iet.unipi.it/~luigi/\n====================================================================\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Dan writes:\n\n> I'd rather see more effort spent on\n> \n> (1) Specifying existing practice on HTTP/1.0, keeping an eye\n> on opportunities to gateway/cache/proxy out of HTTP and into\n> next-generation protocols.\n\ndoin' that\n\n> (2) Carefully crafting HTTP-NG using an interface definition\n> language like ASN/1 or OMG's IDL, so that it can be used\n> with various transports and protocols. (ILU! ILU! ILU! ahem...)\n\nNow that would be interesting!  Given all the packet-level games that\nSimon wants to include in HTTP-NG, it would be interesting to see if it's\neven possible to describe it with ILU.  That would certainly go a long\nway toward making it easier to implement.\n\n> (3) Investigating some name service and replication strategies\n> so that links that we write today will continue to work tomorrow.\n\nyep, no guarantees though.  I prefer small steps.\n\n> (4) Coordinating implementation efforts, and working on\n> a common base of reusable code.\n\ndoin' that, though it is important to have multiple common bases of\nreusable code.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Benjamin Franz:\n>\n>On Sat, 16 Dec 1995, Koen Holtman wrote:\n>> \n>>   Clients (including caching proxies) should disregard Location\n>>   headers in 2xx responses if they do not point to the same server\n>>   that generated the response.\n>> \n>> This restriction still leaves you with a negotiation mechanism\n>> powerful enough to handle the French/English example.\n>> \n>> Note that the above solution assumes that either the content providers\n>> on the same server either trust each other not to spoof, or that the\n>> server has some Location response header filtering mechanism that\n>> excludes spoofing.\n>\n>This is not a safe assumption. Numerous providers sell space to many \n>independent people on single servers. For example: www.xmission.com \n>serves on the order of 1000 independent entities, including many \n>businesses and people, and allows CGI to be owned by the individuals. \n\nThe part `or that the server has some Location response header\nfiltering mechanism that excludes spoofing' above is supposed to cover\nthis situation.\n\nNot that I expect many providers to implement such a filtering\nmechanism, most would treat web spoofing like they treat news spamming\nand mail forging now: forbid it in the terms of service agreement and\ndeal appropriately with any found violations.\n\nAnyway, here is how a Location spoofing filter is supposed to work.\nThe HTTP server does some post-processing on all CGI output (except\nfor nph- scripts, I'll cover them later).  Part of this\npost-processing includes calculating the Content-Length for the CGI\nresponse.  In the same post-processing stage, the server could check\nwhether any Location headers generated by a CGI owned by Joe indeed\npoint to locations on the server controlled by Joe.  Depending on the\nlayout of the web space maintained by the server, this test could be\nas easy as path prefix matching: i.e. the CGI under\nhttp://www.xmission.com/joe/bin/blah is allowed to produce Location\nheaders to http://www.xmission.com/joe/doc.fr.html, but not to\nhttp://www.xmission.com/john/info.html .  Another possibility, for\nunix-based servers, is to compare the used-id's on the CGI executable\nand the file or script pointed to.\n\nAs for nph- scripts: a non-trusting server administrator interested in\nsecurity would either disable them, or only allow them after auditing.\n\nOf course, Shel's idea of making the cache key of a negotiated variant\nbe the pair (request-URI, location-URI) eliminates all spoofing risks,\nwe could switch to such a scheme if the consensus is that Location\nheader filtering is unfeasible.  Shel's scheme is safe no matter how\nmuch the server administrator does about security, but has the\ndisadvantage of allowing less cache hits: it would be much more\ndifficult to let preemptive and reactive content negotiation share\ncache slots for the variants.  [Note: an explanation of this last\nstatement would require a level of detail only appropriate in the\ncontent negotiation or caching subgroups.]\n\n>Clearly there is the opportunity for someone to spoof there under the \n>rule. It is not significantly safer than unrestricted redirections when \n>many (most?) people share common servers.\n\nUnrestricted 3xx redirections are another issue entirely: unrestricted\n3xx redirection will not allow Joe to fool a proxy cache into storing\na response from his script under John's URI.\n\nI see the security issues connected to unrestricted 3xx redirects as\nequal to the security issues connected to unrestricted <a href=..>\ntags.  They are there, but there is nothing much we can do about them\nbeyond making users aware that web link titles may not be telling the\ntruth.\n\n>Benjamin Franz\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "On Sun, 17 Dec 1995, Koen Holtman wrote:\n\n> Benjamin Franz:\n\n> >This is not a safe assumption. Numerous providers sell space to many \n> >independent people on single servers. For example: www.xmission.com \n> >serves on the order of 1000 independent entities, including many \n> >businesses and people, and allows CGI to be owned by the individuals. \n> \n> The part `or that the server has some Location response header\n> filtering mechanism that excludes spoofing' above is supposed to cover\n> this situation.\n> \n> Not that I expect many providers to implement such a filtering\n> mechanism, most would treat web spoofing like they treat news spamming\n> and mail forging now: forbid it in the terms of service agreement and\n> deal appropriately with any found violations.\n\nUmmmm...Considering the immense magnitude of both spamming and forging \ntoday, this is not a convincing argument for leaving it to local option.\n\n[...]\n\n> Of course, Shel's idea of making the cache key of a negotiated variant\n> be the pair (request-URI, location-URI) eliminates all spoofing risks,\n> we could switch to such a scheme if the consensus is that Location\n> header filtering is unfeasible.  Shel's scheme is safe no matter how\n> much the server administrator does about security, but has the\n> disadvantage of allowing less cache hits: it would be much more\n> difficult to let preemptive and reactive content negotiation share\n> cache slots for the variants.  [Note: an explanation of this last\n> statement would require a level of detail only appropriate in the\n> content negotiation or caching subgroups.]\n\nNever-the-less, I believe this is the route that will have to be taken.\nThe other route (local filtering) just places too much reliance on good\nsecurity management at the local level. It amounts really to trusting all \nsystem admins to 'play nice and know what they are doing' - something the \never growing ever growing spam/forgery problems on the Usenet and in \nE-mail have shown just is not a good assumption in general. \n\nJust as the default reporting of people's email addresses with the \nadmonishment not to abuse it proved futile (I routinely get requests from \nmy customers to 'give them the email addresses of everyone who visits \ntheir web site so they can email them' - I fielded exactly that request \nnot two days ago from one customer), it will prove impossible in practice \nto make local filtering work. Too many local system demands (and \ninsufficient knowledge on the part of admins) will make it \nnearly impossible to maintain a secure system for many people.\n\nOn large systems with thousands of customers with many special cases, it \nwould be a logistical nightmare even for experienced admins.\n\n> >Clearly there is the opportunity for someone to spoof there under the \n> >rule. It is not significantly safer than unrestricted redirections when \n> >many (most?) people share common servers.\n> \n> Unrestricted 3xx redirections are another issue entirely: unrestricted\n> 3xx redirection will not allow Joe to fool a proxy cache into storing\n> a response from his script under John's URI.\n\nI did not phrase what I meant well. I meant 2xx redirections without the \nproposed rule.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Benjamin Franz:\n\n On Sun, 17 Dec 1995, Koen Holtman wrote:\n...\n > Of course, Shel's idea of making the cache key of a negotiated variant\n > be the pair (request-URI, location-URI) eliminates all spoofing risks,\n > we could switch to such a scheme if the consensus is that Location\n > header filtering is unfeasible.  Shel's scheme is safe no matter how\n > much the server administrator does about security, but has the\n > disadvantage of allowing less cache hits: it would be much more\n > difficult to let preemptive and reactive content negotiation share\n > cache slots for the variants.  [Note: an explanation of this last\n > statement would require a level of detail only appropriate in the\n > content negotiation or caching subgroups.]\n\nOne note: as long as the cache-validator of an object remains the same\nit is OK for copies of it to persist in the cache under separate keys,\neven though it may be the result of content-negotiation in several\nways, or the result of a direct request.  (Example: you have URLs A,\nB, and C.  Requests for both A and B yield \"location\" C, as do\ndirect requests for C.  Though each request would result in a\ndifferent (key,object) in the cache, ((A,C), (B,C), and (C,-)), if the\ncache validator for C is the same in all cases there is no problem\nleaving the other copies in the cache.  The only performance hit\noccurs when C changes.  Then the next similar request for A, B, or C must\nflush all other occurrences of C from the cache.  So it really isn't\nso bad.)\n\n Never-the-less, I believe this is the route that will have to be taken.\n The other route (local filtering) just places too much reliance on good\n security management at the local level. It amounts really to trusting all \n system admins to 'play nice and know what they are doing' - something the \n ever growing ever growing spam/forgery problems on the Usenet and in \n E-mail have shown just is not a good assumption in general. \n\nEven if this route is taken, it still may be \"robust\" cache design\nto refuse to accept Location headers from an upstream server that do not\nat least match the hostname part of the request's URL. Unless there\nare exception cases I can't think of ...\n\n...\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Content negotiation and MIME registration procedure",
            "content": "Reply-to: masinter@parc.xerox.com,koen@win.tue.nl,hedlund@best.com,ses@tipper.oit.unc.edu\n\nCould I ask those of you involved in 'content negotiation' to review\n\ndraft-ietf-822ext-mime-reg-02.txt\n\nto see if you have any comments about the proposed revised MIME\nregistration procedures for charset values and media types in general?\nI believe of the primary motivations for the revision of the\nregistration procedures has been the demands of web applications.\n\nCurrently, the notes I have are:\n\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0289.html\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0347.html\n\nIn addition, there was some discussion of content negotiation in the\nHTML working group over the particular issue of negotiating variants,\nprofiles, and extensions of HTML.\n\nThere's been some discussion in the past of a desire to negotiate\ncolor usage, image size, etc., but I've not seen seen a proposal.\n\nThe names I have for the content negotiation subgroup are listed in\nthe Reply-to; if there are others who want to be directly involved,\nplease let us know.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Benjamin Franz:\n>\n>On Sun, 17 Dec 1995, Koen Holtman wrote:\n>\n>> Not that I expect many providers to implement such a filtering\n>> mechanism, most would treat web spoofing like they treat news spamming\n>> and mail forging now: forbid it in the terms of service agreement and\n>> deal appropriately with any found violations.\n>\n>Ummmm...Considering the immense magnitude of both spamming and forging \n>today, this is not a convincing argument for leaving it to local option.\n\nHmm, forging does not happen that often AFAIK.  Anyway, the kind of\nweb spoofing we are talking about here does not have the same global\nimpact as spamming and forging: with the rule that 2xx Location\nheaders not pointing to the server that generated the response should\nbe ignored, this kind of web spoofing can only harm users that share\nthe host with the spoofer.\n\n[...]\n>The other route (local filtering) just places too much reliance on good\n>security management at the local level.\n\nAs the impact of mismanagement would be limited to local content, I\nhave little problems with this reliance being placed.  Service\nproviders with security management lousy enough to allow prolonged web\nspoofing will simply loose their customers and die.\n\n[...]\n>On large systems with thousands of customers with many special cases, it \n>would be a logistical nightmare even for experienced admins.\n\nNot if the Location header filter is user-id based as described\nbefore.  Experienced admins could create such a filter in a few hours,\nif it is not already a standard option of future 1.1 http servers.\n\nIn other words, I don't share your pessimism.\n\n>Benjamin Franz\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "On Mon, 18 Dec 1995, Koen Holtman wrote:\n\n> Benjamin Franz:\n> >\n> >On Sun, 17 Dec 1995, Koen Holtman wrote:\n> >\n> >> Not that I expect many providers to implement such a filtering\n> >> mechanism, most would treat web spoofing like they treat news spamming\n> >> and mail forging now: forbid it in the terms of service agreement and\n> >> deal appropriately with any found violations.\n> >\n> >Ummmm...Considering the immense magnitude of both spamming and forging \n> >today, this is not a convincing argument for leaving it to local option.\n> \n> Hmm, forging does not happen that often AFAIK. \n\nIt does happen that often. I am engaged in cancelling a large (in excess \nof 2000 articles) combination spam/forgery (with the intent I think of \nmail bombing the forgery victim) right now. Drop into \nnews.admin.net-abuse.* to appreciate just how bad it has gotten. We are \ngetting daily reports of forgeries with intent to cause harm.\n\n[...]\n\n> >On large systems with thousands of customers with many special cases, it \n> >would be a logistical nightmare even for experienced admins.\n> \n> Not if the Location header filter is user-id based as described\n> before.  Experienced admins could create such a filter in a few hours,\n> if it is not already a standard option of future 1.1 http servers.\n> \n> In other words, I don't share your pessimism.\n\nI think we will just have to agree to diagree on this. Among other things \nit does not address the practice of 'sub-letting' web space. A number of \nsites (including www.xmission.com) allow this as well.\n\n-- \nBenjamin Franz\n\"_Never_ underestimate the power of human stupidity.\"\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and loop",
            "content": "    I totally agree that a HTTP-traceroute will be very useful.  However, I\n    don't think we need TTL to do it.  If I understand correctly,\n    traceroute uses increasing TTLs because the IP LSRR option is/was not\n    always implemented, and the fixed IP header size limit only allowed\n    something like nine addresses to be recorded.  Since HTTP doesn't\n    suffer from either of those problems, wouldn't it be better to just\n    have the Forwarded lines included in the response header?  Then you\n    only need to make one request.\n    \nSince you jumped into the middle of this thread, you may not have\nseen the second message I sent on this topic.  Roy Fielding raised\nmuch the same point, to which I replied:\n\n    TRACE suffers from the same problem as the IP \"record route\" option:\n    it's useless if the path is broken.  (Loops are not the only way\n    to break a forwarding path; it's probably even easier to do that\n    by misconfiguring a \"proxy\" pointer to point at a black hole.)\n\nFor more details, you can read the whole message as\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0409.html\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "I have added \"Spoofing using Location headers (prevention thereof?)\"\nto my list of issues for the caching subgroup, although this is not\na commitment that we will actually solve the problem.\n\nI tend to agree with the view that this is not exactly a protocol\ndesign issue, but rather is a problem for people who are implementing\nshared web servers.  No matter what criteria we put into the HTTP\nprotocol, if www.webcondo.com has sold service to both \"The Good Guys\"\nand \"The Bad Guys\" without providing some security barriers\nbetween them, then nothing we can do in the protocol spec will\nsolve everything.\n\nBut it may be that we can include some recommendations that will\nimprove security without significantly compromising performance.\nAnd some of these may be necessary to provide correct caching\neven without the threat of malicious behavior.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Jeffrey Mogul writes:\n > I have added \"Spoofing using Location headers (prevention thereof?)\"\n > to my list of issues for the caching subgroup, although this is not\n > a commitment that we will actually solve the problem.\n > \n > I tend to agree with the view that this is not exactly a protocol\n > design issue, but rather is a problem for people who are implementing\n > shared web servers.  No matter what criteria we put into the HTTP\n > protocol, if www.webcondo.com has sold service to both \"The Good Guys\"\n > and \"The Bad Guys\" without providing some security barriers\n > between them, then nothing we can do in the protocol spec will\n > solve everything.\n > \n > But it may be that we can include some recommendations that will\n > improve security without significantly compromising performance.\n > And some of these may be necessary to provide correct caching\n > even without the threat of malicious behavior.\n > \n > -Jeff\n\nI agree that this is not exactly a protocol design issue.  However,\nthere are a number of aspects to caching that are not exactly part of\nthe communication protocol.  Larry Masinter wondered (though I think\nit might have been just to me) whether we shouldn't consider doing a\nseparate I-D to cover caching, presumably to address the kinds of\nissues that are not strictly part of the communication protocol, but\nthat need to be, or at least would be far better off being, nailed\ndown in any case.\n\nBut we can talk about this after the caching sub-wg gets going.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "revised charter",
            "content": "I've taken our notes from the minutes and other discussion and created\na proposed new 'charter'. Diffs with the old charter attached.\n\nPlease comment on the charter before it is sent to the area directors\nfor approval.\n\n================================================================\nHYPERTEXT TRANSFER PROTOCOL (HTTP) CHARTER\n\nCHAIR(S)\n    o Larry Masinter <masinter@parc.xerox.com>\n    o Dave Raggett <dsr@w3.org>\n\nAPPLICATIONS AREA DIRECTOR(S):\n    o John Klensin <Klensin@mci.net>\n    o Harald Alvestrand <Harald.T.Alvestrand@uninett.no>\n\nTRANSPORT AREA DIRECTOR(S):\n    o Allison Mankin <mankin@isi.edu>\n\nAREA ADVISOR\n    o John Klensin <Klensin@mci.net>\n\nMAILING LIST INFORMATION\n    o General Discussion:http-wg@cuckoo.hpl.hp.com\n    o To Subscribe: http-wg-request@cuckoo.hpl.hp.com\n        * In Body: subscribe http-wg Your Full Name\n    o Archive: http://www.ics.uci.edu/pub/ietf/http/hypermail\n\nDESCRIPTION OF WORKING GROUP\n\nNote: This working group is jointly chartered by the Applications Area and the\nTransport Services Area.\n\nThe HTTP Working Group will work on the specification of the Hypertext Transfer\nProtocol (HTTP). HTTP is a data access protocol currently run over TCP and is the basis\nof the World-Wide Web. The initial work will be to document existing practice and\nshort-term extensions. Subsequent work will be to extend and revise the protocol.\nDirections which have already been mentioned include: improved efficiency, extended\noperations, extended negotiation, richer metainformation, and ties with security\nprotocols.\n\nNote: the HTTP working group will not address HTTP security extensions as these are\nexpected to be the topic of another working group.\n\nBackground information\n\nThe initial specification of the HTTP protocol was kept in hypertext form and a\nsnapshot circulated as an Internet draft between 11/93 and 5/94. A revision of the\nspecification by Berners-Lee, Fielding and Frystyk Nielsen has been circulated as an\nInternet draft between 11/94 and 5/95. An overview of the state of the specifications\nand a repository of pointers to HTTP resources may be found at\n\nhttp://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nThe working group will expand and complete that document to reflect\nHTTP/1.0 as it has been implemented by World-Wide Web clients and servers prior to\nNovember 1994. The resulting specification of HTTP/1.0 will be published for review as\nan Internet-Draft and will be submitted to the IESG for\nconsideration as an Informational RFC.\n\nIn parallel with the above effort, the working group will consider\nenhancements/restrictions to the current practice in order to form a specification of\nthe HTTP protocol suitable for eventual consideration as a proposed standard.\n\nGOALS AND MILESTONES\n\nDec 95\n    Publication of HTTP/1.0 Internet-Draft prepared to be considered\n    as an Informational RFC describing current practice of HTTP.\n\nDec 95\n    Initial publication of HTTP/1.1 proposal from document editors.\n\nJan 96\n    Complete review of HTTP/1.1 proposal and pending Internet Drafts\n    by subgroups:\n\n    * Persistent connections\n    * Cache-control and proxy behavior\n    * Content negotiation\n    * Authentication\n    * State management\n    * Range retrievals\n    * Extension mechanisms\n    * other new methods and header features\n\nFeb 96\n     Subgroups complete Internet Drafts for separate documents from\n     HTTP/1.1 specification, or prepare HTTP/1.1 differences.\n\nMar 96\n     New HTTP/1.1 Internet-Draft distributed\n\nJun 96\n    HTTP/1.1 submitted to IESG as Proposed Standard.\n\nCURRENT INTERNET-DRAFTS\n    o Hypertext Transfer Protocol -- HTTP/1.0\n    o Hypertext Transfer Protocol -- HTTP/1.1\n    o A Proposed Extension to HTTP : Digest Access Authentication\n    o Mediated Digest Authentication\n    o HTTP Session Extension\n\nNO REQUEST FOR COMMENTS\n\n================================================================\n*** ocharter.txtMon Dec 18 15:44:25 1995\n--- newcharter.txtMon Dec 18 15:43:45 1995\n***************\n*** 1,6 ****\n--- 1,7 ----\n  HYPERTEXT TRANSFER PROTOCOL (HTTP) CHARTER\n  \n  CHAIR(S)\n+     o Larry Masinter <masinter@parc.xerox.com>\n      o Dave Raggett <dsr@w3.org>\n  \n  APPLICATIONS AREA DIRECTOR(S):\n***************\n*** 7,12 ****\n--- 8,16 ----\n      o John Klensin <Klensin@mci.net>\n      o Harald Alvestrand <Harald.T.Alvestrand@uninett.no>\n  \n+ TRANSPORT AREA DIRECTOR(S):\n+     o Allison Mankin <mankin@isi.edu>\n+ \n  AREA ADVISOR\n      o John Klensin <Klensin@mci.net>\n  \n***************\n*** 42,102 ****\n  \n  http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n  \n! Once established, the working group will expand and complete that document to reflect\n  HTTP/1.0 as it has been implemented by World-Wide Web clients and servers prior to\n  November 1994. The resulting specification of HTTP/1.0 will be published for review as\n! an Internet-Draft and, if deemed appropriate, will be submitted to the IESG for\n! consideration as a Proposed Standard or Informational RFC.\n  \n  In parallel with the above effort, the working group will consider\n  enhancements/restrictions to the current practice in order to form a specification of\n  the HTTP protocol suitable for eventual consideration as a proposed standard.\n  \n- Also in parallel with the above efforts, the working group will engage in defining (or\n- selecting from various definitions) a next-generation protocol for hypertext transfer\n- (HTTPng).\n- \n  GOALS AND MILESTONES\n  \n!     Done\n!       Draft working group charter. Establish mailing list and archive.\n  \n!     Done\n!       Review draft charter for discussion at the Chicago WWWF'94 conference. Invest an\n!       interim Chair for the working group. Determine writing assignments for first\n!       draft of HTTP/1.0 document.\n  \n!     Done\n!       Publish an Internet-Draft on HTTP as reflected by current practice (HTTP/1.0)\n  \n!     Done\n!       Meet at the San Jose IETF as a BOF. Review HTTP/1.0 Internet-Draft and decide\n!       whether it should be published as Informational, should be a candidate for\n!       further working group development, or should be allowed to expire. Determine\n!       writing assignments for first drafts of the HTTP/1.1 or HTTPng documents.\n!       Establish charter and submit to IESG\n  \n!     Feb 95\n!       Revise the Internet-Draft on HTTP/1.0 and, if desired, submit to the IESG for\n!       consideration under the category determined at San Jose IETF.\n  \n!     Feb 95\n!       Publish Internet-Drafts on HTTP/1.1 and HTTPng.\n  \n!     Apr 95\n!       Final review of HTTP/1.1 draft at the Danvers IETF. Revise HTTP/1.1 draft and\n!       submit to IESG for consideration as Proposed Standard. Review progress on HTTPng.\n  \n-     Dec 95\n-       Final review of HTTPng draft at the Dallas IETF. Revise HTTPng draft and submit\n-       to IESG for consideration as Proposed Standard. Retrospective look at the\n-       activities of the HTTP WG.\n- \n  CURRENT INTERNET-DRAFTS\n!     o [[Hypertext Transfer Protocol -- HTTP/1.0]] (120852 bytes). (There is also a\n!       [[PostScript version]] [223847 bytes].)\n!     o [[A Proposed Extension to HTTP : Digest Access Authentication]] (666 bytes)\n!     o [[Mediated Digest Authentication]] (666 bytes)\n!     o [[HTTP Session Extension]] (11562 bytes)\n  \n  NO REQUEST FOR COMMENTS\n--- 46,98 ----\n  \n  http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n  \n! The working group will expand and complete that document to reflect\n  HTTP/1.0 as it has been implemented by World-Wide Web clients and servers prior to\n  November 1994. The resulting specification of HTTP/1.0 will be published for review as\n! an Internet-Draft and will be submitted to the IESG for\n! consideration as an Informational RFC.\n  \n  In parallel with the above effort, the working group will consider\n  enhancements/restrictions to the current practice in order to form a specification of\n  the HTTP protocol suitable for eventual consideration as a proposed standard.\n  \n  GOALS AND MILESTONES\n  \n! Dec 95\n!     Publication of HTTP/1.0 Internet-Draft prepared to be considered\n!     as an Informational RFC describing current practice of HTTP.\n  \n! Dec 95\n!     Initial publication of HTTP/1.1 proposal from document editors.\n  \n! Jan 96\n!     Complete review of HTTP/1.1 proposal and pending Internet Drafts\n!     by subgroups:\n  \n!     * Persistent connections\n!     * Cache-control and proxy behavior\n!     * Content negotiation\n!     * Authentication\n!     * State management\n!     * Range retrievals\n!     * Extension mechanisms\n!     * other new methods and header features\n  \n! Feb 96\n!      Subgroups complete Internet Drafts for separate documents from\n!      HTTP/1.1 specification, or prepare HTTP/1.1 differences.\n  \n! Mar 96\n!      New HTTP/1.1 Internet-Draft distributed\n  \n! Jun 96\n!     HTTP/1.1 submitted to IESG as Proposed Standard.\n  \n  CURRENT INTERNET-DRAFTS\n!     o Hypertext Transfer Protocol -- HTTP/1.0\n!     o Hypertext Transfer Protocol -- HTTP/1.1\n!     o A Proposed Extension to HTTP : Digest Access Authentication\n!     o Mediated Digest Authentication\n!     o HTTP Session Extension\n  \n  NO REQUEST FOR COMMENTS\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> with various transports and protocols. (ILU! ILU! ILU! ahem...)\n\nBesides various ILU translators being available (lisp, C++, C, IDL)\nthis strong recommendation from Dan wouldn't be because ILU\nhappens to be very much like Modula-3 now would it? :-)\n\nBTW. Apart from Fresco, and the Sun CFE, does anyone know of any\nfree compilers/translators for IDL, and/or perhaps a free ORB? \n\nNot directly related to HTTP, but...\n\n\n\n"
        },
        {
            "subject": "Re: Content negotiation and MIME registration procedure",
            "content": "> Reply-to: masinter@parc.xerox.com,koen@win.tue.nl,hedlund@best.com,ses@tipper.oit.unc.edu\n\n> Could I ask those of you involved in 'content negotiation' to review\n\n> draft-ietf-822ext-mime-reg-02.txt\n\n> to see if you have any comments about the proposed revised MIME\n> registration procedures for charset values and media types in general?\n> I believe of the primary motivations for the revision of the\n> registration procedures has been the demands of web applications.\n\nThe demands of Web applications have of course been considered, but I would not\ncharacterize this as the primary reason for changing these procedures.\n\nThe motivation for revising the content-type registration procedure is actually\npretty simple: The current procedure does not work! There's nothing specific to\nthe Web, email, or any other aspect of the Internet here -- its equally\ndisfunctional for all comers ;-) I don't think anyone disagrees with my\nassessment on this point -- certainly not anyone who has tried to register\nsomething...\n\nFurthermore, the procedure laid out in this draft is NOT in its final form.\nExpect a MAJOR revision of it within the next month. (I'm waiting on\nsubstantive input from the Application Area Directors before digging into this\nmorass again.)\n\nAs for character set registration, the revisions are mostly to tighten things\nup and make the process work better, based on past experience. The situation\nhere isn't nearly as grim as it is for content types, of course. It was pretty\nclear what the problems were, but Dan Connolly's \"character sets considered\nharmful\" draft and the discussions that preceeded and followed it were\nparticularly helpful in clarifying how to deal with the various issues.\n\nThe resulting registration procedure for character sets is now fairly complete\nand no major revisions are expected. Comments are welcome, of course.\n\nI should also point out that procedures for registering\ncontent-transfer-encodings as well as message/external-body access types are\nspecified in this document. This is all new stuff -- these procedures were\nnever documented before.\n\nThe proper places to discuss these matters are probably on either the ietf-822\nlist or the ietf-types list. (Preferably not on both, however.)\n\nNote that if this document is adopted it will be a proposed standard. There is\nfar too much new stuff to even think about a recycle at draft. The rest of the\nMIME document set hopefully will recycle at draft, however.\n\n> Currently, the notes I have are:\n\n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0289.html\n> http://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0347.html\n\nLarry, let me clarify that these notes are in regards to content negotiation,\nnot type and character set registration matters. (Maybe this was clear to\nother folks, but it confused me.)\n\nNed\n\n\n\n"
        },
        {
            "subject": "Re:  revised charter",
            "content": "Neither the old nor new charter mentions the State-Info draft, \"Proposed\nHTTP State-Info Mechanism\", draft-kristol-http-state-info-01.{txt,ps}.\n\nDave\n\n\n\n"
        },
        {
            "subject": "(Fwd) Re: Solaris HTTP server performance... (fwd",
            "content": "At the Boston conf., I had a couple of requests for the \npatch which significantly improved the TCP/IP performance \nof the NCSA HTTPd on Solaris machines.  It is included\nbelow.  It was submitted to us by Bob McGrath, the NCSA main\nweb site Webmaster, who had been working wih Sun.\n\n- Beth Frank\nefrank@ncsa.uiuc.edu\n\nForwarded message:\n> \n> --- Forwarded mail from Steve Parker <sparker@jurassic-248.eng.sun.com>\n> \n> To: \"Robert McGrath\" <mcgrath@ncsa.uiuc.edu>\n> Cc: nordmark@jurassic-248.eng.sun.com\n> Subject: Re: Solaris HTTP server performance...\n> Date: Wed, 06 Sep 1995 17:27:30 -0700\n> From: Steve Parker <sparker@jurassic-248.eng.sun.com>\n> \n> \n> - I grabbed a fresh copy of the distribution this AM from:\n> -\n> - ftp://ftp.ncsa.uiuc.edu/Web/httpd/Unix/ncsa_httpd/httpd_1.4\n> -\n> - I made the following changes and then compiled as you see\n> - below.\n> \n> So, as Erik pointed out we still see Nagle behavior in the set of\n> changes you made, I made a corrected context diff.  This sets the\n> socket option on the accepted socket to suppress this.\n> \n> We think this will improve the performance you see...\n> \n> Thanks,\n> \n> ~sparker\n> \n> *** httpd.c.origWed Sep  6 17:11:30 1995\n> --- httpd.cWed Sep  6 17:19:36 1995\n> ***************\n> *** 32,37 ****\n> --- 32,41 ----\n>   #include <sys/types.h>\n>   #include <sys/param.h>\n>   #include \"new.h\"\n> + #include <sys/socket.h>\n> + #include <netinet/in.h>\n> + #include <netinet/tcp.h>\n> + #include <arpa/inet.h>\n> \n> \n>   JMP_BUF jmpbuffer;\n> ***************\n> *** 306,311 ****\n> --- 310,316 ----\n>   #ifndef NO_PASS\n>   void child_main(int parent_pipe, struct sockaddr_in *sa_server) {\n>       int x;\n> +     int one = 1;\n> \n>   /*    struct passwd* pwent; */\n> \n> ***************\n> *** 564,569 ****\n> --- 569,576 ----\n>   log_error(\"socket error: accept failed\");\n>       }\n>   } else { /* connection accepted */\n> +   setsockopt(csd, IPPROTO_TCP, TCP_NODELAY, (void *)\n> &one,\n> +       sizeof(one));\n>   #ifndef NO_PASS\n>     if (num_children) {\n>       /*free_child = 0;*/\n> \n> \n> ---End of forwarded mail from Steve Parker\n> <sparker@jurassic-248.eng.sun.com>\n> \n> -- \n> Robert E. McGrath\n> National Center for Supercomputing Applications\n> University of Illinois, Urbana-Champaign\n> Champaign, Illinois 61820\n> (217)-333-6549\n> \n> mcgrath@ncsa.uiuc.edu\n> \n\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "Jeffrey Mogul:\n>\n>I have added \"Spoofing using Location headers (prevention thereof?)\"\n>to my list of issues for the caching subgroup, although this is not\n>a commitment that we will actually solve the problem.\n\nFor the record, I feel that the spoofing using Location headers issue\nis really a sub-problem of content negotiation, not of caching.  But\nI'd be happy to deal with this problem in the caching subgroup instead\nof the negotiation subgroup.  Two of the four members of the\nnegotiation subgroup are also in the caching subgroup, so there is\nlitte chance of these groups getting too much out of sync.\n\n>I tend to agree with the view that this is not exactly a protocol\n>design issue, but rather is a problem for people who are implementing\n>shared web servers.\n\nIn my opinion, considering the impact of the protocol design\n_is_ a protocol design issue, even if this consideration leads to a\nreview of shared web server security mechanisms.\n\nWe can only afford to introduce a web security problem and pass it to\nthe people who are implementing shared web servers after we have\nestablished that these people can actually provide a solution to the\nsecurity problem.  As long as we do not have consensus that they can\nsolve the problem, we had better not put this security problem in the\nHTTP protocol.\n\nI have been thinking up a negotiation header structure that would not\nallow any form of cache spoofing _and_ provide nice cache efficiency\n(preemtive and reactive negotiation sharing cache slots).  The\ndownside of this structure would be the introduction of a new request\nheader with rather unusual semantics.  I'll try to post a description\nin the near future.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: (Fwd) Re: Solaris HTTP server performance... (fwd",
            "content": "> \n> In message <9512191656.AA09771@void.ncsa.uiuc.edu>, Beth Frank writes:\n> \n> > At the Boston conf., I had a couple of requests for the \n> > patch which significantly improved the TCP/IP performance \n> > of the NCSA HTTPd on Solaris machines.  It is included\n> > below.  It was submitted to us by Bob McGrath, the NCSA main\n> > web site Webmaster, who had been working wih Sun.\n> \n> Two questions about running HTTPd under solaris:\n> \n> 1) is the patch needed for HTTPd 1.5 or is it already in the code?\n\nThe patch has been included in NCSA HTTPd 1.5, but needs to be applied\nto 1.4.2.\n\n> 2) will the system tuning guidelines described in \n>    http://www.sun.com/cgi-bin/show?sun-on-net/Sun.Internet.Solutions/\n> performance/tun_mon/index.html\n>    work well with NCSA HTTPd (there is a section describing how \n>    application developers can maximize performance)?  What about apache?\n\nI don't know.  Glancing over it, I'd say yes.  They reference the Nagle\nproblem which is addressed by the patch, and increasing the TCP/IP queue\nwhich I believe is necessary for any heavily loaded Solaris server.  If\nyou are having performance problems under Solaris, you may need to look\nat a threaded server.  (Both NCSA and Apache servers are pre-forking.) \nAlso look at your DNS lookup times.  If DNS is too slow it can slow down\nyour whole server.  You can avoid (or at least significantly decrease)\nthis problem by compiling the NCSA server with GETHOSTBYNAMEISEVIL set. \nThis will prevent the server from doing IP address to hostname translations\n(avoiding most of the DNS calls), but it will also mean that the logs\nwill contain IP addresses instead of host names.\n\n> Any additional information from WWW administrators about maximizing\n> performance on Solaris servers would be much appreciated...\n> \n> - Alberto\n> \n> \n> ============\n> Alberto Accomazzi                       Smithsonian Astrophysical Observatory\n> alberto@cfa.harvard.edu                 60 Garden Street, MS 70\n> http://cfa-www.harvard.edu/~alberto     Cambridge, MA  02138  USA\n> \n\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Digest Authentication. Moving towards last call..",
            "content": "At the IETF HTTP-WG it was agreed to form sub-groups on a number of issues\nincluding Digest Authentication in HTTP. I would like to request anyone with\nobjections to Jeff Hosteltler's draft (now expired) to make them known in the\nnext three weeks - say January 10th?\n\nI would ask Jeff to resubmit the draft so that we can know what the proposal\nis. \n\nTo revise people's memories Digest authentication allows a user to demonstrate\nthat they know a password without sending it over the Internet in a form that\ncan be decrypted. It does require servers to keep authentication databases\nwhich are sensitive in that any compromise to them will compromise their\nsecurity ass access codes. This is the best that can be done without using\npublic key however. The UNIX method of storing passwords means that passwords\nhave to be sen over the network in the clear. digest Authentication is\neffectively providing Kerberos type security without a mediator.\n\nThere are a few outstanding issues:\n\n1) Should we include a mediated form of the authentication?\n\n2) Should we specify a mechanism for defining new Keyed Digest algorithms?\n\n3) Is Kerberos integration a practical proposition?\n\n4) The syntax of the WWW-Authenticate: field is peculiar.\n\n\nWe now have 2 entirely independent implementations, one in Spyglasses deployed\nprducts and another in the Common Lisp Web server. Since both of those products\ntend to get distributed on CD-ROMS there had better be a good reason behind any\nproposed changes.\n\nThe mood at the IETF was that a subgroup go off and nail down the remaining\nquestions and get Digest Auth rolled out ASAP. Hence the need for comments\nASAP. \n\nAppologies for not getting to this sooner after the IETF, I have been busy with\nWWW4 which came the following week.\n\n\n-- \nPhillip M. Hallam-Baker            Not speaking for anoyone else\nhallam@w3.org http://www.w3.org/hypertext/WWW/People/hallam.html\nInformation Superhighway -----> Hi-ho! Yow! I'm surfing Arpanet!\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication. Moving towards last call..",
            "content": "According to Phillip M. Hallam-Baker:\n> \n> At the IETF HTTP-WG it was agreed to form sub-groups on a number of issues\n> including Digest Authentication in HTTP. I would like to request anyone with\n> objections to Jeff Hosteltler's draft (now expired) to make them known in the\n> next three weeks - say January 10th?\n> \n> I would ask Jeff to resubmit the draft so that we can know what the proposal\n> is. \n\n\nThe expired draft is available at \n\nhttp://hopf.math.nwu.edu/digestauth/draft.rfc\n\nThere is also a digest authentication protected document there at\n\nhttp://hopf.math.nwu.edu/simp/index.html\n\nwith user name \"Mufasa\" and password \"CircleOfLife\" which can\nbe used if you have a digest savy client.  Unfortunately\nthat does not include Netscape clients.   :(  \n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Larry writes:\n\n> I think we went through this with HTML; one might have said (many did)\n> that:\n> \n> \"... HTML files are (as the recent spec suggests) SGML-like, not SGML\n> conforming. With so many other deviations from SGML....\"\n\nAnd they would be right.  If it were not for Dan's two years of hard\nwork, they wouldn't be valid SGML.  The reason they can be today is\nbecause there was a very good reason to force HTML into SGML conformancy\nand resulted in only a minor loss of functionality (i.e. <PLAINTEXT>).\n\n> I think the original *intent* was to be MIME conforming, and that it\n> isn't *hard* to be MIME-conforming, and that there are *benefits* to\n> being MIME-conforming.\n\nThe original intent was to use MIME as a means of data typing, and the\nhistory of design choices can be (mostly) viewed on the www-talk archive.\nHTTP has never been MIME-conformant, and threw all possibility of that\nout the window when Content-Encoding was invented.  Until the MIME people\nadd Content-Encoding to the official RFCs, there is no point in even\ndiscussing the issue here.  All that we can do is show how they differ\nand possibly how an HTTP -> SMTP/NNTP gateway should behave.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": ">...\n>A client/proxy that has a cached resource for a URL may want to invoke\n>the same method for the same URL but with different parameters, or in\n>a different context, e.g., when the date has changed, or a helper app\n>has been added and the accept headers might be different, etc.\n>...\n>Most of the current proposals for headers back and forth don't handle\n>this situation correctly. Yet a straight-forward enumeration of\n>'depends-on' in the return from the server to the proxy, along with\n>proxy services that preserve that information and also all headers\n>that comprise the things the value returned depended upon, would be a\n>good first step. For some headers, (accept, for example), you need\n>more than merely knowing what it depended upon, but also the WAY in\n>which it depended upon the original data, so that the proxy itself can\n>decide whether a cached item is appropriate.\n\nAbsolutely. A Vary: 1#(http-header-name) header returned by the server\nwould help solve this problem.\n\nThe 1.1 content negotiation mechanism is only of use for the\ncaching problem if the server provides the client/proxy with sufficient\ninformation for it to do the document selection itself. The 1.1 draft\nsupports this in the limited cases where the variance can be expressed\nby the URI: header.\n\nThe 1.1 mechanism is not of use when\n1. The server does not provide individual URLs for all the possible documents\n   that could be returned when accessing a URL.\n2. The server may have URLs for the individual documents, but does not\n   return an exhaustive list of possibilities (maybe it would be too large)\n3. The document returned depends on a parameter in a manner that cannot\n   be described by a URI: header. (e.g. available in multiple character sets.)\n\nWithout a Vary: header, servers will have to mark the response as not\ncacheable.\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (was &lt;p&gt; ... &lt;/p&gt;",
            "content": "In message <m0tSMkY-000oANC@ccug.wlv.ac.uk>, Jon Wallis writes:\n>At 13:19 19/12/95 -0600, BearHeart/Bill Weinman wrote:\n>>\n>>At 10:40 am 12/19/95 -0800, Walter Ian Kaye wrote:\n>>><A HREF=\"index.html\"><IMG SRC=\"../gifs/btnhome3.gif\" ALT=\"[Home]\"\n>border=1></A>\n>>><A HREF=\"../map.html\"><IMG SRC=\"../gifs/btnmap3.gif\" ALT=\"[Index]\"\n>>\n>>>(I'm gonna be changing the form and cgi soon, btw, cuz Lynx doesn't like\n>>>partial URLs -- tho' Netscape handles this form perfectly.)\n>>\n>>   The problem with the parial URLs may be the \"../\" references. \n>>\n>>   Some servers, and perhaps some browsers too, disallow them because \n>>they've been abused to get around security measures. \n>\n>That really shouldn't be a problem if the system is set up right - but since\n>so many systems are poorly set up in terms of security  I can believe it.\n\nI think there are two issues that are getting confused here:\n(1) whether it's OK to use ../../ in an HREF or SRC attribute\nin an HTML document,\n(2) whether it's OK to _send_ ../../ in the path field of\nand HTTP request.\n\n(1) is cool, (2) is not.\n\nFor example, if the example above was fetched from http://www.foo.com/a/b/c.html,\nthen to fetch the [Home] image, the client must combine the value of the HREF\nattribute with the base URL as per RFC1808, yielding:\n\nhttp://www.foo.com/a/gifs/btnhome3.gif\n\nTo access the resource at that address, it makes a TCP connection to port 80\nof www.foo.com, and sends:\n\nGET /a/gifs/btnhome3.gif HTTP/1.0\nAccept: image/*\n\nWhat's _not_ cool is to try to sidestep the processing of .. on the client side;\nthat is, to just combine the base and HREF into:\n\nhttp://www.foo.com/a/b/../gifs/btnhome3.gifs\n\n(which is _not_ a well-formed HTTP url) and send:\n\nGET /a/b/../gifs/btnhome3.gif HTTP/1.0\n\nThis is illegal because it is a potential secruity risk. Consider a server\nwhose document root is /usr/local/etc/httpd/docs/ and a client who sends:\n\nGET /../../../../etc/passwd HTTP/1.0\nAccept: text/plain\n\na naive server implementation might just do:\nfopen(\"/usr/local/etc/httpd/docs//../../../../etc/passwd\")\nand give away a bunch of sensitive info.\n\nIn stead, any server that sees /../ in the HTTP path is supposed to\nissue a 403 Unauthorized response. (Is this in the HTTP specs somewhere?\nYIKES! I can't find it in draft-ietf-http-v10-spec-02.txt!!!\n\nHTTP-WG folks: this should be addressed in the HTTP 1.0 spec, no?\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (wa",
            "content": ">         GET /a/b/../gifs/btnhome3.gif HTTP/1.0\n> This is illegal because it is a potential secruity risk. Consider a server\n> whose document root is /usr/local/etc/httpd/docs/ and a client who sends:\n\nIt's a potential security risk on *SOME* systems. Works fine on mine.\nYou're quite welcome to fetch <URL:http://www.phone.net/home/mwm/../>,\nwhich will get index.html from the directory \"..\" in my public html\ndirectory.\n\n> In stead, any server that sees /../ in the HTTP path is supposed to\n> issue a 403 Unauthorized response. (Is this in the HTTP specs somewhere?\n> YIKES! I can't find it in draft-ietf-http-v10-spec-02.txt!!!\n> HTTP-WG folks: this should be addressed in the HTTP 1.0 spec, no?\n\nNo. A server should be allowed to issue \"403 Unauthorized\" (or \"404\nNot Found\") for any URL that the server considers insecure. On Unix\nsystems, I'd certainly consider \"/../\" to be such a string. On other\nsystems, another syntax (for instace \"//\") might be used for that end,\nand the server is perfectly justified in rejecting any request\ncontaining that string.\n\nYes, having \"..\" (or \"\") as a path component creates interesting\nproblems in writing relative URLs, and is probably a bad idea on any\nserver. Yes, an attempt to access \"../../../../etc/passwd\" is probably\nsomeone trying to break into the system. However, it's up to the\npeople running the server, not the spec, to decide what is and is not\na security problem and deal with them. Unless some request can be\nshown to create problems independent of the underlying platform, the\nspec should not make any request illegal.\n\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authentication. Moving towards last call..",
            "content": "> At the IETF HTTP-WG it was agreed to form sub-groups on a number of issues\n> including Digest Authentication in HTTP. I would like to request anyone with\n> objections to Jeff Hosteltler's draft (now expired) to make them known in the\n> next three weeks - say January 10th?\n> \n> I would ask Jeff to resubmit the draft so that we can know what the proposal\n> is. \n\nby popular request, we are in the process of resubmitting the draft.\nit should be available tomorrow.  i think jan 10 is a good goal.\n\n\n> To revise people's memories Digest authentication allows a user to demonstrate\n> that they know a password without sending it over the Internet in a form that\n> can be decrypted. It does require servers to keep authentication databases\n> which are sensitive in that any compromise to them will compromise their\n> security ass access codes. This is the best that can be done without using\n> public key however. The UNIX method of storing passwords means that passwords\n> have to be sen over the network in the clear. digest Authentication is\n> effectively providing Kerberos type security without a mediator.\n> \n> There are a few outstanding issues:\n> \n> 1) Should we include a mediated form of the authentication?\n> \n> 2) Should we specify a mechanism for defining new Keyed Digest algorithms?\n> \n> 3) Is Kerberos integration a practical proposition?\n\ni think we should leave these ideas for another authentication scheme.\nthe orignal goal of Digest was that we can do significantly better\nthan Basic with a near-trival set of changes.  we went from practically\nno security [uuencode(\"username:password\")] to something with some nice\nproperties while retaining the existing (2 party) web-model.  we were\nalso able to keep it free of patents and royalties and fully exportable\nand (probably) importable.  i think we should be happy with it as is.\n\n\n> 4) The syntax of the WWW-Authenticate: field is peculiar.\n\nyes, let's discuss this in the sub-group.  whether we change the syntax\nor not, there's a definite need to improve the wording and eliminate\nsome ambiguity.\n\n\n> We now have 2 entirely independent implementations, one in Spyglasses deployed\n> prducts and another in the Common Lisp Web server. Since both of those products\n> tend to get distributed on CD-ROMS there had better be a good reason behind any\n> proposed changes.\n\nFYI, NCSA, John Franks, and David Kristol also have implementations.\nare there any others ??\n\n\njeff hostetler\nspyglass, inc.\n\n\n\n"
        },
        {
            "subject": "Re: rethinking cachin",
            "content": "In message <m0tSQLW-000DJMC@ursa.cus.cam.ac.uk>, David Robinson writes:\n>>...\n>>Most of the current proposals for headers back and forth don't handle\n>>this situation correctly. Yet a straight-forward enumeration of\n>>'depends-on' in the return from the server to the proxy, along with\n>>proxy services that preserve that information and also all headers\n>>that comprise the things the value returned depended upon, would be a\n>>good first step. For some headers, (accept, for example), you need\n>>more than merely knowing what it depended upon, but also the WAY in\n>>which it depended upon the original data, so that the proxy itself can\n>>decide whether a cached item is appropriate.\n>\n>Absolutely. A Vary: 1#(http-header-name) header returned by the server\n>would help solve this problem.\n\nI like this syntax. The URI header would work, but this is short and\nto the point.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (wa",
            "content": "In message <19951212.79A6B78.8840@contessa.phone.net>, Mike Meyer writes:\n>\n>Yes, having \"..\" (or \"\") as a path component creates interesting\n>problems in writing relative URLs, and is probably a bad idea on any\n>server. Yes, an attempt to access \"../../../../etc/passwd\" is probably\n>someone trying to break into the system. However, it's up to the\n>people running the server, not the spec, to decide what is and is not\n>a security problem and deal with them.\n\nIt _is_ \"up to the spec\" to make implementors aware of such issues.\nThat's why SECURITY CONSIDERATIONS is mandatory in all RFCs, no?\n\n\n(I agree that returning 403 on seeing /../ is a should, not a must)\n\nDan\n\n\n\n"
        },
        {
            "subject": "Rethinking content negotiation (Was: rethinking caching",
            "content": ">>>good first step. For some headers, (accept, for example), you need\n>>>more than merely knowing what it depended upon, but also the WAY in\n>>>which it depended upon the original data, so that the proxy itself can\n>>>decide whether a cached item is appropriate.\n>>\n>>Absolutely. A Vary: 1#(http-header-name) header returned by the server\n>>would help solve this problem.\n>\n>I like this syntax. The URI header would work, but this is short and\n>to the point.\n\nI think this syntax is insufficient.  This is pretty much the information\nthat was contained in the HTTP/1.0-01 draft (AKA HTTP/1.1 before there was a\n1.1) and it was presumably changed because others felt likewise.  Using that\nscheme would require that the caching proxy keep the exact header(s) stored\nfor the specified vary quanity for comparison purposes.  This is a huge\nburden on a proxy because it doesn't just have to save headers once, it has\nto save headers for each request that doesn't have the exact same paramters\nas previous requests.\n\nAn example might be useful here, if not longwinded.  This demonstrates the\ninsufficent-ness of the old URI:, and a Vary: header scheme as well.\n\n   ---Request 1\nGET /index HTTP/1.1\nAccept: image/gif, image/jpeg, image/helper-app1, image/helper-app2\n   ---Response\nURI: </index.gif>, </index; vary=language, type>\n   ---cache has to store\n1./index | T | Accept: image/gif, image/jpeg, image/helper-app1,\nimage/helper-app2\n\n   ---Request 2\nGET /index HTTP/1.1\nAccept: image/gif, image/jpeg, image/helper-app1\nAccept-Language: fr\n   ---Response\nURI: </index.gif>, </index; vary=language, type>\n   ---cache has to store\n1. /index | T | Accept: image/gif, image/jpeg, image/helper-app1,\nimage/helper-app2\n2. /index | T | Accept: image/gif, image/jpeg, image/helper-app1,\nimage/helper-app2 | L | Accept-Language: fr\n\n   ---Request 3\nGET /index HTTP/1.1\nAccept: image/gif, image/jpeg, image/helper-app1\n   ---Response\nURI: </index.gif>, </index>; vary=language, type\n   ---cache has to store\n1. /index | T | Accept: image/gif, image/jpeg, image/helper-app1,\nimage/helper-app2\n2. /index | T | Accept: image/gif, image/jpeg, image/helper-app1,\nimage/helper-app2 | L | Accept-Language: fr\n3. /index | T | Accept: image/gif, image/jpeg, image/helper-app1\n\nThis is totally unworkable.  It's not just going to be one header\npermutation combo of charsets/encodings/types/langs per browser either, as\ngood browsers now let people set some of these parameters to their\npreference.  This list will grow forever, and proxies just won't cache\nthings that vary, or will forget about content negotiation altogether.\n\nThe new scheme has the big advantage of including all variants, their\nmetainformaiton, and the qs's of the variants in the URI header.  This gives\nthe proxy the information it needs to do the negotiation itself, and is the\nonly way to assure a trip to the original server would yield the same\nresult, and that serving from the cache is acceptable.  (I completely ignore\nthe vary by User-Agent here, but until we invent the psychic proxy, no proxy\ncan solve for server's varying content outside of the content negotiation\nscheme without listing all 2000 user agents that come through its doors.)\n\nI've always been very hesitant about the server's content variant-picking\nalgorithm being part of the protocol, because I saw that as a server-side\nimplementation issue, but as time goes on, I become more and more convinced\nof the weight of this issue and its impact on the scalabilty of the web.\nThe new URI header solves this caching problem perfectly, and\nsingle-handedly convinces me of the concept of opening the algorithm to the\nprotocol.\n\nNow, if I can just 1) convince Roy to default charsets/encodings to .001 on\nabsent headers, and 2) propose a different sytax than the URL: {} lispism, I\nmight die happy.  The current one can cause extremely big headers (you think\nAccept is bad?) and doesn't parse like existing headers.\n\nHow about:\n\nLocation: http://www.spyglass.com/index.html\nURI: \"/index.html\"; t=text/html; qs=.9,\n     \"/index.en.html\"; t=text/html; l=en; qs=.9\nURI: \"/index.jp.txt\"; t=text/plain; l=jp-JP; c=isp-2022-jp1; qs=.1\n\nAnd maybe even <uhg>:\nURI \"/index.nscp.html\"; ua=\"Mozilla/1.2N (Windows; I; 32bit)\"\n\nwhere the absence of an encoding, language, or charset mean they're\nindeterminate for that resource.  I like saving a few bytes on the\n\"language\" & \"encoding\" etc tags, and on the default to \"variant=\", and I\nlike sticking with a format that parses like the Accept headers.  Besides\nthe current 9/22 draft looks too much like Logic Bags :)  If anyone wishes,\nI will gladly write up a BNF.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: (Fwd) Re: Solaris HTTP server performance..",
            "content": "My apologies to the group.  I have given out incorrect information.\n\nBrandon Long (an actual developer) has corrected me.  I'm sorry,\nthe \"GETHOSTBYNAMEISEVIL\" was either in an internal version or comment\nand the name stuck with me.  I should have checked the code before \nposting.\n\nSorry folks,\nBeth\n\n> \n> What is this?  GETHOSTBYNAMEISEVIL?  In 1.4 and earlier, it was MINIMAL_DNS.\n> In 1.5, it is a runtime configuration option (the httpd.conf directive\n> DNSMode set to None or Minimal).\n> \n> You might also suggest a caching nameserver on the same machine if they\n> require name resolution\n> \n> Brandon\n> \n> -- \n>  Brandon Long\"I think, therefore I am Confused.\" - RAW\n>  HTTPd/SDG/NCSA/UIUC  \"Here's a nickel, kid.  Get yourself a better computer.\"\n>  blong@uiuc.edu-\"Dilbert\" by Scott Adams 6-24-95\n>  http://www.uiuc.edu/ph/www/blongConsider myself properly disclaimed.\n> \n\n\n-- \nElizabeth(Beth) Frank\nNCSA Server Development Team\nefrank@ncsa.uiuc.edu\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (was &lt;p&gt; ... &lt;/p&gt;",
            "content": "On Wed, 20 Dec 1995, Daniel W. Connolly wrote:\n\n> I think there are two issues that are getting confused here:\n> (1) whether it's OK to use ../../ in an HREF or SRC attribute\n> in an HTML document,\n> (2) whether it's OK to _send_ ../../ in the path field of\n> and HTTP request.\n> \n> (1) is cool, (2) is not.\n\nYup. And that's what the specs should say, I suppose. There's some stuff\nin RFC 1738 and the HTTP spec about URIs and \"absolute paths\". On that\nground, we could say that anything with \"..\"  in it is non-compliant if\nincluded in the Request Line. (Probably needs elabroation, though.)\n\n> What's _not_ cool is to try to sidestep the processing of .. on the\n>client side;  that is, to just combine the base and HREF into: \n> \n> http://www.foo.com/a/b/../gifs/btnhome3.gifs\n> \n> (which is _not_ a well-formed HTTP url) and send:\n> \n> GET /a/b/../gifs/btnhome3.gif HTTP/1.0\n> \n> This is illegal because it is a potential secruity risk. Consider a server\n> whose document root is /usr/local/etc/httpd/docs/ and a client who sends:\n> \n> GET /../../../../etc/passwd HTTP/1.0\n> Accept: text/plain\n> \n> a naive server implementation might just do:\n> fopen(\"/usr/local/etc/httpd/docs//../../../../etc/passwd\")\n> and give away a bunch of sensitive info.\n> \n> In stead, any server that sees /../ in the HTTP path is supposed to\n> issue a 403 Unauthorized response. (Is this in the HTTP specs somewhere?\n> YIKES! I can't find it in draft-ietf-http-v10-spec-02.txt!!!\n\nI think this is illegal simply because it's not a well-formed URL. The \nquestion, then, is what the server should do about it.\n\n(1) The euphemism is \"server tolerance of clients\". The truth, of course, \nis buggy client software. As far as server tolerance goes, it could try \nto normalize the path. But even though RFC 1738 does allow for \nheirarchical interpretations of paths in some schemes (HTTP included), \nthere's nothing to suggest that this path while heirarchical can *also* \nbe assumed to be embedded in an encompassing heirarchy. That is \"..\" as \n\"parent directory/component\" is a valid transformation only up to the \n\"root\". Even on UNIX (the inspiration) the parent of \"/\" is \"/\".\n\nSo, GET /../../../../etc/passwd \n== GET /etc/passwd\n== GET /usr/local/etc/httpd/docs/etc/passwd\n--> HTTP/1.0 404 Not Found\n\nis a compliant outcome.\n\n(2) Since the url is illegal to start with, a server could also return a\nstatus code to indicate \"Protocol Error\" or some other indication of\npermanent failure. Some 4xx codes appear to have such an interpretation,\nbut to keep in line with the FTP/SMTP/NNTP style of code code\nclassifications, this should be a 5xx response. My favorite (taken from\nnnrpd) would be\n\nHTTP/1.0 500 What?\n\nOn the issue of security, the typical approach is *not* to clue an \nattacker in to the fact that a security breach was involved in the failure.\nThere's no need to give out such information, and requiring this kind of \na reason in the spec would be a mistake, IMHO. \"Syntax Error, You Dope\" \nis just fine:-)\n\n\nRegards,\n\nArjun \n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "> Besides various ILU translators being available (lisp, C++, C, IDL)\n> this strong recommendation from Dan wouldn't be because ILU\n> happens to be very much like Modula-3 now would it? :-)\n\nThat's good enough for me -- I came reeeeeaaaaaallll close to\nwriting libwww-mod3 instead of libwww-perl.\n\nReplies to www-talk please -- this list is for discussion\nof HTTP-WG efforts only.  (;-)\n\n........Roy\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (was &lt;p&gt; ... &lt;/p&gt;",
            "content": ">\"Syntax Error, You Dope\" \n\n   I forsee the birth of a new series of response codes. The 600 series \nwould be for Scatalogical Errors. \n\n   ;^)\n\nPeace,\n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * Trust everyone, but brand your cattle. \n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (was &lt;p&gt; ... &lt;/p&gt;",
            "content": "According to Arjun Ray:\n> From http-wg-request@cuckoo.hpl.hp.com Wed Dec 20 15:49:15 1995\n> Received: from dehn.math.nwu.edu (root@dehn.math.nwu.edu [129.105.81.5]) by hopf.math.nwu.edu (8.6.10/8.6.9) with ESMTP id PAA18157 for <john@hopf>; Wed, 20 Dec 1995 15:49:14 -0600\n> Received: from hplb.hpl.hp.com (daemon@hplb.hpl.hp.com [15.255.59.2]) by dehn.math.nwu.edu (8.6.12/Math.nwu-v8) with ESMTP id PAA16731 for <john@math.nwu.edu>; Wed, 20 Dec 1995 15:50:13 -0600\n> Received: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Wed, 20 Dec 1995 21:49:06 GMT\n> Received: from http-wglistexploder by cuckoo.hpl.hp.com\n> (1.37.109.16/15.6+ISC) id AA123925977; Wed, 20 Dec 1995 21:46:17 GMT\n> Date: Wed, 20 Dec 1995 16:45:19 -0500 (EST)\n> \n> On Wed, 20 Dec 1995, Daniel W. Connolly wrote:\n> \n> > What's _not_ cool is to try to sidestep the processing of .. on the\n> >client side;  that is, to just combine the base and HREF into: \n> > \n> > http://www.foo.com/a/b/../gifs/btnhome3.gifs\n> > \n> > (which is _not_ a well-formed HTTP url) and send:\n> > \n> > GET /a/b/../gifs/btnhome3.gif HTTP/1.0\n> > \n> > This is illegal because it is a potential secruity risk. Consider a server\n> > whose document root is /usr/local/etc/httpd/docs/ and a client who sends:\n> > \n> > GET /../../../../etc/passwd HTTP/1.0\n> > Accept: text/plain\n> > \n> > a naive server implementation might just do:\n> > fopen(\"/usr/local/etc/httpd/docs//../../../../etc/passwd\")\n> > and give away a bunch of sensitive info.\n> > \n> > In stead, any server that sees /../ in the HTTP path is supposed to\n> > issue a 403 Unauthorized response. (Is this in the HTTP specs somewhere?\n> > YIKES! I can't find it in draft-ietf-http-v10-spec-02.txt!!!\n> \n> I think this is illegal simply because it's not a well-formed URL. The \n> question, then, is what the server should do about it.\n> \n\nAs I recall the draft RFC for URL's specifies that certain characters\n(like space) are forbidden, certain (like '?') have special meaning\nand otherwise the \"path\" part of a URL is an opaque string (which, in\nparticular, may have nothing to do with a path).  Neither '/' nor '.'\nare forbidden or have special meaning.  They do have special meaning\n*for some implementations* and no special meaning for others.\nLikewise the colon may have special meaning for some implementations\nand not for others.\n\nThe fact that certain strings may represent securtity risks for\nsome implementations does not automatically make them illegal.\nI don't believe that \"/../\" is forbidden in HTTP URL's.  If\nI am wrong I would be interested in a reference. \n\nIt would, of course, be quite reasonable for the HTTP spec to have\na UNIX-centric warning to implementors that they should make this\nstring illegal for their implementation (or risk the consequences).\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": "   Hi, Kids, \n\n   I wouldn't bother with this, except that since I sent the \nmessage that got this thread rolling I've seen over 150 messages \nin about 24 hrs. (I don't know the exact number, but it was over \n100 when I cleaned up this afternoon.) Most of them are duplicates \nor triplicates of the same message. (And not all are on this \nthread.)\n\n   Part of the reason for that is that the thread is now being \nechoed to two lists that I subscribe to--but about a third of the \ntraffic is because of the practice of replying to several individuals\n(invariably one of them is me) as well as to the lists. \n\n   Could we try to bring the distribution of this thread back down to \njust the lists? It would help my frazzled nerves a bit. \n\n   <now back to our regularly-scheduled argument>\n\nJohn Franks Wrote:\n>As I recall the draft RFC for URL's specifies that certain characters\n>(like space) are forbidden, certain (like '?') have special meaning\n>and otherwise the \"path\" part of a URL is an opaque string (which, in\n>particular, may have nothing to do with a path).  Neither '/' nor '.'\n>are forbidden or have special meaning.  They do have special meaning\n>*for some implementations* and no special meaning for others.\n>Likewise the colon may have special meaning for some implementations\n>and not for others.\n\n   I think you're right that there is nothing about the \"../\" \nstring that's in violation of URL-law. But then, I don't think \na URL is a very exact science anyway <g>. \n\n>It would, of course, be quite reasonable for the HTTP spec to have\n>a UNIX-centric warning to implementors that they should make this\n>string illegal for their implementation (or risk the consequences).\n\n   Yes, \"/../\" is a unixism, but the path part of a URL is inherently \nplatform specific. I see URLs with \"\\\" in them for DOS-type hosts, and \n\"\\..\\\" is just as much of a problem--maybe more because of the lack \nof permissions-bits in most DOSish OSs. The code I've seen that 403s \nthese things checks for the \"..\" and that seems to be a pretty \nuniversal string for \"go up a level in the file system\", or do you \nknow of an OS with more than 3 servers on the net that doesn't work \nthat way? \n\n   (side note: MS has implemented \"...\" and \"....\" in Win95 for \nreferencing up two- and three- levels respectively. I don't know about \nNT, but if it's not in there now it soon will be. A check for \"..\" \nwould obviously catch this as well.)\n\n   My bottom line here is that \"..\" in a path ought to be illegal \nin HTTP, perhaps with a notation to that effect in HTML. \n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * Trust everyone, but brand your cattle. \n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (was &lt;p&gt; ... &lt;/p&gt;",
            "content": ">As I recall the draft RFC for URL's specifies that certain characters\n>(like space) are forbidden, certain (like '?') have special meaning\n>and otherwise the \"path\" part of a URL is an opaque string (which, in\n>particular, may have nothing to do with a path).  Neither '/' nor '.'\n>are forbidden or have special meaning.\n\nIn the original URL draft spec, \"/\" denoted hierarcy within containers,\nwhile \"..\" referenced the parent container. They were very careful to note\nthat there was no implicit semantic interpretation with respect to a file\nsystem, Unix or otherwise. Whether or not the interpretation of \"/\" and\n\"..\" is the same in the current RFC or not, I can't say. But I can tell you\nthat there are enough clients and servers built upon the assumption that\n\"/\" denotes hierarchy that removing this specificity will break lots of\nstuff.\n\n>They do have special meaning\n>*for some implementations* and no special meaning for others.\n>Likewise the colon may have special meaning for some implementations\n>and not for others.\n\nBeyond the set of chars above (\"/\",\"..\", \"?\") you are right. The path\nportion of a URL is generally considered to be opaque to all but the\nimplementing server, with the semantic exception that \"/\" denotes\nhierarchy.\n\n>The fact that certain strings may represent securtity risks for\n>some implementations does not automatically make them illegal.\n>I don't believe that \"/../\" is forbidden in HTTP URL's.  If\n>I am wrong I would be interested in a reference.\n\nThis references the container of the root of your document tree, whatever\nthat means.\n\n>It would, of course, be quite reasonable for the HTTP spec to have\n>a UNIX-centric warning to implementors that they should make this\n>string illegal for their implementation (or risk the consequences).\n\nAnd by the same token, a warning that URL paths are not file system paths,\nregardless of the one to one mapping in many servers.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                      \"What? Me? WebSTAR?\"\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": "At 8:36 PM 12/20/95, BearHeart/Bill Weinman wrote:\n>   I think you're right that there is nothing about the \"../\"\n>string that's in violation of URL-law. But then, I don't think\n>a URL is a very exact science anyway <g>.\n\nThe most important thing to remember is that this type of URL syntax only\nhas meaning to WWW clients. HTTP servers always receive the complete path\nso all of this relative URL stuff is client-only. If clients are\ninterpreting the \"..\" above the root of the doc tree, you should be very\nworried because they know something about your server that the server\ndidn't tell them.\n\nIf you are worried about encoded \"..\" characters in a URL, then that is\nstrictly a server side problem and the server author should be spanked for\nnot checking.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                      \"What? Me? WebSTAR?\"\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": "  (I got four copies of this message!! I have no idea why that happened. \n<sigh> The Mail Gods must have been upset by my rant.)\n\nAt 09:13 pm 12/20/95 -0600, Chuck Shotton wrote:\n>The most important thing to remember is that this type of URL syntax only\n>has meaning to WWW clients. HTTP servers always receive the complete path\n>so all of this relative URL stuff is client-only. If clients are\n>interpreting the \"..\" above the root of the doc tree, you should be very\n>worried because they know something about your server that the server\n>didn't tell them.\n\n>If you are worried about encoded \"..\" characters in a URL, then that is\n>strictly a server side problem and the server author should be spanked for\n>not checking.\n\n   I think your assumption is in error. \n\n   I have a little testing-server I wrote so I could see how \ndifferent browsers act about stuff. It logs the entire conversation. \n(It's really usefull--it'll be in my book.)\n\n   I typed this into Netscape:  http://luna:8080/../../../etc/passwd\n\n   I got this in my log . . . \n\nGET /../../../etc/passwd HTTP/1.0\nConnection: Keep-Alive\nUser-Agent: Mozilla/2.0b3 (Win95; I)\nHost: luna:8080\nAccept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\n\n370 Request: GET /../../../etc/passwd\n370 403 Forbidden (/../../../etc/passwd contains go-back)\n\n   So it's not just a client problem . . . the client blindly sends \nthat request to the server. The server MUST deal with this, and as \nyou can see, it MUST disallow it. \n\n   (OTOH, I could have sent that request just as easily from telnet, \nand chances are that someone trying to break into a system would not \nbe using Netscape anyway.) \n\n   Personally, I think 403 is the appropriate message. If someone \nis doing a Bad Thing, an ambiguous message adds nothing to the \nsecurity--they know what they're trying to do. And if they aren't \nmalevolent then there's no valid reason to be ambiguous. \n\n   Of course, a 603 Scatalogical message would be okay too! ;^) \n\n   (I think I'll put one in my mini-server. <bg>)\n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * Sex is dirty. So save it for someone you love. \n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": "At 8:36p 12/20/95, BearHeart/Bill Weinman wrote:\n\n>   Yes, \"/../\" is a unixism, but the path part of a URL is inherently\n>platform specific. I see URLs with \"\\\" in them for DOS-type hosts, and\n>\"\\..\\\" is just as much of a problem--maybe more because of the lack\n>of permissions-bits in most DOSish OSs. The code I've seen that 403s\n>these things checks for the \"..\" and that seems to be a pretty\n>universal string for \"go up a level in the file system\", or do you\n>know of an OS with more than 3 servers on the net that doesn't work\n>that way?\n\n\nWhile Unix uses ../ and DOS uses ..\\ and MacOS uses :: natively to \"go up a\nlevel in the file system\", it is quite clear (at least to me) that native\nnotations have no place in a valid URL. As was quoted in another message,\nonly ../ is valid for denoting hierarchy in a URL. I wish browsers were\nstricter about this... ::sigh::\n\n-Walter\n\n__________________________________________________________________________\n    Walter Ian Kaye <boo@best.com>       | Excel | FoxPro | AppleScript |\n          Mountain View, CA              |--------- programmer ---------|\n http://www.natural-innovations.com/     |   Macintosh    |   Windows   |\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": "On Dec 20, 21:48, BearHeart/Bill Weinman wrote:\n> Subject: Re: partial URLs ?\n>    I typed this into Netscape:  http://luna:8080/../../../etc/passwd\n> \n>    I got this in my log . . . \n> \n> GET /../../../etc/passwd HTTP/1.0\n> Connection: Keep-Alive\n> User-Agent: Mozilla/2.0b3 (Win95; I)\n> Host: luna:8080\n> Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\n> \n> 370 Request: GET /../../../etc/passwd\n> 370 403 Forbidden (/../../../etc/passwd contains go-back)\n\nTry that on my server (WebSite, try http://solo.dc3.com/) Try other ugly \ncombinations like \\../\\./\\.. well you get the idea. It doesn't do the \nmulti-dot stuff for multiple \"ups\" though... Not a bad idea. Maybe next \nverision :-).\n\nWebSite \"normalizes\" any of that junk out of a URL. The /../ is assumed to be \nthe same as / (the parent of the root is the root). If it had to change \nanything to get the \"normalized\" form, it sends a redirect to the browser in \nan attempt to \"send a message\" to the browser operator and prevent further \nabuse from relative links in the document.\n\nJust one person's solution to the problem.\n\n  -- Bob\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, John Franks wrote:\n\n> Correct me if I am wrong, but I concluded from Spero's postings that\n> nothing currently proposed including MGET, hold-open, or even HTTP-NG\n> would improve (or even match?) the user's perceived performance\n\nWhat NETSCAPE is doing is orthoganal to these proposals, at least the\ngeneric proposals.  When I use NETSCAPE on a narrow pipe, it isn't \nthat impressive and to me a least a bit distracting but that is mostly\nin the details of the current implemenation which hopefully will improve.\nI think what they are attempting to achieve is good from the end\nuser's perspective but UPP will be horrible if the network's (or server's)\nability to deliver data is choked.\n\nThe varitions of MGET and HTTP-ng are likely to dramatically improve\nthe net responsiveness of all browsers as well as reduce the cost of\nthe network infrastructure for a given service level.\n\nI also suspect that the market is young enough that acceptance will be\nbased on many factors.  New function support (html3 for example) or\nsimply correctness of HTML processing will be important. \n\nCost is also important for many corporate situations. My client would like\nto have a WEB browser available for everyone but considering that a\nlarge percentage of their employees wouldn't use it they are leary of\ncommiting to pay much real money.\n\nAll this is to say that things are moving to fast for me to believe that\none set of desireable characteristics will win out.  Win95 is supposed\nto include a TCP/IP stack.  If Microsoft follows IBM's warp lead and\nprovides a good WEB browser bundled with Win95 their may not be much\nmarket for anything.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ",
            "content": ">    Yes, \"/../\" is a unixism, but the path part of a URL is inherently\n> platform specific. I see URLs with \"\\\" in them for DOS-type hosts, and\n> \"\\..\\\" is just as much of a problem--maybe more because of the lack\n> of permissions-bits in most DOSish OSs. The code I've seen that 403s\n> these things checks for the \"..\" and that seems to be a pretty\n> universal string for \"go up a level in the file system\", or do you\n> know of an OS with more than 3 servers on the net that doesn't work\n> that way?\n\nDo you know of any other OS that *does* work that way? If not, then\npretty much every platform on the net except Unix and Dos based ones\nqualify. I've seen numerous claims that the most common server on the\nis running on neither Unix nor DOS. Of course, server authors for\nthose platforms will probably just ignore this restriction, as they\nhave no reason to deal with it.\n\n>    (side note: MS has implemented \"...\" and \"....\" in Win95 for\n> referencing up two- and three- levels respectively. I don't know about\n> NT, but if it's not in there now it soon will be. A check for \"..\"\n> would obviously catch this as well.)\n\nSo you're arguing that the string \"..\" anywhere in the path - not just\nas a path componenet - ought to be illegal? Is there any point in this\nother than letting incompetent programmers blame the client if their\nsystem is broken into?\n\nSeriously, what is the point of this? Competent programmers on boxes\nthat add special meaning to any path component will deal with it in\nthe server whether it's illegal or not. Making it illegal isn't any\nmore likely to make incompetent programmers check for it than a\nwarning. Programmers for other boxes will ignore it. Should clients\nrefuse to send it if it shows up in a document? Should your email\naddress show up in the error message so users can go direct to you for\nan explanation?\n\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": ">>I like this syntax. The URI header would work, but this is short and\n>>to the point.\n\n>I think this syntax is insufficient.  This is pretty much the information\n>that was contained in the HTTP/1.0-01 draft (AKA HTTP/1.1 before there was a\n>1.1) and it was presumably changed because others felt likewise.  Using that\n>scheme would require that the caching proxy keep the exact header(s) stored\n>for the specified vary quanity for comparison purposes.  This is a huge\n>burden on a proxy because it doesn't just have to save headers once, it has\n>to save headers for each request that doesn't have the exact same paramters\n>as previous requests.\n\nFirstly, I'm not convinced that it's a _huge_ burden; does anyone have\nany statistics on Accept: headers?\n\n>An example might be useful here, if not longwinded.  This demonstrates the\n>insufficent-ness of the old URI:, and a Vary: header scheme as well.\n>...\n>[Example of Accept: header]\n\nYes, there are cases where a Vary: header will not provide enough information\nfor a cache. But are you arguing against the implementation of the\nVary: header? I am suggesting that it is a useful feature for varying\nresources. You seem to be arguing that it may not be enough for caching\nof content-negotatiated resources. Does that matter?\n\n>...\n>(I completely ignore the vary by User-Agent here, but until we invent the\n>psychic proxy, no proxy can solve for server's varying content outside of the\n>content negotiation scheme without listing all 2000 user agents that come\n>through its doors.)\n\nI'm not quite sure what you mean, but half of the point of the Vary: header\nwas to inform about the server's varying content outside the content\nnegotiation scheme.\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "Re: partial URLs ? (wa",
            "content": "> >It would, of course, be quite reasonable for the HTTP spec to have\n> >a UNIX-centric warning to implementors that they should make this\n> >string illegal for their implementation (or risk the consequences).\n> \n> And by the same token, a warning that URL paths are not file system paths,\n> regardless of the one to one mapping in many servers.\n\nActually, the warning doesn't have to be unix centric. It can also\nimply the warning about file systems at the same time. Suggested\nwording:\n\nWhile URLs paths are not file system paths, they may be\nimplemented as such. If this is the case, any path components\nthat have a meaning other than \"descend into the named\ndirectory\" in the file system should be examined for possibly\nsecurity problems and disallowed if there are any. For example, \n\"..\" as a path component on Unix and MS-DOS means to go up one\ndirectory level, which can potentially access files outside\nthe server tree, and should thus be disallowed.\n\nSee - it has a non-Unix-centric warning, a warning that URLs are not\nfile paths, and mentions \"..\" explicitly.\n\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "At 05:39 PM 12/21/95 GMT, David Robinson wrote:\n>>for the specified vary quanity for comparison purposes.  This is a huge\n>>burden on a proxy because it doesn't just have to save headers once, it has\n>>to save headers for each request that doesn't have the exact same paramters\n>>as previous requests.\n>\n>Firstly, I'm not convinced that it's a _huge_ burden; does anyone have\n>any statistics on Accept: headers?\n\nIt's likely not much of a burden now, but as browsers become more willing to\nlet uses set their own q's, or add type and encoding helper-apps to their\nAccept* headers, it will become hugely burdensome.\n\n>Yes, there are cases where a Vary: header will not provide enough information\n>for a cache. But are you arguing against the implementation of the\n>Vary: header?\n\nYes.\n\n>resources. You seem to be arguing that it may not be enough for caching\n>of content-negotatiated resources. Does that matter?\n\nYes.  Caching is integral.\n\n>I'm not quite sure what you mean, but half of the point of the Vary: header\n>was to inform about the server's varying content outside the content\n>negotiation scheme.\n\nSo, tell me again what functionailty the Vary: header would provide?  How\nwould I modify my behavior based on a Vary: header?  Either variances are\nacceptable future responses (in the content provider's mind), and hence\ncachable, or their not, and then the server will send a Cache-Control.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Mirrorin",
            "content": "Some thoughts about mirroring, from a message originally in www-talk.\n\n>From: Martin Hamilton <martin@mrrl.lut.ac.uk>\n>\n>I was just wondering whether there are any HTTP servers out there\n>which let the server admin configure redirects based on the\n>client's domain name.  This seems like a neat way to make browsers\n>automatically use \"nearby\" mirror sites, without having to go\n>round tweaking all the clients.  Obviously it would only be useful\n>if you take the trouble to figure out the client's domain name\n>\n>e.g. to re-direct accesses to everything at http://www.apache.org/\n>to an appropriate mirror site, you would want to put something like\n>this in your server config\n>\n>  RedirectDomain / http://Bond.edu.au/External/Misc/apache/ .au .nz\n>    .jp .kr .cn\n>  RedirectDomain / http://iuinfo.tuwien.ac.at/apache/ .at .de .dk\n>  RedirectDomain / http://sunsite.mff.cuni.cz/web/apache/ .cz\n>  RedirectDomain / http://sunsite.icm.edu.pl/pub/www/apache/ .pl\n>  RedirectDomain / http://sunsite.doc.ic.ac.uk/packages/apache/ .uk\n>    .fr .be\n>\n>(etc...!)\n>\n>If nobody else is working on this sort of thing, I might have a\n>stab at hacking this into the NCSA and Apache servers\n\nI replied:\n>Rather than trying to force the client to use the server that you think\n>would be best for it, it would seem better to provide data to allow the\n>client to choose.\n\nAnd I mentioned the URI: {mirror \"url\"}, {mirror \"url2\"} header as providing\nthis. However, I've subsequently had some thoughts on this.\n\nFirstly, some general complaints about URI:; why is it so overloaded?\nIt provides two pretty distinct features. Firstly, the information required\nfor client/proxy-based content negotiation. Secondly, a list of other URLs that\nidentify the resource. Also, what on earth does URI: {name \"url\"} mean? What\nis a `location-independent name corresponding to the Request-URI'? Independent\nof whose location?\n\nApart from these concerns, I don't think the URI: {mirror} feature provides\nwhat it is needed for effective mirroring.\n\nHow does mirroring currently work?\n\nA site copies the web pages from a master, usually via ftp. It does not\nusually copy CGI scripts, or modify the pages. So admins arrange for the\npages to have relative links to one another, but to have absolute links\nto any CGI scripts (e.g. bug report forms). For example, from the Apache\nweb page bug_report.html:\n...\n<IMG SRC=\"images/apache_sub.gif\" ALT=\"\">\n<H2>Apache Bug Reporting Page</H2>\n...\n<OL>\n<LI>Made sure the bug exists in <A href=\"dist/\">the most recent version</A> of\nApache.\n...\n<FORM METHOD=\"POST\" ACTION=\"http://www.apache.org/bugs.cgi\">\n\nWhat protocol support is needed for automatic use of mirrors?\n\nFirstly, note that if http://xxxx/adir/ is mirrored at http://yyyy/bdir/, one\n_cannot_ assume that http://xxxx/adir/subres is similarly mirrored.\nSecondly, note that by the time you download a document, it may be\ntoo late to use any information about mirrors for the document.\n(Unless you do a HEAD, or the body is slow to transfer.)\n\nSo I suggest the following:\n\nMirror: primary-URI \" \" 1*secondary-URI\n\nthe header says that any of the secondary URIs may be used instead of\nthe primary URI in any context, or vice versa. The header is for informational\npurposes. (Rather like the additional information given by a DNS server.)\nTypically, one of the URIs will be the URI requested by the client.\n\nTypical use:\nClient requests http://www.apache.org/; it receives the request with\nMirror: http://www.apache.org/ http://iuinfo.tuwien.ac.at/apache/\n        http://sunsite.mfff.cuni.cz/web/apache/\n\nIf the user then presses on this anchor\n<IMG SRC=\"images/orange_ball.gif\" alt=\"o\">\n <A HREF=\"info.html\">Background Information</A><BR>\n\nThen the client may access ./info.html relative to any of the mirror URLs\ne.g. http://iuinfo.tuwien.ac.at/apache/, and so retrieve\nhttp://iuinfo.tuwien.ac.at/apache/info.html\nreceiving the header:\nMirror: http://www.apache.org/info.html\n        http://iuinfo.tuwien.ac.at/apache/info.html\n        http://sunsite.mfff.cuni.cz/web/apache/info.html\n\nIf the server sends a Base: or Location: (or even a URI: header) then\nit may wish to add a Mirror: header for the client's information.\nThus the Mirror: header can be applied to any URI passed in HTTP headers.\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-digest-aa02.tx",
            "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : A Proposed Extension to HTTP : Digest Access \n                   Authentication                                          \n       Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n                   A. Luotonen, E. Sink, L. Stewart\n       Filename  : draft-ietf-http-digest-aa-02.txt\n       Pages     : 6\n       Date      : 12/20/1995\n\nThe protocol referred to as \"HTTP/1.0\" includes specification for a Basic \nAccess Authentication scheme.  This scheme is not considered to be a secure\nmethod of user authentication, as the user name and password are passed \nover the network in an unencrypted form.  A specification for a new \nauthentication scheme is needed for future versions of the HTTP protocol.  \nThis document provides specification for such a scheme, referred to as \n\"Digest Access Authentication\".  The encryption method used is the RSA Data\nSecurity, Inc. MD5 Message-Digest Algorithm [3].                           \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-digest-aa-02.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-digest-aa-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-digest-aa-02.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19951220165327.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-digest-aa-02.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-digest-aa-02.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19951220165327.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "interaction of caching with content negotiation, authentication, state, et",
            "content": "I've been on vacation and just got back. I think the interaction of\n'content negotiation' and 'caching' can be limited to acknowledging\nthat:\n\n* the value returned by applying a method to a URI depends on many\nof the other headers in the request (including those supplied during\ncontent negotiation, the user agent, etc.)\n\n* the originating server needs to indicate which request headers were\ninvolved in deciding what content to return, even if those request\nheaders are not replicated in the entity headers of the response\n\n* a cache might legitimately keep around several cached values for the\nsame method applied to the same URI, and want to apply a\n'get-if-different' validator that would request validation of one of\nthe several cached values.\n\nThe same principle for caching applies for content negotiation,\nauthentication, and state sharing.\n\n\n\n"
        },
        {
            "subject": "interaction of caching with content negotiation, authentication, state, et",
            "content": "Larry Masinter writes:\n > I've been on vacation and just got back. I think the interaction of\n > 'content negotiation' and 'caching' can be limited to acknowledging\n > that:\n > \n > * the value returned by applying a method to a URI depends on many\n > of the other headers in the request (including those supplied during\n > content negotiation, the user agent, etc.)\n > \n > * the originating server needs to indicate which request headers were\n > involved in deciding what content to return, even if those request\n > headers are not replicated in the entity headers of the response\n > \n > * a cache might legitimately keep around several cached values for the\n > same method applied to the same URI, and want to apply a\n > 'get-if-different' validator that would request validation of one of\n > the several cached values.\n > \n > The same principle for caching applies for content negotiation,\n > authentication, and state sharing.\n\n\nI think there are additional complications which require more\nattention from the caching subgroup (though it's possible the\nattention will boil down to dismissing this issue).\n\nIf it is possible for a given object to be returned in response to\nrequests on different URIs, as it would be with content negotiation\n(among other possibilities), then either that object must be\npre-expired, so that it will be validated on every request, or it will\nbe possible for multiple versions of the object to be present in, and\nservable from, a cache if the object changes at the origin server\nprior to its stated expiration date.  The consequence of this is that\nusers might see previous versions of an object, even though they are\nonly using a single cache. \n\nrom a user's perspective, this is worse than consistently serving\na version that lags somewhat behind the \"latest & greatest\" version\nfrom the origin server.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-ietf-http-digest-aa02.tx",
            "content": "what is this stuff you are sending me.  I have no clue what it is!!\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, John Franks wrote:\n\n> The fastest way to get all the text is to send it first but it can't be\n> displaye until layout information like the size and shape of all images is \n> known.  This is the point of the Netscape multiple connections.  They get\n> the first few bits of each image which contain the size information.\n\nYes, but ... the document authors can contribute by having meaningful\nalternatives to graphics and folks with slow connections can choose to\nignore the graphics and run in text mode (wibni authors could rank\nimages importance to content and give users a chance to ignore the\nwarm feely stuff but get content graphics like weather maps or\neconomic trends or whatever).\n\nBut beyond that,  NETSCAPE has proposed HTML extentions to code image\nsize in the <img> tag so that would assist the rendering.  And if\nHTTP were extended in a minor fashion, the browser could query shape\nof objects to speed up the process. An agressive browser like NETSCAPE\ncould then use two connections, one to pull in the base text and the\nother to obtain the shape data.  Dynamic evaluation of the RTT could\nlead to more connections if the apparent bandwidth would accept it.\n\nAlso, there is really no reason why the browser can't revise the\nrendering as more information becomes available.\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "Daniel DuBois:\n>\n>I've always been very hesitant about the server's content variant-picking\n>algorithm being part of the protocol, because I saw that as a server-side\n>implementation issue, but as time goes on, I become more and more convinced\n>of the weight of this issue and its impact on the scalabilty of the web.\n\nI still am very hesitant about moving variant-picking algorithms into\nproxies: if it is done in the wrong way, it will have a negative\nimpact on the extensibility of the protocol.\n\nIt is desirable however to put a sub-algorithm that often occurs in\nserver variant-picking algorithms in the protocol.  The sub-algorithm\nin question is the algorithm that can match MIME type, language, and\ncontent coding.\n\n>The new URI header solves this caching problem perfectly, and\n>single-handedly convinces me of the concept of opening the algorithm to the\n>protocol.\n\nI agree with your assessment of the old `vary' method not being a good\nsolution, but I disagree with you that the new URI header `solves the\ncaching problem perfectly'.\n\nThe URI header in draft-ietf-http-v11-spec-00 is a step in the right\ndirection, but it does not offer a perfect solution by itself, we need\nsome additional mechanisms currently under discussion. (One example of\nsuch a mechanism is the request header I called `Send-no-body-for' in\nmy notes on content negotiation (see\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/1995q4/0347.html ).\n\nWe need something like `Send-no-body-for' because variant-picking\nalgorithms can never be moved completely into proxies.  (User-agent\nbased negotiation is the most obvious reason why this cannot be done.)\n\nI expect to be working on these issues inside the content negotiation\nsubgroup.\n\nBy the way, I disagree with Larry Masinter on where to draw the line\nbetween content negotiation and caching. My assignment of header\nresponsibility is:\n\nContent negotiation subgroup:\n Accept-*, URI, Location\n\nCaching subgroup:\n Expires, Cache-control, If-Modified-Since, ....\n\nURI and Location are both used by all types of clients, not just\nproxies, in reactive negotiation.  This makes them primarily the\nresponsibility of the negotiation subgroup.\n\n>Dan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "On Sat, 23 Dec 1995, Koen Holtman wrote:\n\n> By the way, I disagree with Larry Masinter on where to draw the line\n> between content negotiation and caching. My assignment of header\n> responsibility is:\n> \n> Content negotiation subgroup:\n>  Accept-*, URI, Location\n> \n> Caching subgroup:\n>  Expires, Cache-control, If-Modified-Since, ....\n> \n> URI and Location are both used by all types of clients, not just\n> proxies, in reactive negotiation.  This makes them primarily the\n> responsibility of the negotiation subgroup.\n\nTHe only problem with your position is that desigining negotiation \nwhile igoring proxy/caching issues can't be successful. Either negotiation\nwon't work in the general cases or proxies will never be able to determine\nif a response can be delivered in response to another requrest.\n\nFurthermore, to some degree or another all clients provide caching as well\nso caching is not simply a proxy issue.\n\nI think Larry Masinter's split makes much sense, especially considering the\nbroad participation in the proxy/caching sub-group. If the negotiation group\ncan identify how to specify and what to negotiate then the join the \ncaching/proxy discussion to make sure we end up with a cohesive mechanism\nwhich covers all issues.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Moving HTTP 1.0 to informationa",
            "content": "Greetings. This is the first round of the WG's efforts to change the\ndraft-ietf-http-v10-spec-04.txt document from a BCP draft to an informational\ndraft. Larry Masinter asked me to take charge of this effort in Dallas, and\nhere's the first result.\n\nBascially, there are three additions to the -04 draft and no changes. I'm\ndistributing this to the WG as essentially a set of diffs instead of a full\ndraft since the changes are localized and hopefully easy to follow.\n\nDiff summary:\n- Add a paragraph to section 1.1 indicating what the intended status of the\ndraft is to be.\n- Add a new security section, 12.5, reflecting the recent discussion of\npartial URLs and path component attacks.\n- Add Appendix D, which puts back in most of what was removed from the draft\nbetween the -01 and -02 steppings. This is:\n  - PUT, DELETE, LINK, and UNLINK request methods as new section D.1\n  - Section D.2 is all headers from the -01 stepping that were not in -04\n    *except*\n      - Content-Transfer-Encoding (feature already covered in section C.4)\n      - Forwarded (not implemented)\n      - Orig-URL (not implemented)\n      - Public (not implemented)\n  - Section 9 from -01, \"Content Negotiation\", is now section D.3\n  - Section 3.8 from -01, \"Language Tags\", is now section D.4\n  - Section 3.9 from -01, \"Quality Values\", is now section D.5\n\nIn new Appendix D, I have renumbered all the section cross-references to match\nthe -04 draft. Other than that, I did not change any of the wording from the\nsections I moved from the -01 draft.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n--============_-1392122978==_============\nContent-Type: text/plain; name=\"BCP-to-info-00.txt\"; charset=\"us-ascii\"\nContent-Disposition: attachment; filename=\"BCP-to-info-00.txt\"\n\nThe following paragraph is added after the first paragraph of section 1.1 (Purpose):\n\n   This specification is for informational purposes only. It reflects\n   the consensus of the IETF HTTP Working Group about what features\n   would normally be found in an HTTP/1.0 implementation. These\n   features are split into two sections. The features for which there was\n   strong consensus about how they are implemented are listed in the\n   main body of this document. Features for which there was not strong\n   consensus are listed in Appendix D.\n\nIn -04, Section 12.5 is added:\n\n12.5  Attacks Based On Path Names\n\n   Implementations of the HTTP servers should be careful to restrict the\n   documents returned by HTTP requests to be only those that were intended\n   by the administrators. If an HTTP server translates HTTP URIs directly\n   into file system calls, the server must take special care not to serve\n   files outside the desired directory tree. For example, Unix, Microsoft\n   Windows, and other operating systems use \"..\" as a path component to\n   indicate a directory level above the current one. A URL with such\n   constructs can be constructed to potentially allow access to files\n   outside the desired directory structure, and should thus be disallowed.\n\nIn -04, Appendix D is added:\n\nD.  Additional Features\n\n   This appendix documents features which were was not strong consensus in\n   the IETF HTTP Working Group. In some cases, there was strong consensus\n   that the feature was needed but disagreement about how it should be\n   implemented. In other cases, there was no general agreement on the\n   feature. Implementors who add the features in the Appendix should be\n   aware that software using these features are less likely to be\n   interoperable than software using the features from the main part of\n   this specification.\n\nD.1 Additional Request Methods\n\nD.1.1 PUT\n\n   The PUT method requests that the enclosed entity be stored under \n   the supplied Request-URI. If the Request-URI refers to an already \n   existing resource, the enclosed entity should be considered as a \n   modified version of the one residing on the origin server. If the \n   Request-URI does not point to an existing resource, and that URI is \n   capable of being defined as a new resource by the requesting user \n   agent, the origin server can create the resource with that URI. If \n   a new resource is created, the origin server must inform the user \n   agent via the 201 (created) response. If an existing resource is \n   modified, either the 200 (ok) or 204 (no content) response codes \n   should be sent to indicate successful completion of the request. If \n   the resource could not be created or modified with the Request-URI, \n   an appropriate error response should be given that reflects the \n   nature of the problem.\n\n   The fundamental difference between the POST and PUT requests is \n   reflected in the different meaning of the Request-URI. The URI in a \n   POST request identifies the resource that will handle the enclosed \n   entity as an appendage. That resource may be a data-accepting \n   process, a gateway to some other protocol, or a separate entity \n   that accepts annotations. In contrast, the URI in a PUT request \n   identifies the entity enclosed with the request -- the user agent \n   knows what URI is intended and the server must not attempt to apply \n   the request to some other resource. If the server desires that the \n   request be applied to a different URI, it must send a 301 (moved \n   permanently) response; the user agent may then make its own \n   decision regarding whether or not to redirect the request.\n\n   A single resource may be identified by many different URIs. For \n   example, an article may have a URI for identifying \"the current \n   version\" which is separate from the URI identifying each particular \n   version. In this case, a PUT request on a general URI may result in \n   several other URIs being defined by the origin server. The user \n   agent should be informed of these URIs via one or more URI header \n   fields in the response. The Location header field should be used to \n   identify the exact location URI if it is different than the\n   Request-URI.\n\n   A valid Content-Length is required on all HTTP/1.0 PUT requests. An \n   HTTP/1.0 server should respond with a 400 (bad request) message if \n   it cannot determine the length of the request message's content.\n\n   The client can create or modify relationships between the enclosed \n   entity and other existing resources by including Link header \n   fields, as described in Section D.2.6. As with POST, the server may \n   use the Link information to perform other operations as a result of \n   the request. However, no mandatory operation is imposed on the \n   origin server. The origin server may generate its own or additional \n   links to other resources.\n\n   The actual method for determining how the resource is placed, and \n   what happens to its predecessor, is defined entirely by the origin \n   server. If version control is implemented by the origin server, \n   then Link relationships should be defined by the server to help \n   identify and control revisions to a resource; suggested \n   relationship names include \"Derived-From\", \"Obsoletes\", and \n   \"Updates\".\n\n       Note: The model of sending an entire PUT request within a \n       single message, without first checking if the server is \n       willing to accept that data, will break if the server is \n       unwilling to accept the request or desires some form of \n       authentication beforehand. Worse, the client won't be \n       notified of the reason for error if a TCP reset is received \n       prior to reading the response buffer (see note in \n       Section 9.4). It should therefore be recognized that \n       HTTP/1.0 PUT and large POST requests will only work reliably \n       if the client's intentions and server's desires are \n       negotiated prior to the request.\n\nD.1.2 DELETE\n\n   The DELETE method requests that the origin server delete the \n   resource identified by the Request-URI. This method may be \n   overridden by human intervention (or other means) on the origin \n   server. The client cannot be guaranteed that the operation has been \n   carried out, even if the status code returned from the origin \n   server indicates that the action has been completed successfully. \n   However, the server should not indicate success unless, at the time \n   the response is given, it intends to delete the resource or move it \n   to an inaccessible location.\n\n   A successful response should be 200 (ok) if the response includes \n   an entity describing the status, 202 (accepted) if the action has \n   not yet been enacted, or 204 (no content) if the response is OK but \n   does not include an entity.\n\nD.1.3 LINK\n\n   The LINK method establishes one or more Link relationships between \n   the existing resource identified by the Request-URI and other \n   existing resources. The difference between LINK and other methods \n   allowing links to be established between resources is that the LINK \n   method does not allow any Entity-Body to be sent in the request and \n   does not result in the creation of new resources.\n\nD.1.4 UNLINK\n\n   The UNLINK method removes one or more Link relationships from the \n   existing resource identified by the Request-URI. These \n   relationships may have been established using the LINK method or by \n   any other method supporting the Link header. The removal of a link \n   to a resource does not imply that the resource ceases to exist or \n   becomes inaccessible for future references.\n\nD.2  Additional Header Field Definitions\n\n   This section defines the syntax and semantics of all standard \n   HTTP/1.0 header fields. For Entity-Header fields, both sender and \n   recipient refer to either the client or the server, depending on \n   who sends and who receives the entity.\n\nD.2.1  Accept\n\n   The Accept header field can be used to indicate a list of media \n   ranges which are acceptable as a response to the request. The \n   asterisk \"*\" character is used to group media types into ranges, \n   with \"*/*\" indicating all media types and \"type/*\" indicating all \n   subtypes of that type. The set of ranges given by the client should \n   represent what types are acceptable given the context of the \n   request. The Accept field should only be used when the request is \n   specifically limited to a set of desired types, as in the case of a \n   request for an in-line image, or to indicate qualitative \n   preferences for specific media types.\n\n   The field may be folded onto several lines and more than one \n   occurrence of the field is allowed, with the semantics being the \n   same as if all the entries had been in one field value.\n\n       Accept         = \"Accept\" \":\" #(\n                             media-range\n                             [ \";\" \"q\" \"=\" qvalue ]\n                             [ \";\" \"mxb\" \"=\" 1*DIGIT ] )\n\n       media-range    = ( \"*/*\"\n                      |   ( type \"/\" \"*\" )\n                      |   ( type \"/\" subtype )\n                        ) *( \";\" parameter )\n\n   The parameter q is used to indicate the quality factor, which \n   represents the user's preference for that range of media types. The \n   parameter mxb gives the maximum acceptable size of the Entity-Body, \n   in decimal number of octets, for that range of media types. \n   Section D.3 describes the content negotiation algorithm which makes \n   use of these values. The default values are: q=1 and mxb=undefined \n   (i.e., infinity).\n\n   The example\n\n       Accept: audio/*; q=0.2, audio/basic\n\n   should be interpreted as \"I prefer audio/basic, but send me any \n   audio type if it is the best available after an 80% mark-down in \n   quality.\" \n\n   If no Accept header is present, then it is assumed that the client \n   accepts all media types with quality factor 1. This is equivalent \n   to the client sending the following accept header field:\n\n       Accept: */*; q=1\n\n   or\n\n       Accept: */*\n\n   A more elaborate example is\n\n       Accept: text/plain; q=0.5, text/html,\n               text/x-dvi; q=0.8; mxb=100000, text/x-c\n\n   Verbally, this would be interpreted as \"text/html and text/x-c are \n   the preferred media types, but if they do not exist then send the \n   Entity-Body in text/x-dvi if the entity is less than 100000 bytes, \n   otherwise send text/plain.\"\n\n       Note: In earlier versions of this document, the mxs \n       parameter defined the maximum acceptable delay in seconds \n       before the response would arrive. This has been removed as \n       the server has no means of obtaining a useful reference \n       value. However, this does not prevent the client from \n       internally measuring the response time and optimizing the \n       Accept header field accordingly.\n\n   Media ranges can be overridden by more specific media ranges or \n   specific media types. If more than one media range applies to a \n   given type, the most specific reference has precedence. For example,\n\n       Accept: text/*, text/html, text/html;version=2.0, */*\n\n   have the following precedence:\n\n       1) text/html;version=2.0\n       2) text/html\n       3) text/*\n       4) */*\n\n   The quality value associated with a given type is determined by \n   finding the media range with the highest precedence which matches \n   that type. For example,\n\n       Accept: text/*;q=0.3, text/html;q=0.7, text/html;version=2.0,\n               */*;q=0.5\n\n   would cause the following values to be associated:\n\n       text/html;version=2.0                      = 1\n       text/html                                  = 0.7\n       text/plain                                 = 0.3\n       image/jpeg                                 = 0.5\n       text/html;level=3                          = 0.7\n\n   It must be emphasized that the Accept field should only be used \n   when it is necessary to restrict the response media types to a \n   subset of those possible or when the user has been permitted to \n   specify qualitative values for ranges of media types. If no quality \n   factors have been set by the user, and the context of the request \n   is such that the user agent is capable of saving the entity to a \n   file if the received media type is unknown, then the only \n   appropriate value for Accept is \"*/*\".\n\n       Note: A user agent may be provided with a default set of \n       quality values for certain media ranges. However, unless the \n       user agent is a completely closed system which cannot \n       interact with other rendering agents, this default set \n       should be configurable by the user.\n\nD.2.2  Accept-Charset\n\n   The Accept-Charset request header field can be used to indicate a \n   list of preferred character set encodings other than the default\n   US-ASCII and ISO-8859-1. This field allows clients capable of \n   understanding more comprehensive or special-purpose character set \n   encodings to signal that capability to a server which is capable of \n   representing documents in those character set encodings.\n\n       Accept-Charset = \"Accept-Charset\" \":\" #charset\n\n   Character set encoding values are described in Section 3.5. An \n   example is\n\n       Accept-Charset: iso-8859-5, unicode-1-1\n\n   The value of this field should not include \"US-ASCII\" or\n   \"ISO-8859-1\", since those values are always assumed by default. If \n   a resource is only available in a character set encoding other than \n   the defaults, and that character set encoding is not listed in the \n   Accept-Charset field, it is only acceptable for the server to send \n   the entity if the character set encoding can be identified by an \n   appropriate charset parameter on the media type or within the \n   format of the media type itself.\n\n       Note: User agents are not required to be able to render the \n       characters associated with the ISO-8859-1 character set \n       encoding. However, they must be able to interpret their \n       meaning to whatever extent is required to properly handle \n       messages in that character set encoding.\n\nD.2.3  Accept-Encoding\n\n   The Accept-Encoding request header field is similar to Accept, but \n   restricts the encoding-mechanism values which are acceptable in the \n   response.\n\n       Accept-Encoding         = \"Accept-Encoding\" \":\" \n                                 #( encoding-mechanism )\n\n   An example of its use is\n\n       Accept-Encoding: compress, gzip\n\n   If no Accept-Encoding field is present in a request, the server \n   should assume that the client will accept any encoding-mechanism.\n\nD.2.4  Accept-Language\n\n   The Accept-Language request header field is similar to Accept, but \n   restricts the set of natural languages that are preferred as a \n   response to the request.\n\n       Accept-Language         = \"Accept-Language\" \":\"\n                                 #( language-tag [ \";\" \"ql\" \"=\" qvalue ] )\n\n   The language-tag is described in Section D.4. Each language may be \n   given an associated quality value which represents an estimate of \n   the user's comprehension of that language. The quality value \n   defaults to \"ql=1\" (100% comprehension) for listed languages. This \n   value may be used in the server's content negotiation algorithm \n   (Section D.3). For example,\n\n       Accept-Language: da, en-gb;ql=0.8, de;ql=0.55\n\n   would mean: \"I prefer Danish, but will accept British English (with \n   80% comprehension) or German (with a 55% comprehension).\"\n\n   If the server cannot fulfill the request with one or more of the \n   languages given, or if the languages only represent a subset of a \n   multi-linguistic Entity-Body, it is acceptable to serve the request \n   in an unspecified language. This is equivalent to asssigning a \n   quality value of \"ql=0.001\" to any unlisted language.\n\n   If no Accept-Language header is present in the request, the server \n   should assume that all languages are equally acceptable.\n\n       Note: As intelligibility is highly dependent on the \n       individual user, it is recommended that client applications \n       make the choice of linguistic preference available to the \n       user. If the choice is not made available, then the Accept-\n       Language header field must not be given in the request.\n\nD.2.5  Content-Language\n\n   The Content-Language field describes the natural language(s) of the \n   intended audience for the enclosed entity. Note that this may not \n   be equivalent to all the languages used within the entity.\n\n       Content-Language = \"Content-Language\" \":\" #language-tag\n\n   Language tags are defined in Section D.4. The primary purpose of \n   Content-Language is to allow a selective consumer to identify and \n   differentiate resources according to the consumer's own preferred \n   language. Thus, if the body content is intended only for a Danish-\n   literate audience, the appropriate field is\n\n       Content-Language: dk\n\n   If no Content-Language is specified, the default is that the \n   content is intended for all language audiences. This may mean that \n   the sender does not consider it to be specific to any natural \n   language, or that the sender does not know for which language it is \n   intended.\n\n   Multiple languages may be listed for content that is intended for \n   multiple audiences. For example, a rendition of the \"Treaty of \n   Waitangi,\" presented simultaneously in the original Maori and \n   English versions, would call for\n\n       Content-Language: mi, en\n\n   However, just because multiple languages are present within an \n   entity does not mean that it is intended for multiple linguistic \n   audiences. An example would be a beginner's language primer, such \n   as \"A First Lesson in Latin,\" which is clearly intended to be used \n   by an English-literate audience. In this case, the Content-Language \n   should only include \"en\".\n\n   Content-Language may be applied to any media type -- it should not \n   be limited to textual documents.\n\nD.2.6  Link\n\n   The Link header provides a means for describing a relationship \n   between the entity and some other resource. An entity may include \n   multiple Link values. Links at the metainformation level typically \n   indicate relationships like hierarchical structure and navigation \n   paths. The Link field is semantically equivalent to the <LINK> \n   element in HTML [4].\n\n       Link           = \"Link\" \":\" #(\"<\" URI \">\"\n                        [ \";\" \"rel\" \"=\" relationship ]\n                        [ \";\" \"rev\" \"=\" relationship ]\n                        [ \";\" \"title\" \"=\" quoted-string ] )\n\n       relationship   = sgml-name\n                      | ( <\"> sgml-name *( SP sgml-name) <\"> )\n\n       sgml-name      = ALPHA *( ALPHA | DIGIT | \".\" | \"-\" )\n\n   Relation values are not case-sensitive and may be extended within \n   the constraints of the sgml-name syntax. There are no predefined \n   link relationship values for HTTP/1.0. The title parameter may be \n   used to label the destination of a link such that it can be used as \n   identification within a human-readable menu. Examples of usage \n   include:\n\n       Link: <http://www.cern.ch/TheBook/chapter2>; rel=\"Previous\"\n\n       Link: <mailto:timbl@w3.org>; rev=\"Made\"; title=\"Tim Berners-Lee\"\n\n   The first example indicates that the entity is previous to chapter2 \n   in a logical navigation path. The second indicates that the person \n   responsible for making the resource available is identified by the \n   given e-mail address.\n\nD.2.7  Retry-After\n\n   The Retry-After response header field can be used with a 503 \n   (service unavailable) response to indicate how long the service is \n   expected to be unavailable to the requesting client. The value of \n   this field can be either an HTTP-date or an integer number of \n   seconds (in decimal) after the time of the response.\n\n       Retry-After    = \"Retry-After\" \":\" ( HTTP-date | delta-seconds )\n\n   Two examples of its use are\n\n       Retry-After: Wed, 14 Dec 1994 18:22:54 GMT\n\n       Retry-After: 120\n\n   In the latter example, the delay is 2 minutes.\n\nD.2.8  Title\n\n   The Title header field indicates the title of the entity \n\n       Title          = \"Title\" \":\" *text\n\n   An example of the field is\n\n       Title: Hypertext Transfer Protocol -- HTTP/1.0\n\n   This field is isomorphic with the <TITLE> element in HTML [4].\n\nD.2.9  URI\n\n   The URI-header field may contain some or all of the Uniform \n   Resource Identifiers (Section 3.2) by which the Request-URI \n   resource can be identified. There is no guarantee that the resource \n   can be accessed using the URI(s) specified. \n\n       URI-header     = \"URI\" \":\" #( \"<\" ( absoluteURI | relativeURI ) \">\"\n                        [ \";\" vary ] *( \";\" characteristic) )\n\n       vary           = \"vary\" \"=\"\n                        ( vary-dimension | ( <\"> 1#vary-dimension <\"> ) )\n\n       vary-dimension = \"type\" | \"charset\" | \"language\" | \"encoding\"\n                      | \"user-agent\" | \"version\" | token\n\n       characteristic = ( \"type={\" media-type \"}\" )\n                      | ( \"language={\" 1#language-tag \"}\" )\n                      | ( \"encoding={\" 1#encoding-mechanism \"}\" )\n                      | ( \"length=\" 1*DIGIT )\n                      | ( \"qs=\" qvalue )\n\n   Any URI specified in this field can be either absolute or relative \n   to the Request-URI.\n\n   If the Location header field is present in a 2xx response, its \n   value defines an implicit URI header with the characteristic \n   parameters defined by the associated Content-* header fields.\n\n   The URI-header may be used by a client performing a POST request to \n   suggest a URI for the new entity. Whether or not the suggested URI \n   is used is entirely up to the server to decide. In any case, the \n   server's response must include the actual URI(s) of the new \n   resource if one is successfully created (status 201).\n\n   If a URI refers to a set of variants, then the dimensions of that \n   variance must be given with a vary parameter. One example is:\n\n       URI: <http://info.cern.ch/hypertext/WWW/TheProject.multi>;\n            vary=\"type,language\"\n\n   which indicates that the URI covers a group of entities that vary \n   in media type and natural language. A request for that URI will \n   result in a response that depends upon the client's request headers \n   for Accept and Accept-Language. Similar dimensions exist for the \n   Accept-Encoding, Accept-Charset, and User-Agent header fields, as \n   demonstrated in the following example.\n\n       URI: <TheProject.ps>; vary=\"encoding,version\";\n            type={application/postscript},\n            <TheProject.html>; vary=\"user-agent,charset,version\";\n            type={text/html},\n            <TheProject.html3;v=25>; type={text/html; level=3}; qs=0.9\n\n   User agents may use this information to notify the user of \n   additional formats.\n\n   The vary parameter has an important effect on cache management, \n   particularly for caching intermediaries which service a diverse set \n   of user agents. Since the response to one user agent may differ \n   from the response to a second user agent if the two agents have \n   differing request profiles, a caching intermediary must keep track \n   of the content metainformation for resources with varying \n   dimensions. Thus, the vary parameter tells the intermediary what \n   entity headers must be part of the key for caching that URI. When \n   the caching proxy gets a request for that URI, it must forward the \n   request toward the origin server if the request profile includes a \n   variant dimension that has not already been cached.\n\n   If the origin server provides the characteristics of each \n   identified resource as part of the URI header, then the recipient \n   may improve its cached response behavior by attempting to duplicate \n   the content negotiation that would be provided by the server. This \n   is not required by the protocol, but may improve the accuracy or \n   timeliness of responses to the end-user.\n\nD.3  Content Negotiation\n\n   Content negotiation is an optional feature of the HTTP protocol. It \n   is designed to allow for selection of a preferred content \n   representation, within a single request-response round-trip, and \n   without intervention from the user. However, this may not always be \n   desirable for the user and is sometimes unnecessary for the content \n   provider. Implementors are encouraged to provide mechanisms whereby \n   the amount of preemptive content negotiation, and the parameters of \n   that negotiation, are configurable by the user and server \n   maintainer.\n\n   The first step in the negotiation algorithm is for the server to \n   determine whether or not there are any content variants for the \n   requested resource. Content variants may be in the form of multiple \n   preexisting entities or a set of dynamic conversion filters. These \n   variants make up the set of entities which may be sent in response \n   to a request for the given Request-URI. In most cases, there will \n   only be one available form of the resource, and thus a single \n   \"variant\".\n\n   For each variant form of the resource, the server identifies a set \n   of quality values (Section D.5) which act as weights for measuring \n   the desirability of that resource as a response to the current \n   request. The calculated weights are all real numbers in the range\n   0 through 1, where 0 is the minimum and 1 the maximum value. The \n   maximum acceptable bytes for each media range and the size of the \n   resource variant are also factors in the equation.\n\n   The following parameters are included in the calculation:\n\n      qs      Source quality is measured by the content provider as \n              representing the amount of degradation from the original \n              source. For example, a picture originally in JPEG form \n              would have a lower qs when translated to the XBM format, \n              and much lower qs when translated to an ASCII-art \n              representation. Note, however, that this is a function of \n              the source -- an original piece of ASCII-art may degrade in \n              quality if it is captured in JPEG form. The qs value should \n              be assigned to each variant by the content provider; if no \n              qs value has been assigned, the default is generally \n              \"qs=1\". A server may define its own default qs value based \n              on the resource characteristics, but only if individual \n              resources can override those defaults.\n\n      qe      Encoding quality is measured by comparing the variant's \n              applied encoding-mechanisms (Section 3.5) to those listed \n              in the request message's Accept-Encoding field. If the \n              variant has no assigned Content-Encoding, or if no Accept-\n              Encoding field is present, the value assigned is \"qe=1\". If \n              all of the variant's content encodings are listed in the \n              Accept-Encoding field, then the value assigned is \"qe=1\". \n              If any of the variant's content encodings are not listed in \n              the provided Accept-Encoding field, then the value assigned \n              is \"qe=0.001\".\n\n      qc      Charset quality is measured by comparing the variant media-\n              type's charset parameter value (if any) to those character \n              set encodings (Section 3.4) listed in the request message's \n              Accept-Charset field. If the variant's media-type has no \n              charset parameter, or the variant's charset is US-ASCII or \n              ISO-8859-1, or if no Accept-Charset field is present, then \n              the value assigned is \"qc=1\". If the variant's charset is \n              listed in the Accept-Charset field, then the value assigned \n              is \"qc=1\". Otherwise, if the variant's charset is not \n              listed in the provided Accept-Encoding field, then the \n              value assigned is \"qc=0.001\".\n\n      ql      Language quality is measured by comparing the variant's \n              assigned language tag(s) (Section D.4) to those listed in \n              the request message's Accept-Language field. If no variant \n              has an assigned Content-Language, or if no Accept-Language \n              field is present, the value assigned is \"ql=1\". If at least \n              one variant has an assigned content language, but the one \n              currently under consideration does not, then it should be \n              assigned the value \"ql=0.5\". If any of the variant's \n              content languages are listed in the Accept-Language field, \n              then the value assigned is the maximum of the \"ql\" \n              parameter values for those language tags (Section D.2.4); if \n              there was no exact match and at least one of the Accept-\n              Language field values is a complete subtag prefix of the \n              content language tag(s), then the \"ql\" parameter value of \n              the largest matching prefix is used. If none of the \n              variant's content language tags or tag prefixes are listed \n              in the provided Accept-Language field, then the value \n              assigned is \"ql=0.001\".\n\n      q       Media type quality is measured by comparing the variant's \n              assigned media type (Section 3.6) to those listed in the \n              request message's Accept field. If no Accept field is \n              given, then the value assigned is \"q=1\". If at least one \n              listed media range (Section D.2.1) matches the variant's \n              media type, then the \"q\" parameter value assigned to the \n              most specific of those matched is used (e.g., \n              \"text/html;version=3.0\" is more specific than \"text/html\", \n              which is more specific than \"text/*\", which in turn is more \n              specific than \"*/*\"). If no media range in the provided \n              Accept field matches the variant's media type, then the \n              value assigned is \"q=0\".\n\n      mxb     The maximum number of bytes in an Entity-Body that the \n              client will accept is also obtained from the matching of \n              the variant's assigned media type to those listed in the \n              request message's Accept field. If no Accept field is \n              given, or if no media range in the provided Accept field \n              matches the variant's media type, then the value assigned \n              is \"mxb=undefined\"  (i.e., infinity). Otherwise, the value \n              used is that given to the \"mxb\" parameter in the media \n              range chosen above for the q value.\n\n      bs      The actual number of bytes in the Entity-Body for the \n              variant when it is included in a response message. This \n              should equal the value of Content-Length.\n\n   The mapping function is defined as:\n\n       Q(qs,qe,qc,ql,    { if mxb=undefined, then (qs*qe*qc*ql*q) }\n             q,mxb,bs) = { if mxb >= bs,     then (qs*qe*qc*ql*q) }\n                         { if mxb <  bs,     then 0               }\n\n   The variants with a maximal value for the Q function represent the \n   preferred representation(s) of the entity; those with a Q values \n   less than the maximal value are therefore excluded from further \n   consideration. If multiple representations exist that only vary by \n   Content-Encoding, then the smallest representation (lowest bs) is \n   preferred.\n\n   If no variants remain with a value of Q greater than zero (0), the \n   server should respond with a 406 (none acceptable) response \n   message. If multiple variants remain with an equally high Q value, \n   the server may either choose one from those available and respond \n   with 200 (ok) or respond with 300 (multiple choices) and include an \n   entity describing the choices. In the latter case, the entity \n   should either be of type \"text/html', such that the user can choose \n   from among the choices by following an exact link, or of some type \n   that would allow the user agent to perform the selection \n   automatically.\n\n   The 300 (multiple choices) response can be given even if the server \n   does not perform any winnowing of the representation choices via \n   the content negotiation algorithm described above. Furthermore, it \n   may include choices that were not considered as part of the \n   negotiation algorithm and resources that may be located at other \n   servers.\n\n   Servers that make use of content negotiated resources are strongly \n   encouraged to include URI response headers which accurately \n   describe the available variants and include the relevant parameters \n   necessary for the client (user agent or proxy) to evaluate those \n   variants.\n\n   The algorithm presented above assumes that the user agent has \n   correctly implemented the protocol and is accurately communicating \n   its intentions in the form of Accept-related header fields. The \n   server may alter its response if it knows that the particular \n   version of user agent software making the request has incorrectly \n   or inadequately implemented these fields.\n\nD.4  Language Tags\n\n   A language tag identifies a natural language spoken, written, or \n   otherwise conveyed by human beings for communication of information \n   to other human beings. Computer languages are explicitly excluded. \n   The HTTP/1.0 protocol uses language tags within the\n   Accept-Language, Content-Language, and URI-header fields.\n\n   The syntax and registry of HTTP language tags is the same as that \n   defined by RFC 1766 [1]. In summary, a language tag is composed of \n   1 or more parts: A primary language tag and a possibly empty series \n   of subtags:\n\n        language-tag  = primary-tag *( \"-\" subtag )\n\n        primary-tag   = 1*8ALPHA\n        subtag        = 1*8ALPHA\n\n   Whitespace is not allowed within the tag and all tags are not case-\n   sensitive. The namespace of language tags is administered by the \n   IANA. Example tags include:\n\n       en, en-US, en-cockney, i-cherokee, x-pig-latin\n\n   where any two-letter primary-tag is an ISO 639 language \n   abbreviation and any two-letter initial subtag is an ISO 3166 \n   country code.\n\n   In the context of the Accept-Language header (Section D.2.4), a \n   language tag is not to be interpreted as a single token, as per \n   RFC 1766, but as a hierarchy. A server should consider that it has a \n   match when a language tag received in an Accept-Language header \n   matches the initial portion of the language tag of a document. An \n   exact match should be preferred. This interpretation allows a \n   browser to send, for example:\n\n       Accept-Language: en-US, en; ql=0.95\n\n   when the intent is to access, in order of preference, documents in \n   US-English (\"en-US\"), 'plain' or 'international' English (\"en\"), \n   and any other variant of English (initial \"en-\").\n\n       Note: Using the language tag as a hierarchy does not imply \n       that all languages with a common prefix will be understood \n       by those fluent in one or more of those languages; it simply \n       allows the user to request this commonality when it is true \n       for that user.\n\nD.5  Quality Values\n\n   HTTP content negotiation (Section D.3) uses short \"floating point\" \n   numbers to indicate the relative importance (\"weight\") of various \n   negotiable parameters. The calculated weights are normalized to a \n   real number in the range 0 through 1, where 0 is the minimum and 1 \n   the maximum value. In order to discourage misuse of this feature, \n   HTTP/1.0 applications must not generate more than three digits \n   after the decimal point. User configuration of these values should \n   also be limited in this fashion.\n\n       qvalue         = ( \"0\" [ \".\" 0*3DIGIT ] )\n                      | ( \".\" 0*3DIGIT )\n                      | ( \"1\" [ \".\" 0*3(\"0\") ] )\n\n   \"Quality values\" is a slight misnomer, since these values actually \n   measure relative degradation in perceived quality. Thus, a value of \n   \"0.8\" represents a 20% degradation from the optimum rather than a \n   statement of 80% quality.\n\n\n\n\n--============_-1392122978==_============--\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> I'm becoming more and more convenced that single-exchange PUT is\n> unworkable. That is, PUT might have to happen in two steps:\n> \n> 1. client: wanna PUT this file of this size of this media type \n>      in this location with these PERMS with\n>    this validator \n> 2. server: OK, send file (or permission denied)\n> 3. client: OK, here's the file.\n> 4. server: OK, got the file\n\nOne of the possible solutions we (Henrik and I) discussed this summer was\na two-request form of PUT (using PUTQ as the probe method).  That solution\nwas discarded because the server's requirements/status may change between\nthe two requests and thus the second request must still be protected using\nthe two-phased approach (or something similar).  Therefore, there was no\nimmediate advantage to using two requests other than for obtaining the\naccess requirements -- the OPTIONS method does that.\n\n> Step 1 might offer more than one media type with step 2 accepting a\n> particular one or a subset. Step 1 might say that it doesn't know the\n> file size, for example.\n\nIf we allow such negotiation, we must also provide a mechanism for\nexpressing it.  None is available currently and it's a bit of a rats\nnest to come up with one that can encompass the server's options.\nOn the whole, I think we are better-off having the client ask\n\"can I do this\" for each combination of request until the server\nsays \"go ahead and do that\".\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": "I thought we were only going to include a brief note about these\nfeatures which are implemented on *some* systems, and not bother\nto specify them fully (i.e., list the additional header field names\nand their purpose, and explain why each one is not in the main spec). \nThese features were not included because there do not exist sufficient\ninteroperable implementations which use them, not because of any lack\nof consensus within the WG.  Any specification of them is therefore\nincorrect, because the specification does not reflect current practice.\n\nFor example, current practice and \"best practice\" differ for Accept\nand Accept-Language, since they are not implemented as specified, and\nthere is no point in including the actual implementation in the\nspecification because we already know that it doesn't work and\nwas never fully-implemented, and will probably change for HTTP/1.1.\nThus, I think we should just explain why it is not specified rather than\ntry to turn an appendix into an auxiliary specification.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, Brian Behlendorf wrote:\n\n> On Fri, 16 Dec 1994, John Franks wrote:\n> > users jump to a new document without waiting for the current one to\n> > completely download.  But all that is the price we pay for quality service\n> > (from the user's point of view).\n> \n> The user isn't the one paying that price, though.  \n\nTrue, but if the user is discouraged by the responsiveness they perceive\nthen they won't come back or won't be back as often.\n\nAdditionally, from a providers perspective, I presume that some form\nof 'advertising' is the motivation.  The more information which can be\npresented to a user per unit of time the user is willing to spend, the\ngreater the chance that the user will react to some piece of information\nas desired by the provider.\n\nDave\n\n\n\n"
        },
        {
            "subject": "RE: Moving HTTP 1.0 to informationa",
            "content": "D.1.1 PUT\n      Note: The model of sending an entire PUT request within a\n       single message, without first checking if the server is\n       willing to accept that data, will break if the server is\n       unwilling to accept the request or desires some form of\n       authentication beforehand. Worse, the client won't be\n       notified of the reason for error if a TCP reset is received\n       prior to reading the response buffer (see note in\n       Section 9.4). It should therefore be recognized that\n       HTTP/1.0 PUT and large POST requests will only work reliably\n       if the client's intentions and server's desires are\n       negotiated prior to the request.\n\nWell said.  I would fully expect to have my users beating down my door if \nthey always had to upload their files twice because of authentication \nfailures/requests!  We are talking file upload here, not \"upload lotto\".\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": "At 12:00 AM 12/27/95, Roy T. Fielding wrote:\n>I thought we were only going to include a brief note about these\n>features which are implemented on *some* systems, and not bother\n>to specify them fully (i.e., list the additional header field names\n>and their purpose, and explain why each one is not in the main spec).\n\nAh. I hadn't understood that was the desire, but I certainly agree with it.\nI'll do another round quite soon with much less in it.\n\nThere is also the question of downgrading \"musts\", since this is an\ninformational document.\n\nAnd, on a separate note, I'll be taking the \"Allow\" header out of the 1.0\ndoc since only one known server appears to emit it. Worse, that one server\ndoes it incorrectly (it always emits GET HEAD POST even for URIs where POST\nis not allowed...).\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": "> And, on a separate note, I'll be taking the \"Allow\" header out of the 1.0\n> doc since only one known server appears to emit it. Worse, that one server\n> does it incorrectly (it always emits GET HEAD POST even for URIs where POST\n> is not allowed...).\n\nThat means there's at least three, because I know of two (AWS and\nAServe) that implement Allow and don't include POST where it is not\nallowed.\n\nI don't know of any CLIENTS that pay attention to the Allow header,\nthough, which would also imply that it should come out of the draft.\n\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": ">> And, on a separate note, I'll be taking the \"Allow\" header out of the 1.0\n>> doc since only one known server appears to emit it. Worse, that one server\n>> does it incorrectly (it always emits GET HEAD POST even for URIs where POST\n>> is not allowed...).\n>\n>That means there's at least three, because I know of two (AWS and\n>AServe) that implement Allow and don't include POST where it is not\n>allowed.\n\nSpyglass also reports that they use the header, so I'll leave it in. This\nwas not true in September when I ran the survey. I'll be running another in\nearly January, I hope.\n\n--Paul Hoffman\n--Internet Mail Consortium\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> I'm becoming more and more convenced that single-exchange PUT is\n> unworkable. That is, PUT might have to happen in two steps:\n\nWhat are the drawbacks to single-exchange PUT if clients are full-duplex? \n\nThe current NaviPress/NaviServer interaction goes like this:\n\nPress 1.1Server 1.1\n-----------\ntries to save file\n-> PUT headers ->\ndoesn't have auth\n<- 401 headers <-\nclient hasn't noticed\n-> PUT buffer of body ->\ntosses data into bitbucket\n-> PUT buffer of body ->\n:\"\n:\n-> PUT buffer of body ->\n\"\nclient finally\nrealizes something's wrong,\nwhile reading response\n\nNow, this isn't too bad at LAN speeds, but it sure is a pain over dialup.\n\nIf the server only replies early for failure, then the new logic wouldn't\ntoo bad - put a check for the availability of the response into the\nbody writing loop:\n\nPress 2.0Server 1.1\n-----------\ntries to save file\n-> PUT headers ->\ndoesn't have auth\n<- 401 headers <-\nclient hasn't noticed\n-> PUT 0 - 1 buffers worth of body ->\nsocket is readable!\nabort, read response...\n\nThis gets slightly uglier if the server is allowed to 200 early, in that\nwe would wind up running the request and response logic in parallel, and\nabort the request on a failure response.\n\nAs you can see, the NaviServers already support a full-duplex single\nexchange (they need to do the bitbucketing anyway, for other reasons),\nand I was going to get the logic into NaviPress Real Soon Now.\n(Sorry, Roy, a bunch of OEM work came up in the last couple of weeks...)\n\nIf y'all have your hearts set on dual exchange PUT, or can tell me\nwhy this single-exchange is too simplistic, I'll abandon efforts on it.\n\n-Dave\n\nI'll probably do the only-failures-early model first, because my experience\nhas been that an FTP server is the only thing that most of our users ever\nget a chance to PUT to, so the NaviServer is at the limits of my world of\ninteroperability.  If you've got a PUT'able server, send me mail, and I'll\nwork on keeping us interoperable.\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": ">Well said.  I would fully expect to have my users beating down my door if \n>they always had to upload their files twice because of authentication \n>failures/requests!  We are talking file upload here, not \"upload lotto\".\n\nAs one can tell by my previous message, we are concerned about having\nto upload files twice, but there are many cases where it isn't as bad\nas it sounds:\n\n- on a LAN, twice is often only a second or two longer than once\n\n- even over dialup, because of the way the URL permission\n  scheme works, we don't have that many twices:\n\nif someone saves something they edited, they\nhad to GET it to begin with, so if they used\na PUT-authorized identification, the PUTs go\nthrough on the first try.\n\nif someone does a save as, the embedded assets\ntend to fall under the same path-prefix and\nhave the same access control as the HTML file,\nso even if the HTML file (which is short) has\nto be retransmitted, the images (which may be\nlarge) tend to go through on the first try.\n\nSo, in regular use, there aren't that many retransmissions (certainly\nnot \"always twice\").  Where we get bitten by permissions are where we\nfake up atomic transactions by saving an entire URL tree in one HTTP\ntransaction -- if one does a \"save as\", one may wind up with many MB\nhitting a particular URLspace for the first time.\n\n-Dave\n\n\n\n"
        },
        {
            "subject": "Potential HTTP Security Ris",
            "content": "   I apologize if this has been discussed--I've gotten behind \nin my reading as I head down the final stretch to by book deadline. \n\n   I just noticed in the WWW Security FAQ a notation that some \nservers, including NCSA, allow the file \".htaccess\" to be retrieved. \nI tried it with my Apache 1.0 server and I got the file. \n\n   Perhaps the following modification of the proposed section 12.5 \nwould help: (change marks in the left column are relative to Paul \nHoffman's message that began this thread)\n\n | 12.5  Attacks Based On URL Contents\n\n   Implementations of the HTTP servers should be careful to restrict the\n   documents returned by HTTP requests to be only those that were intended\n   by the administrators. If an HTTP server translates HTTP URIs directly\n   into file system calls, the server must take special care not to serve\n   files outside the desired directory tree. For example, Unix, Microsoft\n   Windows, and other operating systems use \"..\" as a path component to\n   indicate a directory level above the current one. A URL with such\n   constructs can be constructed to potentially allow access to files\n   outside the desired directory structure, and should thus be disallowed.\n\n + Many servers implement a system of access-control files within the \n + document directory tree that may contain sensitive security- or \n + implementation-related information. A URL which references a filename \n + which is used for access-control files, or a filename pattern \n + commonly used for system files (e.g. \"/.\" for Unix systems, or \".PWL\" \n + for Microsoft Windows systems), should be disallowed. A server should \n + make a configuration option available to the system administrator to \n + ensure that this protection is made sufficiently flexible for \n + site-specific security considerations. \n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * \"To enjoy life, take big bites. Moderation is for monks.\" \n                                                       --Lazarus Long\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "A few weeks ago, I wrote:\n    Roy has taken a pessimistic approach to the byte-deluge problem.\n    In particular, every use of every method with a two-phase requirement\n    must pay either the arbitrary timeout period OR at least one\n    round-trip time.  In other words, by taking this approach, we\n    build in \"extra\" delay to every POST (etc.) invocation for millions\n    of users for many years.  Hmm.\nTo which Roy responded:\n    Yes, this is an excellent analysis of the tradeoffs.  However, it\n    fails to consider that all of the two-phase methods are not\n    speed-dependent:  it is far more important to get it right the\n    first time (before data is sent across the wire, if possible) then\n    it is to save one or two round-trips.  Given that, it is okay to be\n    pessimistic and pay for that round-trip every time.\n\nI would like to dig a little deeper into this tradeoff before conceding\nthat pessimism is preferrable to optimism.\n\nI should start with a quick summary of my proposed solution, which\nI will call the \"optimistic two-phase\" scheme:\n\nInitially, the client tries to use a one-phase PUT.  If the\nserver wants to reject the PUT, it sends an error status and\nsimply closes the TCP connection, forcing the client to\nabort its transmission.  If the client sees the error status,\nfine.  If it simply sees the connection close prematurely,\nthen it may retry the PUT, but this time must use a two-phase\nprotocol (send the headers, wait for a \"Continue\", then\nsend the data).\n\nNote that a very similar problem arises with persistent connections.\nIn the current one-request-per-TCP-connection model, we assume that\nthe server does not close the TCP connection before sending a full\nresponse.  In the persistent-connection world, however, once the server\nhas responded to at least one request, it is free to close the connection\nat any time.  This might happen after the client has transmitted a\nsubsequent request (perhaps before the server receives it, though) and\nso to the client, this will look like a premature close by the server.\nSo in order to make persistent connections work, the client has to be\nable to retry after a premature close (although this is not to say that\na client must implement persistent connections).\n\nOne possible problem with this approach is that the client might\nretry ad infinitum.  We can ban this in HTTP 1.1 (just as easily\nas we can require any two-phase behavior), but we may have a problem\nwith those HTTP 1.0 clients that already support persistent connections.\nI'll assume for the purposes of this analysis that this problem does\nnot arise, but I may have to concede this point.\n\nOn with the analysis:\nWe want to find the costs of these schemes, in addition to the basic\ncost of the HTTP interaction (which might be less than 1 round-trip\ntime [RTT], if persistent connections are used, or several RTTs in\nother cases).\n\nThe additional cost of a two-phase interaction is one RTT, since the\nclient must pause for at least one RTT before it sees the \"Continue\"\nstatus.\n\nThe cost of a *failed* one-phase interaction (i.e., client tries\na PUT, server says \"no way\" and closes the TCP connection) is a\nlittle harder to measure.  The entire first (aborted) connection\nis wasted, so there are at least two or possibly three RTTs involved\nin that.  I'll assume 3 RTTs, to be concrete and conservative.\nThen there is one more RTT to go through the two-phase interaction,\nwhich is guaranteed to result in a rejection (but the client doesn't\nyet know that)\n\nSo we have these cases:\n\nPessimistic (pure) two-phase, if server accepts: cost = 1 RTT\nPessimistic (pure) two-phase, if server rejects: cost = 1 RTT\n\nOptimistic (my) two-phase, if server accepts: cost = 0\nOptimistic (my) two-phase, if server rejects: cost = 4 RTTs\n\nNow define\nP(a) = \"probability\" that the server accepts the PUT\nI put \"probability\" in quotes because this isn't really a\nnon-deterministic process, but from the point of view of a client,\nit might as well be one.\n\nSo the overall \"expected values\" of the costs of the two schemes are:\n\nPessimistic two-phase: (P(a) * 1 RTT) + ((1 - P(a)) * 1 RTT)\nwhich simplifies to 1 RTT\nOptimistic two-phase: (P(a) * 0 RTT) + ((1 - P(a)) * 4 RTT)\nwhich simplifies to (1 - P(a)) * 4 RTT\n\nSo \"optimistic\" costs less than \"pessimistic\" if\n(1 - P(a)) * 4 RTT < 1 RTT\nwhich simplifies to:\nP(a) > 0.75\n\nIn other words, if servers will accept PUTs more often than 75% of\nthe time, then under this analysis (which, I admit, might be buggy)\nit pays off \"on the average\" to use an optimistic two-phase approach.\n\nThe numbers might shift slightly if one counts packets or bytes\ninstead of RTTs, but since the speed of light is the only true\nconstant, I prefer to think in terms of RTTs.\n\nOn the other hand, my analysis assumes that all servers speak HTTP 1.1\n(and thus respond immediately with a Continue or an error code.\nWhenever a pre-1.1 server is involved, the pessimistic two-phase scheme\nadds an additional 5 seconds, which is probably worth several RTTs in\nmost instances.  And this is \"wasted\" time in this case, because a\npre-1.1 server won't respond with an error at this point.  But I'll\nassume for the sake of the argument that almost all servers will adopt\nHTTP 1.1.\n\nWould anyone like to argue that more than 25% of PUTs (and similar\nHTTP methods) are going to be rejected?  To me, this seems unlikely\n(most users will learn not to ask for things that the servers don't\nallow), but I don't have enough experience to know.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "On Wed, 27 Dec 1995, Jeffrey Mogul wrote:\n\n\n[...]\n\n> \n> Would anyone like to argue that more than 25% of PUTs (and similar\n> HTTP methods) are going to be rejected?  To me, this seems unlikely\n> (most users will learn not to ask for things that the servers don't\n> allow), but I don't have enough experience to know.\n\nI would add to the assumptions that support Jeff's <25% reject\nconclusion the assumption that a PUT doesn't happen (in general)\nout of the 'blue'.  Rather it happens in response to a previous\nrequest which probably obtained the form which resulted in the PUT.\n\nIn general, it should be possible for the more mundane aspects of\nauthorization to be handled in conjunction with the previous\nrequest. Thus, when the user is given a 'FORM' which will be 'PUT',\nthey have been pre-verified such that it is likely that the subsequent\nPUT/POST will be acceptable.  Now the probability of rejection is \nmuch smaller as we are dealing with the probability that something\nhas changed such that the PUT would no longer be acceptable or some\nspecific characteristic of the PUT is not acceptable. We can't help\nthe changed status stituation but I would postulate that 'customers'\nof that specific server will let their 'feet' do the talking if it\nis a frequent problem.  I would also postulate that in this changed\nstatus case, there will often be nothing the user/user agent can\ndo to fix the situation.\n\nThat leaves the question of pre-approval. If we presume that most\nPUTs will be preceeded by a request that stages the PUT request, then\nperhaps we could provide for 'pre-negotiation' via headers on the\nstaging response which deal with common reasons for ultimate\nrejection.  Size and content type, language, etc. would seem well\nunderstood.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: MIME and binary transpor",
            "content": "On Fri, 16 Dec 1994, Jeffrey Mogul wrote:\n\n> A little experiment should help shed some light on this.  On a reasonably\n> fast machine (a DEC3000/600), I did (after first arranging for /vmunix\n> to be in the filesystem cache):\n>     % time wc /vmunix \n>  15102     97927   6351808 /vmunix\n>     0.93u 0.14s 0:01 94% 0+1k 0+0io 15pf+0w\n>     a% time ngrep masinter@parc.xerox.com /vmunix \n>     0.27u 0.19s 0:00 74% 0+1k 4+3io 47pf+0w\n>     % \n> ngrep uses a modified Boyer-Moore algorithm.  Larry is right;\n\nI'm sorry but I'm quite skeptical about your experimental design and\nit's applicability to the HTTP context as well.  For starters finding\nsplit points in an inbound byte stream is a different problem than\npresented by a file full of bytes all present and waiting for processing.\n\nSecondly, I am suspicious that there is a marked difference in the quality\nof implementation of wc vs. ngrep interms of program organization.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: interaction of caching with content negotiation, authentication, state, et",
            "content": "    If it is possible for a given object to be returned in response to\n    requests on different URIs, as it would be with content negotiation\n    (among other possibilities), then either that object must be\n    pre-expired, so that it will be validated on every request, or it will\n    be possible for multiple versions of the object to be present in, and\n    servable from, a cache if the object changes at the origin server\n    prior to its stated expiration date.  The consequence of this is that\n    users might see previous versions of an object, even though they are\n    only using a single cache. \n    \nThere's a false dichotomy here.  It should be possible to separate the\nproblem of validating a cached response (is this the response that the\nserver would give for this variant) from the problem of determining\ncompatability between a specific response and a set of requests that\ndiffer in their Accept headers (is this the variant that the server\nwould return).\n\nI would agree that one could solve both problems by presenting every\nrequest to the server, but I would not agree that this is the only\npossible solution.  For example, if the origin server's response\ncontains sufficient information about the existence of other variants,\nthe cache could (in principle) decide that a non-expired (\"fresh\")\ncached response obtained with one set of request headers is in fact\nappropriate to return for a different set of request headers.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Probably should take this to the persist list, or the http-ng mailing list\n(now at http-ng@cuckoo.hpl.hp.com)\n\nThe NG approach to HTTP/1.X encapsuation and persistent connections makes \nthis tradeoff easier, since cancellation doesn't require the connection \nto be torn down.\n\nThe RTT costs with this approach using optimistic transmission are 0 RTT \nfor the successful case, and approximately 1 in the event of failure (the \ncosts can be greater than one if data is being  enqueued at the TCP layer \nfaster than the TCP transmission rate; the excess data is then drained. \n\nSimon\n\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1) ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": ">Would anyone like to argue that more than 25% of PUTs (and similar\n>HTTP methods) are going to be rejected?  To me, this seems unlikely\n>(most users will learn not to ask for things that the servers don't\n>allow), but I don't have enough experience to know.\n\nOnly two examples come to my mind, both of which I put in the\nnot impossible area:\n    Using HTTP for replication consistency\n    Recasting Usenet (and NNTP) into an HTTP modus operandi\n\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> We want to find the costs of these schemes, in addition to the basic\n> cost of the HTTP interaction (which might be less than 1 round-trip\n> time [RTT], if persistent connections are used, or several RTTs in\n> other cases).\n> \n> The additional cost of a two-phase interaction is one RTT, since the\n> client must pause for at least one RTT before it sees the \"Continue\"\n> status.\n> \n> The cost of a *failed* one-phase interaction (i.e., client tries\n> a PUT, server says \"no way\" and closes the TCP connection) is a\n> little harder to measure.  The entire first (aborted) connection\n> is wasted, so there are at least two or possibly three RTTs involved\n> in that.  I'll assume 3 RTTs, to be concrete and conservative.\n> Then there is one more RTT to go through the two-phase interaction,\n> which is guaranteed to result in a rejection (but the client doesn't\n> yet know that)\n\nThis is interesting analysis, but doesn't apply to what I said.\nRTTs are not sufficiently important for the two-phase methods for them\nto be the measure of what is \"best\".  What is important is:\n\n   1) reliability (which boils down to both sides eventually knowing\n      what happened)\n\n   2) not sending unwanted data over the wire (if possible).\n\nI agree that an optimistic approach is capable of handling (1).\nHowever, it is not capable of handling (2).\n\nIn most cases, this difference is trivial.  However, some security-related\nsystems consider the ability to refuse a vulnerable operation before it\noccurs to be a showstopper.  Also, some networks will require the user\nto pay by the amount of data sent, regardless of whether that data was\ninitially rejected by the server.\n\nGiven these cases exist, a pessimistic approach is \"best\".  This does not\nmean that the 5 second delay is the best solution -- it is just a way of\nforcing a real solution to be created.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> As you can see, the NaviServers already support a full-duplex single\n> exchange (they need to do the bitbucketing anyway, for other reasons),\n\nYep, the server will need to do the bitbucketing just to prevent a TCP\nreset (when communicating over TCP).  You may want to try closing only\nthe write-half of the server's connection after the error response\nand let the read buffer become full (which will lower the client's \nwrite window eventually) instead of bitbucketing.  Unfortunately, I have\nno idea if this is portable across all TCP implementations, since closing\nonly half a socket may be incorrectly implemented in some OS's.\n\n> and I was going to get the logic into NaviPress Real Soon Now.\n> (Sorry, Roy, a bunch of OEM work came up in the last couple of weeks...)\n\nNo problem -- everyone is busy right now.  I'm happy to see that you made\nit onto the list.\n\n> If y'all have your hearts set on dual exchange PUT, or can tell me\n> why this single-exchange is too simplistic, I'll abandon efforts on it.\n\nOn the contrary, I think you should find the best implementation for your\nneeds and present that to the WG.  Right now, we suffer from inadequate\nimplementation experience with PUT (and other editing requirements).\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Moving HTTP 1.0 to informationa",
            "content": "> There is also the question of downgrading \"musts\", since this is an\n> informational document.\n\nDon't worry about that -- almost all of the \"musts\" currently present\nin the 04 draft are implementation \"musts\" (i.e., it won't work unless\nthe application does it like this).  The only thing that would need taking\nout are things like \"standard\" or \"Internet requirement\", which I think\nI removed already (but may have missed one or two).\n\n> And, on a separate note, I'll be taking the \"Allow\" header out of the 1.0\n> doc since only one known server appears to emit it. Worse, that one server\n> does it incorrectly (it always emits GET HEAD POST even for URIs where POST\n> is not allowed...).\n\nIt is okay by me if this gets moved to the appendix.  Either way is okay,\nsince the Allow header is only informational anyway.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "At WWW4, someone in the hallway convinced me that I was wrong about\ntwo-phase send, in that:\n\na) single transaction PUT was a win for small transactions\nb) two-phase PUT might win for large transactions, but only because\n   the initial 'is this OK to PUT' 'yes, go ahead' was going to be \n   a small RTT compared to the entire PUT transaction.\n\nSo, we might leave the 'is this OK to PUT' as an optional part of the\nprotocol, and let clients use heuristics (how fast/reliable is the\nconnection to the server vs how big is the transaction) to decide\nwhether or not to use the optional part.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "On Wed, 27 Dec 1995, Roy T. Fielding wrote:\n\n> write window eventually) instead of bitbucketing.  Unfortunately, I have\n> no idea if this is portable across all TCP implementations, since closing\n> only half a socket may be incorrectly implemented in some OS's.\n\nNot implemented may be more like the case. I can't find any reference\nin WINSOCK for example to closing 1/2 a connection.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send  concerns ",
            "content": "Larry Masinter writes in <95Dec28.011521pst.2733@golden.parc.xerox.com>:\n>So, we might leave the 'is this OK to PUT' as an optional part of the\n>protocol, and let clients use heuristics (how fast/reliable is the\n>connection to the server vs how big is the transaction) to decide\n>whether or not to use the optional part.\n\nI would also argue for this because of the security concerns raised by Roy. \n It should, however, be an option because probably most systems do not need \nthis level of security.  Some do, though...\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "David W. Morris:\n>\n> If the negotiation group\n>can identify how to specify and what to negotiate then the join the \n>caching/proxy discussion to make sure we end up with a cohesive mechanism\n>which covers all issues.\n\nI agree, doing it this way is the best solution.  Of course, this\nimplies that the caching/proxy group stops discussing the caching of\nnegotiated responses for some time.\n\nWhile the content negotiation subgroup is working out the negotiation\nmechanism, the caching/proxy subgroup can work on the caching model\nfor non-negotiated responses.  There are still plenty of things to\ndiscuss even if negotiation is presumed absent.\n\nAccording to the current 1.1 draft (Section 12, second paragraph), a\nresponse is non-negotiated if it does not have an URI header listing\nthe available variants.  I doubt that the content negotiation subgroup\nwill propose a change of this rule.\n\n>Dave Morris\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "On Fri, 16 Dec 1994, Brian Behlendorf wrote:\n\n> > Do any of you all out there with gig and gig of log files have any data on \n> > what percentage of requests for HTML docs come from Netscape?\n> \n> For the last week, NetScape (any platform) accounted for 65% of hits to \n> our home page.\n\nI can't resist asking ... how much was this statistic distorted by restarts\nof the same page?  If each NetScape user ended up having 3 requests made\nbecause the first two transmissions were incomplete, ...\n\nDave Morris \n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Roy T. Fielding:\n>\n>This is interesting analysis, but doesn't apply to what I said.\n>RTTs are not sufficiently important for the two-phase methods for them\n>to be the measure of what is \"best\".  What is important is:\n>\n>   1) reliability (which boils down to both sides eventually knowing\n>      what happened)\n>\n>   2) not sending unwanted data over the wire (if possible).\n>\n>I agree that an optimistic approach is capable of handling (1).\n>However, it is not capable of handling (2).\n>\n>In most cases, this difference is trivial.  However, some security-related\n>systems consider the ability to refuse a vulnerable operation before it\n>occurs to be a showstopper.\n\nInteresting point.  However, the two-phase send that is now in the 1.1\ndraft cannot reliably do (2) either.  If the `vulnerable operation\nrefused' response takes longer than 5 seconds to make and transmit,\nthe client will proceed with the vulnerable operation anyway.  And a\ndetermined attacker can easily cause a 5 second delay by bombing the\nserver with ping packets (this is a tried and tested means of attack\nin the IRC world).\n\nSo if we want to cater for this kind of security-related systems, we\nhave to throw out the 5 second delay.  I would consider that a\nshowstopper any kind of smooth upgrading from 1.0 to 1.1.\n\nI already have my doubts about upgrading from 1.0 to 1.1 even with the\n5 second delay.\n\n>  Also, some networks will require the user\n>to pay by the amount of data sent, regardless of whether that data was\n>initially rejected by the server.\n\nThat may be true, but waiting for an extra RTT in each POST is not\nfree either.  With dial-up networks, you pay for connect time, so each\nRTT costs.\n\nRoy, I'm sorry, but if matters were put to the vote now, I would vote\nagainst 2-phase sends.  You have failed to convince me\n\n a) that the pessimistic approach is superior to the optimistic approach\n\n b) that we even need either approach at all.  Do we really need to\n    prevent wasteful uploads of unwanted data at the HTTP protocol\n    level?  Would this actually save any meaningful amount of\n    bandwidth?  Until we get far better caching of downloads, it would\n    not.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "More content negotiatio",
            "content": "Koen wrote a long time ago:\n>Perhaps even more importantly, having a safe way to disable the\n>client/proxy side `variant selection engine' would allow service\n>providers to use special purpose variant selection algorithms that\n>cannot be `run' on client-side `variant selection engines'.  One\n>example of such a special purpose algorithm is negotiation around\n>known bugs in user agents based on the user-agent header.\n\nI'm not a big fan of any scheme that purposely defeats the ability of a\nproxy to serve cached objects because of user-agent rendering differences.\nHowever, some people seem intent on doing this, so it has to be addressed.\nBut, I'm not convinced any HTTP headers or protocol changes can overcome or\nsignifigantly reduce the performance hit of the existance of opaque\nvariant-picking schemes.\n\nI'm also not a fan of any client-side variant-picking algorithm going into\nthe protocol.  What's the point?  Defining variant-picking algorithm for\nservers allows the variant choices to be reproduced by proxies, but what\ndoes defining the client-side variant-picking algorithm gain?\n\nIf some server has variant-picking schemes that cannot be represented in the\nscope of content negotiation, why dont they just send (on base URI requests)\nnon-cachable redirects to the appropriate variant URI?  A caching proxy can\nthen choose not to access the redirected-to variant based on that variant\nalready being present in the proxy's cache.  This behavior requires no flag\nform the proxy to indicate the presence of variants in the requestor proxy's\ncache, no directive from the server to the proxy to indicate that things\nbehave outside the scheme of the (content negotiation portion of the)\nprotocol, and no unnessary associations with the base URI to be saved in the\nproxy's database (like additional URI information or a list of user agent\nheaders or a list of any arbitrary header for that matter).  It requires no\nchanges to existing clients or proxies.  It requires no more requests than\nreactive negotiation.\n\nThe only downside is a 100-byte body that says \"This resource is located at\nhttp:/blahblahblah\" instead of Koen's proposed \"20x No Body\" scheme.  Not a\nbiggie.\n\nPS:  When I say base URI, I mean the negotiable resource, the \"/index\" as\nopposed to the variant \"/index.html\" or \"index.fr.html.zip\"\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": ">>>>> David W Morris writes:\n\n>> write window eventually) instead of bitbucketing.  Unfortunately, I\n>> have no idea if this is portable across all TCP implementations,\n>> since closing only half a socket may be incorrectly implemented in\n>> some OS's.\n\nDave> Not implemented may be more like the case. I can't find any\nDave> reference in WINSOCK for example to closing 1/2 a connection.\n\nConsider this a field report to the effect that there is strong\nevidence that this is the case.  Our winsock guru (one of the original\nimplementers of the FTP Software stack) spent 2 solid weeks trying to\ncoerce winsock to cope with the half-closed case.  His conclusion was\nthat it is either unimplemented or there is a bug in their\nimplementation.\n\nPersonally, I'd love to make the spec require a correct TCP\nimplementation, but somehow I don't think this will wash.\n\nThis implies to me that something -must- be put into the spec about\nthe required server behavior (i.e. must be willing to eat bytes for a\nwhile) and suggestions about approaches to defending against\ndenial-of-service attacks.  (I.e. eat bytes for at most 5 seconds.)\n\nPerhaps we could just require clients that are running on broken\nplatforms to send up a header that mentions what sort of stupid\ngyrations the server needs to do:\n\n  X-Braindamage: WinsockReset\n\n:-)\n\n-Roger\n\nRoger Gonzalez                    NetCentric Corporation\nrg@server.net                     56 Rogers Street\nhome   (617) 646-0028             Cambridge, MA 02142\nmobile (617) 755-0635             work (617) 868-8600\n\n\n\n60 09 3A EE FE 6A 1E CC   -pgp-   B7 F7 6B 0F 00 1D 01 C7 \n\n\n\n"
        },
        {
            "subject": "Re: Potential HTTP Security Ris",
            "content": "According to BearHeart / Bill Weinman:\n>    I just noticed in the WWW Security FAQ a notation that some \n> servers, including NCSA, allow the file \".htaccess\" to be retrieved. \n> I tried it with my Apache 1.0 server and I got the file. \n> \n>    Perhaps the following modification of the proposed section 12.5 \n> would help: (change marks in the left column are relative to Paul \n> Hoffman's message that began this thread)\n> \n>  | 12.5  Attacks Based On URL Contents\n> \n>    Implementations of the HTTP servers should be careful to restrict the\n>    documents returned by HTTP requests to be only those that were intended\n>    by the administrators. If an HTTP server translates HTTP URIs directly\n>    into file system calls, the server must take special care not to serve\n>    files outside the desired directory tree. For example, Unix, Microsoft\n>    Windows, and other operating systems use \"..\" as a path component to\n>    indicate a directory level above the current one. A URL with such\n>    constructs can be constructed to potentially allow access to files\n>    outside the desired directory structure, and should thus be disallowed.\n> \n>  + Many servers implement a system of access-control files within the \n>  + document directory tree that may contain sensitive security- or \n>  + implementation-related information. A URL which references a filename \n>  + which is used for access-control files, or a filename pattern \n>  + commonly used for system files (e.g. \"/.\" for Unix systems, or \".PWL\" \n>  + for Microsoft Windows systems), should be disallowed. A server should \n>  + make a configuration option available to the system administrator to \n>  + ensure that this protection is made sufficiently flexible for \n>  + site-specific security considerations. \n> \n> \n\nThese are just a couple of instances of a whole class of potential\nproblems associated with a design decision for some server\nimplementations.  Most (but not all) server implementations treat the\npath part of a requested URL as a file system path and make the\ndefault action to serve that file unless it is somehow explicitly\nforbidden.  This is the reason that /../ is problematic and that it is\nnecessary to pay so much attention keeping the server restricted to\nits data hierarchy.  It is also the reason that minor bugs so often\nturn into security problems (e.g. the NCSA bug that caused a\n\"cgi-bin//\" in a URL to be treated as a path but not to a cgi-bin and\nhence to allow the serving of script sources).  It is also the reason\nthat .htaccess files get served.  They seem especially problematic\nsince you can't set permissions on .htaccess files to make them\nunreadable by the server.\n\nThere are still more examples, like viewing CGI sources by requesting\nfoo.cgi~ in a directory where foo.cgi lives.  And there are almost\nsurely other problems we aren't yet aware of.  In my view these all\nstem from taking a (potentially hostile) request from a user and by\ndefault serving it unless it is somehow explicitly forbidden.  I am\nall for warnings it the HTTP specification, but it is not very\nrealistic to think that any collection of warnings will really remedy\nthe situation.  You simply can't warn against all the possible risks\nassociated with this design.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "[Roy T. Fielding]\n> write window eventually) instead of bitbucketing.  Unfortunately, I have\n> no idea if this is portable across all TCP implementations, since closing\n> only half a socket may be incorrectly implemented in some OS's.\n\n[David W. Morris] \n> Not implemented may be more like the case. I can't find any reference\n> in WINSOCK for example to closing 1/2 a connection.\n\nshutdown().\n\nTCP connections are full-duplex, and the ability to control each direction\nseparately is an integral part of the spec. (Each of the two paired\nsimplex \"channels\" are closed by sending a FIN packet to the other side.)\nThe classic example is where the server is a sort program reading data\nfrom its standard input: it must have an end-of-data indication before it\ncan produce the sorted results that have to be conveyed back to the\nclient. The client achieves this by a half-close on its \"write side\",\nleaving its \"read side\" open to retrieve the server's response. \n\nSee RFC 793, Section 3.5 \"Closing a Connection\":\n\n---8<---\n  CLOSE is an operation meaning \"I have no more data to send.\"  The\n  notion of closing a full-duplex connection is subject to ambiguous\n  interpretation, of course, since it may not be obvious how to treat\n  the receiving side of the connection.  We have chosen to treat CLOSE\n  in a simplex fashion.  The user who CLOSEs may continue to RECEIVE\n  until he is told that the other side has CLOSED also.\n---8<---\n\nAny TCP implementation that does not support such full-duplex control \n(i.e. the TCP half-close) should be considered fundamentally broken.\n\n\nRegards,\n\nArjun Ray\n\n\n\n"
        },
        {
            "subject": "Re: interaction of caching with content negotiation, authentication, state, et",
            "content": "Jeffrey Mogul writes:\n >     If it is possible for a given object to be returned in response to\n >     requests on different URIs, as it would be with content negotiation\n >     (among other possibilities), then either that object must be\n >     pre-expired, so that it will be validated on every request, or it will\n >     be possible for multiple versions of the object to be present in, and\n >     servable from, a cache if the object changes at the origin server\n >     prior to its stated expiration date.  The consequence of this is that\n >     users might see previous versions of an object, even though they are\n >     only using a single cache. \n >     \n > There's a false dichotomy here.  It should be possible to separate the\n > problem of validating a cached response (is this the response that the\n > server would give for this variant) from the problem of determining\n > compatability between a specific response and a set of requests that\n > differ in their Accept headers (is this the variant that the server\n > would return).\n > \n > I would agree that one could solve both problems by presenting every\n > request to the server, but I would not agree that this is the only\n > possible solution.  For example, if the origin server's response\n > contains sufficient information about the existence of other variants,\n > the cache could (in principle) decide that a non-expired (\"fresh\")\n > cached response obtained with one set of request headers is in fact\n > appropriate to return for a different set of request headers.\n > \n > -Jeff\n\nI see your point, though I wonder how tractable your alternative would be.\nTo ensure cache coherency using the method I think you are suggesting,\neach time a given object is returned in response to any request, the\nserver would have to indicate all the possible request URIs for which that\nobject might be returned (in addition, possibly, to the\ncontent-negotiation info by which the object would be selected).\n\nThere is also a spoofing issue, especially in the present design with\nits Location headers.  We can't let responses claim anything\nthat would allow the returned objects to be cached under a bogus key\nthat would allow later requests to erroneously return the object.\nThis is why people appropriately shot me down some months ago when I\nsuggested that, in order to solve the duplicate object problem as\nmentioned above, caches replace any objects they contain that used the\nsame Location header.\n\nSo, the only thing I have been able to think of that will work --\nby which I mean it will ensure cache coherence and also a\nreasonable hit rate -- is for the cache to flush any other objects it\ncontains which claimed the same URI as the returned Location header,\nbut only if they do not have the same cache validator.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: On the connection between content negotiation and cachin",
            "content": "On Thu, 28 Dec 1995, Daniel DuBois wrote:\n\n> >I can think of at least one important case where a simple optimization\n> >might work nicely: when the server *knows* that there is exactly\n\nThe question here would be whether the server *knows* that there will \nbe no variants in the future.  I accept that it could and often does but\nit may not.\n\n> But even in that case, it's unacceptable to serve up anything without\n> checking the content negotiation algorithm.  You can't send back text/html\n> to someone who sends \"Accept: text/html;q=0.0\".\n\nThe current draft 1.1 does not seem to support this position.  By my \nmy reading it is correct to send whatever the server has if it \nhave only one choice.  There is no special significance to q=0.0 that I\ncould find. Just really undesired by the requestor.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: More content negotiatio",
            "content": "On Thu, 28 Dec 1995, Daniel DuBois wrote:\n\n> I'm also not a fan of any client-side variant-picking algorithm going into\n> the protocol.  What's the point?  Defining variant-picking algorithm for\n> servers allows the variant choices to be reproduced by proxies, but what\n> does defining the client-side variant-picking algorithm gain?\n\nI can't see any point in a client-side variant-picking algorithm, BUT if\nuse is reasonably balanced I can see much value in having some form\nof conditional markup in the data which allows the client to select and\nutilize the appropriate sections of the data. In fact, designed properly\nthere would be nothing to prevent a smart server from providing variants\nbased on the conditional markup. The client variant conditional source\nwould be completely cachable. In the case where the server is involved,\nit just falls under whatever variant cache management ends up deployed.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "According to Roy T. Fielding:\n> \n>    For example, an experimental client may send:\n> \n>       Connection: keep-alive\n> \n>    to indicate that it desires to keep the connection open for multiple\n>    requests. The server may then respond with a message containing:\n> \n>       Connection: keep-alive, timeout=10, maxreq=5\n>    \n>    to indicate that the connection will be kept open for a maximum of 5\n>    requests, but will timeout if the next request is not received within\n>    10 seconds. \n>\n...\n> \n> Comments, please?\n> \n\nI have some some serious reservations about this.  Perhaps someone\nmore knowledgeable about TCP/IP can alleviate them.  Won't this\nmaintain the connection during the entire time that a client takes to\ndownload and layout and display a document containing containing\nmulitple images?  This could represent a substantial amount of time\nover a slow link.  For a heavily loaded server, say like HotWired,\nkeeping the connection open during the entire time a user enters her\npassword and multiple images in a document are downloaded over a 14.4\nlink could add up to quite a bit of time.  I would think that the\nmaximum number of allowed connections would be exceded quite often.\nThere would probably be a large number of timeouts each one representing\nalmost 10 seconds of unused connect time.\n\nBy contrast, I would think that an MGET connection sending the same\ndata could be a relatively short transaction for the server.  Am I\nwrong about this?  Of course, MGET would require two connections --\none GET for the HTML doc and image size headers and a second MGET for\nall the images.  But the difference between these two would be\nneglible for the client and potentially a very significant win for the\nserver.\n\nI don't see that MGET poses any problems for proxies.  They would have\nto parse the MGET and decide which files they have cached and which\nthey need to fetch.  Old (HTTP1.0) servers and proxies would send an\nunknown method message when they get an MGET and the client would then\nre-issue a sequence of GETs. \n\n> \n> ......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n>                                      <fielding@ics.uci.edu>\n>                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n> \n\n\n\n"
        },
        {
            "subject": "Re: On the connection between content negotiation and cachin",
            "content": ">> checking the content negotiation algorithm.  You can't send back text/html\n>> to someone who sends \"Accept: text/html;q=0.0\".\n\n>The current draft 1.1 does not seem to support this position.  By my \n>my reading it is correct to send whatever the server has if it \n>have only one choice.  There is no special significance to q=0.0 that I\n>could find. Just really undesired by the requestor.\n\nWell, I get this position from page 73, \"If no variants remain with a value\nof Q greater than 0 the server should respond with a 406 None Acceptable.\"\nAdditionally to support my interpretation there are references to empty\nAccept-Charset and Accept-Encoding headers allowing user agents to refuse\nresources (no mention of a multiple-variant requirement), so there is some\nimplication that those headers must be looked at even for non-varying resources.\n\nBut given that Content Negotiation is labeled an optional feature, that some\nof the statements int the spec could be interepreted the way you did, and\nthat some people may not feel negotiation applies to non-varying resources,\nI realize the intention of the spec might not be as I had perceived.  If\nthat's true, I think it should be made more explicit one way or the other.\n-----\nDan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\nI absolutely do not speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "    RTTs are not sufficiently important for the two-phase methods for them\n    to be the measure of what is \"best\".\n\nThat's a strong statement to be made without at least some elaboration.\nRTTs are the only delay in the Web (except for human \"think time\") that\ncannot be improved by technology.  (As Mike Powell once said, you can\nbuy better bandwidth but only God can change the speed of light.)\n\nIt may be true that PUT-like methods are not latency-sensitive, but\nif so, this requires some justification.\n\n    In most cases, this difference is trivial.  However, some security-related\n    systems consider the ability to refuse a vulnerable operation before it\n    occurs to be a showstopper.\n\nThe security issue here is new, and seems to have several components\n(I'm reading between the lines in your message):\n\n    (1) The transmission of some data that would have been rejected\n    might expose it to eavesdropping.\n\n    (2) The mere attempt to do a \"vulnerable operation\" that would\n    be rejected could cause some havoc at the server side.\n    \nAm I missing any others?  Frankly, I don't buy either of these\narguments; especially, as Koen points out, the 5-second timeout\ncan be manipulated by an external agent (via a temporary\ndenial-of-service attack) but also because we ought not to be\npretending that security can be accomplished without encryption\nfor privacy and authentication for authorization.\n\n    Also, some networks will require the user to pay by the amount of\n    data sent, regardless of whether that data was initially rejected\n    by the server.\n    \nA good point.  But even if the HTTP protocol allows the use of an\noptimistic two-phase scheme (as I am suggesting), this does not mean\nthat it would be required (since the optimistic scheme includes the\npessimistic scheme as a backstop).  In other words, a user that was\nbeing charged by the packet could choose to employ the pessimistic\nscheme, on its own initiative.\n\n    Given these cases exist, a pessimistic approach is \"best\".  This\n    does not mean that the 5 second delay is the best solution -- it is\n    just a way of forcing a real solution to be created.\n    \nWhich is exactly what I think has happened: you have done us a\nservice by forcing us to think about the problem.  But I still\nassert that the optimistic approach is \"better\" (perhaps not \"best\")\nif one believes that, most of the time, RTTs do matter and servers\nwill not reject PUT-like methods.  And it leaves the issue of\nsensitivity to usage-pricing to the transmitter of the data, not\nto an a priori choice in the protocol design.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: On the connection between content negotiation and cachin",
            "content": "On Thu, 28 Dec 1995, Daniel DuBois wrote:\n\n> But given that Content Negotiation is labeled an optional feature, that some\n> of the statements int the spec could be interepreted the way you did, and\n> that some people may not feel negotiation applies to non-varying resources,\n> I realize the intention of the spec might not be as I had perceived.  If\n> that's true, I think it should be made more explicit one way or the other.\n\nYes. I'm not sure if this is the right way, but there clearly must\nbe a way to pre-reject certain content forms. */*;q=0 would I guess \nmeans don't send anything I haven't listed explictly. But since\nthis is optional, the client must be prepared to receive junk anyway.\n\nAssuming this is the interpretation or clarification, Jeff's original\ncomment basically still holds as the proxy would know that the origin\nhad only one choice and it wasn't acceptable.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "    I agree, doing it this way is the best solution.  Of course, this\n    implies that the caching/proxy group stops discussing the caching of\n    negotiated responses for some time.\n    \n    While the content negotiation subgroup is working out the negotiation\n    mechanism, the caching/proxy subgroup can work on the caching model\n    for non-negotiated responses.  There are still plenty of things to\n    discuss even if negotiation is presumed absent.\n\nIf the content-negotiation subgroup is aware of the need to provide\na solution to the caching problem, I think the caching subgroup can\ngo on our merry way for a while.  Ultimately, the content-negotiation\nsubgroup ought to provide a precise definition of how a cache can\ndetermine if a new request matches the cached response to a previous\nrequest.\n\nThe current 1.1 draft contains this paragraph:\n   Servers that make use of content negotiated resources must include \n   URI response headers which accurately describe the available \n   variants, and include the relevant parameters necessary for the \n   client (user agent or proxy) to evaluate those variants.\n\nI think the key words here are \"accurately describe\", \"relevant\nparameters\", and \"evaluate.\"  We need precise specifications to\nreplace these somewhat ambiguous phrases.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": ">>>>> Jeffrey Mogul writes:\n\nJeff> But I still assert that the optimistic approach is \"better\" \nJeff> (perhaps not \"best\") if one believes that, most of the time,\nJeff> RTTs do matter and servers will not reject PUT-like methods.\n\nWhile I agree with you overall, I don't buy this.  The first PUT\nin any session will almost -always- be rejected.  Here's my reasoning:\n\n1) Most servers that provide PUT are not going to allow arbitrary\n   uploading; the target is going to be an \"approved\" location.\n\n2) The only current mechanism in the protocol for passing information\n   that can be used to determine approval is the Authorization header.\n\n3) Clients cannot send the Authorization header unprompted, because\n   this would be a big security issue.  (Evil servers would just save\n   the auth info and look at the referer.)\n\n4) Therefore, the first upload in any session will usually be returned\n   a 401.\n\nWhile I would prefer to leave the choice to the client software (based\non size heuristics or whatever), this may not be adequate.\n\nConsider the following scenario:\n\nA server has a portion of its document space that requires encrypted\naccess.  All requests for documents under this tree are redirected to\nthe \"https\" version of the server listening on a different port.  The\nclient wants to upload data to this space, and is chatting with the\nserver on the unencrypted port.  When it tries to upload a small file\nto this area, the server returns a 301.  Unfortunately, a Bad Guy was\nsnooping the wire, and captured the rejected data.  Since one of the\nphilosophies of the protocol is that the client shouldn't need to know\nabout the type of resource it refers to, there is no way that the\nclient would know -not- to upload to this URL.  The only way around\nthis would be to -require- a 2-phase.  I'm not thrilled about this.\n\nLets just nail it down soon; I'd prefer -any- definition to one\nthat changes every month or so.  :-)\n\n-Roger\n\nRoger Gonzalez                    NetCentric Corporation\nrg@server.net                     56 Rogers Street\nhome   (617) 646-0028             Cambridge, MA 02142\nmobile (617) 755-0635             work (617) 868-8600\n\n\n\n60 09 3A EE FE 6A 1E CC   -pgp-   B7 F7 6B 0F 00 1D 01 C7 \n\n\n\n"
        },
        {
            "subject": "Re: More content negotiatio",
            "content": "Daniel DuBois:\n>\n>Koen wrote a long time ago:\n>>Perhaps even more importantly, having a safe way to disable the\n>>client/proxy side `variant selection engine' would allow service\n>>providers to use special purpose variant selection algorithms that\n>>cannot be `run' on client-side `variant selection engines'.  One\n>>example of such a special purpose algorithm is negotiation around\n>>known bugs in user agents based on the user-agent header.\n>\n>I'm not a big fan of any scheme that purposely defeats the ability of a\n>proxy to serve cached objects because of user-agent rendering differences.\n>However, some people seem intent on doing this, so it has to be\n>addressed.\n\nYes.\n\n>But, I'm not convinced any HTTP headers or protocol changes can overcome or\n>signifigantly reduce the performance hit of the existance of opaque\n>variant-picking schemes.\n\nAs you explain in your message, with a 302 `moved temporarily'\nredirection scheme, you can already do opaque variant picking at the\nserver side for the cost of one RTT if the picked variant is in the\ncache.  So there is not that big a performance hit already.\n\n`Real opaque content negotiation' will improve over 302 redirection by\n 1) making the availability of variants explicit, so that the\n    user can pick another variant if desired\n 2) improving on the cost of the 302 redirection scheme if the\n    selected variant is _not_ in the cache: 302 needs 2 RTTs for this\n    case, real opaque negotiation will only need 1 RTT in most cases.\n\n>I'm also not a fan of any client-side variant-picking algorithm going into\n>the protocol.  What's the point?\n\nThe client (the user agent) doing reactive negotiation is the point.\n\n> [Description of opaque `negotiation' doing 302 redirects deleted to\n>  save space]\n\n>Dan DuBois, Software Animal             http://www.spyglass.com/~ddubois/\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "> A good point.  But even if the HTTP protocol allows the use of an\n> optimistic two-phase scheme (as I am suggesting), this does not mean\n> that it would be required (since the optimistic scheme includes the\n> pessimistic scheme as a backstop).  In other words, a user that was\n> being charged by the packet could choose to employ the pessimistic\n> scheme, on its own initiative.\n\nIf this would still require the 100 Continue response be sent upon receipt\nof the initial headers (which is what the application desiring a pessimistic\nscheme would be looking for anyway), it sounds good to me.  It is okay if\nthe pessimistic scheme only works \"well\" for HTTP/1.1 servers.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": ">>Jeffrey Mogul writes:\n>> But I still assert that the optimistic approach is \"better\" \n>> (perhaps not \"best\") if one believes that, most of the time,\n>> RTTs do matter and servers will not reject PUT-like methods.\n\n>>While I agree with you overall, I don't buy this.  The first PUT\n>>in any session will almost -always- be rejected.\n\nI disagree.  Imagine a WWW-based conferencing system that is like\na super version of Usenet or Notes.  The URL could be a Message-ID\ngenerated by the client and \"meta-data\" in the headers and/or body\nwould let the server link it into the right place(s) in the\ndocument tree(/web).\n\nI think such systems are possible -- heck, I want one now :) -- and\nthat they will have automated gateways doing lots of puts as they\nfeed tickerdata or Usenet into the web.  The extra RTT will be a bad\nthing.  We're seeing this right now with NNTP.\n/r$\n\n\n\n"
        },
        {
            "subject": "ISPP BOF repor",
            "content": "text/enriched attachment: stored\n\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "On Thu, 28 Dec 1995, Roger Gonzalez wrote:\n\n> \n> >>>>> Jeffrey Mogul writes:\n> \n> Jeff> But I still assert that the optimistic approach is \"better\" \n> Jeff> (perhaps not \"best\") if one believes that, most of the time,\n> Jeff> RTTs do matter and servers will not reject PUT-like methods.\n> \n> While I agree with you overall, I don't buy this.  The first PUT\n> in any session will almost -always- be rejected.  Here's my reasoning:\n\nWell, my sample is a couple versions of Netscape, but I believe your\nreasoning is based on an untrue premise.  I am a regular user of\nwww.quote.com and have been prompted for basic authentiation data\nand supplied it since my lasst restart of Netscape.  The next time\nI communicate with www.quote.com Netscape provides the authentication\ndata w/o prompting by the server.\n\nIt has been and is my belief that most PUTs will occur in response to\na prompt page from the same server/authentication domain. If the\nchallenge/response occurs before the setup/prompt page is delivered,\nit is likely that the UA will provide the same response with the\nPUT and avoid the challenge.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "A Hybrid PUT Scheme (was Re: 'PUT' transaction reconsidered",
            "content": "I think the ideal approach would be:\n\n1) to send small files in one-phase PUT (where extra RTT has big effects)\n2) to send large files in two-phase PUT (where extra RTT has less effects)\n\nNow, here is a hybrid scheme:\n\nStep one:   send header PLUS part of the data (fit into one packet)\nStep two:   wait for \"continue\" or timeout\nStep three: send the rest of the data if there are any \n\nFor small files (ie, header and data fit into one packet), it is effectively\na one-phase PUT while for large files, a two-phase PUT.\n\nCheers\nZheng\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "> According to Roy T. Fielding:\n> > \n> >    For example, an experimental client may send:\n> > \n> >       Connection: keep-alive\n> > \n> >    to indicate that it desires to keep the connection open for multiple\n> >    requests. The server may then respond with a message containing:\n> > \n> >       Connection: keep-alive, timeout=10, maxreq=5\n> >    \n> >    to indicate that the connection will be kept open for a maximum of 5\n> >    requests, but will timeout if the next request is not received within\n> >    10 seconds. \n> >\n> ...\n> > \n> > Comments, please?\n> > \n> \n> I have some some serious reservations about this.  Perhaps someone\n> more knowledgeable about TCP/IP can alleviate them.  Won't this\n> maintain the connection during the entire time that a client takes to\n> download and layout and display a document containing containing\n> mulitple images?\n\nThe idea is that a client can either:\n\n1) Send one request, wait for the response and then send the next one\n\n2) Send multiple responses without waiting for the responses. The\nessential thing is that the server always serves the requests in\nserial. This is about as far as you can get without a session layer -\nSimon ?\n\nWell, let's look at the possible senarios\n\n1) Experimental client / old server: The server will not understand the\nheader but will serve the first request and close the connection. No\nextra round trip time will be needed as the server understands the rest\nof the request. It might cause some problems if an eager client sends\nmore requests to a server without waiting for the response. In this\ncase some server applications might get confused (even though they\nshouldn't)\n\n2) Experimental client / old proxy: This will break if the remote host\ndoes in fact understand the connection header. As Roy points out, this\nis a weakness of the design.\n\n3) Experimental client / Experimental server / slow link: The server\nwill serve the first request but leave the connection open until it has\nreached the timeout indicated in the Connction header. New requests\narriving after the closing will be refused. The client should then try\nto reopen the connection and possibly use a new Connection header.\n\n4) Experimental client / Experimental server / interrupt: The client\nwill then close the connection and the server will get a SIG_PIPE as\nusual. Any request already in the pipe will get lost.\n\nI still think that it is better to keep the connection open for a small\namount of time as the server socket will go into TIME_WAIT state when\nit is closed and then can't be reused before about 240 seconds\n(recommended timeout accorsing to Simon)\n\n> By contrast, I would think that an MGET connection sending the same\n> data could be a relatively short transaction for the server.  Am I\n> wrong about this?  Of course, MGET would require two connections --\n> one GET for the HTML doc and image size headers and a second MGET for\n> all the images.  But the difference between these two would be\n> neglible for the client and potentially a very significant win for the\n> server.\n> \n> I don't see that MGET poses any problems for proxies.  They would have\n> to parse the MGET and decide which files they have cached and which\n> they need to fetch.  Old (HTTP1.0) servers and proxies would send an\n> unknown method message when they get an MGET and the client would then\n> re-issue a sequence of GETs. \n\nThe MGET causes an extra roundtrip time as many servers will think:\n\n\"Gee - nobody has told me about this method. I simply refuse to\ndo anything about it\"\n\nThis is the reason for waiting with a SESSION method to HTTP/1.1 as Roy\npoints out. Also we would like a more general approach than only having\nGET as a possibility. A SESSION method can also be used for interactive\nsessions.\n\nHowever, we need some experimental implementations. The plan is to make\na prototype implementatation of the CERN server and a GUI client (for\nexample Arena) to see the actual effect when somebody has generated\n9Giga of tcp dumps :-)\n\n\n-- cheers --\n\nHenrik Frystyk\nfrystyk@W3.org\n+ 41 22 767 8265\nWorld-Wide Web Project,\nCERN, CH-1211 Geneva 23,\nSwitzerland\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send  concerns ",
            "content": "In <9512282109.AA06423@acetes.pa.dec.com>, you wrote, Jeff:\n>The security issue here is new, and seems to have several components\n>(I'm reading between the lines in your message):\n>\n>    (1) The transmission of some data that would have been rejected\n>    might expose it to eavesdropping.\n>\n>    (2) The mere attempt to do a \"vulnerable operation\" that would\n>    be rejected could cause some havoc at the server side.\n>\n>Am I missing any others?  Frankly, I don't buy either of these\n>arguments; especially, as Koen points out, the 5-second timeout\n>can be manipulated by an external agent (via a temporary\n>denial-of-service attack) but also because we ought not to be\n>pretending that security can be accomplished without encryption\n>for privacy and authentication for authorization.\n\nOne additional risk is traffic analysis, as in, \"Gee, EDS is sure sending a \nlot of encrypted messages to GM today\".  Remember that Kocher's attack on \nRSA involves timing analysis, a form of traffic analysis.  Sending the whole \nPUT could open it up to Kocher's or a similar attack, especially if the \nobject being PUT is publicly readable, as some of the data would then be \nknown plaintext.\n======================================================================\nMark Leighton Fisher                   Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n"
        },
        {
            "subject": "Re: Rethinking content negotiation (Was: rethinking caching",
            "content": "Jeffrey Mogul:\n>\n>If the content-negotiation subgroup is aware of the need to provide\n>a solution to the caching problem, I think the caching subgroup can\n>go on our merry way for a while.  Ultimately, the content-negotiation\n>subgroup ought to provide a precise definition of how a cache can\n>determine if a new request matches the cached response to a previous\n>request.\n\nI agree this is what the content-negotiation subgroup ought to\nprovide.  It is on our list of topics.\n\nWe will also be discussing whether or not the negotiation mechanism\nshould yield variant URIs in Location headers, and, if so, how to\nprevent spoofing.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "    > A good point.  But even if the HTTP protocol allows the use of an\n    > optimistic two-phase scheme (as I am suggesting), this does not mean\n    > that it would be required (since the optimistic scheme includes the\n    > pessimistic scheme as a backstop).  In other words, a user that was\n    > being charged by the packet could choose to employ the pessimistic\n    > scheme, on its own initiative.\n    \n    If this would still require the 100 Continue response be sent upon\n    receipt of the initial headers (which is what the application\n    desiring a pessimistic scheme would be looking for anyway), it\n    sounds good to me.  It is okay if the pessimistic scheme only works\n    \"well\" for HTTP/1.1 servers.\n\nThat sounds good to me.  Let me try to make this more specific, with\nan attempt to cover all of the cases (I'll probably botch this).  Places\nwhere I wasn't sure about the language I've marked with \"[]\" brackets.\n\n    Upon receiving a PUT-like method from a 1.1 (or later) client,\n    a 1.1 (or later) server immediately [as soon as possible] either\n    responds with \"100 Continue\" and continues to read from the\n    input stream, or responds with an error status.  If it responds\n    with an error status, it MAY close the TCP [transport] connection\n    or it MAY continue to read and discard the rest of the request.\n    \n    A 1.1 (or later) client doing a PUT-like method SHOULD monitor\n    the network connection for an error status while it is transmitting\n    the body of the request.  If it sees an error status, it SHOULD\n    immediately [as soon as possible] cease transmitting the body.\n    If the body was preceeded by a Content-length: header, the client\n    MUST close the connection to mark the end of the request.\n    \n    A 1.1 (or later) client MUST be prepared to accept a 100 Continue\n    status followed by a regular response.\n    \n    A 1.1 (or later) client that sees the connection close before\n    receiving any status from the server SHOULD retry the request,\n    but if it does so, it MUST use the two-phase method.  In the\n    two-phase method, the client first sends the request headers,\n    then waits for the server to respond with either a 100 Continue\n    (in which case the client SHOULD continue) or an error status\n    (in which case the client MUST NOT continue, and MUST close\n    the connection if it had sent a Content-length: header as part\n    of the request).\n    \n    If the client knows that the server is a 1.1 (or later) server,\n    because of the server protocol version returned with a previous\n    request on the same persistent connection [alternatively: within\n    the past <N> hours], it MUST wait for a response.  If the client\n    believes that the server is a 1.0 or earlier server, it SHOULD\n    continue transmitting its request after waiting at least 5 [?]\n    seconds for a status response.\n    \n    [If the client has seen the server respond with a version of 1.0\n    (or earlier) within the last <M> minutes, it MAY skip the two-phase\n    method.]\n\n    A 1.1 (or later) client that sees the connection close after\n    receiving a \"100 Continue\" but before receiving any other\n    status SHOULD retry the request, and need not use the two-phase\n    method (but MAY do so if this simplifies the implementation).\n\n    A 1.1 (or later) server that receives a request from a 1.0\n    (or earlier) client MUST [SHOULD] NOT transmit the \"100 Continue\"\n    response, and MUST accept the entire request body (without\n    prematurely closing the TCP connection) if the request includes\n    a Keep-Alive: header.  Otherwise, it MAY close the connection\n    prematurely.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: 'PUT' transaction reconsidered (was Re: twophase send concerns ",
            "content": "Another thought:\n\nOne way for a server to unilaterally prevent the transmission of \"too\nmuch\" data before it gets a chance to check the header is for it to\nadvertise a relatively small TCP window.  For example, if the server\nsets its receive window size to, say, 512 bytes, the client will not be\nable to send more than 512 bytes beyond the point that the server has\nread.  If the server decides to accept the request, it can open up the\nwindow and let the bytes flow at full speed.  Otherwise, it can send an\nerror status and close the connection.\n\nSo we could use the \"optimistic two-phase\" scheme in the HTTP/1.1\nprotocol spec, and then advise people who consider that to be too\ninsecure to use a window-based scheme to further protect their\nservers.\n\nI'll admit right away that there are some drawbacks to this approach.\nFirst of all, it seems to be nearly impossible in BSD-based systems to\nset the receive window size before the connection has been accept()ed,\nwhich means that a client with a large congestion window could send a\nlot of data before the window shrinks.  I think this is a design\ndeficiency in BSD, but I don't know if it is likely to be widely\nfixed.  Anyway, most clients slow-start with a 1-MSS congestion window,\ngiving the server some time to set its receive window down.\n\nSecond, it's probably hard to combine this with a persistent-connection\nmodel that uses pipelining, since the client could use \"excess\" window\nremaining from one request to start another one.  So a server that\nreally wants to prevent transmission of unwanted request bodies has to\nstick to a single-request-per-connection approach.\n\nThird, it doesn't prevent the client from sending at least a little of\nthe request content; although I think it is idiotic to base one's\nsecurity on preventing this kind of thing, people who do want to do\nthat might complain.\n\nFourth, it could introduce a few extra round-trips into the exchange\n(depending on the TCP MSS).\n\nJust a thought.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "review of byte range draft: please repl",
            "content": "As far as I can tell, the current members of the 'range retrieval'\nreview subgroup (Ari Luotonen and Steve Zilles) are content with \n\n         draft-luotonen-http-url-byterange-02.txt\n\nand do not plan to do any additional work on the proposal or any\nelements of range retrieval that might be in the current HTTP/1.1\ndraft, or review to see if there are any differences between the\nLuotonen draft and the Fielding http/1.1 draft.\n\nGiven this situation, there's no particular reason to delay. Thus, I\nwould like to poll the working group to see if there is consensus to\naccept 'draft-luotonen-http-url-byterange-02.txt' as the way in which\nrange retrieval should be done in HTTP/1.1.\n\nWhat say you?\n\n\n\n"
        },
        {
            "subject": "review of byte range draft: please repl",
            "content": "I'm sorry to have sent out my previous message too quickly.\nPlease ignore it, and instead reply to the following request:\n\n> I do have interest in getting to consensus about byteranges, and\n> additions to the protocol to address cache control with byteranges,\n> and other directives in context of that; but I do not want the things\n> already defined by the BR spec to change (Accept-ranges:, Range:,\n> Content-range:, Content-type:, 206 Partial Content).\n\nIf you have any comments about the byte range draft with regard to\nthose issues, would you please send them asap to the members of the\nbyte range group?\n\n\n\n"
        },
        {
            "subject": "safe PU",
            "content": "My perusal of the mailing list is that the following people\n\n  dave@navisoft.com,\n  fisherm@is3.indy.tce.com,\n  dmw@shell.portal.com,\n  rg@caffeine.server.net,\n  koen@win.tue.nl,\n  mogul@pa.dec.com,\n  Z.Wang@cs.ucl.ac.uk,\n  fielding@avron.ics.uci.edu,\n  aray@pipeline.com\n\nhave commented on the issues surrounding sending a 'PUT' transaction\nsafely and reliably. I also believe that there is sufficient negative\nreaction to Roy's proposal in the HTTP/1.1 draft to the '5 second\ndelay' as to indicate that there is not 'rough consensus' on that\nparticular design element.\n\nWe've had a long discussion with various counter-proposals floated,\nbut not much convergence. I'll ask all of you to decide between one of\nthe following three alternatives (I can think of no others):\n\n- Are you interested in drafting a counter-proposal?\n- Should PUT go into HTTP/1.1 as originally specified, but\n  with a warning as to its unreliability?\n- Should PUT be removed from HTTP/1.1?\n\n\n\n"
        },
        {
            "subject": "Re: safe PU",
            "content": "Well, after finally getting to play around a bit with working\ncode*, I'm sticking with my initial counter-proposal of full-duplex\nPUT (or full-duplex methods, in general):\n\na - the client MAY stop sending request data after a response\n    is detected\n\nA naive client can use old the HTTP/1.0 request-response, but a more\nsophisticated one (well, <<10 lines of code more sophisticated :-)\nwill stop sending its request after response data is detected in\nits read queue.\n\nrom the server's point of view, all this means is that it had\nbetter not send a response until it has all the information it\nwants from the request.  Somehow, this doesn't seem like much\nof a hardship...\n\nIf one wishes to support the 100 continue, then we get up to ~10 lines\nof code, and the rule changes to:\n\nb - the client MAY stop sending request data after a _non-100_\n    response is detected\n\noff the top of my head:\n\n+ it's simple, especially (a)\nany argument?\n\n- data might be sent that wouldn't have been\n  if the client had to wait for a 100.\nWe seem to be in mid-argument over\nhow important this is...\n\n- data might not be sent if a server returned its\n  200 response early.\nThis is not an introduced problem.  Our\ncustomers have encountered servers that\nhappily read all the PUT request, then\nsend a 200 OK, but don't actually do\nanything!\n\n-Dave\n\n* I have a windows client that implements something close to (b); let me\n  know if you'd like it, or like it on a different platform.\n\n\n\n"
        },
        {
            "subject": "Re: safe PU",
            "content": "A mini-rant, from the application developer's perspective.\n\nIt's certainly worth considering low-level transport issues\nwhen working on these protocols, however:\n\n* As an apps guy, I have _no control_ over the stack underneath\n  me.\n\nUnix boxes tend to implement TCP very well.  WINSOCKs come\nin all flavors, some better than others.  We've even used\na package on the Mac that required a patch to select() to\nreport reset connections.\n\n* Is HTTP tied to TCP?\n\nI tend to think that many of the \"problems\" that HTTP-NG\naddresses are actually with TCP, not HTTP itself.   We've\ndone work for firms that (horrors!) don't run a TCP network,\nbut they still see that value of HTTP as a backbone IS protocol.\n\nSo, when I see references to \"advertising different window sizes\",\nor \"doing a half-duplex shutdown()\", I think that it's nice to know\nthat we can optimize implementations for the commonly used (and\ncorrectly implemented!) transports, but it's much more important\nthat the HTTP protocol solve problems *at its level of abstraction*,\nwithout making unusual demands upon the transport.\n\nNot usually such a luddite,\n-Dave\n\n\n\n"
        },
        {
            "subject": "Re: safe PU",
            "content": "On Fri, 29 Dec 1995, Dave Long wrote:\n\n> \n> A mini-rant, from the application developer's perspective.\n> \n> It's certainly worth considering low-level transport issues\n> when working on these protocols, however:\n> \n> * As an apps guy, I have _no control_ over the stack underneath\n>   me.\n>\n> Unix boxes tend to implement TCP very well.  WINSOCKs come\n> in all flavors, some better than others.  We've even used\n> a package on the Mac that required a patch to select() to\n> report reset connections.\n\nSorry, why are broken stacks _your_ problem? Given an API to program to, \nhow do you really expect to cater to broken implementations of the API? \n\n> * Is HTTP tied to TCP?\n\nNot in so many words, but IMHO yes. The spec talks of the connection as a \n\"virtual circuit\". In the Introduction:\n\n  \"It builds on the discipline of reference provided by the Uniform \n   Resource Identifier (URI), as a location (URL) or name (URN) [16], \n   for indicating the resource on which a method is to be applied.\"\n\nand RFC 1738 (section 3.1) identifies the HTTP scheme as based on Internet \nprotocols; hence the \"//\" after the \"http:\". There's always an IP \naddress, and the port defaults to 80 -- *TCP* 80.\n\n> I tend to think that many of the \"problems\" that HTTP-NG\n> addresses are actually with TCP, not HTTP itself.   We've\n> done work for firms that (horrors!) don't run a TCP network,\n> but they still see that value of HTTP as a backbone IS protocol.\n> \n> So, when I see references to \"advertising different window sizes\",\n> or \"doing a half-duplex shutdown()\", I think that it's nice to know\n> that we can optimize implementations for the commonly used (and\n> correctly implemented!) transports, but it's much more important\n> that the HTTP protocol solve problems *at its level of abstraction*,\n> without making unusual demands upon the transport.\n\nIt really sounds as if you want HTTP to take extraordinary precautions \nagainst implementations. Dealing with TCP implementation failures \nis properly outside the scope of HTTP. As for a \"non-TCP network\", once \nyou're past the issues of what an IP address could mean on such a \nnetwork, you still have to figure out what HTTP could mean without a \nvirtual circuit, i.e.\n\n  (i) reliable delivery of stream data in sequence\n  (ii) paired simplex channels to differentiate direction of data flow\n\nTCP:-)\n\nOr are you arguing that HTTP should assume a half-duplex transport\nservice (like, say, LU6.2)? \n\n\nRegards,\n\nArjun Ray \n\n\n\n"
        },
        {
            "subject": "Re: Potential HTTP Security Ris",
            "content": "> For the 'security considerations' portion of the 1.1 draft, with your\n> concurrence:\n\n>> >    Implementations of the HTTP servers should be careful to restrict\n>> >    the documents returned by HTTP requests to be only those that\n>> >    were intended by the administrators. If an HTTP server translates\n>> >    HTTP URIs directly into file system calls, the server must take\n>> >    special care not to serve files that were not intended to be\n>> >    delivered to HTTP clients. For example, Unix, Microsoft Windows,\n>> >    and other operating systems use \"..\" as a path component to\n>> >    indicate a directory level above the current one. A URL with such\n>> >    constructs can be constructed to potentially allow access to\n>> >    files outside the desired directory structure, and should thus be\n>> >    disallowed. Similarly, access control files, configuration files,\n>> >    script implementations of the HTTP server itself must be\n>> >    adequately protected if they might contain sensitive information,\n>> >    as long as the HTTP server translates the path of the URL into a\n>> >    file system identity and sends it unless otherwise prohibited.\n>> >    Experience has shown that minor bugs in such HTTP server\n>> >    implementations have turned into security risks.\n\nAs an addition, it should go to the mailing list first (also, I think\nit applies equally to 1.0).\n\nI would suggest a few changes (in the latter half):\n\n>    Implementations of the HTTP servers should be careful to restrict\n>    the documents returned by HTTP requests to be only those that\n>    were intended by the administrators. If an HTTP server translates\n>    HTTP URIs directly into file system calls, the server must take\n>    special care not to serve files that were not intended to be\n>    delivered to HTTP clients. For example, Unix, Microsoft Windows,\n>    and other operating systems use \"..\" as a path component to\n>    indicate a directory level above the current one.\n     On such a system, an HTTP server must disallow any such construct\n     in the Request-URI if it would otherwise allow access to a resource\n     outside those intended to be accessible via the HTTP server.\n     Similarly, files intended for reference only internally to the server\n     (such as access control files, configuration files, and script code)\n     must be protected from inappropriate retrieval, since they might\n     contain sensitive information.\n>    Experience has shown that minor bugs in such HTTP server\n>    implementations have turned into security risks.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authenticatio",
            "content": "On Fri, 29 Dec 1995, Larry Masinter wrote:\n\n> The Digest Access Authentication mechanism has been resubmitted to the\n> HTTP working group for consideration for inclusion in HTTP/1.1. The\n> boundary between HTTP-WG and WTS-WG is fuzzy in this area, but I would\n> like to make sure that members of WTS-WG and the Security Area have an\n> adequate chance to review and comment on security-related items in\n> HTTP-WG documents.\n> \n> Does anyone believe that HTTP-WG should *not* proceed with digest-aa?\n> \n> ================================================================\n>        Title     : A Proposed Extension to HTTP : Digest Access \n>                    Authentication                                          \n>        Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n>                    A. Luotonen, E. Sink, L. Stewart\n>        Filename  : draft-ietf-http-digest-aa-02.txt\n>        Pages     : 6\n>        Date      : 12/20/1995\n> \n> The protocol referred to as \"HTTP/1.0\" includes specification for a Basic \n> Access Authentication scheme.  This scheme is not considered to be a secure\n> method of user authentication, as the user name and password are passed \n> over the network in an unencrypted form.  A specification for a new \n> authentication scheme is needed for future versions of the HTTP protocol.  \n> This document provides specification for such a scheme, referred to as \n> \"Digest Access Authentication\".  The encryption method used is the RSA Data\n> Security, Inc. MD5 Message-Digest Algorithm [3].                           \n> \n\nWill this be available to people outside the US, or will the ITAR \nregulations mean that only those in the US can legally use it.\n\n\n-----------------------------------------------------------------------------\n\nAndrew Cameron\nInternet : andrew@andy.alt.za\nX.400    : C=ZA G=Andrew S=Cameron Admd=TELKOM400\n\n----------------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authenticatio",
            "content": "> > This document provides specification for such a scheme, referred to as \n> > \"Digest Access Authentication\".  The encryption method used is the RSA Data\n> > Security, Inc. MD5 Message-Digest Algorithm [3].                           \n> Will this be available to people outside the US, or will the ITAR \n> regulations mean that only those in the US can legally use it.\n\nITAR does not limit authentication as much as encryption, (and MD5\ncode seems to be widely available), so I think this should not be a problem.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Simon Spero wrote:\n\n> The perceived performance of Netscape is merely a function of their\n> rendering order. Netscape renders GIFS as they come in. THis is completely\n> independent of the protocol. The new parser for Arena is based on a \n> restartable model- it will render everything that it possibly can as it \n> comes in - I haven't seen Dave's latest design, but I think it can even start\n> rendering GIFS before it's finished retrieving the text. \n\nThe multithreaded feature in the Arena is a result of the new\nachitecture of the WWW Library of Common Code. It allows data-driven\nparsing or rendering as soon as data comes in on the network, and new\nrequests to be issued at any time. So rendering GIFS before finishing\nthe text is perfectly possible.\n\n-- cheers -- \n\nHenrik Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Potential HTTP Security Ris",
            "content": "At 10:07 am 12/28/95 -0600, John Franks wrote:\n>all for warnings it the HTTP specification, but it is not very\n>realistic to think that any collection of warnings will really remedy\n>the situation.  You simply can't warn against all the possible risks\n>associated with this design.\n\n   That's why I suggested language that would suggest that the \nserver have a configuration option for the sysadmin to specify \nwild-cards that would protect against classes of files: \n\n>>  + which is used for access-control files, or a filename pattern \n>>  + commonly used for system files (e.g. \"/.\" for Unix systems, or \".PWL\" \n>>  + for Microsoft Windows systems), should be disallowed. A server should \n\n   That way, I could have entries like this in my access.conf \nfile: \n\n<Directory /web/doctree>\n<Limit GET>\norder allow,deny\nallow from all\ndeny files \"..*\"\ndeny files \".*\"\ndeny files \"*.cgi\"\ndeny files \"nph-*\"\n</Limit>\n</Directory>\n\n   Currently, I have a server that I'm planning to put on the net. \nI am the only user on the system so I didn't see any particular \nrisk in having local ACFs in the filesystem until I realized that \nthey could be retrieved by a GET. If I could restrict them as \nI've shown here, I would be able to use them. \n\n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * \"To enjoy life, take big bites. Moderation is for monks.\" \n                                                       --Lazarus Long\n\n\n\n"
        },
        {
            "subject": "Re: Potential HTTP Security Ris",
            "content": "At 12:10 am 12/30/95 -0800, Roy T. Fielding wrote:\n>> For the 'security considerations' portion of the 1.1 draft, with your\n>> concurrence:\n>     On such a system, an HTTP server must disallow any such construct\n>     in the Request-URI if it would otherwise allow access to a resource\n>     outside those intended to be accessible via the HTTP server.\n>     Similarly, files intended for reference only internally to the server\n>     (such as access control files, configuration files, and script code)\n>     must be protected from inappropriate retrieval, since they might\n>     contain sensitive information.\n\n   I like the wording here \"outside those intended to be accessible\", as \nthat is more general than what I had suggested. I would also like to \nsee some suggestion that the sysadmin be able to specify what is and \nis not \"intended to be accessible\". \n\n   Unix, in particlar, is flexible enough that a sysadmin may have \nnon-standard filenames for sensitive files. Some do this as an \nadded security precaution. \n\n   This language was in the paragraph that I had suggested earlier:\n\n + A server should \n + make a configuration option available to the system administrator to \n + ensure that this protection is made sufficiently flexible for \n + site-specific security considerations. \n\n\n+----------------------------------------------------------------------+\n * BearHeart / Bill Weinman \n * BearHeart@bearnet.com *            * http://www.bearnet.com/ *\n * Author of The CGI Book:    * http://www.bearnet.com/cgibook/ *\n * \"To enjoy life, take big bites. Moderation is for monks.\" \n                                                       --Lazarus Long\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authenticatio",
            "content": "On Sat, 30 Dec 1995, Andrew Cameron wrote:\n\n>Will this be available to people outside the US, or will the ITAR\n>regulations mean that only those in the US can legally use it.\n\nPutting Albert Lunde's point more emphatically: since Digest Access\nAuthentication does not provide confidentiality (doesn't use encryption)\nit doesn't fall under ITAR at all.\n\n-Allan\n\n\n\n"
        },
        {
            "subject": "Caching data returned from POST, and conditional POS",
            "content": "I noticed that Netscape (2.0b4) sends a conditional POST when data\ncached that was returned from a POST expires.\n\nHowever, draft-ietf-v10-spec-04 p.27 says \"Applications must not cache responses\nto a POST request\", and draft-ietf-v11-spec-00 p35. says \"Responses to this\nmethod are not cachable\".\n\nI think this is a case where both drafts are clearly incorrect.\n\nThe logic for this is that the GET and POST methods are logically\nindistinguishable. Both send a request with arguments and result in\nthe return of data. An end user sees no difference between them and expects\nthe \"Back\" and \"Forward\" buttons to act in the normal manner in retrieving\ncached data. That is, a browser MUST cache the results of a POST if it is\nto behave correctly.\n\nThe server has ample control over caching at the browser or by proxies\nand can use this to ensure that such caching is not problematic to a\ntransaction sequence that changes the state of data at the server.\n\nThere are many POST-based transaction sequences that remain stateless\nat the server by using hidden fields to pass state information back to the\nclient. These sequences rely on browser caching to allow the \"Back\" command\nto be used as an \"Undo\".\n\nI propose that the statements about caching be removed from both specifications.\n\nI propose that conditional POST have the same status as conditional GET.\nIt is harmless to existing practice since servers can continue to take no\nnotice of the \"If-Modified-Since\" field for a POST if they wish.\n\nb.\n\nDr Brian R Gaines               Knowledge Science Institute\n                                University of Calgary\ngaines@cpsc.ucalgary.ca         Calgary, Alberta, Canada T2N 1N4\n403-220-5901  Fax:403-284-4707  http://ksi.cpsc.ucalgary.ca/KSI\n\n\n\n"
        },
        {
            "subject": "Vulnerabilities in digest-aa I",
            "content": "I've only spent about an hour looking at the most recent I-D (with some\nhelp from Eric Rescorla), but we've found quite a few problems in that\ntime. I think that digest-aa should not proceed at least until these\nproblems are addressed.\n\nThere are five vulnerabilities which struck us immediately:\n\n1) The gross structure of the digests allows for the exploitation of MD5\ncollisions. This is possibly not worth worrying about, since the best\nattack we can come up with requires effort on the order of 2^64 operations.\nIn general, the sharing of long common prefixes between the digests and the\nlack of secret or random material beyond the initial amount leads us to\nsuspect that there might be many other cryptoanalytic attacks we haven't\nthought of.\n\n2) The fine structure of the digests allows one to be substituted for\nanother. This allows for straightforward splicing and reflection attacks\nwhich undercut the rationale for the protocol. This could be fixed by\ninsisting that each digest type have some sort of type-distinguishing data\nor structure in them (there are three specified in the document: client\nplain digests, client \"message-digests\" [sic], and server \"message-digests\"\n[sic again]). Vulnerability to substitution is increased given the\none-sided and unstructured nature of the freshness material. One could\neasily arrange that the client always provided freshness material, and\ninsist that freshness have structure that the either side can count on (say\nthat it must monotonically increase).\n\n3) The \"optional-ness\" of the client message-digest and server\nmessage-digests means that neither can be used for authentication given a\ndowngrade attack (the attacker removes the digest and substitutes\nunauthenticated material).\n\n4) The fact that no headers are included in the digesting process combined\nwith the fact that HTTP headers change the semantics of requests (and\nreplies) means that authenticated requests and replies can be transformed\nby an attacker undetectably. For example, consider byte ranges where the\nauthorized request or only wants one portion of a document and the attacker\ntransforms the request into one for the entire document. This is difficult\nto fix while retaining the spirit of the proposal.\n\n5) There is no treatment of the security implications of retries and\nmultiple authorization headers. Absent this, I can imagine many flawed\nimplementation possibilities. Also, I think that it is assumed that this\nmechanism works for proxy authentication, and if this is permitted, new\nsorts of attacks are possible.\n\nOutside of these immediate security vulnerabilities, I wonder about the\nwisdom of wiring-in a single digest algorithm (the selection of MD5 could\neasily be parameterized with no damage to the spec). I also wonder about\nthe wisdom of referencing Dave Kristol's extension mechanism (sounds like\nwhat used to be called at PARC \"error 33\" -- making one risky project\ndependent on another).\n\nGiven the above, here's an off-the-top-of-my-head attempt at addressing\nthese vulnerabilities, while retaining as much spirit of the design as\npossible. It is an admittedly bad practice I am indulging in here -- this\nis not a thought-out design, it's only meant to illustrate fixes.\n\n=====================================\n\n1) The nonces are mandatory, and have the following structure:\n        <host-id><sep1><tod><sep2><integer>\nhost-id is the principal's DNS name or the \"realm\", I don't care.\ntod is seconds since Unix epoch in hex.\ndiscrim is a hex integer so that multiple nonces generated in a given\nsecond monotonically increase.\nI don't care what sep1 and sep2 are (slashes?).\nThis is so the principals can check for replay with finite memory.\nClients have nonces too.\n\n2) The client auth header is:\nAuthorization: Digest\n        algorithm=MD5,\n        username=\"<username>\",\n        realm=\"<realm>\",\n        snonce=\"<server-nonce>\",\n        cnonce=\"<client-nonce>\",     -- must be fresh\n        uri=\"<requested-uri>\",       -- [Why was this in the original I-D?]\n        request=\"<client-digest>\",\n        message=\"<message-digest>\",\n        opaque=\"<opaque>\"            -- required if provided by server\nwhere:\n        <client-digest> := H( H(A1) + CN + SN + OP + BI + H(H(A1) + A2) )\n        <message-digest> := H( H(A1) + <client-digest> + H(H(A1) + CB) )\nand:\n        A1 := 'MD5' + U + R + P\n        A2 := <Method> + <requested-uri>\n        BI := \"cbody\" | \"no cbody\"                      -- depending\n        OP := \"opaque\" + <opaque> | \"no opaque\"         -- depending\n        CB := \"cbody\" + <message-body> | \"no cbody\"     -- depending\nwith:\n        SN, CN -- server and client nonce values\n        U -- username\n        R -- realm\n        P -- password\n        <Method> -- entire request header line 0\n        <requested-uri> -- uri sans proxy/routing\n\n3) server response\nWhen authorization succeeds, the Server MUST provide the following:\n\nHTTP/1.1 200 OK\nAuthorization-Response: Digest\n        algorithm=MD5,\n        username=\"<username>\",\n        realm=\"<realm>\",\n        snonce=\"<server-nonce>\",\n        cnonce=\"<client-nonce>\",\n        response=\"<server-digest>\"\nwhere:\n        <server-digest> := H( H(A1) + CD + SB + H(H(A1) + <Response>))\nand with:\n        A1 as above\n        CD := <client-digest> -- from above\n        SB := \"sbody\" + <message-body> | \"no sbody\" -- depending\n        <Response> -- entire response header line 0\n\n4) This mechanism must be outlawed for \"Proxy-Authentication:\" or it we need\nto make the structure of A1 dependent on proxy vs. non-proxy use.\n\n5) The headers that change the effect of a request or response such as:\n        Range, Unless, If-Modified-Since\n(I'm worried that there are others) must either be outlawed when using\nDigest-Authentication, or these headers must be accounted-for in the\ndigests. Figuring-out which are the headers that have this property is a\nsurprisingly hard problem.\n\n6) Multiple Authorization headers are forbidden.\n\n7) Servers must either disregard the request line 0 URI (in favor of the\nuri field of the authorization header) or reject requests where these are\nnot identical. Even better would be to drop the uri field from the\nauthorization header.\n\n=====================================\n\nHappy Holidays,\n\n-Allan\n\n\n\n"
        },
        {
            "subject": "Caching data returned from POST, and conditional POS",
            "content": "Brian Gaines writes:\n > I noticed that Netscape (2.0b4) sends a conditional POST when data\n > cached that was returned from a POST expires.\n > \n\nWhat does that mean?  A POST with if-modified-since?  I don't believe\nthis is has been defined, and it doesn't really make sense unless the\nif-modified-since only applies to the returned data, not to the action\nof the POST, since POST can have side effects at the server, so doing\ndoing it twice is not the same as doing it once.\n\n\n > However, draft-ietf-v10-spec-04 p.27 says \"Applications must not cache responses\n > to a POST request\", and draft-ietf-v11-spec-00 p35. says \"Responses to this\n > method are not cachable\".\n > \n > I think this is a case where both drafts are clearly incorrect.\n > \n\nDepending on the interpretation, I either agree or not.\nIf \"to cache\" is interpreted as \"to store into a cache\", then I agree\nthat the existing specs are incorrect.  If \"to cache\" means \"to\nreturn a document from the cache\", then I believe the spec is OK.\n\n\n > The logic for this is that the GET and POST methods are logically\n > indistinguishable. Both send a request with arguments and result in\n > the return of data.\n\nBut there is a convention that GET isn't supposed to have side\neffects, but POST can.  Without that convention, no caching (in the\nsense of \"returning documents from a cache\" is possible, since there\nwould be no way to tell by looking at a GET request whether it will\nhave side effects or not.  Because of the presumption of no side\neffects, GET and HEAD can be served from a cache, while POSTs cannot.\nHowever, in my opinion the returned results from GET and POST can both be\nstored in a cache, under control of the appropriate headers, and I\nhave strong reasons to believe that is current practice in most browsers.\nI think this last issue may require some discussion in the caching subgroup.\n\n An end user sees no difference between them and expects\n > the \"Back\" and \"Forward\" buttons to act in the normal manner in retrieving\n > cached data. That is, a browser MUST cache the results of a POST if it is\n > to behave correctly.\n > \n\nNo, you're confusing history functions of the browser with caching.\nBACK and FORWARD can show you documents that are stored by your\nbrowser, but are not necessarily the same set of documents as are in\nyour cache.  For instance, your history list may contain several\nversions of the same document, e.g. a stock quote report you've asked\nfor every 15 minutes for the past several hours.  In addition,\nalthough these documents are in your history list, some or all of them\nmay be \"stale\" (marked as \"expired\"), but BACK will show you them\nanyway -- or should!  The cache comes into play when you make a fresh\nrequest, by entering a URL, clicking on a hyperlink, or submitting a\nform.  In this case the cache decides if it can simply present the\ndocument it already has at hand, or if it must fetch a new copy.  POST\nrequests always have to go through to the origin server, because of\nthe possibility of server side effects.  Of the existing methods in\nHTTP, only GET and HEAD can be served from a cache without the\nnecessity of contacting the origin server.  OTOH, the headers in the\nresource returned by GET or POST are sufficient to control whether\nthat returned resource can be \"cached\" -- stored into a cache so that\nlater GETs can get that document from the cache.  (Again, this last\nstatement is apparently not fully agreed upon).\n\n\n > The server has ample control over caching at the browser or by proxies\n > and can use this to ensure that such caching is not problematic to a\n > transaction sequence that changes the state of data at the server.\n > \n > There are many POST-based transaction sequences that remain stateless\n > at the server by using hidden fields to pass state information back to the\n > client. These sequences rely on browser caching to allow the \"Back\" command\n > to be used as an \"Undo\".\n > \n\nDon't get me started on hidden fields.  They're about the worst way to\nhandle state.  First off, not all browsers hide them, and some of\nthose that don't make them editable, guaranteeing that some loser will\nedit them.  Secondly, precisely because using BACK loses this state,\nit tends to be confusing to users.  If hidden fields must be used at\nall for state handling, they should only be used extremely locally -- from one\npage to the next.\n\n\n > I propose that the statements about caching be removed from both specifications.\n > \n\nThe caching subgroup should discuss this.  I hadn't noticed these\nstatements in the spec before.  Thanks for pointing them out.\n\n > I propose that conditional POST have the same status as conditional GET.\n > It is harmless to existing practice since servers can continue to take no\n > notice of the \"If-Modified-Since\" field for a POST if they wish.\n > \n > b.\n > \n > Dr Brian R Gaines               Knowledge Science Institute\n >                                 University of Calgary\n > gaines@cpsc.ucalgary.ca         Calgary, Alberta, Canada T2N 1N4\n > 403-220-5901  Fax:403-284-4707  http://ksi.cpsc.ucalgary.ca/KSI\n > \n > \n\nShel Kaphan\nsjk@amazon.com\n\n\n\n"
        },
        {
            "subject": "Re: Caching data returned from POST, and conditional POS",
            "content": "Shel Kaphan writes\n> Brian Gaines writes\n> > I noticed that Netscape (2.0b4) sends a conditional POST when data\n> > cached that was returned from a POST expires.\n> >\n>\n>What does that mean?  A POST with if-modified-since?  I don't believe\n>this is has been defined, and it doesn't really make sense unless the\n>if-modified-since only applies to the returned data, not to the action\n>of the POST, since POST can have side effects at the server, so doing\n>doing it twice is not the same as doing it once.\n>\n\nI think it is clear what it means. The server should be able to identify\nthe data it sent back from a POST has changed in the same way as it does\nthat from a GET. It is up to the server to keep track of its state\nchanges in doing this.\n\nAs a general comment on your responses, I think it inappropriate for\nthe HTTP protocol itself to attempt to take account of server state.\nIt should provide the means for the server to control the browser so\nthat it can manage user interaction properly, but should not attempt\nto enforce arbitrary models of what is a general client-server system.\n\nThere are many different ways of handling state and contention in\nclient-server systems and the appropriate mechanism varies widely\naccording to the application. The HTTP specification should allow\nthe system designer to design the most effective system. It should give\ncontrol over caching, not say it should not occur when it clearly\nshould and clearly does.\n\n> An end user sees no difference between them and expects\n> > the \"Back\" and \"Forward\" buttons to act in the normal manner in retrieving\n> > cached data. That is, a browser MUST cache the results of a POST if it is\n> > to behave correctly.\n> >\n>\n>No, you're confusing history functions of the browser with caching.\n>BACK and FORWARD can show you documents that are stored by your\n>browser, but are not necessarily the same set of documents as are in\n>your cache.  For instance, your history list may contain several\n>versions of the same document, e.g. a stock quote report you've asked\n>for every 15 minutes for the past several hours.  In addition,\n>although these documents are in your history list, some or all of them\n>may be \"stale\" (marked as \"expired\"), but BACK will show you them\n>anyway -- or should!  The cache comes into play when you make a fresh\n>request, by entering a URL, clicking on a hyperlink, or submitting a\n>form.  In this case the cache decides if it can simply present the\n>document it already has at hand, or if it must fetch a new copy.  POST\n>requests always have to go through to the origin server, because of\n>the possibility of server side effects.  Of the existing methods in\n>HTTP, only GET and HEAD can be served from a cache without the\n>necessity of contacting the origin server.  OTOH, the headers in the\n>resource returned by GET or POST are sufficient to control whether\n>that returned resource can be \"cached\" -- stored into a cache so that\n>later GETs can get that document from the cache.  (Again, this last\n>statement is apparently not fully agreed upon).\n\nWhat you describe would be very bad practice. I want the browser to\ncheck with the server when the user uses \"Back\" and \"Forward\" so that\nthe server can ensure that the data is up to date. Netscape does precisely\nthis and enables the user interface to be kept in sync with the state\nof the server.\n\n>Don't get me started on hidden fields.  They're about the worst way to\n>handle state.  First off, not all browsers hide them, and some of\n>those that don't make them editable, guaranteeing that some loser will\n>edit them.  Secondly, precisely because using BACK loses this state,\n>it tends to be confusing to users.  If hidden fields must be used at\n>all for state handling, they should only be used extremely locally -- from one\n>page to the next.\n>\n\nWe have major interactive applications that use hidden fields to carry\nstate data effectively. I detailed the advantages of this in the\n\"Porting\" tutorial at WWW4 <http://ksi.cpsc.ucalgary.ca/articles/WWW/PortWeb>.\n\nThe major browsers all deal correctly with hidden fields and minor ones\nthat do not will presumably be debugged to do so.\n\nStoring state in hidden fields make it possible for \"Back\" to act as \"Undo\"\nwhich is what users expect. When the \"Undo\" is impossible or ambiguous\nthe server can query the user in the normal way for a transaction-processing\nsystem.\n\nI guess the generic point is to avoid building specific TP architectures\ninto HTTP. It is a general transport and control protocol capable of supporting\nmany TP architectures.\n\nb.\n\nDr Brian R Gaines               Knowledge Science Institute\n                                University of Calgary\ngaines@cpsc.ucalgary.ca         Calgary, Alberta, Canada T2N 1N4\n403-220-5901  Fax:403-284-4707  http://ksi.cpsc.ucalgary.ca/KSI\n\n\n\n"
        },
        {
            "subject": "Re: Caching data returned from POST, and conditional POS",
            "content": "I think your comments on the cachability of POST requests are\nwell-taken. I would guess that the v10 specification, as an\nInformational RFC describing 'current' practice, should soften the\nwording to describe what current clients and servers actually DO.\n\nAs for HTTP/1.1, there's a cache subgroup that will be meeting, and\nperhaps they can deal with caching of other methods. (My guess is that\nthe 'method' and attached data can just be added to the set of input\nparameters that might affect the output of a HTTP call, and that the\nprimary difference between GET and POST is the presumption of Expires:\nfor POST is 'immediate'.)\n\n\n\n"
        },
        {
            "subject": "Re: Caching data returned from POST, and conditional POS",
            "content": "Larry Masinter writes\n>As for HTTP/1.1, there's a cache subgroup that will be meeting, and\n>perhaps they can deal with caching of other methods. (My guess is that\n>the 'method' and attached data can just be added to the set of input\n>parameters that might affect the output of a HTTP call, and that the\n>primary difference between GET and POST is the presumption of Expires:\n>for POST is 'immediate'.)\n\nIs there any need for such a presumption since the server has 100% control\nover what it sends in the Expires: field?\n\nNetscape Navigator 2.0b4 does take note of the Expires: field, and reposts\nthe request. This was how I noted that it makes the request under these\ncircumstances a conditional POST.\n\nWe could certainly do with a very clearly defined architecture for caching\nat proxy servers and browsers so that the intended usage of each controlling\nfield is well-defined. It should be as general as possible to\nsupport a wide range of client-server architectures. The essence is to\nprovide the information through the protocol that enables servers and\nbrowsers to coordinate effectively.\n\nSince the designer of a web-based transaction processing system has total\ncontrol at the server end but will be using commercial browsers, it is\nimportant in the specification to put most of the design flexibility at\nthe server. The browser should be a well-defined automaton responding\nto information from the server. This still leaves scope for user control\nat the browser if required, e.g. turning off browser interaction with the\nserver for \"Back\" if required so that a past, and possibly inconsistent,\nstate can be inspected as Shel suggested. However, as is the situation\nwith local control of typography now, most users will not be that\nsophisticated and will prefer to use a system that behaves as the designer\nspecifies.\n\nb.\n\n\nDr Brian R Gaines               Knowledge Science Institute\n                                University of Calgary\ngaines@cpsc.ucalgary.ca         Calgary, Alberta, Canada T2N 1N4\n403-220-5901  Fax:403-284-4707  http://ksi.cpsc.ucalgary.ca/KSI\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "According to Henrik Frystyk Nielsen:\n> \n> The multithreaded feature in the Arena is a result of the new\n> achitecture of the WWW Library of Common Code. It allows data-driven\n> parsing or rendering as soon as data comes in on the network, and new\n> requests to be issued at any time. So rendering GIFS before finishing\n> the text is perfectly possible.\n> \n\nSounds very nice! However, as as been pointed out several times, the\nreason Netscape is popular is the *reverse*.  It can layout all the\ntext before rendering all the GIFS.  Presumably arena can do this too,\nif it can get the information about GIF sizes.  But the ability to do\n*that* is a function of the protocol and hence relevant to the topic\nof this list.  :)\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authenticatio",
            "content": "Actually, I have one message archived that indicates that MD5 does\ncome under ITAR - that all crypto comes under ITAR.\n\nI used to have another message tucked away, saying that authentication\ncame under ITAR, but was far easier to get past the review, than is\nencryption.  Yes, there is Much misinformation flying about ITAR - the\nmessages I've seen in the past could be wrong, while Alan's is\ncorrect.\n\nIf MD5 is used for auth, MD5 isn't just MD5 anymore - it's not just\ndigests, it's authentication.  Now the US goverment can be kind of\nwacked, but in an ideal world (gov't) they will judge a system based\non the purpose to which the algorithms are being put - not the purpose\nfor which the algorithms were originally intended.\n\nNote that it's also easy to turn MD5 into an encryption system, not\nsolely into an authentication algorithm.  This could explain why some\nhave said that MD5 Is subject to ITAR.  Again, however, since the\nintended purpose is only auth, it should not be subjected to\nencryption-style scrutiny, when reviewed for export.\n\n\"Snefru\" is widely available, tho probably not widely used, and uses\nMD5 for authentication.  The US goverment does not appear to have gone\nafter the author.  I wrote a system using MD5 for auth, inspired by\nsnefru, but I have no intention of allowing it off campus, at this\npoint.  :(\n\nAll in all, ITAR just needs to die, or at least be thoroughly\nclarified and weakened.  ITAR quite simply shackles US producers of\ncryptography (and hence, cryptography-utilizing software), while\nleaving numerous other countries running unfettered.  Many times, I've\nconsidered writing cryptographic systems to give away on the net, and\nconcluded I should allow someone outside the US to do it, so everyone\ncould have access to it.  Investors in crypto almost have to feel\nsimilarly (tho I personally don't care about commercial software as\nmuch as the body of free software out there).\n\nPlease prove me wrong.  I wanna be wrong on this one.\n\nIn message <v02130500ad0b76285e77@[205.226.39.192]>you write:\n>On Sat, 30 Dec 1995, Andrew Cameron wrote:\n\n>\n>>Will this be available to people outside the US, or will the ITAR\n>>regulations mean that only those in the US can legally use it.\n>\n>Putting Albert Lunde's point more emphatically: since Digest Access\n>Authentication does not provide confidentiality (doesn't use encryption)\n>it doesn't fall under ITAR at all.\n>\n>-Allan\n>\n>\n\nDan Stromberg - OAC/DCS                         strombrg@uci.edu\n\n\n\n"
        },
        {
            "subject": "Re: Digest Authenticatio",
            "content": "Just to reiterate: Keyed-MD5 and in general all technologies that are \nused soley for authentication and not privacy, are not treated as \nmunitions under 121.1.XIII.b. I'm not a lawyer, but this is what I've \nbeen told by lawyers.\n\n There was a thread on this subject on cypherpunks about 6-8 weeks ago - \ntake a look at the archives for more citations on this.\n\nSimon\n\n\n\n(defun modexpt (x y n)  \"computes (x^y) mod n\"\n  (cond ((= y 0) 1) ((= y 1) (mod x n))\n((evenp y) (mod (expt (modexpt x (/ y 2) n) 2) n))\n(t (mod (* x (modexpt x (1- y) n)) n))))\n\n\n\n"
        },
        {
            "subject": "Re: Caching data returned from POST, and conditional POS",
            "content": "Brian Gaines writes\n Shel Kaphan writes\n > Brian Gaines writes\n > > I noticed that Netscape (2.0b4) sends a conditional POST when data\n > > cached that was returned from a POST expires.\n > >\n >\n >What does that mean?  A POST with if-modified-since?  I don't believe\n >this is has been defined, and it doesn't really make sense unless the\n >if-modified-since only applies to the returned data, not to the action\n >of the POST, since POST can have side effects at the server, so doing\n >doing it twice is not the same as doing it once.\n >\n\n I think it is clear what it means. The server should be able to identify\n the data it sent back from a POST has changed in the same way as it does\n that from a GET. It is up to the server to keep track of its state\n changes in doing this.\n\nSo far, the only conditional allowed in HTTP is GET with\nif-modified-since.  Perhaps you're referring to the fact that in\nresponse to BACK and FORWARD commands, Netscape allows the user to OK\na new send of a POST when it no longer has the data (or when it has\nexpired, which is against the rules in the spec, by the way).  Please\ntake a look at the description of the Expires header in the spec.\nThere's a short disclaimer about interactions with browser history\nfunctions.\n\n\n As a general comment on your responses, I think it inappropriate for\n the HTTP protocol itself to attempt to take account of server state.\n\nI agree, except insofar as there are already necessary conventions\nwhich are somewhat implicit and have to do with caching, as I already\nmentioned (GET & HEAD are \"servable from a cache\", other methods,\nnot).\n\n It should provide the means for the server to control the browser so\n that it can manage user interaction properly, but should not attempt\n to enforce arbitrary models of what is a general client-server system.\n\nDid I seem to suggest otherwise?\n\n There are many different ways of handling state and contention in\n client-server systems and the appropriate mechanism varies widely\n according to the application. The HTTP specification should allow\n the system designer to design the most effective system. It should give\n control over caching, not say it should not occur when it clearly\n should and clearly does.\n\nYes, however you might want to take a look at the various state-management\nproposals floating around.  For instance, the Netscape \"cookie\"\nproposal, and Dave Kristol's \"state-info\" proposal.  There is likely\nto be some convergence between these two approaches, and it is likely\nthat the protocol *will* address state handling, but of course will\nnot enforce that you use that mechanism if you want to do it some\nother way.\n\n > An end user sees no difference between them and expects\n > > the \"Back\" and \"Forward\" buttons to act in the normal manner in retrieving\n > > cached data. That is, a browser MUST cache the results of a POST if it is\n > > to behave correctly.\n > >\n >\n >No, you're confusing history functions of the browser with caching.\n >BACK and FORWARD can show you documents that are stored by your\n >browser, but are not necessarily the same set of documents as are in\n >your cache.  For instance, your history list may contain several\n >versions of the same document, e.g. a stock quote report you've asked\n >for every 15 minutes for the past several hours.  In addition,\n >although these documents are in your history list, some or all of them\n >may be \"stale\" (marked as \"expired\"), but BACK will show you them\n >anyway -- or should!  The cache comes into play when you make a fresh\n >request, by entering a URL, clicking on a hyperlink, or submitting a\n >form.  In this case the cache decides if it can simply present the\n >document it already has at hand, or if it must fetch a new copy.  POST\n >requests always have to go through to the origin server, because of\n >the possibility of server side effects.  Of the existing methods in\n >HTTP, only GET and HEAD can be served from a cache without the\n >necessity of contacting the origin server.  OTOH, the headers in the\n >resource returned by GET or POST are sufficient to control whether\n >that returned resource can be \"cached\" -- stored into a cache so that\n >later GETs can get that document from the cache.  (Again, this last\n >statement is apparently not fully agreed upon).\n\n What you describe would be very bad practice.\n\nWhich thing I describe?  Except for the very last point, this is what\nthe current spec calls for.  The problem, I feel, is that there are\nmultiple incompatible interpretations floating around of the WWW\n\"computing model\" as enshrined in both browsers and protocols.  \n\n I want the browser to\n check with the server when the user uses \"Back\" and \"Forward\" so that\n the server can ensure that the data is up to date. Netscape does precisely\n this and enables the user interface to be kept in sync with the state\n of the server.\n\nAgain, please look at the description of Expires in the spec.  You\nmight also want to look at Koen Holtman's and my report from some\nmonths ago in http://www.amazon.com/expires-report.{html,txt} which\ndiscusses some of these issues.\n\n >Don't get me started on hidden fields.  They're about the worst way to\n >handle state.  First off, not all browsers hide them, and some of\n >those that don't make them editable, guaranteeing that some loser will\n >edit them.  Secondly, precisely because using BACK loses this state,\n >it tends to be confusing to users.  If hidden fields must be used at\n >all for state handling, they should only be used extremely locally -- from one\n >page to the next.\n >\n\n We have major interactive applications that use hidden fields to carry\n state data effectively. I detailed the advantages of this in the\n \"Porting\" tutorial at WWW4 <http://ksi.cpsc.ucalgary.ca/articles/WWW/PortWeb>.\n\nI'm familiar with the approach -- to the extent I have tried to use\nit, I have had to back out for the reasons mentioned.  On the other\nhand, I'm not trying to stop you from using it if you want to, and\nthere's nothing in the spec that talks about it, nor should there be.\n\n The major browsers all deal correctly with hidden fields and minor ones\n that do not will presumably be debugged to do so.\n\nThere's enough users of minor ones that the commercial services I have\nworked with have had to eliminate all uses of hidden fields due to data\ncorruption caused by inadvertent user editing, because the mistakes\nare too expensive to clean up after the fact.\n\n Storing state in hidden fields make it possible for \"Back\" to act as \"Undo\"\n which is what users expect.  When the \"Undo\" is impossible or ambiguous\n the server can query the user in the normal way for a transaction-processing\n system.\n\nSorry, but most users do *not* equate the BACK button with \"undo\", and\nin fact, most naive users don't know there's a difference between a\nlink that says \"go back\" and using the browser's BACK button, and to\nthe extent it is possible to preserve the lack of requirement for\nusers to know how these things work, I think it should be preserved.\n\n I guess the generic point is to avoid building specific TP architectures\n into HTTP. It is a general transport and control protocol capable of supporting\n many TP architectures.\n\nI agree.\n\n b.\n\n Dr Brian R Gaines               Knowledge Science Institute\n University of Calgary\n gaines@cpsc.ucalgary.ca         Calgary, Alberta, Canada T2N 1N4\n 403-220-5901  Fax:403-284-4707  http://ksi.cpsc.ucalgary.ca/KSI\n\n\ncheers, happy new year, \n\nShel Kaphan\nsjk@amazon.com\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Frank,\n\n> According to Henrik Frystyk Nielsen:\n> > \n> > The multithreaded feature in the Arena is a result of the new\n> > achitecture of the WWW Library of Common Code. It allows data-driven\n> > parsing or rendering as soon as data comes in on the network, and new\n> > requests to be issued at any time. So rendering GIFS before finishing\n> > the text is perfectly possible.\n> > \n> \n> Sounds very nice! However, as as been pointed out several times, the\n> reason Netscape is popular is the *reverse*.  It can layout all the\n> text before rendering all the GIFS.  Presumably arena can do this too,\n> if it can get the information about GIF sizes.  But the ability to do\n> *that* is a function of the protocol and hence relevant to the topic\n> of this list.  :)\n\nIf no information on the size is given in the link, then it _is_ the\nsame thing. Before you then can get the size of the image, you need to\ndownload the first part of it giving its size, that is issue new\nrequests at any time when parsing through the text file. Remember, I am\ntalking from a programmer interface's viewpoint, not a user interface's\nviewpoint. The only `real' user interface to me is the Line Mode\nBrowser ;-)\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "/*\n * \"Re: Connection Header\" by frystyk@ptsun00.cern.ch (Henrik Frystyk Nielsen)\n *    written Sat, 17 Dec 94 16:54:07 +0100\n * \n * The MGET causes an extra roundtrip time as many servers will think:\n *\n * \"Gee - nobody has told me about this method. I simply refuse\n * to do anything about it\"\n\nIt seems to me that this could be alleviated by having regular GET\nmethods on MGET-capable servers return\n\nAllow: MGET\n\nso that the client, after the first GET, knows whether it should send\nMGET or GET.\n\n * This is the reason for waiting with a SESSION method to HTTP/1.1 as\n * Roy points out. Also we would like a more general approach than\n * only having GET as a possibility. A SESSION method can also be used\n * for interactive sessions.\n */\n\nDoes the SESSION method you propose allow for servers to mix pieces of\nresponses to different requests together like HTTP-NG?\n\n--Rob\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": ">  * The MGET causes an extra roundtrip time as many servers will think:\n>  *\n>  * \"Gee - nobody has told me about this method. I simply refuse\n>  * to do anything about it\"\n> \n> It seems to me that this could be alleviated by having regular GET\n> methods on MGET-capable servers return\n> \n> Allow: MGET\n\nMGET is definitely more realistic approach than the keep-connection\nproposal.  MGET is clean and fully backword compatible.\n\nIf I'm not totally lost in space, keep-connection will not even work\nwith all TCP implementations.  There is no way to know beforehand that\nthe remote really supports keep-connection and that the connection\nreally will stay up, without making an assumption that it will, and\nsend the second request to try it out.  You can't even wait until the\nentire document has been transferred and see if the connection stays\nup, because with a congested network or loaded remote server we may\nsee the connection staying up for a while before it actually closes.\nSame may happen also with CGI scripts that do some cleanup after the\ndocument has already been fully returned.  This is a fact with current\nservers out there, which are the ones *not* supporting MGET, and with\nwhich the problems I'll now explain, would happen.\n\nAs we are parsing the HTML doc and come accross an inlined image we\nissue a new request to the socket.  If we do that while we are still\nreading the data, and if the remote doesn't support keep-connection,\nwe will receive ECONNRESET (?) and all pending incoming data will be\ndiscarded.  The same thing sometimes happened with HTTP0/HTTP1 (which\nwas supposed to be fully backword compatible, but wasn't really), NCSA\nMosaic did another connection, other clients simply failed, or\ndisplayed partial data.  This was fully dynamic depending on the state\nand speed of the network, sometimes you would end up with the entire\ndocument, sometimes with a truncated one, and sometimes with an empty\npage or client-generated error message when no data was returned.\nThis was when the HTTP1 header write was still going on while the\nremote had actually sent all the data and closed the connection.  Data\nwas streaming in from the remote TCP kernel buffers, and when the\nwrite failed on client side all pending data was lost.\n\nI don't see any other way to do this in a backword compatible way than\nwith MGET and accept the fact that we make two connections (which\nwould still be a huge improvement to the current situation).\n\nCheers,\n--\nAri Luotonenhttp://home.mcom.com/people/ari/\nNetscape Communications Corp.\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "On Sat, 17 Dec 1994, Ari Luotonen wrote:\n\n> If I'm not totally lost in space, keep-connection will not even work\n> with all TCP implementations.  There is no way to know beforehand that\n\nWell, I must admit I've thought about the problem rather than having \nhad to implement, but if the TCP/IP connection can't be kept open then\nall kinds of things are eventually going to break .... like simple\nTELNET connections into that host.  The CGI scripts should be fixed\nif necessary to avoid behaviors which close the connection.  It could\nalso be possible for a server to be paramatized to refuse keepalive\nwhen certain child processes are invoked. The headers sent with each\nresponse should confirm the intent to continue keeping the connection\nopen so the server can tell the client when to expect the current\ntransmission to end the connection.  There is nothing wrong with MGET\nthat would precluding implementing it that I know of but it will require\nmore code changes in clients and servers than simply keeping a connection\nopen.  The connection open can be tuned to serve many documents if\ndesired and for a proxy/server connection can remain open over a very\nlong interval if desired potentially handling many requests. (There\nare security issues but then caching satisfied by a proxy has the same\nproblem since cache serving is logically a long proxy/server connection.)\n\nTo solve the oldproxy new server problem, a new hack in the server would\nprovide a workaround by allowing the admin to specify proxys who can't\nhandle open connections.  The reply would go back as today.\n\nBut as has been suggested some real experience to back up the simulations\nSimon and Jeff have reported would be helpful.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Rob McCool <robm@neon.mcom.com> wrote:\n\n> /*\n>  * \"Re: Connection Header\" by frystyk@ptsun00.cern.ch (Henrik Frystyk Nielsen)\n>  *    written Sat, 17 Dec 94 16:54:07 +0100\n>  * \n>  * The MGET causes an extra roundtrip time as many servers will think:\n>  *\n>  * \"Gee - nobody has told me about this method. I simply refuse\n>  * to do anything about it\"\n> \n> It seems to me that this could be alleviated by having regular GET\n> methods on MGET-capable servers return\n> \n> Allow: MGET\n> \n> so that the client, after the first GET, knows whether it should send\n> MGET or GET.\n> \n\nMGET and Connection header are not synonyms, The Connection header is\nmore general than the MGET method as the connection can be used for any\nmethod, for example alternating POST and GET.\n\nUsing Allow has the same problem as if a Connection header is passed\nthrough a proxy. Imagine the following setup\n\nnew client      ->      old proxy       ->      new server\n\nOn the first request, the new server will then send back a\n\nAllow: GET, MGET, HEAD, etc.\n\nOn the second request, the new client will try a MGET, but the proxy\ndoesn't understand this and then we are back to the old problem having\nlost one roundtrip time. UNDER the assumption that the Proxy server\nALWAYS picks out the Connection header, this problem will not exist.\nOtherwise the proxy needs to strip out all methods that it doesn't\nunderstand from the Allow header. I think the first solution is far\neasier.\n\n>  * This is the reason for waiting with a SESSION method to HTTP/1.1 as\n>  * Roy points out. Also we would like a more general approach than\n>  * only having GET as a possibility. A SESSION method can also be used\n>  * for interactive sessions.\n>  */\n> \n> Does the SESSION method you propose allow for servers to mix pieces of\n> responses to different requests together like HTTP-NG?\n\nNo, in order to do this you need a session layer managing the virtual\nconnections. I think we should leave this for HTTP-NG. This approach\nonly allows the server to serve the responses in serial. For the user\nit might not be as `efficient' as opening several TCP connections, but\nit prevents servers from melt down in which case the user has lost\nanyway.\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "> >  * The MGET causes an extra roundtrip time as many servers will think:\n> >  *\n> >  * \"Gee - nobody has told me about this method. I simply refuse\n> >  * to do anything about it\"\n> > \n> > It seems to me that this could be alleviated by having regular GET\n> > methods on MGET-capable servers return\n> > \n> > Allow: MGET\n> \n> MGET is definitely more realistic approach than the keep-connection\n> proposal.  MGET is clean and fully backword compatible.\n\nIf you read my answer to Rob you will find that MGET is _not_ fully\nbackward compatible.\n\n> If I'm not totally lost in space, keep-connection will not even work\n> with all TCP implementations.\n\nNo need to get lost ;-)\n\n> There is no way to know beforehand that\n> the remote really supports keep-connection and that the connection\n> really will stay up, without making an assumption that it will, and\n> send the second request to try it out.  You can't even wait until the\n> entire document has been transferred and see if the connection stays\n> up, because with a congested network or loaded remote server we may\n> see the connection staying up for a while before it actually closes.\n> Same may happen also with CGI scripts that do some cleanup after the\n> document has already been fully returned.  This is a fact with current\n> servers out there, which are the ones *not* supporting MGET, and with\n> which the problems I'll now explain, would happen.\n\nThis is not a problem as the server sends back a new Connection header\nin the first response, for example indicating how long the connection\nwill stay open and how many requests the client can issue. If this is\nnot present then the client should not send any more down the same\npipe. This means that the client should not send multiple requests the\nfirst time unless it knows apriori that the server does in fact accept\nit. In my opinion a minor limitation!\n \n> As we are parsing the HTML doc and come accross an inlined image we\n> issue a new request to the socket.  If we do that while we are still\n> reading the data, and if the remote doesn't support keep-connection,\n> we will receive ECONNRESET (?) and all pending incoming data will be\n> discarded.  The same thing sometimes happened with HTTP0/HTTP1 (which\n> was supposed to be fully backword compatible, but wasn't really), NCSA\n> Mosaic did another connection, other clients simply failed, or\n> displayed partial data.  This was fully dynamic depending on the state\n> and speed of the network, sometimes you would end up with the entire\n> document, sometimes with a truncated one, and sometimes with an empty\n> page or client-generated error message when no data was returned.\n> This was when the HTTP1 header write was still going on while the\n> remote had actually sent all the data and closed the connection.  Data\n> was streaming in from the remote TCP kernel buffers, and when the\n> write failed on client side all pending data was lost.\n\nThe problem here is simply that the TCP window fills up with\nunacknowledged bytes and hence the 0.9 server is forced to reset the\nconnection. Otherwise there would be a deadlock: both client and server\ntries to write, but the client will not do a read. This is still a\nproblem for proxies as there is no way of changing the status code on\nthe fly when piping the response to the proxy client. If the client\ndoesn't check the Content-Length it ends up with a partial document\nwith a 200 OK code!\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and DigestMessageDiges",
            "content": "John:\n\nAre you just talking about D-MD, or Digest Auth for \nProxy-Authentication and Proxy-Authorization as well?\n\nIf the latter, I only made the proposals because of the 1.1 spec on\nProxy-Authentication and Proxy-Authorization -- my reading of it is \nthat an HTTP\nauthentication scheme should work identically for both proxy and \nend-to-end (by this I\nmean use the same challenge and response formats).\n\nFor example, the definition of the WWW-Authenticate header from 1.1 is\nWWWAuthenticate = \"WWW-Authenticate\" \":\" challenge\nand\nProxyAuthentication = \"Proxy-Authentication\" \":\" challenge\nsnd similarly for Authorization and Proxy-Authorization.\n\nIf it's the former (just D-MD), then if you're saying that you want to \nwithdraw D-MD in it's\nentirety as a big change, when made to work correctly, then I couldn't \nargue with you.\nIt's a new feature for which no implementations exist, and it \ncan/should be discussed as to appropriateness at the WG meeting.\n\nBut if D-MD is going to be included, then I think there's a very strong \nargument that it\nshould work with proxies, otherwise pretty soon it won't work at all \nfor a huge number of clients, whose corporations or ISPs will be \ncausing them to go through proxies.  There are other approaches than \nthe one I suggested to make D-MD work when going through proxies -- \ntunneling comes to mind, and maybe putting Pragma: no-cache or \nequivalent on requests and responses -- but the details would need to \nbe worked out.\n\nIn any case, it's not obvious how D-MD is intended to be used (or \nmusn't be used) if there are proxies, and something should be said if \nD-MD stays in the draft.\n\nIf you're saying that you want to defer any digesting of headers, (but \nkeep <message-digest>)  I'm happy to have a draft that doesn't include \nthem... I believe my response to the issues raised there was correct, \nbut I've heard nothing back on it, so given the deadline, I'm happy to \nwait until there's more discussion, and perhaps include it in a later \nrevision of the draft.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and DigestMessageDiges",
            "content": "On Thu, 29 Feb 1996, Paul Leach wrote:\n\n> Are you just talking about D-MD, or Digest Auth for \n> Proxy-Authentication and Proxy-Authorization as well?\n> \n\nDigest-MessageDigest has been part of the draft since its very early\nversions.  It has limitations. I don't think we are in a position\nto either remove it or overcome its limitations.  The new nextnonce\nfield seems to me to be a useful addition which is is a very modest\nchange and not likely to lead to any unpleasant surprises.  I also\nagree with Paul that there is not much reason to keep the user, nonce\nand realm fields.  In the fullness of time we can and will create\nstronger ways of dealing with authentication, proxies, headers, etc.\n\nI propose that the D-MD section of this draft be:\n\n--------------------------------------\n\nHTTP/1.1 200 OK\nDigest-MessageDigest:\n              message=\"<message-digest>\",\n              nextnonce=\"<nextnonce>\"\n\n   The Digest-MessageDigest header indicates that the server\n   wants to communicate some information regarding the\n   successful authentication (such as a message digest or a\n   new nonce to be used for the next transaction).\n\n   <message-digest> is computed by the same algorithm given\n   above for the body of the client request.  This allows the\n   client to verify that the body of the response has not been\n   changed en-route.  The server would probably only send this\n   when it has the document and can compute it.  The server would\n   probably not bother generating this header for CGI output.\n\n   <nextnonce> is the nonce the server wishes the client to use for\n   the next authentication response.  Either field is optional.  In\n   particular the server may send the Digest-MessageDigest header\n   with only the nextnonce=<nextnonce> field as a means of\n   implementing one-time nonces.  If the nextnonce field is present\n   the client is strongly encouraged to use it for the next\n   WWW-Authenticate header.  Failure of the client to do so may\n   result in a request to re-authenticate from the server with \n   the \"stale=TRUE.\"\n\n   The Digest-MessageDigest header has many limitations.  Only the\n   entity body is digested not any headers.  This limitation is due\n   to the fact that proxy caches may (and do) alter the headers of\n   documents which they relay. Future authentication schemes will\n   have to deal with the complexities imposed by the behavior of\n   intermediaries handling documents on their way from the origin\n   server to the client, but those issues are beyond the scope of\n   digest authentication whose purpose is to replace Basic\n   Authentication.  Despite its limitations the Digest-MessageDigest\n   can be useful.\n\n--------------------------------------\n\nThe full text of the latest draft is at\n\nhttp://hopf.math.nwu.edu/~john/new_rfc.txt\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "4.2 Accept-Charset\n\nI think last sentence of first paragraph should be written as\n\"The ISO-8859-1 character set can be assumed to be acceptable\nto all user agents.\". Rationale: per HTTP/1.1 draft\n(section 3.7.1) entity body without explicit charset can be\nUS-ASCII only or ISO-8859-1. Thus any conforming user agent must\nbe able to handle ISO-8859-1.\n\n4.6 Alternates\n\nCan media-type contain charset? Is this a valid exmaple?\n\nAlternates: {\"TheProject.fr.html\" 1.0\n      {type \"text/html\"} {language \"fr\"}},\n    {\"TheProject.en.html\" 1.0\n      {type \"text/html\"} {language \"en\"}},\n    {\"TheProject.ru.html\" 1.0\n      {type \"text/html;charset=iso-8859-5\"} {language \"ru\"}}\n    (\"/cgi-bin/xlate?koi8-r+TheProject.ru.html\" 1.0\n      {type \"text/html;charset=koi8-r\"} language \"ru\"}}\n\n5.1 Reactive negotiation\n\nIf two alternates are differ by charset only, how\nspecify preferred one?\n\nThanks\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "> \n> 4.2 Accept-Charset\n> \n> I think last sentence of first paragraph should be written as\n> \"The ISO-8859-1 character set can be assumed to be acceptable\n> to all user agents.\". Rationale: per HTTP/1.1 draft\n> (section 3.7.1) entity body without explicit charset can be\n> US-ASCII only or ISO-8859-1. Thus any conforming user agent must\n> be able to handle ISO-8859-1.\n> \n> 4.6 Alternates\n> \n> Can media-type contain charset? Is this a valid exmaple?\n> \n> Alternates: {\"TheProject.fr.html\" 1.0\n>       {type \"text/html\"} {language \"fr\"}},\n>     {\"TheProject.en.html\" 1.0\n>       {type \"text/html\"} {language \"en\"}},\n>     {\"TheProject.ru.html\" 1.0\n>       {type \"text/html;charset=iso-8859-5\"} {language \"ru\"}}\n>     (\"/cgi-bin/xlate?koi8-r+TheProject.ru.html\" 1.0\n>       {type \"text/html;charset=koi8-r\"} language \"ru\"}}\n> \n> 5.1 Reactive negotiation\n> \n> If two alternates are differ by charset only, how\n> specify preferred one?\n\nBy the quality factor.  This has been promoted in Lynx browser, and\nI havent been able to see Netscape and Mosiac NCSA support this. -- Mirsad\n\n-- \n    | Mirsad Todorovac|\n    | Faculty of Electrical Engineering and Computing|\n    | University of Zagreb|\n    | Unska 3, Zagreb, Croatia 10000|\n    ||\n    | e-mail: mirsad.todorovac@fer.hr|\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "> > If two alternates are differ by charset only, how\n> > specify preferred one?\n> \n> By the quality factor.  This has been promoted in Lynx browser, and\n> I havent been able to see Netscape and Mosiac NCSA support this. -- Mirsad\n\nThere is NO quality factor for Accept-Charset in current draft.\n\n\n\n"
        },
        {
            "subject": "Re: remove PATCH, COPY, MOVE, DELETE, etc.? Upgrade",
            "content": "Roy T. Fielding:\n>\n>  State management is\n>completeley (last time I checked) orthogonal to the HTTP version\n>number, and thus can be progressed as a separate specification\n>[the one being worked on by the state management subgroup].\n\ndraft-kristol-http-state-mgmt-00 is not orthogonal to the HTTP version\nnumber: see section 10.  Servers need the version number to know\nwhether they should send an old Netscape format Set-Cookie header or a\nnew HTTP/1.1 Set-Cookie header.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "> \n> > > If two alternates are differ by charset only, how\n> > > specify preferred one?\n> > \n> > By the quality factor.  This has been promoted in Lynx browser, and\n> > I havent been able to see Netscape and Mosiac NCSA support this. -- Mirsad\n> \n> There is NO quality factor for Accept-Charset in current draft.\n> \n\nYeah, remembered the thread.  However, from reviewing the thread I was not\nnear to understand why.  (I've noted who started the thread 'Charsets\nrevisited').\n\nIt seems to me like a part of HTTP problematics, because it's decided here\nwhich document to transfer.  To those whose native language/encoding is\nen.us/us-ascii it may seem irrelevant, but to all the others it is a\nmatter of high importance (the only workaround here seem to be CGI scipts\nwhich select language/charset to send -- yet they still need to know what\nthe client side wants.\n\nSo, IMVHO there should be a way to specify prefered language/encoding, with\nquality factors (which fit into current scheme for Accept: header), or by\nmeans of some other method.\n\nEg. I want document in my native language/encoding,\nif there isn't one, I'd be happy with native language/us-ascii,\nand fallback would be en.us/us-ascii.  --  Mirsad\n\n\n\n-- \n    | Mirsad Todorovac|\n    | Faculty of Electrical Engineering and Computing|\n    | University of Zagreb|\n    | Unska 3, Zagreb, Croatia 10000|\n    ||\n    | e-mail: mirsad.todorovac@fer.hr|\n\n\n\n"
        },
        {
            "subject": "Re: Transparency vs. Performance: survey of opinio",
            "content": "Roy T. Fielding:\n>\n>Koen writes:\n>> Two new cache-control request directives, \"may-cache\" and \"min-age\"\n>> that indicate possible weakening of any \"no-cache\" and \"max-age\"\n>> restrictions, would work for me.  ...\n>\n>I think that the proposed \"max-stale\" directive does indicate what\n>Koen would like the client to indicate.\n\nI just read the \"stale-max\" description in\ndraft-mogul-http-caching-00, and it indeed seems to indicate what I\nwant. \"stale-max\" would work like my \"min-age\".\n\n>  \"may-cache\" would therefore\n>be redundant.\n\nI used \"may-cache\" to indicate that the client may disregard a\n\"no-cache\" directive (since \"no-cache\" imposes stronger caching\nrestrictions than \"max-age=0\").  \"may-cache\" is not really needed to\nsolve the cookie reliability problem, so in that sense it is\nredundant.  I added it for symmetry, and also because someone once\nsaid that servers which send secure documents with no-cache will want\nto know about it if the client will ignore this directive.\n\n[...]\n\n>In other words, I think we reached an acceptable compromise before\n>this thread started.\n\nI want a requirement that it not mentioned in draft-mogul:\n\n  if the user loosens caching restrictions of the browser cache, the\n  user agent _must_ send a stale-max directive in every request.\n\nWithout this _must_, a server which uses cookies cannot reliably send\nwarning messages in pages if the user has set the browser to \"never\ncheck\".\n\nIn another message, you write:\n>Max-stale is better than a Cache-Warning on a request because the former\n>provides a useful service for the browser, and thus the browser will be\n>more likely to send it.\n\nThat is true. \n\nBut could someone ever want to loosen caching restrictions of the\nbrowser cache without loosening restrictions on proxy caches?  In that\ncase, there would be an incentive _not_ to send stale-max.\n\nMaybe modem users will want a browser setting in which the browser\ncache is set to \"never check\", without a \"max-stale\" being sent in\nrequests for pages not in the browser cache.  The rationale would be\n\"If I do have to wait for receipt of data over the phone line, I'd\nrather have fresh data\".\n\nSo here is what I propose:\n\nAdd two new cache-control request directives:\n\n    \"max-stale\"       \"=\" stale-time \n    \"local-max-stale\" \"=\" stale-time\n\n  with\n\n    stale-time = ( delta-seconds | \"inf\" )\n\n  If the client is configured to pass on (after the user clicks a\n  link, or, if the client is a proxy, after a request is received from\n  a downstream client) responses from its local cache memory or from\n  upstream servers which are stale-time seconds or less past their\n  expiration time, then one of these directives _must_ be present in\n  the request.  This is to give origin servers the chance to put\n  warning messages in the response entities if the correct working of\n  the service is not possible under such client configurations.\n\n  If the client is willing to pass on stale responses from its own\n  cache memory, but not willing to receive stale responses from\n  upstream servers, it can use the \"local-max-stale\" directive.\n \n  [Note: What is the semantics of a max-stale in a conditional GET\n  request?  I don't believe this has been discussed yet.]\n  \nAre we converging?\n\n>......Roy\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "At 02:04 PM 3/1/96 +0100, Mirsad Todorovac wrote:\n> Alternates: {\"TheProject.fr.html\" 1.0\n>       {type \"text/html\"} {language \"fr\"}},\n>     {\"TheProject.en.html\" 1.0\n>       {type \"text/html\"} {language \"en\"}},\n>     {\"TheProject.ru.html\" 1.0\n>       {type \"text/html;charset=iso-8859-5\"} {language \"ru\"}}\n>     (\"/cgi-bin/xlate?koi8-r+TheProject.ru.html\" 1.0\n>       {type \"text/html;charset=koi8-r\"} language \"ru\"}}\n\nCharsets are not to appear in mime type tags in URI/Alternates headers.\nThey ave their own slot.  The above examples would be:\n> Alternates: {\"TheProject.fr.html\" 1.0\n>       {type \"text/html\"} {language \"fr\"}},\n>     {\"TheProject.en.html\" 1.0\n>       {type \"text/html\"} {language \"en\"}},\n>     {\"TheProject.ru.html\" 1.0\n>       {type \"text/html\"} {charset \"iso-8859-5\"} {language \"ru\"}}\n\nThis variant below would be invalid by the anti-spoofing content negotiaion\nclause because they don't have matching prefixes.  Soemthing more valid\nwould be:\n>     (\"TheProject.ru2.html\" 1.0\n>       {type \"text/html\" {charset \"koi8-r\"} {language \"ru\"} }\n\n>> There is NO quality factor for Accept-Charset in current draft.\n\n>So, IMVHO there should be a way to specify prefered language/encoding, with\n>quality factors (which fit into current scheme for Accept: header), or by\n>means of some other method.\n>Eg. I want document in my native language/encoding,\n>if there isn't one, I'd be happy with native language/us-ascii,\n\nThere is a mechanism for specifying qualities on language, just not on\ncharset.  so you could ask for \"Accept-Language: native-language; ql=1.0,\nen; ql=.7\" and \"Accept-Charset: native-charset\" (which implicitly includes\niso-8859-1).\n\nIf that is not sufficient, we now that we have Koen's method for reactive\nnegotiation, by which you will be able to precisely pick which varaint you\nwant, which might be useful if you recieve the the language you wanted, but\nnot the charset you wanted. (You recieve TheProject.ru.html, but you realize\nthere was also a TheProject.ru2.html, so you ask for it by name.)\n-----\nDan DuBois, Software Animal           http://www.spyglass.com/~ddubois/\n    Download a totally free copy of the Spyglass Web Server today!\n        http://www.spyglass.com/products/server_download.html\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "> Charsets are not to appear in mime type tags in URI/Alternates headers.\n> They ave their own slot.\n\nHoltman paper says (4.6 Alternates):\n\nThe type, language, encoding, and length attributes of an\nalternate description refer to their Content-* header\ncounterparts.\n\nContent-Type has charset for text/html entities (for iso-8859-1\nit is implicit). Where is own slot for charset? As an extension\npostponed till HTTP/1.2?\n\n> If that is not sufficient, we now that we have Koen's method for reactive\n> negotiation, by which you will be able to precisely pick which varaint you\n> want, which might be useful if you recieve the the language you wanted, but\n> not the charset you wanted. (You recieve TheProject.ru.html, but you realize\n> there was also a TheProject.ru2.html, so you ask for it by name.)\n\nAn example\n\nAccept-Language: ru, *;q=0\nAccept-Charset: iso-8859-5, koi8-r, unicode-1-1-utf8\n\nServer has alternates with all charsets.  By current papers all my\nalternates has the same quality factor.  With what charset I would\nreceive document, if any, with preemptive negotiation?  In what order\nalternates will be present to user agent for reactive negotiation?  Per\ndrafts the order is significant, because the first alternate is the\nbest one. Why not to have quality factor charset? Like this\n\nAccept-Language: ru, *;q=0\nAccept-Charset: koi8-r, iso-8859-5;q=0.8; *;q=0\n\n\n\n"
        },
        {
            "subject": "DigestMessageDigest doesn't work with proxie",
            "content": "John said:\n----------\n]]\n] > Are you just talking about D-MD, or Digest Auth for\n] > Proxy-Authentication and Proxy-Authorization as well?\n] >\n]\n] Digest-MessageDigest has been part of the draft since its very early\n] versions.  It has limitations. I don't think we are in a position\n] to either remove it or overcome its limitations.  The new nextnonce\n] field seems to me to be a useful addition which is is a very modest\n] change and not likely to lead to any unpleasant surprises.  I also\n] agree with Paul that there is not much reason to keep the user, nonce\n] and realm fields.  In the fullness of time we can and will create\n] stronger ways of dealing with authentication, proxies, headers, etc.\n]\n] I propose that the D-MD section of this draft be:\n]\n[omitted]\n\nThe problem is, this is broken when a proxy is involved.  I don't care \nif you enhance the section along the lines of my suggestions, but it \nhas to work when proxies are involved, otherwise huge numbers of \nclients can't use digest auth -- perhaps even the majority of users in \nthe near future.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "According to Henrik Frystyk Nielsen:\n> \n> Using Allow has the same problem as if a Connection header is passed\n> through a proxy. Imagine the following setup\n> \n> new client      ->      old proxy       ->      new server\n> \n> On the first request, the new server will then send back a\n> \n> Allow: GET, MGET, HEAD, etc.\n> \n> On the second request, the new client will try a MGET, but the proxy\n> doesn't understand this and then we are back to the old problem having\n> lost one roundtrip time. \n\nI don't see this as a very big problem.  We have lost one roundtrip\ntime, but only a roundtrip between client and proxy.  In normal use\nclient and proxy are very close with low latency.  I don't think this\nis a very big price to pay.  This proposal then has the big advantage\nthat it is backwards compatible.  It isn't necessary to try to change\nHTTP1.0 proxies to get them to strip out anything. (I am assuming that\ncurrent proxies will return a standard error status/message for an\nunrecognized method.)\n\nThe criticism of MGET that it is not as general as keep-open is true,\nbut I think there is serious danger of performance degradation if that\ngenerality is used.  More likely unless those concerns can be met keep-open\nwould not be widely implemented.  \n\nI would assume that an MGET addition to the protocol would be accompanied\nby a parallel MHEAD method.  It would still not be possible to mix \nGETs and POSTs or even do multiple POSTs. I don't see any pressing demand\nfor these, but perhaps I am just not aware of it.\n\nWhile it is not part of an MGET/MHEAD proposal, a closely related proposal\nis the Image-size header suggested by Eric Sink.  In the response to a \nGET for an HTML doc with inlined images a new server should return\n\nImage-size: foo.gif100x200 33456\nImage-size: bar.gif100x120 22543\netc.\n\nThis allows the client to do page layout while waiting for the MGET with\nall the images.  \n\nThere is an important principle to keep in mind here.  Any proposal\nthat can't at least match the user's perceived performance which\nNetscape obtains with multiple connections is not viable. It's a\ncompetitive world out there and browser writers will have to go with\nmultiple connections if nothing else matches their performance.  Any\nproposed standard, MGET or stay-open, which can't measure up to\nmultiple connection performance just won't be implemented.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Proxies and DigestMessageDiges",
            "content": "John said:\n----------\n]\n] > Are you just talking about D-MD, or Digest Auth for\n] > Proxy-Authentication and Proxy-Authorization as well?\n] >\n]\n] Digest-MessageDigest has been part of the draft since its very early\n] versions.  It has limitations. I don't think we are in a position\n] to either remove it or overcome its limitations.  The new nextnonce\n] field seems to me to be a useful addition which is is a very modest\n] change and not likely to lead to any unpleasant surprises.  I also\n] agree with Paul that there is not much reason to keep the user, nonce\n] and realm fields.  In the fullness of time we can and will create\n] stronger ways of dealing with authentication, proxies, headers, etc.\n]\n] I propose that the D-MD section of this draft be:\n\n[proposal omitted -- same as in current version]\n\nWhat about Proxy-Authentication and Proxy-Authorization?  Can they use \nDigest Auth challenges and responses?\n\nMy reading of the HTTP spec is that any auth scheme must support both \norigin-server (WWW-Authenticate and Authorization) and proxy \n(Proxy-Authentication and Proxy-Authorization) authentication.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: DigestMessageDigest doesn't work with proxie",
            "content": "On Fri, 1 Mar 1996, Paul Leach wrote:\n\n> John said:\n> ----------\n> ]]\n> ] > Are you just talking about D-MD, or Digest Auth for\n> ] > Proxy-Authentication and Proxy-Authorization as well?\n> ] >\n> ]\n> ] Digest-MessageDigest has been part of the draft since its very early\n> ] versions.  It has limitations. I don't think we are in a position\n> ] to either remove it or overcome its limitations.  The new nextnonce\n> ] field seems to me to be a useful addition which is is a very modest\n> ] change and not likely to lead to any unpleasant surprises.  I also\n> ] agree with Paul that there is not much reason to keep the user, nonce\n> ] and realm fields.  In the fullness of time we can and will create\n> ] stronger ways of dealing with authentication, proxies, headers, etc.\n> ]\n> ] I propose that the D-MD section of this draft be:\n> ]\n> [omitted]\n> \n> The problem is, this is broken when a proxy is involved.  I don't care \n> if you enhance the section along the lines of my suggestions, but it \n> has to work when proxies are involved, otherwise huge numbers of \n> clients can't use digest auth -- perhaps even the majority of users in \n> the near future.\n> \n\nAre you saying it is broken for end-to-end transactions when the data\npasses through proxies?  If so why?  Or is it just that it doesn't \nhandle proxy authentication?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "DigestMessageDigest doesn't work with proxie",
            "content": "John said:\n----------\n] > The problem is, this is broken when a proxy is involved.  I don't care\n] > if you enhance the section along the lines of my suggestions, but it\n] > has to work when proxies are involved, otherwise huge numbers of\n] > clients can't use digest auth -- perhaps even the majority of users in\n] > the near future.\n] >\n]\n] Are you saying it is broken for end-to-end transactions when the data\n] passes through proxies?  If so why?  Or is it just that it doesn't\n] handle proxy authentication?\n\nIt's not just that it doesn't handle proxy authentication. The most \nobvious problem is that it isn't explained how to do digest auth when \nthere is a proxy in the middle.\n\nI can't give a complete answer to the \"why\" question easily without\nknowing whether Digest Auth can be used with Proxy-Auth* headers, but\nlet me see if I can at least present a convincing argument that \noperation of Digest Auth with proxies with or without Proxy-Auth* isn't \ntrivial to understand from the current draft, and so needs some explicit\nexplanation.\n\nConsider: if the client does a GET and the proxy serves it from the cache,\nwhere does the \"nonce\" come from that is needed to compute and\ncheck <message-digest> -- cached data, the proxy's nonce from\nproxy-auth, or does the proxy have to always go to the origin-server?\n\nWhich user name, realm, and password are used to check the\n<message-digest>: the one for the proxy (if Proxy-Auth* are being used)\nor the one from the origin-server?  If it's the one from the origin-server,\nthen the proxy will have to keep some new information that it wouldn't\nhave had to keep if it weren't using digest auth -- that needs be be \nexplained. If it's not, then it must be made clear that there is no \nguarantee that the proxy didn't munge the entity-body, only that it \nwasn't munged in between the proxy and the client.\n\nIf a proxy has a cached resource because it was fetched by user X, and \nuser Y asks for the same resource, how does the proxy get a\n<message-digest> computed with Y's username and password? Does it go to \nthe origin-server, or does it use Y's proxy username and password to \ncompute <message-digest>.\n\nCan both oriigin-server and proxy server add Digest-MessageDigest \nheaders? If so, how does the client tell which is which?\n\nDoes a proxy pass through the user identity to the origin-server, or \nauthenticate itself to the origin-server, or both?  How does a client \nauthenticate itself to both proxy and origin-server with Digest Auth?\n\nThe simplest fix I can think of is:\na.  to require that requests to proxies that carry digest auth in \nAuthorization headers must have Pragma: no-cache or\nCache-Control: no-cache.\nb. state that Proxy-Auth* is completely independent of origin-server\nauth (clients can get both challenges in a single response, and need\nto include both credentials in the retry of the request).\nc. State that proxies only  add D-MD on responses served from the cache \n(which by rule a will never be in response to requests with \nAuthorization: Digest in them).\nd. State that proxy supplied D-MD only guarantees that noone munged\nentirty-body between the client and proxy, but that the proxy could have\n(i.e., no end-to-end guarantee).\n\nThis fix has the (big) disadvantage that it doesn't allow end-to-end \nauthenticated requests to be served from cache. The caching subgroup \nmade a request for a change to the HTTP spec specifically to allow it, \nso they thought it was important.  But at least if we can agree at this \nlevel, we can worry about making it more efficient later.  As it is, I \ndon't think proxy implementors or clients would know what to do.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: DigestMessageDigest doesn't work with proxie",
            "content": "On Fri, 1 Mar 1996, Paul Leach wrote:\n> \n> Consider: if the client does a GET and the proxy serves it from the cache,\n> where does the \"nonce\" come from that is needed to compute and\n> check <message-digest> -- cached data, the proxy's nonce from\n> proxy-auth, or does the proxy have to always go to the origin-server?\n> \n\nIt always has to go to the origin-server.  Here is a quote from\nfrom section on Access Authentication from  the HTTP/1.1 spec draft at\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/1.1/spec.html\n\n   \"Proxies must be completely transparent regarding user\n   agent authentication. That is, they must forward the\n   WWW-Authenticate and Authorization headers untouched, and\n   must not cache the response to a request containing\n   Authorization.\"\n\n\nThe problems you are addressing are important and need to be solved.\nBut Digest Authentication is not the mechanism to solve those problems.\nIt is a very small step in the right direction, intended only to replace\na misstep, viz.  Basic Authentication.\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "HTTP version numbe",
            "content": "Koen writes:\n> draft-kristol-http-state-mgmt-00 is not orthogonal to the HTTP version\n> number: see section 10.  Servers need the version number to know\n> whether they should send an old Netscape format Set-Cookie header or a\n> new HTTP/1.1 Set-Cookie header.\n\nThis won't work.  For example\n\n     1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n\nwill result in a 1.1 cookie being sent to a 1.0 user agent.\n\nThe only features of HTTP that can depend on a minor version number\nchange are those that are interpreted by neighbors in the communication.\nOther feature changes require changes to the content, such as a \ndifferent header field name.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: DigestMessageDigest doesn't work with proxie",
            "content": "John says:\n----------\n]\n] It always has to go to the origin-server.  Here is a quote from\n] from section on Access Authentication from  the HTTP/1.1 spec draft at\n]\n] http://www.w3.org/pub/WWW/Protocols/HTTP/1.1/spec.html\n]\n]    \"Proxies must be completely transparent regarding user\n]    agent authentication. That is, they must forward the\n]    WWW-Authenticate and Authorization headers untouched, and\n]    must not cache the response to a request containing\n]    Authorization.\"\n\nI know that. And this is precisely the section that the caching \nsubgroup asked to be modified, which is why I commented about it. \nPeople wanted to be able to use Basic\njust for identification, and still let entities be served form the \ncache.  If Digest is to be a replacement for Basic, it needs to have \nthe same capability.\n\nSo, if their request is granted, the no-cache requirement will no \nlonger be there, and you'll need to at least make a specific \nDigest-related no-cache requirement, Even if it isn't granted you'll \nneed to augment that statement with one about proxies not generating \nD-MD and passing D-MDs from origin servers it through untouched.\n\n]\n] The problems you are addressing are important and need to be solved.\n] But Digest Authentication is not the mechanism to solve those problems.\n] It is a very small step in the right direction, intended only to replace\n] a misstep, viz.  Basic Authentication.\n\nWould you please read my  whole message instead of just replying to one \nlittle piece of it , and then rejecting it in it's entirety? There were \nproblems pointed out in it even if you didn't want to cache results, \nbut just want to use Digest to replace Basic when authenticating to \nproxies or for simple identification to servers that preserves the \nability to cache.\n\nSo: are you going to say that Digest can be used for Proxy-Auth? Basic \ncan be, and the HTTP spec (I'll say again) says that challenges and \ncredentials are the same for both origin-server and proxy auth.  Both \nfacts argue for Digest being able to be used for both purposes.\n\nIf you say it can't be, are you going to ask for modifications to the \nHTTP 1.1 spec to eliminate that requirement? If you say it can be used \nfor both purposes, then questions in my previous message remain \nunanswered, and I'll claim that what's in the Digest Auth draft is not \nadequate to tell an implementor how to implement when proxies are being \nused, or to describe the security ramifications.\n\nBy adding D-MD, Digest already goes beyond just replacing Basic, and \nleads directly to the issues with proxies. But even without D-MD, proxy \nissues still exist and need to be addressed just for Digest to be a \nreplacement for Basic.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Variant ID",
            "content": "Jeff Mogul and I received an action item at the caching subgroup \nmeeting to write up a proposal for variant IDs. Jeff didn't get the \nwriteup finished before going on vacation, and asked me to take a crack \nat putting the ideas (mostly his) into \"spec\" form. So I did, borrowing \nwholesale from a mail message he had sent earlier to the list, and just \nadding some HTTP-BNF. However, Jeff has not seen this at all, so while \nhe deserves credit for all the correct stuff, anything that's wrong is \nmy fault.\n\nOne of the things that may be wrong is that I define the Vary header \nhere, and it may be defined elsewhere -- I checked that those \nelsewheres didn't include Jeff's caching I-D or Koen's content \nnegotiation I-D, but it could be somewhere else. If so, I apologize and \none should substitute that deifnition for the one herein.  But if I had \nsearched any longer, this wouldn't have been ready at all before IETF.\n\nPaul\n-------------------------------------------------------------------------\n\n\n\n\n\n\nHTTP Working Group                            Paul J. Leach/Microsoft\nInternet-Draft                                   Jeffrey Mogul/DECWRL\nExpires: 1 September 1996                               1 March 1996\n\n\n\n\n\n\n                               Variant IDs\n                            Preliminary Draft\n\n\n\nSTATUS OF THIS MEMO\n\n  THIS IS A PRELIMINARY DRAFT OF AN INTERNET-DRAFT.  IT DOES NOT\n  REPRESENT THE CONSENSUS OF THE HTTP WORKING GROUP.\n\n  This document is an Internet-Draft. Internet-Drafts are working\n  documents of the Internet Engineering Task Force (IETF), its areas,\n  and its working groups. Note that other groups may also distribute\n  working documents as Internet-Drafts.\n\n  Internet-Drafts are draft documents valid for a maximum of six months\n  and may be updated, replaced, or obsoleted by other documents at any\n  time. It is inappropriate to use Internet-Drafts as reference\n  material or to cite them other than as \"work in progress\".\n\n  To learn the current status of any Internet-Draft, please check the\n  \"1id-abstracts.txt\" listing contained in the Internet-Drafts Shadow\n  Directories on ftp.is.co.za (Africa), nic.nordu.net (Europe),\n  munnari.oz.au (Pacific Rim), ds.internic.net (US East Coast), or\n  ftp.isi.edu (US West Coast).\n\n  Distribution of this document is unlimited.  Please send comments to\n  the HTTP working group at <http-wg@cuckoo.hpl.hp.com>.  Discussions\n  of the working group are archived at\n  <URL:http://www.ics.uci.edu/pub/ietf/http/>.  General discussions\n  about HTTP and the applications which use HTTP should take place on\n  the <www-talk@w3.org> mailing list.\n\nABSTRACT\n\n  This draft proposes a mechanism to be used by caches to handle\n  variant resources. It is appropriate when the number of variants is\n  too large to express conveniently with the URI header, or when the\n  content negotiation algorithm used by an origin-server is unknown or\n  more complicated than the cache wishes to emulate.\n\n  The intent is that this document be incorporated into the HTTP/1.1\n  specification [1], if the HTTP working group accepts the design.\n\n\n\n\n  DO NOT IMPLEMENT TO THIS DOCUMENT!               [Page 1]\n\n\n\n  Internet-Draft Variant-IDs            (DRAFT)   13 February 1996\n\nTable of Contents\n\n1. Introduction............................................2\n2. Scenario................................................2\n3. Specification...........................................3\n 3.1 Vary .................................................3\n 3.2 Variant-ID ...........................................4\n 3.3 Variant-Set ..........................................4\n4. Security Considerations.................................5\n5. References..............................................5\n6. Author's addresses......................................5\n\n\n1. Introduction\n\n  This draft proposes a mechanism to be used by caches to handle\n  variant resources. It is appropriate when the number of variants is\n  too large to express conveniently with the URI header, or when the\n  content negotiation algorithm used by an origin-server is unknown or\n  more complicated than the cache wishes to emulate.\n\n  The draft is organized as follows: first we present a scenario to\n  motivate the use of Variant IDs, then we present the specification of\n  the three new headers that are needed to support them.\n\n2. Scenario\n\n  Consider a single resource with multiple variants (e.g., a document\n  that has been translated into several dozen languages).  Consider a\n  proxy which already has several variants in its cache (for\n  concreteness, let's say it has three such variants: English, French,\n  and German).  Let's also assume that the server sent \"Vary: Content-\n  Language\" because it didn't want to encode all of the possible\n  variants in a URI: header.\n\n  So now the cache gets a request for this resource that says:\n\n          Accept-language: da, en;q=0.5, de;q=0.3\n\n  which (if I've got the tags right) means that the user prefers Danish\n  but is willing to accept English or maybe German.\n\n  Since the server has not told the cache whether or not it has a\n  Danish variant of the resource, the cache has to forward the request\n  to the origin server.  Here's the problem: when the origin server\n  receives this request, it has no idea which variants the cache is\n  currently holding.  This means that even if the cache currently holds\n  exactly the response that the server would provide, the server has to\n  retransmit it to the cache.\n\n  This is what variant IDs are supposed to solve.  Suppose that the\n  origin server tags each response with a Variant-ID: header. E.g.,\n          Variant-ID: \"xy\"\n  or\n\n\n  DO NOT IMPLEMENT TO THIS DOCUMENT!               [Page 2]\n\n\n\n  Internet-Draft Variant-IDs            (DRAFT)    13 February 1996\n\n          Variant-ID: \"97\"\n  The value of the Variant-ID: field is meant to be opaque and\n  relatively compact (i.e., it should not take a lot of bytes to\n  transmit it).\n\n  So suppose that our cache holds these entries for the resource R\n      R1: (Content-Language: en, Validator: zzzz, Variant-ID: \"1\")\n      R2: (Content-Language: fr, Validator: qqqq, Variant-ID: \"3\")\n      R3: (Content-Language: de, Validator: xxxx, Variant-ID: \"97\")\n\n  Now when it is time for the cache to forward the request to the\n  origin server, it tacks on this new header:\n          Variant-set: id=1;\"zzzz\", id=3;\"qqqq\", id=97;\"xxxx\"\n  That is, the set of the variants it currently holds and their\n  associated validators.\n\n  The server goes through its normal content-negotiation algorithm to\n  decide which variant to return (i.e., for this purpose it ignores the\n  Variant-set: header).  Once it has made this decision, it then checks\n  to see if the variant it plans to return is in the cache's variant\n  set.\n\n  For this example, if the server DOES have a Danish variant, then it\n  would return a status code of \"200 OK\", headers including\n          Content-language: da\n          Variant-ID: 192\n  and the full entity body for the Danish variant.  If, on the other\n  hand, it does not have a Danish variant, it would presumably want to\n  return the English variant that the cache already knows\n  about.  In this case, the Variant-set in the request indicates that\n  the cache-validator for variant-ID 1 (which is the English variant)\n  is zzzz, so the origin server does its normal validator check to see\n  if this cached copy is still valid.  If so, it returns a status code\n  of \"304 Not modified\" plus these headers:\n          Content-language: en\n          Variant-ID: 1\n  and otherwise it returns the same headers but sends \"200 OK\" and the\n  entire entity body.\n\n  Since this is just a performance optimization, it does not matter if\n  either the cache or the origin server doesn't implement it.  I.e.,\n  neither \"Variant-ID:\" nor \"Variant-set:\" is mandatory.\n\n3. Specification\n\n  This section contains the proposed modifications and additions to the\n  HTTP/1.1 specification to support variant IDs.\n\n3.1  Vary\n\n  The Vary entity header is used by an origin-server to indicate the\n  manner in which variants of a single resource may vary, by returning\n  in the Vary header the names of the entity header fields that have\n  values that are different between variants. The URI header also\n\n\n  DO NOT IMPLEMENT TO THIS DOCUMENT!               [Page 3]\n\n\n\n  Internet-Draft Variant-IDs            (DRAFT)    13 February 1996\n\n  indicates the possible variants of a resource, but if there are many\n  of them, this header may be more compact. (See section XXX on content\n  negotiation.)\n\n      Vary          = \"Vary\" \":\" 1#field-name\n\n  A cache can use this information to know what it has to include in\n  its \"cache key\" in order to indentify whether it has a particular\n  variant cached. The presence of a \"field-name\" in a Vary header means\n  that a cache should include the value of that field in the cache key\n  for the entity returned in the same message as the Vary header.\n\n  Example:\n  if a server returns\n      Content-Language: de\n      Vary: Content-Language\n  in a response to a GET request for URL \"http://www.foo.com/bar.htm\"\n  then the cache should include \"Content-Language: de\" in the cache key\n  for this variant of the \"http://www.foo.com/bar.htm\" resource, and\n  should not serve up that instance in response to a request where\n  content negotiation indicates that the Content-Language should be\n  \"fr\".\n\n\n3.2 Variant-ID\n\n  The Variant-ID entity header can be used by a server to indicate that\n  the entity-body in the same message is one of several variants of a\n  single resource, and to give the client an ID for it unique among all\n  the variants. The client can later use this ID as part of a Variant-\n  Set header to indicate to a server which variants it has cached, so\n  that the server need not retransmit a variant needlessly.\n\n      VariantID     = \"Variant-ID\" \":\" varnt-id\n      varnt-id      = quoted-string\n\n  It is recommended that the quoted-string be relatively short, as a\n  moderate number of them may need to be transmitted in a single\n  request message.\n\n      Note: XXX are Variant-IDs ever needed in POST and PUT (e.g.)?\n\n\n3.3 Variant-Set\n\n  The Variant-Set request header can be used to indicate that the\n  client (user agent or proxy) has some of the variants of a resource\n  cached, and therefore that if it should turn out that content\n  negotation selects one of them as the most appropriate variant, the\n  server need not transmit it to the client, if the client's copy is\n  still valid.\n\n      Variant-Set   = \"Variant-Set\" \":\" 1#variant-info\n      variant-info  = \"id\" \"=\" varnt-id \";\" validator\n\n\n  DO NOT IMPLEMENT TO THIS DOCUMENT!               [Page 4]\n\n\n\n  Internet-Draft Variant-IDs            (DRAFT)    13 February 1996\n\n      validator     = quoted-string | HTTP-Date\n\n  The \"varnt-id\" is as defined for the Variant-ID header. The validator\n  is either an opaque string or a date like in the Date or Last-\n  Modified headers. The former is used if the cache is using opaque\n  validators to manage its cache; the former if it is using Last-\n  Modified.\n\n  If the server determines, after selecting a variant via content\n  negotiation (see [2]), that the client has the selected variant\n  cached, and from the validator that the cached copy is valid, it may\n  return \"304 Not modified\" plus a Variant-ID header whose value is the\n  variant ID of the variant selected, and not transmit the entity body.\n  Otherwise it returns these same headers but sends \"200 OK\" and the\n  entire entity body.\n\n\n4. Security Considerations\n\n  TBS\n\n5. References\n\n  [1]  Roy T. Fielding, Henrik Frystyk Nielsen, and Tim Berners-Lee.\n     Hypertext Transfer Protocol -- HTTP/1.1.  Internet-Draft draft-\n     ietf-http-v11-spec-01.txt, HTTP Working Group, January 19, 1996.\n  [2] Koen Holtman, Proposed Content Negotiations for HTTP/1.1.\n     Internet Draft draft-holtman-http-negotiation-00.txt, HTTP Content\n     Negotiation Subgroup, February 22, 1996\n\n6. Author's addresses\n\n  Jeffrey Mogul\n  Western Research Laboratory\n  Digital Equipment Corporation\n  250 University Avenue\n  Palo Alto, California, 94395, U.S.A\n  Email: mogul@wrl.dec.com\n\n  Paul J. Leach\n  Microsoft Corporation\n  1 Microsoft Way\n  Redmond, Washington, 98052, U.S.A.\n  Email: paulle@microsoft.com\n\n\n\n\n\n\n\n\n\n\n\n\n  DO NOT IMPLEMENT TO THIS DOCUMENT!               [Page 5]\n\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "Nickolay Saukh:\n>\n>4.2 Accept-Charset\n>\n>I think last sentence of first paragraph should be written as\n>\"The ISO-8859-1 character set can be assumed to be acceptable\n>to all user agents.\".\n\nThere was a long discussion about ISO-8859-1 versus US-ASCII recently,\nand I must admit that I did not read all messages in that discussion.\nMy impression at the end was that most people wanted US-ASCII to stay\nas the character set which can be assumed to be acceptable to all user\nagents.\n\n> Rationale: per HTTP/1.1 draft\n>(section 3.7.1) entity body without explicit charset can be\n>US-ASCII only or ISO-8859-1. \n\nYes.\n\n>Thus any conforming user agent must\n>be able to handle ISO-8859-1.\n\nNo, that is not a correct inference.  It would make sense for every\nuser agent to be able to handle the all entity bodies without explicit\ncharset, but Section 3.7.1 does not require it.\n\n>4.6 Alternates\n>\n>Can media-type contain charset? Is this a valid exmaple?\n>\n>Alternates: {\"TheProject.fr.html\" 1.0\n>      {type \"text/html\"} {language \"fr\"}},\n>    {\"TheProject.en.html\" 1.0\n>      {type \"text/html\"} {language \"en\"}},\n>    {\"TheProject.ru.html\" 1.0\n>      {type \"text/html;charset=iso-8859-5\"} {language \"ru\"}}\n>    (\"/cgi-bin/xlate?koi8-r+TheProject.ru.html\" 1.0\n>      {type \"text/html;charset=koi8-r\"} language \"ru\"}}\n\nYes.  Contrary to what Daniel DuBois said in this thread, \n\n   {type \"text/html;charset=iso-8859-5\"} \n\nis indeed the way to denote the charset. \n\nThis mirrors use of the Content-Type header, which specifies the MIME\ntype and optionally the charset.  Note that we do not have a\nContent-Charset header, but that we _do_ have an Accept-Charset\nheader.  I believe that this asymmetry was caused by early versions of\nHTTP trying to inherit as much semantics from the MIME specifications.\nAs far as I know, it is too late to fix it now.\n\nAlso, contrary to what Daniel DuBois said,\n\n>    (\"/cgi-bin/xlate?koi8-r+TheProject.ru.html\" 1.0\n>      {type \"text/html;charset=koi8-r\"} language \"ru\"}}\n\nis a legal alternate description.  What the anti-spoofing clause (the\norigin server restriction) in Section 5.2 of draft-holtman says is\nthat origin servers may not return this alternate in a preemptive\nnegotiation response.  This means that, if this alternate is the best\none, the origin server should send a reactive negotiation response,\nwhich causes the client to retrieve the best alternate with a direct\nrequest on /cgi-bin/xlate?koi8-r+TheProject.ru.html.\n\n>5.1 Reactive negotiation\n>\n>If two alternates are differ by charset only, how\n>specify preferred one?\n\nThe service author can specify the preferred one using the source\nquality factors in the Alternates header:\n\n    {\"notpreferred.html\" 0.9 {type \"text/html;charset=iso-8859-5\"}} \n    {\"preferred.html\"    1.0 {type \"text/html;charset=koi8-r\"}}\n\nor by the order in which the alternates are listed:\n\n    {\"preferred.html\"    1.0 {type \"text/html;charset=koi8-r\"}}\n    {\"notpreferred.html\" 1.0 {type \"text/html;charset=iso-8859-5\"}} \n\nSo it is up to the service author so decide for you which charset of\nthe ones you accept would give you the best results.  The decision\nmade is reflected in the Alternates header.\n\nYou, as a user agent user, can not express a preference for one\ncharset over another, you can only say which ones you can handle.\nThere are no quality factors in the Accept-Charset header.\n\nThis means that the HTTP/1.1 draft spec assumes that if a user agent\nputs a charset in its Accept-Charset header, it can handle this\ncharset perfectly, not just through some lossy on-the-fly filter.  If\nanything lossy happens, it must be done at the server side, and be\nreflected in the Alternates header.\n\nI don't know if this assumption of being able to handle perfectly all\ncharsets included in the Accept-Charset header is correct for all\ncurrent browsers.  If it is not, we would have to decide if a) the\ncurrent browsers need to be improved, or b) the draft spec needs to be\nextended.  I would go for a), though I realize that this puts browsers\nthat don't use a bitmapped screen, like Lynx, in a difficult position.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Non Latin1 charsets (draft-holtman-http-negotiation00.txt",
            "content": "> \n> Nickolay Saukh:\n> >\n> >4.2 Accept-Charset\n> >\n> >I think last sentence of first paragraph should be written as\n> >\"The ISO-8859-1 character set can be assumed to be acceptable\n> >to all user agents.\".\n> \n> There was a long discussion about ISO-8859-1 versus US-ASCII recently,\n> and I must admit that I did not read all messages in that discussion.\n> My impression at the end was that most people wanted US-ASCII to stay\n> as the character set which can be assumed to be acceptable to all user\n> agents.\n\nI wouldn't say I _want_ this. \n\nOne could argue that US-ASCII is the default character set in MIME\nmail, but historically, prior versions of the HTTP/HTML spec have\nspecificed ISO-8859-1 as the default character set for the Web.\n\nWhat I recall someone suggesting recently was that because of the\n(IMHO broken) state of current practice, one couldn't safely \nstate that user agents actually were defaulting to ISO-8859-1.\n\nIndependent of this line of argument, the discussion of internationalization\nthat went into the HTML 2.0 RFC established the requirement that\nall HTML interpreters should use an SGML document character set\nthat included at least the characters of ISO-8859-1. This\ndoesn't say anything much about the encoding, but it tends\nto imply that support for ISO-8859-1 is a practical requirement\nto comply with the HTML 2.0 spec.\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "  > From research!cuckoo.hpl.hp.com!http-wg-request  Sun Dec 18 11:00:08 1994\n  > Received: from research (research.att.com) by allegra.tempo.att.com; id AA29983; Sun, 18 Dec 94 11:00:06 EST\n  > Received: by research.att.com; Sun Dec 18 10:59 EST 1994\n  > Received: from cuckoo.hpl.hp.com by hplb.hpl.hp.com; Sun, 18 Dec 1994 15:53:38 GMT\n  > Received: from http-wg (list exploder) by cuckoo.hpl.hp.com\n  > (1.37.109.8/15.6+ISC) id AA03566; Sun, 18 Dec 1994 15:55:14 GMT\njohn@math.nwu.edu (John Franks) said on Sun, 18 Dec 1994 09:55:02 -0600 (CST):\n  [...]\n  > The criticism of MGET that it is not as general as keep-open is true,\n  > but I think there is serious danger of performance degradation if that\n  > generality is used.  More likely unless those concerns can be met keep-open\n  > would not be widely implemented.  \n  > \n  > I would assume that an MGET addition to the protocol would be accompanied\n  > by a parallel MHEAD method.  It would still not be possible to mix \n  > GETs and POSTs or even do multiple POSTs. I don't see any pressing demand\n  > for these, but perhaps I am just not aware of it.\n  [...]\n\nA SESSION method is preferable to a proliferation of M* methods.\nSESSION is useful in contexts that are important for future uses of\nWWW.  Specifically, SESSION makes security and payment enhancements to\nHTTP more efficient.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Some randomness concerning chunked encoding",
            "content": "Hi,\n\nI'm just looking into the chunked encoding stuff and writing \na little code. I had an idea which may save us a compatibility problem \nat a later date but which could probably be implemented now without\ndifficulty:-\n\nThe point is that in future we may want to have attributes\nassociated with each chunk, eg a running message digest or the like.\nThere is also the question of multiplexing channels. forget for the\nmoment the issue of client/server compatibility but consider the action\nof a gateway. We may wish to avoid having to upgrade proxies etc if\nwe incorporate channel type features into the spec.\n\nThe spec at present is for chunks of the following form:-\n\n\n1A\nabcdefghijklmnopqrstuvwxyz\n10\n0123456789\n0\nFooter: foo\n\n\nWhat I would like to do is to require parsers to be tolerant of \nadditional garbage on the chunk size line:-\n\n\n1A ajwiuwenbq3iuy3ig23==\nabcdefghijklmnopqrstuvwxyz\n10 This will be ignored but might be an MD5 of the chunk\n0123456789\n0\nFooter: foo\n\n\nThe 1.1 spec would not define semantics for the additional parameters but\nwould require parsers to be tolerant.\n\nThis effectively means that the parser needs to introduce a \"throw\naway data loop\" into the FSM:-\n\nStart -- hex --> Number\nNumber-- hex -->Number\nNumber-- CR -->GotCR\nNumber-- LF -->Start[If NUMBER > 0]\nFooter[Otherwise]\n\n# We add the following transitions :\n\nNumber  -- SP -->Junk\nJunk-- CR -->GotCR\nJunk-- ANY -->Junk\n\n\nNaturally no 1.1 compliant generator would generate such data.\n\n\nFor the price of three extra transitions in the state table of the\nparse we get a really big win. Is there anyone who would get trodden\non if we went this route?\n\n\nThis raises another problem. If I want to release a product that has\na subset of 1.1 implemented what do I report in the protocol? If I\nreport 1.0 this may be wrong because I may have used a non 1.0 feature.\nIf I report 1.1 then this may also be wrong.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "RE: Variant ID",
            "content": "Since sending my writeup for variant IDs, which contained a section on \nthe Vary header, I found a writeup by D.R.T. Robinson on the Vary \nheader -- I think the reason I didn't find it in my search is that he \nposted a URL to it, rather than the writeup itself, thus confounding my \nwhole text search for the string \"Vary:\". His writeup is much more \ncomprehensive than mine, and is correct for use other than with \nVariant-IDs. Mine should be ignored.\n\nIt turns out that the differences do not break Variant IDs.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Some randomness concerning chunked encoding",
            "content": "Phill,\nThis brings up some good points, because I would assert that chunked\nencoding does not yet have consensus of this working group for anything\nbeyond experimental use (at this point). I have spoken with several people\nwho feel that it is an unnecessary mechanism at the wrong OSI level. So as\nyour paragraph below suggests, it does start to become a problem if people\nstart shipping/sending out software claiming to be HTTP/1.1.\n\nI will post more a bit later regarding chunked encoding.\n\n>\n>This raises another problem. If I want to release a product that has\n>a subset of 1.1 implemented what do I report in the protocol? If I\n>report 1.0 this may be wrong because I may have used a non 1.0 feature.\n>If I report 1.1 then this may also be wrong.\n>\n>\n>Phill\n>\n>\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "Roy T. Fielding:\n>Koen writes:\n>> draft-kristol-http-state-mgmt-00 is not orthogonal to the HTTP version\n>> number: see section 10.  Servers need the version number to know\n>> whether they should send an old Netscape format Set-Cookie header or a\n>> new HTTP/1.1 Set-Cookie header.\n>\n>This won't work.  For example\n>\n>     1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n>\n>will result in a 1.1 cookie being sent to a 1.0 user agent.\n\nIsn't the 1.1 proxy required to convert the 1.1 response from the\nserver to a 1.0 response before sending it to the user agent?  This\nconversion would presumably also convert the 1.1 cookie in the\nSet-Cookie header to a 1.0 cookie.  At least this is how I read\nSection 3.1 of the draft 1.1 spec.\n\nYou can argue that cookie conversion is too big a requirement to make\non proxies, especially because the Netscape cookie format is not\n`official 1.0'.  In fact, I'm wondering about this myself.  We will\nprobably at least have to change state management draft to ensure that\nconversion is more easy that it is now.\n\nAnyway, conversion would only affect 1.1 set-cookie headers which have\nmax-age fields, these fields will need to be translated to expires\nfields to get Netscape (1.0) format cookies.\n\n\nA more general remark: I have been putting off thinking through\n1.0<->1.1 interoperability problems for some time now.  I do not want\nto rule out the possibility that the new 1.1 mechanisms that will be\nintroduced (hopefully) this month, combined with the current Section\n3.1, will place so much subtle conversion requirements on proxy caches\nthat we will only ever see non-conforming 1.1 proxy caches.\n\nMy plan is to start worrying about this in April.\n\n>The only features of HTTP that can depend on a minor version number\n>change are those that are interpreted by neighbors in the communication.\n\nI'm not sure what you means by this; I hope you don't mean \"when\npassing on a 1.1 response to a 1.0 client, a proxy may convert the 1.1\nresponse to a 1.0 response by rewriting the version number in the\nresponse status line\".\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: If-ID and UnlessID Precondition",
            "content": "Roy T. Fielding:\n>[...]\n>In summary, the proposal now looks like:\n>\n>      200 == OK\n>      206 == Partial Content\n>      304 == Use Local Copy [includes the Content-ID of the copy chosen]\n>      412 == Precondition False\n>\n>   GET unless we would be given one of the variants already cached:\n>\n>     Request:   GET + Unless-ID\n>     Response:  304 if Content-ID  is in Unless-ID\n>                200 if Content-ID not in Unless-ID\n>\n>   Cache validation:\n>\n>     Request:   GET + IMS + Unless-ID\n>     Response:  304 if Content-ID  is in Unless-ID\n>                200 if Content-ID not in Unless-ID\n\nMake that:\n\n     Response:  304 if Content-ID  is in Unless-ID and not modified since\n                200 if Content-ID not in Unless-ID or modified since\n\n>   Partial GET (w/OK response if changed):\n>\n>     Request:   GET + Range + If-ID\n>     Response:  206 if Content-ID  is in If-ID\n>                200 if Content-ID not in If-ID\n>\n>   Partial GET (w/bad response if changed):\n>\n>     Request:   GET + Range + If-ID + [some indicator in Range]\n>     Response:  206 if Content-ID  is in If-ID\n>                412 if Content-ID not in If-ID\n>\n>   Precondition for all other methods:\n>\n>     Request:   method + If-ID\n>     Response:  2xx if Content-ID  is in If-ID\n>                412 if Content-ID not in If-ID\n>\n>     Request:   method + Unless-ID\n>     Response:  412 if Content-ID  is in Unless-ID\n>                2xx if Content-ID not in Unless-ID\n>\n>\n>I think that is a reasonable and simple design.\n\nI agree. \n\nNow we can argue about the syntax of the Content-ID.  You write\n\n               Content-ID  =  \"Content-ID\" \":\" cid\n               cid  =  <a content-id as defined in RFC 1521>\n\nI believe this means \n\n               cid = local-part \"@\" domain\n\nDo we really want the domain?  I think it would not hurt to make the\nwhole thing an opaque string.  I don't think that a plaintext domain\nstring has enough value to justify the bandwidth.  Because of spoofing\nissues, you can never use a cid without knowing its resource URI\nanyway.\n\nA quick calculation: to uniquely identify a version of a normal\nresource on a UNIX based server, you would need:\n\n  - server ip number:     32 bits\n  - server port number:   16 bits (?)\n  - file inode:           32 bits (?)\n  - file ctime:           32 bits\n                         --------- + \n                         112 bits\n\nin base 64, this would be 19 characters.  Small enough.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "[This was meant for the least -- ange]\n\n------- Forwarded Message\n\nDate:    Sat, 02 Mar 1996 18:59:36 -0500\nFrom:    hallam@w3.org\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: Netscape Bug or KeepAlive Feature ? \n\n\nI have been looking into the action of posting forms. \n\n- --------------\nPOST / HTTP/1.0\nReferer: http://************\nConnection: Keep-Alive\nUser-Agent: Mozilla/2.0b6a (X11; I; OSF1 V3.2 alpha)\nHost: zorch.w3.org:5000\nAccept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\nContent-type: application/x-www-form-urlencoded\nContent-length: 36\n\naction=Submit&text=This+is+some+data\n\n- --------------\n\nNow by my reconing the final CRLF is bogus. I would expect the next \ntransaction to start. The HTTP/1.1 spec does not require a trailing\nCRLF and the entity is precisely 36 bytes:-\n\naction=Submit&text=This+is+some+data\n123456789012345678901234567890123456\n\n\nThis is worrying me, is there a plan to permit footers on keepalive \ntransactions and the CRLF is simply an artifact of that. Or is there\na \"feature\" here that people have been writing code ?\n\n\nI got the same result from Mosaic:\n\n- --------------\nPOST / HTTP/1.0\nAccept: */*, q=0.300\nAccept: text/plain\nAccept: text/html\nUser-Agent: Spyglass_Mosaic/2.10 OSF1V2.0 Spyglass/9\nAccept-Language: en\nConnection: Keep-Alive\nReferer: http://www.w3.org/pub/WWW/WIT/Specification/entry.html\nContent-type: application/x-www-form-urlencoded\nContent-length: 36\nExtension: Security/Digest\nExtension: Payment/FirstVirtual\n\ntext=This+is+some+data&action=Submit\n\n- --------------\n\n\nIs this simply me misreading the 1.1 spec or have we got an issue \nhere? It would be \"upsetting\" to discover that forms handling programs\nwere dependent on a bogus trailing CRLF to teminate a www-form-urlencoded\ndata item.\n\n\nPhill\n\n[Isn't it nice to be able to get some coding done with everyone at the \nIETF :-) ]\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: If-ID and UnlessID Precondition",
            "content": "> Now we can argue about the syntax of the Content-ID.  You write\n> \n>                Content-ID  =  \"Content-ID\" \":\" cid\n>                cid  =  <a content-id as defined in RFC 1521>\n> \n> I believe this means \n> \n>                cid = local-part \"@\" domain\n\nActually, I should have put \"msg-id\" as in MIME, which is defined\nby reference to RFC 822:\n\n     msg-id      =  \"<\" addr-spec \">\"\n\nwhich is actually a bit more complicated than <local-part@domain>.\n\nSuffice it to say that we should come up with a set of suggested\nformats.  For instance, I like\n\n     <8fdea782afd3973fg@md5.my.server>\n\nbut there are many other possibilities that give us the desired\ncharacteristics of world-unique, must change if content changes,\nand should not change if content remains the same (for that Request-URI).\n\nThe reason to stay with the MIME syntax is for maximum interoperability.\nWe should ask Ned if there are any plans/desires to change that syntax\nbefore trying to come up with something more compact.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "> Roy T. Fielding:\n>>Koen writes:\n>>> draft-kristol-http-state-mgmt-00 is not orthogonal to the HTTP version\n>>> number: see section 10.  Servers need the version number to know\n>>> whether they should send an old Netscape format Set-Cookie header or a\n>>> new HTTP/1.1 Set-Cookie header.\n>>\n>>This won't work.  For example\n>>\n>>     1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n>>\n>>will result in a 1.1 cookie being sent to a 1.0 user agent.\n> \n> Isn't the 1.1 proxy required to convert the 1.1 response from the\n> server to a 1.0 response before sending it to the user agent?  This\n> conversion would presumably also convert the 1.1 cookie in the\n> Set-Cookie header to a 1.0 cookie.  At least this is how I read\n> Section 3.1 of the draft 1.1 spec.\n\nGood question, but the answer is no (this used to be clear in the draft,\nbut I deleted the original wording because somebody said it\nwas overspecifying future versions -- *sigh*).\n\nNo conversion occurs within the same major version number.\nIn fact, that is the main distinguisher between the major and\nminor version changes.  Any 1.x proxy can safely forward any\n1.x request or response without changing anything except\nthe Request-Line/Status-Line (it always sends its own native version)\nand the Connection, Forwarded, and Host header fields.\n\nVisualizing that ....\n\n              1.0                  1.1\n    1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n           <---------           <---------\n              1.1                  1.0\n\nLikewise,\n\n              1.0                  1.1\n    1.0 UA ---------> 1.1 Proxy ---------> 1.1 Origin Server\n           <---------           <---------\n              1.1                  1.1\n\n              1.1                  1.1\n    1.1 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n           <---------           <---------\n              1.1                  1.0\n\n              1.1                  1.0\n    1.1 UA ---------> 1.0 Proxy ---------> 1.1 Origin Server\n           <---------           <---------\n              1.0                  1.1\n\n>>The only features of HTTP that can depend on a minor version number\n>>change are those that are interpreted by neighbors in the communication.\n> \n> I'm not sure what you means by this; I hope you don't mean \"when\n> passing on a 1.1 response to a 1.0 client, a proxy may convert the 1.1\n> response to a 1.0 response by rewriting the version number in the\n> response status line\".\n\nUmmm, not exactly.  All messages within the same major version\n(e.g., HTTP/1.x) must be sent with the sending application's native\nversion supported.  So, a HTTP/1.1 proxy will always forward a received\nHTTP/1.0 message with a new version of HTTP/1.1 the Status/Request-Line.\n\nI'd say more, but its time to register.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "> Is this simply me misreading the 1.1 spec or have we got an issue \n> here? It would be \"upsetting\" to discover that forms handling programs\n> were dependent on a bogus trailing CRLF to teminate a www-form-urlencoded\n> data item.\n\nYes, existing form handlers were dependent on that trailing CRLF, and\nthat's why it was added to Netscape.  I'm wondering if it would break\nanything if the content-length was just incremented by two.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Thus wrote John Franks:\n>While it is not part of an MGET/MHEAD proposal, a closely related proposal\n>is the Image-size header suggested by Eric Sink.  In the response to a \n>GET for an HTML doc with inlined images a new server should return\n>\n>Image-size: foo.gif100x200 33456\n>Image-size: bar.gif100x120 22543\n>etc.\n>\n>This allows the client to do page layout while waiting for the MGET with\n>all the images.  \n>\n>There is an important principle to keep in mind here.  Any proposal\n>that can't at least match the user's perceived performance which\n>Netscape obtains with multiple connections is not viable.\n\nThere is another important principle, which is generality.  The\nImage-Size: header as proposed is incredibly un-general.  HTTP is\nsupposed to be a generally useful protocol, not just an HTML delivery\nmechanism.\n\nIf you're going to have HTTP headers to describe meta-information\nabout entities inlined within the current entity, it should be at\nleast a little more general, like maybe:\n\nInlined-Object: foo.gif, image/gif; width=100; height=150; colors=16\n\nalthough I remain convinced that, in the case of HTML, it's better to\nput this information inside the <IMG> or <IMAGE> or <FIG> or whatever\ntag rather than make the server try to understand it.  For static\ndocuments, software to automatically update these tags is pretty easy\nto write (and is orthogonal to existing servers, rather than requiring\nthem all to be changed.)  Dynamic documents are built on-the-fly\nanyway, so including extra tag information isn't any more complex than\nincluding HTTP headers.\n\nObviously in either case this information should be considered\nadvisory only, and if it's wrong the display will flicker as the\npresentation is corrected.  People will live with that somehow.\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": ">Yes, existing form handlers were dependent on that trailing CRLF, and\n>that's why it was added to Netscape.  I'm wondering if it would break\n>anything if the content-length was just incremented by two.\n\nMy vote would be to do this. We will have to resolve the problem one way or \nanother to make keepalive work.\n\nAfter all the form encoding is a bit of a hack in any case.\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "The extra CRLF is sent to fix bugs in cgi programs.  The\nproblem stems from the use of PERL to parse cgi input.\nIf you use a getline function it will only return data\nat every CRLF boundary.  If browsers don't send a final\nCRLF at the end of the post data, many cgi programs will\nnever get the data since the post data does not normally\ncontain a CRLF at the end.  \n\nPerhaps we can do something to fix this for the 1.1 draft\nso that we can stop sending the extra CRLF.\n\n:lou\n\nAndy Norman wrote:\n> \n> [This was meant for the least -- ange]\n> \n> ------- Forwarded Message\n> \n> Date:    Sat, 02 Mar 1996 18:59:36 -0500\n> From:    hallam@w3.org\n> To:      http-wg-request@cuckoo.hpl.hp.com\n> Subject: Netscape Bug or KeepAlive Feature ?\n> \n> I have been looking into the action of posting forms.\n> \n> - --------------\n> POST / HTTP/1.0\n> Referer: http://************\n> Connection: Keep-Alive\n> User-Agent: Mozilla/2.0b6a (X11; I; OSF1 V3.2 alpha)\n> Host: zorch.w3.org:5000\n> Accept: image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, */*\n> Content-type: application/x-www-form-urlencoded\n> Content-length: 36\n> \n> action=Submit&text=This+is+some+data\n> \n> - --------------\n> \n> Now by my reconing the final CRLF is bogus. I would expect the next\n> transaction to start. The HTTP/1.1 spec does not require a trailing\n> CRLF and the entity is precisely 36 bytes:-\n> \n> action=Submit&text=This+is+some+data\n> 123456789012345678901234567890123456\n> \n> This is worrying me, is there a plan to permit footers on keepalive\n> transactions and the CRLF is simply an artifact of that. Or is there\n> a \"feature\" here that people have been writing code ?\n> \n> I got the same result from Mosaic:\n> \n> - --------------\n> POST / HTTP/1.0\n> Accept: */*, q=0.300\n> Accept: text/plain\n> Accept: text/html\n> User-Agent: Spyglass_Mosaic/2.10 OSF1V2.0 Spyglass/9\n> Accept-Language: en\n> Connection: Keep-Alive\n> Referer: http://www.w3.org/pub/WWW/WIT/Specification/entry.html\n> Content-type: application/x-www-form-urlencoded\n> Content-length: 36\n> Extension: Security/Digest\n> Extension: Payment/FirstVirtual\n> \n> text=This+is+some+data&action=Submit\n> \n> - --------------\n> \n> Is this simply me misreading the 1.1 spec or have we got an issue\n> here? It would be \"upsetting\" to discover that forms handling programs\n> were dependent on a bogus trailing CRLF to teminate a www-form-urlencoded\n> data item.\n> \n>                 Phill\n> \n> [Isn't it nice to be able to get some coding done with everyone at the\n> IETF :-) ]\n> \n> ------- End of Forwarded Message\n\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "Ari Luotonen wrote:\n> \n> > Is this simply me misreading the 1.1 spec or have we got an issue\n> > here? It would be \"upsetting\" to discover that forms handling programs\n> > were dependent on a bogus trailing CRLF to teminate a www-form-urlencoded\n> > data item.\n> \n> Yes, existing form handlers were dependent on that trailing CRLF, and\n> that's why it was added to Netscape.  I'm wondering if it would break\n> anything if the content-length was just incremented by two.\n> \n\nI think it would break binary files in weird ways.  For instance\ntransfering an executable would cause it to gain 2 bytes on\nevery transfer.  It would certainly break anything that contained\na checksum.\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "URI http: specificatio",
            "content": "Hello!\n\nI apologize if this question is inappropriate in this forum, but it does\nraise an issue with the current specification that may be of interest to\nthis group. If it is inappropriate, then please indicate what you feel\nwould be a more correct venue.\n\nI'm having a problem resolving the \"correct\" specification for URIs using\nthe \"http\" scheme.\n\nIn the HTTP specification (draft-ietf-http-v10-spec-05.html) the grammar\nshows that \"params\" are legal in a URI in section 3.2.1 and 3.2.2 (http_URL\nhas an abs_path which allows params).\n\nThe conflict is where the HTTP spec refers to RFC 1738, section 3.3. That\nsection does not show parameters.\n\nWhich is the correct form? I would like to use the parameters in a system I\nam putting together (passing parameters in the query section is a bit weird\nsemantically). Most software seems to deal with the params fine, but I'd\nlike some form of acceptance before relying on this aspect of URIs.\n\nThank you.\n\nRegards,\n\nGreg Stein, eShop Inc.\ngreg_stein@eshop.com\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": ">Perhaps we can do something to fix this for the 1.1 draft\n>so that we can stop sending the extra CRLF.\n\nAs I said to Ari, I don't really care about CRLF or not CRLF. We can add any \nnumber of bytes at the ent of the URI and it does not much bother me.\n\nWhat bothers me is that the content-length given is wrong. If we rewrite the \nwww-uri-encoded-form spec to allow (sounds as if we need to require) trailing \nCRLF and give the size of the message as the complete chunk up to and including \nthe CRLF.\n\nI know this sounds picky but that is what stards are about:-)\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": ">I think it would break binary files in weird ways.  For instance\n>transfering an executable would cause it to gain 2 bytes on\n>every transfer.  It would certainly break anything that contained\n>a checksum.\n\nI wasn't suggesting this. I was suggesting that we change the \nspec for www-url-encoded-form. I would not want to change the way \nbinary files are transmitted!\n\nThe problem is that netscape is saying it is sending a content length of 36 \nbytes but actually transfering 38. This is a bug IMHO. Whenever a length of 36 \nis stated it should mean 36.\n\nIf Netscape is sending a bogus CRLF after binary files then I would expect \nproblems to arise in any case. \n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: remove PATCH, COPY, MOVE, DELETE, etc.? Upgrade",
            "content": "Shel writes:\n    Dave Long writes:\n     > \n     > >>semantically equivalent to a CREATE.  For now I'm using a \"magic\"\n     > >\n     > >Why isn't put of a non-existent object good enough?\n     > \n     > It *is* good enough.\n     > \n     > I was referring to a problem which arose in the context of\n     > version-control, where one might wish a PUT request to fail\n     > if there was an existing (possibly very recently created)\n     > object.\n     > \n     > That problem is probably outside the scope of this working-group;\n     > perhaps I shouldn't have piggy-backed it upon my reply to a\n     > different issue.\n     > \n     > -Dave\n    \n    It seems to me to be very much in the domain of this working group,\n    and has been discussed before, in connection with conditional requests\n    of all sorts.  There was a discussion about \"unless-modified-since\"\n    (or \"if-valid\") headers some time back, to make the action of PUT\n    conditional on the server's version still being the same as the most\n    recent one the PUTter had received.  I don't think this has been\n    fully worked out yet.\n    \nThis should be possible to do with the opaque-validator mechanism\nI described in draft-mogul-http-caching-00.txt (and probably with\nmodification-date-based validators, or logic bags, as well).\n\nTwo similar situations:\n\n(1) PUT should fail if resource already exists\nPUT /foo HTTP/1.1\nIf-valid: \"\"\n\nThe null validator (\"\") is defined to never match the actual\nvalidator, so if the resource /foo exists, this PUT fails.\nWe would need to add a few words explaining what the server\nshould do for a non-existent resource (i.e., treat it as if\nit had a null validator).\n\n(2) PUT should fail if the underlying resource has been\nchanged since the last time that the client retrieved it:\nPUT /foo HTTP/1.1\nIf-valid: \"asdka123897\"\nassuming that \"asdka123897\" was the validator returned from\na previous GET, this request succeeds if the resource is\nunchanged, fails otherwise.\n\n-Jeff\n    \n\n\n\n"
        },
        {
            "subject": "Feature Negotiation Headers and Calculations, v",
            "content": "I wrote this in the past few hours, as background material for\npossible feature negotiation discussions at the IETF (which I won't be\nattending).  I hope that I will be able to expand section 3 soon.\n\n  Koen.\n\n\n------snip-----\n\nEXPERIMENTAL DEFINITION\n\n            Feature Negotiation Headers and Calculations\n            ---------------------------------------------\n\n                                          Koen Holtman, koen@win.tue.nl\n                                          Version 1, 4 Mar 1996\n\n\nINTRODUCTION\n\n   Parts of this document are unfinished.\n\n   This is an experimental definition.  It is expected that the\n   mechanism defined here, and the definition itself, can be improved\n   if more work is done.\n\n   Feature negotiation as we currently think of it will have lots and\n   lots of different feature identifiers.  The main purpose of this\n   experiment is to find a possible definition of a feature\n   negotiation mechanism that will not put pressure on browsers to\n   send very long lists of all supported features in every request.\n\n   This document should be read in the context of\n   draft-holtman-http-negotiation-00.txt, which should be read in the\n   context of draft-ietf-http-v11-spec-01.txt.  The meaning and\n   purpose of `feature negotiation' is assumed to be known.\n\n\n1. Overview\n\n   Feature negotiation as defined here is a refinement of the content\n   negotiation mechanism defined in\n   draft-holtman-http-negotiation-00.txt.  The mechanism is refined\n   by adding extra headers and header elements:\n\n    o Accept-Features\n    o Content-Features\n    o {features ...} attribute for the Alternates header.\n\n   The definitions of the overall quality calculations in\n   draft-holtman are extended with a new factor: qf for `feature\n   quality factor'.  Also, the rule associated with the\n   reactive-on-wildcard directive is extended.\n\n\n2. Feature identifiers\n\n   This document does not define what a feature identifier (fid) looks\n   like.  For the purpose of this document, we use\n\n     fid = token\n\n   where token is defined in draft-ietf-http-v11-spec-01.txt.  It is\n   assumed that tokens are not very long: a typical length would be 5\n   to 10 characters.  Examples of fids used in this document are:\n\n     f1  f2  f3\n\n   The mechanisms defined in this document will work with fids much\n   longer than 10 characters, but they will be less efficient in this\n   case.\n\n   Some ways to get short fids:\n\n     - The feature registration process assigns a short fid to every\n       registered feature.\n\n     - Fids are the sequence numbers (in base 10, 16, or 64) of the\n       feature registration process.\n\n     - The feature registration authority pre-assigns blocks of\n       numbers to browser authors, and registers some of these numbers\n       as feature identifiers later.\n\n     Note: If fids _must_ be long for some reason, not all is lost:\n     see the Accept-Feature-Hashed definition at the end of this text.\n\n\n3. Requirements on feature negotiation mechanisms\n\n\n   This section lists some requirements for feature negotiation\n   mechanisms.  It can be used as a checklist when considering (other)\n   definitions of such mechanisms.\n\n\n   TBS\n\n\n4. Content-Features header\n\n   The Content-Features header specifies some of the features that\n   need to used for rendering the enclosed entity.\n\n      Content-Features = \"Content-Features\" \":\" 1#fid\n\n   This header is purely informational, this specification does not\n   intend there to be some required use of this header.\n\n\n5. Features attribute of the Alternates header\n\n   The features attribute of the Alternates header describes some of\n   the features that need to used for rendering the alternate.\n\n       Alternates = \"Alternates\" \":\" 1#( alternate-descr  \n                                       | caching-directive )\n\n       alternate-descr = \n               \"{\" <\"> URI <\"> source-quality\n                   [ \"{\" \"type\" <\"> media-type <\"> \"}\" ]\n                   [ \"{\" \"language\" <\"> 1#language-tag <\"> \"}\" ]\n                   [ \"{\" \"encoding\" <\"> 1#content-coding <\"> \"}\" ]\n                   [ \"{\" \"length\" 1*DIGIT \"}\" ]\n                   [ \"{\" \"description\" quoted-string \"}\" ]\n                   [ \"{\" \"features\" feature-list \"}\" ]\n                   [ extension-attribute ]\n               \"}\"\n\n       source-quality = qvalue\n\n       feature-list = 1#feature-list-element\n\n       feature-list-element = ( fid | fid-bag ) [ \"/\" missing-degradation ] \n\n       fid-bag = \"[\" 1#fid \"]\"\n\n       missing-degradation = qvalue\n\n   If a missing-degradation value is not present in a feature list\n   element, the default value is 0.\n\n   TBS [text about interpretation of feature list elements, see the\n   example below for now]\n\n   An example is:\n\n       Alternates: {\"blah.1.html\" 1.0\n                   {type \"text/html\"} {features f1, f2/0.95}},\n                   {\"blah.2.html\" 1.0\n                   {type \"text/html\"} {features [f1, f3], [f4, f5]/0.8}},\n\n   The two feature lists could be written in words as: \"blah.1.html\n   needs the feature f1 for rendering.  The availability of feature f2\n   is also desired, if this feature is missing, the overall quality of\n   the end result will decrease with a factor 0.95.\", and \"blah.2.html\n   needs the feature f1 or the feature f3 for rendering.  The\n   availability of feature f4 or f5 is also desired, if both are\n   missing, the overall quality of the end result will decrease with a\n   factor 0.8.\".\n\n   Note: If an overall quality of 0.5 decreases with a factor 0.95,\n   the resulting quality is 0.5*0.95=0.475, NOT 0.5-(0.5*0.475)=0.025.\n\n   Note: The fid-bags are needed \n      1) to cope with overlap in features (this page can be rendered\n         with the <blah> tag implementations of both browser X and\n         browser Y) and\n      2) to allow for conditional HTML\n\n\n6. qf calculation for reactive negotiation\n\n   If feature negotiation is included into content negotiation, the\n   definition for the overall quality in reactive negotiation becomes\n\n      Q = qs * qe * qc * ql * q * qml * qf\n\n   where the computation of qf, the feature quality factor, is\n   defined as follows.\n\n      qf The feature quality factor for the alternate is 1 if there is\n         no features attribute in the alternate description.  If there\n         is a features attribute in the alternate description, it is\n         the product of the individual degradation factors iqf of the\n         elements of the included feature list.  The iqf of a feature\n         list element equals 1 if the element is supported by the user\n         agent, and equals the missing-degradation factor of the\n         element (default 0) if the element is not supported.  A\n         feature list element of the form fid is supported if the\n         corresponding feature is available in the user agent.  A\n         feature list element of the form fid-bag is supported if at\n         least one of the corresponding features is available in the\n         user agent.\n\n\n7. Accept-Features header\n\n   The Accept-Features request header specifies some of the features\n   which are available or are not available in the user agent.  It is\n   indented that user agents send short Accept-Features headers by\n   default.\n\n      Accept-Features = \"Accept-Features\" \":\" 1#( fid | not-fid | \"*\" )\n\n      not-fid = \"/\" fid\n\n   A fid form in this field indicates that the corresponding feature\n   is available.  A not-fid form in this field indicates that the\n   corresponding feature is not available. \n\n   If the wildcard \"*\" is included, this indicates that all features\n   not explicitly listed as a fid or a not-fid must be considered\n   available when a preemptive negotiation calculation is performed.\n   If the wildcard \"*\" is not included, all features not explicitly\n   listed must be considered unavailable.  An example is:\n\n      Accept-Features: f1, /f2, /f3, f5, *\n\n   This header indicates that the features f1 and f5 are available in\n   the user agent.  When a preemptive negotiation calculation is\n   performed, the header indicates that all features except f2 and f3\n   are available.\n\n   The wildcard \"*\" and the not-fid form are useful for for\n   optimizing the negotiation process if the reactive-on-wildcard\n   directive is used in the Accept header.\n\n      Note: draft-holtman has the following to say about short accept\n      headers: \"As most resources will be un-negotiable, user agents\n      are encouraged to send empty or small accept headers, or even\n      omit some accept headers entirely, by default.  If a user agent\n      knows or discovers that an origin server provides negotiated\n      resources, it is encouraged to use data from the negotiated\n      responses received so far to dynamically add or extend accept\n      headers sent in future requests on resources provided by that\n      origin server, in order to increase the probability that\n      preemptive negotiation can be used instead of the slower\n      reactive negotiation.\".  This also applies to the\n      Accept-Features header.\n\n\n8. qf calculation for preemptive negotiation\n\n      Note: this section contains the tricky part.\n\n   If feature negotiation is included into content negotiation, the\n   definition for the overall quality in preemptive negotiation\n   becomes\n\n      Q = qs * qe * qc * ql * q * qml * qf\n\n   where the computation of qf, the feature quality factor, is\n   defined as follows.\n\n      qf The feature quality factor for the alternate is 1 if there is\n         no features attribute in the alternate description.  If there\n         is a features attribute in the alternate description, the\n         factor is 1 if there is no Accept-Features header present in\n         the request.  If there is a features attribute in the\n         alternate description, and an Accept-Features header is\n         present in the request, the feature quality factor is the\n         product of the individual degradation factors iqf of the\n         elements of the included feature list.  The iqf of a feature\n         list element equals 1 if the element is indicated as\n         supported by the Accept-Features header, and equals the\n         missing-degradation factor of the element (default 0)\n         otherwise.  A feature list element of the form fid is\n         indicated as supported if the corresponding feature is\n         available according to the Accept-Features header.  A feature\n         list element of the form fid-bag is indicated as supported if\n         at least one of the corresponding features is available\n         according to the Accept-Features header.\n\n   For example, the header\n\n      Accept-Features: f1, /f2, /f3, f5, *\n\n   would cause the following iqf factors to be assigned:\n\n      f1             = 1\n      f2/0.95        = 0.95\n      f99            = 1\n      [f1, f3]       = 1\n      [f4, f5]/0.8   = 1\n      [f2, f3]       = 0\n      [f2, f3, f99]  = 1\n\n\n9. Extension of conditions for sending a preemptive response if a\n   reactive-on-wildcard directive is included in the Accept header\n\n   Section 5.2 of draft-holtman says:\n\n     If the best alternate has an overall quality factor greater than\n     zero, and an Accept header in the request contains a\n     reactive-on-wildcard directive, then the server may generate a\n     preemptive response, provided that the origin server restriction,\n     if applicable, is met, if\n\n       o the type quality factor (q) of the best alternate was not\n         derived from a match to a media range containing an asterisk\n         \"*\" wildcard character in an Accept header, and\n\n       o the language quality factor (ql) of the best alternate was\n         not derived from a match to a \"*\" language-range in the\n         Accept-Language header.\n\n     In all other cases, the server must generate a reactive response.\n\n   If feature negotiation is included into content negotiation, a\n   third condition must be added:\n\n     o the feature quality factor (qf) of the best alternate was\n       derived without using the \"*\" wildcard, if present, in the\n       Accept-Features header.\n\n\n10. Accept-Features-Hashed\n\n      Note: the text below is too sketchy.  This section can be\n      ignored, the rest of this text does not depend on it.\n\n      Note: this header could be used instead of the Accept-Features\n      header if fids must be long for some reason.  Note that the fids\n      in the Alternates header will _not_ be hashed under the scheme\n      described here.\n\n   The Accept-Features-Hashed request header specifies hash values of\n   some of the features which are available or are not available in\n   the user agent.  It is indented that user agents send short\n   Accept-Features-Hashed headers by default.\n\n      Accept-Features-Hashed = \"Accept-Features-Hashed\" \":\"\n                      1#( fid-hashed | not-fid-hashed | \"*\" )\n\n      fid-hashed = <the hash value of a fid (for example a 32 bit hash\n                    encoded in base 64, hash function to be specified)>\n\n      not-fid = \"/\" fid-hashed\n\n   The interpretation and use of this header is the same as that of\n   the Accept-Features header, modulo the hashing.\n\n   In a preemptive negotiation feature quality factor calculation, a\n   fid-hashed value matches any fid which hashes to that value.  This\n   means that there is a possibility that hash collisions produce a\n   preemptive response which includes the wrong alternate.  The user\n   agent should do a reactive negotiation calculation to find out if\n   the correct alternate was sent.  If it is found that hash\n   collisions caused the wrong alternate to be sent, the user agent\n   can do a direct request to retrieve the real best alternate.  As\n   hash collisions will be rare, no noticeable loss of performance is\n   to be expected because of this mechanism.\n\n\n[End of text.]\n\n\n\n"
        },
        {
            "subject": "Re: Cache validator",
            "content": "    That is not what I mean.  First, I don't think that opaque\n    validators are necessary -- they may be useful, but not necessary.\n\nOf course they are not \"necessary.\"  But then, neither is caching.\n\n    However, I am willing to give-in to that notion IF the opaque\n    validator is sufficiently useful to cover the cost of sending it.\n    That is, the opaque validator must be generally interoperable with\n    existing systems and carry sufficient semantics for use for things\n    other than cache updates.\n\nFirst of all, the generally understood meaning of the word \"opaque\"\nis \"has no meaning to clients\", and therefore if you want the\nvalidator to carry other semantics, you're not talking about an\n\"opaque\" validator.\n\nSecond, while I agree with your emphasis on the cost of sending\nit (vs. the benefit, of course), I have no idea what your next\nsentence has to do with \"cost\".  Interoperability is a requirement\nof all HTTP/1.1 additions; it's not a \"cost\" issue.\n    \n    In order to provide that additional usefulness, we need three things:\n    \n       1) A guarantee that the validator will change if the content changes\n  and should not change if the content remains the same;\n       2) A guarantee that the validator is byte-comparable (i.e., equal\n  validators mean equal content);\n       3) A guarantee that the validator is world-unique.\n    \n    (1) is obvious.\n\nNot necessarily.  To be useful as a cache validator (unburdened\nby any other semantics), it is sufficient that the value changes\nif the content is semantically different.  It is not necessary\nthat the value change on every insignificant change in the content\n(where \"significant\" is defined by the application that generates\nthe content).\n\n    (3) is necessary for it to be used as a cache key.\n\nWho said anything about using the validator as a cache key?  This\nis something that might be an interesting design point (using\nsomething beside the URL as a cache key) but this is so clearly\nat variance with current practice that I think you are being\nhighly inconsistent with your insistence (stated elsewhere) that\nwe ought to stick to current practice.\n    \n    Not too surprisingly, this also happens to be the definition of\n    Content-ID in MIME.  Therefore, for maximum interoperabilty with\n    existing systems, we should use Content-ID if we are to have an\n    opaque validator.\n    \nWhat existing systems?\n\n    So, if people would like a simple precondition syntax that is useful\n    for all of the currently identified protocol needs, including cache\n    validation, byte ranges, and content negotiation, then I have the following\n    suggestion:\n    \n       1) Require Content-ID in HTTP/1.1 responses\n    \n Content-ID  =  \"Content-ID\" \":\" cid\ncid  =  <a content-id as defined in RFC 1521>\n    \n       2) Implement the following precondition syntax:\n    \n If-ID  =  \"If-ID\" \":\" 1#cid\n    \n  wherein the condition evaluates to true if the response to the\n  request would have had a Content-ID equal to one of the ones\n  given in the If-ID header field value.  Like the current definition\n  of Unless in draft 01, the response to a \"false\" evaluation\n  depends on whether or not Range or IMS is also present.\n    \n    That should make a sufficient number of people happy to make the\n    overhead of doing it worthwhile.  If not, then the only reasonable\n    solution is to use an IF header field with a generic syntax.\n\nI don't think it's possible to solve both the conditional-GET\nproblem and the byte-range problem with a single If-ID header;\nwe would need both If-ID and If-not-ID (or some other syntax).\nWhile you try to get around this by using this table:\n\n                        ID is in    ID not in\n                        Check-ID    Check-ID (or IMS true)\n                        ---------   ---------\n    GET                    200         412\n    GET + IMS              304         200\n    GET + Range            206         412\n    GET + IMS + Range      206         200\n    other methods       as normal      412\n\nit's not at all clear that this is easier to implement than\nsimply having two different headers, and probably easier to\nget the specification right (Koen has pointed out several\nproblems).  And this is almost certainly not extensible!\n\nAs Koen points out, there are circumstances when one needs\n\"action-if-something-matches\" and other circumstances when\none needs \"no-action-if-something-matches\", so why not just\ndo the obvious thing and have paired headers?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "/*\n * \"Re: Connection Header\" by john@math.nwu.edu (John Franks)\n *    written Sun, 18 Dec 1994 09:55:02 -0600 (CST)\n * \n * According to Henrik Frystyk Nielsen:\n **  Using Allow has the same problem as if a Connection header is\n ** passed through a proxy. Imagine the following setup\n ** \n ** new client -> old proxy -> new server\n ** \n ** On the first request, the new server will then send back a\n ** \n ** Allow: GET, MGET, HEAD, etc.\n ** \n ** On the second request, the new client will try a MGET, but the\n ** proxy doesn't understand this and then we are back to the old\n ** problem having lost one roundtrip time.\n *\n * I don't see this as a very big problem.  We have lost one roundtrip\n * time, but only a roundtrip between client and proxy.  In normal use\n * client and proxy are very close with low latency.  I don't think\n * this is a very big price to pay.  This proposal then has the big\n * advantage that it is backwards compatible.  It isn't necessary to\n * try to change HTTP1.0 proxies to get them to strip out anything. (I\n * am assuming that current proxies will return a standard error\n * status/message for an unrecognized method.)\n\nThis is true, but why not modify either Connection or Allow:MGET so\nthey are ignored when a client is talking to a proxy? That is, if I'm\na Connection or Allow:MGET aware client, I ignore these headers unless\nI also get Proxy-connection or Proxy-allow:MGET?\n\n * The criticism of MGET that it is not as general as keep-open is\n * true, but I think there is serious danger of performance\n * degradation if that generality is used.  More likely unless those\n * concerns can be met keep-open would not be widely implemented.\n\nI agree... for instance, I'm not sure how I would implement something\nto keep track of how idle multiple server processes are without using\nshared memory or writable mmap()ed files, which I already have found\nare not available on certain platforms we support...\n\n * I would assume that an MGET addition to the protocol would be\n * accompanied by a parallel MHEAD method.  It would still not be\n * possible to mix GETs and POSTs or even do multiple POSTs. I don't\n * see any pressing demand for these, but perhaps I am just not aware\n * of it.\n\nI'm not sure where the demand for this is either... The generality of\na session seems to me to be not that great a win if the session is\nstill a batch request-response cycle. I'd rather have real sessions\nsimilar to what Simon proposes in HTTP-NG, where we can have\ninteractive stock quotes streaming in through one window while at the\nsame time the user is browsing the same server's other HTML documents,\nall in one connection.\n\n * There is an important principle to keep in mind here.  Any proposal\n * that can't at least match the user's perceived performance which\n * Netscape obtains with multiple connections is not viable. It's a\n * competitive world out there and browser writers will have to go\n * with multiple connections if nothing else matches their\n * performance.  Any proposed standard, MGET or stay-open, which can't\n * measure up to multiple connection performance just won't be\n * implemented.\n */\n\nYes, and to that end we have to keep in mind that while <img width=N\nheight=M> is useful, it means that in order to see that performance\nwin, everyone has to edit their HTML. That will take time, and\nNetscape's perceived performance works everywhere without having to\nedit (or parse) every single HTML doc on a server...\n\n--Rob\n\n\n\n"
        },
        {
            "subject": "Can proxies rewrite Date:",
            "content": "Henrik Frystyk Nielsen writes:\n\n    John Franks writes:\n    \n    > What about the date header being part of the data digested for the\n    > message-digest.  Is it unsafe to assume that proxies will not mangle\n    > the origin-server Date: header?\n    \n    Caching proxies will do so.\n\nHuh?  Do you mean mean \"existing caches do so\" or \"the HTTP/1.1 spec\nallows them to do so?\"  The latter is probably false (although not\nexplicitly so).\n\nOur discussions (in the caching subgroup) have assumed that Date:\nis set when the response is generated by the origin server, and\nwe have used this assumption to define the meaning of Expires:.\nIf proxies rewrite Date: response headers, we are in some trouble.\n\n-Jeff\n\nP.S.: We're OK if the rewriting results in semantically equivalent\ndates (i.e., only a format change), but not if the \"mangling\" is\nmore general.\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "On Mon, 4 Mar 1996, Lou Montulli wrote:\n> The extra CRLF is sent to fix bugs in cgi programs.  The\n> problem stems from the use of PERL to parse cgi input.\n> If you use a getline function it will only return data\n> at every CRLF boundary.  If browsers don't send a final\n> CRLF at the end of the post data, many cgi programs will\n> never get the data since the post data does not normally\n> contain a CRLF at the end.  \n\nPerl's getline (or <STDIN>, etc) uses both CRLF or EOF - when a server is\nreceiving POST data, and it has received as many bytes as the content-length\nheader says it should have received, doesn't the server close the stdin pipe\nto the CGI script?  Or is it left up to the CGI script to start responding\nautonomously? \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: remove PATCH, COPY, MOVE, DELETE, etc.? Upgrade",
            "content": "Jeffrey writes:\n   \n> This should be possible to do with the opaque-validator mechanism\n> I described in draft-mogul-http-caching-00.txt (and probably with\n> modification-date-based validators, or logic bags, as well).\n> \n> Two similar situations:\n> \n> (1) PUT should fail if resource already exists\n> PUT /foo HTTP/1.1\n> If-valid: \"\"\n> \n> The null validator (\"\") is defined to never match the actual\n> validator, so if the resource /foo exists, this PUT fails.\n> We would need to add a few words explaining what the server\n> should do for a non-existent resource (i.e., treat it as if\n> it had a null validator).\n> \n> (2) PUT should fail if the underlying resource has been\n> changed since the last time that the client retrieved it:\n> PUT /foo HTTP/1.1\n> If-valid: \"asdka123897\"\n> assuming that \"asdka123897\" was the validator returned from\n> a previous GET, this request succeeds if the resource is\n> unchanged, fails otherwise.\n\nThese are the situations which I alluded to; I am pretty sure\nthat my use of Derived-From with the magic Content-Version (\"nonexistent\")\nis isomorphic to If-Valid with the magic null validator (\"\")\n\n-Dave\n\n\n\n"
        },
        {
            "subject": "TRACE:  HTTP/1.1 open issu",
            "content": "To avoid his getting lost, I wanted to put the following item out as, I\nbelieve, an open issue for HTTP/1.1.\n\nI like the idea of the TRACE method.  However, I would like some discussion\nabout its intended use, which affects how it should be specified.\n\nHere's my concern:  can TRACE take an Entity in the request?  If not, I see\nno implementation problems, but TRACE is less useful.  (It would be nice to\nreplace any method, including POST and PUT, with TRACE and see what the\nserver thought it got.)  If so, either there must be a content-length, or\nthere must be chunks or MIME multipart to delimit it.  If either of the\nlast two, my question is:  what is the valid form for reflecting that\nentity to the sender?  That is, must the server send the entity back to the\nclient exactly as it saw it?  Or can the server consume the entity and\nregurgitate it in its favorite form?\n\nConsider this request:\n\nTRACE * HTTP/1.1\nTransfer-encoding: chunked\n\n1\na\n1\nb\n1\nc\n1\nd\n1\ne\n0\n\nIs this a (one of many, given that chunking can be done many ways) valid\nresponse?  (Boy, is this hard to do, and I probably got it wrong!)\n\nHTTP/1.1 200 OK\nTransfer-encoding: chunked\n\n18                                      ; count for response line\nTRACE * HTTP/1.1\n28                                      ; count for line\nTransfer-encoding: chunked\n2                                       ; count for blank (CRLF) line\n\n3                                       ; count for count of regurgitated chunk\n5\n7                                       ; count for regurgitated chunk + CRLF\nabcde\n3                                       ; count for end-of-chunks 0 + CRLF\n0\n0                                       ; end of response chunking\n\nThe alternative (in case you're wondering) is that the server must reflect\nback *exactly* what it got, including the exact incoming chunking or MIME\nmultipart separators.  I believe this alternative is unacceptably difficult\nfor the server:  for chunking, for example, the server cannot just read\neverything \"to the end\", because it doesn't know where the end is unless it\ninterprets the chunks.  That means it would have to remember the chunk\nsizes and reproduce them exactly, which, in turn, probably means two\nseparate routines to handle chunked input.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "> Our discussions (in the caching subgroup) have assumed that Date:\n> is set when the response is generated by the origin server, and\n> we have used this assumption to define the meaning of Expires:.\n> If proxies rewrite Date: response headers, we are in some trouble.\n\nNeither one of the proxies I've written changes the Date: header, and\nI'm not aware of any proxy that does it.  I would say it's safe to\ntrust the Date: header not to be mangled by intermediaries.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Compression of HTML documents over HTT",
            "content": "Just a thought, but looking at my server there are a number of documents which if I PKZIP them, reduce in side anywhere from 20-60%\nAs bandwidth becomes more of a premium, and response times more critical as the number of users increase would it be appropriate to implement some form of compression of the raw text in the .HTM / .HTML documents ?\n\nObviously this would require support at two places - compression (on the fly - as some documents need to refresh frequently, or cached) at the server, and at the browser.\nAs long as the (de)compression algorithm was fast enought to reduce the overall transmission time there are benefits.\n\nJeremy E Cath\nJeremy_Cath@Sterling.Com\nhttp://193.130.124.60/rufan-redi\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "Phill wrote:\n> The problem is that netscape is saying it is sending a content length of 36\n> bytes but actually transfering 38. This is a bug IMHO. Whenever a length of\n> 36  is stated it should mean 36.\n\nI agree. \n\nBy the way, I encountered this last Fall while working out the keep-alive \nsupport in WebSite 1.1. My solution was to accept blank lines prior to the \nstart of the next HTTP request.\n\n  -- Bob\n\n\n\n"
        },
        {
            "subject": "Re: Compression of HTML documents over HTT",
            "content": "> \n\nI'm not [the prime] authority on this subject, but some browsers handle\ncompress'd and gzip'ed content well.  However, they are not well-behaved\nenough to indicate whether they do or not (concerning Mozilla and NCSA Mosaic),\nso the server doesn't know about that.\n\nIt seems that Accept-Encoding: and Content-Encoding: serve this purpose well,\nthe browsers just do not seem to send them (although Content-Encoding is\nrecognized in browsers above, even Netscape2.0 proved not to send\nAccept-Encoding, but IMHO this issue may better be discussed through\nwww-talk [in HTTP/1.0 draft it's covered well enough, just the current practice\nstays behind]).\n\n> Just a thought, but looking at my server there are a number of documents which if I PKZIP them, reduce in side anywhere from 20-60%\n> As bandwidth becomes more of a premium, and response times more critical as the number of users increase would it be appropriate to implement some form of compression of the raw text in the .HTM / .HTML documents ?\n> \n> Obviously this would require support at two places - compression (on the fly - as some documents need to refresh frequently, or cached) at the server, and at the browser.\n> As long as the (de)compression algorithm was fast enought to reduce the overall transmission time there are benefits.\n\ngzip/ungzip proved to be fast enough to speed up transfer, even when fetching\nfrom local server (from the same host).  on-the-fly-compression overhead on\nserver side may be considerable enough to advise caching of compressed files\n(on server), but again we go off the HTTP issues.\n\n[1]T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen,\n\"Hypertext Transfer Protocol -- HTTP/1.0\", 02/19/1996.  URL :\nhttp://www.rasip.fer.hr/cgi-bin/draft/draft-ietf-http-v10-spec-05.txt\n(or your nearest site), chapter 10.3, D.2.3\n\n> \n> Jeremy E Cath\n> Jeremy_Cath@Sterling.Com\n> http://193.130.124.60/rufan-redi\n> \n\n-- \n    | Mirsad Todorovac|\n    | Faculty of Electrical Engineering and Computing|\n    | University of Zagreb|\n    | Unska 3, Zagreb, Croatia 10000|\n    ||\n    | e-mail: mirsad.todorovac@fer.hr|\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "subscribe\n\n\n\n"
        },
        {
            "subject": "Re[2]: Compression of HTML documents over HTT",
            "content": "     >It seems that Accept-Encoding: and Content-Encoding: serve this\n     >purpose well, the browsers just do not seem to send them (although\n     >Content-Encoding is recognized in browsers above, even Netscape2.0\n     >proved not to send Accept-Encoding, but IMHO this issue may better be\n     >discussed through www-talk [in HTTP/1.0 draft it's covered well\n     >enough, just the current practice stays behind]).\n     \n     I guess the important thing to do is get it drafted into (if not the \n     standard then) recommended guidelines, so if browsers / servers want \n     to support http/1.1 they need to support the gzip/ungzip of the html \n     and data they are transfering.\n     \n     If will follow up on the suggested reading, reason for this posting \n     was that it seemed to make sense to do to save bandwidth\n     \n     Jeremy E Cath\n     Jeremy_Cath@Sterling.Com\n     http://193.130.124.60/rufan-redi\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "Roy T. Fielding:\n>\n>No conversion occurs within the same major version number.\n>In fact, that is the main distinguisher between the major and\n>minor version changes.  \n\nOK, I just re-read Section 3.1, and I see now where my interpretation\nwas wrong.  The correct interpretation makes me just as uneasy,\nthough.\n\n>Any 1.x proxy can safely forward any\n>1.x request or response without changing anything except\n>the Request-Line/Status-Line (it always sends its own native version)\n>and the Connection, Forwarded, and Host header fields.\n\nI don't see immediately why Connection, Forwarded, and Host would be\nspecial, but never mind that now, I will worry about such details in\nApril.\n\nI guess the HTTP/1.1 spec will need a long section about\ninteroperating with 1.0 applications.\n\nGeneral comments on section 3.1:\n\n1) Is there any rationale for the rules in Section 3.1?  \n\n2) I'm not sure that I really like the weak requirements for\ntranslation by proxies that are in the draft spec now: these\nrequirements will basically shift the burden of making things\ninteroperable from proxy authors to httpd and CGI authors, and there\nare a lot more CGI authors than proxy authors.\n\n3) With proxies being allowed to upgrade and downgrade the minor\nversion number, it seems that the server, it it gets a 1.1 request,\nwill not be able to find out if there are any 1.0 applications in the\nrequest chain.\n\n4) So the server has to make things interoperable, but it does not\neven know the capabilities (versions) of the software it is serving\nto!  So, for example, if the server uses a 1.1 Cache-Control header,\nit must always include a 1.0 Expires header.\n\n5) Wouldn't things be easier if we did a major version number change\nand defined HTTP/2.0 instead of HTTP/1.1?\n\n6) In summary, I suspect that the version rules will need to be\nseriously reviewed.\n\nI want to propose adding the issue \"version rules in Section 3.1\" to\nthe HTTP/1.1 issues list.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": ">Phill wrote:\n>> The problem is that netscape is saying it is sending a content length of 36\n>> bytes but actually transfering 38. This is a bug IMHO. Whenever a length of\n>> 36  is stated it should mean 36.\n>\n>I agree.\n>\n>By the way, I encountered this last Fall while working out the keep-alive\n>support in WebSite 1.1. My solution was to accept blank lines prior to the\n>start of the next HTTP request.\nMy implementations also accept blank lines before the start of the next request.\n\nHowever lets be really clear. When you say Content length the data stream\nmust be exactelly as follows: (Note: I am making each CR and LF explicit.\nIgnore line endings, they are just for convenience)\n\nPOST / HTTP/1.1\nContent-Length: X<CR><LF>\nMore-Headers: yyyy<CR><LF>\n<CR><LF>\n<X bytes of data>\nGET / HTTP/1.1\n\netc...\nNOTE: The GET is the second request.\n\nThe following is non-compliant with every current proposal (Please fix this\nfor HTTP 1.1 implementations!)\n\nPOST / HTTP/1.1\nContent-Length: X<CR><LF>\nMore-Headers: yyyy<CR><LF>\n<CR><LF>\n<X bytes of data>\n<CR><LF>\nGET / HTTP/1.1\n\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "First, the problem of 'old proxy' systems isn't a particularly serious\ndesign constraint; it's reasonable that if anyone gets a new client,\nyou tell them that it doesn't work with old proxies. After all, the\nclient->proxy protocol isn't the same as the client->server protocol,\nsince client->proxy sends the entire URL etc.\n\nI keep on thinking that if we're going to some kind of session-based\nscheme, we should also look for changes that would allow other\nfeatures that are only possible when you have sessions, e.g., the\nability of a server to spontaneously update the page the client is\nlooking at. This is a requirement for a lot of collaborative activity\nservices where you can (in real time) interact with the remote\nservice.\n\n\n\n"
        },
        {
            "subject": "PUT, DELETE vs. POS",
            "content": "I'm told that Vermeer's FrontPage uses POST instead of PUT, DELETE,\netc. in order to modify the content of the backing data storage. Using\nPOST allows the transaction to include more information, e.g., access\ncontrol information for the new information, versioning relationships,\nmultiple alternatives for variant resources, etc.\n\n\"PUT and Derived-From with magic Content-Version\" and \"PUT and\nIf-Valid with magic null validator\" are less magic than \"POST with\nmagic posted data\"?\n\nDoes \"POST\" allow a \"Modifies: URI\" response header?\n\n\n\n"
        },
        {
            "subject": "proxies rewriting URL",
            "content": "Apologies if this is clear in the text, but it didn't seem to be when\nI scanned 1.1. Some older proxies seemed to be modifying URLs, e.g.,\n\nif you  \n\n    GET http://foo.com/test#frob HTTP/1.0\n\nthey might ask foo.com for\n\n    GET /test%23frob HTTP/1.0\n\nor vice versa. Is there any reason to disallow this, and if so, what\nlanguage would be put in the spec to disallow it; alternatively, if\nproxies might do this kind of transformation, what should we say?\n\n\n\n\n\n\n\n"
        },
        {
            "subject": "Allowing servers to contact proxie",
            "content": "Dear All,\n        following an exchange on the list of the Spinner web server, it was\npointed out that your working group is already busily formulating an\nimproved system for caching. Having now read draft-mogul-http-caching-00.txt\nI would like to make 2 suggestions as to how the caching system could better\nhandle the problems of running a web server.\n\n1) It has been noted that Webmasters would like to receive data from the\nproxy on how the cache has been accessed, and that there are various ideas\non what information is appropriate. I think the most important information,\naffter the number of hits, is the type of browser which has been used. This\ndefines the version of HTML which can be used, whether java can be added etc.\n\n2) Mostly it is not possible to know in advance when the file will next be\nmodified. The \"customer\" may bring a new version the next day, or never.\nTherefore it may be sensible for the server to log proxies and send a\nmessage to invalidate the cached version when a file is modified.\n\nYours\nIan\n*************************************************\n* Ian Carr-de Avelon     avelon@phys.uva.nl     *\n*        http://www.lmg.com/krzyzowa/avelon.htm *\n*                                               *\n* dept. of Physics Education                    *\n* University of Amsterdam                       *\n* Netherlands                                   *\n*************************************************\n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "Ari Luotonen:\n>Neither one of the proxies I've written changes the Date: header, and\n>I'm not aware of any proxy that does it.  I would say it's safe to\n>trust the Date: header not to be mangled by intermediaries.\n\nI believe that caching proxies are supposed to rewrite the Date:\nheader in a response that is refreshed with a `not modified' response\nto a conditional GET.  See the last line of the 304 definition in the\n1.0 draft spec.\n\nI don't know if this is actually done, though.\n\nBut if the date header is part of the data digested for the\nmessage-digest, this would certainly give problems under 1.1.\n\n>Ari Luotonenari@netscape.com\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Confirm",
            "content": "        We want to know if server HTTPD CERN run in plataform windows95.And\nif it=B4s true, it is compatible with Cold Fusion.\n        Thanks for your attention.\n\n        Daniel, Sonia e Teresa\n\n\n----------------------/\\/\\-----------------------\nGrupo Estagio - Portucalense Online\nE-Mail: i1834@uportu.uportu.pt\nUniversidade Portucalense - Infante D. Henrique\nDepartamento de Informatica\nAv. Bernardino de Almeida, 541-619\nPorto   =20\nPORTUGAL                 =20\n----------------------/\\/\\-----------------------\n\n\n\n"
        },
        {
            "subject": "Re: Allowing servers to contact proxie",
            "content": "There are several proposed drafts for dealing with demographic\nmaterial recently submitted by Phill Hallam-Baker. I don't think we're\ngoing to add these features into \"1.1\" but we will likely address them\nin future versions of HTTP. Perhaps you might view his internet drafts\nand send him comments directly?\n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "There is no such implication of the Date being changed by a proxy in \nthe 1.1 section on 304 -- it does say the cache should update its \nstored copy of Date from what it gets from the origin server in the 304 \nresponse...\n----------\n] From: http-wg-request@cuckoo.hpl.hp.com\n] To:  <\"luotonen@netscape.com\">;  <luotonen@netscape.com>\n] Cc:  <\"mogul@pa.dec.com\">;  <mogul@pa.dec.com>;  <\"frystyk@w3.org\">;  \n<frystyk@w3.org>\n] ;  <\"http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\">;\n] <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: Re: Can proxies rewrite Date:?\n] Date: Thursday, March 07, 1996 6:47AM\n]\n] Ari Luotonen:\n] >Neither one of the proxies I've written changes the Date: header, and\n] >I'm not aware of any proxy that does it.  I would say it's safe to\n] >trust the Date: header not to be mangled by intermediaries.\n]\n] I believe that caching proxies are supposed to rewrite the Date:\n] header in a response that is refreshed with a `not modified' response\n] to a conditional GET.  See the last line of the 304 definition in the\n] 1.0 draft spec.\n]\n] I don't know if this is actually done, though.\n]\n] But if the date header is part of the data digested for the\n] message-digest, this would certainly give problems under 1.1.\n]\n] >Ari Luotonenari@netscape.com\n]\n] Koen.\n]\n]\n] \n\n\n\n"
        },
        {
            "subject": "RE: proxies rewriting URL",
            "content": "Any digest that included the URI would be wrong if the URI is munged by \nthe proxy. It would break Digest Authentication, for example.\n----------\n] From: Larry Masinter  <masinter@parc.xerox.com>\n] To:  <\"http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\">;\n] <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: proxies rewriting URLs\n] Date: Wednesday, March 06, 1996 11:52PM\n]\n] Apologies if this is clear in the text, but it didn't seem to be when\n] I scanned 1.1. Some older proxies seemed to be modifying URLs, e.g.,\n]\n] if you\n]\n]     GET http://foo.com/test#frob HTTP/1.0\n]\n] they might ask foo.com for\n]\n]     GET /test%23frob HTTP/1.0\n]\n] or vice versa. Is there any reason to disallow this, and if so, what\n] language would be put in the spec to disallow it; alternatively, if\n] proxies might do this kind of transformation, what should we say?\n]\n]\n]\n] \n]\n]\n] \n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "    I believe that caching proxies are supposed to rewrite the Date:\n    header in a response that is refreshed with a `not modified' response\n    to a conditional GET.  See the last line of the 304 definition in the\n    1.0 draft spec.\n\nI think this interpretation reflects a confusion about what caches\ndo.  People who are on the http-caching mailing list may recall a\ndiscussion of whether we should model caches as storing \"values\"\n(of entities + assocatied entity headers) or \"responses\".\n\nIn this case, it doesn't really matter which model you follow,\nas long as you keep in consistently in mind when deciding how\nto interpret what is going on.\n\nConsider a concrete example:\n\nclient A does\nGET /home.html HTTP/1.1\n\ncache forwards this to server\n\nserver replies with response R1, which includes\nDate: Thu, 15 Feb 1996 15:05:20 GMT\nwhich gets put into the cache, and returned to the client\n\nlater on, client B does\nGET /home.html HTTP/1.1\n\nthe cache forwards this to the server as\nGET /home.html HTTP/1.1\nIf-Modified-Since: .....\n\nand the server responds with response R2\nHTTP/1.1 304 Not Modified\nDate: Fri, 16 Feb 1996 12:00:20 GMT\nwhich therefore does not include an entity body.\n\nOK, what response does the cache return to client B?  It probably\ndoes not return R1 (it has the old Date:, and perhaps a very\nold Expires: field).  It can't just return R2 (since client B\ndid an unconditional GET, it needs the body).\n\nIn the \"caches hold responses\" model, therefore, the cache\nneeds to create a new response R3 that is some combination\nof R1 and R2.  Presumably, it does this by taking the union\nof the two responses, and using R2's values for any field that\nis present in both R1 and R2.\n\nIn the \"caches hold values\" model, the cache constructs a new\n\"value\" out of the pieces provided by R1 and R2 (using essentially\nthe same algorithm), and then returns this new value as R3\nto client B.\n\nEither way, the cache needs to perform a function\nR3 = F(R1, R2)\nor\nnewvalue = G(R2, oldvalue)\nthat ensures that a consistent set of entity headers remains attached\nto the entity body.\n\nNote that the cache is NOT \"rewriting\" the Date: header in either\ncase; it's simply shuffling a set of headers and bodies that it has\nreceived at various times.\n\nSo if Date: is included in the message-digest, then either function\n(F or G, depending on your model) has to make sure that these two\nheaders are consistent: either they both come from the new response,\nor they both come from the old response.  But I don't think this is\nat all difficult to get right, if the origin server doesn't do\nsomething downright foolish (such as sending a new Date: value with\nR2, but not a new message-digest).\n\nAll the protocol has to say (with respect to cache behavior) is\n\"when combining headers from an old response and a refreshed\n304 response, always use the more recent value for any fields present\nin both responses.\"\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "According to Rob McCool:\n> \n> ... why not modify either Connection or Allow:MGET so\n> they are ignored when a client is talking to a proxy? That is, if I'm\n> a Connection or Allow:MGET aware client, I ignore these headers unless\n> I also get Proxy-connection or Proxy-allow:MGET?\n> \n\nOf course you are right.  This is a much better solution.\n\n> \n>  * Any proposal\n>  * that can't at least match the user's perceived performance which\n>  * Netscape obtains with multiple connections is not viable. It's a\n>  * competitive world out there and browser writers will have to go\n>  * with multiple connections if nothing else matches their\n>  * performance.  Any proposed standard, MGET or stay-open, which can't\n>  * measure up to multiple connection performance just won't be\n>  * implemented.\n>  */\n> \n> Yes, and to that end we have to keep in mind that while <img width=N\n> height=M> is useful, it means that in order to see that performance\n> win, everyone has to edit their HTML. That will take time, and\n> Netscape's perceived performance works everywhere without having to\n> edit (or parse) every single HTML doc on a server...\n> \n\nThis is a very important point and one that I hadn't fully considered.\nSince the WN server keeps information about files in a data base and\nparses the header of an HTML doc when it is entered in the data base\n(to extract title and <meta> information like Keywords and Expires) I\nhad in mind parsing the whole doc at that time for information about\nimages.  But, but I am not sure how NCSA and CERN httpd would handle\nthis.  Since the vast majority of HTML docs served on the Web are\nserved by NCSA and CERN httpds we need something that can easily be\nmade to work with them.  I don't think it is reasonable to expect\nauthors to add the height and width.  Well, it may be reasonable, but\nit is not realistic.\n\nAlso Mark VanHeyningen rightly points out that maybe headers devoted to\na single content-type (HTML) aren't really appropriate.  Maybe the\ninlined file sizes could be put in the HTML header with some \nkind of <META> tag.  I would be easy to write utilities to insert these\nheaders for new or existing HTML documents, but getting people to use\nthe utilities is another issue.\n\nI think this problem of communicatin inline image sizes before the\nimages themselves are downloaded is really central.  If an MGET or\nSESSION methnod can't deal with this issue then there isn't much point\nin implementing such a method in HTTP1.1.  Browsers won't use it but\nwill opt for multiple connections instead.\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": ">  No conversion occurs within the same major version number.\n>  In fact, that is the main distinguisher between the major\n>  and minor version changes.  Any 1.x proxy can safely\n>  forward any 1.x request or response without changing\n>  anything except the Request-Line/Status-Line (it always\n>  sends its own native version) and the Connection,\n>  Forwarded, and Host header fields.\n\nI think that *is* overspecifying future protocol versions...\n\nPEP, for example, is designed as a minor-version upgrade to 1.x -- this only  \nworks because it defines that a PEP-savvy proxy does \"extensive\" rewriting of  \nan arbitrary number of headers at each step. More to the point, PEP  \ncompatibility depends on savvy agents (say, 1.2) looking askance at 1.1 or  \nbelow responses to see if they contain PEP headers the earlier agent would  \nhave had to act on but didn't.\n\nHere, there has got to be minor-version upgrading across single-links, and I  \nthink the language above is too absolutist...\n\nRohit\n\nBegin forwarded message:\n\nTo: Koen Holtman <koen@win.tue.nl>\nCc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject: Re: HTTP version number\nIn-Reply-To: Your message of \"Sun, 03 Mar 1996 23:41:27 +0100.\"  \n<199603032241.XAA05373@wsooti04.win.tue.nl>\nDate: Mon, 04 Mar 1996 08:21:18 -0800\nFrom: \"Roy T. Fielding\" <fielding@avron.ics.uci.edu>\nSender: http-wg-request@cuckoo.hpl.hp.com\n\n> Roy T. Fielding:\n>>Koen writes:\n>>> draft-kristol-http-state-mgmt-00 is not orthogonal to the HTTP version\n>>> number: see section 10.  Servers need the version number to know\n>>> whether they should send an old Netscape format Set-Cookie header or a\n>>> new HTTP/1.1 Set-Cookie header.\n>>\n>>This won't work.  For example\n>>\n>>     1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n>>\n>>will result in a 1.1 cookie being sent to a 1.0 user agent.\n>\n> Isn't the 1.1 proxy required to convert the 1.1 response from the\n> server to a 1.0 response before sending it to the user agent?  This\n> conversion would presumably also convert the 1.1 cookie in the\n> Set-Cookie header to a 1.0 cookie.  At least this is how I read\n> Section 3.1 of the draft 1.1 spec.\n\nGood question, but the answer is no (this used to be clear in the draft,\nbut I deleted the original wording because somebody said it\nwas overspecifying future versions -- *sigh*).\n\nNo conversion occurs within the same major version number.\nIn fact, that is the main distinguisher between the major and\nminor version changes.  Any 1.x proxy can safely forward any\n1.x request or response without changing anything except\nthe Request-Line/Status-Line (it always sends its own native version)\nand the Connection, Forwarded, and Host header fields.\n\nVisualizing that ....\n\n              1.0                  1.1\n    1.0 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n           <---------           <---------\n              1.1                  1.0\n\nLikewise,\n\n              1.0                  1.1\n    1.0 UA ---------> 1.1 Proxy ---------> 1.1 Origin Server\n           <---------           <---------\n              1.1                  1.1\n\n              1.1                  1.1\n    1.1 UA ---------> 1.1 Proxy ---------> 1.0 Origin Server\n           <---------           <---------\n              1.1                  1.0\n\n              1.1                  1.0\n    1.1 UA ---------> 1.0 Proxy ---------> 1.1 Origin Server\n           <---------           <---------\n              1.0                  1.1\n\n>>The only features of HTTP that can depend on a minor version number\n>>change are those that are interpreted by neighbors in the communication.\n>\n> I'm not sure what you means by this; I hope you don't mean \"when\n> passing on a 1.1 response to a 1.0 client, a proxy may convert the 1.1\n> response to a 1.0 response by rewriting the version number in the\n> response status line\".\n\nUmmm, not exactly.  All messages within the same major version\n(e.g., HTTP/1.x) must be sent with the sending application's native\nversion supported.  So, a HTTP/1.1 proxy will always forward a received\nHTTP/1.0 message with a new version of HTTP/1.1 the Status/Request-Line.\n\nI'd say more, but its time to register.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    <http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] Date: Friday, March 08, 1996 11:16AM\n]\n] All the protocol has to say (with respect to cache behavior) is\n] \"when combining headers from an old response and a refreshed\n] 304 response, always use the more recent value for any fields present\n] in both responses.\"\n\nIf digesting is going on, the requirement will likely be a little \nstronger -- the cache won't be able to combine headers from different \nresponses, so the server should make sure that it always sends a \ncomplete set, or that it sends a digest of the set that the cache will \nchoose to send, and perhaps the cache should detect if it gets \nout-of-date headers.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "FW: &quot;Value&quot; caches (was Can proxies rewrite Date:?",
            "content": "Resend -- mail problem:\n\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] Date: Friday, March 08, 1996 11:16AM\n]\n] In the \"caches hold responses\" model, therefore, the cache\n] needs to create a new response R3 that is some combination\n] of R1 and R2.  Presumably, it does this by taking the union\n] of the two responses, and using R2's values for any field that\n] is present in both R1 and R2.\n]\n] In the \"caches hold values\" model, the cache constructs a new\n] \"value\" out of the pieces provided by R1 and R2 (using essentially\n] the same algorithm), and then returns this new value as R3\n] to client B.\n]\n] Either way, the cache needs to perform a function\n] R3 = F(R1, R2)\n] or\n] newvalue = G(R2, oldvalue)\n] that ensures that a consistent set of entity headers remains attached\n] to the entity body.\n\nWarning!!! Philosophy!!!\n\nI think that what Jeff says about the \"caches hold values\" (CHV) model \ndoesn't capture the essence of that model.  What I think the model \nmeans is that the responses R aren't atomic, that we can look at them \nand see a finer structure:\nR = (C, V)\ni.e, that a response has some \"communications stuff\" (basically, \nresponse and general headers) and some \"value stuff\" (Date, entity \nheaders and entity-body). Using this elaborated CHV model, we can \ntherefor rewrite Jeff's response function as\nnewvalue = G(V, oldvalue)\nwhere G takes each of the subparts of V (individual headers or \nentity-body ranges) and replaces the corresponding subpart of oldvalue \nto create newvalue; whereas\nthe communications stuff is synthesized anew on each response.\n\nThis elaborated CHV model isn't a complete explanation.  It doesn't \naccount for Location headers, for example. In fact, it doesn't account \nfor content negotiation (CN) at all, really, because CN deals with \n\"virtual\" entities, which by definition can't be cached.  A fuller \nexplanation would say that the cache has two kinds of entries:\nmappings from virtual entities to real entities\nreal entities\nOnly the latter fit the CHV model, but they form the \"reality \ngrounding\" function on top of which one can build virtual entities.\n\nThis fits nicely in with the observation at the WG meeting about \nLocation. A particular variant ( a real entity) always has to have \neither a URI or a URI and a variant ID if it wants to be cached,\n\nIt fits nicely with Vary -- Vary specifies how to construct a mapping \nfrom certain request headers toa real entity that might be in the cache.\n\nIt also fits nicely with issues of caching POST and GETs with \"?\" in \nthe URI.  When they are cacheable,  they are a mapping  that includes \nthe query string or the POST entity-body.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "HTTP meeting summar",
            "content": "The area directors want a summary of the meeting before the week's\nend. This is what I sent them (I'd cc'd http-wg but mistyped the\nmailing list address.)\n\nHopefully we'll get the minutes out by the end of next week, including\nminutes of the Thursday afternoon session.\n\n================================================================\n\nHTTP/1.0 has been submitted to become an Informational RFC. We are\nfocusing on getting a proposed standard for a new version of HTTP with\nan aggressive schedule: submission of a Proposed Standard by May 1.\nThis schedule will mean dropping some issues and features in this\nfirst standards-track version and considering them for standardization\nin a subsequent version. Of top priority are those fixes that will\nhelp relieve HTTP-caused Internet congestion: host identification,\ncaching, persistent connections.\n\nWe had formed a number of subgroups to evaluate the HTTP/1.1 draft.\nIn our two originally scheduled meetings, we reviewed the subgroup's\nconclusions and open issues, in the areas of caching, persistent\nconnections, content negotiation, state management, range retrieval,\nauthentication, extension methods, and extension methods.  In\naddition, we also had a lively and productive interaction with the WTS\nmembers where HTTP security work is proceeding, and a third meeting on\nThursday to triage our task list and assign ownership:\n\n   http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n\nJim Gettys is now the lead editor.\n\n\n\n"
        },
        {
            "subject": "[hallam&#64;w3.org: Footers and Chunked ",
            "content": "To:Larry Masinter <masinter@parc.xerox.com>\nCc:hallam@w3.org\nSubject: Footers and Chunked \nDate:Fri, 8 Mar 1996 11:30:00 -0800\nFrom:hallam@w3.org\nX-Mts: smtp\ncontent-length: 3311\n\n\nLarry,\n\nI've been asked about whether footers should be in the HTTP/1.1 chunked \nencoding or not. I have code written which has a vital and essential need for \nfooters, specifically to add signatures to the end of documents.\n\nSince footers have been in the 1.1 spec for some time and since nobody \nhas made a case on the list for removing them I would be extreemly and deeply \nunhappy if the footers were removed. I would not be unhappy if chunked was \nremoved from 1.1 entirely and defferred to 1.2 however. \n\nThe need to be able to put footers at the end of the documents is a \nserious one, one which both Jeff Schiller and myself are very firm on - if not \nto say wedged. The biggest mistake made in PGP was a decision to put a length \nencoding at the front of the document which prevents it from being used as a \nfilter to encrypt a backup tape to a disk. The parallels with the Web are \nobvious.\n\nIdeally I would like to have footers plus a requirement that \nimplementations be tolerant of additional material following the length code of \na chunk and the CRLF. This will permit the chunked encoding to be progressed to \none which supports multiple streams or segment by segment message digests at a \nfuture date. Ie my existing code (almost) produces:\n\nHTTP/1.1 201 O.K. Here is some data\nServer: condom/1.0 Prevent virii! always take net.precautions!\nContent-Encoding: chunked\nSignature-RSA: key-id=KEY:RSA:server; place=footer; digest=RSA-MD5\n\n20\n<BODY><H1>This is a test\n63\nMessage</h1><p>Hello</p><HR><address>PHB</ADDRESS></body>\n0\nSignature-RSA: key-id=KEY:RSA:server; \nsignature=fLQk4ZyOdUbuoldrNTPX3P/Yb6PXXhS9xCnTe9xMihEdvDt66rXDpf34NAzfjayyWWfekM\n2qArK+xqcUNbxOZw==\n\n\n\nI would like to be able to produce\n\n\nHTTP/1.1 201 O.K. Here is some data\nServer: condom/1.0 Prevent virii! always take net.precautions!\nContent-Encoding: chunked\nSignature-RSA: key-id=KEY:RSA:server; place=footer; digest=RSA-MD5\nAuthentication: key-id=KEY:SYMETRIC:fred; algorithm=RSA-KD5; \nmask=1A237E28F28123B021\n\n20 auth=2qArK+xqcUNbxOZw==\n<BODY><H1>This is a test\n63 auth=63P/Yb6PXXhS9xCn==\nMessage</h1><p>Hello</p><HR><address>PHB</ADDRESS></body>\n0 auth=o7ldrNTPX3P/Yb6P==\nSignature-RSA: key-id=KEY:RSA:server; \nsignature=fLQk4ZyOdUbuoldrNTPX3P/Yb6PXXhS9xCnTe9xMihEdvDt66rXDpf34NAzfjayyWWfekM\n2qArK+xqcUNbxOZw==\n\n\nClearly I would be happier if this would not break proxies which are based on \nthe 1.1 spec. I see no possibility whatsoever of providing the same \nfunctionality if signatures are required to go at the beginning. The content \nproduced is likely to be generated by an automaton such as a gateway interface. \n\nI have recently been expreimenting with a number of highly interactive systems\nwhere there is a clear need for continuous authentication at the segment level. \n\n\nUnless someone can come up with a clear reason why footers are bad I think that\nwe should continue with the status quo rather than make a change at this stage \nwhich would inevitably lead to delay in reaching consensus. \n\nI would like us to be able to finish 1.1 as soon as possible in order that we \ncan start on some new topics. In addition to the demographics issues I raised \nthere is a long standing problem of notification which I beleive can be solved \nwith a few minor but significant additions to the spec..\n\n\nPhill\n\n\n\n\n\n"
        },
        {
            "subject": "Re: Footers and Chunke",
            "content": "The next HTTP train to leave the station is not the last train.\nHowever, it became clear that there is increasing pressure from IETF\nthat we produce a revised version of HTTP in the next few months that\nreduces pressure on Internet backbone and service providers by\nsupporting persistent connections, caching, and host identification,\nand that everything else is lower priority.\n\nOn the other hand, we're not going to scuttle everything else\nintentionally, it's just a matter of getting all the issues worked\nout.\n\nI'm not sure what prompted you to generate such an impassioned plea:\nthis is a short-term effort to get something short-term out; we're\ngoing to immediately begin on the next version (you know, 1.2) as soon\nas we finish this one.\n\n\n\n"
        },
        {
            "subject": "Re: Footers and Chunke",
            "content": "The point is not whether the chunked encoding is in or out of HTTP/1.1\nIn fact I would if anything prefer it to be out if anything. What I was\nreacting to was a suggestion that the footers be dropped from chunked \nand put in HTTP/1.1. This is a change I can see absolutely no technical\njustification for whatsoever. I also fail to see any political advantage\nwhich would mean getting the spec to proposed standard earlier.\n\nThinking that reports of such a change might have been subject to a certain\ndegree of confusion I thought it best to send a private message to the\nchair rather than exciting the whole list over what could well have been\na simple misunderstanding or confusion.\n\nIf people were indeed worried by any additional complexity involved in the\nhandling of footers they need only construct a very simple FSM to strip\nout the additional items.\n\nWhat I am concerned about that in a rush to get 1.1 out the door decisions\nwill be made which will cause us compatibility problems in 1.2. It would\nbe something of an embarassment to have to introduce a second encoding for\nno other reason than the first one had been fumbled. \n\nIn summary I suggest that we ensure that wherever possible the 1.1. spec\nrequires implementations to be tolerant in accepting and call to attention\nthe areas where extension is likely.\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: Can proxies rewrite Date:",
            "content": "Jeffrey Mogul:\n>    [Koen Holtman:]\n>    I believe that caching proxies are supposed to rewrite the Date:\n>    header in a response that is refreshed with a `not modified' response\n>    to a conditional GET.  See the last line of the 304 definition in the\n>    1.0 draft spec.\n>\n>I think this interpretation reflects a confusion about what caches\n>do.\n\nI don't think I'm confused about what caches do: the concrete example\nyou give is exactly the example I would give.\n\n>  People who are on the http-caching mailing list may recall a\n>discussion of whether we should model caches as storing \"values\"\n>(of entities + assocatied entity headers) or \"responses\".\n\nI remember the discussion, and I don't think that now is the time to\nrestart it.\n\n>In this case, it doesn't really matter which model you follow, \n[...]\n\nYes, it does not really matter.  We seem to be agreeing on what\nhappens in caches, though we all have different explanations (models)\nof how it happens.\n\nWhat we need now is not some general model, but concrete text.\n\nWe need text, that exactly defines how caches must behave, for the new\n1.1 draft which should be ready on April 1st.  Whether this text talks\nabout `responses' or `values' or `entities' as being stored by caches,\nis an editorial decision that can be made while writing the text.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Footers and Chunke",
            "content": "I'm sorry, I was just completely confused by your message. It was rude\nof me to forward it to the list, though, and I apologize.\n\nI don't think our procedure is 'make random changes to get things out\nthe door'. I think our procedure is 'make things solid if we can; if\nthey're not solid, leave them out of 1.1 and revert to 1.0'. So,\nchunked isn't in 1.0. If we put it in 1.1, it will be with agreed-upon\nspecification.\n\nChunked is handy for persistent connections, because it lets you deal\nwith content where you don't know the length.  If we really have\ntrouble specifying chunked agreeably, we could take it out, and leave\npersistent connections restricted to those cases where either the\nclient accepts multipart and the server wants to send it, or else the\nlength is known.\n\n\n\n"
        },
        {
            "subject": "RE: proxies rewriting URL",
            "content": "On Fri, 8 Mar 1996, Paul Leach wrote:\n\n> Any digest that included the URI would be wrong if the URI is munged by \n> the proxy. It would break Digest Authentication, for example.\n> ----------\n> ] From: Larry Masinter  <masinter@parc.xerox.com>\n> ] To:  <\"http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\">;\n> ] <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n> ] Subject: proxies rewriting URLs\n> ] Date: Wednesday, March 06, 1996 11:52PM\n> ]\n> ] Apologies if this is clear in the text, but it didn't seem to be when\n> ] I scanned 1.1. Some older proxies seemed to be modifying URLs, e.g.,\n> ]\n> ] if you\n> ]\n> ]     GET http://foo.com/test#frob HTTP/1.0\n> ]\n> ] they might ask foo.com for\n> ]\n> ]     GET /test%23frob HTTP/1.0\n> ]\n> ] or vice versa. Is there any reason to disallow this, and if so, what\n> ] language would be put in the spec to disallow it; alternatively, if\n> ] proxies might do this kind of transformation, what should we say?\n> ]\n\nActually, a proxy munging the URL will cause no problem for digest\nauthentication.  The URL is duplicated in the uri field of the \nauthentication header to deal with exactly this issue.  Of course,\nif a proxy munges the Authorization: header then there will be \nproblems.\n\nPreventing proxies from munging headers is still a good idea though.\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "> Apologies if this is clear in the text, but it didn't seem to be when\n> I scanned 1.1. Some older proxies seemed to be modifying URLs, e.g.,\n> \n> if you  \n> \n>     GET http://foo.com/test#frob HTTP/1.0\n> \n> they might ask foo.com for\n> \n>     GET /test%23frob HTTP/1.0\n> \n> or vice versa. Is there any reason to disallow this, and if so, what\n> language would be put in the spec to disallow it; alternatively, if\n> proxies might do this kind of transformation, what should we say?\n\nWell, it's really a question of what should a proxy do when it\nreceives a syntactically invalid Request-URI.  My preference is that\nit make no changes whatsoever to the path part of the Request-URI\nwhen it forwards the request.  However, this would mean the proxy\nwould be sending a non-compliant request, which is also bad.\n\nI guess that the best thing to include is a paragraph saying\n\n   A proxy may encode portions of the Request-URI prior to forwarding\n   the message if and only if such encoding is necessary to make the\n   message compliant with HTTP.  Where possible, proxies should attempt\n   to use the same characters in the forwarded Request-URI as were\n   received from the client, since some servers improperly apply\n   special semantics to characters outside the \"reserved\" set.\n\nNote, however, that \"#\" is never allowed in a Request-URI.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "open/request/close in one packet",
            "content": "What with all this talk about round-trip time, folks might want to\nlook at RFC1644: T/TCP -- TCP Extensions for Transactions Functional\nSpecification.\n\nAlso, even without TCP Extensions, I'm told that it seems to be\npossible to open a TCP connection, send some data (e.g., a HTTP\nrequest) and request the close of the connection all in one packet.\nOf course, few network implementations support this from the sender\nsize.\n\n================================================================\nrom the intro of RFC1644:\n\n   TCP was designed to around the virtual circuit model, to support\n   streaming of data.  Another common mode of communication is a\n   client-server interaction, a request message followed by a response\n   message.  The request/response paradigm is used by application-layer\n   protocols that implement transaction processing or remote procedure\n   calls, as well as by a number of network control and management\n   protocols (e.g., DNS and SNMP).  Currently, many Internet user\n   programs that need request/response communication use UDP, and when\n   they require transport protocol functions such as reliable delivery\n   they must effectively build their own private transport protocol at\n   the application layer.\n\n   Request/response, or \"transaction-oriented\", communication has the\n   following features:\n\n   (a)  The fundamental interaction is a request followed by a response.\n\n   (b)  An explicit open or close phase may impose excessive overhead.\n\n   (c)  At-most-once semantics is required; that is, a transaction must\n        not be \"replayed\" as the result of a duplicate request packet.\n\n   (d)  The minimum transaction latency for a client should be RTT +\n        SPT, where RTT is the round-trip time and SPT is the server\n        processing time.\n\n   (e)  In favorable circumstances, a reliable request/response\n        handshake should be achievable with exactly one packet in each\n        direction.\n\n   This memo concerns T/TCP, an backwards-compatible extension of TCP to\n   provide efficient transaction-oriented service in addition to\n   virtual-circuit service.  T/TCP provides all the features listed\n   above, except for (e); the minimum exchange for T/TCP is three\n   segments.\n\n\n\n"
        },
        {
            "subject": "Re: Footers and Chunke",
            "content": "Well the situation from my perspective is that I am quite happy\nwith the 1.1 spec for chunked encoding, I think that as specified \nit is OK. There is the one piece of wording which I would like to have\nchanged  which would have no impact on 1.1 generators but would allow \nlater specs to possibly extend the chunked encoding. That is not so\nimportant however.\n\nWhat I was reacting to was a number of reports that the meeting at\nthe IETF had suddenly decided on a 90 degree turn, to change the spec\nfor the worse but without anyone being able to give a reason for\nthe change.\n\n\nI don't think that there is actually any real disagreement, just some people\nnervous that there might be. Chunked encoding has been suggested for a very\nlong time. I remember us discussing it back in '93. I really don't think that \nwe need to kick it arround forever. Once one has decided to render a stream\nas a sequence of chunks there are only three places where one can attach attributes, at the beginning, at the end and on each chunk. I se a need for the first and the second of these today, they allow digital signatures to be attached to d\ndynamically created content. There is a posibility we might need the third at \na later date so I would like us to hold open the door for that but not make\na commitment at this time. \n\n\nPhill \n\n\n\n"
        },
        {
            "subject": "Re: Cache validator",
            "content": ">>    However, I am willing to give-in to that notion IF the opaque\n>>    validator is sufficiently useful to cover the cost of sending it.\n>>    That is, the opaque validator must be generally interoperable with\n>>    existing systems and carry sufficient semantics for use for things\n>>    other than cache updates.\n> \n> First of all, the generally understood meaning of the word \"opaque\"\n> is \"has no meaning to clients\", and therefore if you want the\n> validator to carry other semantics, you're not talking about an\n> \"opaque\" validator.\n\nActually, it means \"can't see through it\" -- it does not mean that\nthe string cannot hold a given set of semantics.  A validator is\nworthless if it doesn't require some mechanism for comparing its\nvalue independent of the source (e.g., byte-equality), since the\nsource cannot be contacted for all comparisons.\n\n>>    In order to provide that additional usefulness, we need three things:\n>>    \n>>       1) A guarantee that the validator will change if the content changes\n>>  and should not change if the content remains the same;\n>>       2) A guarantee that the validator is byte-comparable (i.e., equal\n>>  validators mean equal content);\n>>       3) A guarantee that the validator is world-unique.\n>>    \n>>    (1) is obvious.\n> \n> Not necessarily.  To be useful as a cache validator (unburdened\n> by any other semantics), it is sufficient that the value changes\n> if the content is semantically different.  It is not necessary\n> that the value change on every insignificant change in the content\n> (where \"significant\" is defined by the application that generates\n> the content).\n\nI said \"additional usefulness\".  The particular additional usefulness\nI have in mind is for a basic indicator of change which would be usable\nfor preconditions [i.e., the most often used precondition is\n\"don't do this if a change has already been made that I don't know about\"].\nDual application reduces the cost of implementing both, and I personally\nneed this functionality more than I need transparent caching.\n\nSince it is unlikely that anyone will ever implement a system which\nonly changes the validator for \"significant\" changes, I think it\nwould be silly to lose the additional functionality gained from a\nguaranteed change indicator.\n\nNote also that if we have (1) and (2), we also have a clear algorithm for\nconstructing a unique ID identical to Content-ID, which would be\nuseful for gateways and cache tables even if we don't use the Content-ID\nsyntax of MIME [because we get (3) by combining the validator with the\nRequest-URI].\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "> General comments on section 3.1:\n> \n> 1) Is there any rationale for the rules in Section 3.1?  \n\nYes.  The rules exist to assist deployment of multiple protocol revisions\nand to prevent the HTTP protocol designers from forgetting that deployment\nof the protocol is an important aspect of its design.\n\nThey do so by making it easy to differentiate between compatible changes\nto the protocol and incompatible changes.  Compatible changes are easy\nto deploy and communication of the differences can be achieved within\nthe protocol stream.  Incompatible changes are difficult to deploy because\nthey require some determination of acceptance of the protocol before\nthe protocol stream can commence.\n\nThe Upgrade header field reduces the difficulty of deploying incompatible\nchanges by allowing the client to advertize its willingness for a better\nprotocol while communicating in an older protocol stream.  However, we\nneed a 1.1 standard in order to use Upgrade.\n\n> 2) I'm not sure that I really like the weak requirements for\n> translation by proxies that are in the draft spec now: these\n> requirements will basically shift the burden of making things\n> interoperable from proxy authors to httpd and CGI authors, and there\n> are a lot more CGI authors than proxy authors.\n\nCGI authors will have to do better than they do now in order to be\nHTTP compliant.  That would be true for HTTP/1.0 as well, since many\nCGI scripts are non-compliant with anything HTTP.  How the server is\nimplemented is not our concern -- what we care about is the interface\non the HTTP side, and that needs to be HTTP compliant regardless of\nwho or what is responsible for generating the message.\n\n> 3) With proxies being allowed to upgrade and downgrade the minor\n> version number, it seems that the server, it it gets a 1.1 request,\n> will not be able to find out if there are any 1.0 applications in the\n> request chain.\n\nYes.  We have talked about adding that information to Forwarded, but\nwe also need to come up with a more compact encoding for that header.\nIn any case, the recipient doesn't need to know about 1.0 applications\nin the request chain if there are no incompatible changes in the\nrequest chain applying to more than just the immediate connection.\n\n> 4) So the server has to make things interoperable, but it does not\n> even know the capabilities (versions) of the software it is serving\n> to!  So, for example, if the server uses a 1.1 Cache-Control header,\n> it must always include a 1.0 Expires header.\n\nYes.  There is no way to avoid it without requiring a major protocol\nchange, and thus a major version number change.\n\n> 5) Wouldn't things be easier if we did a major version number change\n> and defined HTTP/2.0 instead of HTTP/1.1?\n\nNo, that only shifts the burden from the protocol designers to the\nimplementors and deployment.  We should not make a major protocol\nchange when we can solve 95% of our problems with a minor change,\nparticularly since that minor change includes the primary deployment\nmechanism for future major changes.\n\nWe can, quite easily, change what we are calling HTTP/1.2 + PEP + \"cookies\ndone right\" into the label \"HTTP/2.0\".  However, we need the most important\nissues in front of us (Host, Caching, Persistent Connections) to be easily\ndeployed.  That means we must have an HTTP/1.1 proposed standard before\nmaking any incompatible changes.\n\n> 6) In summary, I suspect that the version rules will need to be\n> seriously reviewed.\n\nI would hope that the entire document gets \"seriously reviewed\".\nHowever, changing the version rules would be a mistake, since it would\nonly make it easier on the protocol designers and make it harder to\ndeploy implementations.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "Given that a Variant ID also needs a Validator in order to be useful,\nwouldn't it make more sense just to require that the validator contain\nthe equivalent of a Variant ID when variants are present?\n\nCombined with my other suggestion (to make validators a change indicator),\nwe could considerably reduce the number of additional header fields\nsent on these requests and responses.\n\nIn other words, this is the same as my IF-ID/Unless-ID/Content-ID\nproposal but without the \"world uniqueness\" and usage of the <mailbox>\nformat.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "UCI archives will be down sometime Tuesda",
            "content": "I and all of my research group's machines will be physically moved\nto a new building on Tuesday, March 13.  Hopefully, this will only\nbe a temporary disruption and access will be restored within a\nfew hours of the move (exact times are not available).\n\nSo, don't panic, but do plan ahead.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "Roy T. Fielding writes:\n...\n > \n > I guess that the best thing to include is a paragraph saying\n > \n >    A proxy may encode portions of the Request-URI prior to forwarding\n              ^^^\n >    the message if and only if such encoding is necessary to make the\n                  ^^^^^^^^^^^^^^\n >    message compliant with HTTP.\n\nThis language doesn't really go together.  \"If and only if\" implies\nnecessity, at least to any mathematically oriented readers,\nso it ought to be \"must\", not \"may\", or else you should\nget rid of the \"if\" part.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": ">Well, it's really a question of what should a proxy do when it\n>receives a syntactically invalid Request-URI.  My preference is that\n>it make no changes whatsoever to the path part of the Request-URI\n>when it forwards the request.  However, this would mean the proxy\n>would be sending a non-compliant request, which is also bad.\n\nI think that it is a question of what people can encode into URIs. \nURIs do not specify a canonical form so that there can be several\nURIs which logically mean the same thing.\n\nIf we are clear on the \"meaning\" part then two syntactic variants\nof the same URI should be interchangeable. If something breaks\nbecause of this problem then it is something which relied upon a\nsyntactic variation and was therefore broken.\n\nFor example I might embed <a href=\"http://foo.bar$\"> into a document\nwhere $ is some ambiguous character. I might know that the xyz browser \nwill canonicalize differently if the URI comes from the browser or from\nthe bookmark file and have one of those $5million venture capital \n\"companies\" wrapped arround this hack (don't laugh I've seen this type of\nthing more than once).\n\nPersonally I think that trying to provide support for such constructs\nis ill advised. It is likely to close up options down the line. In\nparticular I'm thinking of HTTP-NG here. I can think of many reasons why\na proxy might choose to canonicalize URIs internally, cache matches\nfor one. If one considers the action of a caching proxy I think that\ncanonicalization of URIs in passed on requests is likely to be highly\ndesirable.\n\nSince Larry reports that there are already proxies doing this sort of \ntransformation I think it best to leave things as they are but include\na warning to state that problems might occur.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": ">I think that it is a question of what people can encode into URIs. \n>URIs do not specify a canonical form so that there can be several\n>URIs which logically mean the same thing.\n>\n>If we are clear on the \"meaning\" part then two syntactic variants\n>of the same URI should be interchangeable. If something breaks\n>because of this problem then it is something which relied upon a\n>syntactic variation and was therefore broken.\n>...\n>I can think of many reasons why\n>a proxy might choose to canonicalize URIs internally, cache matches\n>for one. If one considers the action of a caching proxy I think that\n>canonicalization of URIs in passed on requests is likely to be highly\n>desirable.\n>\n>Since Larry reports that there are already proxies doing this sort of \n>transformation I think it best to leave things as they are but include\n>a warning to state that problems might occur.\n\nAbsolutely!\n\nA proxy that did not return the cached document http://foo/%7Euser\nin response to a requset for http://foo/~user is not behaving efficiently.\nSo proxies need to canonicalise the URL for the cache key.\nHence it would be confusing if the proxy did not use this canonicalised URL\nin requests it issued.\n\nThis is why the proxy in Apache 1.1 beta does a lot of URL rewriting;\nnot only %xx <-> char as appropiate, but also\nhttp://foo  -> http://foo/\nand (perhaps dubiously) http://foo/bar? -> http://foo/bar\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "RE: proxies rewriting URL",
            "content": "----------\n] From: John Franks  <john@math.nwu.edu>\n] Date: Sunday, March 10, 1996 4:41PM\n]\n] On Fri, 8 Mar 1996, Paul Leach wrote:\n]\n] > Any digest that included the URI would be wrong if the URI is munged by\n] > the proxy. It would break Digest Authentication, for example.\n]\n] Actually, a proxy munging the URL will cause no problem for digest\n] authentication.  The URL is duplicated in the uri field of the\n] authentication header to deal with exactly this issue.  Of course,\n] if a proxy munges the Authorization: header then there will be\n] problems.\n\nInteresting.  What happens if I  do this:\nGET /secret.txt HTTP/1.1\nAuthorization:  uri=\"/public.txt\",\n username=\"fred\", realm=\"www.foo.com\",\n nonce=\"deadbeef\", response=\"0123456789abcdef0123456789abcdef\"\n\nIf the server checks the authorization header and its URI, but then \nuses the URI from the Request-URI in the request line, the whole \nexercise will have been wasted.\nAnd if proxies are allowed to munge the URI in unknown ways, the server \ncan't compare the request-URI with the uri in the Authorization header.\n\nThe Digest draft should say the the server MUST use the URI from the \nAuthorization header, as that is the only one that has been authenticated.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "----------\n] From: \"Roy T. Fielding\"  <fielding@avron.ICS.UCI.EDU>\n] Date: Sunday, March 10, 1996 8:51PM\n]\n] Given that a Variant ID also needs a Validator in order to be useful,\n] wouldn't it make more sense just to require that the validator contain\n] the equivalent of a Variant ID when variants are present?\n]\n] Combined with my other suggestion (to make validators a change indicator),\n] we could considerably reduce the number of additional header fields\n] sent on these requests and responses.\n]\n] In other words, this is the same as my IF-ID/Unless-ID/Content-ID\n] proposal but without the \"world uniqueness\" and usage of the <mailbox>\n] format.\n\nGood point. The one thing that I think still differs is that lost is \nthe ability for the cache to know that several things with the same URI \nare variants of one another, as opposed to just a single instance that \nhas changed over time. With IF-ID/Unless-ID/Content-ID:\nClientServer\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nContent-ID: \"asdasfa\"\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nContent-ID: \"xxxxx\"\n\nHow does the cache know if the two \"foo.txt\" are variants of one \nanother, or if the one-and-only \"foo.txt\" has changed? With Variant-ID, \nit would be clear:\nClientServer\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nCache-Validator: \"asdasfa\"\nVariant-ID: 1\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nCache-Validator: \"xxxx\"\nVariant-ID: 2\n\nI think that you're right that we can condense the number of headers. \nOne suggestion: add an \"id=\" parameter to Content-ID, and allow \nmultiple validator/id pairs on IF-ID and Unless-ID. Then the above would be:\nClientServer\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nContent-ID:\"asdasfa\";id=1\nGET /foo.txt HTTP/1.1\nHTTP/1.0 200 OK\nContent-ID:\"xxxxx\";id=2\nGET /foo.txt HTTP/1.1\nIF-ID: \"asdasfa\";id=1,\n \"xxxxx\";id=2\n\nI thought that Content-ID was aligned with some other MIME standard \nheader, though, so this syntax may not be acceptable for that reason -- \nbut any other spelling would be OK.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "FW: &quot;Value&quot; caches (was Can proxies rewrite Date:?",
            "content": "Resend -- mail problem:\n\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] Date: Friday, March 08, 1996 11:16AM\n]\n] In the \"caches hold responses\" model, therefore, the cache\n] needs to create a new response R3 that is some combination\n] of R1 and R2.  Presumably, it does this by taking the union\n] of the two responses, and using R2's values for any field that\n] is present in both R1 and R2.\n]\n] In the \"caches hold values\" model, the cache constructs a new\n] \"value\" out of the pieces provided by R1 and R2 (using essentially\n] the same algorithm), and then returns this new value as R3\n] to client B.\n]\n] Either way, the cache needs to perform a function\n] R3 = F(R1, R2)\n] or\n] newvalue = G(R2, oldvalue)\n] that ensures that a consistent set of entity headers remains attached\n] to the entity body.\n\nWarning!!! Philosophy!!!\n\nI think that what Jeff says about the \"caches hold values\" (CHV) model \ndoesn't capture the essence of that model.  What I think the model \nmeans is that the responses R aren't atomic, that we can look at them \nand see a finer structure:\nR = (C, V)\ni.e, that a response has some \"communications stuff\" (basically, \nresponse and general headers) and some \"value stuff\" (Date, entity \nheaders and entity-body). Using this elaborated CHV model, we can \ntherefor rewrite Jeff's response function as\nnewvalue = G(V, oldvalue)\nwhere G takes each of the subparts of V (individual headers or \nentity-body ranges) and replaces the corresponding subpart of oldvalue \nto create newvalue; whereas\nthe communications stuff is synthesized anew on each response.\n\nThis elaborated CHV model isn't a complete explanation.  It doesn't \naccount for Location headers, for example. In fact, it doesn't account \nfor content negotiation (CN) at all, really, because CN deals with \n\"virtual\" entities, which by definition can't be cached.  A fuller \nexplanation would say that the cache has two kinds of entries:\nmappings from virtual entities to real entities\nreal entities\nOnly the latter fit the CHV model, but they form the \"reality \ngrounding\" function on top of which one can build virtual entities.\n\nThis fits nicely in with the observation at the WG meeting about \nLocation. A particular variant ( a real entity) always has to have \neither a URI or a URI and a variant ID if it wants to be cached,\n\nIt fits nicely with Vary -- Vary specifies how to construct a mapping \nfrom certain request headers toa real entity that might be in the cache.\n\nIt also fits nicely with issues of caching POST and GETs with \"?\" in \nthe URI.  When they are cacheable,  they are a mapping  that includes \nthe query string or the POST entity-body.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "john@math.nwu.edu (John Franks) wrote:\n\n> According to Henrik Frystyk Nielsen:\n> > \n> > Using Allow has the same problem as if a Connection header is passed\n> > through a proxy. Imagine the following setup\n> > \n> > new client      ->      old proxy       ->      new server\n> > \n> > On the first request, the new server will then send back a\n> > \n> > Allow: GET, MGET, HEAD, etc.\n> > \n> > On the second request, the new client will try a MGET, but the proxy\n> > doesn't understand this and then we are back to the old problem having\n> > lost one roundtrip time. \n> \n> I don't see this as a very big problem.  We have lost one roundtrip\n> time, but only a roundtrip between client and proxy.  In normal use\n> client and proxy are very close with low latency.  I don't think this\n> is a very big price to pay.  This proposal then has the big advantage\n> that it is backwards compatible.  It isn't necessary to try to change\n> HTTP1.0 proxies to get them to strip out anything. (I am assuming that\n> current proxies will return a standard error status/message for an\n> unrecognized method.)\n\nYEP, but they will also close the connection and put the socket into\nwait state for about 240 seconds, so that it can't be reused. Then we\nhave quite the opposite result than what we wanted to begin with. I\ndon't think the question \"which idea is most backwards compatible\" is\nthe most important. Both require in practice that some kind of\nmodification is made to the current implementations in order to work\nwell. Another factor is that there often is a `human interaction'\nbetween proxy clients and proxy servers: they are under the same sys\nadm etc.\n\n> The criticism of MGET that it is not as general as keep-open is true,\n> but I think there is serious danger of performance degradation if that\n> generality is used.\n\nI don't quite follow, could you please give some examples?\n\n>More likely unless those concerns can be met keep-open\n> would not be widely implemented.  \n>\n> I would assume that an MGET addition to the protocol would be accompanied\n> by a parallel MHEAD method.  It would still not be possible to mix \n> GETs and POSTs or even do multiple POSTs. I don't see any pressing demand\n> for these, but perhaps I am just not aware of it.\n\nI can see several things that I would like to do. First, it would be\nnice to use the Web to have destributed code development. When you\ncheck in your files in a remote code management system, you can use\nPUT. Now, as you often have a lot of files, it would be great to do all\nthis using one connection. Or using many POST over one TCP instead of\nFTP. In the spec we have done a great deal of work trying to give the\nclient the same possibilities as the server and we should not limit\nthis using MGET, MHEAD, MPUT etc.\n\nIn the next six months you will see many clients being a lot more\ninteractive and I will do everthing I can to support this in the\nLibrary of Common Code.\n\n> There is an important principle to keep in mind here.  Any proposal\n> that can't at least match the user's perceived performance which\n> Netscape obtains with multiple connections is not viable. It's a\n> competitive world out there and browser writers will have to go with\n> multiple connections if nothing else matches their performance.  Any\n> proposed standard, MGET or stay-open, which can't measure up to\n> multiple connection performance just won't be implemented.\n\nAs I wrote in an earlier mail: This is ok if one does it, but if\neverybody does it then they all loose. In other words: it must be\nscalable!\n\n-- cheers --\n\nHenrik\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "paulle@microsoft.com said:\n>fielding@avron.ICS.UCI.EDU said:\n>] Given that a Variant ID also needs a Validator in order to be useful,\n>] wouldn't it make more sense just to require that the validator contain\n>] the equivalent of a Variant ID when variants are present?\n>HTTP/1.0 200 OK\n>Content-ID: \"xxxxx\"\n>How does the cache know if the two \"foo.txt\" are variants of one \n>another, or if the one-and-only \"foo.txt\" has changed?\n\nThe presence of a \"Vary:\" header, ie:\nHTTP/1.0 200 OK\nContent-ID: \"xxxxx\"\nVary: Foo-Header\n\n[I think under the David Robinson Vary: header proposal, there's a mechanism\nfor Vary: random-whenever-I-feel-like-it-not-on-any-header, which is\neffectively \"validate every time\" AKA Cache-control: max-age=0 ?]\n\nWe still have the question of what does a caching proxy do when it just\nkeeps validating and getting new Content-ID:s + Vary: every time on the same\nURL.  Does it continually append \"If-ID: id1, id2, id3, id4, id5....\" ad\ninfinitum on later responses?  The idea of Expires: foo, or Cache-control:\nmax-age=foo is to indicate when things need to be validated, not when\ncaching should flush old stuff.  Obviously, on a non-Vary:ing resource, the\nproxy would just replace a cached entity if it got a new thing, but here,\nthe flushing of an old item on the receipt of a new one is not implicit.\n\nSo is the flushing of id1 purely an implementation issue (probably) or..\n\nIn \"http://weeble.lut.ac.uk/lists/http-caching/0114.html\" Dan said:\n b) Does the proxy still store the other non-matched variants?\n c1) Do we need to give the server a way of saying \"clear all your variants\"?\n c2) Do we need to give the serve a way of saying \"clear these particular\n      variants\"?  (Presumably they'll clear eventually anyway as the proxy runs\n      out of space and/or pushes them out of their working set.)\n e) What about renegade proxies who serve variants willy-nilly without\n      contacting the server?  etc...\n\n>I think that you're right that we can condense the number of headers. \n>One suggestion: add an \"id=\" parameter to Content-ID, and allow \n>multiple validator/id pairs on IF-ID and Unless-ID. Then the above would be:\n>ClientServer\n>GET /foo.txt HTTP/1.1\n>HTTP/1.0 200 OK\n>Content-ID:\"asdasfa\";id=1\n>GET /foo.txt HTTP/1.1\n>HTTP/1.0 200 OK\n>Content-ID:\"xxxxx\";id=2\n>GET /foo.txt HTTP/1.1\n>IF-ID: \"asdasfa\";id=1,\n> \"xxxxx\";id=2\n\nI don't mind this suggestion because I think it will be clearer to protocol\nimplementors.  Someone else might complain about the non-opaqueness of it.\nI do still think you need the Vary: header however - the presence of \"id=\"\ndoesn't convey enough information.\n\n-----\nDan DuBois, Software Animal           http://www.spyglass.com/~ddubois/\n    Download a totally free copy of the Spyglass Web Server today!\n        http://www.spyglass.com/products/server_download.html\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "    A proxy that did not return the cached document http://foo/%7Euser\n    in response to a request for http://foo/~user is not behaving\n    efficiently.  So proxies need to canonicalise the URL for the cache\n    key.  Hence it would be confusing if the proxy did not use this\n    canonicalised URL in requests it issued.\n    \nI think it would be more \"confusing\" if by canonicalizing a URL\nthe proxy turned it into something that identified a different\nresource.  I.e., we should be quite cautious about pursuing \"efficiency\"\nhere.  Efficiency is great as long as it gets the right answers.\n\n    This is why the proxy in Apache 1.1 beta does a lot of URL rewriting;\n    not only %xx <-> char as appropiate, but also\n\nThis might be appropriate in some cases, but it's clearly not\nappropriate for every instance of %xx.  RFC1738 specifically\nstates, for example,\nThe character \"#\" is unsafe and should\n   always be encoded because it is used in World Wide Web and in other\n   systems to delimit a URL from a fragment/anchor identifier that might\n   follow it.  The character \"%\" is unsafe because it is used for\n   encodings of other characters.\nso a proxy that converted\nhttp://foo%23bar\nto\nhttp://foo#bar\nwould be non-compliant with RFC1738.\n\n    http://foo  -> http://foo/\n\nI'd like to see a specific citation to a standard or even an\nI-D that makes this a compliant transformation.  In\ndraft-ietf-http-v11-spec-01.txt, for example, I find this\nBNF:\n       URI            = ( absoluteURI | relativeURI ) [ \"#\" fragment ]\n       absoluteURI    = scheme \":\" *( uchar | reserved )\nwhich suggests that the URI need not end in \"/\".\n\n    and (perhaps dubiously) http://foo/bar? -> http://foo/bar\n    \nVery dubiously, especially since we also have a well-understood\nheuristic that caches do not store responses to GETs with \"?\"\nin the URL.  If you have a series of caching proxies in the\npath, and the first one does this transformation (dropping the \"?\"),\nthe second one will not realize that the GET is a potentially\nnon-cachable query.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "----------\n] From: Daniel DuBois  <ddubois@rafiki.spyglass.com>\n] Date: Monday, March 11, 1996 12:59PM\n]\n] paulle@microsoft.com said:\n] >fielding@avron.ICS.UCI.EDU said:\n] >] Given that a Variant ID also needs a Validator in order to be useful,\n] >] wouldn't it make more sense just to require that the validator contain\n] >] the equivalent of a Variant ID when variants are present?\n] >HTTP/1.0 200 OK\n] >Content-ID: \"xxxxx\"\n] >How does the cache know if the two \"foo.txt\" are variants of one\n] >another, or if the one-and-only \"foo.txt\" has changed?\n]\n] The presence of a \"Vary:\" header, ie:\n] HTTP/1.0 200 OK\n] Content-ID: \"xxxxx\"\n] Vary: Foo-Header\n\nHmmm... first I thought you were right, then I thought you were wrong, \nnow I think you're both right and maybe wrong.  You can indeed tell \nthat what you got back is a variant from the fact that there's a Vary \nheader in the response.  So you're right.  However, the Vary: header \nvalue is a _request_ header name. How do I match it up the Accept-* \nvalues in the Vary header with the Content-* entity headers in the \nresponse? I suppose I've saved the Accept-* headers in the request, so \nmaybe that is what I use to see if there is already another instance in \nthe cache.  But can't I have two different Accept-* configurations \ntranslate to the same entity?  E.g., the resource can vary on content \nlanguage, but Accept-Language: for Japanese and Chinese might both map \nto English content, whereas French and German might have variants just \nfor themselves. I think it can probably be done, but there might be \ngotchas -- having the origin-server just _tell_ the cache that it's \nvariant number 23 would be a lot easier, and it saves transmitting the \nentity-body when you've already got it cached, even though it's got \ndifferent Accept-* header values.\n\n]\n] [I think under the David Robinson Vary: header proposal, there's a mechanism\n] for Vary: random-whenever-I-feel-like-it-not-on-any-header, which is\n] effectively \"validate every time\" AKA Cache-control: max-age=0 ?]\n]\n] We still have the question of what does a caching proxy do when it just\n] keeps validating and getting new Content-ID:s + Vary: every time on the same\n] URL.  Does it continually append \"If-ID: id1, id2, id3, id4, id5....\" ad\n] infinitum on later responses?  The idea of Expires: foo, or Cache-control:\n] max-age=foo is to indicate when things need to be validated, not when\n] caching should flush old stuff.  Obviously, on a non-Vary:ing resource, the\n] proxy would just replace a cached entity if it got a new thing, but here,\n] the flushing of an old item on the receipt of a new one is not implicit.\n\nWhereas, if there is a variant ID, you know what validator to overwrite.\n\n]\n] So is the flushing of id1 purely an implementation issue (probably) or..\n]\n] In \"http://weeble.lut.ac.uk/lists/http-caching/0114.html\" Dan said:\n]  b) Does the proxy still store the other non-matched variants?\n]  c1) Do we need to give the server a way of saying \"clear all your\n] variants\"?\n]  c2) Do we need to give the serve a way of saying \"clear these particular\n]       variants\"?  (Presumably they'll clear eventually anyway as the \nproxy runs\n]       out of space and/or pushes them out of their working set.)\n]  e) What about renegade proxies who serve variants willy-nilly without\n]       contacting the server?  etc...\n]\n] >I think that you're right that we can condense the number of headers.\n] >One suggestion: add an \"id=\" parameter to Content-ID, and allow\n] >multiple validator/id pairs on IF-ID and Unless-ID. Then the above would\n] be:\n] >ClientServer\n] >GET /foo.txt HTTP/1.1\n] >HTTP/1.0 200 OK\n] >Content-ID:\"asdasfa\";id=1\n] >GET /foo.txt HTTP/1.1\n] >HTTP/1.0 200 OK\n] >Content-ID:\"xxxxx\";id=2\n] >GET /foo.txt HTTP/1.1\n] >IF-ID: \"asdasfa\";id=1,\n] > \"xxxxx\";id=2\n]\n] I don't mind this suggestion because I think it will be clearer to protocol\n] implementors.  Someone else might complain about the non-opaqueness of it.\n] I do still think you need the Vary: header however - the presence of \"id=\"\n] doesn't convey enough information.\n\nMore precisely, it doesn't convey enough info to permit the cache to \nserve up a variant without a round-trip to the origin-server to get \nback at least the the variant ID of the entity to serve (and the entity \niteself if it isn't in the cache or has changed). If the cache wants to \nbe able to cut out tthat round trip, then it needs the information in \nthe Vary: header (or Alternates: from Koen's proposal).\n\nIn this sense Vary and Content-ID are orthogonal. A really stupid cache \ncould not use Vary and just go back to the origin-server to do all \ncontent negotitation, and still get the advantage of caching for all \nentity-bodies. In some cases, like when authentication needs to be done \nfor each request, this may in fact be the best that can be done.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Cache validator",
            "content": "    >>    However, I am willing to give-in to that notion IF the opaque\n    >>    validator is sufficiently useful to cover the cost of sending it.\n    >>    That is, the opaque validator must be generally interoperable with\n    >>    existing systems and carry sufficient semantics for use for things\n    >>    other than cache updates.\n    > \n    > First of all, the generally understood meaning of the word \"opaque\"\n    > is \"has no meaning to clients\", and therefore if you want the\n    > validator to carry other semantics, you're not talking about an\n    > \"opaque\" validator.\n    \n    Actually, it means \"can't see through it\" -- it does not mean that\n    the string cannot hold a given set of semantics.\n\nIn the sense that the word \"opaque\" is usually employed when discussing\ndata types in type-safe programming languages, it means \"the client\nis not aware of the internal details of the implementation of the\ndata type.\"  In this context, it seems most natural to use \"opaque\"\nto mean \"the client is not aware of any internal structure of the\nstring of octets used as the validator.\"\n\nAnother example: the \"file handle\" data type in NFS is meant to\nbe opaque; clients are not allowed to make any assumptions about\nhow to parse it.  I'll admit to some sinning in this regard, since\nI once wrote a \"file handle parser\" for use in the \"nfswatch\" program.\nBecause of this (i.e., the utility of parsing file handles for\ndebugging NFS problems), I would have preferred a not-entirely-opaque\nfile handle format.  However, I'm not sure that there is an analogous\nsituation with validators; NFS file handles are more like URLs, and\nwe've already pretty much given up on the myth that the suffixes\nof URLs are entirely opaque.\n\n    A validator is\n    worthless if it doesn't require some mechanism for comparing its\n    value independent of the source (e.g., byte-equality), since the\n    source cannot be contacted for all comparisons.\n\nYup, byte-equality is what makes sense.  Anything more elaborate\nis a potential can o' worms.\n    \n    >>    In order to provide that additional usefulness, we need three things:\n\n    > Not necessarily.\n    \n    I said \"additional usefulness\".\n    \nSorry, I misread that.  (Reading 500 email messages in 8 hours\ndoes that to me.)\n\n    The particular additional\n    usefulness I have in mind is for a basic indicator of change which\n    would be usable for preconditions [i.e., the most often used\n    precondition is \"don't do this if a change has already been made\n    that I don't know about\"].  Dual application reduces the cost of\n    implementing both, and I personally need this functionality more\n    than I need transparent caching.\n\nCan you provide a concrete example of what this would be useful\nfor, including the requests and responses that you would employ?\n\n    Since it is unlikely that anyone will ever implement a system which\n    only changes the validator for \"significant\" changes, I think it\n    would be silly to lose the additional functionality gained from a\n    guaranteed change indicator.\n\nWell, here's one example: supposing that a busy site with\nslowly-varying content wants to maintain a hit-count image on\nits home page.  (Whether you yourself think this is something worth\ndoing, clearly a lot of people want to do it.)  It's probably not\nall that important if a user sees a hit-count that is slightly\ninaccurate, nor is it worth using up a lot of network bandwidth\nto provide accurate hit counts.\n\nSo supposing that the server constructed an opaque validator\nfor a hit-count GIF that is simply an encoding of the hit-counter\nmod 10 (or mod 100, or mod 128, or whatever).  Then a large\nfraction of the GET+If-Valid: requests on this GIF would return\n304 Not Modified, even though the count might have increased\nslightly.\n\nThis seems to be a nice alternative to giving the GIF image\na moderately long expiration time, since it may not be possible\nto know in advance just how fast the counter is increasing.  I.e.,\nusing a long expiration time bounds the untimeliness of the cached\ncopies, but fails to bound their numeric inaccuracy.\n\nI think one could apply the same technique to other gradually-changing\nresources where small differences between versions are inherently\ninsignificant, but the versions don't change smoothly with time.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "> Good point. The one thing that I think still differs is that lost is \n> the ability for the cache to know that several things with the same URI \n> are variants of one another, as opposed to just a single instance that \n> has changed over time. With IF-ID/Unless-ID/Content-ID:\n> ClientServer\n> GET /foo.txt HTTP/1.1\n> HTTP/1.0 200 OK\n> Content-ID: \"asdasfa\"\n> GET /foo.txt HTTP/1.1\n> HTTP/1.0 200 OK\n> Content-ID: \"xxxxx\"\n\n> How does the cache know if the two \"foo.txt\" are variants of one \n> another, or if the one-and-only \"foo.txt\" has changed?\n\nThe 'last-modified' of a negotiable resource should be the\nmaximum of the last-modified of all presentations. If the two\nresponses have the same last-modified, the two contents are variants\n(and need a Vary header); if the second has a different last-modified,\nthen it marks all the other variants than the one being served as\nstale.\n\n\n\n"
        },
        {
            "subject": "DRAFT Minutes, 35th IETF HTTPW",
            "content": "Many thanks to Ted Hardie for taking notes at the session. I've\nreviewed and revised his notes, and added my own recollection of what\nhappened in the Thursday afternoon session. (Credit to Ted, complaints\nto me.) If there are no additional editors or corrections, I'd like to\nsend this on as the official minutes in the next few days.\n\n\n================================================================\nMinutes of the HTTP Working Group at the 35th IETF (Los Angeles, March\n4-8, 1996).  Reported by Ted Hardie (hardie@nasa.gov), revised by\nLarry Masinter (masinter@parc.xerox.com).\n\nThe HTTP working group held two regular sessions, the first on March\n5th and the second on March 7th; in addition, an extra meeting took\nplace the afternoon of March 7th and is reported here. Larry Masinter\nchaired the sessions.\n\nThe HTTP 1.0 specification has been taken up by the IESG, and the\nproposal to move it forward as an Informational RFC is expected to be\nprocessed at their next meeting. The group applauded Roy Fielding for\nhis work in producing the specification.\n\nJim Gettys will be taking over as primary editor of the HTTP 1.1\nspecification, continuing Roy Fielding's work and coordinating with\nothers who will aid in the editing process (including Roy, Henrik\nFrystyk, and others.)  The Area Directors approved Jim's selection as\neditor; as is expected of WG editors, Jim confirmed that he would act\non behalf of the working and that he did not see his role at Digital\nor in the W3C as in conflict with his role as editor.  One of the\nfirst items Jim will do is to decide on how best to split the HTTP/1.1\nspecification into separate documents.\n\nMost of the meeting was consisted of reports from the subgroups\nconstituted at the Dallas IETF.  This marks the formal end point of\nthe subgroups.  The members of the individual subgroups may continue\nto use those mailing lists to discuss issues, but further internet\ndrafts from the subgroups are not expected.\n\nCaching:\n\nJeff Mogul presented the status report of the caching subgroup.  The\ngroup generally agreed on cache control headers for Expires, Age, and\nCache-control; some differences remain on the exact syntax of the\nCache-control headers related to object \"freshness\", but the subgroup\ndid not see these as anything more than a stylistic disagreement.  The\nsubgroup also agreed on the need for a Warning header, and proposed a\nseries in the 90 range; some disagreement on how to associate text\nwith the different warnings remains and will have to be worked out on\nthe list.  The subgroup had also reached consensus on the need to\ndistinguish between history buffers and caches; history buffers are\nmeant to replay pages which have already been seen, as they were seen,\nand they should thus not be subject to the same directives as caches.\n\nThe caching subgroup has reached general agreement on how caching\nshould interact with the Location: header and the Vary: header, but it\nhas not yet worked out the details.\n\nStill unresolved are: the general question of semantic transparency\nvs. performance in default behavior; the use (or re-use) of opaque\nvalidators; whether protocol elements were needed to control history\nbuffers; what defaults should be set for caches in the absence of any\nexpiration or age directives; and whether or how to cache PUTs and\nPOSTs.\n\nIn the course of discussion, the working group agreed to delay work on\nbulk validation of cached material to a release after HTTP 1.1.  It\nwas also agreed that we can currently allow for multiple warnings, but\nthat the issue might need to be revisited if contradictory warnings\nwere ever introduced (the current set includes no mutually exclusive\nstatements).  Discussion of Content-id: as validators also took place\nat the meeting, but no consensus emerged.\n\nContent Negotiation:\n\nBrian Behlendorf presented the status report of the content\nnegotiation subgroup.  The subgroup adopted a strategy that allows for\nboth pre-emptive and reactive content negotiation and which will\nnormally progress from pre-emptive to reactive if the pre-emptive is\nnot immediately successful.  As it is now, the draft proposes content\nnegotiation on four axes: media-type, charset, language, and encoding.\nThe subgroup had agreed that to prevent spoofing, all variants would\nhave to be extensions of the original URL.  Discussion at the meeting\non this issue and on the issue of what to do when variant entities\ndon't have URLs did not come to a resolution, and will have to be\nworked out on the list.\n\nrom the subgroup's point of view, Koen Holtmann's draft is ready for\ncomments and implementation experience; some experimentation has\noccurred, but more experience on interoperation is needed.  The\ncontent negotiation subgroup has not yet addressed feature set\nnegotiation at the content-level; Koen has just submitted a draft on\nthat, but it has not yet received much review.\n\nHarald Alvestrand expressed some misgivings about a draft which\nallowed for negotiation on only four axes and which did not have a\nmechanism for further extendibility.  He felt that it might be\npossible to crate a more open framework which used the RFC process to\ndeclare dimensions for variability.  The group agreed that adding\nextendibility to the draft would be considered.\n\nState Management:\n\nDavid Kristol gave the status report of the State Management subgroup.\nThe group has adopted a variant of the Cookie proposal originally\nproposed by Netscape.  A tighter syntax has been proposed as well as\nnew hostname matching rules which help protect privacy by avoiding\nleakage of usage data across sites.  Open issues for the group are:\nthe response header for Set Cookie: ; whether spaces are allowed\naround the equals sign in attribute value pairs; default behavior of\nthe Path attribute; the domain matching rules; and what to do when\nmultiple matching cookies are available.  On a larger level,\ninteroperability between the older cookie format and the newer format\nmay force the creation of a version number for the cookie headers,\nthough every effort has been made to make the two very similar in\nusage.\n\nThe working group came to the consensus that cookies were an optional\npart of the HTTP specification but that those who created cookies\nwithin the HTTP context should do so according to this document's\nspecifications.  The group also appeared to agree that this would be a\nseparate document which advanced with the main HTTP 1.1 document (or\ndocument set), rather than being folded into the base document.\n\nThursday session:\n\nHTTP MIB:\n\nA separate BOF considering a MIB for HTTP servers met at IETF this\nweek. Carl Kalbfleisch reported on the meeting of HTTPMIB BOF.  That\ngroup is interested in extending the work of the MADMAN group to\ncreate a standard way to monitor the health of a web server.  Two\ndraft mibs are already available.  Further information is available at\nhttp://http-mib.onramp.net/ ; to join the mailing list, send mail to\nhttp-mib-request@onramp.net with \"subscribe http-mib Your Name\" in the\nbody of the message.\n\nHOST Redux:\n\nJohn Klensin raised issues about the deployment of the Host: header\nfield under constraints that appeared in the HTTP/1.1 draft.\nHe presented an argument for changing the syntax of the base\nhttp methods so that all use fully qualified domain names and full\nURLs.  This would provide a more elegant solution to the \"vanity\ndomain\" problem which is currently consuming IP addresses than the\nproposed HOST command, which he believes might easily be\nmisimplemented.  The sesne of the meeting was that, while this was\nan elegant solution, it would break a large number of existing servers\nand the desire for interoperability with older implementations would\nforce providers to keep associating vanity domains with unique IP\naddresses.  As an implementation plan, the group will tighten the\nwording on HOST to make it clear that it is a MUST, and plan to make\nthe shift after widespread deployment of HTTP 1.1.\n\nAPPS/TSV:\n\nOn Monday, there'd been an APP/TSV open meeting on the impact of the\nWeb on the Net. Jim Gettys reviewed his presentation.  The basic\nproblem is a \"tragedy of success\".  There are so many users of the web\nthat congestion is becoming endemic on many links, especially those\nwhich are transatlantic or transpacific.  The routing table caches are\nalso being rendered useless by the relatively short packet trains of\nhttp.  Possible long term solutions include multiplexing connections,\na multicast transport layer, and extensive caching networks.\nProjections are for an eventual growth in the web of four orders of\nmagnitude, so we need to address this problem as quickly as possible.\nPersistent connections should help to some degree, but there will\nremain potential fairness problems in allocating bandwidth.\n\nWeb Transaction Security:\n\nLarry Masinter reported on the results of WTS working group meeting.\nThe WTS working group will clarify the relationship of SHTTP and HTTP\nin their current draft, and then it will likely move forward as a\nproposed standard.  This will put some constraints on what the HTTP\n1.1 documents can say about security.  As working group chair of HTTP,\nLarry feels that the work on security relatives to HTTP is taking\nplace in WTS and encourages those interested in that area to work with\nboth groups.  Digest Authentication is a special case which will be\nfinished in this working group; it should, subject to problems with\nthe wording, be in HTTP 1.1.\n\nPersistent connections:\n\nAlex Hopmann presented the results of the persistent connection\nsubgroup.  The persistent connection subgroup took as its goals proxy\nsupport, simplicity, 1.0 compatibility, and fast deployment.  The\nsubgroup has come to consensus on the use of Connection: persist as\nheader, along with Persist:<server-name> (Keepalive: must also be sent\nto maintain 1.0 compatibility).  These headers must be deleted by\nproxies; they apply only for one hop.  If the next hop server replies\nwith the same headers, it will then keep open the connection after\nsending its response.  The client may pipeline requests and the server\nmay pipeline responses.  The server must, however, reply to requests\nin the order they were received.\n\nDiscussion in the working group of marking entity boundaries came to\nthe consensus that chunked transfer encoding would be required but not\nwhether support for multi-part mime would be optional, required, or\nomitted.  Further discussion on the list will be needed to clarify\nsupport level.\n\nRange retrieval:\n\nAri Loutonen presented the changes to the Range-retrieval proposal;\nthey are very few--only the if-valid and if-invalid sections have\nchanged substantially.  Discussion of how range retrieval interacted\nwith caching concentrated on reducing the number of headers, with the\nconclusion that logic bags would not actually reduce the number of\npossible choices needed for completeness, even if those choices were\nsqueezed into a smaller number of headers.\n\nExtensions:\n\nRohit Khare reviewed the status of PEP, the Protocol Extension\nProtocol.  He believes that the syntax for PEP is currently complete\nand ready for review by this group, but he does not believe that it is\na critical path item for HTTP 1.1.  The principal changes in PEP are\nthe addition of scope and the deprecation of a central registration of\nprotocols.  PEP does require a minor version upgrade in HTTP, but only\na minor version.  Discussion presented a number of issues related to\nthe association of related extension protocols (or version of\nextension protocols); no resolution was reached, however, and they\nwill be continued in conversation with the authors.\n\nDemographics:\n\nBrian Behlendorf gave an introduction to the demographics work\ncurrently going on in the W3C; this work would set up a common logging\nformat for proxies and develop methods for servers to interact with\nproxies to retrieve information on transactions they have handled.\nThis system sees the proxies as acting on behalf of the server; John\nKlensin challenged this trust model by noting that historically\nproxies have acted on behalf of the user.  Legal issues were also\nraised about both privacy and legality of storing cached material in\nthe absence of an agreement; after polling the group, Larry Masinter\ndeclared demographics not on the critical path for 1.1, but within the\nscope of the group.\n\nLogistics:\n\nIn a discussion on logistics led by Jim Gettys, the group established\na timeline which would lead to a draft being sent to the IESG on May\n1st.  According to this timeline, a small group will conduct an\nimmediate triage on issues which can, might, or cannot be resolved in\ntime for a May 1st draft.  This will be reviewed by the group and sent\nto the ADs; a feature list and a structure for the set of documents\nwill be presented by March 18th.  A draft will be ready for working\ngroup review by April 1st.  Jim intends for the group to follow the\nissues list very closely, and asks that clarifying discussions be\nconducted off-list and then reported to the list.  Concrete wording\nfor suggested changes will be essential for this timeframe, and he\nencourages us to agree to defer quickly what cannot be resolved, so\nthat we can accomplish what is needed immediately.\n\nThursday afternoon:\n\nA smaller group met on Thursday afternoon to continue the discussion\nof logistics. The group reviewed a calendar and worked backward:\n\nWe accepted that it was important to influence the next release of\nHTTP products from major vendors; we were given information that\nhaving a proposed standard by May 15 is important. No product can\nclaim to 'comply' with the draft until the draft has been accepted by\nIESG as an action item by the IETF secretariat.  Giving allowance for\nthe web conference, and the possibility that the first draft might\nbounce, it seems important to submit the HTTP/1.1 draft to IESG by May\n1. Working backward from that, this meant having a new draft ready by\nApril 1.\n\nThe group then reviewed the issues list:\n\n    http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n\nand assigned 'owners' to shepherd the issue through and propose\nrevised text within the next week.\n\n\n\n"
        },
        {
            "subject": "RE: HTTP meeting summar",
            "content": "A small suggestion -- copy all the \"not in 1.1\" entries to the bottom, \nso that the ones that need work are easier to focus on.\n----------\n] From: Larry Masinter  <masinter@parc.xerox.com>\n] To:  <\"http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\">;\n] <http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: HTTP meeting summary\n] Date: Saturday, March 09, 1996 12:06AM\n]\n] The area directors want a summary of the meeting before the week's\n] end. This is what I sent them (I'd cc'd http-wg but mistyped the\n] mailing list address.)\n]\n] Hopefully we'll get the minutes out by the end of next week, including\n] minutes of the Thursday afternoon session.\n]\n] ================================================================\n]\n] HTTP/1.0 has been submitted to become an Informational RFC. We are\n] focusing on getting a proposed standard for a new version of HTTP with\n] an aggressive schedule: submission of a Proposed Standard by May 1.\n] This schedule will mean dropping some issues and features in this\n] first standards-track version and considering them for standardization\n] in a subsequent version. Of top priority are those fixes that will\n] help relieve HTTP-caused Internet congestion: host identification,\n] caching, persistent connections.\n]\n] We had formed a number of subgroups to evaluate the HTTP/1.1 draft.\n] In our two originally scheduled meetings, we reviewed the subgroup's\n] conclusions and open issues, in the areas of caching, persistent\n] connections, content negotiation, state management, range retrieval,\n] authentication, extension methods, and extension methods.  In\n] addition, we also had a lively and productive interaction with the WTS\n] members where HTTP security work is proceeding, and a third meeting on\n] Thursday to triage our task list and assign ownership:\n]\n]    http://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n]\n] Jim Gettys is now the lead editor.\n]\n]\n]\n]\n] \n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": ">if you  \n>\n>    GET http://foo.com/test#frob HTTP/1.0\n>\n>they might ask foo.com for\n>\n>    GET /test%23frob HTTP/1.0\n>\n>or vice versa. Is there any reason to disallow this, and if so, what\n>language would be put in the spec to disallow it; alternatively, if\n>proxies might do this kind of transformation, what should we say?\n\nI would say we have to let this pass. Principally because of the large \nnumber of proxies out there which already do this. \n\nThere is a principled reasoning behind this. We distinguish a proxy or \ngateway from a tunnel by the level at which the agent acts. Tunnels act \npurely at the syntactic level. Proxies and gateways act at the semantic \nlevel. We permit a proxy to perform any action provided it is semantically \nneutral and in addition may permit a number of semantic transformations.\n\n[Another area to consider is using URLs as a subliminal channel or a basis \nfor steganography. The URL above effectively allows us to encode 0s and 1s. \nThere may well be cases where we don't want that to happen. This is a fairly \npointless nit to pick but I thought I would get in first]\n\n\nThe reason why the semantic transformation stuff matters is that a firewall \nproxy may want to strip out all header lines it does not understand, \neffectively down-rating the connection to HTTP/1.0 or some other known safe \nsubset. This requires a complete parse of the request and regeneration of \nthe passed on request. The parser may wish to specifically check URLs for \nstrict validity to avoid possible holes occurring with illegal escape \nsequences. Thus it is a legitimate proceedure to parse out the URL, \ncannonicalise and regenerate.\n\n\nI think that if we want strict syntactic netrality in a proxy we have to use \nthe wrapped method.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: Cache validator",
            "content": "Jeffrey Mogul:\n>\n>    Since it is unlikely that anyone will ever implement a system which\n>    only changes the validator for \"significant\" changes, I think it\n>    would be silly to lose the additional functionality gained from a\n>    guaranteed change indicator.\n>\n>Well, here's one example: supposing that a busy site with\n>slowly-varying content wants to maintain a hit-count image on\n>its home page.  (Whether you yourself think this is something worth\n>doing, clearly a lot of people want to do it.)  It's probably not\n>all that important if a user sees a hit-count that is slightly\n>inaccurate, nor is it worth using up a lot of network bandwidth\n>to provide accurate hit counts.\n>\n>So supposing that the server constructed an opaque validator\n>for a hit-count GIF that is simply an encoding of the hit-counter\n>mod 10 (or mod 100, or mod 128, or whatever).  Then a large\n>fraction of the GET+If-Valid: requests on this GIF would return\n>304 Not Modified, even though the count might have increased\n>slightly.\n\nThe correct thing to do in this case is to make the comparison function used\nfor servicing the GET+If-Valid requests a mod 10 comparison, not to make the\nvalidator the mod 10 of the version number.\n\nIf 10 subsequent (but semantically almost equal) versions of the .gif get\nthe same opaque validator, this means that the validator is useless for\nsafely contining an interrupted GET with a GET+If-Valid+Range request.\n\nI see safe continuation of interrupted GETS as the main reason for having an\nopaque validator in the first place.\n\n>-Jeff\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "> > The criticism of MGET that it is not as general as keep-open is true,\n> > but I think there is serious danger of performance degradation if that\n> > generality is used.\n> \n> I don't quite follow, could you please give some examples?\n\nIf you use Connection, then the server process stays active for quite\na long time.  If you use MGET the processing still happens in batch\nmode, and you get to exploit the TCP kernels buffering quite a lot.\nThat's a big win, and losing that would hit hard on all servers.  In\nfact, I'm fearing that this would make the situation worse from the\nservers' point of view, rather than alleviate it (this would require\nsomeone doing some load testing).\n\n> I can see several things that I would like to do. First, it would be\n> nice to use the Web to have destributed code development. When you\n> check in your files in a remote code management system, you can use\n> PUT. Now, as you often have a lot of files, it would be great to do all\n> this using one connection.\n\nThis is a couple of orders of magnitude smaller problem -- you PUT\nfiles a lot less than people GET them (at least I wouldn't want to be\nthe webmaster of such an unpopular site...).  Even then you can have\nan MPUT.  I have a hard time imagining that someone would\nrealistically do multiple GETs/POSTs/PUTs intermixed.  Really.\n\n> In the spec we have done a great deal of work trying to give the\n> client the same possibilities as the server and we should not limit\n> this using MGET, MHEAD, MPUT etc.\n\nMake the most common cases fast and easy, don't burden them with\nextra, frankly unnessary flexibility to accommodate with every\npossible (unused) combination.\n\n> In the next six months you will see many clients being a lot more\n> interactive\n\nThen we'll use a new generation of HTTP.  Now we're talking about\nHTTP/1.1 (we are, aren't we??), that would be a relatively small step\nfrom 1.0 and would solve a just the worst problems that we're now\nfacing with 1.0.\n\n\n> > Netscape obtains with multiple connections is not viable. It's a\n> \n> As I wrote in an earlier mail: This is ok if one does it, but if\n> everybody does it then they all loose. In other words: it must be\n> scalable!\n\nThis is not a scalability issue -- your GUI client fetches the images,\none way or another.  You'll have a small overhead (1 connection) or a\nlittle larger overhead (1 connection per image), but this is not\nexponential.  Scalability problems come from somewhere else, and\nfetching inlined images with a lower overhead will only slightly\nprolong the time it takes to hit that bottleneck.\n\nCheers,\n--\nAri Luotonenhttp://home.mcom.com/people/ari/\nNetscape Communications Corp.\n650 Castro Street, Suite 500\nMountain View, CA 94041, USA\n\n\n\n"
        },
        {
            "subject": "Re: Cache validator",
            "content": ">>So supposing that the server constructed an opaque validator\n>>for a hit-count GIF that is simply an encoding of the hit-counter\n>>mod 10 (or mod 100, or mod 128, or whatever).  Then a large\n>>fraction of the GET+If-Valid: requests on this GIF would return\n>>304 Not Modified, even though the count might have increased\n>>slightly.\n\nFirst, it's 'div' not 'mod'. :)\n\n> The correct thing to do in this case is to make the comparison function used\n> for servicing the GET+If-Valid requests a mod 10 comparison, not to make the\n> validator the mod 10 of the version number.\n\nthis makes a lot of sense.\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": ">] >How does the cache know if the two \"foo.txt\" are variants of one\n>] >another, or if the one-and-only \"foo.txt\" has changed?\n>]\n>] The presence of a \"Vary:\" header, ie:\n>] HTTP/1.0 200 OK\n>] Content-ID: \"xxxxx\"\n>] Vary: Foo-Header\n>\n>header in the response.  So you're right.  However, the Vary: header \n>value is a _request_ header name.\n\nRight -- telling a proxy/client that a resource varied on a response header\nwould be pointless, because the proxy/client would have to re(HEAD?)request\nthe object every time to see what the response header would be, to know if\nthis request varied.  Whereas, if we Vary: on a request header, the\nadvantage is that a proxy could make attempts to store old headers and\npreemptively serve from its cache w/o any validation request to the origin\nserver.\n\n>How do I match it up the Accept-* \n>values in the Vary header with the Content-* entity headers in the \n>response?\n\nDefine \"I\" in this sentence.  You, the response-receiving proxy don't match\nthe Accept-* headers with the Content-* headers.  The idea of the Vary:\nheader is that the server has gone off and done something you shouldn't be\ntrying to make inferences about.  It's negotiating opaquely, and the best\nthe proxy can hope to do is to do equality matches on subsequent request\nheaders.  Anything except exact equality on the Vary:ed request headers\nnecessitates the origin server performing its proprietary negotiation\nalgorithm locally.\n\n>I suppose I've saved the Accept-* headers in the request, so \n>maybe that is what I use to see if there is already another instance in \n>the cache.  But can't I have two different Accept-* configurations \n>translate to the same entity?\n\nSure you can.  You are describing the flaws with the Vary: system.  Even\nthough different request headers may map to the same variant, the Vary:\nheader doesn't give enough information for proxies/client to make inferences\nabout the origin server's negot. algorithm, the origin server's variants, or\nthe choice an origin server would make on an individual request.  When an\norigin server is using the straight-out-of-the-book content negotiation\nalgorithm in the HTTP spec, using the URI/Alternates proposal is much better\nfor everyone involved, especially the users, who can then pick the variant\nthey want directly and bypass us protocol writers.\n\n>gotchas -- having the origin-server just _tell_ the cache that it's \n>variant number 23 would be a lot easier, and it saves transmitting the \n>entity-body when you've already got it cached, even though it's got \n>different Accept-* header values.\n\nI certainly agree that it's important to know which variant you are dealing\nwith.  But I think Roy is arguing that a URL/Content-ID pair is effectively\nequal to a Variant-ID for sensing 'equality of variant' which is the only\nnecessary variant operation that should be done outside the origin server.\nAdditionally, an over-dependence on a Variant-ID construct leads to a bunch\nof questions of what happens when the list of variants changes.  If each\nvariant only differs by Content-ID, then those questions are answered for\nthe inductive case.\n\n>Whereas, if there is a variant ID, you know what validator to overwrite.\n\nAssuming a static list of variants that always map to the same VariantID's.\nA Content-ID system wouldn't have that additional maintenance requirement\nplaced on the server.\n\n>In this sense Vary and Content-ID are orthogonal. A really stupid cache \n>could not use Vary and just go back to the origin-server to do all \n>content negotitation, and still get the advantage of caching for all \n>entity-bodies.\n\nYour description of a stupid cache is true for either a Variant-ID or a\nContent-ID system.  This is what you mean when you say \"Vary and Content-ID\nare orthogonal\" right?\n\n-----\nDan DuBois, Software Animal           http://www.spyglass.com/~ddubois/\n    Download a totally free copy of the Spyglass Web Server today!\n        http://www.spyglass.com/products/server_download.html\n\n\n\n"
        },
        {
            "subject": "Re: Variant ID",
            "content": "  Hi -\n   I think we're converging enough on models of caching theory in the\nface of content negotiation to ask folks to try to\nactually propose wording for use in the sections\nfor  content negotiation and caching. I think the discussions\nabout  vary and content-ID is interesting, and seems to\nbe  converging enough for us to to be talking about the\nthe words. Not just concepts. OK?\n\n           -- Larry\n\n\n\n"
        },
        {
            "subject": "As you probably know by now... (schedule and important points",
            "content": "I've taken over as editor of the HTTP 1.1 document.\n\nJim's koans for HTTP:\n\no The perfect is the enemy of the good\no If a bus does not leave this station, there will be no future busses\n  to interesting destinations from this station\no If you win at pin-ball, you get to play again\n\nThose being said:\n\nI managed to forget the calendar we wrote on at the IETF meeting\nwhen I came into work today; I'll post the dates from that calendar\nto the list this evening when I get home, to confirm what Larry\nhas said in his minutes.  Here is the schedule as Larry's minutes\nnoted:\n\no March 15 - Issue list review by WG.\no March 18 - Tentative feature list of 1.1, and structure\nof resulting document(s).\no April 1 - New draft for comment by WG.\no May 1 - submission to IESG\n\n1) We need a pretty complete issues list by the end of this week.\nLarry will be out of town after this week for two weeks (a good\nboondoggle to Japan), and in his absense, I'll maintain the issues\nlist.\n\nIt is VERY important for you all to PLEASE REVIEW and COMMENT (when\nneeded) on the Issues list this week.\n\nSend mail to Larry and myself on problems/additions/resolution of\nitems on the list.\n\n2) We need to keep the signal to noise ratio on the list as favorable\nas possible.  I am making a request of those on the list to try\nto resolve ambiguities/issues with the major subgroup chairs, and\nreport the results of these side conversations as a single message to the\nlist once complete, if possible.  This may help the keep messages\ndelving into a given topic from swamping the list, as it has been\nin danger of from time to time.\n\n3) We need to quickly move from general closure of issues to\ndrafting exactly the words required to implement the closure.\n\n4) I plan to organize a committee of people to help with drafting\nthe words in the document, and hold a teleconference with those\npeople (at least) weekly to cover issues.   Various people will likely\nbe drafted into those teleconferences as needed.\n\n5) I will attempt to organize the document to allow separate pieces to\nbecome separate documents so that if we find that we cannot reach closure\non those pieces as part of basic 1.1, they will be able to proceed independently\nwithout holding up the core of 1.1.\n\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: Variant IDs",
            "content": "Larry Masinter:\n>\n>  Hi -\n>   I think we're converging enough on models of caching theory in the\n>face of content negotiation to ask folks to try to\n>actually propose wording for use in the sections\n>for  content negotiation and caching.\n\nYes.\n\n> I think the discussions\n>about  vary and content-ID is interesting, and seems to\n>be  converging enough for us to to be talking about the\n>the words. Not just concepts. OK?\n\nI just sent mail to Jeff and Roy to discuss details, so that we can\nstart writing words for the April draft.  I guess we will report to\nthe main list as soon as we have agreement.  I prefer to keep the\ndiscussion of details off the mail list to reduce the s/n ratio.  If\nother people want to join in, I suggest they mail me.\n\n>           -- Larry\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "subscrib",
            "content": "subscribe\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "Personally, I don't think proxies *should* rewrite URLs, and that\norigin servers should decide on logical equivalence and report it.\n\nThis is the only way that servers that are not 'multihomed' but happen\nto have multiple DNS entries can be declared to be equivalent.\n\nOtherwise, we have to define equivalence for URLs unambiguously, and\nI'm reluctant to engage in that because it's likely we'll get it\nwrong.\n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "In the example I quoted, I showed\n\n    GET http://foo.com/test#frob HTTP/1.0\nturned into\n   GET /test%23frob HTTP/1.0\n\nbut this wasn't actually the example; it was a proxy that turned\n\n   GET http://foo.com/test!place HTTP/1.0\ninto\n   GET /test%21place HTTP/1.0\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "Well, my client's staff and I have spend something between 40 and 200\nperson hours tracking down and resolving a problem in our server like\nproduct caused by these two bytes which we didn't realize existed.\n\nIf only I'd been current on my e'mail .... >-:). The symptom was that\nour code running on Win/95's winsock would leave the two bytes hanging\nand close the connection. Since the winsock stack would know it had\ntwo unreceived bytes, it would apparently do a RST instead of FIN.\nThis resulted in NETSCAPE complaining about the connection being\nreset by the peer.\n\nInterstingly, if we inserted a delay before the close, then apparently\nNETSCAPE would close first and never complain. I have no idea what\nstimulated the thought that it might be outstanding receive data\nwhich caused the problem we were seeing but we tried alomst everything\nelse first.\n\nIn summary, I'm not sure it would be practical to change the content\nlength or remove the characters but their existance surely *MUST* be\ndocumented, and if perchance we have to make more edits on 1.0 we \nshould add a comment there as well.\n\nMy inclination is to document the content as being followed by optional\ntrailers whose end is indicated by a NULL line. Current programs and CGIS\nkeep working and new applications which might expect such trailers\ncan work as well.\n\nperl code *CAN* be written to not require trailing CR/LF but it is\nmore complex so new applications could be made to work.\n\nDave Morris\n\nOn Mon, 4 Mar 1996 hallam@w3.org wrote:\n\n> \n> >I think it would break binary files in weird ways.  For instance\n> >transfering an executable would cause it to gain 2 bytes on\n> >every transfer.  It would certainly break anything that contained\n> >a checksum.\n> \n> I wasn't suggesting this. I was suggesting that we change the \n> spec for www-url-encoded-form. I would not want to change the way \n> binary files are transmitted!\n> \n> The problem is that netscape is saying it is sending a content length of 36 \n> bytes but actually transfering 38. This is a bug IMHO. Whenever a length of 36 \n> is stated it should mean 36.\n> \n> If Netscape is sending a bogus CRLF after binary files then I would expect \n> problems to arise in any case. \n> \n> Phill\n> \n\n\n\n"
        },
        {
            "subject": "Re: proxies rewriting URL",
            "content": "On Tue, 12 Mar 1996, Larry Masinter wrote:\n\n> Personally, I don't think proxies *should* rewrite URLs, and that\n> origin servers should decide on logical equivalence and report it.\n\nIn general, I'm inclined to agree.  IF the proxy believes the\nrequest to be a protocol violation which would require re-writing,\nit should instead reject the request with appropriate diagnostic\ninsight.\n\nBUT the current specs indicate that:\n\n   http://foo:80/xyz\n\nshould become:\n\n   http://foo/xyz\n\nIf I understand they wording. I would not that at least one\nbrowser in common use does the opposite.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Dates for HTTP 1.",
            "content": "These are from the calendar slides agreed to at the WG meeting\nin Los Angeles.\n\nMarch 7- Issue list triage\nMarch 8 - Issue list edit to reflect triage\nMarch 13 - Review issues\nMarch 15 - Feature list & structure of the document(s) set\nApril 1 - Updated draft to W.G.\nApril 2 - Start new issue list specifically on updated draft\nApril 24- W.G. last call\nMay 1- IESG submission of 1.1 draft.\n\nNote that the next WWW conference is May 6-10, in Paris.  I am\nin the wedding party of a friend the weekend before this (May 4-5).\nSo if we don't make that date, it will likely miss by several weeks,\nand the area directors made it clear that that was not acceptable.\n\nPeople: PLEASE REVIEW THE ISSUES LIST ASAP.  Issues not raised now\nwill likely be deferred until later, unless critical.\n\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: hallam&#64;w3.org: Netscape Bug or KeepAlive Feature ",
            "content": "[tail* of woe deleted]\n\n>My inclination is to document the content as being followed by optional\n>trailers whose end is indicated by a NULL line. Current programs and CGIS\n>keep working and new applications which might expect such trailers\n>can work as well.\n\nThis does not work because there are still a large number of clients which \nread until the close of connection. Footers can incorporate whitespace but text \ncharacters would appear en-screen.\n\nI would like to see footers on all messages but I think that this is only \ncurrently practical with chunked :-(\n\nPerhaps we simply define messages as having optional CRLF sequences for the time \nbeing.\n\n\nPhill\n\n\n[*Geddit?]\n\n\n\n"
        },
        {
            "subject": "Re: PUT, DELETE vs. POS",
            "content": "Larry wrote:\n>\"PUT and Derived-From with magic Content-Version\" and \"PUT and\n>If-Valid with magic null validator\" are less magic than \"POST with\n>magic posted data\"?\n\nFirst, maybe we need to replace the idea of \"magic\" with the idea\nthat an URI is *always* in a well-defined state, rather than being\nin an indeterminate state if it \"does not exist\".  This removes the\nboundary conditions upon creation or deletion of a resource.\n\nSecondly, yes, they are less magic, as your next question points out:\n\n>Does \"POST\" allow a \"Modifies: URI\" response header?\n\nIn the WG, we've gone around several times over the issue of what\nconstitutes a cache-key, both for caching and for content-negotation.\nIn this case, I have to say that I think it simplifies our job\ntremendously if updates to the data store can follow the same semantics\nas other requests.\n\nDetermining which entity may be affected by a request should be a\nwell-defined function of the URI and headers*, which is not the case\nif one has to go grubbing around in POST fields, especially since the\ngeneral POST request probably has no effect upon the internal state of\nthe web.\n\nThe best analogy I can come up with at the moment is the distinction\nbetween memory addresses and I/O addresses in a processor system.  It\nmakes the system architects (both hardware and software) job much\neasier to maintain that distinction, since memory can be cached, mapped,\netc. in a semantically transparent manner, and I/O space allows one\nto retain all of the more complex functionality for which one might wish.\n\n-Dave\n\n* this also implies that new/unknown methods can be treated in a very\n  orthogonal manner, for caching, proxying, access control, etc.\n\n\n\n"
        },
        {
            "subject": "HTTP:  T-T-TTalking about MIME Generatio",
            "content": "I take a weekend off to enjoy the nice weather and I miss an e-mail storm.\n\n|From: \"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU>\n\n|Marc Salomon writes:\n|\n|> 3. Operating under the assumption that at this time in the life cycle of the\n|> web, most HTML documents contain components that reside on the same server as\n|> the document itself, why not trade the multiple network connections from the\n|> client to the same server for some up-front packaging by the server.\n|\n|Because it defeats caching.\n\nI have sketched a way that caching can be simply implemented and optimized using\nmultipart.  If a client has previously rendered a document, it can use this \nscheme to issue an (MGET, SESSION, at this level, what its called or the the \nproxy-related concerns are irrelevant to me--I trust people with greater \nexpertise on this than I will Do The Right Thing--multipart should sit on top \nof that) to optimize subsequent reloads of that document or other documents \nthat share its inclusions.  \n\nIndeed, the restrictions on the use of Message-ID: in HTTP/1.0 defeat caching\nWRT MIME.\n\n|Besides, some people may want to use MIME multipart types on HTTP for other\n|reasons -- why should the server treat them any differently than other content\n|types?  [just server here -- I know why clients would want to do so]\n\nI would appreciate any specific examples of how what I propose poisons the\nwell for anyone else as well as specific suggestions as to how this can be \ndone correctly.\n\nPerhaps some of the work being done at EIT on S-HTTP, which used application/\nhttp should be considered as well.\n\n|> 6. Interoperating efficiently with client cache management brings up some\n|> interesting issues.  The ability to check the HTML document's requirements\n|> against the client-side cache before issuing a precisely tailored HTTP MGET\n|> request (which would be returned as multipart/mixed*).\n|\n|It's much easier to just keep the connection open (with possibly a very\n|short timeout as defined by the server) and respond with multiple responses\n|(each one of which may be a multipart entity).\n\nBut TCP is much less efficient if you are sending bunch of small objects--you\nnever get out of first gear as it were.  Wasn't it argued that TCP doesn't get \nefficient till it gets up to speed on a data transfer?  Would having to satisfy \na set of time-spaced serial requests allow the connection to ever get up to \nspeed?\n\n|> 7. An instance of the proposed MIME encoding scheme for HTTP follows.  This\n|> is currently in the process of a feasibility study in METHADONE (Mime\n|> Encoding\n|> THreAded Daemon Optimized for Network Efficiency), a caching, lightweight\n|> threaded, MIME multipart encoding HTTP server for solaris 2.x (exploiting the\n|> rich base functionality of NCSA's httpd) currently under beta development at\n|> the UCSF Library and Center for Knowedge Management.\n|\n|Sounds like fun, but that's not the hard part.  Serving up MIME multipart\n|is not the problem.  What you need to do is get clients to understand\n|(and parse) MIME multiparts.  And that means ALL clients.\n\nIf this job weren't fun...\n\nThis is a test, this is only a test.  If it works, I'll come back with some\nnumbers.  If it doesn't, I'll come back with some numbers.  If you connect to\nmy server and don't specify Accept: multipart/mixed, there will be no problem--\nfor now, (accept for the delay time) or at least until there is some \nstandardized implementation of multipart in HTTP/1.1 to which I will, of \ncourse, adhere.\n\nIndeed, people browsing the 500 gig of online medical journals in the interim \nmight not have to wait 4 minutes for a screen of 70 20K journal cover images \nto load.  If it takes more than 3 minutes for this page to load on my SS10 \nrunning both client and server, imagine how long it will take on an xterm \nacross town at SF General, connecting to our SC2K along with 500 other \nimpatient physicians.\n\nThe system as it exists is so inefficient as to be broken in certain cases\nthat creep up again and again in our application.  I want to be sure that my \nattempt to fix it for my user community be consistent with specification, \nexisting and proposed and compatible with other information systems.  Any of \nus implement a quick fix for this in an afternoon, but its not worth my time \nif I am going to contravene specifications.\n\n|> Message-ID: <http://host.domain/http_mime.html>\n|\n|This is an invalid use of Message-ID -- it must be unique.\n|\n|> Content-ID: <http://host.domain/path/http_mime.html>\n|\n|This is an invalid use of Content-ID -- it must be unique.\n\nStupid me.  I had assumed that the draft HTTP spec-00 talked about current \npractice, but it deals with protocol-as-documented but unimplemented as well.\n\nThe specifications in HTTP/1.0 for both the format and use of the Content-ID: \nare more restrictive than that of RFC 1521.  The MIME spec says that the\nprimary purpose of this field is to assist caching and offers no format\ntemplate, while HTTP/1.0 dedicates this field to a transaction identifier and \nrequires a strict format template.\n\nI could make an argument that a URL uniquely identifies the content of a\nbody-part in that a URL cannot identify but one object at one time.  The main\nreason for the MIME optional Content-ID field is to allow for caching, which is\nfacilitated by the use of a URL (in conjunction with the Date: header) in this \nfield.  The URL would be valid and cachable from Date: till Expires: and is an\nexcellent candidate for a RFC 1521 style Content-ID.  A contradictory use for \nthe Content-ID field is specified in HTTP/1.0-draft-00, although not currently \nused in any implementation.\n\nThe Content-Description header field as specified in RFC 1521 would probably\nbe an appropriate place for this, although I do not see a Message-Description \nfield there.  One would be tempted to look to the URI header to indicate the \nURI associated with a body part, but from what I can tell, it is only used to \nredirect a request and its use here seems inappropriate.\n\n|And what do you do about Content-Encoding?  It's not valid MIME at all.\n\nand later in the weekend Roy writes:\n\n|Until the MIME people add Content-Encoding to the official RFCs, there is no \n|point in even discussing the issue here.  All that we can do is show how they \n|differ and possibly how an HTTP -> SMTP/NNTP gateway should behave.\n\nHTTP currently uses:\n\nContent-Type: application/postscript\nContent-Encoding: x-gzip\n\nTo express a two-level encoding.  Two level encodings are not included in\nMIME because of concerns about machines that are unable to easily perform\nthe (de)compression.  Since only machines that are able to (de)compress (should)\npresent this information during negotiation, it does not seem problem for the\nweb.  In short, current practice of HTTP conflicts with the limitations of \nMIME, so we could validate the out that the MH people took:\n\n<comp.mail.mime-FAQ>\nMiscellaneous parameters:\n\nx-conversions=compress  MH 6.8.x viamail: used with application/octet-stream;\n                          type=tar.  See also tar(1) and compress(1).\n</comp.mail.mime-FAQ>\n\nThe solution for a multipart MIME compliant expression of Content-Encoding \nwould be to use something like one the following:\n\nContent-Type:  application/postscript\nContent-Transfer-Encoding: binary; x-conversions=gzip\n\n|> Two consecutive boundary strings indicate an EOF.\n|\n|What?  See RFC 1521.  Or, for that matter, read the HTTP/1.0 draft -- it\n|already defines EObody-part and EOentity for Multipart entities.\n\nIt had a hard time jumping out at me while preparing this document, but you are\nagain correct.  EOF is indicated by --boundary--.\n\n% grep -i EObody-part draft-fielding-http-spec-00.txt rfc1521.txt\n% grep -i EOentity draft-fielding-http-spec-00.txt rfc1521.txt\n\nWhat are EObody-part and EOentity?\n\nSince the draft HTTP 1.0 spec-00 is supposed to document current practice in\nthe same manner as the HTML 2.0 effort, why is multipart/mixed documented,\nwhich is most certainly *not* current practice in any implementation I know of\n(is it?) although it is described in BASIC HTTP?\n\n|> 8.  I plan to bring this up on the sgml-internet list as well, for a broader,\n|> more general perspective.\n|\n|They already want to ship SGML as a multipart/mixed -- adding a bunch of\n|HTTP-specific controls to it just mucks-up things for the mail people.\n\nTaking a case study first might yield some valuable lessons for the general\ncases.\n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "Roy T. Fielding:\n>[Koen Holtman:]\n>> General comments on section 3.1:\n>> \n>> 1) Is there any rationale for the rules in Section 3.1?  \n>\n>Yes. [...]\n\nRoy, thanks for your lengthy reply, it answered many of my questions.\nNevertheless, I still have some problems with Section 3.1, I will\noutline them below.\n\n>> 2) I'm not sure that I really like the weak requirements for\n>> translation by proxies that are in the draft spec now: these\n>> requirements will basically shift the burden of making things\n>> interoperable from proxy authors to httpd and CGI authors, and there\n>> are a lot more CGI authors than proxy authors.\n>\n>CGI authors will have to do better than they do now in order to be\n>HTTP compliant.\n\nYes, but will they?  Statistics show that we cannot expect too much\nfrom the average CGI author.  Most CGI scripts that produce dynamic\ncontent don't add an expires header, and this situation is unlikely to\nimprove soon.\n\n>[...]  How the server is\n>implemented is not our concern \n\nYes, I agree we should not care whether script authors use CGI or some\nfuture more advanced interface.  The point is: how much attention to\ncompliance from script authors, CGI script or otherwise, can we\nexpect?  Not much, so it would be foolish to put protocol elements in\nHTTP/1.1 that require script writers to pay a lot of attention to\ncompliance.  If we do, the result will be lots of new `missing header\nheuristics' in proxy caches.\n\n>> 3) With proxies being allowed to upgrade and downgrade the minor\n>> version number, it seems that the server, it it gets a 1.1 request,\n>> will not be able to find out if there are any 1.0 applications in the\n>> request chain.\n>\n>Yes.\n\nI was afraid it would be yes.  I find this completely unacceptable,\nbecause 1.0 will be an informational standard.  This means that you\ncan make no assumptions about a particular 1.0 feature being\nimplemented as described.  The 301 and 302 response codes are a good\nexample.\n\nSo under the current scheme, with 1.0 software in the request chain\nundetectable, even if 95% of the software is 1.1, the service author\n_will still have to guess_ which protocol elements can be safely used\nin a response, as the response may go to some ancient 1.0 client.\n\n>  We have talked about adding that information to Forwarded, but\n>we also need to come up with a more compact encoding for that header.\n>In any case, the recipient doesn't need to know about 1.0 applications\n>in the request chain if there are no incompatible changes in the\n>request chain applying to more than just the immediate connection.\n\nI'm not worrying about the problem of the server interpreting the\nrequest.  I am worrying about the problem of the server having to\ngenerate a response that the client chain is capable of processing.\n\nThe current scheme uses the version numbers in requests and responses\nto indicate capabilities of the immediate connections in the chain.\nYet, AFAIK, all the proposed 1.1 mechanisms that act on immediate\nconnections, like persistent connections, have their own headers to\nsignal capabilities, so they won't need the version numbers in the\nfirst lines of the requests and responses.\n\nI propose use of the version number in the request-line to indicate\nthe _minimal_ protocol version used in the request chain.\n\nWe can leave the Forwarded header as it is, and require that proxies\nnever increase the version number in a request-line that is relayed.\nProxies may however add 1.1 request headers to 1.0 requests that are\nrelayed to signal special capabilities like being able to do\npersistent connections.\n\nFurthermore, I propose use of the version number in the response-line\nto indicate the protocol version used by the origin server.\n\nIs this acceptable?  Both proposals are compatible to current\npractice.\n\n>> 4) So the server has to make things interoperable, but it does not\n>> even know the capabilities (versions) of the software it is serving\n>> to!  So, for example, if the server uses a 1.1 Cache-Control\nheader, >> it must always include a 1.0 Expires header.  > >Yes.\nThere is no way to avoid it without requiring a major protocol\n>change, and thus a major version number change.\n\nI think my above proposal is a way to avoid it.\n\n[...]\n>I would hope that the entire document gets \"seriously reviewed\".\n>However, changing the version rules would be a mistake, since it would\n>only make it easier on the protocol designers and make it harder to\n>deploy implementations.\n\nI think my proposed version rules make thinks easier, not harder.  The\nrules have no disadvantages for proxy authors, but a advantage for\nservice authors: the server can detect whether it is talking to\nsoftware whose features are unambiguously documented in a\nnon-informational specification.\n\nUnder my rules, when some 95% of web software is 1.1, service authors\nof complicated services that need to be very reliable can finally stop\nguessing about what is actually implemented in 1.0 user agents,\nbecause they will be able to generate a response that says `please\nupgrade your software, you have to use 1.1 software if you want to use\nthis complicated service' for the 5% of users still using 1.0 agents.\nThe ability to so this is needed because service authors are sometimes\nrequired by law or by the market place to prevent users from shooting\nthemselves in the foot.\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Minutes, HTTP working group, 35th IET",
            "content": "Minutes of the HTTP Working Group at the 35th IETF (Los Angeles, March\n4-8, 1996).  Reported by Ted Hardie (hardie@nasa.gov), revised by\nLarry Masinter (masinter@parc.xerox.com).\n\nThe HTTP working group held two regular sessions, the first on March\n5th and the second on March 7th; in addition, an extra meeting took\nplace the afternoon of March 7th and is reported here. Larry Masinter\nchaired the sessions.\n\nThe HTTP 1.0 specification has been taken up by the IESG, and the\nproposal to move it forward as an Informational RFC is expected to be\nprocessed at their next meeting. The group applauded Roy Fielding for\nhis work in producing the specification.\n\nJim Gettys will be taking over as primary editor of the HTTP 1.1\nspecification, continuing Roy Fielding's work and coordinating with\nothers who will aid in the editing process (including Roy, Henrik\nFrystyk, and others.)  The Area Directors approved Jim's selection as\neditor; as is expected of WG editors, Jim confirmed that he would act\non behalf of the working and that he did not see his role at Digital\nor in the W3C as in conflict with his role as editor.  One of the\nfirst items Jim will do is to decide on how best to split the HTTP/1.1\nspecification into separate documents.\n\nMost of the meeting was consisted of reports from the subgroups\nconstituted at the Dallas IETF.  This marks the formal end point of\nthe subgroups.  The members of the individual subgroups may continue\nto use those mailing lists to discuss issues, but further internet\ndrafts from the subgroups are not expected.\n\nCaching:\n\nJeff Mogul presented the status report of the caching subgroup.  The\ngroup generally agreed on cache control headers for Expires, Age, and\nCache-control; some differences remain on the exact syntax of the\nCache-control headers related to object \"freshness\", but the subgroup\ndid not see these as anything more than a stylistic disagreement.  The\nsubgroup also agreed on the need for a Warning header, and proposed a\nseries in the 90 range; some disagreement on how to associate text\nwith the different warnings remains and will have to be worked out on\nthe list.  The subgroup had also reached consensus on the need to\ndistinguish between history buffers and caches; history buffers are\nmeant to replay pages which have already been seen, as they were seen,\nand they should thus not be subject to the same directives as caches.\n\nThe caching subgroup has reached general agreement on how caching\nshould interact with the Location: header and the Vary: header, but it\nhas not yet worked out the details.\n\nStill unresolved are: the general question of semantic transparency\nvs. performance in default behavior; the use (or re-use) of opaque\nvalidators; whether protocol elements were needed to control history\nbuffers; what defaults should be set for caches in the absence of any\nexpiration or age directives; and whether or how to cache PUTs and\nPOSTs.\n\nIn the course of discussion, the working group agreed to delay work on\nbulk validation of cached material to a release after HTTP 1.1.  It\nwas also agreed that we can currently allow for multiple warnings, but\nthat the issue might need to be revisited if contradictory warnings\nwere ever introduced (the current set includes no mutually exclusive\nstatements).  Discussion of Content-id: as validators also took place\nat the meeting, but no consensus emerged.\n\nContent Negotiation:\n\nBrian Behlendorf presented the status report of the content\nnegotiation subgroup.  The subgroup adopted a strategy that allows for\nboth pre-emptive and reactive content negotiation and which will\nnormally progress from pre-emptive to reactive if the pre-emptive is\nnot immediately successful.  As it is now, the draft proposes content\nnegotiation on four axes: media-type, charset, language, and encoding.\nThe subgroup had agreed that to prevent spoofing, all variants would\nhave to be extensions of the original URL.  Discussion at the meeting\non this issue and on the issue of what to do when variant entities\ndon't have URLs did not come to a resolution, and will have to be\nworked out on the list.\n\nFrom the subgroup's point of view, Koen Holtmann's draft is ready for\ncomments and implementation experience; some experimentation has\noccurred, but more experience on interoperation is needed.  The\ncontent negotiation subgroup has not yet addressed feature set\nnegotiation at the content-level; Koen has just submitted a draft on\nthat, but it has not yet received much review.\n\nHarald Alvestrand expressed some misgivings about a draft which\nallowed for negotiation on only four axes and which did not have a\nmechanism for further extendibility.  He felt that it might be\npossible to crate a more open framework which used the RFC process to\ndeclare dimensions for variability.  The group agreed that adding\nextendibility to the draft would be considered.\n\nState Management:\n\nDavid Kristol gave the status report of the State Management subgroup.\nThe group has adopted a variant of the Cookie proposal originally\nproposed by Netscape.  A tighter syntax has been proposed as well as\nnew hostname matching rules which help protect privacy by avoiding\nleakage of usage data across sites.  Open issues for the group are:\nthe response header for Set Cookie: ; whether spaces are allowed\naround the equals sign in attribute value pairs; default behavior of\nthe Path attribute; the domain matching rules; and what to do when\nmultiple matching cookies are available.  On a larger level,\ninteroperability between the older cookie format and the newer format\nmay force the creation of a version number for the cookie headers,\nthough every effort has been made to make the two very similar in\nusage.\n\nThe working group came to the consensus that cookies were an optional\npart of the HTTP specification but that those who created cookies\nwithin the HTTP context should do so according to this document's\nspecifications.  The group also appeared to agree that this would be a\nseparate document which advanced with the main HTTP 1.1 document (or\ndocument set), rather than being folded into the base document.\n\nThursday session:\n\nHTTP MIB:\n\nA separate BOF considering a MIB for HTTP servers met at IETF this\nweek. Carl Kalbfleisch reported on the meeting of HTTPMIB BOF.  That\ngroup is interested in extending the work of the MADMAN group to\ncreate a standard way to monitor the health of a web server.  Two\ndraft mibs are already available.  Further information is available at\nhttp://http-mib.onramp.net/ ; to join the mailing list, send mail to\nhttp-mib-request@onramp.net with \"subscribe http-mib Your Name\" in the\nbody of the message.\n\nHOST Redux:\n\nJohn Klensin raised issues about the deployment of the Host: header\nfield under constraints that appeared in the HTTP/1.1 draft.\nHe presented an argument for changing the syntax of the base\nhttp methods so that all use fully qualified domain names and full\nURLs.  This would provide a more elegant solution to the \"vanity\ndomain\" problem which is currently consuming IP addresses than the\nproposed HOST command, which he believes might easily be\nmisimplemented.  The sesne of the meeting was that, while this was\nan elegant solution, it would break a large number of existing servers\nand the desire for interoperability with older implementations would\nforce providers to keep associating vanity domains with unique IP\naddresses.  As an implementation plan, the group will tighten the\nwording on HOST to make it clear that it is a MUST, and plan to make\nthe shift after widespread deployment of HTTP 1.1.\n\nAPPS/TSV:\n\nOn Monday, there'd been an APP/TSV open meeting on the impact of the\nWeb on the Net. Jim Gettys reviewed his presentation.  The basic\nproblem is a \"tragedy of success\".  There are so many users of the web\nthat congestion is becoming endemic on many links, especially those\nwhich are transatlantic or transpacific.  The routing table caches are\nalso being rendered useless by the relatively short packet trains of\nhttp.  Possible long term solutions include multiplexing connections,\na multicast transport layer, and extensive caching networks.\nProjections are for an eventual growth in the web of four orders of\nmagnitude, so we need to address this problem as quickly as possible.\nPersistent connections should help to some degree, but there will\nremain potential fairness problems in allocating bandwidth.\n\nWeb Transaction Security:\n\nLarry Masinter reported on the results of WTS working group meeting.\nThe WTS working group will clarify the relationship of SHTTP and HTTP\nin their current draft, and then it will likely move forward as a\nproposed standard.  This will put some constraints on what the HTTP\n1.1 documents can say about security.  As working group chair of HTTP,\nLarry feels that the work on security relatives to HTTP is taking\nplace in WTS and encourages those interested in that area to work with\nboth groups.  Digest Authentication is a special case which will be\nfinished in this working group; it should, subject to problems with\nthe wording, be in HTTP 1.1.\n\nPersistent connections:\n\nAlex Hopmann presented the results of the persistent connection\nsubgroup.  The persistent connection subgroup took as its goals proxy\nsupport, simplicity, 1.0 compatibility, and fast deployment.  The\nsubgroup has come to consensus on the use of Connection: persist as\nheader, along with Persist:<server-name> (Keepalive: must also be sent\nto maintain 1.0 compatibility).  These headers must be deleted by\nproxies; they apply only for one hop.  If the next hop server replies\nwith the same headers, it will then keep open the connection after\nsending its response.  The client may pipeline requests and the server\nmay pipeline responses.  The server must, however, reply to requests\nin the order they were received.\n\nDiscussion in the working group of marking entity boundaries came to\nthe consensus that chunked transfer encoding would be required but not\nwhether support for multi-part mime would be optional, required, or\nomitted.  Further discussion on the list will be needed to clarify\nsupport level.\n\nRange retrieval:\n\nAri Loutonen presented the changes to the Range-retrieval proposal;\nthey are very few--only the if-valid and if-invalid sections have\nchanged substantially.  Discussion of how range retrieval interacted\nwith caching concentrated on reducing the number of headers, with the\nconclusion that logic bags would not actually reduce the number of\npossible choices needed for completeness, even if those choices were\nsqueezed into a smaller number of headers.\n\nExtensions:\n\nRohit Khare reviewed the status of PEP, the Protocol Extension\nProtocol.  He believes that the syntax for PEP is currently complete\nand ready for review by this group, but he does not believe that it is\na critical path item for HTTP 1.1.  The principal changes in PEP are\nthe addition of scope and the deprecation of a central registration of\nprotocols.  PEP does require a minor version upgrade in HTTP, but only\na minor version.  Discussion presented a number of issues related to\nthe association of related extension protocols (or version of\nextension protocols); no resolution was reached, however, and they\nwill be continued in conversation with the authors.\n\nDemographics:\n\nBrian Behlendorf gave an introduction to the demographics work\ncurrently going on in the W3C; this work would set up a common logging\nformat for proxies and develop methods for servers to interact with\nproxies to retrieve information on transactions they have handled.\nThis system sees the proxies as acting on behalf of the server; John\nKlensin challenged this trust model by noting that historically\nproxies have acted on behalf of the user.  Legal issues were also\nraised about both privacy and legality of storing cached material in\nthe absence of an agreement; after polling the group, Larry Masinter\ndeclared demographics not on the critical path for 1.1, but within the\nscope of the group.\n\nLogistics:\n\nIn a discussion on logistics led by Jim Gettys, the group established\na timeline which would lead to a draft being sent to the IESG on May\n1st.  According to this timeline, a small group will conduct an\nimmediate triage on issues which can, might, or cannot be resolved in\ntime for a May 1st draft.  This will be reviewed by the group and sent\nto the ADs; a feature list and a structure for the set of documents\nwill be presented by March 18th.  A draft will be ready for working\ngroup review by April 1st.  Jim intends for the group to follow the\nissues list very closely, and asks that clarifying discussions be\nconducted off-list and then reported to the list.  Concrete wording\nfor suggested changes will be essential for this timeframe, and he\nencourages us to agree to defer quickly what cannot be resolved, so\nthat we can accomplish what is needed immediately.\n\nThursday afternoon:\n\nA smaller group met on Thursday afternoon to continue the discussion\nof logistics. The group reviewed a calendar and worked backward:\n\nWe accepted that it was important to influence the next release of\nHTTP products from major vendors; we were given information that\nhaving a proposed standard by May 15 is important. No product can\nclaim to 'comply' with the draft until the draft has been accepted by\nIESG as an action item by the IETF secretariat.  Giving allowance for\nthe web conference, and the possibility that the first draft might\nbounce, it seems important to submit the HTTP/1.1 draft to IESG by May\n1. Working backward from that, this meant having a new draft ready by\nApril 1. The calendar we came up with was:\n\nMarch 13 - Review issues\nMarch 15 - Feature list & structure of the document(s) set\nApril 1 - Updated draft to W.G.\nApril 2 - Start new issue list specifically on updated draft\nApril 24- W.G. last call\nMay 1- IESG submission of 1.1 draft.\n\nThe group then reviewed the issues list:\n\nhttp://www.w3.org/pub/WWW/Protocols/HTTP/Issues/http-wg.html\n\nand assigned owners to each item to shepherd the issue through and\npropose revised text.\n\n\n\n"
        },
        {
            "subject": "revised HTTP working group charte",
            "content": "I was asked to assert that the working group charter was up to date.\nSince we've changed the schedule somewhat, I've prepared the following\nproposed revised charter, which I hope will be accepted without\nobjections. (Current official charter\nhttp://www.ietf.cnri.reston.va.us/html.charters/http-charter.html).\n\n\n================================================================\nHYPERTEXT TRANSFER PROTOCOL (HTTP) CHARTER\n\nCHAIR(S)\n    o Larry Masinter <masinter@parc.xerox.com>\n    o Dave Raggett <dsr@w3.org>\n\nAPPLICATIONS AREA DIRECTOR(S):\n    o Keith Moore <moore@cs.utk.edu>\n    o Harald Alvestrand <Harald.T.Alvestrand@uninett.no>\n\nAREA ADVISOR\n    o John Klensin <Klensin@mci.net>\n\nMAILING LIST INFORMATION\n    o General Discussion:http-wg@cuckoo.hpl.hp.com\n    o To Subscribe: http-wg-request@cuckoo.hpl.hp.com\n        * In Body: subscribe http-wg Your Full Name\n    o Archive: http://www.ics.uci.edu/pub/ietf/http/hypermail\n       ftp://www.ics.uci.edu/pub/ietf/http/hypermail.tar.Z\n\nDESCRIPTION OF WORKING GROUP\n\nNote: This working group is jointly chartered by the Applications Area\nand the Transport Services Area.\n\nThe HTTP Working Group will work on the specification of the Hypertext\nTransfer Protocol (HTTP). HTTP is a data access protocol currently run\nover TCP and is the basis of the World Wide Web. The initial work has\nbeen to document existing practice and short-term extensions. Current\nwork is to clarify, extend and revise the protocol. Directions which\nhave already been mentioned include: improved efficiency, extended\noperations, extended negotiation, richer metainformation, and ties\nwith security protocols.\n\nNote: the HTTP working group will not address HTTP security extensions as these are\nexpected to be the topic of another working group.\n\nBackground information\n\nThe initial specification of the HTTP protocol was kept in hypertext\nform and a snapshot circulated as an Internet draft between 11/93 and\n5/94. A revision of the specification by Berners-Lee, Fielding and\nFrystyk Nielsen has been circulated as an Internet draft between 11/94\nand 5/95. An overview of the state of the specifications and a\nrepository of pointers to HTTP resources may be found at\n\nhttp://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nA description of HTTP/1.0 as it is generally practiced currently on\nthe Internet has been submitted to become an Informational RFC.  The\nworking group is considering enhancements/restrictions to the current\npractice in order to form a specification of the HTTP protocol\nsuitable for eventual consideration as a proposed standard.\n\nGOALS AND MILESTONES\n\n   Done\n      Submit HTTP/1.0 specification for publication as an Informational RFC.\n\n   Done\n      Complete review of HTTP/1.1 proposal and pending I-Ds by\n      subgroups: Persistent connections; cache-control and proxy\n      behavior; content negotiation; authentication;state\n      management;range retrievals; extension mechanisms; other new\n      methods and header features.\n\n   Apr 96\n      Revised HTTP/1.1 draft (by editing team led by Jim Gettys <jg@w3.org>).\n\n   May 96\n      Submit HTTP/1.1 as Proposed Standard\n\n   Jun 96\n      Review additional features for HTTP/1.2\n\n   Oct 96\n      Submit HTTP/1.2 as Proposed Standard\n\nCURRENT INTERNET-DRAFTS\n\n    o Hypertext Transfer Protocol -- HTTP/1.0 (126509 bytes). (There is also a\n      PostScript version [233107 bytes].)\n    o A Proposed Extension to HTTP : Digest Access Authentication (14989 bytes)\n    o Byte Range Retrieval Extension to HTTP (23286 bytes)\n    o Persistent HTTP Connections (17300 bytes)\n    o Hypertext Transfer Protocol -- HTTP/1.1 (227455 bytes)\n\n*** The following should be listed as related drafts:\n\n   14401 Feb 25 22:51 draft-hallam-http-logfile-00.txt\n\n   11054 Feb 25 22:50 draft-hallam-http-proxy-note-00.txt\n\n   25690 Feb 25 22:51 draft-hallam-http-session-id-00.txt\n\n   54102 Feb 23 23:13 draft-holtman-http-negotiation-00.txt\n\n   17300 Feb 21 23:19 draft-ietf-http-ses-ext-01.txt\n\n  136425 Mar 13 03:40 draft-khare-http-pep-01.ps\n   57385 Mar 13 03:34 draft-khare-http-pep-01.txt\n\n   31350 Sep 25 19:15 draft-kristol-http-state-info-01.ps\n   16168 Sep 25 19:15 draft-kristol-http-state-info-01.txt\n\n   92668 Feb 20 02:01 draft-mogul-http-caching-00.txt\n\nNO REQUEST FOR COMMENTS\n\n\n\n"
        },
        {
            "subject": "RE: proxies rewriting URL",
            "content": "On Mon, 11 Mar 1996, Paul Leach wrote:\n> \n> Interesting.  What happens if I  do this:\n> GET /secret.txt HTTP/1.1\n> Authorization:  uri=\"/public.txt\",\n>  username=\"fred\", realm=\"www.foo.com\",\n>  nonce=\"deadbeef\", response=\"0123456789abcdef0123456789abcdef\"\n> \n> If the server checks the authorization header and its URI, but then \n> uses the URI from the Request-URI in the request line, the whole \n> exercise will have been wasted.\n> And if proxies are allowed to munge the URI in unknown ways, the server \n> can't compare the request-URI with the uri in the Authorization header.\n> \n> The Digest draft should say the the server MUST use the URI from the \n> Authorization header, as that is the only one that has been authenticated.\n> \n\nThe latest draft I sent you addresses this as follows:\n\n\"The authenticating server must\n   assure that the document designated by the \"uri\" field is the\n   the same as the document served.  The purpose of duplicating\n   information from the request URL in this field is to deal with\n   the possibility that an intermediate proxy may alter the client's\n   request.  This altered (but presumably semantically equivalent)\n   request would not result in the same digest as that calculated\n   by the client.\"\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "entity header fiel",
            "content": "The TITLE entity header field, like the LINK field have an explicit correlation \nwith the same HTML tag; but MUST a server process an HTML file to extract these\nvalues or a Server can ignore them even if it receives an explicit request of\nthese fields ????\n\nDavide \n\n-----------------------------------------------------------------------------\nDavide Musella\nInstitute for Multimedia Technologies, National Research Council, Milan, ITALY\ntel. +39.(0)2.70643271\ne-mail: davide@jargo.itim.mi.cnr.it     http://jargo.itim.mi.cnr.it/\n\n\n\n"
        },
        {
            "subject": "Re: entity header fiel",
            "content": ">The TITLE entity header field, like the LINK field have an explicit correlation \n>with the same HTML tag; but MUST a server process an HTML file to extract these\n>values or a Server can ignore them even if it receives an explicit request of\n>these fields ????\n\nI think the title field is poorly motivated wrt a HTML document. I see little \nadvantage in getting the Title of the document up a few packets earlier. There \nis an issue with the HEAD method however, if one does a head then one wishes to \nobtain meta-information relating to the entity without obtainig a body and hence \nTitle does have some use. \n\nTitle and link are rather more usefull when dealing with content types which do \nnot provide for header or link attributes. It is usefull to add a title to a \nJPEG or GIF, it would have been nice if Link could be used to add links into \nsuch objects - ie client side image maps.\n\nOverall I'm not too keen on the link tag/header because I don't think that \nanyone has proposed a comprehensive and coherent semanitcs for them.\n\nMy view of what link should be is annotations in the Hyper-G model. Ie they \nshould be used to inform a client about links which relate to an entity which \nare not a part of it. Annotations are one example.\n\nThis model would indicate a need to incorporate a position indicator into the \ntag:\n\nLink: <http://www.cern.ch/TheBook/chapter2>; rel=\"Previous\";\n    pos=1654-1754\n\nWhere pos in this case is a simple character offset. I suspect that we really \nneed a comprehensive notation for describing positions within a hypertext \nstructure.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: entity header fiel",
            "content": "> I think the title field is poorly motivated wrt a HTML document. I see little \n> advantage in getting the Title of the document up a few packets earlier. There \n> is an issue with the HEAD method however, if one does a head then one wishes to \n> obtain meta-information relating to the entity without obtainig a body and hence \n> Title does have some use.\nyes, I was thinking about it. The problem is how a server can extract this meta info??\ndoing a full text parsing? isn't it too heavy??\nA robot could have many advantages using only HEAD request to retrieve all the meta\ninfo related to an html file...but which server handle this kind of info??\nWhy don't insert among the entity header fields the content of the META HTTP-EQUIV\ntag???\n  \n\n> Overall I'm not too keen on the link tag/header because I don't think that \n> anyone has proposed a comprehensive and coherent semanitcs for them.\na proposal is in the draft-ietf-html-relrev-00.txt.\n\nDavide\n\n\n\n"
        },
        {
            "subject": "ISSUES LIST: Chunked Encodin",
            "content": "We seem to have two questions related to Chunked encoding, and I\nwonder whether answering one of them may make it easier to answer the\nother.\n\nSpecifically, we have an open question on whether we will support both\nchunked encoding and multi-part MIME for persistent connections.  We\nalso have an open question about whether and how to fix the current\nspecification of chunked encoding so that it allows the kinds of\nfooter information the Phillip Hallam-Baker needs.\n\nIf we agree we will support both chunked encoding (as a MUST) and\nmulti-part MIME (perhaps as a SHOULD), can we leave messages which\nrequire footers to multi-part MIME?  If this is possible, it seems\nthat we could retain some of the ease of implementation that the Area\nDirectors indicated chunking would buy us, and help us get persistent\nconnections deployed more quickly\n\n\n\nTed\n\n\n\n"
        },
        {
            "subject": "Re: ISSUES LIST: Chunked Encodin",
            "content": ">Specifically, we have an open question on whether we will support both\n>chunked encoding and multi-part MIME for persistent connections.  We\n>also have an open question about whether and how to fix the current\n>specification of chunked encoding so that it allows the kinds of\n>footer information the Phillip Hallam-Baker needs.\n\nNo, this is exactly the opposite of what I am saying. Chunked as specified\nis OK, it includes footers and there is a minor tweak to the grammar\nwhich I beleive should be made which would hold open interesting developments in \n1.2.\n\nThe problem is not how to \"fix\" chunked. The problem is how to keep people from \ntaking the footers out. The message I put out 8 weeks ago was a result of \nmisreading the draft. Roy cleared this up but it does not appear that everyone \nnoted this.\n\n\n>If we agree we will support both chunked encoding (as a MUST) and\n>multi-part MIME (perhaps as a SHOULD), can we leave messages which\n>require footers to multi-part MIME?  If this is possible, it seems\n>that we could retain some of the ease of implementation that the Area\n>Directors indicated chunking would buy us, and help us get persistent\n>connections deployed more quickly\n\nNo, multiparts are intollerable over an 8-bit clean connection. They are not\nacceptable as the sole means of incorporating footers.\n\n\nI suggest we:\n\n1) Leave chunked in as a MUST *including* *footers* (as is currently the case).\n\n2) Make the technical change to the chunk a grammar to allow chunk attributes to \nbe incorporated at a later date.\n\n3) Relegate MIME multipart support to SHOULD.\n[If indeed coding time is an issue].\n\nI am not proposing an onerous quantity of code. It is simply requiring 1.1 \nparsers to tolerate possible 1.2 extensions. It has no impact on the normal flow \nof control, it is merely a question of what is considered an error.\n\nI really do not think there is any major problem here. I think that it is simply \na case of miscommunication which is easy enough via e-mail. \n\nIt appears that the louder I tell people that removing footers from chunked \nwould be a very stupid mistake the more people suggest this course of action as \na means of obtaining consensus!\n\nIf people want consensus and want chunked in 1.1 then can we please have a \ndiscussion that addresses the *technical* issues. \n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: ISSUES LIST: Chunked Encodin",
            "content": "Phillip Halam-Baker writes: \n> \n> Because they introduce an unnecessary osurce of error.\n> \n> The assumption that one can form boundary strings with negligible\n> probability of collision is unfortunately false. I have a system which\n> provides a hyperterminal interface using HTML and HTTP. If multiparts\n> were put into the HTTP spec that system could not be introspective,\n> if the terminal ever relayed any of its own output it would fail.\n> \n> The Web as knowledge system has to be introspective. That is not\n> a negotiable position. If the IETF wants to break our protocols because\n> they don't want to understand what the project is about then the IETF\n> will not be the venue for standardization.\n> \n> Mutliparts are very much less efficient than the chunked encoding. They\n> require every byte to be interpreted in serial mode. This is simply not \n> acceptable if we are dealing with a live video feed. In that case one \n> would want to use low level switching to direct content at a destination \n> without processor intervention.\n> \n> \n> Phill\n> \n\nPhill,\nNo one is interested in breaking anything; we are (all, I\nhope) interested in getting a system which interoperates as widely as\npossible, and which can be deployed in a reasonable manner.  I think\nyou would find most people would agree with you that using Multi-part\nMIME is less efficient than chunked encoding because of the need for\ninterpretation; that was, in any case, my reading at the meetings in\nL.A.  My question is simply: for the situations where you need signed\nparts or other footer data, what can't be done using Multipart?\nLooking over what you've posted on this matter, I see\nsituations in which footer data is needed, and I see situations in\nwhich what you call the introspective nature of the Web makes the use\nof boundary strings problematic.  I don't, however, see any overlap\nbetween the two.  If there are overlapping cases, describing them\nwould help me a lot in understanding the problem.\nRemember, we are talking here about what to put in 1.1, not\nthe whole future of HTTP; what can we get done now that moves us forward?\n\nTed\n\n\n\n"
        },
        {
            "subject": "Re: ISSUES LIST: Chunked Encodin",
            "content": "My apologies; I forgot to cc the list on one of my questions to\nPhil, so that following exchange is probably a bit hard to follow.\nBasically, I asked why using Multiparts in an 8-bit clean environment\nwas not acceptable.  The following should be understood as a reply\nto that question, and my response.\nTed \n\n> \n> Phillip Halam-Baker writes: \n> > \n> > Because they introduce an unnecessary osurce of error.\n> > \n> > The assumption that one can form boundary strings with negligible\n> > probability of collision is unfortunately false. I have a system which\n> > provides a hyperterminal interface using HTML and HTTP. If multiparts\n> > were put into the HTTP spec that system could not be introspective,\n> > if the terminal ever relayed any of its own output it would fail.\n> > \n> > The Web as knowledge system has to be introspective. That is not\n> > a negotiable position. If the IETF wants to break our protocols because\n> > they don't want to understand what the project is about then the IETF\n> > will not be the venue for standardization.\n> > \n> > Mutliparts are very much less efficient than the chunked encoding. They\n> > require every byte to be interpreted in serial mode. This is simply not \n> > acceptable if we are dealing with a live video feed. In that case one \n> > would want to use low level switching to direct content at a destination \n> > without processor intervention.\n> > \n> > \n> > Phill\n> > \n> \n> Phill,\n> No one is interested in breaking anything; we are (all, I\n> hope) interested in getting a system which interoperates as widely as\n> possible, and which can be deployed in a reasonable manner.  I think\n> you would find most people would agree with you that using Multi-part\n> MIME is less efficient than chunked encoding because of the need for\n> interpretation; that was, in any case, my reading at the meetings in\n> L.A.  My question is simply: for the situations where you need signed\n> parts or other footer data, what can't be done using Multipart?\n> Looking over what you've posted on this matter, I see\n> situations in which footer data is needed, and I see situations in\n> which what you call the introspective nature of the Web makes the use\n> of boundary strings problematic.  I don't, however, see any overlap\n> between the two.  If there are overlapping cases, describing them\n> would help me a lot in understanding the problem.\n> Remember, we are talking here about what to put in 1.1, not\n> the whole future of HTTP; what can we get done now that moves us forward?\n> \n> Ted\n> \n> \n\n\n\n"
        },
        {
            "subject": "Re: ISSUES LIST: Chunked Encodin",
            "content": ">I don't, however, see any overlap\n>between the two.  If there are overlapping cases, describing them\n>would help me a lot in understanding the problem.\n\n1Signed live video feed.\n2Signed content generated on the fly\n3Authenticated messages\n\nI have *code* which *runs* which covers item 2.\n\n\n>Remember, we are talking here about what to put in 1.1, not\n>the whole future of HTTP; what can we get done now that moves us forward?\n\nRemember you are suggesting taking the current 1.1 spec and changing it.\n\nNobody has provided any reason why this should either:-\n\n1) Be any simpler to implement.\n\n2) Allow aggrement to be reached in a shorter time frame.\n\nIf people don't want to have a large burden writing code for 1.1 then what is \nwrong with simply specifying a footer but defining no valid entity-footers for \nuse in it except for extension-tag?\n\nWhat is the reason for introducing a broken version of chunked in the 1.1 spec \nsimply so that we can fix it in 1.2? That will involve a lot more code and it \nwill mean that 1.1 compliant caches and gateways will not know how to interact \nwith 1.2 messages.\n\n\nWhy can't people simply agree with *me* in order to move the spec forward?\n\nI generally think that the best specs are those which anticipate the next \nrevision. That is all I am saying we should do. \n\nI'm willing to hear a technical argument. Simply asking me to go along with\na *CHANGE* to the spec for the *WORSE* makes absolutely no *SENSE* to me.\n\n\nTed, please at the very least acknowledge that you are the one who is proposing \na change.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: ISSUES LIST: Chunked Encodin",
            "content": "Ted Wrote:\n> > \n> > Phillip Halam-Baker writes: \n> > > \n> > > Because they introduce an unnecessary osurce of error.\n> > > \n> > > The assumption that one can form boundary strings with negligible\n> > > probability of collision is unfortunately false. I have a system which\n> > > provides a hyperterminal interface using HTML and HTTP. If multiparts\n> > > were put into the HTTP spec that system could not be introspective,\n> > > if the terminal ever relayed any of its own output it would fail.\n> > > \n> > > The Web as knowledge system has to be introspective. That is not\n> > > a negotiable position. If the IETF wants to break our protocols because\n> > > they don't want to understand what the project is about then the IETF\n> > > will not be the venue for standardization.\n\nAlthoug I quite agree with that the parsing of multipart mime is expensive, I'd\ndo have a factual problem with the statement that 'the probability of collision\nis unfortunately false'. \n\nIt should be understood that the boundary string does not need to be random,\nnor does it need to be unpredictable; in fact making it random and/or unpredictable\nincreases chance of collision. Where the text 'whole unpredictable' a completely\nfixed string whould be the best boundary string; that is any fixed string of \ngiven lenght. (Sorry for the crappy translations; but I lack the english \nvocabulary to get the mathematics across)\n\nHowever, as you assert rightly, the message conveyed is not unpredictable. Now proof\ncan be given that a multi variant string which is a function of the text it signs and\na function of history has almost the same chance of collision as a *fixed* string \nwith a random text. \n\nIn practive, some cheap Lenght, MD5 or CRC of the text signed combined with the \nresults of a function which is a function of the previous resoult (say a counter, \ntimer, etc) nears the optimum. You might want to consult literature on Number \ntheory and most books on encryption might help you out as well.\n\n> > What can be done to move us forward.\n\nI would very much prefer to have *both* chunked and multipart.  As this allow the \nserver builders to choose and to optimize; it is easy to envision senarios where\neither of the two would excell, or where the difference between server resources\nand programming resources are large.\n\nDw.\n\n\n\n"
        },
        {
            "subject": "RE: ISSUES LIST: Chunked Encodin",
            "content": ">Why can't people simply agree with *me* in order to move the spec\n>forward?\n\nI agree with Phil. There seems to be this belief that removing footers\nwill \nmake implementation of chunked easier and cause 1.1 to converge faster. \nMaybe I'm not too bright, but I don't see that. What exactly is the\ntechnical \nproblem with footers?\n\nHenry\nhenrysa@microsoft.com\n>\n\n\n\n"
        },
        {
            "subject": "unnecessary chatter on closed issues (multipart vs chunked",
            "content": "Dear working group members:\n\nCan you please try to keep others from reopening topics that we've\nalready discussed and closed? I'm sure there are many topics besides\nmultipart vs. chuncked that we *haven't* already discussed and beaten\nto death.\n\nIf someone raises something and you think the issue is closed or was\nalready addressed, please reply privately.\n\nPhill: chill. I have no notes anywhere that anyone is going to do\nanything to break 'chunked in 1.1'. I answered you privately that this\ndidn't appear in the minutes, wasn't a decision that I believe is\ncredible. Why you continue to flame about this baffles me. Stop it.\n\nI suppose it's just my selfish desire not to come back to a mailbox\nfull of topic repeats.\n\nAs far as I'm concerned, we are now beyond the realm of 'discussion of\nissues'. Jim Gettys is now preparing a revised 1.1 draft based on the\ndiscussion held so far, with some help from designated assistants on\nspecific sections. The WG mailing list is open for those who are doing\ndrafting of various subjects to solicit advice on wording or\nclarification of positions if they want, but it's out of scope at the\nmoment to raise new issues.\n\nWhen there's a new 1.1 draft, there'll be time to critique it.\n\n\n\n"
        },
        {
            "subject": "New section for Hos",
            "content": "As discussed during the LA meeting, we need a stronger definition\nfor the Host header field in 1.1.  The following is what I propose\nto be the new definition.  Unfortunately, I will be off-net at a\nconference until Thursday, but this should be carefully reviewed\nright now.\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n=====================================================================\n\n10.22  Host\n\n   The Host request-header field specifies the Internet host and port\n   number of the resource being requested, as obtained from the original\n   http URL (Section 3.2.2) given by the user or referring resource.\n   The Host field value must represent the network location of the\n   origin server or gateway given by the original http URL.  This allows\n   the origin server or gateway to differentiate between\n   internally-ambiguous URLs (such as the root \"/\" URL of a server\n   harboring multiple virtual hostnames).\n\n       Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n\n   A \"host\" without any trailing port information is equivalent to\n   a value of \"host:80\".  For example, a request on the origin server\n   for <http://www.w3.org/pub/WWW/> must include:\n\n       GET /pub/WWW/ HTTP/1.1\n       Host: www.w3.org\n\n   The Host header field must be included in all HTTP/1.1 request\n   messages on the Internet (i.e., on any message corresponding to a\n   request for an http URL, as described in Section 3.2.2).  If the\n   Host field is not already present, an HTTP/1.1 proxy must add a Host\n   field to the request message prior to forwarding it on the Internet.\n   If an HTTP/1.1 request message lacking a Host header field is\n   received via the Internet by an origin server or gateway, that server\n   must respond with a 400 status code.\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "I think people are getting quite a bit carried away with all this\nspeculation about keep-alive versus MGET.  A couple salient points:\n\n1) The Connection header (or something with identical semantics)\n   is necessary regardless of whether or not it is used to support\n   a keep-alive. There is simply no other way to communicate in-band options\n   to your nearest-neighbor in a world of multiple hierarchical proxies,\n   without requiring that your nearest-neighbor always be a proxy.\n\n2) Just because the Connection header exists doesn't mean a server has\n   to support it or do anything with it aside from NOT forwarding it.\n\n3) The SESSION method (or something with identical semantics) is necessary\n   regardless of whether or not it is used to support a multi-request\n   connection.  There is simply no other way to send in-band commands to your\n   nearest neighbor without risk of that action being forwarded downstream\n   (where it would be misinterpreted as coming from the downstream's\n   neighbor).\n\n4) Just because the SESSION method exists doesn't mean a server has\n   to support it or do anything with it aside from NOT forwarding it.\n\nThe above is what is covered for HTTP/1.0.  For 1.1, we can consider\nwhat can be done with this stuff once it is available.  Starting from the\nkeep-alive example I described, here are some points which do not seem\nto have been made clear:\n\n5) The decision on whether or not to allow a kept-alive connection,\n   how long the connection should stay open, and the maximum number\n   of requests that are accepted, IS ENTIRELY UP TO THE SERVER!\n   Whether or not it is in the 1.1 spec has no effect on any server\n   software which doesn't wish to implement it, nor does it prevent\n   methods like MGET from also being defined and used.\n\n6) Because the server's decision regarding keep-alive is, if accepted,\n   visible in the returned response's headers, the client does not have\n   to wait until the end of the connection in order to find out what\n   the server will do.\n\n7) The decision on whether or not keep-alive and/or SESSION and/or\n   MGET/MPUT/MPOST/MWHIP/... *should* be implemented will be made\n   based on actual measurements of test implementations, some indication\n   of how difficult they are to implement, and an explicit test of\n   that warm-fuzzy-feeling that engineers get when they think they've\n   found the right solution.\n\n8) There is no such thing as a bloody MGET proposal, because no one\n   has ever attempted to specify it beyond some vague notion, let alone\n   actually implement the sucker.  I don't want to hear any more arguments\n   about how much *more* efficient this method is until someone gets up\n   the gumption to specify exactly what the beastie will entail. \n   Volunteers will be gratefully accepted!!!!\n\n9) If one or more of the above methods are implemented and can achieve\n   a user perceived performance improvement to within 20% of the visible\n   performance of multiple simultaneous requests for a page with four\n   inlined images on a congested line, I will personally block out all\n   requests from my site (and will recommend the same for all sites)\n   of any browser which continues to use multiple independent connections.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "> So under the current scheme, with 1.0 software in the request chain\n> undetectable, even if 95% of the software is 1.1, the service author\n> _will still have to guess_ which protocol elements can be safely used\n> in a response, as the response may go to some ancient 1.0 client.\n\nNo.  All 1.1 elements which are not specific to the nearest-neighbor\nin the communication chain are already safe to be used in a response\nto any 1.0 client.  We cannot introduce unsafe elements to the\nprotocol until we are willing to change the version to 2.0, which,\nas I said before, cannot be done until we have deployed the\nimmediate infrastructure present in 1.1.\n\n> The current scheme uses the version numbers in requests and responses\n> to indicate capabilities of the immediate connections in the chain.\n> Yet, AFAIK, all the proposed 1.1 mechanisms that act on immediate\n> connections, like persistent connections, have their own headers to\n> signal capabilities, so they won't need the version numbers in the\n> first lines of the requests and responses.\n\nWe cannot send a 1xx Informational message to a 1.0 client and there is\nnothing other than the HTTP-version number to indicate that.  In any\ncase, the version rules are meant to be generic and not restricted to\nthe scope of changes currently on the table for 1.1.\n\n> I propose use of the version number in the request-line to indicate\n> the _minimal_ protocol version used in the request chain.\n\nNo, that is not acceptable to me.  It prevents forward progress and\nefficient introduction of protocol-upgrading proxies solely for the\npurpose of introducing incompatible changes to the protocol without changing\nthe major version number.\n\n\n ...Roy T. Fielding\n    Department of Information & Computer Science    (fielding@ics.uci.edu)\n    University of California, Irvine, CA 92717-3425    fax:+1(714)824-4056\n    http://www.ics.uci.edu/~fielding/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "Roy T. Fielding:\n>[Koen Holtman:]\n>> So under the current scheme, with 1.0 software in the request chain\n>> undetectable, even if 95% of the software is 1.1, the service author\n>> _will still have to guess_ which protocol elements can be safely used\n>> in a response, as the response may go to some ancient 1.0 client.\n>\n>No.  All 1.1 elements which are not specific to the nearest-neighbor\n>in the communication chain are already safe to be used in a response\n>to any 1.0 client.\n\nNo.  Numerous 1.1 elements and 1.0 elements which are not specific to\nthe nearest-neighbor in the communication chain are _unsafe_ when used\nin a request chain containing a 1.0 client. (I define `unsafe' as\n`you do not know what will happen if you use it'.)\n\nHTTP/1.0 will be an informational standard, and this means that, in\ntheory, you cannot safely use anything in a response to a HTTP/1.0\nclient.  In practice, only a number of things cannot be safely used,\nfor example\n\n - 301 and 302 responses to a POST request\n - the Expires header (many 1.0 clients will ignore it)\n - 200 responses to a POST request (some 1.0 clients will cache them)\n\nare unsafe with the collection of 1.0 clients in operation now.\nFuture 1.0 clients may add new unsafe elements without violating any\nstandard.\n\n>We cannot introduce unsafe elements to the\n>protocol until we are willing to change the version to 2.0,\n\nUnsafe elements were introduced when 1.0 was made informational.\nThere is nothing you can do about that.  In particular, it won't help\nat all if you lecture me about how bad it is to propose the\nintroduction of unsafe elements.  I do not propose such things, I\nmerely assert that we already have them.  I am painfully aware of the\nconsequences of unsafety: I have spent two months in 1994\ninvestigating unsafety of 1.0 protocol elements, and this was when we\nhad only 4 major browsers.\n\nI want a protocol in which the presence of 1.0 clients in the request\nchain is detectable, so that, in 1998, it won't be necessary anymore\nto spend two months investigating unsafety of 1.0 protocol elements,\njust because there is no reliable way to avoid serving the 3% of users\nstill using broken 1.0 software.\n\n[...]\n>> I propose use of the version number in the request-line to indicate\n>> the _minimal_ protocol version used in the request chain.\n>\n>No, that is not acceptable to me.  It prevents forward progress and\n>efficient introduction of protocol-upgrading proxies\n\nI believe you mean to say \"it breaks Upgrade:\".  I just went over the\nspec, and you seem to be right on this.\n\nOK, I understand your objections to changing the rules for the version\nnumber in the request line.  I hope you understand my reasons for\nwanting to detect 1.0 clients.  So here is a new proposal:\n\nWe add the following requirement for proxies:\n\nIf a proxy upgrades a request message from HTTP/1.0 to HTTP.1.1, it\nmust include a request header\n\n Upgraded-From: HTTP/1.0\n\nin the upgraded request message.  Such an Upgraded-From header could\nalso be useful if a request message is upgraded through the Upgrade\nheader mechanism.  Thus I get to the following text for the 1.1 draft:\n\n 10.x Upgraded-From\n\n The Upgraded-From request header must be used by a proxy upgrading a\n request message to a different protocol version to record the\n original protocol version of the request message.  The information in\n this header can be used by servers when generating response message\n status codes or headers that must be understood by all clients in the\n response chain.\n\n   Upgraded-From: 1#product\n\n If a proxy upgrades a request message that already contains an\n Upgraded-From field, it must add the protocol version upgraded from\n to the end of the Upgraded-From field.  For example, a request\n message upgraded from HTTP/1.0 to HTTP/1.1 by a proxy earlier in the\n request chain, and upgraded to HTTP/1.2 by the current proxy, must\n contain the request header\n\n   Upgraded-From: HTTP/1.0, HTTP/1.1\n\nWith this header, my detectability requirements are met.  Roy, is this\nheader acceptable to you?\n\n> ...Roy T. Fielding\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version numbe",
            "content": "I propose a simpler rule:\n\n> You may use \"HTTP/1.1\" as your protocol version only if you are\n> willing to guarantee that all HTTP/1.1 headers used will mean what\n> HTTP/1.1 says they mean, and all HTTP/1.1 directives will be\n> followed.\n\nIn practice, this will mean that simple 1.1 proxies that don't want to\nrewrite headers will use the \"minimum\" algorithm: if the client is\n1.0, they'll pass on 1.0.  More complex proxies might want to upgrade\neven 1.0 clients to 1.1 protocol on the way out, but at an additional\ncost, e.g., to upgrade a 1.0 client to a 1.1 call to a 1.1 origin\nserver, a proxy might have to introduce \"cache-busting\" techniques to\nguarantee that 1.1 cache directives are followed, or deal\ndirectly with 301 or 302 results from a POST, etc.\n\n\n\n"
        },
        {
            "subject": "draft-ietf-http-logfile00.txt and multiple directive",
            "content": "In working on converting aws to this format, I've encountered an\ninteresting problem. To wit:\n\n   The log file format described permits customized logfiles to be\n   recorded in a format readable by generic analysis tools. A header\n   specifying the data types recorded is written out at the start of\n   each log. \n\nand\n\n   The directives Version and Fields are required and should preceed\n   all entries in the log. The Fields directive specifies the data\n   recorded in the fields of each entry. \n\nOk - every log file should start with a Fields header. No problem.\nHowever, the model for log files that I'm used to is that\nstopping/restarting the server doesn't change the log file in use -\nthe restarted server starts appending data to the log file.\n\nThe problem appears with servers that can change the format of the log\nfile data. What happens if they format has changed from the old one?\nI can see three options:\n\n1) The server closes the old log file and starts a new one. I dislike\nthe idea of forcing a log file rename on a user.\n\n2) The server edits the old log file entries to bring them in line\nwith the new format. This is expensive at best, and impossible\nat worst.\n\n3) The server spits out a new Fields line, and software that's reading\nit analyzing the log file is expected to deal with it.\n\nI believe #3 is the correct answer.  However, I can't find anything in\nthe draft mentioning that this these fields can appear anywhere.\nAdding such wording seems appropriate. I'd recommend something like:\n\nExcept for the Start-Date and End-Date directives, more than\noccurrence of any directive may appear in the log file. They\napply to any entries that follow them in the log file until the\nnext occurrence of that directive. Analysis tools should be\nable to deal with a new, changed Field directive in the middle\nof the log file.\n\nThe exceptions may be wrong; allowing only one start-date & end-date\nseems logical for a single log file. Allowing multiple version\ndirectives has implications for backwards compatability that may not\nbe desirable.\n\nFinal note: count and comment claim that they have a type '<>', but\nthat type isn't defined in the table.\n\nThanx,\n<mike\n\n\n\n"
        },
        {
            "subject": "About that Host: header...",
            "content": "Hi,\ncould someone please elucidate this sentence from the minutes for me?\n\n>As an implementation plan, the group will tighten the\n>wording on HOST to make it clear that it is a MUST, and plan to make\n>the shift after widespread deployment of HTTP 1.1\n\nI can read this two ways:\n\n- The group wants to stick with HOST, but make it (more) mandatory\n- The group wants to change to full URL, but wants to do that in\n  HTTP 1.2 instead of HTTP 1.1\n\nThe first reading seems to me to be less than sound, architecturally.\nThe second reading seems to me to be based on the premise that it will\nbe easier to change the 200.000 HTTP 1.1 servers 1 year from now than\nthe current crop of 50.000 HTTP 1.0 servers. Somehow that doesn't\nstrike me as completely obvious either.\n\nIs there a third reading that I missed?\n\n                 Harald T. Alvestrand\n\n\n\n"
        },
        {
            "subject": "Re: About that Host: header...",
            "content": "I own this problem on the issues list.  I will clarify the issues\nlist, as it and the minutes do not capture John Klensin's presentation\nor the discussion that occurred at the IETF meeting.\n\nWe have four options, as I see it.\n\n1) (LeaveAlone) leave things as in HTTP 1.0.  No host header, incomplete URL's in\nthe protocol.\n\nAdvantages: No change at all\nDisadvantages: Still use one IP address/vanity name.\n\nI believe this solution is unacceptable to all concerned who understand\nthe problem at all.  The current situation is operationally critical at \nlarge web sites, and addresses need to be conserved in the Internet \nuntil V6 transition actually happens.  I believe the IESG would (correctly) \nreject any 1.1 specification that does not address this issue in\na believable fashion.\n\n\n2) (FullURL's) \na) Require full URL's in HTTP requests.\nb) Require 1.1 server to accept full URL from clients.\n\nAdvantages: simplicity (if we ignore the transition problems)\nProtocol not crufted up and complicated forever.\nSimplicity is next to godliness.\nDisadvantages: Interoperability problems of 1.1 clients with\nmany/most 1.0 servers during transition.  Solutions to \nthese problems involve one RTT, as a 1.1 client would have to \nhandle an error, and retry using 1.0 syntax. (i.e. protocol is \nkept simple, at expense of network traffic during transition,\nand complexity in clients during transition).  Extra\npackets are exchanged during transition to 1.1 to handle\nthis error condition.  Client implementations are\nmore complex, and performance poorer during transition.\nMight discourage transition to 1.1 (gives incentive to\nclients to lie about their version).\n\nThis is clearly what the protocol should have been from the beginning.  Sigh...\n\n3) (Host) Current 1.1 draft:\na) Add host header, with current (or improved wording) in the specification.\nb) Require 1.1 server to accept full URL from 1.1 or later client.\nTransition to requiring full URL in 1.2, after 1.0 servers have been\nreplaced.\nAdvantages: no interoperability problems.  No extra RTT during\ntransition, or extra packets on the wire during transition.\nDisadvantages: High potential for implementors to \"get it wrong\"\nand thereby defeat transition to multiple servers at single \nIP address.\nAnother piece of cruft and complexity is added to the\nprotocol, to be supported forever more.\n\nJohn Klensin's assertion, which I believe personally, is that the risk is too \nhigh of implementors not correctly implementing the specification even\nif the speciification is tightened up.\nSuch buggy implementations would not be detected anywhere, and deployment\nof such buggy applications would likely occur.\nUnless the problem is easily detected (and obvious) to broken client \nimplementations, it is likely that code will be deployed which is\nbroken.  All it takes is one major client or server implementation\nto get it wrong to impede transition another version cycle.\n\n\n4) (Host+Error) My alternate on 3, suggested at the IETF meeting:\n\na) Add host header, with improved wording in the specification.\nb) Require 1.1 server to accept full URL from 1.1 or later client.\n  (so far, same as option 3).\nc) Require server to generate an error if a 1.1 client is detected, and no host\ninformation present (or more strongly, at the expense of extra bytes on\nthe wire, no host header present). \nTransition to requiring full URL in 1.2, after 1.0 servers have been\nreplaced.\n\nAdvantages/disadvantages: same as option 3, but without the\ndisadvantage of buggy implementations going undetected.\nIf we succeed in getting even only one major server vendor \nto implement error reporting correctly, I believe this\nsolution will succeed in forcing transition.\nThis requirement can be written into RFP's easily.\nBut 1.X protocol is still crufty forever, and this\nsolution adds another piece of cruft, and a small bit\nof implementation effort on servers.\n\nOnce either 3 and 4 have been implemented, a transition to 2) could\noccur in a later HTTP version (presumably 1.2).\n\nDiscussion:\n\nI personally see only options 2 and 4 having a high likelyhood of the \n\"correct outcome\" allowing multiple sites to be served from the same server, \nwithin finite time of starting to transition to 1.1.  \nI believe the W.G. needs to select either 2 or 4 to resolve this issue. \n\nThere was a statement by Ari Luotonen at the IETF meeting that he believed that\nsolution 2) to be unacceptable to Netscape.  Ari, is this true now that\nyou've had time to think about it?  Paul, can you see what Microsoft's\nopinion on this topic is?\n\nWhat say you all?\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Proposed structure of HTTP 1.1 document(s)",
            "content": "By this date, you all should have reviewed the issues list and commented\nand/or raised additional issues. I will be working over the next several\ndays on the issues list to make it somewhat clearer and better organized.\n\nThe schedule calls for me to publish an editorial organization of 1.1 today,\nso here it is.\n\nJohn Klensin (no longer applications area director, but generally worth listening\nto), expressed the opinion that no more documents than necessary be processed \nby IESG.  If items are left to separate documents\nunnecessarily, it is less likely they will be implemented, and the result\nis more confusing to implemetors.  On the other hand, we cannot allow\nless essential issues to hold up getting the basic part of HTTP to draft\nstandard status.\n\nThere are two possible space travel-inspired models for handling the \nHTTP 1.1 documents.\nAllow for \"explosive bolts\", so that it is possible to\nexplode non-essential issues from the core\ndocument if controversy arises.\nAllow for \"soft docking\", so that controversial issues can\ndock with the core document if IETF \"rough consensus\"\nis reached in time.\n\nI propose we allow for both models.\n\nI expect we will end up with no more than 4 documents as part of 1.1.  I hope\nfor only two or three.  Ideally, we end up with only one.\n\nA group of us held a teleconference last Thursday (3/14) and reviewed the state\nof all parts of 1.1 and the issues list.  After some thought, I think the \nfollowing overall editorial organization makes the most sense.\n\nCore HTTP 1.1 document:\nsolution to the host problem.\nintegrate caching subgroup work into core\nThis appears essential, and pervasive, and so will have to\nbe integral to the core document.\npersistent connections. (as separate chapter, so that\nif there is some hold up, we can eject it).\nbyte range retrival extension (as separate chapter, so that\nif there is some hold up, we can eject it).\nresolution of outstanding issues in the 1.1 draft 1, per issues list.\n\nInitially separate documents (will be handled independently until/unless\nit is clear they are adopted in time for W.G. last call of core document):\n\n1) Digest Access Authentication\nWe believe a separate document here is the wisest course, as both\nthe HTTP and the WTS groups need to review this document before\nadoption.\n2) Proposed HTTP state management mechanism\nThis might be able to become part of the core 1.1 document as\na chapter.  The authors, however, are not available until around the\nbeginning of April, so unless/until it becomes clear that closure\ncan be reached in time for 1.1 we will keep it separate.\n3) Content negotiation (still under discussion)\nAt the moment, the largest uncertainty seems to be in content\nnegotiation.  Dan Connolly and Tim Berners-Lee expressed during\nour teleconference last Thursday serious reservations \n(more than just small details) about the current\ndraft of the subgroup.  They owe a writeup of a reputedly simpler\ncounter proposal on Thursday, March 21 to the working group.\n\nBy far the most worrying issue for 1.1 are potential interactions\nbetween content negotiation and caching.\n\nIt is possible that any/all of these three documents may become\nstandards at the same time as 1.1, if rough consensus over\nthem can be reached.  The sooner consensus is reached on them,\nthe more likely they could be integrated into the core document.\nConsensus will have to be reached in time for editorial grunt work\nbefore W.G. last call on the core document to allow docking of these\nindependent documents.\n\nVery likely left to after 1.1:\nAny issues we can't reach closure in time on the documents above, or \nissues already marked as deferred in the issues list.\nPEP: An Extension Mechanism for HTTP\nDemographics issues.\nExtended log file format\nSession Identification URI\nNotification for Proxy caches\n\nPlease let me know of any comments you have to this editorial proposal...\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Re: New section for Hos",
            "content": "Roy T. Fielding wrote:\n> \n> 10.22  Host\n> \n>    The Host request-header field specifies the Internet host and port\n>    number of the resource being requested, as obtained from the original\n>    http URL (Section 3.2.2) given by the user or referring resource.\n>    The Host field value must represent the network location of the\n>    origin server or gateway given by the original http URL.  This allows\n>    the origin server or gateway to differentiate between\n>    internally-ambiguous URLs (such as the root \"/\" URL of a server\n>    harboring multiple virtual hostnames).\n> \n>        Host = \"Host\" \":\" host [ \":\" port ]    ; see Section 3.2.2\n> \n>    A \"host\" without any trailing port information is equivalent to\n>    a value of \"host:80\".  For example, a request on the origin server\n>    for <http://www.w3.org/pub/WWW/> must include:\n> \n>        GET /pub/WWW/ HTTP/1.1\n>        Host: www.w3.org\n> \n>    The Host header field must be included in all HTTP/1.1 request\n>    messages on the Internet (i.e., on any message corresponding to a\n>    request for an http URL, as described in Section 3.2.2).  If the\n>    Host field is not already present, an HTTP/1.1 proxy must add a Host\n>    field to the request message prior to forwarding it on the Internet.\n>    If an HTTP/1.1 request message lacking a Host header field is\n>    received via the Internet by an origin server or gateway, that server\n>    must respond with a 400 status code.\n\n\nThis looks great.  The only thing I would change would\nbe to point out that when a port is missing, the port\nis implied to be the default port, which is 80 for\nstandard HTTP and 443 for HTTP over SSL (i.e. https urls).\n\n:lou\n-- \nLou Montulli                 http://www.netscape.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Content negotiation statu",
            "content": "As input to discussions (either on this list or in future phone\nconferences), here is my personal view of the status of the various\ncontent negotiation issues.  We will have to decide on how much\ncontent negotiation to put in the HTTP/1.1 draft fairly soon; see the\nend of my message for a proposal.\n\n1. Status of the content negotiation discussion\n\nThe content negotiation subgroups and caching subgroups have consensus\non many aspects of content negotiation.  There is text for the 1.1\ndraft in draft-holtman-http-negotiation-00.txt, this text captures\nmany of the points there is consensus on, but also adds some new\nelements not previously discussed that were necessary to get a\ncomplete mechanism.\n\nSome people in the http-wg have reviewed\ndraft-holtman-http-negotiation-00.txt and posted comments, but many\nhave not.  I have the impression that this absence of discussion does\nnot indicate consensus, but a lack of review.\n\ndraft-holtman-http-negotiation-00.txt is a complicated text.  It is\nunknown whether those people who have not yet reviewed draft-holtman\nwould agree to it being incorporated into 1.1.\n\nRoy has said that content negotiation text in the current 1.1 draft is\nno longer his current proposal for content negotiation.\n\n\n2. Long term goals for content negotiation\n\nThe content negotiation subgroup has consensus on the following scheme\nfor content negotiation.  This is what we want to be supported in the\nlong term.\n\n   Resources can be \n\n     1) un-negotiated: there is only one version of the content\n\n     2) negotiable as in draft-holtman-http-negotiation-00.txt,\n        the Alternates header specifies all available alternates,\n        the user agent allows the user to select another alternate if\n        desired.\n\n     3) negotiable with a Vary header.  The origin server has multiple\n        variants, and selects one based on, for example, the contents\n        of the user-agent header in the request.  There is no standard\n        mechanism to allow the user to select other variants.\n\n   Type 3 is to be used in cases where use of type 2 is not possible\n   for some reason.\n\nIn the long term, content negotiation must allow for\n\n   - negotiation on mime type, charset, and language\n   - small accept headers\n   - feature negotiation\n   - near-optimal caching\n\n\n3. draft-holtman-http-negotiation-00.txt\n\nMy current proposal for content negotiation consists of draft-holtman\nwith the following changes:\n\n - the Rep-Header mechanism is removed and replaced by a\n   content-ID/Unless-ID mechanism, according to emerging consensus in\n   the caching discussion.\n\n - negotiation on Content-Encoding is moved outside the main\n   negotiation mechanism.  This allows proxies and server to encode\n   and decode (zip and unzip) relayed content on the fly, depending on\n   the capabilities of the next client in the chain.\n\n - the origin server restriction is weakened to `the alternate URI up\n   to its last slash must be equal to the negotiable resource URI up\n   to its last slash'.  Text is added to make the scope of this\n   restriction more clear.\n\n - the following rule is added to make future addition of more\n   negotiation dimensions smoother: `Proxies should not engage in\n   alternate selection calculations on behalf of the origin server if\n   an unrecognized attribute is present in the Alternates header.'\n\n - the charset all browsers are supposed to be able to handle may be\n   changed to from US-ASCII to ISO-8859-1, if the http-wg has\n   consensus about this change.\n\n - the `mxb' stuff may be taken out to simplify things\n\nI believe draft-holtman to be technically mature.  Given the large\nrequirements on content negotiation (must be downwards compatible,\nmust be completely optional, must allow small Accept headers, must\nallow a large number of supported MIME types, must work optimally with\ncaches, must be extensible, must be as fast as possible, must be\nsecure, must protect privacy, ...), I do not believe that the\nnegotiation text can be made much simpler.\n\nThe main problem with the text is acceptance.  Unlike some other http\ninternet drafts currently under review, draft-holtman completely\ndefines all aspects of negotiation and its interaction with other http\nfeatures.  This completeness may paradoxically delay acceptance rather\nthan speed it up.\n\n4. Proposal\n\nI propose to put `docking rings' for content negotiation in the 1.1\ndraft of April 1.  I will release a new content negotiation internet\ndraft at the same time.  This new draft will also define the Vary\nheader.\n\nThe docking rings consist of text that adds requirements to 1.1 to\nensure that 1.1 caches\n\n  - can interoperate with content negotiation as defined in a \n    future separate standards document or in HTTP/1.2.\n  - can cache such negotiation with a reasonable efficiency.\n\nIn other words, the docking rings put in the foundations on which a\nfuture content negotiation mechanism can be built, allowing a smooth\ntransition to negotiation-capable software.\n\nThe docking rings will allow for negotiation by origin server side\nalgorithms in 1.1, for example on negotiation on language.  1.1\nproxies will have to contact the origin server whenever getting a\nrequest on such a 1.1 server-negotiated resource, and can then get a\nshort response saying `return the entity X you have cached'.\n\nTo allow for origin server side negotiation, short descriptions of the\nvarious Accept-* headers will be included in the draft.\n\nI am confident that I can make docking ring text that satisfies the\nabove requirements.  Note that this text _will_ make 1.1 proxies a bit\nharder to implement.  This is the price we pay for laying foundations.\n\nThis proposal allows us to delay consensus on the April 1 content\nnegotiation draft until after May 1, when we must have consensus on\nthe core 1.1 draft.  If we have consensus on content negotiation\nbefore May 1, drafts can of course be merged.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Additional comments on the Host: header...",
            "content": "I think the following comments are worth adding to Jim's summary (and that\nof the minutes) of the \"host:\" header situation and especially to the\n\"disadvantages\" of a couple of the options.\n\n\n* Protocols can accumulate only a certain amount of crud  -- extensions,\npatches, fixes -- to remedy errors in the original or for which there wasn't\nan adequate anticipated framework before they become too clumsy for further\nuse, development, or accurate implementation and start relying on the\nrobustness principle alone to keep them working.   It is very much in our\ninterest to postpone as long as possible the date on which we _must_ invent\nand deploy an HTTPng in order to just keep the Internet working.   We\naccelerate that date by adding complex interactions to HTTP now and push it\noff by trying to make extensions as clean and minimally complexity-adding as\npossible.  \"You must use this if the format of that isn't thus-and-so\"\ncreates unneeded complexity and, given that there are other alternatives,\nshould be avoided on that basis alone.  \n\n\n* The extra turnarounds of a \"try, and, if that fails, try something else\"\nsituation are unfortunate, but have to be considered against the backdrop of:\n\n   -- Very short halflives of Web clients and servers: if we push something\nlike this out there, it is likely to be a problem for a fairly short period.\nA kludge that is forever is, well, forever.\n\n   -- The growth of the Web and the Internet are such, if a new protocol\nmodel is deployed tomorrow, a year from now half the users will never have\nencountered the old protocol.\n\n   -- Since the \"fix\" --upgraded software-- is known, vendors who move to\nthe newer protocol will have a competitive advantage and those who don't\nwill be put to a proportionate disadvantage.  In today's market, that is\nlikely to further encourage rapid deployment.  On the other hand, anything\nthat makes it less painful for an implementation to stay in place than\nconvert will put early converters at a competitive disadvantage.\n\nIt may also be worth noting that the \"extra turnaround per connection\" Jim\nnotes is a worst case.  I'd assume that smart clients, designed to optimize\nwhenever possible, would cache negative responses for a while so that, if\nthere are several retrievals in a row from the same server (a common\nsituation), only the first of the series would incur the extra overhead of\ndealing with an older server. \n\n\nI contend that we have to bite the bullet sooner or later and that, given\nthe present growth rate and software rotation rate, doing it quickly and\nwell, even at the price of some short-term transitional marginally worse\nperformance, is likely to be less painful than trying to work through a\ngradual transition and will leave us in much better shape a year or eighteen\nmonths from now.\n\n    john\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Rob asked:\n\n> Does the SESSION method you propose allow for servers to mix pieces of\n> responses to different requests together like HTTP-NG?\n\nActually, I have not yet proposed any meaningful semantics for a SESSION\nmethod other than it cannot be forwarded.\n\nHowever, it is natural to assume that it would have similar semantics\nto the discussion we had at the HTTP BOF, and thus I should probably give\nyou a straight answer anyway.  The answer is no, since that would require\ntagging each request with synchronization info and would make it vastly\nmore complicated to implement.  Besides, we have to leave something fancy\nfor HTTP-NG to implement.  ;-)\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "According to Roy T. Fielding:\n> \n> 8) There is no such thing as a bloody MGET proposal, because no one\n>    has ever attempted to specify it beyond some vague notion, let alone\n>    actually implement the sucker.  I don't want to hear any more arguments\n>    about how much *more* efficient this method is until someone gets up\n>    the gumption to specify exactly what the beastie will entail. \n\nHere is a proposal I circulated a month or two ago.  It was on\nwww-talk.  I have made a one minor addition reflecting the recent\ndiscussions here.  This is far from a careful specification, but it is\nmore than I have seen concerning SESSION and Connection: headers.\n\nProposal for an HTTP MGET Method\n \n   A much discussed pressing need for HTTP is a mechanism for the client\n   to request and the server to deliver multiple files in a single\n   transaction. The primary (but not sole) motivation is the need to\n   improve efficiency of downloading an HTML file and multiple inline\n   images for that file. Here is a concrete proposal for the addition of\n   an MGET method to HTTP to meet this need.\n   \n   _Design Objectives_\n\n    1. The number of transactions to download an HTML file with inline\n       images should be minimized.\n    2. The client should have the option of downloading all, some or none\n       of the inline images. It should be able to request some or all\n       files with an \"If-Modified-Since:\" header.\n    3. Additions to the protocol should be simple to implement for\n       servers and browsers.\n    4. The statelessness of the server should be preserved.\n    5. New client / old server and old client / new server transactions\n       should work as should new client / old proxy etc.\n    6. The server must be able to transmit the multiple files \"on the\n       fly\" without knowing their size in advance and without making a\n       copy of the whole package before transmission.\n    7. The server must have the option of returning some of the requested\n       files while denying access or reporting an error for others. In\n       particular, it must return a separate status header for each\n       requested file.\n   \n   \n   In order to achieve the second objective above a minimum of two\n   transactions will be required. In the first transaction the client\n   receives the base HTML document (this is a GET transaction and should\n   be identical to HTTP/1.0). The client is then in a position to decide\n   which inline images to request. It may want all, some, or none, as it\n   may have some cached or it may be incapable of displaying images.\n   \n   The second transaction is an MGET transaction. The client lists the\n   URIs it wants each followed by an \"Accepts:\" header applicable to that\n   URI alone. The client can also provide an \"If-Modified-Since:\" header\n   for any of the requested files which should work like it does in\n   HTTP/1.0 The server returns the files with a packet transfer-content\n   encoding beginning each packet with the exact number (in ASCII) of\n   bytes in that packet and an empty packet (size -1) to indicate end of\n   file. Here is an example of a client server exchange.\n   \n\n\n        C: GET /foo.html HTTP/2.0<CRLF>\n           Accept: text/html, text/plain<CRLF>\n        S: HTTP/2.0 200 Success<CRLF>\n           Content-Type: text/html<CRLF>\n   Accept: GET, MGET, POST, HEAD, MHEAD<-- see note (*) below\n           ...<other headers>\n           <CRLF>\n           <sends file>\n           < closes connection>\n\n\n  [This was a GET request, identical to HTTP/1.0 except for the additional\n  header line \"Accept: MGET...\" from the server.  The second request uses\n  MGET.]\n\n        C: MGET HTTP/2.0<CRLF>\n           URI: /images/bar1.gif<CRLF>\n           If-Modified-Since: Saturday, 29-Oct-94 20:04:01 GMT\n           Accept: image/gif, image/x-xbm<CRLF>\n           URI: /images/bar2.gif<CRLF>\n           If-Modified-Since: Saturday, 29-Oct-94 20:04:01 GMT\n           Accept: image/gif, image/x-xbm<CRLF>\n           URI: /images/bar3.gif<CRLF>\n           Accept: image/gif, image/x-xbm<CRLF>\n           URI: /images/bar4.gif<CRLF>\n           Accept: image/gif, image/x-xbm<CRLF>\n           <CRLF>\n\n        S: HTTP/2.0 200 Success<CRLF>\n           URI: /images/bar1.gif<CRLF>\n           Content-Type: image/gif<CRLF>\n           Content-Transfer-Encoding: packet<CRLF>\n           <CRLF>\n           8000<CRLF>\n           ... 8000 bytes of image data first packet...\n           2235<CRLF>\n           ... 2235 bytes of image data completing file...\n           -1<CRLF>\n           <CRLF>\n           HTTP/2.0 304 Not Modified<CRLF>\n           URI: /images/bar2.gif<CRLF>\n           Expires: Saturday, 29-Oct-95 20:04:01 GMT\n           <CRLF>\n           HTTP/2.0 403 Forbidden\n           URI: /images/bar3.gif<CRLF>\n           <CRLF>\n           HTTP/2.0 200 Success<CRLF>\n           URI: /images/bar4.gif<CRLF>\n           Content-Type: image/gif<CRLF>\n           Content-Transfer-Encoding: packet<CRLF>\n           <CRLF>\n           150213<CRLF>\n           ... 150213 bytes of image data (complete file)...\n           -1<CRLF>\n           <CRLF>\n         S: <closes connection>\n\n\n   \n   \n   This seems to me to meet all the objectives listed above. Comments are\n   welcome.\n\n   Note (*):  This line was added as a result of recent discussions on\n   the http-wg mailing list.  The presence of MGET notifies the client\n   that the server will accept the MGET method.  This should be ignored\n   if the client is communicating with a proxy, because the proxy might\n   not be MGET capable.  Perhaps a new proxy should add an additional\n   header like \"Proxy-Allow: MGET etc...\" if it wishes to allow the\n   client to use MGET.\n\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: HTTP: T-T-TTalking about MIME Generatio",
            "content": "Marc Salomon writes:\n\n> Indeed, the restrictions on the use of Message-ID: in HTTP/1.0 defeat caching\n> WRT MIME.\n\nThere are no restrictions on Message-ID in HTTP/1.0 which do not already\nexist in the MIME RFC 1521.  I quote [page 19]:\n\n    \"Like the Message-ID values, Content-ID values must be generated\n     to be world-unique.\"\n\nThe MIME BNF references \"msg-id\" of RFC 822, which has the EXACT same\nsyntax as that defined for HTTP/1.0.  A longer explanation of Message-ID\ncan be found in the USENET spec (RFC 1036).\n\n> [...]\n> |\n> |It's much easier to just keep the connection open (with possibly a very\n> |short timeout as defined by the server) and respond with multiple responses\n> |(each one of which may be a multipart entity).\n> \n> But TCP is much less efficient if you are sending bunch of small objects--you\n> never get out of first gear as it were.  Wasn't it argued that TCP doesn't\n> get efficient till it gets up to speed on a data transfer?\n\nNo, the slow-start effect only applies to connection setup and immediately\nthereafter -- once the window has been determined, it applies for the rest\nof the connection.\n\n> Would having to satisfy \n> a set of time-spaced serial requests allow the connection to ever get up to \n> speed?\n\nYes, though we intend to test that just to be sure.\n\n> |> Message-ID: <http://host.domain/http_mime.html>\n> |\n> |This is an invalid use of Message-ID -- it must be unique.\n> |\n> |> Content-ID: <http://host.domain/path/http_mime.html>\n> |\n> |This is an invalid use of Content-ID -- it must be unique.\n> \n> Stupid me.  I had assumed that the draft HTTP spec-00 talked about current \n> practice, but it deals with protocol-as-documented but unimplemented as well.\n\nIt documents preferred practice, as in \"what the programmers would have\nimplemented had they checked the existing Internet specifications before\nhacking away with abandon.\"  More importantly, it is intended to reflect\nthe combined wisdom of \"what should be done to implement a good HTTP\napplication\" as defined by the history of www-talk and the participants\nin this list, while at the same time not breaking any implementation that\nis not already known to be broken.\n\n> The specifications in HTTP/1.0 for both the format and use of the Content-ID: \n> are more restrictive than that of RFC 1521.  The MIME spec says that the\n> primary purpose of this field is to assist caching and offers no format\n> template, while HTTP/1.0 dedicates this field to a transaction identifier and \n> requires a strict format template.\n\nRTFM.  It requires <string@domain>, as does RFC 822, RFC 1521, and RFC 1036.\n\n> I could make an argument that a URL uniquely identifies the content of a\n> body-part in that a URL cannot identify but one object at one time.  The main\n> reason for the MIME optional Content-ID field is to allow for caching, which is\n> facilitated by the use of a URL (in conjunction with the Date: header) in this \n> field.  The URL would be valid and cachable from Date: till Expires: and is an\n> excellent candidate for a RFC 1521 style Content-ID.  A contradictory use for \n> the Content-ID field is specified in HTTP/1.0-draft-00, although not currently \n> used in any implementation.\n\nBull puckies.  Again, RTFM, and this time look at the BNF (the actual\nspecification) and not just the verbage.\n\n> The Content-Description header field as specified in RFC 1521 would probably\n> be an appropriate place for this, although I do not see a Message-Description \n> field there.  One would be tempted to look to the URI header to indicate the \n> URI associated with a body part, but from what I can tell, it is only used to \n> redirect a request and its use here seems inappropriate.\n\nCrikey! It explicitly states in the last sentence on Section 4.2.1 of\ndraft 00 (on Multipart Types):  \"A URI-header field (Section 7.9) should\nbe included in the body-part for each enclosed object which can be identified\nby a URI.\"  I can't get any more explicit than that!\n\n> |And what do you do about Content-Encoding?  It's not valid MIME at all.\n> \n> and later in the weekend Roy writes:\n> \n> |Until the MIME people add Content-Encoding to the official RFCs, there is no \n> |point in even discussing the issue here.  All that we can do is show how they \n> |differ and possibly how an HTTP -> SMTP/NNTP gateway should behave.\n> \n> HTTP currently uses:\n> \n> Content-Type: application/postscript\n> Content-Encoding: x-gzip\n> \n> To express a two-level encoding.  Two level encodings are not included in\n> MIME because of concerns about machines that are unable to easily perform\n> the (de)compression.  Since only machines that are able to (de)compress (should)\n> present this information during negotiation, it does not seem problem for the\n> web.  In short, current practice of HTTP conflicts with the limitations of \n> MIME, so we could validate the out that the MH people took:\n> \n> <comp.mail.mime-FAQ>\n> Miscellaneous parameters:\n> \n> x-conversions=compress  MH 6.8.x viamail: used with application/octet-stream;\n>                           type=tar.  See also tar(1) and compress(1).\n> </comp.mail.mime-FAQ>\n> \n> The solution for a multipart MIME compliant expression of Content-Encoding \n> would be to use something like one the following:\n> \n> Content-Type:  application/postscript\n> Content-Transfer-Encoding: binary; x-conversions=gzip\n\nUmmmm, I think that would have to be\n\n  Content-Type:  application/postscript; x-conversions=gzip\n  Content-Transfer-Encoding: binary\n\nsince CTE doesn't allow parameters. Anybody know if this appears in one\nof the MIME-extension drafts currently up for review?  It would be hard\nfor me to reference a FAQ as the \"official\" workaround.\n\nIt's a pity we can't turn back the clock and redefine Content-Encoding.\n\n> |> Two consecutive boundary strings indicate an EOF.\n> |\n> |What?  See RFC 1521.  Or, for that matter, read the HTTP/1.0 draft -- it\n> |already defines EObody-part and EOentity for Multipart entities.\n> \n> It had a hard time jumping out at me while preparing this document, but you are\n> again correct.  EOF is indicated by --boundary--.\n> \n> % grep -i EObody-part draft-fielding-http-spec-00.txt rfc1521.txt\n> % grep -i EOentity draft-fielding-http-spec-00.txt rfc1521.txt\n> \n> What are EObody-part and EOentity?\n\nSorry, that was my short-ref for End-of-body-part and End-of-entity,\nwhere body-part and entity are defined in rfc 1521.\n\n> Since the draft HTTP 1.0 spec-00 is supposed to document current practice in\n> the same manner as the HTML 2.0 effort, why is multipart/mixed documented,\n> which is most certainly *not* current practice in any implementation I know of\n> (is it?) although it is described in BASIC HTTP?\n\nThe HTTP/1.0 spec IS NOT supposed to document current practice \"in the\nsame manner as the HTML 2.0 effort.\"  HTTP has always been capable of\ntransporting multipart types, its just that the most popular servers and\nclients have not done so.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Draft 01 of HTTP/1.",
            "content": "A revised draft of the HTTP/1.0 specification is now available from\n\n    http://www.ics.uci.edu/pub/ietf/http/\n     ftp://www.ics.uci.edu/pub/ietf/http/\nand \n    http://info.cern.ch/hypertext/WWW/Protocols/Overview.html\n\nand will eventually be available at an IETF shadow directory near you.\n\nIt includes roughly 60% of the changes requested from draft 00 -- we would\nhave done (and will do) more, but the holidays are upon us.\n\nHappy reading,\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "I've been looking at how much work it would take do add a simple session\nbased connection-reuse method to existing implementations of HTTP 1.0, \nand it seems pretty simple.\n\nlibwww can be upgraded by putting a connection cache in HTTP.c, replacing\nthe HTConnect with a look at the cache, and a possible session probe, and \nby by using function variables for the NETREAD and NETWRITE calls. MGET could\nbe emulated by adding a varargs version of the LoadDocument call. \n\nNCSA httpd could be upgraded by a making a few hacks to process request, and \nby running it a couple of times under purify or test-center to get rid of \nany memory leaks. \n\nSCP is a good a way as any to do the packetisation; SCP does allow the responses\nto be interleaved; however taking advantage of this requires enough rewriting to\nmake this sort of functionality worth leaving to HTTP-ng \n\nI think there's definitely a place for a quick hack HTTP 1.1 designed to \nbe as small a change as possible to HTTP 1.0- re-using connections is the\nbiggest network performance  win- interleaved rendering gets most of the rest-\nafter that most of the gains are in functionality like challenge based payment,\nsecurity, better meta-information, and better proxy support.\n\nSimon \n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Here's another wrinkle to consider for a SESSION method.  Some transactions\nhave a model that resembles the Basic authentication scheme:\n1) The client innocently asks for something.\n2) The server rejects the request and asks for something extra.\n3) The client reissues the original request, plus the something extra.\n4) The server honors the request.\n\nThis model applies for some payment schemes and some security schemes.  It\nwould be nice if the client had a way to tell the server it's willing to\nkeep a connect open (create a session), even though it didn't request to do\nso at first (with a SESSION method).  Perhaps the client can send the same\nConnection: headers that it would have sent with a SESSION method.\n\nIf I understand the SESSION proposal correctly, the client must wait until\nthe server responds before it can send its first \"real\" request.  Wouldn't\nit make more sense to be able to send the first \"real\" request along with\nthe SESSION request?  If so, we're starting to look like my extensions\nproposal (http://www.research.att.com/~dmk/extend.txt, which needs some\npolishing per Larry Masinter's suggestion at IETF), in which you can \"wrap\"\nrequests in layers.  In this instance, SESSION resembles my WRAPPED request,\nand the contained request(s) is(are) the \"real\" request.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Minutes of Hypertext Transfer protocol BOF at 31st IET",
            "content": "Minutes of the Hypertext Transfer Protocol (HTTP) BOF from\nthe 31st IETF Meeting (San Jose). Wednesday 7th December 1994.\n\nChaired by Dave Raggett <dsr@w3.org>\n\nFurther info on HTTP is currently available from the URLs\n\n  http://info.cern.ch/hypertext/WWW/Protocols/Overview.html\n  http://www.ics.uci.edu/pub/ietf/http/\n\nMailing list info:\n\n General Discussion: http-wg@cuckoo.hpl.hp.com\n Archive: http://www.ics.uci.edu/pub/ietf/http/hypermail/\n To subscribe mail http-wg-request@cuckoo.hpl.hp.com\n and include the body:  subscribe http-wg Your Full Name\n\nThis BOF followed an earlier one at the World Wide Web Conference at Chicago\nin October in which it was decided to pursue setting up a working group for\nthe World Wide Web hypertext transfer protocol, to standardise existing\npractice, and then to work on improved performance, security and payments,\nand other improvements such as support for transaction mechanisms for\ndistributed updates.\n\nThe meeting started with a summary by the chair of the steps leading up to\nthe BOF. HTTP originated as a very simple protocol indeed. This version is now\nknown as HTTP/0.9. It was soon extended to include a MIME style wrapper to\nconvey the content type and encoding of the returned document. A number of\nRFC 822 style headers are used to support negotiation and convey meta\ninformation, as well as a basic authentication mechanism. This version is\nknown as HTTP/1.0 and is in very widespread use. The online documentation\nat CERN is no longer adequate. Other pressures on HTTP include the need to\nimprove performance, to avoid the penalties associated with making separate\nconnections for the document and each of the inlined images. We also need to\nstandardise ways of adding support for authentication and encryption to support\nprivacy and commercial services.  Setting up an IETF working group is now a\nmatter of urgency to ensure the continued health of both HTTP and the Web.\n\nThe meeting continued with a presentation by Henrik Frystyk Nielsen and Roy\nFielding on the Internet draft for HTTP/1.0 (draft-fielding-http-spec-00.ps).\ntogether with some ideas for extensions for consideration in HTTP/1.1. The\nconsensus of the meeting was that the Internet draft for HTTP/1.0 be proposed\nfor the standards track as soon as possible. Revisions to the HTTP/1.0 draft\nspecification are being discussed on the http-wg mailing list (see above).\n\nThe list of extensions for the 1.1 revision were:\n\n    *  Session method\n         o  Intended for short-term session only (~10 sec. request timeout)\n            though specialised servers may use longer timeout\n         o  Supports multiple transactions over single connection\n         o  Allows session-long negotiation of Accept-*, authentication,\n            and privacy extensions\n         o  More than just an MGET\n    *  Packetised Content-Encoding\n    *  Better Access Authentication\n    *  Base-URL as a New Object header\n    *  Allow relative time in Expires header (seconds)\n----\nSimon Spero talked briefly on the performance problems with HTTP/1.0.\nThe current protocol uses a separate connection for the document and\neach inlined image. Each connection requires at least two round trip times,\nand in practice more due to the lengthy accept headers in common use.\nFurthermore, tcp/ip uses a slow start mechanism to avoid congestion and\ngradually increases the throughput to match the actual bandwidth available. \nAs a result, most HTTP transactions operate at a reduced bandwidth. Simon\nreported measurements indicating that over a congested link from Bristol,\nEngland to North Carolina for a representative home page, the current\nprotocol only uses about 10 % of the bandwidth then available (as measured\nby a bulk transfer over a single connection). The approach used in the\nNetscape browser of opening multiple concurrent tcp/ip connections gives\nbetter performance, but still fails to utilise the full bandwidth. One\nexplanation, is that most tcp/ip implementations fail to share congestion\ninformation between connections to the same site. The approach also leads\nto problems with many more connections left in the time-wait state at the\nserver.\n----\nDiscussion on HTTP Issues:\n\nLarry Masinter asked whether proposals for extensions such as keep-alive\nwere based on experience or speculation. He prefers MIME boundary markers\nto packetisation schemes for determining message boundaries. Larry also\nsuggested we should consider richer mechanisms for determining whether\na document has expired. For instance, downloading conditions in SafeTCL\nto be evaluated by the client.\n\nAlex Hopmann argued in favor of reusing the connection e.g. for a series of\nimages, and increasing the use of proxy servers. He has tried out a session\nmethod scheme, and has a written proposal for this and a separate proposal\nfor a notification proposal (for more info email: alex.hopmann@resnova.com).\n\nSimon Spero described work on log analysis which showed clear groupings of\nrequests at 3 to 4 images per document. He mentioned problems in analysing\nlogs due to accesses by netscape browsers which initiate connections for\nimages concurrently and are then cancelled as the user surfs to the next\ndocument.\n\nTim Berners-Lee argued that users connecting over phone lines need browsers\nthat can do things concurrently with dynamic changes in priority as the user\nchanges his/her actions, e.g. the browser should keep the pipe full by\nfollowing links and then abort if the user does something else. He also raised\nthe issue of abstraction layers for HTTP.\n\nBrian Behlendorf discussed the need for user authentication and realms. He\nwants to be able to distinguish accesses to a given machine according to the\nalias used for the host name, and advocates using the full URL in the GET\nrequest.\n\nDigital have collected some 9 gigabytes of log data for requested URL and the\nduration the connection was kept open. A paper analysing this data was\npresented at the World Wide Web Fall Conference held in October 1994, and\ncan be found in the online proceedings.\n\nSomeone asked \"what does HTTP do in a couple of words?\". It is currently used\nfor a wide variety of things - should these be unbundled?.  Roy Fielding\nanswered that HTTP is an extensible protocol used for information transfer.\nTim Berners-Lee replied that this was a good question, and asked is MIME\nappropriate for small messages (for online accesses not for email)?\n\nDave Crocker agreed with the need for performance improvements, and said\nthat the current problem is in making connections. He argued in favor of\nTCPng and other ideas for optimising the underlying protocol rather than\nhacking session protocols etc. above tcp. We should feel free to adapt the\nMIME syntax to better suit the needs of the Web for online use.\n\nEric Sink asked when we can start writing code for this (improved\nperformance). The general reaction was that now was a good time to\ntry out experiments to feed into the next versions of HTTP.  Alex Hopmann\ndescribed his session method for multiple requests.  He was encourage to\ncarry on with this work and to repost the results so far. Tim asked Alex\nwhy a session method rather than as an attribute of GET (e.g. a keep-alive\npragma). The main thing is to avoid unnecessary round-trip delays.\n\nThe question was raised as to the possible impact of keep-alive versus\nthe session method on the operation of proxies.  The discussion was taken\noff-line.\n----\nHTTP Security Update from Tim Berners-Lee:\n\nTim reported on the HTTP Security BOF, which had taken place the preceeding\nevening. The idea was to split work on security off from the HTTP working\ngroup. This would lessen the workload and make it easier to involve security\nexperts in a wider context than that of HTTP alone. See the minutes for more\ndetails.\n----\nDave Krystal talked about a proposed extension mechanism for HTTP.\n\nAs the Web has grown, pressures have mounted to add a variety of facilities\nto HTTP. Some of the new features that have been proposed include: keep-alive,\npacketized data, compression, security and payment. Dave described an\nalternative: well-defined hooks in a slightly modified HTTP framework\nthat make it possible to add extensions to the basic protocol in a way that\nwill retain compatible behaviour between clients and servers, yet allow both\nclients and servers to discover and use extended capabilities. The proposed\nextension mechanism has two fundamental concepts: wrapping and negotiation.\nThe idea is to avoid a proliferation of new methods and header fields. Instead,\nthese would be handled through extensions with new stuff passed though to\nfeature managers. For further information please read the document URL:\nhttp://www.research.att.com/~dmk/extend.txt\n\nDave also asked whether payments would be covered by the HTTP security\nworking group.\n----\nSimon Spero on the HTTPng proposal\n\nSimon raced through the major ideas for HTTPng. A new protcol is needed\nwhich is more efficient; has security built-in from the start; and is caching\nand payment aware. HTTPng uses a session protocol above tcp/ip to support\nmultiple asynchronous transactions interleaved on the same connection.\nThis allows a browser to send the request for an inlined image before\nfinishing reading the html document that references it. The approach avoids\nthe delays associated with starting up new connections and makes better use\nof available bandwidth and server resources. The proposal uses a subset of\nASN.1 and the packed encoding rules to formally specify messages. This\nsimplifies implementations compared to using text based representations.\nThe lengthy Accept headers in HTTP/1.0 are avoided by using a bit vector for \ncommon cases with an extension mechanism for the rare occasions when\nthese are insufficient.  Servers can challenge for payment. A simple\nimplementation of HTTPng was found to operate approaching an order of\nmagnitude faster than HTTP/1.0 over an intercontinental link. A transition\nstrategy was described that allows existing HTTP/1.0 clients and servers\nto interoperate with HTTPng via proxy servers supporting both HTTP/1.0 and\nHTTPng.\n----\nDiscussion of the charter\n\nA show of hands indicated unanimous support for recommending to the area\ndirectors that a working group be set up for HTTP. We briefly discussed the\ndraft charter prepared by Roy Fielding. Some minor revisions were agreed.\nThe meeting expressed confidence in Dave Raggett continuing as chair.\nSubsequently, following a suggestion by John Klensin, to co-opt an IETF\noldtimer as co-chair, I bludgeoned Tim Berners-Lee into agreeing to taking\non this role.\n\nDave Raggett - 19th December 1994\n\n\n\n"
        },
        {
            "subject": "Re: Minutes of Hypertext Transfer protocol BOF at 31st IET",
            "content": "On Tue, 20 Dec 1994, Dave Raggett wrote:\n> Brian Behlendorf discussed the need for user authentication and realms. He\n> wants to be able to distinguish accesses to a given machine according to the\n> alias used for the host name, and advocates using the full URL in the GET\n> request.\n\nJust to correct history - I brought up as issues (since I didn't see them\naddresses directly as issues to be considered for 1.1 or -NG) that\n\n1) We have some way to allow servers to express \"your password is not \nonly good here, but at these other servers/directories, so give it a try \nautomatically when you go there\".  There were a few bits of email here \nabout it a few weeks ago but I just didn't want it to go unnoticed as I \nand others consider it important, even as we ditch basic authentication \nand go towards MD5 signatures or whatever.\n\n2) Having the GET request be changed to the full URL would be horrible\nnon-backwards-compatible :) I suggested adding a header in the client\nrequest so foo.com, when CNAME'd by bar.com, knows to return bar.com's\nhome page rather than foo.com's.  Yes, vanity domain names are a scourge\non the net and all that, but the alternative is to burn up IP numbers for\nthe same effect.  Something like\n\nRequest-URI: http://bar.com/ or\nHost-requested: bar.com\n\nI don't know of a slick warm fuzzy solution short of a new header.\n\nBrian\n\n\n\n"
        },
        {
            "subject": "Closure on canonicalization, I hop",
            "content": "I'm sure nobody is surprised to hear me say that I believe the\ntreatment of cannonicalization of objects in draft 01 of the spec\nstill needs some work.\n\nUnder the new spec, a server is compliant if it serves up ASCII\nplaintext as having, say CRs as line delimiters.  A client, however,\nis compliant if it recognizes only CRLFs as line delimiters\n(recognizing CRs is recommended for tolerance but not required.)  So\ndifferent implementations may both be compilant with the spec but not\ninteroperate properly.  I hope we can agree that is not OK.\n\nMind you, I don't even know what exactly the current spec is saying.\nIf we are effectively requiring all implementations to understand all\nnon-canonical forms that exist, I think we need to enumerate exactly\nwhat they are.  Other than line breaks, I have no idea what\nnon-canonical representations clients (and servers, for that matter)\nmust understand.\n\nIn the interest not of philosophical purity (as Phil H-B might say,\nscrew philosophical purity) but making something clear and reasonable,\nI'd suggest something like the following phrasing:\n----\n  Conversion to canonical form:\n\n  Internet media types [cite 1590] are registered in a canonical form.\n  In general, Object-Bodies transferred via HTTP must be represented\n  in the appropriate canonical form prior to the application of\n  Content-Encoding and/or Content-Transfer-Encoding, if any, and\n  transmission.\n\n  Object-Bodies with a Content-Type of text/*, however, may represent\n  line breaks not only in the canonical form of CRLF, but also as CR\n  or LF alone, used consistently within an Object-Body.  Conforming\n  implementations *must* accept any of these three byte sequences as\n  representing a single line break in text/* Object-Bodies.\n\n  RATIONALE:  A handful of different local representations of textual\n  files exists in current practice.  Conversion to canonical form can\n  pose a significant performance loss, while understanding different\n  line break representations is not an inordinate burden, nor an\n  excessive requirement beyond current practice.\n----\nPart of the question is whether it's implicit that all text/* types\nare ASCII-based and that CR and LF are the appropriate interpretations\nof octets 0D and 0A.  The newest draft of MIME calls for such, as does\nsection A.2 in the current HTTP draft, so I stuck to that path.\n\nI'm not thrilled about requiring implementations to accept different\nline break sequences, but we kind of have to either:\n\n1. Require all implementations to canonicalize\n2. Require all implementations to understand certain specific\n   non-canonical forms (and say exactly what they are)\n3. Require some kind of negotiation process by which servers and\n   clients can indicate what variations each can understand\n\nWe seem to mostly have consensus that 1) is a potentially too\nexpensive.  I think 3) is more complexity than we particularly want to\nendure right now [it would lead to something like \"Accept-Encoding:\nunix-linebreaks\" or maybe \"Content-Transfer-Encoding:\n8bit-sloppy-eol\".]  That leaves 2), which is probably what will come\nclosest to satisfying folks.\n\n- Marc\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "It is possible to send the first request with the session establishment \npacket, so you don't have to wait for the round trip. If the session method\nis rejected, the wrapped request will just be discarded.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: MIME and binary transpor",
            "content": "    > A little experiment should help shed some light on this.  On a reasonably\n    > fast machine (a DEC3000/600), I did (after first arranging for /vmunix\n    > to be in the filesystem cache):\n    >     % time wc /vmunix \n    >  15102     97927   6351808 /vmunix\n    >     0.93u 0.14s 0:01 94% 0+1k 0+0io 15pf+0w\n    >     a% time ngrep masinter@parc.xerox.com /vmunix \n    >     0.27u 0.19s 0:00 74% 0+1k 4+3io 47pf+0w\n    >     % \n    > ngrep uses a modified Boyer-Moore algorithm.  Larry is right;\n    \n    I'm sorry but I'm quite skeptical about your experimental design and\n    it's applicability to the HTTP context as well.  For starters finding\n    split points in an inbound byte stream is a different problem than\n    presented by a file full of bytes all present and waiting for processing.\n    \nWell, I tried the same experiment, except with ngrep getting its\ndata from a pipe:\n\n    % cat /vmunix | /bin/time ngrep masinter@parc.xerox.com \n    \n    real   0.6\n    user   0.3\n    sys    0.1\n    %\nI used /bin/time because the csh built-in \"time\" command won't\naccept piped input.  So, within the resolution of /bin/time's\nmeasurement, the user-mode time is the same.  The extra 0.2 seconds\nof \"real\" time may come from having to share the CPU with the\ncat command, which takes about that long when copying the file\nto /dev/null.\n\n    Secondly, I am suspicious that there is a marked difference in the quality\n    of implementation of wc vs. ngrep interms of program organization.\n    \nWell, you caught me there.  Wc is pretty darned slow; after all,\n\"cat /vmunix >/dev/null\" takes 0.0 seconds of user-mode CPU time.\nBut that's largely irrelevant to Larry's argument; the absolute\ncost of the Boyer-Moore string search is still too small to worry\nabout.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Simon E Spero <ses@tipper.oit.unc.edu> said on Tue, 20 Dec 94 13:53:33 -0500:\n  > \n  > It is possible to send the first request with the session establishment \n  > packet, so you don't have to wait for the round trip. If the session method\n  > is rejected, the wrapped request will just be discarded.\n\nI think your remarks are a response to something I wrote, but I'm not sure\nwhat.  One concern I mentioned was that a server might want to initiate a\nsession, something the client wouldn't have known about when it made its\noriginal request.\n\nConsider Basic authentication as an example (ONLY as an example, not\nnecessarily a good one):\n1) Client issues GET\n2) Server says, \"No, authentication needed\".\n3) Client issues another GET, this time with authentication\n4) Server responds with object.\n\nIf the server knew, at step 2, that the client would tolerate a session,\nit (server) wouldn't have to close the connection.  The client could do\nthe GET again on the same session.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "The following section from Draft 01 seems odd.\n\n>6.4.2 WWW-Authenticate\n>\n>   The WWW-Authenticate header field must be included as part of the\n>   response if the server sends back a \"401 Unauthorized\" Status-Code\n>   on a request from the client as part of the Basic Authentication\n>   Scheme described in Section 9. This header field indicates the\n>   authentication scheme in use and the realm in which the requested\n>   URI belongs.\n>\n>       WWW-Authenticate        = \"WWW-Authenticate\" \":\" (\n>                                   ( \"Basic\" realm )\n>                                 | ( extension-scheme realm ) )\n>\n>       realm                   = \"Realm\" \"=\" 1#( \"<\" URI \">\" )\n>\n>   The first word of the field value identifies the authorization\n>   scheme in use and is followed by the realm of the protected URI.\n>   The realm is a comma separated list of URIs, where relative URLs\n>   should be interpreted relative to the URI of the requested resource\n>   in the RequestLine. If a request is authenticated and a realm\n>   specified, the User-ID and password should be valid for all other\n>   requests within this realm.\n>\n>       Note: The realm may span more than one origin server.\n\nI'm not aware that people generally use the realm as a comma separated\nlist of URIs.  I remember hearing someone say at the BOF that it would\nbe nice to be able to specify that URIs on other machines can be accessed\nas part of the same realm.  Perhaps this section has been phrased\naccordingly?\n\nWas this section written to reflect current practice, or was it written to\nbe better than current practice?\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\"Can I get a direct flight back to reality, or do I have to change planes\nin Denver?\" - The Santa Clause\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "According to Roy T. Fielding:\n> \n> Actually, I have not yet proposed any meaningful semantics for a SESSION\n> method other than it cannot be forwarded.\n> \n\nI noticed.  This makes it difficult to compare to an MGET proposal.  I am\neagerly awaiting some description of how SESSION and Connection headers are\ngoing to achieve the same user perceived performance as multiple connections.\n\nI don't mean to be impolite, but I haven't seen much evidence of thought being\ngiven to the issue doing something in HTTP1.1 that can eliminate the\nneed for multiple connections.  Temper tantrums directed at Netscape don't\ncount. :)\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "    2) Experimental client / old proxy: This will break if the remote host\n    does in fact understand the connection header. As Roy points out, this\n    is a weakness of the design.\n\nOK, I modified my simulator (described in a previous message,\nhttp://www.ics.uci.edu/pub/ietf/http/hypermail/current/0181.html)\nto use the algorithm I suggested (in http/hypermail/current/0198.html),\nusing an adaptive timeout on the server.\n\nWhat I simulated is that the server would \"learn\" which clients were\nreusing a connection (i.e., were not \"new client/old proxy\" situations)\nand those clients would see a long timeout.  \"New client/old proxy\"\nconnections would be timed out after 1 second.\n\nThis has the effect of adding an extra second of connection time for\nthese clients, which is NOT a delay in the retrieval time (i.e., it\ndoesn't affect UPP).  This is because new clients and servers will be\nusing Content-length or boundary delimiters to mark the ends of the\ndata.  The client will certainly not be looking for the end of the\nconnection (since it is hoping that the connection will never end!)\n\n(I'm assuming that the relay pushes the returned data to the client\nwithout waiting for the end of the connection; is this not what relays\ndo?  If the relay waits, then this does add 1 second to the UPP for\neach retrieval.)\n\nIt also has the effect of giving up on the opportunity to reuse\nconnections, if the client is relatively slow about generating a\nnew request (or if the network RTT > 1 second).  Of course, \"1\nsecond\" is an arbitrary value, and it perhaps could be somewhat longer\n(depending whether old relays behave \"right\").\n\nI used the logs from 1 day's accesses (243177 retrievals) to our\nelection server (actually, to one of the three server machines) and\nassumed that all the clients are saying \"hold connection open\".\nI think this log includes references from 5198 distinct client IP\naddresses.\n\nI added a new parameter to the simulation: the number of clients that\nthe server should keep adaptive-timeout state about.  This is managed\nas an LRU cache; if a client drops out of the cache and then reappears,\nthe server then makes the conservative assumption that this client is\nbehind an \"old proxy\" and goes back to using a 1-second timeout.\n\nI fixed the other simulation parameters: 64 processes max, and a 120-second\nidle timeout for \"known reusers\".\n\nWith vanilla HTTP, there were 243177 TCP connections and the maximum\nPCB table size was 2496.  With persistent-connection HTTP and no\nadaptive timeouts (i.e., all timeouts = 120 seconds), there were\n21955 TCP connections and 440 PCB entries max.\n\nWith 100 entries in the adaptive-timeout cache, there were 74285\nTCP connections and 763 PCB entries max.  So this scheme does give\nup on some of the potential savings from persistent-connection HTTP,\nbut still reaps most of them.\n\nWith 1000 entries in the adaptive-timeout cache, there were 62679\nTCP connections and 642 PCB entries max.  With 10000 entries in\nthe cache (i.e., enough to hold all 5198 client hosts), there were\n60632 TCP connections and 643 PCB entries max.  So increasing the\ncache size doesn't help much; presumably, the missed opportunities\ncome from clients that don't reuse the connection quickly, not\nfrom cache-thrashing.\n\nComparing the adaptive-timeout scheme to a fixed-timeout scheme\nwith a 1-second timeout, I found that the latter resulted in\n149355 connections and 1396 PCB entries max.  So, there is some\nbenefit to the adaptive scheme.\n\nFinally, I simulated the adaptive-timeout scheme with a 120-second\n\"long\" timeout and a 10-second (instead of 1-second) \"short\" timeout.\nThis would only be safe to use if we believe that \"old proxy\" relays\ntransfer all the returned data without waiting for the connection\nto close.  This approach resulted in 25551 TCP connections and\n447 PCB entries max, almost the same as the non-adaptive scheme.\nBut because it would tie up fewer server and proxy resources in\nan environment full of \"old proxies\", it might be worth doing.\n\nBottom line: the \"new client/old proxy/new server\" problem is not\nthat hard to solve, if one doesn't insist on optimal performance\nin every case.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "After taking a longer look at the proxy problem it looks like there might be\na work around based on using a different version number; I believe that \nproxy servers generate the header version themselves independent of the \nversion given on the string. The end-server can be configured to ignore all \nconnection headers unless the version is >1.0.\n\nThe adaptive timesouts sound useful - there are very definite usage patterns\nthat show up if log data is sorted into virtual sessions. Some patterns (like\nimage fetches) are general, but there also some interesting local patterns (\nif someone looks at the Dr Fun home page, they're very likely to get the\ncartoon of the day.)\n\nDo you have some more info about the data set you ran the traces on? I \nseem to recall you mentioning that the traces were taken from the California\nElection server over a couple of days? Do you have any information on the size of \nthe data set, and the relative access frequencies of the various documents?\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: open/request/close in one packet",
            "content": "    What with all this talk about round-trip time, folks might want to\n    look at RFC1644: T/TCP -- TCP Extensions for Transactions Functional\n    Specification.\n\nWhich might make interesting reading, but there's no way that this\nprotocol will be widely implemented any time soon.  And we can solve\nessentially all of the RTT issues with vanilla TCP, if used properly.\n    \n    Also, even without TCP Extensions, I'm told that it seems to be\n    possible to open a TCP connection, send some data (e.g., a HTTP\n    request) and request the close of the connection all in one packet.\n    Of course, few network implementations support this from the sender\n    size.\n\nWhich makes this also rather a moot issue.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "    Do you have some more info about the data set you ran the traces on? I\n    seem to recall you mentioning that the traces were taken from the\n    California Election server over a couple of days? Do you have any\n    information on the size of the data set, and the relative access\n    frequencies of the various documents?\n    \nJust before I received your message I sent this correction to the\nminutes of the BOF:\n    This is wrong in several ways:\nDigital have collected some 9 gigabytes of log data for requested URL\nand the duration the connection was kept open. A paper analysing this\ndata was presented at the World Wide Web Fall Conference held in\nOctober 1994, and can be found in the online proceedings.\n    \n    (1) For the 1994 California Election server, we collected several\n    different kinds of data.  We have full logs (including connection\n    durations with 1-msec resolution), but these are nowhere near 9 GB.\n    We also have a few full days of tcpdump traces; these come to 9 GB\n    after compression.  (For privacy reasons, we cannot provide these\n    logs or traces to others.)\n    \n    (2) The paper given at the conference in October obviously could\n    not have been based on this data (which was collected in early\n    November!)  It is based on simple experiments performed with modified\n    clients and servers.\n    \nTo answer your questions a bit more specifically:\n\nThe data set size varied over time.  Before 8pm (PST) on election\nnight, it was mostly \"static\" pages, such as the text of ballot\npropositions, candidate statements, campaign finances, photos of\nfaces, etc.  After 8pm, we started updating the actual returns every\n5 minutes.  (The \"returns\" pages were online before that, but were\nonly stand-in pages, nothing useful.)\n\nI don't have the precise sizes handy, but I suppose I should get\nsomeone to do a \"find\" on that server now.\n\nI haven't gotten around to calculating file size distributions.  To\ndo this right, I should probably break it down by file type, or even\nfurther. For example, in order to draw bar graphs showing the results,\nwe generated 101 different .gifs representing values from 0% to 100%.\n(See http://www.election.digital.com/e/returns/returns/page.html for\nan example.)  We sure hoped that most people browsing these pages\nwould end up with most of the bar-graph .gifs cached before too long!\n\nI looked at the logs for the busiest day, and the average number of\nbytes returned was about 2100-2200 (depending on whether you count\n0-byte responses in the average).  Of course, we knew when we constructed\nthese pages that we were expecting a heavy load, and we tried to keep\nthings fairly compact.\n\nFYI, not counting the tcpdump traces, we have about 1 gigabyte of\nlog information, although this includes an awful lot of different\nthings.  Needless to say, we could be busy for years trying to figure\nout what it all means, although by then the Web will be quite different.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "    I have some some serious reservations about this.  Perhaps someone\n    more knowledgeable about TCP/IP can alleviate them.  Won't this\n    maintain the connection during the entire time that a client takes to\n    download and layout and display a document containing containing\n    mulitple images?  This could represent a substantial amount of time\n    over a slow link.  For a heavily loaded server, say like HotWired,\n    keeping the connection open during the entire time a user enters her\n    password and multiple images in a document are downloaded over a 14.4\n    link could add up to quite a bit of time.  I would think that the\n    maximum number of allowed connections would be exceded quite often.\n    There would probably be a large number of timeouts each one representing\n    almost 10 seconds of unused connect time.\n    \nOther people have already responded, pointing out the basic\nmisunderstanding here about TCP connection state records.\n\nI just wanted to address the \"maximum number of connections\" issue.\nEven with persistent-connection HTTP, you never need to support\nany more simultaneous connections than with vanilla HTTP.  However,\nthe more connections you have available, the better the performance.\n\nOne caveat: my simulations assumed that the request rate would\nremain constant even if the UPP improved (because responses would\nbe received faster, using persistent connections).  This could\nactually lead to a slightly higher request rate.  On the other hand,\nI would assume it would cause a higher request rate from exactly\nthose clients that have open connections, so it would not actually\nincrease the number of processes needed to respond to those requests.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "TIME_WAI",
            "content": "    I still think that it is better to keep the connection open for a small\n    amount of time as the server socket will go into TIME_WAIT state when\n    it is closed and then can't be reused before about 240 seconds\n    (recommended timeout accorsing to Simon)\n    \nThis not just Simon's recommendation.  This is a MUST (i.e., mandatory)\nrequirement of RFC1122 (section 4.2.2.13).\n\nAnd, to clarify: what cannot be reused is the memory allocated to the\nconnection state record; all other resources are reusable.  However, in\nmost BSD-based systems, this memory usage can be significant (one\nmbuf).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "    I don't see this as a very big problem.  We have lost one roundtrip\n    time, but only a roundtrip between client and proxy.  In normal use\n    client and proxy are very close with low latency.  I don't think this\n    is a very big price to pay.\n\nIt is not always the case that client and proxy are close.  In fact,\nsometimes it's quite the opposite.\n\nFor example, Digital has two or three proxies that I know of.  These\nare all located in parts of the company that are \"well-connected\"\nto the Internet.  So the proxies are usually fairly close to the\nservers.\n\nHowever, the clients for these proxies are spread out all over Digital,\nwhich is a world-wide company.  And in many parts of the world outside\nthe US, our internal networks are not as well-connected as they should\nbe (after all, high-bandwidth transoceanic lines aren't cheap, and\nsatellite links are intrinsically high-delay).  So we indeed have\nmany clients that are hundreds, even thousands, of milliseconds away\nfrom their proxies.\n\n    There is an important principle to keep in mind here.  Any proposal\n    that can't at least match the user's perceived performance which\n    Netscape obtains with multiple connections is not viable. It's a\n    competitive world out there and browser writers will have to go with\n    multiple connections if nothing else matches their performance.  Any\n    proposed standard, MGET or stay-open, which can't measure up to\n    multiple connection performance just won't be implemented.\n    \nI would turn this around and say that any proposal that doesn't scale\nas well as persistent-connection HTTP is not viable.  It will sooner\nor later collapse under its own weight.  Sure, the Netscape approach\nmakes the browsers seem a little faster, but only as long as the\nserver and network have lots of excess capacity.  When this is not\ntrue, multiple connections could be a disaster.\n\nEven the original single-connection HTTP model is likely to cause\ntrouble; it's not really Netscape's fault.\n\nAnyone who tried to use the Internet before widespread adoption of\nVan Jacobson's TCP congestion-avoidance mechanism will appreciate the\nrelative virtues of scalability and greediness.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "    I agree... for instance, I'm not sure how I would implement something\n    to keep track of how idle multiple server processes are without using\n    shared memory or writable mmap()ed files, which I already have found\n    are not available on certain platforms we support...\n    \nWell, I have some suggestions.  Have Netscape send a check to Digital,\nand I'll share them with you :-).\n\n    I'm not sure where the demand for this is either... The generality of\n    a session seems to me to be not that great a win if the session is\n    still a batch request-response cycle. I'd rather have real sessions\n    similar to what Simon proposes in HTTP-NG, where we can have\n    interactive stock quotes streaming in through one window while at the\n    same time the user is browsing the same server's other HTML documents,\n    all in one connection.\n    \nThis is the kind of situation where I could imagine it would be\nbetter to use two TCP connections, instead of trying to multiplex\none.  That is, if you have enough data flow to amortize the costs\nof two connections, then it's probably worth following that relatively\nsimple approach, instead of going through all the work of implementing\nHTTP-NG.\n\n    Yes, and to that end we have to keep in mind that while <img width=N\n    height=M> is useful, it means that in order to see that performance\n    win, everyone has to edit their HTML. That will take time, and\n    Netscape's perceived performance works everywhere without having to\n    edit (or parse) every single HTML doc on a server...\n\nThis is only true if one believes that Netscape's perceived performance\nis independent of network dynamics.  Which is, I believe, an open question.\nMy own intuition is that we can follow two paths:\n(1) Stick with the parallel connection approach, which gives\nreasonably good performance today but which might turn into\na global performance disaster in the future.\n(2) Encourage people to shift ASAP to an alternative (besides\nthe WIDTH and HEIGHT tags, several HTTP-level mechanisms have\nbeen proposed).  This might avert future global problems, but\nin the short term it might be a while before enough servers\nsupport it to make it highly beneficial.  I would also expect\nthat in the long run (i.e., with widespread support), it would\nimprove UPP because you wouldn't have to wait even for the initial\nbits of the images to arrive.\n\nThis dilemma is similar to the global warming issue: do we reduce CO2\nemissions now, or do we wait and see if the icecaps really will melt?\nAnd are there other benefits to reducing emissions, beyond averting\nglobal warming?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: TIME_WAI",
            "content": "At 6:18 PM 12/20/94, Jeffrey Mogul wrote:\n>    I still think that it is better to keep the connection open for a small\n>    amount of time as the server socket will go into TIME_WAIT state when\n>    it is closed and then can't be reused before about 240 seconds\n>    (recommended timeout accorsing to Simon)\n>\n>This not just Simon's recommendation.  This is a MUST (i.e., mandatory)\n>requirement of RFC1122 (section 4.2.2.13).\n>\n>And, to clarify: what cannot be reused is the memory allocated to the\n>connection state record; all other resources are reusable.  However, in\n>most BSD-based systems, this memory usage can be significant (one\n>mbuf).\n\nJust a reminder. This issue only really applies to Unix implementations,\nand not O/S2, VMS, Windows, Windows NT, Mac O/S, VM, or any other non-Unix\nserver and/or TCP/IP implementations not descended from the BSD network\nkernel.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "On Tue, 20 Dec 1994, Eric W. Sink wrote:\n> The following section from Draft 01 seems odd.\n> \n> >6.4.2 WWW-Authenticate\n> >\n> >   The WWW-Authenticate header field must be included as part of the\n> >   response if the server sends back a \"401 Unauthorized\" Status-Code\n> >   on a request from the client as part of the Basic Authentication\n> >   Scheme described in Section 9. This header field indicates the\n> >   authentication scheme in use and the realm in which the requested\n> >   URI belongs.\n> >\n> >       WWW-Authenticate        = \"WWW-Authenticate\" \":\" (\n> >                                   ( \"Basic\" realm )\n> >                                 | ( extension-scheme realm ) )\n> >\n> >       realm                   = \"Realm\" \"=\" 1#( \"<\" URI \">\" )\n> >\n> >   The first word of the field value identifies the authorization\n> >   scheme in use and is followed by the realm of the protected URI.\n> >   The realm is a comma separated list of URIs, where relative URLs\n> >   should be interpreted relative to the URI of the requested resource\n> >   in the RequestLine. If a request is authenticated and a realm\n> >   specified, the User-ID and password should be valid for all other\n> >   requests within this realm.\n> >\n> >       Note: The realm may span more than one origin server.\n> \n> I'm not aware that people generally use the realm as a comma separated\n> list of URIs.  I remember hearing someone say at the BOF that it would\n> be nice to be able to specify that URIs on other machines can be accessed\n> as part of the same realm.  Perhaps this section has been phrased\n> accordingly?\n\nDat waz me :)  The documentation for NCSA's httpd states (in \nhttp://hoohoo.ncsa.uiuc.edu/docs/setup/access/AuthName.html): \n\n\"The AuthName directive sets the name of the authorization realm for this\ndirectory. This realm is a name given to users so they know which username\nand password to send.\"\n\n...meaning right now at least this server interprets Realm to just be a\nstring of characters, a name, that defines the authentication space on a\ngiven host.  I.e. hostname + realm string = authentication realm.  Some\nbrowsers are generous in presuming what other docs might also be under\nauthentication, for example resending the name and password with every\nsubsequent access to the same machine, or only to within the same\ndirectory as previously authenticated areas.  The CERN docs\n(http://info.cern.ch/hypertext/WWW/AccessAuthorization/Overview.html) were\nnever clear on how the browser should determine ahead of time if a request\nshould have the authentication info sent along by default, given the\nhistory of which documents needed authentication - the realm string was\nall that was supposed to be used.  Some conservative browsers only go by\nsubdirectory - an authenticated request under /dirone doesn't mean they\nshould resend the authentication info for an access in /dirtwo, but do so\nimmediately after getting the 401 message.  This is fine by me, since the \nuser still only types his password in once.  Some even more conservative\nbrowsers consider hostname + path + realm string to be the definition of\nauthentication realm, and thus require reauthentication between different\nsubdirs on the same host.  This is something I've run into here (our root\nlevel is unprotected but almost all subdirs are), and since the most\ncommon browsers don't require complete reauthentication between\ndirectories and the CERN documentation is vague I don't think I'm in\nviolation, but this system is clearly a mess. \n\nIf, instead of a string, we specified exactly where (both by hostname and\nby path) the password authentication is valid, there's be no ambiguity as\nto when the browser automatically resends the authentication information. \n\n> Was this section written to reflect current practice, or was it written to\n> be better than current practice?\n\nThe above clearly does not describe current practice, but is definitely \nsomething I'd like to see in 1.1.  \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "list of URIs in authorization real",
            "content": "Eric W. Sink writes:\n\n> I'm not aware that people generally use the realm as a comma separated\n> list of URIs.  I remember hearing someone say at the BOF that it would\n> be nice to be able to specify that URIs on other machines can be accessed\n> as part of the same realm.  Perhaps this section has been phrased\n> accordingly?\n> \n> Was this section written to reflect current practice, or was it written to\n> be better than current practice?\n\nI think Henrik got a little carried-away here -- it was a last minute\nchange prompted by a very good idea, yet I believe it to be in conflict\nwith current practice (and prior specs) and thus will remove it on the\nnext draft.\n\nIt will probably be proposed for HTTP/1.1.\n\nBTW, if anyone sees something which is in the 1.0 spec, but which\nconflicts with current practice in a way that cannot be described\nas a \"bug\" in the implementation, please bring it to the attention of\nthe list.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "John Franks writes:\n\n> According to Roy T. Fielding:\n>> \n>> Actually, I have not yet proposed any meaningful semantics for a SESSION\n>> method other than it cannot be forwarded.\n> \n> I noticed.  This makes it difficult to compare to an MGET proposal.  I am\n> eagerly awaiting some description of how SESSION and Connection headers are\n> going to achieve the same user perceived performance as multiple connections.\n\nQuite frankly, I don't give a damn at the present time whether SESSION\nand/or \"Connection: keep-alive\" achieve the same user perceived performance\nas multiple connections -- that has absolutely nothing to do with why they\nmust be recognized by 1.0 proxies, and thus why they should be in the 1.0 spec.\nThat is the only discussion I am interested in at the moment, because\nI have no time right now to rehash all the discussions that were held in\nthe meetings and hallways of the IETF -- maybe later, but not until the\n1.0 spec is reasonably stable and my mail inbox becomes livable.\n\nThe reality is that there must be some way to enable in-band communication\nwith an agent's nearest-neighbor in the communication chain, while at the\nsame time being reasonably assured that the communication is not mistakenly\nforwarded down the communication chain.  The question is NOT why we need\nthat communication, or what the communication will entail, but what\nmechanism will be used to transport it and in which version of the protocol\n(1.0 or 1.1) it should first appear.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "At 1:09 AM 12/21/94, Brian Behlendorf wrote:\n>If, instead of a string, we specified exactly where (both by hostname and\n>by path) the password authentication is valid, there's be no ambiguity as\n>to when the browser automatically resends the authentication information.\n\nThis is predicated on the assumption that all data being served is coming\nfrom a traditional file system and that's what the path represents. It\nwould be very tedious to have to specify authentication rules for every\nrecord in a database, for example. A specific server (or client)\nimplementation of HOW authentication realms are defined doesn't seem\ngermain to the HTTP/1.0 discussion. What a particular string in a httpd\nconfig file means is up to a specific server implementation to determine,\nnot the HTTP/1.0 standard.\n\nThe current standard is clear about when to send an authentication header.\nIt's sent in response to an authentication request from a server. In an\nentirely separate transaction, the server receives the second request and\nvalidates whether or not the authentication info is valid. This allows the\nclient to send authentication info whenever the CLIENT feels it is\nrequired. I'm not sure I understand why you care if a client sends improper\nauthentication info for a realm it has wandered into. It is equivalent to\nsending no information at all, which is acceptable.\n\nHow the server is mapping configuration info for security (a \"string\") to\nthe actual authentication of data it serves is completely transparent to\nthe client. Even a scheme as simple as having the client always send the\nlast valid authentication info to the same server until it fails for a\nlink, then switching to the new info supplied by the user, works just fine.\nWhether a server interprets a \"AuthName\" command as a string to match, an\nexact path, a complete URL, or something entirely different is something\nfor a NCSA httpd server to decide. All the client needs to know is that the\nauthentication info it supplied is invalid. Don't forget that httpd is only\none of MANY server implementations, and they don't all define\nauthentication realms the same way, or with the same definition of what\nconstitutes a realm.\n\nYour assumption that \"realms\" have some meaning in the context of the\nHTTP/1.0 protocol is probably forcing the standard to do something that is\nproperly the domain of client and server implementations of security and\nnot their HTTP implementation. Namely, define what constitutes a security\nrealm for all client and server implementations. Given the infinite variety\nand structure of data that can be served via HTTP, it seems inappropriate\nto try to accomodate a specific interpretation of what constitutes a\n\"realm\" into the standard.\n\nCurrent clients make a generic assumption based on the hierarchy present in\na URL (and this does NOT mean they are assuming that these are\nsubdirectories in a file system) regarding when to send the last used\nauthentication info. This is all they can safely do, given the requirement\nthat the interpretation of a path portion of a URL is the sole domain of a\nserver. Clients that try to interpret a URL as a series of Unix\nsubdirectories ending in a file name and then do something cute with that\ninfo won't last very long with the servers coming down the pipe.\n\nThe same is true for realm interpretations. As far as the client is\nconcerned, the realm info present in the server's authentication reply\nserves two purposes. The first is as a prompt for the client user. The\nsecond is as a way to easily look up authentication already entered that\nmay be maintained internally to the client at runtime.\n\n>> Was this section written to reflect current practice, or was it written to\n>> be better than current practice?\n>\n>The above clearly does not describe current practice, but is definitely\n>something I'd like to see in 1.1.\n\nNo, it doesn't reflect current practice. It does reflect an opinion of a\nfuture *implementation*. There's nothing wrong with discussing future\nimplementation possibilities in the context of the HTTP standard. But,\ntrying to capture implementation specific details of HTTP clients and\nservers in a protocol standard is different than trying to capture the\nsemantics of the authentication exchange and it shouldn't be done.\n\nTo sum up. Clients cannot know the authentication info they send (or fail\nto send) is inappropriate for the realm they are accessing until they try.\nSo, clients make some smart assumptions about when to send authentication\ninfo to avoid hassling users with password requests for every URL. What\nconstitutes a \"realm\" is an implementation-specific detail of the SERVER\nand the client cannot know how the server has partitioned its data with\nrespect to security. This is just good practice when trying to develop\nsecure systems. Clients are only told that their access attempt failed. Not\nwhy, or what the actual structure on the server looks like. Just that they\nfailed. Given this, it seems impossible to define ahead of time WHEN a\nclient should send authentication info. I think the current practice, and\ncurrent HTTP/1.0 standard accomodates these requirements just fine.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Proposal for new authentication schem",
            "content": "At the BOF in San Jose, it was mentioned that a new authentication scheme\nwas desired for inclusion in HTTP/1.1.  We have been specifying and\nprototyping such a scheme, so we wrote it up as a proposal for discussion\nin the group and inclusion in HTTP/1.1.\n\n--\n\n[PROPOSED] HTTP Working Group                       Jeffery L. Hostetler\nINTERNET-DRAFT                                              Eric W. Sink\n<draft-NOT_YET_SUBMITTED-simplemd5-aa-00.txt>\nExpires SIX MONTHS FROM--->                            December 21, 1994\n\n      A Proposed Extension to HTTP : SimpleMD5 Access Authentication\n\nStatus of this Memo\n\n   This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time. It is inappropriate to use Internet-\n   Drafts as reference material or to cite them other than as\n   \"work in progress.\"\n\n   To learn the current status of any Internet-Draft, please check\n   the \"1id-abstracts.txt\" listing contained in the Internet-\n   Drafts Shadow Directories on ds.internic.net (US East Coast),\n   nic.nordu.net (Europe), ftp.isi.edu (US West Coast), or\n   munnari.oz.au (Pacific Rim).\n\n   Distribution of this document is unlimited. Please send comments\n   to the proposed HTTP working group at <http-wg@cuckoo.hpl.hp.com>.\n   Discussions of the working group are archived at\n   <URL:http://www.ics.uci.edu/pub/ietf/http/>. General discussions\n   about HTTP and the applications which use HTTP should take place\n   on the <www-talk@info.cern.ch> mailing list.\n\nAbstract\n\n   The protocol referred to as \"HTTP/1.0\" includes specification\n   for a Basic Access Authentication scheme.  This scheme is not\n   considered to be a secure method of user authentication, as the\n   user name and password are passed over the network in an\n   unencrypted form.  A specification for a new authentication scheme\n   is needed for future versions of the HTTP protocol.  This document\n   provides specification for such a scheme, referred to as \"SimpleMD5\n   Access Authentication\".  The encryption method used is the RSA Data\n   Security, Inc. MD5 Message-Digest Algorithm.\n\nTable of Contents\n\n   1.  Introduction\n       1.1  Purpose\n       1.2  Overall Operation\n   2.  Basic Access Authentication Scheme\n       2.1  Specification\n       2.2  Security protocol negotiation\n       2.3  Example\n   3.  Acknowledgments\n   4.  References\n   5.  Authors Addresses\n\n\n1. Introduction\n\n1.1  Purpose\n\n   The protocol referred to as \"HTTP/1.0\" includes specification\n   for a Basic Access Authentication scheme[1].  This scheme is not\n   considered to be a secure method of user authentication, as the\n   user name and password are passed over the network in an\n   unencrypted form.  A specification for a new authentication scheme\n   is needed for future versions of the HTTP protocol.  This document\n   provides specification for such a scheme, referred to as \"SimpleMD5\n   Access Authentication\".\n\n   The SimpleMD5 Access Authentication scheme is not intended to be\n   a complete answer to the need for security in the World Wide Web.\n   This scheme provides no encryption of object content.  The intent\n   is simply to facilitate secure access authentication.\n\n   It is proposed that this access authentication scheme be included\n   in the the proposed HTTP/1.1 specification.\n\n1.2  Overall Operation\n\n   Like Basic Access Authentication, the SimpleMD5 scheme is based on\n   a simple challenge-response paradigm.  The SimpleMD5 scheme challenges\n   using a nonce value.  A valid response contains the MD5 checksum of\n   the password and the given nonce value.  In this way, the password\n   is never sent in the clear.  Just as with the Basic scheme, the\n   username and password must be prearranged in some fashion.\n\n\n2. SimpleMD5 Access Authentication Scheme\n\n2.1 Specification\n\n   The SimpleMD5 Access Authentication scheme is conceptually similar to\n   the Basic scheme.  The formats of the modified WWW-Authenticate header line\n   and the Authorization header line are specified below.\n\n   If a server receives a request for an access-protected object,\n   and an acceptable Authorizatation header is not sent, the server\n   responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: SimpleMD5     realm=\"<realm>\",\n                                                        domain=\"<domain>\",\n                                                        nonce=\"<nonce>\",\n                                                        opaque=\"<opaque>\",\n                                                        oldNonce=\"<true |\nfalse>\"\n\n   The client is expected to retry the request, passing an Authorization header\n   line as follows:\n\nAuthorization: SimpleMD5        username=\"<username>\",\n                                                        realm=\"<realm>\",\n                                                        nonce=\"<nonce>\",\n\nresponse=\"<MD5response>\",\n                                                        opaque=\"<opaque>\"\n\n   The meanings of the identifers used above are as follows:\n\n      <realm>\n         A name given to users so they know which username and password\n         to send.\n\n      <domain>\n         A comma separated list of URIs, as specified for HTTP/1.0.  The\n         intent is that the client could use this information to know the\n         set of URIs for which the same authentication information should be\n         sent.  The URIs in this list may exist on different servers.  If\n         this keyword is omitted or empty, the client should assume that\n         the domain consists of all URIs on the responding server.\n\n      <nonce>\n         A server-specified integer value which may be uniquely generated\neach time\n         a 401 response is made.  Servers may defend themselves against replay\n         attacks by refusing to reuse nonce values.\n\n      <opaque>\n         A string of data, specified by the server, which should returned by\n         the client unchanged.  It is recommended that this string be\n         base64 or hexadecimal data.  Specifically, since the string is passed\n         in the header lines as a quoted string, the double-quote character\n         is not allowed.\n\n      <username>\n         It is assumed that both client and server know of a prearranged\n         username and password pair.  The Authorization header returned\n         by the client specifies the username.  The password is not specified\n         in the clear.\n\n      <response>\n         The MD5 encoding of \"<nonce> <password>\".  The resulting string\n         should be a 32 digit hexadecimal string.\n\n      <oldNonce>\n         A flag, indicating that the previous request from the client\n         was rejected because the nonce value was stale.  If oldNonce\n         is TRUE, the client may wish to simply retry the request with\n         a new encrypted response, without reprompting the user for a\n         new username and password.\n\n   When constructing the WWW-Authenticate header, the domain, opaque,\n   and oldNonce keywords may be omitted.\n\n   All keyword-value pairs must be expressed in characters from the\n   US-ASCII character set, excluding control characters.\n\n   As with the basic scheme, proxies must be completely transparent in\n   the SimpleMD5 access authentication scheme. That is, they must forward the\n   WWW-Authenticate and Authorization headers untouched. If a proxy\n   wants to authenticate a client before a request is forwarded to\n   the server, it can be done using the Proxy-Authenticate and\n   Proxy-Authorization headers.\n\n2.2 Security Protocol Negotiation\n\n   It is useful for a server to be able to know which security schemes\n   a client is capable of handling.  It is recommended that the HTTP extension\n   mechanism proposed by Dave Kristol [2] be used.  If the client includes\n   the following header line with the request, then a server can safely assume\n   that the client can handle SimpleMD5 authentication.\n\nExtension: Security SimpleMD5\n\n   If this proposal is accepted as a required part of the HTTP/1.1\n   specification, then a server may assume SimpleMD5 support when a client\n   identifies itself as HTTP/1.1 compliant, by sending:\n\nHTTP-Version: HTTP/1.1\n\n   It is possible that a server may want to require SimpleMD5 as its\n   authentication method, even if the server does not know that the client\n   supports it.  A client is encouraged to fail gracefully if the server\n   specifies any authorization scheme it cannot handle.\n\n2.3 Example\n\nThe following example assumes that an access-protected document is being\nrequested from the server.  Both client and server know that the username\nfor this document is \"Mufasa\", and the password is \"CircleOfLife\".\n\nThe first time the client requests the document, no Authorization header\nis sent, so the server responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: SimpleMD5     realm=\"PrideRock\",\n                                                        domain=\"\",\n                                                        nonce=\"67654464\",\n\nopaque=\"76da3afcb7c9a0\",\n                                                        oldNonce=\"false\"\n\nThe client may prompt the user for the username and password, after which it\nwill respond with a new request, including the following Authorization header:\n\nAuthorization: SimpleMD5        username=\"Mufasa\",\n                                                        realm=\"PrideRock\",\n                                                        nonce=\"67654464\",\n\nresponse=\"9524c2516e37df5c6a3c7ef5e334a31b\",\n                                                        opaque=\"76da3afcb7c9a0\"\n\nNote that the hexadecimal string for the response is the MD5 encoding of\n\"67654464 CircleOfLife\".\n\n3. Acknowledgments\n\n   Larry Stewart, at OpenMarket, Inc., contributed to the development and\n   implementation of this authorization scheme.\n\n   Source code in C for the RSA Data Security, Inc. MD5 Message-Digest\n   Algorithm is available free of charge from RSA Data Security, Inc.\n\n4. References\n\n   [1]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen.\n        \"Hypertext Transfer Protocol -- HTTP/1.0\"\n        Internet-Draft (work in progress), UC Irvine,\n        <URL:http://ds.internic.net/internet-drafts/\n        draft-fielding-http-spec-01.txt>, December 1994.\n\n   [2]  D. Kristol. \"A Proposed Extension Mechanism for HTTP\"\n        <URL:http://www.research.att.com/~dmk/extend.txt>,\n        December 1994.\n\n5. Authors Addresses\n\n   Jeffery L. Hostetler\n   jeff@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   1800 Woodfield Drive\n   Savoy, IL  61874\n\n   Eric W. Sink\n   eric@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   1800 Woodfield Drive\n   Savoy, IL  61874\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\"Can I get a direct flight back to reality, or do I have to change planes\nin Denver?\" - The Santa Clause\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "On Wed, 21 Dec 1994, Chuck Shotton wrote:\n> At 1:09 AM 12/21/94, Brian Behlendorf wrote:\n> >If, instead of a string, we specified exactly where (both by hostname and\n> >by path) the password authentication is valid, there's be no ambiguity as\n> >to when the browser automatically resends the authentication information.\n> \n> This is predicated on the assumption that all data being served is coming\n> from a traditional file system and that's what the path represents. It\n> would be very tedious to have to specify authentication rules for every\n> record in a database, for example. A specific server (or client)\n> implementation of HOW authentication realms are defined doesn't seem\n> germain to the HTTP/1.0 discussion. What a particular string in a httpd\n> config file means is up to a specific server implementation to determine,\n> not the HTTP/1.0 standard.\n\nHTTP/1.0 is documenting current practice, and I outlined what current \npractice is and how it was so badly broken and what needs to be fixed.  I \ntotally agree that we shouldn't tie the definition to a server to the way \na normal filesystem works by looking at a path - do we have a definition \nyet for wildcard characters in URI's?\n\n> The current standard is clear about when to send an authentication header.\n> It's sent in response to an authentication request from a server. In an\n> entirely separate transaction, the server receives the second request and\n> validates whether or not the authentication info is valid. This allows the\n> client to send authentication info whenever the CLIENT feels it is\n> required. \n\nIf browsers actually did this, almost every file reteival on our site \nwould require two separate connections.  If this negotiation were to go \non within one session, as hopefully HTTP/1.1 will allow it to, then there \nwould be no problem.  However it is very useful, at least in the current \nsetup, for a browser to know ahead of time that an access to a particular \nURL will require an authentication header, thus elimanating the 401 error \ncode and retry step.  \n\n> I'm not sure I understand why you care if a client sends improper\n> authentication info for a realm it has wandered into. It is equivalent to\n> sending no information at all, which is acceptable.\n\nBut then what happens?  Does the browser 1) retry a previous password, or \n2) prompt the user for another password?  If it's 1), then we need some \nheuristic for determining which previous password to use, and sending the \nwrong password would be a security risk.  A simple string as the realm \nkey is dangerous - the only secure way is \"okay, you've connected to me, \nnow you may use this same password here, here, and here\".  2) is \nunacceptible for those who want to create a seamless authentication space.\n\n> How the server is mapping configuration info for security (a \"string\") to\n> the actual authentication of data it serves is completely transparent to\n> the client. \n\nBut it's not, in current practice.  Try this.  Telnet to www.hotwired.com \nport 80, and issue the command \"GET /Login/ HTTP/1.0\\n\\n\".  You'll see:\n\nWWW-Authenticate: Basic realm=\"HotWired!\"\n\nWhen clients get that the first time, they usually ask the user \"enter \nyour name and password for \"HotWired!\"\".  So, I don't see how you can say \nit's transparent.\n\n> Whether a server interprets a \"AuthName\" command as a string to match, an\n> exact path, a complete URL, or something entirely different is something\n> for a NCSA httpd server to decide. All the client needs to know is that the\n> authentication info it supplied is invalid. Don't forget that httpd is only\n> one of MANY server implementations, and they don't all define\n> authentication realms the same way, or with the same definition of what\n> constitutes a realm.\n\nThat's not current practice.  I can set up two different realms on my \nserver here, set up links between them, and users should only ever have \nto type their passwords twice to bounce between them.  The client \ndistinguishes between them by the realm string.  That just happens to be \nthe way things are defined now.\n\n> Your assumption that \"realms\" have some meaning in the context of the\n> HTTP/1.0 protocol is probably forcing the standard to do something that is\n> properly the domain of client and server implementations of security and\n> not their HTTP implementation. Namely, define what constitutes a security\n> realm for all client and server implementations. Given the infinite variety\n> and structure of data that can be served via HTTP, it seems inappropriate\n> to try to accomodate a specific interpretation of what constitutes a\n> \"realm\" into the standard.\n\nIf you're saying turn it over to the HTTP-security working group, fine.  \nIf you're saying it doesn't belong in HTTP, I disagree.  \n\n...\n> As far as the client is\n> concerned, the realm info present in the server's authentication reply\n> serves two purposes. The first is as a prompt for the client user. The\n> second is as a way to easily look up authentication already entered that\n> may be maintained internally to the client at runtime.\n\nAnd for HTTP/1.1 we're trying to make the process more intelligent, since\nthe heuristic for that \"look up\" and retry step is too vague right now. If\nwe can get beyond the simple request-response-closeconnection mode to\nrequest-needmoreinfo-newrequest-response-closeconnection then the need for\na realm is *almost* abrogated, since the client doesn't have to guess\nwhich areas need authentication to save a transaction.  There's still a\nneed for the client to know when a realm is the same as a previously\nauthenticated area, and when it needs to ask the user for another\npassword.  This is very important - giving the same password over and over\nfor the same logical area, whether it's a collection of directories on the\nsame machine or a particular collection of directories on remote machines,\nis considered a *bug* by the user. \n\n> To sum up. Clients cannot know the authentication info they send (or fail\n> to send) is inappropriate for the realm they are accessing until they try.\n> So, clients make some smart assumptions about when to send authentication\n> info to avoid hassling users with password requests for every URL. What\n> constitutes a \"realm\" is an implementation-specific detail of the SERVER\n> and the client cannot know how the server has partitioned its data with\n> respect to security. This is just good practice when trying to develop\n> secure systems. Clients are only told that their access attempt failed. Not\n> why, or what the actual structure on the server looks like. Just that they\n> failed. Given this, it seems impossible to define ahead of time WHEN a\n> client should send authentication info. I think the current practice, and\n> current HTTP/1.0 standard accomodates these requirements just fine.\n\nWe're defining current practice, which is clearly different from what \nChuck thinks is the best way to do this.  Current practice is what allows \nour server to only require the user to enter their password once for \nalmost all actions, and whether Chuck believes it or not clients do use \nthe realm string to differentiate between authentication areas.  Future \nversions of the spec should expand upon this instead of deprecating it - \nI think we all agree that requiring the user to type the same name and \npassword for every access to the same authentication space is a bad idea, \nyet being \"promiscuous\" in throwing names and passwords about is also \nundesireable.\n\nSo, here's my attempt at rewriting that part of the HTTP/1.0 spec to meet \ncurrent practice (my edited lines marked with a *):\n\n--------------------------------------------------------------------------\n\n6.4.2 WWW-Authenticate\n\n   The WWW-Authenticate header field must be included as part of the \n   response if the server sends back a \"401 Unauthorized\" Status-Code \n   on a request from the client as part of the Basic Authentication \n   Scheme described in Section 9. This header field indicates the \n   authentication scheme in use and the realm in which the requested \n   URI belongs.\n\n       WWW-Authenticate        = \"WWW-Authenticate\" \":\" (\n                                   ( \"Basic\" realm )\n                                 | ( extension-scheme realm ) )\n\n*      realm                   = \"Realm\" \"=\" *text\n\n*   The first word of the field value identifies the authorization \n*   scheme in use and is followed by the name of the authorization \n*   realm, which in combination with the hostname of the machine\n*   being accessed defines the authorization space. If a request is \n*   authenticated and a realm specified, the User-ID and password \n*   can be assumed to be valid for all other requests within this \n*   authentication space - thus, any 401 error response to a \n*   non-authenticated request that specifies this realm \n*   should be immediately retried with the name and password for\n*   that realm.  \n\n*       Note: The realm may not span more than one hostname.\n\n-------------------------------------------------------------------------\n\nOkay, I think that defines current practice better than what was there. \nIt doesn't make any suggestions about when clients should try and guess\nthat a given URL will need authentication ahead of time, but it also\npreserves the enter-password-just-once mechanism.  Browser writers should\nwrite their code carefully to avoid a retry loop like the one Netscape had\nin one of their early betas. \n\nDoes this solve it?\n\nBrian\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": ">> The current standard is clear about when to send an authentication header.\n>> It's sent in response to an authentication request from a server. In an\n>> entirely separate transaction, the server receives the second request and\n>> validates whether or not the authentication info is valid. This allows the\n>> client to send authentication info whenever the CLIENT feels it is\n>> required.\n>\n>If browsers actually did this, almost every file reteival on our site\n>would require two separate connections.\n\nBrowsers work this way now and it only requires n+1 connections for n\nrequests to the same realm, with the extra connection being the first one\nthat failed and required that the server respond with a \"Not Authorized.\"\nAre you sure you're clear on how authentication handshaking currently\nworks? Maybe I wasn't typing clearly. It WAS early in the morning!\n\n>> I'm not sure I understand why you care if a client sends improper\n>> authentication info for a realm it has wandered into. It is equivalent to\n>> sending no information at all, which is acceptable.\n>\n>But then what happens?\n\nThe server tells the client that it needs to authenticate for the current\nrealm. If the client doesn't have a current password for the realm, it must\nprompt the user. This is what all clients do now. Do you feel this needs to\nchange? If so, how and why? This is as simple a handshake as can be defined\nand I cannot see how you can make it any more efficient.\n\nClient says, \"Let me have this file\"\nServer says, \"No, give me a valid password first.\"\nClient says, \"User, gimme a password\"\nClient says, \"Server, gimme the file, here's the password.\"\nServer says, \"OK\"\n  -or-\nServer says, \"No, give me a password first.\", and the process repeats.\n\nOR, the client already has a password for what it thinks is the current\nrealm. Then,\n\nClient says, \"Let me have this file, here's a password\"\nServer says, \"OK\"\n  -or-\nServer says, \"No, give me a valid password first.\", and the process repeats\nas in the first example.\n\n\n>Does the browser 1) retry a previous password, or\n>2) prompt the user for another password?  If it's 1), then we need some\n>heuristic for determining which previous password to use\n\nThat's what the REALM is for.\n\n>, and sending the\n>wrong password would be a security risk.\n\nSending ANY password is a security risk. I see what you are getting at,\nhowever. It's only a REAL risk if the realm spans multiple servers. In\npractical, real-world terms, I can only see realms that span multiple\nservers as being huge, administrative nightmares. Why not just implement a\nKerberized (or YP) server and let all the distributed servers share a\npassword file?\n\n> A simple string as the realm key is dangerous\n\nYou are obviously attaching more importance to the realm than I am. My\ninterpretation is that a realm is simply a way to subdivide the content\nprovided by a single server (or common content across multiple servers). It\ngives a user a hint about which password to type in, and it tells a client\nwhich cached password it should try. As far as clients are concerned, it's\nsimply a key for a table look-up or a prompt string for the user. Is there\nmore to it than that?\n\n> - the only secure way is \"okay, you've connected to me,\n>now you may use this same password here, here, and here\".  2) is\n>unacceptible for those who want to create a seamless authentication space.\n\nWhat exactly is \"seamless authentication space\", and from whose perspective\nis it seamless? The user, the client, or the server?\n\n>> How the server is mapping configuration info for security (a \"string\") to\n>> the actual authentication of data it serves is completely transparent to\n>> the client.\n>\n>But it's not, in current practice.  Try this.  Telnet to www.hotwired.com\n>port 80, and issue the command \"GET /Login/ HTTP/1.0\\n\\n\".  You'll see:\n>\n>WWW-Authenticate: Basic realm=\"HotWired!\"\n>When clients get that the first time, they usually ask the user \"enter\n>your name and password for \"HotWired!\"\".  So, I don't see how you can say\n>it's transparent.\n>\n\nTo WWW client software, it is a meaningless string of characters to be\ncompared with other meaningless strings or displayed to a user. It's\nsemantic meaning to a WWW server is invisible (transparent) to the client.\nThe fact that a user can see it with a telnet session is irrelevant. It\nreveals nothing about the security of the server, or what that string\nrelates to on a server.\n\n>> Whether a server interprets a \"AuthName\" command as a string to match, an\n>> exact path, a complete URL, or something entirely different is something\n>> for a NCSA httpd server to decide. All the client needs to know is that the\n>> authentication info it supplied is invalid. Don't forget that httpd is only\n>> one of MANY server implementations, and they don't all define\n>> authentication realms the same way, or with the same definition of what\n>> constitutes a realm.\n>\n>That's not current practice.  I can set up two different realms on my\n>server here, set up links between them, and users should only ever have\n>to type their passwords twice to bounce between them.\n\nYou are confusing the behavior of a specific server implementation, Unix\nfile system specifics, and the logical concept of \"realms\" in the HTTP\nstandard. It's not the HTTP standard's fault if httpd is not implemented\nconsistently.\n\n>  The client\n>distinguishes between them by the realm string.  That just happens to be\n>the way things are defined now.\n\nWhat else is the realm string for, as far as the client is concerned? I\nfeel like I must be missing something obvious here, because it appears very\nsimple.\n\n>> Your assumption that \"realms\" have some meaning in the context of the\n>> HTTP/1.0 protocol is probably forcing the standard to do something that is\n>> properly the domain of client and server implementations of security and\n>> not their HTTP implementation. Namely, define what constitutes a security\n>> realm for all client and server implementations. Given the infinite variety\n>> and structure of data that can be served via HTTP, it seems inappropriate\n>> to try to accomodate a specific interpretation of what constitutes a\n>> \"realm\" into the standard.\n>\n>If you're saying turn it over to the HTTP-security working group, fine.\n>If you're saying it doesn't belong in HTTP, I disagree.\n\nI'm saying that the semantics of what a realm string means to a server is\nimplementation specific, just like what the path portion of a URL means to\na server is implementation specific. You are saying (as best I can tell)\nthat the implementation details of realms should become part of the HTTP\nstandard. I'm saying that what a server maps realm \"strings\" to is none of\nthe client's business, is semantically meaningless to the client, and\ntherefore is no more part of the HTTP standard than how to interpret the\npath portion of URLs is.\n\n>...\n>> As far as the client is\n>> concerned, the realm info present in the server's authentication reply\n>> serves two purposes. The first is as a prompt for the client user. The\n>> second is as a way to easily look up authentication already entered that\n>> may be maintained internally to the client at runtime.\n>\n>And for HTTP/1.1 we're trying to make the process more intelligent, since\n>the heuristic for that \"look up\" and retry step is too vague right now.\n\nPlease elaborate on what is vague about them. It seems relatively\nstraightforward to implement both clients and servers from the current\nstandard. It wasn't hard when I tried it, and wasn't at all ambiguous,\nunless you mean that the term \"realm\" is imprecise with respect to its\nmapping to one/many servers.\n\n>We're defining current practice, which is clearly different from what\n>Chuck thinks is the best way to do this.\n\nI must have been drunk when I was typing this morning. What I thought I\ntyped was a very clear description of current practice, in response to\nsomething I saw as an inappropriate direction for a future standard. I am\nfairly certain that I understand what current practice is. I know I\nunderstand the challenge/response exchange that happens between clients and\nservers, because I've built it more than once.\n\n> Current practice is what allows\n>our server to only require the user to enter their password once for\n>almost all actions, and whether Chuck believes it or not clients do use\n>the realm string to differentiate between authentication areas.\n\nI said this repeatedly in my previous post. I agree 100% that clients use\nthe realm string to determine what password to send. It's just that it has\nno meaning to the client in terms of how the server implements the division\nof data amongst realms. It is simply a key into a table of passwords that\nhave previously been entered by the user. Was this confusing in my earlier\nletter?\n\n>  Future\n>versions of the spec should expand upon this instead of deprecating it -\n>I think we all agree that requiring the user to type the same name and\n>password for every access to the same authentication space is a bad idea,\n>yet being \"promiscuous\" in throwing names and passwords about is also\n>undesireable.\n\nI am unaware of any current client that repeatedly asks users for passwords\nwhen accessing files in the same authentication space. However, I disagree\nthat throwing bad passwords at the SAME server is much of a security risk.\nIf realms span servers, yes, I agree. But in CURRENT PRACTICE, this isn't\nthe case.\n\n[snip]\n>*       Note: The realm may not span more than one hostname.\n\nAgreed.\n\n>\n>Okay, I think that defines current practice better than what was there.\n>It doesn't make any suggestions about when clients should try and guess\n>that a given URL will need authentication ahead of time, but it also\n>preserves the enter-password-just-once mechanism.  Browser writers should\n>write their code carefully to avoid a retry loop like the one Netscape had\n>in one of their early betas.\n>\n>Does this solve it?\n\nLooks fine to me. (Were we agreeing all along? I hate it when that happens!)\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "On Wed, 21 Dec 1994, Chuck Shotton wrote:\n[my proposed changes to the spec]\n> >Does this solve it?\n> \n> Looks fine to me. (Were we agreeing all along? I hate it when that happens!)\n\nYes, I think we were, just naggling on terminology and which direction it \nshould be developed in the future.  As an easy way out, for 1.1 we could \ndo what eric sink's MD5 proposal does and define a \"domain\" tag that \ndefines with a list of URI's the real authentication space.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nYour slick hype/tripe/wipedisk/zipped/zippy/whine/online/sign.on.the.ish/oil\npill/roadkill/grease.slick/neat.trick is great for what it is. -- Wired Fan #3\n brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "On Sun, 18 Dec 1994, John Franks wrote:\n\n> According to Rob McCool:\n> > \n> > Yes, and to that end we have to keep in mind that while <img width=N\n> > height=M> is useful, it means that in order to see that performance\n> > win, everyone has to edit their HTML. That will take time, and\n> > Netscape's perceived performance works everywhere without having to\n> > edit (or parse) every single HTML doc on a server...\n> \n> This is a very important point and one that I hadn't fully considered.\n> Since the WN server keeps information about files in a data base and\n> parses the header of an HTML doc when it is entered in the data base\n> (to extract title and <meta> information like Keywords and Expires) I\n> had in mind parsing the whole doc at that time for information about\n> images.  But, but I am not sure how NCSA and CERN httpd would handle\n> this.  Since the vast majority of HTML docs served on the Web are\n> served by NCSA and CERN httpds we need something that can easily be\n> made to work with them.  I don't think it is reasonable to expect\n> authors to add the height and width.  Well, it may be reasonable, but\n> it is not realistic.\n\nNo, not reasonable.  Too much opportunity for information to be out of\ndate or impossible for the user to obtain.  Even tools are problematic\nsince the image may not be local to the user.  I'm hearing my consulting\nclients tell me that they edit HTML on platforms which may not have\naccess to the WEB, etc.  I think it is useful to allow for coding\ninformation in the document as in <IMG ...> etc but that is really\nmore useful if scaling to fit is included and as a specification of\nexpected scaling.  For basic size it ought to be the court of last resort.\n\nI just know I've missed a critical piece of configuration information \nfor the CERN and NCSA servers vis a vis telling the server how to know\nwhat kinds of formats are available to use in the content type\n'negotiation'.  All I've found is that a hidden 'user' CGI like program\ncan provide the logic and send the right data.... Either the server(s)\nare or will be smart enough to understand how to recogize many \ndocument types to send in response to requests or installation code\nwill provide the logic or both, but in any event it doesn't seem\nlike a far leap for the same servers to know enough about the content\nformats to be able to obtain the shape.  It also seems reasonable for\nthe provider of the IMAGE (not the referencing HTML or whatever) to \nprovide meta information in a standard form for servers.  Shape\nparameters should relate to the data type.  Images suggest size, \nvideo might add frames or time as a metric, etc.  An acceptable\nresponse from a server must be \"can't figure it out\", etc.\n\nDown the road, it might be helpful if the HTML files had optional\nsize metrics.  I would hope at some point that an acceptable 'image'\nformat would be nested HTML and it would certainly help in that case.\nI know shape/size leaves a lot to the rendering processor, but it \nmight help to know a length in some standard measure given a specified\nwidth and font size assumptions...\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "On Mon, 19 Dec 1994, Ari Luotonen wrote:\n\n> This is a couple of orders of magnitude smaller problem -- you PUT\n> files a lot less than people GET them (at least I wouldn't want to be\n> the webmaster of such an unpopular site...).  Even then you can have\n> an MPUT.  I have a hard time imagining that someone would\n> realistically do multiple GETs/POSTs/PUTs intermixed.  Really.\n\nI have no problem at all ... consider the connection is between a proxy\nand a client.  It could be quite appropriate for that connection to remain\nopen for extended periods of time ... that should be a client/proxy\ndecision.  As this support becomes available, it is important that\nthe parameters be quite tunable.  For some smart proxies with the \nability to utilize sessions between themselves and servers to\nsupport multiple clients it is quite possible that multiple\ntransactions could be intermixed.  Why limit what can be done.\n\nFinally, my experience over many years suggests that the ultimate users\nwill utilize the facilities we design and implement in ways we have\nnot conceived of.  Lets not avoid reasonable features because we don't \nknow how we would utilize the function.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Dave Kristol writes:\n\n> Here's another wrinkle to consider for a SESSION method.  Some transactions\n> have a model that resembles the Basic authentication scheme:\n> 1) The client innocently asks for something.\n> 2) The server rejects the request and asks for something extra.\n> 3) The client reissues the original request, plus the something extra.\n> 4) The server honors the request.\n> \n> This model applies for some payment schemes and some security schemes.  It\n> would be nice if the client had a way to tell the server it's willing to\n> keep a connect open (create a session), even though it didn't request to do\n> so at first (with a SESSION method).  Perhaps the client can send the same\n> Connection: headers that it would have sent with a SESSION method.\n\nYes, that is the intention.  The Connection header (on ANY request)\nwould be used to indicate connection options requested by the client,\nand on the response it would indicate what connection options were\naccepted by the server.\n\nThe SESSION method would simply be a way to initiate a conversation with\nthe nearest neighbor without performing some action on an object,\nand avoiding the message being forwarded downstream.  Note that this\nis different than what was originally proposed at the BOF -- the design\nchanged due to the discussions we had there.\n\nThe SESSION method does not, in and of itself, constitute a continuous\nconnection, though it may include a Connection header which requests\nthat option.  This is very similar to the NULL method used by SHEN,\nbut applies only to the nearest neighbor.\n\nWould it help if we used a different name?  Any suggestions?\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "Paul Burchard <burchard@math.utah.edu> writes:\n\n> A couple of questions and comments on the new HTTP/1.0 spec...\n> \n> (1)  In the definition of the Accept request header, there is no  \n> mention of a \"version\" modifier, e.g.,\n> \n> Accept: text/html; version=3.0\n> \n> This would seem like the right way to determine HTML client  \n> capabilities.  Going by the User-Agent field is a real quagmire.\n\nThe BNF for Accept (Section 5.5.8) includes *(\";\" parameter):\n\n       Accept         = \"Accept\" \":\" 1#(\n                             (\"*\" | type) \"/\" (\"*\" | subtype)\n                            *(\";\" parameter)\n                             [ \";\" \"q\" \"=\" ( \"0\" | \"1\" | float ) ]\n                             [ \";\" \"mxb\" \"=\" 1*DIGIT ] )\n\nthough I neglected to add an explanation of why it is there and\nan example of how it would be used.  It's also a bit ambiguous,\nbut I can fix that.  \"parameter\" is defined under media types.\n\n> (2)  The semantics of Expires should discourage clients from  \n> _indiscriminately_ trying to refetch objects, just because they have  \n> expired.  Dynamically-created pages can be ephemeral, without having  \n> time-dependent content that needs to be updated.\n\nI will try to clarify that.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Proposal for new authentication schem",
            "content": "According to Eric W. Sink:\n> --\n> \n> [PROPOSED] HTTP Working Group                       Jeffery L. Hostetler\n> INTERNET-DRAFT                                              Eric W. Sink\n> <draft-NOT_YET_SUBMITTED-simplemd5-aa-00.txt>\n> Expires SIX MONTHS FROM--->                            December 21, 1994\n> \n>       A Proposed Extension to HTTP : SimpleMD5 Access Authentication\n> \n\nThis is a very good proposal and should be included as part of HTTP1.1.\n\n\nThere are a couple of things I would like to clarify though.\n\n> \n>    If a server receives a request for an access-protected object,\n>    and an acceptable Authorizatation header is not sent, the server\n>    responds with:\n> \n> HTTP/1.1 401 Unauthorized\n> WWW-Authenticate: SimpleMD5     realm=\"<realm>\",\n>                                                         domain=\"<domain>\",\n>                                                         nonce=\"<nonce>\",\n>                                                         opaque=\"<opaque>\",\n>                                                         oldNonce=\"<true |\n> false>\"\n> \n\nDoes this mean the following?\n\nHTTP/1.1 401 Unauthorized<CRLF>\nWWW-Authenticate: SimpleMD5 realm=\"<realm>\", domain=\"<domain>\",\\\\\nnonce=\"<nonce>\", opaque=\"<opaque>\", oldNonce=\"<true | false>\"<CRLF>\n\n\nI.e. should the WWW-Authenticate header be a single line of ',' separated\nkey-value pairs terminated by <CRLF>?  Is whitespace also ok between\nkey-value pairs?\n\nPresumably the same format would apply to this:\n> \n> Authorization: SimpleMD5        username=\"<username>\",\n>                                                         realm=\"<realm>\",\n>                                                         nonce=\"<nonce>\",\n> \n> response=\"<MD5response>\",\n>                                                         opaque=\"<opaque>\"\n\n\nAnother question:\n\n>       <response>\n>          The MD5 encoding of \"<nonce> <password>\".  The resulting string\n>          should be a 32 digit hexadecimal string.\n\n\n\nIs there a space between <nonce> and <password> that gets encoded?  If\nso that should be explicitly stated.  It might be better to use a\nvisible character there, e.g.  make response be the MD5 encoding of\n\"<nonce>:<password>\".\n\n\nFinally, what is the intended function (or functions) of opaque?  \nIt might be nice to have a sentence like \"Possible uses of the \n<opaque> string include...\"   Things like an encoded timestamp or\nfor nonce aging or encoded IP address of the client come to mind,\nbut there are likely other potential uses.\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Jeffrey Mogul writes:\n>My own intuition is that we can follow two paths:\n>(1) Stick with the parallel connection approach, which gives\n>reasonably good performance today but which might turn into\n>a global performance disaster in the future.\n>(2) Encourage people to shift ASAP to an alternative (besides\n>the WIDTH and HEIGHT tags, several HTTP-level mechanisms have\n>been proposed).  This might avert future global problems, but\n>in the short term it might be a while before enough servers\n>support it to make it highly beneficial.  I would also expect\n>that in the long run (i.e., with widespread support), it would\n>improve UPP because you wouldn't have to wait even for the initial\n>bits of the images to arrive.\n\nIf we go with something like an HTTP-based image hinting scheme, I don't\nthink the rate of adoption is a big an issue as you imply.  A browser like\nNetscape could continue to have multiple parallel connections to older\nservers if it wanted to.  Path (1) as you've described is really not a path\nat all - it's just staying with the limits of what we have today.\n\nI think the most important point you make is that an image hinting approach\nwill have a greatly higher UPP than a parallel connection scheme, especially\nover very slow connections.  The way I see it, there are two main goals:\n\n1) Have the formatted text appear as quickly as possible.\n2) Have the entire document with pictures appear as quickly as possible.\n\nEvery image that's being downloaded takes away bandwidth from the text.  For\nthe first goal, image hinting on a single connection clearly wins over\nparallel connections.\n\nThe parallel connection scheme helps somewhat with the second goal, since\nthe RTTs are occuring simultaneously.  However, keeping the connection alive\nand using an MGET for the retrieval would remove most of the RTT penalty\nanyway.  Again, even if the adoption of these enhancements were slow, a\nclient would still have the option of multiple connections.\n\nAlso, my belief is that defining the extensions and getting them into\nbrowsers will propel server authors to add them fairly quickly.  The high\nresource utilization of an http server being subjected to parallel\nconnection clients have been discussed in some depth on c.i.w.providers.\nMany providers would jump on a single-connection approach just for server\nefficiency reasons.\n\n--\nJim Seidman\nSenior Software Engineer\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "> The SESSION method would simply be a way to initiate a conversation with\n> the nearest neighbor without performing some action on an object,\n> and avoiding the message being forwarded downstream.  Note that this\n> is different than what was originally proposed at the BOF -- the design\n> changed due to the discussions we had there.\n> \n> The SESSION method does not, in and of itself, constitute a continuous\n> connection, though it may include a Connection header which requests\n> that option.  This is very similar to the NULL method used by SHEN,\n> but applies only to the nearest neighbor.\n> \n> Would it help if we used a different name?  Any suggestions?\n\nIt might, since the main semantics don't seem to be that it has to do\nwith \"sessions\" but just that your neighbor does not forward it.\n\nSome thoughts for a new name for a method with this semantics:\n PROXY\n NEIGHBOR\n POPTION\n NOFORWARD\n NOFOR\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "SimpleMD5 access authentication, second draf",
            "content": "I submitted this proposal yesterday to the WG list.  I have incorporated a\nfew revisions, and am submitting it again:\n\n1.  Remove tab characters to clean up formatting\n2.  Clarify intent of <opaque>\n3.  Clarify intent of <oldNonce>\n4.  Clarify that the header lines are single-line.\n5.  Clarify the server's responsibility for comparing the MD5 hash.\n\nThanks for the comments.  I will be gone for a week or so over the Christmas\nholiday.\n\nHappy Holidays!\n\n-- cut here --\n\n[PROPOSED] HTTP Working Group                       Jeffery L. Hostetler\nINTERNET-DRAFT                                              Eric W. Sink\n<draft-NOT_YET_SUBMITTED-simplemd5-aa-00.txt>\nExpires SIX MONTHS FROM--->                            December 21, 1994\n\n      A Proposed Extension to HTTP : SimpleMD5 Access Authentication\n\nStatus of this Memo\n\n   This document is an Internet-Draft. Internet-Drafts are working\n   documents of the Internet Engineering Task Force (IETF), its areas,\n   and its working groups. Note that other groups may also distribute\n   working documents as Internet-Drafts.\n\n   Internet-Drafts are draft documents valid for a maximum of six\n   months and may be updated, replaced, or obsoleted by other\n   documents at any time. It is inappropriate to use Internet-\n   Drafts as reference material or to cite them other than as\n   \"work in progress.\"\n\n   To learn the current status of any Internet-Draft, please check\n   the \"1id-abstracts.txt\" listing contained in the Internet-\n   Drafts Shadow Directories on ds.internic.net (US East Coast),\n   nic.nordu.net (Europe), ftp.isi.edu (US West Coast), or\n   munnari.oz.au (Pacific Rim).\n\n   Distribution of this document is unlimited. Please send comments\n   to the proposed HTTP working group at <http-wg@cuckoo.hpl.hp.com>.\n   Discussions of the working group are archived at\n   <URL:http://www.ics.uci.edu/pub/ietf/http/>. General discussions\n   about HTTP and the applications which use HTTP should take place\n   on the <www-talk@info.cern.ch> mailing list.\n\nAbstract\n\n   The protocol referred to as \"HTTP/1.0\" includes specification\n   for a Basic Access Authentication scheme.  This scheme is not\n   considered to be a secure method of user authentication, as the\n   user name and password are passed over the network in an\n   unencrypted form.  A specification for a new authentication scheme\n   is needed for future versions of the HTTP protocol.  This document\n   provides specification for such a scheme, referred to as \"SimpleMD5\n   Access Authentication\".  The encryption method used is the RSA Data\n   Security, Inc. MD5 Message-Digest Algorithm.\n\nTable of Contents\n\n   1.  Introduction\n       1.1  Purpose\n       1.2  Overall Operation\n   2.  Basic Access Authentication Scheme\n       2.1  Specification\n       2.2  Security protocol negotiation\n       2.3  Example\n   3.  Acknowledgments\n   4.  References\n   5.  Authors Addresses\n\n\n1. Introduction\n\n1.1  Purpose\n\n   The protocol referred to as \"HTTP/1.0\" includes specification\n   for a Basic Access Authentication scheme[1].  This scheme is not\n   considered to be a secure method of user authentication, as the\n   user name and password are passed over the network in an\n   unencrypted form.  A specification for a new authentication scheme\n   is needed for future versions of the HTTP protocol.  This document\n   provides specification for such a scheme, referred to as \"SimpleMD5\n   Access Authentication\".\n\n   The SimpleMD5 Access Authentication scheme is not intended to be\n   a complete answer to the need for security in the World Wide Web.\n   This scheme provides no encryption of object content.  The intent\n   is simply to facilitate secure access authentication.\n\n   It is proposed that this access authentication scheme be included\n   in the the proposed HTTP/1.1 specification.\n\n1.2  Overall Operation\n\n   Like Basic Access Authentication, the SimpleMD5 scheme is based on\n   a simple challenge-response paradigm.  The SimpleMD5 scheme challenges\n   using a nonce value.  A valid response contains the MD5 checksum of\n   the password and the given nonce value.  In this way, the password\n   is never sent in the clear.  Just as with the Basic scheme, the\n   username and password must be prearranged in some fashion.\n\n\n2. SimpleMD5 Access Authentication Scheme\n\n2.1 Specification\n\n   The SimpleMD5 Access Authentication scheme is conceptually similar to\n   the Basic scheme.  The formats of the modified WWW-Authenticate header line\n   and the Authorization header line are specified below.\n\n   Due to formatting constraints, both the WWW-Authenticate and the\n   Authorization header are depicted on multiple lines.  In actual usage,\n   they are required to be a single line of comma-separated attribute-value\n   pairs, terminated by <CRLF>.  Whitespace between the attribute-value pairs\n   is allowed.\n\n   If a server receives a request for an access-protected object,\n   and an acceptable Authorizatation header is not sent, the server\n   responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: SimpleMD5 realm=\"<realm>\",\n                            domain=\"<domain>\",\n                            nonce=\"<nonce>\",\n                            opaque=\"<opaque>\",\n                            oldNonce=\"<TRUE | FALSE>\"\n\n   The client is expected to retry the request, passing an Authorization header\n   line as follows:\n\nAuthorization: SimpleMD5    username=\"<username>\",\n                            realm=\"<realm>\",\n                            nonce=\"<nonce>\",\n                            response=\"<MD5response>\",\n                            opaque=\"<opaque>\"\n\n   The meanings of the identifers used above are as follows:\n\n      <realm>\n         A name given to users so they know which username and password\n         to send.\n\n      <domain>\n         A comma separated list of URIs, as specified for HTTP/1.0.  The\n         intent is that the client could use this information to know the\n         set of URIs for which the same authentication information should be\n         sent.  The URIs in this list may exist on different servers.  If\n         this keyword is omitted or empty, the client should assume that\n         the domain consists of all URIs on the responding server.\n\n      <nonce>\n         A server-specified integer value which may be uniquely generated\neach time\n         a 401 response is made.  Servers may defend themselves against replay\n         attacks by refusing to reuse nonce values.\n\n      <opaque>\n         A string of data, specified by the server, which should returned by\n         the client unchanged.  It is recommended that this string be\n         base64 or hexadecimal data.  Specifically, since the string is passed\n         in the header lines as a quoted string, the double-quote character\n         is not allowed.\n\n      <username>\n         It is assumed that both client and server know of a prearranged\n         username and password pair.  The Authorization header returned\n         by the client specifies the username.  The password is not specified\n         in the clear.\n\n      <response>\n         The MD5 encoding of \"<nonce> <password>\".  The resulting string\n         should be a 32 digit hexadecimal string.\n\n      <oldNonce>\n         A flag, indicating that the previous request from the client\n         was rejected because the nonce value was stale.  If oldNonce\n         is TRUE, the client may wish to simply retry the request with\n         a new encrypted response, without reprompting the user for a\n         new username and password.\n\n   Upon receiving the Authorization information, the server may check its\n   validity by looking up its known password which corresponds to the submitted\n   <username>.  Then, the server must perform the same MD5 operation performed\n   by the client, and compare the result to the given <response>.\n\n   When the server is constructing the WWW-Authenticate header, the domain,\n   opaque, and oldNonce keywords may be omitted.\n\n   All keyword-value pairs must be expressed in characters from the\n   US-ASCII character set, excluding control characters.\n\n   A client may remember the username, password and nonce values, so that\n   future requests within the specified <domain> may include the Authorization\n   line preemptively.  The server may choose to accept the old Authorization\n   information, even though the nonce value included might not be fresh.\n   Alternatively, the server could return a 401 response with a new nonce\n   value, causing the client to retry the request.  By specifying oldNonce=TRUE\n   with this response, the server hints to the client that the request should\n   be retried with the new nonce, without reprompting the user for a new\n   username and password.\n\n   The <opaque> data is useful for transporting state information around.\n   For example, a server could be responsible for authenticating content\n   which actual sits on another server.  The first 401 response would include\n   a <domain> which includes the URI on the second server, and the <opaque>\n   for specifying state information.  The client will retry the request, at\n   which time the server may respond with a 301/302 redirection, pointing\n   to the URI on the second server.  The client will follow the redirection,\n   and pass the same Authorization line, including the <opaque> data which\n   the second server may require.\n\n   As with the basic scheme, proxies must be completely transparent in\n   the SimpleMD5 access authentication scheme. That is, they must forward the\n   WWW-Authenticate and Authorization headers untouched. If a proxy\n   wants to authenticate a client before a request is forwarded to\n   the server, it can be done using the Proxy-Authenticate and\n   Proxy-Authorization headers.\n\n2.2 Security Protocol Negotiation\n\n   It is useful for a server to be able to know which security schemes\n   a client is capable of handling.  It is recommended that the HTTP extension\n   mechanism proposed by Dave Kristol [2] be used.  If the client includes\n   the following header line with the request, then a server can safely assume\n   that the client can handle SimpleMD5 authentication.\n\nExtension: Security SimpleMD5\n\n   If this proposal is accepted as a required part of the HTTP/1.1\n   specification, then a server may assume SimpleMD5 support when a client\n   identifies itself as HTTP/1.1 compliant, by sending:\n\nHTTP-Version: HTTP/1.1\n\n   It is possible that a server may want to require SimpleMD5 as its\n   authentication method, even if the server does not know that the client\n   supports it.  A client is encouraged to fail gracefully if the server\n   specifies any authorization scheme it cannot handle.\n\n2.3 Example\n\n   The following example assumes that an access-protected document is being\n   requested from the server.  Both client and server know that the username\n   for this document is \"Mufasa\", and the password is \"CircleOfLife\".\n\n   The first time the client requests the document, no Authorization header\n   is sent, so the server responds with:\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: SimpleMD5 realm=\"PrideRock\",\n                            domain=\"\",\n                            nonce=\"67654464\",\n                            opaque=\"76da3afcb7c9a0\",\n                            oldNonce=\"FALSE\"\n\n   The client may prompt the user for the username and password, after which it\n   will respond with a new request, including the following Authorization\nheader:\n\nAuthorization: SimpleMD5    username=\"Mufasa\",\n                            realm=\"PrideRock\",\n                            nonce=\"67654464\",\n                            response=\"9524c2516e37df5c6a3c7ef5e334a31b\",\n                            opaque=\"76da3afcb7c9a0\"\n\n   Note that the hexadecimal string for the response is the MD5 encoding of\n   \"67654464 CircleOfLife\".  The space character between the nonce value and the\n   password is required, and part of the encrypted result.\n\n3. Acknowledgments\n\n   Larry Stewart, at OpenMarket, Inc., contributed to the development and\n   implementation of this authorization scheme.\n\n   Source code in C for the RSA Data Security, Inc. MD5 Message-Digest\n   Algorithm is available free of charge from RSA Data Security, Inc.\n\n4. References\n\n   [1]  T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen.\n        \"Hypertext Transfer Protocol -- HTTP/1.0\"\n        Internet-Draft (work in progress), UC Irvine,\n        <URL:http://ds.internic.net/internet-drafts/\n        draft-fielding-http-spec-01.txt>, December 1994.\n\n   [2]  D. Kristol. \"A Proposed Extension Mechanism for HTTP\"\n        <URL:http://www.research.att.com/~dmk/extend.txt>,\n        December 1994.\n\n5. Authors Addresses\n\n   Jeffery L. Hostetler\n   jeff@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   1800 Woodfield Drive\n   Savoy, IL  61874\n\n   Eric W. Sink\n   eric@spyglass.com\n   Senior Software Engineer\n   Spyglass, Inc.\n   1800 Woodfield Drive\n   Savoy, IL  61874\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\"Can I get a direct flight back to reality, or do I have to change planes\nin Denver?\" - The Santa Clause\n\n\n\n"
        },
        {
            "subject": "New draft; ContentLengt",
            "content": "I see a lot of work has gone into the new draft; it's looking good (but\nthere do seem to be some surprising additions, such as 'Mandatory'!).\n\nOne question which I don't think is covered: if a client sends a\nheader with more than one Content-Length line, should the server\nreturn an error, use the first (or last) Content-Length encountered,\nor just ignore all the Content-Length lines?\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "On Thu, 22 Dec 1994, Jim Seidman wrote:\n\n> If we go with something like an HTTP-based image hinting scheme, I don't\n> think the rate of adoption is a big an issue as you imply.  A browser like\n> Netscape could continue to have multiple parallel connections to older\n> servers if it wanted to.  Path (1) as you've described is really not a path\n> at all - it's just staying with the limits of what we have today.\n> \n> I think the most important point you make is that an image hinting approach\n> will have a greatly higher UPP than a parallel connection scheme, especially\n> over very slow connections.  The way I see it, there are two main goals:\n\nFor http hinting to achieve the generally accepted goals, the information\nwould have to be provided when the rendering engine needs it.  That could\nimply that the http server actually analyze the html document for <img>\nreferences or have the reference analysis preformed in advance and stored\nin some form so that the http server can anticipate and provide the\ninformation.  If we are in the http-ng timeframe with multiple logical\nstreams on the connection then of course this is moot but otherwise a\nfair compromise might be for a browser to open 1 additional connection for\nthe purpose of requesting image shape information while the primary\nfile is transfered.  While some of the costs exists they would be minimzed\ncompared with four or more images actually flowing concurrently.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "As long as we're going to redefine the Internet media types to be \"as\nregistered for MIME with some exceptions\", I think we might as well\nhandle the character set issue as well as the EOL convention one.\n\nThat is: object types in HTTP denote the corresponding Internet Media\nType as defined in RFC 1521 and are registered via the procedure\noutlined in RFC 1590. However, HTTP makes two modifications to the \ninterpretation of \"text/*\" media types:\n\na) the default character set (if no charset parameter is supplied) is\n   ISO-8859-1, not US-ASCII.\n\nb) no requirement is placed that documents which *could* be\n   interpreted as US-ASCII must be labelled such. (This was a MIME\n   requirement, but shouldn't be for HTTP).  \n\nc) the end of line convention depends on the character set. In\n   particular, while US-ASCII requires 'CR LF' as a character set,\n   the end of line in ISO-8859-1 may consist of\n      a single CR\n      a single LF\n      a combined CR LF\n   and recievers must be prepared to determine the end of line\n   convention used in the text/ type.\n\nd) character sets such as UNICODE (where the data is represented as \n   a sequence of pairs of octets representing the hex coding of\n   the data) are allowed. (This is apparently not true in the latest\n   MIME draft).\n\ne) character sets must be registered for use within HTTP; in addition\n   to the current information that is included in the character set\n   registration by IANA, the registration must describe the end of\n   line convention for the character set, and include information\n   about how to map the character set definition into other character\n   sets or icons. \n\nThe HTTP standard should define use of character sets US-ASCII and\nISO-8859-1, at the least, and probably could include descriptions of\nUNICODE-1-1 (two octets per character), UNICODE-1-1-UTF-8 and\nUNICODE-1-1-UTF-7 as per RFC1641 and RFC1642.\n     \nI'm happier with a definition that defines how clients are supposed to\n*interpret* the data than one that speaks of 'canonicalization' or\n'conversion', or where it is somehow the transport (HTTP) that is\nmessing up. \n\n\n> I'd suggest something like the following phrasing:\n> ----\n>   Conversion to canonical form:\n\n>   Internet media types [cite 1590] are registered in a canonical form.\n>   In general, Object-Bodies transferred via HTTP must be represented\n>   in the appropriate canonical form prior to the application of\n>   Content-Encoding and/or Content-Transfer-Encoding, if any, and\n>   transmission.\n\n>   Object-Bodies with a Content-Type of text/*, however, may represent\n>   line breaks not only in the canonical form of CRLF, but also as CR\n>   or LF alone, used consistently within an Object-Body.  Conforming\n>   implementations *must* accept any of these three byte sequences as\n>   representing a single line break in text/* Object-Bodies.\n\n>   RATIONALE:  A handful of different local representations of textual\n>   files exists in current practice.  Conversion to canonical form can\n>   pose a significant performance loss, while understanding different\n>   line break representations is not an inordinate burden, nor an\n>   excessive requirement beyond current practice.\n> ----\n> Part of the question is whether it's implicit that all text/* types\n> are ASCII-based and that CR and LF are the appropriate interpretations\n> of octets 0D and 0A.  The newest draft of MIME calls for such, as does\n> section A.2 in the current HTTP draft, so I stuck to that path.\n\n> I'm not thrilled about requiring implementations to accept different\n> line break sequences, but we kind of have to either:\n\n> 1. Require all implementations to canonicalize\n> 2. Require all implementations to understand certain specific\n>    non-canonical forms (and say exactly what they are)\n> 3. Require some kind of negotiation process by which servers and\n>    clients can indicate what variations each can understand\n\n> We seem to mostly have consensus that 1) is a potentially too\n> expensive.  I think 3) is more complexity than we particularly want to\n> endure right now [it would lead to something like \"Accept-Encoding:\n> unix-linebreaks\" or maybe \"Content-Transfer-Encoding:\n> 8bit-sloppy-eol\".]  That leaves 2), which is probably what will come\n> closest to satisfying folks.\n\n> - Marc\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "I like both proposals, and will endeavor to fit both within\nthe next draft.  I think it is reasonable to include both how\nHTTP interprets media types and what procedure is needed to transform\nHTTP objects to conformant MIME objects.\n\nThanks for the detailed explanations,\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "I was just on the verge of finishing up a paper explaining the issues,\nand proposing use of UTF etc. I will still send this out as it's\nrelevant for browser writers, and HTML authors.\n\nJust to clarify, in Larry's proposal, the Accept-charset is not required\nbecause we can do the same thing via:\n\n   Accept: text/html; charset=xxxx\n\nHowever, how do we handle text/sgml, text/plaintext etc? Can they be\nhandled the same way, or do we still need Accept-charset.\n\nI will be sending out my text tommorrow.\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "Thus wrote: Larry Masinter\n>As long as we're going to redefine the Internet media types to be \"as\n>registered for MIME with some exceptions\", I think we might as well\n>handle the character set issue as well as the EOL convention one.\n\nThey do seem rather intertwined.  I, of course, have an absurdly naive\nhope that, because the MIME folks have not yet set this issue into\neven RFC-level stone let alone STD-level stone, it might be possible\nfor MIME and HTTP to doe it the same way.\n\n>That is: object types in HTTP denote the corresponding Internet Media\n>Type as defined in RFC 1521 and are registered via the procedure\n>outlined in RFC 1590. However, HTTP makes two modifications to the \n>interpretation of \"text/*\" media types:\n>\n>a) the default character set (if no charset parameter is supplied) is\n>   ISO-8859-1, not US-ASCII.\n>\n>b) no requirement is placed that documents which *could* be\n>   interpreted as US-ASCII must be labelled such. (This was a MIME\n>   requirement, but shouldn't be for HTTP).  \n\nAs I read the MIME spec, it says that US-ASCII text \"should\" be\nlabeled as US-ASCII and not some superset like ISO-8859-1 (and, in\ngeneral, meta-data should be specify the lowest common denominator\npossible for understanding.)  I guess I don't really see the advantage\nof changing the default in this fashion, though I don't see it as a\nbig problem either.  I just worry that a nontrivial subset of people\nthink that allowing 8859-1 addresses cross-linguistic use, which of\ncourse it doesn't except for a few languages in a small part of the\nworld.\n\nEnvironments where US-ASCII can be displayed but 8859-1 cannot are\nstill plentiful; I'm using one right now, and run it under Lynx all\nthe time (and have 8859-1 characters quietly converted to graphical\ngibberish, which is not ideal behavior.)\n\n>c) the end of line convention depends on the character set. In\n>   particular, while US-ASCII requires 'CR LF' as a character set,\n>   the end of line in ISO-8859-1 may consist of\n>      a single CR\n>      a single LF\n>      a combined CR LF\n>   and recievers must be prepared to determine the end of line\n>   convention used in the text/ type.\n\nHmmm... I guess I don't see the connection.  Is broadening the\nacceptable representations for EOL in 8859-1 significantly less\nradical, more in line with current practice, or in some other\nmeaningful way preferable to just doing it for US-ASCII?\n\n[ some stuff I mostly agree with deleted ]\n\n>I'm happier with a definition that defines how clients are supposed to\n>*interpret* the data than one that speaks of 'canonicalization' or\n>'conversion', or where it is somehow the transport (HTTP) that is\n>messing up. \n\nI think it's the same thing by different terms; as long as the\nspecification is clear and not guesswork-oriented.\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "> I was just on the verge of finishing up a paper explaining the issues,\n> and proposing use of UTF etc. I will still send this out as it's\n> relevant for browser writers, and HTML authors.\n\nYep.\n\n> Just to clarify, in Larry's proposal, the Accept-charset is not required\n> because we can do the same thing via:\n> \n>    Accept: text/html; charset=xxxx\n\nYep.\n\n> However, how do we handle text/sgml, text/plaintext etc? Can they be\n> handled the same way, or do we still need Accept-charset.\n\nAll text/* types use the charset parameter.\nSGML would likely be application/sgml, and its character set is defined\nin one of the declarations, though it could also have a charset paramenter\nif that's how they define the media type.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: New draft; ContentLengt",
            "content": "> I see a lot of work has gone into the new draft; it's looking good (but\n> there do seem to be some surprising additions, such as 'Mandatory'!).\n\nIt may get taken out and postponed until 1.1, unless there is a great\ncry to leave it in.  It seemed like a good idea at the time ...\n\n> One question which I don't think is covered: if a client sends a\n> header with more than one Content-Length line, should the server\n> return an error, use the first (or last) Content-Length encountered,\n> or just ignore all the Content-Length lines?\n\nGood question -- I'm not sure.  My gut feeling is that if it is received\nby the server (in a POST), the server should return a bad request error\nunless the lengths are identical.  For clients receiving it from a server,\nI would think the client should use the last one encountered.\nDoes that sound reasonable?\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": "Accept-charset could be included as a possible shorthand for\nmultiplexing acceptable charset parameters for ALL types that have a\n'charset' parameter, but I think it is only worthwhile if the client\naccepts multiple charsets and multiple types that take a charset\nparameter.\n\nI'm not sure it's worth the complexity of the model, or whether we\nwant a more general way of multiplexing parameters for the types that\nare named by accept clauses, and I don't think it belongs in the\nstandard without at least a little implementation experience.\n\n\n\n"
        },
        {
            "subject": "Re: Closure on canonicalization, I hop",
            "content": ">Accept-charset could be included as a possible shorthand for\n>multiplexing acceptable charset parameters for ALL types that have a\n>'charset' parameter, but I think it is only worthwhile if the client\n>accepts multiple charsets and multiple types that take a charset\n>parameter.\n\nI would like to see it in there, but II will leave the reasoning\nuntil I send out my paperr.\n\n>I'm not sure it's worth the complexity of the model, or whether we\n>want a more general way of multiplexing parameters for the types that\n>are named by accept clauses, and I don't think it belongs in the\n>standard without at least a little implementation experience.\n\nThe last sentence I can agree with. Hence I will not push strongly\nfor Accept-charset: until it actually appears to be needed. Until\nthen, the charset parameter will suffice.\n\nI'm *very* glad that content parsing is now a function of\nthe character encoding system. This will simplify many things.\n\n\n\n"
        },
        {
            "subject": "Re: New draft; ContentLengt",
            "content": "> > One question which I don't think is covered: if a client sends a\n> > header with more than one Content-Length line, should the server\n> > return an error, use the first (or last) Content-Length encountered,\n> > or just ignore all the Content-Length lines?\n>\n> Good question -- I'm not sure.  My gut feeling is that if it is received\n> by the server (in a POST), the server should return a bad request error\n> unless the lengths are identical.  For clients receiving it from a server,\n> I would think the client should use the last one encountered.\n> Does that sound reasonable?\n\nWell, my server currently implements 'lazy receive' -- that is, it\nonly receives data that's actually needed by the script or filter.\nFor example, if the URI has a syntax error, the script may never do\nanything that needs information from the header, and in that case the\nheader is not received at all.\n\nSimilarly, if the script asks for the Content-length, only\nenough of the header is received and parsed to find the first\n\"Content-Length: xxx\" line.  If the body data is requested, all the\nheader is received, of course, but parsing of the header can stop\nwhen the length has been determined.\n\nHence from a pure efficiency/server point of view, I suppose I'd be\nhappiest if either the only the first was used, or the state was\n(explicitly) undefined if more than one \"Content-Length:\"  were\nspecified.  Similar arguments apply to any Request Header fields that\nare normally expected only once (such as \"If-Modified-Since\").  And\nclients connecting to my server will get a slightly faster response if\nany 'must be parsed for this request' fields are near the top of the\nheader.\n\nMike Cowlishaw\nIBM UK Laboratories\n\n\n\n"
        },
        {
            "subject": "A truly multilingual WW",
            "content": "Below is a paper that I have put together proposing a method for\nmaking the WWW a truly international information repository. It is\nsubmitted in the hope that it will raise discussion of the issues\ninvolved, and perhaps to serve as a goal for which we should all\nstrive. Certainly, the architecture outlined will not come to pass\novernight, but both client and server implementors could start\nimplementing the changes now, and as the tools spread, so the\narchitecture will too.\n\nI would appreciate any and all comments. All errors of language or\nfact are solely my responsibility (ie. if I am making a fool of\nanyone, I am making it of myself ;-)), and I would appreciate\ncorrection.  \n\nAn HTML version of this paper is available upon request.\n\nMerry Christmas to all!\n\n-------------------------\n\n                  HANDLING MULTILINGUAL DOCUMENTS IN THE WWW\n                \n                                           Gavin T. Nicol\n                                           Electronic Book Technologies, Japan\n                                           1-29-9 Tsurumaki, Setagaya-ku,\n                                           Tokyo 154,\n                                           Japan\n                                           +81-3-3706-7351\n                                           gtn@ebt.com \n    \nABSTRACT.\n     The World Wide Web has enjoyed explosive growth in recent years, and\n     there are now millions of people using it all around the world.\n     Despite the fact that the Internet, and the World Wide Web, span\n     the globe, there is, as yet, no well-defined way of handling\n     documents that contain multiple languages, character sets, or\n     encodings thereof. In this document, a method is proposed for\n     cleanly handling such multilingual text on the WWW. \n     \n1. Requirements for multilingual applications\n   \n   There are many issues facing a system claiming to be multilingual,\n   though all issues fall into one of 3 categories:\n    1. Data representation issues\n    2. Data manipulation issues\n    3. Data display issues\n       \n   This document is primarily concerned with data representation, though\n   display issues are also discussed in some detail. Data manipulation is\n   little discussed.\n   \n  1.1 DATA REPRESENTATION ISSUES\n  \n   In general, the major data representation issues are character set\n   selection, and character set encoding. The biggest problem with\n   character set selection is that most are standards, and as Andy\n   Tanenbaum once noted:\n   \n     The nice thing about standards is that there are so many to choose\n     from. \n     \n   Character set encodings suffer in much the same way. There are a large\n   number of character set encodings, and the number is *not* decreasing.\n   \n   For any application that claims to be multilingual, it must obviously\n   support the character sets, and encodings, used to represent the\n   information it is processing. It should be noted that multilingual\n   data could conceivably contain multiple languages, character sets, and\n   encodings, further complicating the problem.\n   \n  1.2 DATA DISPLAY ISSUES\n   \n   Quite obviously, for a viewing application to be considered\n   multilingual, it must be able to present multilingual data to the\n   reader in a sensible manner. The common problem to overcome here is\n   obviously font mapping, however, languages around the world have\n   different writing directions as well, and some languages have mixed\n   writing directions, which should also be handled \"correctly.\"\n\n   Note that in the above paragraph \"in a sensible manner\" does not\n   necessarily mean \"be able to rendered in it's native format.\" One\n   other possiblilty would be to render the multilingual document as a\n   phonetic rendering in ASCII. For example, if some Japanese text was\n   sent to a person that cannot read Kanji, Hiragana, or Katakana, the\n   browser could conceivably map the Japanese text into something like\n   the following:\n\n       Nihongo, tokuni Kanji, wa totemo muzukashii desu.\n\n   possibly with some extra text indicating that this is Japanese.\n   Another possibility is machine translation (which is becoming more\n   viable year by year).\n   \n  1.3 DATA MANIPULATION ISSUES\n   \n   In order to be multilingual, the application must be able to\n   manipulate multilingual data (including such issues as collation,\n   though this is not currently needed for browsing the WWW). One major\n   issue here is the representation of a character within the\n   application, and the representation of strings. In some applications,\n   multibyte formats are used throughout, in others, fixed width, wide\n   characters are used throughout. In others, a combination is used.\n   \n  1.4 SUMMARY OF REQUIREMENTS\n   \n   A multilingual application must be able to:\n    1. Support the character sets and encodings used to represent the\n       information being manipulated.\n    2. Present the data meaningfully if the application is required to\n       display the data.\n    3. Manipulate multilingual data internally.\n       \n2. Bringing multilingual capabilities to the WWW\n\n   Having established some basic requirements, it is now time to look at\n   how the above fits into the World Wide Web.\n   \n  2.1 MIME ISSUES\n   \n   One of the problems with representing multilingual documents in the\n   WWW is that MIME explicitly merges the character set and character set\n   encodings together. In fact, it is probably more accurate to say that\n   MIME specifies only the character set encoding, which in turn defines\n   a character set by implication. For example:\n\n    charset=unicode-1-1-utf-7\n\n   actually specifies the UTF-7 encoding of Unicode explicitly, but only\n   specifies Unicode implicitly.\n   \n   In addition, the MIME specification states that for the text/* data\n   types, all line breaks must be indicated by a CRLF pair. This implies\n   that certain encodings cannot be used within the text/* data types if\n   the WWW is to be strictly MIME conformant.\n   \n  2.2 SGML ISSUES\n   \n   SGML does not define the representation of characters. ISO 8879\n   defines a code set as a set of bit combinations of equal size, a\n   character as an atom of information, a character repetoire as a group\n   of characters, and a character set as a mapping of a character\n   repetoire onto a code set such that each character is represented by a\n   unique bit combination within the code set. As such, an SGML parser is\n   independent of the physical representation of the data, and there is\n   often an internal representation of characters that could be quite\n   different to that used in data storage.\n   \n   ISO 8879 also defines some methods for handling things like ISO-2022,\n   but some encodings for languages such a Thai cannot be handled by\n   SGML, even if the SGML declaration is altered (though, it is possible\n   for the application to deal with this within, or before, the entity\n   manager).\n   \n  2.3 BROWSERS ISSUES\n  \n   Basic requirements for a multilingual WWW browser were listed in\n   section 1.4. Let's now look at each as they apply to WWW browser like\n   Mosaic.\n   \n    2.3.1 Support the character sets and encodings\n   \n   As noted above, there are a huge number of character sets, and\n   encodings. If the above requirement is taken literally, it means that\n   in order to have a multilingual WWW, each browser must potentially be\n   able to understand this huge number of character sets and encodings.\n   Taken at face value it also means that the SGML parser would need to\n   handle a large number of character sets, and one would not be able to\n   have multiple character sets within a document (or to be precise,\n   SGML provides no way for the parser to handle a given bit combination\n   representing more than one character within a given document). As\n   such, the brute force approach would almost certainly have to map the\n   multiple character sets and encodings to a single internal\n   representation (though it is conceivable that character sets could be\n   paged in and out, this approach would be very complicated to\n   implement). This internal representation would almost certainly have\n   to be 16 bits wide, or wider. SGML working group 8 has stated that\n   multilingual systems should map data storage encodings to wide\n   characters before, on in the entity manager (and the sp parser from\n   James Clark serves as a good model of thier recommendations).\n   \n    2.3.2 Present the data meaningfully\n   \n   One of the great benefits of SGML (and HTML to a lesser degree), is\n   that it is independent of the display technology. As such, the\n   presentation issue depends very much on the application (for example,\n   rendering multilingual documents on a TTY might require the\n   aforementioned phonetic rendering, while GUI based systems require\n   font mapping).\n   \n   A common thread running through all display technologies is the need\n   for some mapping from the character codes to some output\n   representation. While not required, using 16 bit (or greater) codes\n   will probably simplify the mapping task as it requires only a single\n   lookup table.\n   \n    2.3.3 Internal multilingual data manipulation\n   \n   There are a large number of issues here, but in general, having a\n   fixed width character eases the task of manipulating that data,\n   especially in languages such as C, because it makes memory management\n   and indexing easier. In addition, fixed width codes require no\n   sychronisation.\n   \n3. Is Unicode the answer?\n   \n   In a word, YES! Though there are a number of issues that need to be\n   resolved in order for it to be used effectively.\n   \n  3.1 WHAT ARE THE BENEFITS?\n   \n   First, let's step back and look at the overall architecture\n   \n   The desired features in a multilingual WWW system are:\n    1. Allow publishers to create and manipulate documents in the local\n       character set and character encoding scheme\n    2. Allow readers to follow a URL to a document, and expect to be able\n       to read it irrespective of the character set and encoding used for\n       document creation.\n    3. Allow readers to save the document in their preferred local\n       encoding.\n    4. Scalability. Will the solution work even when large numbers of\n       languages, character sets, and encodings are used within a single\n       document?\n    5. Keep implementations as simple as possible.\n       \n   Unicode solves most if these items quite nicely.\n   \n   The second and fourth points are covered because Unicode provides\n   codes for most languages of the world. It does not cover all languages\n   completely, but it certainly contains enough for most common uses, and\n   there are ways for handling the uncommon cases.\n   \n   The last point is covered because the UCS-2 encoding of Unicode is a 16\n   bit wide, fixed width encoding. As pointed out above, using such fixed\n   width characters simplifies SGML parsing, display, and data\n   manipulation.\n   \n   The first and third points are not covered directly, but simple\n   translation techniques can be used to achieve them as outlined below. \n   \n  3.2 INCORPORATING UNICODE INTO THE WWW\n     \n   The following outlines a proposal for incorporating Unicode into the\n   WWW in such a way that all of the above points are solved. Where the\n   word server is used, it should be taken to mean a HyperText Transfer\n   Protocol (HTTP) server unless specifically qualified otherwise.\n   \n    3.2.1 Unicode incorportation architecture\n   \n   In order to make multilingual support as painless as possible, it is\n   proposed that all HTTP servers for multilingual documents *should* be\n   able to convert documents from the local character set encoding to\n   UCS-2, UTF-8, and UTF-7 (16, 8 and 7 bit encodings of Unicode). It is\n   also proposed that all HTTP clients *should* be able to parse UCS-2,\n   UTF-8 and UTF-7. It is *recommended* that browsers allow the data to be\n   saved as UTF-7, UTF-8, or UCS-2 (similar to the current ftp\n   interface). If possible, a browser *should* also allow the data to be\n   saved in the local character set encoding, but that might not always\n   be possible (for example, saving a document containing Arabic on an\n   ASCII based system). Documents sent from servers would then use a\n   content type of:\n\n     Content-Type: text/...; charset=UNICODE-1-1-UTF-7\n     Content-Type: text/...; charset=UNICODE-1-1-UTF-8\n     Content-Type: text/...; charset=UNICODE-1-1-UCS-2\n\n   Though UTF-8 and UCS-2 will need some additional encoding applied to\n   them in order to be strictly MIME compliant. An alternative is to use\n   an application/* type specifier instead.\n   \n   This architecture has the following benefits:\n   \n   Conceptual Simplicity\n          \n          The model is very clear conceptually: a document is created in\n          the local encoding, which is then converted into a commonly \n          understood form. Client-side processing occurs using this\n          representation.\n          \n   Ease of client implementation\n          \n          The number of clients far exceeds that of servers. Thus,\n          forcing clients to deal with a large number of characters sets\n          and encodings requires more effort, and the lag in update\n          period will be greater. Imagine for example, if someone wants\n          to add support for some new character set encoding they\n          invented. In order for support to be added to clients, one\n          would have to update all clients that could potentially access\n          such data. Compare this to having to update only the servers\n          working with that encoding directly. In addition, because\n          Unicode and UCS-2 are already well defined, it is possible to\n          write an SGML parser, display subsystem, and data manipulation\n          functions optimised for that representation, realising\n          significant performance gains.\n          \n          While initial implementations will need to write the code for\n          handling Unicode, and the encodings thereof, it is expected\n          that freely distributable libraries for such things will appear\n          \n   MIME and ASCII compatibility\n          \n          UTF-7 and UTF-8 are largely ASCII compatible. In addition UTF-7\n          was designed as a method for encoding Unicode in MIME, so it is\n          perfect for WWW and MIME uses\n          \n   HTML compatability\n          \n          Recent drafts of the HTML specification state that a MIME\n          charset parameter will override the default IS0-8859-1, so this\n          presents no problem. In addition, it should not require large\n          changes to current HTML parsers.\n          \n   Truly multilingual\n          \n          As noted, this proposal uses Unicode. All languages defined\n          within ISO-10646 can be used within a single document. SGML\n          (HTML) parsers will work with UCS-2 directly, so different\n          languages would be treated identically at the parsing level.\n          \n   While the cost of performing translations from the local encoding to\n   one of the Unicode encodings might appear prohibitive, it is believed\n   that intelligent servers will cache translated documents in a manner\n   similar to current proxy caches. Between this, and the fact that most\n   WWW documents are small, the performance hit should not be overly\n   significant.\n   \n    3.2.2 Extension to the basic architecture\n   \n   Requiring that *all* non-ASCII documents be converted to Unicode is\n   probably a very poor idea as it would incur significant overhead.\n   Instead, HTTP clients can indicate encoding preferences via the\n   Accept: field in the request header. For example:\n\n      Accept: text/html; charset=iso-2022-jp\n      Accept: text/html; charset=unicode-1-1-ucs-2\n      Accept: text/*;    charset=unicode-1-1-utf-7\n   \n   If a server is able to deliver the document in one of the preferred\n   encodings, it should do so. This will allow clients and servers\n   sharing a common local encoding to transfer documents without the\n   overhead of Unicode translation. Note that most encodings will need\n   additional encoding to strictly conform to the text/* MIME types.\n   \n   Assuming that all clients are indeed able to parser UTF-7, UTF-8, and\n   UCS-2, the server should default to delivering multilingual documents\n   in one of these encodings as it will provide the greatest probability\n   that the client recieves something it can meaningfully process.\n   \n    3.2.3 Accept-charset\n   \n   While the charset parameter is sufficient for implementing the\n   extended architecture, it requires that for each MIME type in which\n   character set encoding negotiation is desired, a charset parameter\n   must be defined. Also the server must be able to parse all the type\n   specifications, and make meaningful decisions based upon them. As the\n   number of deliverable types increases, so does the complexity of the\n   server format negotiation subsystem.\n   \n   It seems desirable to be able to say to the server \"for all data in\n   which charset is meaningful, send me it encoded as xxxxx\", as it\n   would tend to simplify decoding the data into the applications'\n   internal character representation. This can be accomplished via\n   wildcarding the Accept: field, but a somewhat cleaner alternative\n   would be to have an addition field Accept-charset: which sets the\n   default encoding; requests like:\n\n    Accept: text/html; charset=iso-2-22-jp\n\n   could be used to decide the next best encoding if that of \n   Accept-charset: cannot be delivered. Such cases would probably be \n   uncommon if one assumes that multilingual data will be sent as Unicode.\n   \n    3.2.4 Presentational hints for Unicode\n    \n   While Unicode certainly serves as an excellent lowest common\n   denominator for multilingual documents, systems using Unicode require\n   more information than that contained in the character codes\n   themselves. Probably the most well known example of this is in the Han\n   unification used in Unicode. Unicode defines codes for characters that\n   are shared between Chinese, Korean, and Japanese, but the glyph images\n   used in each language are different. Hence, we need to know the\n   language in which the character code occurred in order to display it\n   \"correctly\". Another interesting case occurs in corporate names in\n   Japan. It is common for Japanese corporations to use a slightly\n   different glyph image for the characters that make up the company name\n   as a way of distinguishing themselves. Again, we need extra\n   information to map the base code onto the correct glyph image. Such\n   data are referred to as presentational hints within this document.\n   \n   Given that a conversion from the local character set to Unicode is\n   being performed by the server, and that this conversion is automatic,\n   it seems possible for the conversion process to automatically include\n   presentation hints in the converted output. Applications that\n   understand the hints can use them to improve the conversion resolution\n   where necessary, while other applications can simply ignore them, or\n   remove them from the data stream. (Strictly speaking, presentational\n   hints are not necessary as in most (perhaps all) cases, the text will\n   be legible, even if the glyph image is not quite correct. Rather, they\n   are desirable for top quality, and especially, typograhic quality,\n   output.)\n   \n   The problem of representing presentational hints is a difficult one.\n   Obviously, it is better to represent such data as tags, rather than as\n   codes, and HTML 3.0 includes a <LANG> tag in the DTD specifically for\n   this.\n   \n   However, high-level tag use (eg. defining them in a DTD) fails for\n   the following reasons:\n    1. It is not transparent. The application processing the data stream\n       must be able to parse the tags, even if it can not do anything\n       with them. This necessarily complicates the parser.\n    2. There are probably a huge number of presentation hints that could\n       be used, and the list is dynamic as societal trends tend to alter\n       languages. Good examples can be found by comparing almost any\n       current written form of a language to that used 100 years ago.\n       Some languages have even changed dramatically in the last 50\n       years.\n       \n   This argues for a low-level tag which is basically transparent to\n   anything parsing the input data stream. This in turn implies that the\n   presentational hints either take effect before the parser, or that\n   they can be manipulated unambiguously as data (or that they can\n   unambiguously be removed from the data stream).\n   \n   This paper will not attempt to define a format for presentation hints.\n   Rather, 3 methods are outlined below, and it is hoped that subsequent\n   discussion will lead to a decision as to which is more applicable to\n   the WWWW.\n   \n      Method 1: Code-based presentation hints\n   \n   Here, codes from the Private Use Area are allocated to represent\n   presentational hints. The advantages of this method is that hints and\n   data can be treated identically, and that hints can be removed\n   transparently. The disadvantages are that it stops other applications\n   from using the Private Use Area, and also, the Private Use Area has a\n   limited range. In addition, this \"pollutes\" the character set with\n   non-character data.\n   \n      Method 2: Encoding-based presentation hints\n   \n   Here, an encoding is used which has space for presentation hints. An\n   example of this is the ICODE encoding proposed by Masataka Ohta which\n   uses 21 bits. Mr. Ohta also defines an encoding called IUTF which is\n   upwardly compatible with UTF-2. This method has all of the advantages\n   of method 1, but would require at least 21 bits of storage per\n   character.\n   \n      Method 3: Tag-based presentation hints\n   \n   Here, tags are defined to represent presentational hints. One tag\n   might potentially serve multiple purposes (for example, a LANG tag can\n   serve to specify any language). The key difference between this\n   method, and the high-level tag method is that tag interpretation here\n   occurs before the application proper sees the data. As such, the tags\n   can be removed transparently. This method would require a certain\n   amount of API for handling tags: probably based on callbacks. In\n   addition, it would appear that one code from the private use area will\n   be needed in order to make tag identification completely unambiguous,\n   for all data streams. \n   \n4. Summary\n   \n   To summarise, this document proposes the following:\n    1. All servers of multilingual documents should be able to convert\n       documents from their local encoding into UCS-2, UTF-8, and UTF-7.\n    2. All clients should be able to at least parse such data.\n    3. Clients and servers should also be able to directly transfer data\n       if they share local encodings\n    4. A method should be decided upon which allows presentational hints\n       to be inserted into the data stream to aid in glyph image\n       disambiguation.\n       \n   It is beleived that given such an architecture, the World Wide Web\n   will become truly multilingual, and truly, a World Wide Web.\n   \n5. Discussion\n   \n   The following are a few notes on recent developments which have some\n   bearing on the contents of this document.\n   \n  5.1 NOTES ON RELAXED CONTENT PARSING IN HTTP\n  \n   A recent development is that the HTTP Working Group has basically\n   decided that HTTP will not require strict MIME conformance for textual\n   data types. In effect, the recent decision says that the parsing of\n   the message data should be done in accordance with the specified\n   character set encoding. This thereby allows multilngual servers to\n   send any encoding the client claims to understand, including UCS-2,\n   the 16 bit encoding of Unicode. This should simplify data processing\n   enormously.\n   \n  5.2 EXTENDED REFERENCE CONCRETE SYNTAX\n   \n   Recently, a proposal for an extended reference concrete syntax for\n   SGML was sent to SGML Open. In this ERCS, the SGML declaration defines\n   that the BASESET is \"ISO 10646:199?//CHARSET UCS-2//EN\", and that the\n   DECSET parameter gives 16 bits for each character.\n   \n  5.3 ISOLATING APPLICATIONS FROM LANGUAGE AND CHARACTER REPRESENTATION ISSUES\n   \n   The author beleives that most applications, and especially those based\n   on SGML, are basically independent of the underlying language and\n   character representations. Obviously, characters needs to be assigned\n   to sets for parsing purposes (like LCNMSTRT for example), but most\n   applications should never need to know anything more about a character\n   until the glyph image, or information about the glyph image, is\n   required. In other words, most parsers and processors needn't be \n   language-aware, but a text display system probably does. This proposal\n   tries to emphasise this as much as possible by trying to provide\n   a uniform character code stream for the application to process.\n   \n    5.3 Unicode data: data manipulation and font handling.\n       \n   For an excellect look at the issues involved, and one possible\n   solution to them, it is well worth reading the papers about the Plan 9\n   system from Bell Laboratories.\n   \n6. Bibliography\n\n    The Plan 9 Papers\n    ftp://research.att.com/\n   \n    East Asian Character Set Issues:\n    A Proposal For An Extended Reference Concrete Syntax\n    Rick Jelliffe\n    Allette Systems\n    Sydney, Australia\n   \n    The SGML Handbook\n    Oxford University Press\n    Written by Charles Goldfarb\n    ISBN 0-19-853737-9\n    \n    The Unicode Standard, Version 1.1\n    Version 1.0, Volume 1, ISBN 0-201-56788-1\n    Version 1.0, Volume 2, ISBN 0-201-60845-6\n   \n    Using Unicode with MIME\n    D. Goldsmith\n    http://ds.internic.net/rfc/rfc1641\n   \n    UTF-7: A Mail Safe Transformation Format of Unicode\n    D. Goldsmith and M. Davis\n    http://ds.internic.net/rfc/rfc1642\n   \n    MIME (Multipurpose Internet Mail Extensions) Part 1\n    N. Borenstein and N. Freed\n    http://ds.internic.net/rfc/rfc1521.ps\n   \n    MIME (Multipurpose Internet Mail Extensions) Part 2\n    K. Moore\n    http://ds.internic.net/rfc/rfc1522.txt\n   \n    Hypertext Transfer Protocol -- HTTP/1.0\n    T. Berners-Lee, R. T. Fielding, H. Frystyk Nielsen\n    ftp://ds.internic.net/internet-drafts/draft-fielding-http-spec-01.txt\n    \n_______________________________________________________________________________\n\n   This document in no way reflects the opinions of Electronic Book\n   Technologies. All opinions contained herein are solely those of the\n   author. \n_______________________________________________________________________________\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "I agree wholeheartedly that Unicode is The Answer.  (it may not be\nperfect, but it solves a whole pile of problems).  In the argument over\nwho should do the conversions, the client or the server, I would vote\nfor the client: if only because it limits translation to the very last\nstep, where the true target is known.\n\nYour paper deals mostly with transport issues (things that http\ncould help with), but there are a pile of issues that are related:\n\n>    There are many issues facing a system claiming to be multilingual,\n>    though all issues fall into one of 3 categories:\n>     1. Data representation issues\n>     2. Data manipulation issues\n>     3. Data display issues\n\n4. Data entry issues?\nWhat about entering data into forms?  This is essentially the dual of\n1&2.  Some of the issues are mostly browser specific (like \"how\ndo I type this Kanji character\") but some are not.  Some are\nmore universal, like \"what character sets will the server\naccept in POSTed data??\".  Some are a real quagmire, like \"I\nexpect a date to be entered here and it's value returned to me\nusing the ISO conventions\".\n\n>   1.1 DATA REPRESENTATION ISSUES\n>   \n>    In general, the major data representation issues are character set\n>    selection, and character set encoding.\n\nOther things get represented in a document besides characters.  This is\nalmost certainly outside of the realm of http, but might fit in with\nhtml-42.0.  For example, dates and measures.  A hypthetical document\nmight contain:\n\nI vow to lose <measure 10 pounds> this year.\nWhich when read by someone in the US could come out something like:\nI vow to lose 10 pounds this year.\nAnd when read by someone in Canada could come out something like:\nI vow to lose 4.54 kilograms this year.\nOr when read by someone in the UK could come out something like:\nI vow to lose .71 stone this year.\n\n:-)\n\n>     3.2.1 Unicode incorportation architecture\n>    \n>    In order to make multilingual support as painless as possible, it is\n>    proposed that all HTTP servers for multilingual documents *should* be\n>    able to convert documents from the local character set encoding to\n>    UCS-2, UTF-8, and UTF-7 (16, 8 and 7 bit encodings of Unicode). It is\n>    also proposed that all HTTP clients *should* be able to parse UCS-2,\n>    UTF-8 and UTF-7. It is *recommended* that browsers allow the data to be\n>    saved as UTF-7, UTF-8, or UCS-2 (similar to the current ftp\n>    interface). If possible, a browser *should* also allow the data to be\n>    saved in the local character set encoding, but that might not always\n>    be possible (for example, saving a document containing Arabic on an\n>    ASCII based system). Documents sent from servers would then use a\n>    content type of:\n> \n>      Content-Type: text/...; charset=UNICODE-1-1-UTF-7\n>      Content-Type: text/...; charset=UNICODE-1-1-UTF-8\n>      Content-Type: text/...; charset=UNICODE-1-1-UCS-2\n> \n>    Though UTF-8 and UCS-2 will need some additional encoding applied to\n>    them in order to be strictly MIME compliant. An alternative is to use\n>    an application/* type specifier instead.\n\nBut http isn't strictly MIME compliant.  In particular, the full 8 bit\nnature of UTF8 fits in well with http.  While UTF7 makes sense in the\nMIME mail world of corrosive transport mechanisms, it is not needed\nin http.  For simiplicity I'd recommend a really limited set of\nallowed encodings: ISO-8859-1 and UTF8.\n\n>     3.2.4 Presentational hints for Unicode\n>     \n>    While Unicode certainly serves as an excellent lowest common\n>    denominator for multilingual documents, systems using Unicode require\n>    more information than that contained in the character codes\n>    themselves\n....    \n>    However, high-level tag use (eg. defining them in a DTD) fails for\n>    the following reasons:\n>     1. It is not transparent. The application processing the data stream\n>        must be able to parse the tags, even if it can not do anything\n>        with them. This necessarily complicates the parser.\n>     2. There are probably a huge number of presentation hints that could\n>        be used, and the list is dynamic as societal trends tend to alter\n>        languages. Good examples can be found by comparing almost any\n>        current written form of a language to that used 100 years ago.\n>        Some languages have even changed dramatically in the last 50\n>        years.\n\nThese problems affect even low-level tags such as those you proposed.\nThis whole area should be left to standards above http.\n\n>       Method 1: Code-based presentation hints\n\nThe big problem with the use of the private use area in this way is that\nit is \"syntax without semantics\".  These numbers are meaningless unless there\nis some mechanism for defining how they should be interpreted.  Something\nhigher-level is required if, for example, a document using one of these\nextended characters is ever to be displayed.\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "> I agree wholeheartedly that Unicode is The Answer.  \n[...]\n> >    In order to make multilingual support as painless as possible, it is\n> >    proposed that all HTTP servers for multilingual documents *should* be\n> >    able to convert documents from the local character set encoding to\n> >    UCS-2, UTF-8, and UTF-7 (16, 8 and 7 bit encodings of Unicode). It is\n> >    also proposed that all HTTP clients *should* be able to parse UCS-2,\n> >    UTF-8 and UTF-7. It is *recommended* that browsers allow the data to be\n> >    saved as UTF-7, UTF-8, or UCS-2 (similar to the current ftp\n> >    interface). If possible, a browser *should* also allow the data to be\n> >    saved in the local character set encoding, but that might not always\n> >    be possible (for example, saving a document containing Arabic on an\n> >    ASCII based system). Documents sent from servers would then use a\n> >    content type of:\n> > \n> >      Content-Type: text/...; charset=UNICODE-1-1-UTF-7\n> >      Content-Type: text/...; charset=UNICODE-1-1-UTF-8\n> >      Content-Type: text/...; charset=UNICODE-1-1-UCS-2\n> > \n> >    Though UTF-8 and UCS-2 will need some additional encoding applied to\n> >    them in order to be strictly MIME compliant. An alternative is to use\n> >    an application/* type specifier instead.\n> \n> But http isn't strictly MIME compliant.  In particular, the full 8 bit\n> nature of UTF8 fits in well with http.  While UTF7 makes sense in the\n> MIME mail world of corrosive transport mechanisms, it is not needed\n> in http.  For simiplicity I'd recommend a really limited set of\n> allowed encodings: ISO-8859-1 and UTF8.\n\nI'd suggest that (to interoperate well with the MIME software community)\nHTTP should allow the same ISO-8859-X codes as MIME (as I recall, X=1 to 9).\n\nStill, advocating something like UTF8 as a preferred transport for\nstuff that would otherwise be in multiple character sets seems\nreasonable. (I get the impression that Unicode is not universally\nloved ... would anyone care to comment more on the down side?)\n\nAt what point do char set issues get handed off to the HTML standard?\n\nIt seems like some of the alternative glyph rendering issues\ncould get mixed in with things like font changes and presentation\ncontrol (which are possible, though controversial, issues for HTML).\n\nOf course text/html is not the only text/* type we might have to\ntransport....\n\n\n\n"
        },
        {
            "subject": "Help me for HTML editor",
            "content": "$)C\n\nHi, everybody;\n\nDoes anybody know of html editors\nincluding those for any platform?\nI want to survey the kind of html editors of all platform.\n\nEspecially, I am writing the guideline manual for\nhtml editors for X-window platform.\n\nAny help you can provide in this regard would be appreciated.\nPlease send the information directly to sck@turing.postech.ac.kr\nof my email address.\n\n---------------------------------------------------------------------\nTo all who participate on these lists, who share support, advice,\nencouragement and friendship.\n\nWhat better place to say:\n\nSeasons Greetings, and Warmest Wishes for a Happy, Healthy and\nProsperous New Year.\n\n ----------------------------------------------------------------------\n |   ___________           *                                          |\n |  |   ) | (   |          ^            Seasons Greetings!!           |\n |  |  )  |  (  |         o^o               Happy Holidays!!          |\n |  | )___|___( |        ^^                  Happy New Year!!      |\n |  |)    |    (|       ^                                       |\n |  |  o  |  o  |      ^^^^^^                    ___________       |\n |  |__U__|__U__|     ^^                  [___________]      |\n |                   ^^^^^^^                 |   _____   |      |\n |                  o^^^^o                |  |     |  |      |\n |              *         |_|         *    *       |  |     |  |      |\n |   __________[X]_______/___\\_______[X]__[X]______|__|(wVw)|__|___   |\n |                                                                    |\n ----------------------------------------------------------------------\n-- \nThanks.\n\n     Seongcheon Kim, Ph. D. Candidate\n     E-MAIL : sck@turing.postech.ac.kr\n     TEL(Home) : +82-562-279-3483\n     TEL(Offi) : +82-562-279-5663\n     WWW : http://einstein.postech.ac.kr/sck/\n     FAX : +82-562-279-5699\n     Computer Commun. Lab., Dept. of Computer Science and Engineering\n     POSTECH (Pohang Univ. of Science and Technology), Rep. of Korea\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "jag@scndprsn.Eng.Sun.COM (James Gosling) writes:\n\n>Your paper deals mostly with transport issues (things that http\n>could help with), but there are a pile of issues that are related:\n\nUndoubtedly. I don't claim to be an expert in this field\neither. Rather, I am just basing things on my practical experience. \n\n>4. Data entry issues?\n\nYes. I'd forgotten about those ;-) Seriously though, these are\nprimarily outside the scope of the paper, except to not that the\nproposed method for data exchange should also be used for data a\nclient generates. ie. All servers *should* be able to accept data in\nUTF-[78] and UCS-2.\n\n>Some are a real quagmire, like \"I expect a date to be entered here\n>and it's value returned to me using the ISO conventions\".\n\nFor this, other work will need to be done, but this is outside the\nscope of my paper.\n\n>Other things get represented in a document besides characters.  This is\n>almost certainly outside of the realm of http, but might fit in with\n>html-42.0.  For example, dates and measures.  A hypthetical document\n>might contain:\n\nTrue. Again, my proposal is only about establishing a multilingual\ndocument exchange foundation. If the different systems cannot\ninteroperate at the lowest level, the higher level doesn't stand a\nchance :-)\n\n>in http.  For simiplicity I'd recommend a really limited set of\n>allowed encodings: ISO-8859-1 and UTF8.\n\nI thought about this. I think that UTF-7 and UCS-2 are so trivial to\nparse (and to to convert to in most cases), that the mail-safety of\nUTF-7, and the compactness of UCS-2 for Asian languages offer enough\nbenfit that they should also be included.\n\n>>    However, high-level tag use (eg. defining them in a DTD) fails for\n>>    the following reasons:\n>>     1. It is not transparent. The application processing the data stream\n>>        must be able to parse the tags, even if it can not do anything\n>>        with them. This necessarily complicates the parser.\n>>     2. There are probably a huge number of presentation hints that could\n>>        be used, and the list is dynamic as societal trends tend to alter\n>>        languages. Good examples can be found by comparing almost any\n>>        current written form of a language to that used 100 years ago.\n>>        Some languages have even changed dramatically in the last 50\n>>        years.\n>\n>These problems affect even low-level tags such as those you proposed.\n>This whole area should be left to standards above http.\n\nTo a degree, you are correct. However, most (and especially SGML)\nparsing takes place at an abstract level somewhat above the actual\ncharacter codes. The choice between low-level and high-level is driven\nby the choice of complicating the upper layers, and having to modify\nthem at the slightest change, or to complicate the lower layers, and\nhave the higher layers work irrespective (because they do not\n*require* that the presentational hints even be present).\n\nHigh level tags (or attributes) *might* be useful for things like\ndeciding data and measurement formats, but I am not convinced of that\nyet. \n\nThe presentation hints I talk about are primarily useful for glyph\nimage disambiguation, not data display format decisions. Note that in\nMethod 3. \"tag\" does not mean \"SGMl tag\".\n\n>> Method 1: Code-based presentation hints\n>The big problem with the use of the private use area in this way is that\n>it is \"syntax without semantics\".  These numbers are meaningless unless there\n>is some mechanism for defining how they should be interpreted.  Something\n>higher-level is required if, for example, a document using one of these\n>extended characters is ever to be displayed.\n\nI think you misunderstand what I mean. In this method, the private use\narea codes do *not* represent characters, but rather presentation hint\ndata. As such, applications can safely ignore this data entirely\n(ie. it would be thrown away at the lowest level) if they are simple\nminded. The results will be legible, and the higher level parser will\nwork with, or without them.\n\nAlbert Lunde writes:\n\n>At what point do char set issues get handed off to the HTML standard?\n>\n>It seems like some of the alternative glyph rendering issues\n>could get mixed in with things like font changes and presentation\n>control (which are possible, though controversial, issues for HTML).\n\nIn my opinion, HTML should not be concerned with character sets at\nall. If the parser assigns a characters' class based on Unicode, and\nthe lowest level of the application passes Unicode characters up to\nthe parser proper, then the parser is totally divorced from data\nstorage format. The ERCS proposal goes into more detail, and proposes\nbasically the above.\n\n>Of course text/html is not the only text/* type we might have to\n>transport....\n\nSurely most parsers for other textual data types need not grovel\naround at the data storage format level? Most can surely be abstracted\nenough that they can deal with a character as an atom of information\nrather than as a sequence of bytes?\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "At  9:51 AM 12/28/94 -0500, Gavin Nicol wrote:\n>Albert Lunde writes:\n>>Of course text/html is not the only text/* type we might have to\n>>transport....\n>\n>Surely most parsers for other textual data types need not grovel\n>around at the data storage format level? Most can surely be abstracted\n>enough that they can deal with a character as an atom of information\n>rather than as a sequence of bytes?\n\nThis is a reasonable assumption, I'm not sure if it is a safe assumption.\n\nSGML seems to take a more abstract view of text than an average software\npackage.\n\nThough it does seem like there aren't many text/* types actually registered\nso far to give us a guide as to what to expect. Some of the potential\ntext-like hard cases (i.e. postscript) are registered as application types.\n\nAccording to the list at:\n\n<ftp://ftp.isi.edu/in-notes/iana/assignments/media-types/media-types>\n\nregistered text types are:\n\ntext            plain                                   [RFC1521,NSB]\n                richtext                                [RFC1521,NSB]\n                enriched                                    [RFC1563]\n                tab-separated-values                   [Paul Lindner]\n\n\nThe comp.mail.mime FAQ lists unregistered text types in actual use (by some\nmail software) as:\n\nText types:\n\ntext/html               MHonArc: WWW HTML\ntext/unknown            Worldtalk\ntext/x-html             MHonArc: WWW HTML\ntext/x-setext           MHonArc: setext\ntext/x-usenet-faq       Ohio State WWW FAQ document format\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "character sets in HTTP: translation table",
            "content": "(The conversation about character sets seems to be intertwined on a\ncouple of mailing lists.)\n\n> 3. Is Unicode the answer?\n\nUnicode is one of the answers. It is perfectly possible to do\nmulti-lingual HTML without Unicode, for those who would prefer to do\nso. Unicode may be the answer for you and for me, but it's become\nabundantly clear that it isn't the answer for some folks, and we don't\nactually need to force the issue this way.\n\nI've found the capabilities alluded to in <URL:http://www.nada.kth.se/\ni18n/c3/> intruiging as part of \"the answer\", in particular, the\nability to translate richer character sets into legible US ASCII when\nnecessary (e.g., Lynx and Vt100 terminals with limited font\nrepertoires).\n\nReceivers might be expected to either (1) understand the character\nsets the sender is able to present or (2) call some proxy that is able\nto translate the document into a character set the client knows.\n\nOf course, those proxies and translators may well use Unicode\ninternally (as does C3), but you don't have to declare any particular\ncharacter set as the 'canonical' one.\n\nI'm thinking of a regime where:\n\n* conforming servers use only registered character encodings (\"charset\")\n* character encoding registration requires supplying translation\n  tables (making sure there are widely available replicated tables &\n  services) that allow translation (transliteration, etc.) of the\n  character encoding given to the other registered character\n  encodings, either directly or via some intermediate form.\n* Senders that wish to send documents in a non-standard character\n  encoding may do so, but only if they also (are willing to) send (a\n  pointer to a receiver-accessible copy of) the translation table for\n  that character encoding.\n\nIn this way:\n\n* conforming document senders are required to supply documents in\n  registered character encoding, but may choose whichever encodings\n  they want to use.\n* conforming document receivers may choose the character encodings\n  they're able to accept.\n* if a conforming sender and a conforming receiver don't have a common\n  character encoding, they may find a translation table to do the\n  mapping.\n\nWithin the context of HTTP, I think it is most reasonable to say that\nsenders *may* do translation, but receivers *must* do translation (or\nelse not display/accept the sender's document.)\n\nFortunately, code translation tables are small enough that we might\nactually expect receivers to be able to dynamically fetch them.\nCharacter set translation tables seem like a good application for URN\nreplication services.\n\nClients used unconnected from the network will of course have to have\nbuilt in the translation tables for the encodings used in the\ndocuments they're likely to encounter.\n\nI don't think that it is necessary or actually possible to require\nservers to translate the character sets of their documents. It might\nbe useful to ask that HTTP servers that supply documents in\nnon-standard (or infrequently used) character sets also be able to\nsupply the translation tables for those character sets.  But in\ngeneral isn't possible really to 'require' servers to do more work\nthan they're prepared to do.\n\nCurrent practice is that servers offer documents in the character sets\nthey have, and clients either display them correctly or attempt to\ntranslate them.  If you browse the net looking for web sites by\ncountry, you'll come up with lots of examples.\n\nFor example, <URL:http://www.ariadne-t.gr/apodimoi/index.html> says\n\"The information is in Greek using the ELOT 928 chraracter set. It\nwill display nicely in Mosaic for MS Windows IF you have installed the\nGreek version of Windows 3.1 or WfW.\"\n\nIf you look at <URL:http://www.free.net/Docs/cyrillic/notes.en.html>,\nit notes that the servers support ISO-8859-5 cyrillic character set,\nKOI8 charset, as well as two DOS and MS-Windows charsets.\n\nIf you look at <URL:http://www.huji.ac.il/WWW_DIR/default.html>,\nyou'll see HTML that 'assumes a VT terminal with hebrew characters'.\n\nIn general, users with large collections of documents in national\ncharacter sets will just make them available in those forms. The\nservers don't have the computational resources to translate the\ndocuments on-the-fly to Unicode, nor is such translation particularly\nefficient in terms of network bandwidth.\n\nThey're currently expecting the reader, if they're really interested,\nto obtain support for the character encoding used in the documents on\nthose servers. A regime where the documents are properly labelled, and\nthe client fetches the translation table the first time in order to\nproperly display the documents:\n\n a) will work well\n b) is efficient\n c) has a reasonable transition from current practice.\n\n================================================================\n>   3.2.3 Accept-charset\n\nAs for the HTTP protocol element, I think we might be better off with\n\n   accept-parameter: charset=unicode-1-1-utf7\n\nthan \n\n   accept-charset: unicode-1-1-utf7\n\nFor example, imagine that we may want to extend image/* types to have\na 'colors' and a 'width' and 'height' parameter, and to allow\n\n   accept: image/gif\n   accept: image/jpg\n   accept: image/tiff\n   accept-parameter: width<=640\n   accept-parameter: height<=480\n   accept-parameter: colors<=256\n\nIn general, accept-parameter could be defined to be \"indicate\nacceptable paramater values for those media types that take those\nparameters\". Unlike \"accept\", I think it should be within the protocol\nspec for the server to ignore accept-parameter and supply what it\nhas if it cannot translate.\n\n================================================================\nAs a final note, re:\n\n>  In addition, the MIME specification states that for the text/* data\n>  types, all line breaks must be indicated by a CRLF pair. This implies\n>  that certain encodings cannot be used within the text/* data types if\n>  the WWW is to be strictly MIME conformant.\n\nThe MIME draft standard makes no such claims. There is a document\nbeing circulated by the mail extensions working group which makes\nstronger claims about text/* data types, but that document is not yet\neven a proposed standard.\n\n\n\n"
        },
        {
            "subject": "Re: Connection Heade",
            "content": "Jim Seidman writes:\n>Jeffrey Mogul writes:\n>>My own intuition is that we can follow two paths:\n>>(1) Stick with the parallel connection approach, which gives\n>>reasonably good performance today but which might turn into\n>>a global performance disaster in the future.\n>>(2) Encourage people to shift ASAP to an alternative (besides\n>>the WIDTH and HEIGHT tags, several HTTP-level mechanisms have\n>>been proposed).  This might avert future global problems, but\n>>in the short term it might be a while before enough servers\n>>support it to make it highly beneficial.  I would also expect\n>>that in the long run (i.e., with widespread support), it would\n>>improve UPP because you wouldn't have to wait even for the initial\n>>bits of the images to arrive.\n>\n>If we go with something like an HTTP-based image hinting scheme, I don't\n>think the rate of adoption is a big an issue as you imply.  A browser like\n>Netscape could continue to have multiple parallel connections to older\n>servers if it wanted to.  Path (1) as you've described is really not a path\n>at all - it's just staying with the limits of what we have today.\n\nPath 1 is most definitely a path.  It happens to the be the path that\nwe'll probably follow if nothing is done, but it doesn't imply that\nwe will stay within the limits of our network resources ... it only\nstays within the limits of our current software implementations.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: character sets in HTTP: translation table",
            "content": "At 12:26 PM 12/28/94, Larry Masinter wrote:\n>> 3. Is Unicode the answer?\n>\n>Unicode is one of the answers. It is perfectly possible to do\n>multi-lingual HTML without Unicode, for those who would prefer to do\n>so. Unicode may be the answer for you and for me, but it's become\n>abundantly clear that it isn't the answer for some folks, and we don't\n>actually need to force the issue this way.\n\nI certainly agree that Unicode has to win on its own merits, rather than\nbeing stuffed down people's throats, but I think one of those merits is its\nability to act as a character set \"hub\" for translation. In that way, it\ncan be used for HTTP without either the content on the server, or the\ndisplay end on the client, having to know anything about Unicode. This use\nof Unicode is transparent to end users and suppliers of content, and so is\nreally an internal implementation detail of the client/server protocol.\n\n>\n>As for the HTTP protocol element, I think we might be better off with\n>\n>   accept-parameter: charset=unicode-1-1-utf7\n>\n>than\n>\n>   accept-charset: unicode-1-1-utf7\n>\n\nUTF-7 doesn't strike me as being appropriate for HTTP, since that's a\nguaranteed 8-bit protocol. It seems like UTF-8 or UCS-2 would be more\nappropriate. I realize people are just using it as an example now, but I\nthought I'd point this out.\n\n>\n>================================================================\n>As a final note, re:\n>\n>>  In addition, the MIME specification states that for the text/* data\n>>  types, all line breaks must be indicated by a CRLF pair. This implies\n>>  that certain encodings cannot be used within the text/* data types if\n>>  the WWW is to be strictly MIME conformant.\n>\n>The MIME draft standard makes no such claims. There is a document\n>being circulated by the mail extensions working group which makes\n>stronger claims about text/* data types, but that document is not yet\n>even a proposed standard.\n\nThis actually comes from an Internet draft, not a document circulating on\nthe mail extensions working group, and this is the next draft of the MIME\nspec, so while what you say is literally true, unless the draft is changed\nthis is what's going to be in the MIME spec in its next edition.\n\nDavid Goldsmith\nSenior Scientist\nTaligent, Inc.\n10201 N. DeAnza Blvd.\nCupertino, CA 95014-2233\ndavid_goldsmith@taligent.com\n\n\n\n"
        },
        {
            "subject": "Re: character sets in HTTP: translation table",
            "content": "Larry Masinter writes:\n\n>Unicode is one of the answers. It is perfectly possible to do\n>multi-lingual HTML without Unicode, for those who would prefer to do\n>so. \n\n  I have never said that Unicode is \"the only answer\", and in fact, I\n  specifically mention ways of *not* using it in my paper. Indeed,\n  there is absolutely no reason why what you outline, and what I\n  describe in my paper, cannot coexist. \n\n>Unicode may be the answer for you and for me, but it's become\n>abundantly clear that it isn't the answer for some folks, and we don't\n>actually need to force the issue this way.\n\n  I have heard many objections to Unicode, and upon discussion, I have\n  found that most are related to display, or unsupported language\n  feature issues. If we have \"some folks\" out there, please raise your\n  objections now: I would be very interested in hearing them, and even\n  more interested to hear if some kind of \"hint\" is not sufficient to\n  handle the objection.\n\n>I've found the capabilities alluded to in <URL:http://www.nada.kth.se/\n\n  Yes, this is interesting stuff, and can be used for part of what I\n  talked about.\n\n>Receivers might be expected to either (1) understand the character\n>sets the sender is able to present or (2) call some proxy that is able\n>to translate the document into a character set the client knows.\n.....\n[Some detail deleted]\n\n>Current practice is that servers offer documents in the character sets\n>they have, and clients either display them correctly or attempt to\n>translate them.  If you browse the net looking for web sites by\n>country, you'll come up with lots of examples.\n....\n[Some examples of charset specific browsers and servers deleted]\n\n>In general, users with large collections of documents in national\n>character sets will just make them available in those forms. \n\n  Because there is nothing else to make them available in?\n\n>They're currently expecting the reader, if they're really interested,\n>to obtain support for the character encoding used in the documents on\n>those servers. A regime where the documents are properly labelled, and\n>the client fetches the translation table the first time in order to\n>properly display the documents:\n>\n> a) will work well\n\n  This seems a rather premature claim.\n\n> b) is efficient\n\n  Hmm. When I think of 200,000 accesses from the US to a Japanese site\n  from a browser that does not support SJIS, EUC, Arabic, and Thai,\n  all of which are used within a single document (lord only knows\n  how!), resulting in 200,000 requests for each of those translation\n  tables, and compare this to a single server \"wasting\" cycles doing a\n  conversion to UTF-8 (which would probably be cached after the first\n  access), I have trouble imagining this to be more efficient.\n  Requiring the browsers to call a proxy is at least as inefficient as\n  server conversion (actually more so due to redirect), and relies\n  upon the same (or probably, more complicated) caching techniques for\n  performance. \n   \n  Worse, doing in-client translation will almost certainly require\n  16 bits *somewhere* within the client... and what is everything\n  being translated *into*? Unless you are translating into some\n  virtual character set (which would need to be at least as large as\n  the largest national character set: read 16 bits), or into Unicode,\n  or transliterating, you will also need to reprogram the parsers for\n  the different data formats you purport to support, so that they will\n  be able to recognise the characters as belonging to a particular\n  class of characters to be used in parsing (think of SGML parsing, or\n  even HTML parsing.) In addition, your proposal generally assumes a\n  single language, and a single character set per document, mine\n  assumes (in the \"core\" case), a single character set and multiple\n  languages (though your proposal can indeed emulate mine via proxy).\n\n  My proposal requires people to update clients once (to achieve basic\n  functionality). Your proposal requires alls client to also be changed\n  at least once to add support for translation tables (unless they be\n  forever stuck with transliteration, and the overhead thereof). My\n  proposal requires changes to servers, so does your proposal. \n\n c) has a reasonable transition from current practice.\n\n  True. So does my proposal. In the worste case, current practise (or\n  something close to it) is supported. \n\n>The servers don't have the computational resources to translate the\n>documents on-the-fly to Unicode, nor is such translation particularly\n>efficient in terms of network bandwidth.\n\n  I don't buy this, especially if the server uses good caching\n  techniques. \n\n  The biggest problem I have with this idea is that in the worste case\n  (and indeed, I would argue in many cases), the client and server\n  will not be able to communicate without help. Additionally, it\n  encourages the creation of browsers with widely disparate\n  capabilities, and version numbers (read: bug lists), and breaking the\n  web into \"villages\" with their own culture, own language, own GUI,\n  etc. \n\n  We need a common base, a lingua franca, so that in the worste case\n  (no external help available), we can communicate. Unicode is the\n  only thing I know of that offers a reasonable many->one->many\n  path. Additionally, it promises to offer a *single* processing model\n  for character data of *mixed* languages (at least until it needs to\n  be displayed), and most (all?) parsers could use Unicode a base for\n  character classification, and hence, the parsing tables would not\n  need to be reprogrammed. Eventually, free tools, and libraries\n  *will* appear to aide in processing, and display. \n\n> accept-parameter: charset=unicode-1-1-utf7\n\n  OK. This looks fine to me. (Perhaps utf8 or ucs-2??)\n\n>The MIME draft standard makes no such claims. There is a document\n>being circulated by the mail extensions working group which makes\n\n  Thank you for clarifying this.\n\nBTW. I tend to beleive that unless a program was specifically designed\nto manipulate charsets and/or encodings, that the actual processing\nwithin it can be abstracted away from concrete codes. If a\nprogram/system relies upon concrete codes, it is BAD (Broken As\nDesigned) in my books, though certainly, using Unicode codes gives a\ngreater range of flexibility than other code sets.\n\n\n\n"
        },
        {
            "subject": "Re: character sets in HTTP: translation table",
            "content": "I've watched this content encoding discussion for the past few weeks,\nwaiting for someone to ask this question. No one has, so I will.\n\nWhat business does a standard for a transport protocol have addressing the\nencoding schemes of the various data types it sends between a source and\ndestination?\n\nSpecifically, for ALL data types, HTTP is ignorant of the actual content of\nthe data it conveys from point A to point B. It supports passing a few\nheader fields between a server and a client that describe this content, but\nit doesn't in current practice require ANY dabbling in the content itself.\n\nIt seems that there is a mixing of apples and oranges in this discussion.\nAside from the possible trade infringements with Snapple and Fruitopia, it\nseems wise to separate the best way to encode text for WWW applications\nfrom the best way to transport that text for WWW applications. People are\nmixing WWW client and server implementation issues up with the HTTP\nstandard. WWW servers are not equivalent to a HTTP protocol engine. Servers\ndo much more work to find and interpret data before stuffing it into the\nHTTP pipe. Making text/* MIME types something that HTTP implementations\nhave to treat differently than all other data types doesn't seem\nconsistent. It may be that servers acting as interfaces between a file\nsystem and the HTTP world have to care. But HTTP is data type independent\nby its very nature. Is the HTTP standard document devolving into a \"How to\nbuild a server\" cookbook?\n\nLikewise, HTTP clients don't need to know how to interpret the data they\nreceive. The VIEWER functions that WWW client software pipes this data into\ndo need to understand this data, but as with servers, client side HTTP\nprotocol engines are not equivalent to WWW clients. Clients do a lot more\nafter the data is received that falls completely outside the scope of the\nHTTP protocol.\n\nIf the HTML people want to come up with a standard encoding for HTML\ndocuments, or the MIME folks want to further specify encodings for all the\nother text/* MIME types, that is fine. I don't understand why the HTTP\nstandards folks should have to care at all. If some sort of encoding is\nneeded for request and response headers, yes, I agree that it is a valid\ntopic of discussion. But given that the HTTP protocol supports a full 8 bit\nbinary pipe for transfers, issues such as encoding text for transport seem\nbeyond the scope of the HTTP standard.\n\nI am honestly not trying to split semantic hairs here. I agree that the\nmechanisms for standardizing text transport *over* HTTP connections need to\nbe put in place. I don't agree that the HTTP/1.x standard document is the\nplace to do this. This seems more properly the province of a RFC describing\na particular encoding scheme which *may* be transported via HTTP or any\nother number of protocols, for that matter. (An aside, this is true of all\nthe image hinting discussions as well.)\n\nI have an ongoing concern that people who may not realize the implications\nof their recommendations are continuing to place unreasonable demands on\nthe server side of HTTP implementations by demanding all sorts of data\nconversions in conjunction with the simple transport functions of the\nprotocol. If all of these proposals become part of the standard, servers\nwill be so bogged down with translation exercises as to make their\ntransport tasks impossible.\n\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: character sets in HTTP: translation table",
            "content": ">What business does a standard for a transport protocol have addressing the\n>encoding schemes of the various data types it sends between a source and\n>destination?\n\nWell, apart from the various Accept: proposals, not much. Still,\nthe discussion is broader than HTML, and HTTP must be at least \na little sensitive to the expected content (or we would not\neven have Accept-language:). Nothing lives in a vacuum.\n\nIf people generally feel this is not an appropriate place to conduct\nthese discussions, I would be happy to desist.\n\n\n\n"
        },
        {
            "subject": "new version of extensions proposa",
            "content": "I've produced a new version of my proposal for an HTTP extensions\nmechanism.  The easiest way to get to it is through\nhttp://www.research.att.com/~dmk/extend.txt\nwhich will provide pointers to the old proposal and the new one\n(.txt and .ps forms).\n\nThe new version includes more introductory material to motivate the\nproposal and more examples.\n\nThe documents have also been submitted as Internet Drafts.\n\nI welcome your comments.\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: character sets in HTTP: translation table",
            "content": "There are several places where character set issues do impact the design of \nHTTP; One example of this is in human readable meta data such as titles and\nauthor names. HTTP-NG defines all human readable headers to be variable length\nstrings of bytes - I've argued for making all such text be UTF-* encoded \nUCS-2 or UCS-4. \n\nYou're right in that the bitstream sent over the data channels is not\nan HTTP issue; however, using UCS-* as the preferred charset for HTML would\nprobably be a really good idea.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: new version of extensions proposa",
            "content": "Are these pages linked in to the HTTP page on www.w3.org?\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "  Date: Sun, 25 Dec 1994 10:56:45 -0500\n  From: Gavin Nicol <gtn@ebt.com>\n\n   For example, if some Japanese text was\n   sent to a person that cannot read Kanji, Hiragana, or Katakana, the\n   browser could conceivably map the Japanese text into something like\n   the following:\n\n       Nihongo, tokuni Kanji, wa totemo muzukashii desu.\n\nUnless you store bunsetsu boundaries and yomi (phonetic reading) along\nwith the original text, you can pretty much forget about automatic\ntransliteration of Japanese (or Chinese for that matter).\n\n   3. Is Unicode the answer?\n\n   In a word, YES! Though there are a number of issues that need to be\n   resolved in order for it to be used effectively.\n\nUnless one has a strong masochistic streak, Unicode (or ISO/IEC 10646)\nwill certainly be part of the answer.  However, Unicode wasn't designed\nto solve every problem facing designers of multilingual systems. Never-\ntheless, it makes a great foundation upon which to build such systems.\n[Like HTML, Unicode's designers knew they couldn't solve every problem\nif they wanted an implementable standard; the emphasis on a plain text\nencoding which intentionally disregarded certain higher-level language\nspecific issues was an explicit design decision to aid in achieving\nan implementable result.]\n\nFolks should be aware the Unicode (as a profile of ISO/IEC 10646) is\nessentially becoming a national standard in many countries: e.g.,\nJapan expects to publish JIS X 0221 this coming year which will be\na national standard version of ISO/IEC 10646; China has already\npublished GB 13000, also a national standard version of 10646. I\nwouldn't be surprised to see Korea do the same in the not too distant\nfuture.\n\n  Date: Fri, 30 Dec 1994 07:44:26 -0500\n  From: Gavin Nicol <gtn@ebt.com>\n\n  >>ISO 8879 also defines some methods for handling things like ISO-2022,\n  >>but some encodings for languages such a Thai cannot be handled by\n  >>SGML, even if the SGML declaration is altered (though, it is possible\n  >>for the application to deal with this within, or before, the entity\n  >>manager).\n\n  >Just out of curiosity, what is so special about Thai?\n\n  One encoding is very complicated: variable length, and no canonical\n  order for certain bytes. In practise, various groups define a local\n  \"standard\".\n\nI'm afraid I have to disagree with Gavin's original statement.  There\nis nothing difficult about employing Thai in SGML.  The encoding is\nactually not complicated at all, is not variable length, and the fact\nthere is no canonical order for certain combining characters is\nirrelevant since they have a fixed position on the base consonant\nirregardless of their coding order in a sequence of multiple combining\nmarks after a base consonant.  Further, there is a standard Thai encoding,\nTIS 620, which is now commonly used. Keep in mind that the Unicode Thai  \ncharacter block is based on TIS 620.\n\nGavin, please note that a variable length encoding (in the\ncontext you were speaking of, i.e., vis-a-vis ISO 2022) means that\na variable number of bytes are used to encode one coded character element.\nIn the case of the Thai character set TIS 620 (and Unicode), a fixed\nnumber of bytes is used to encode each coded character element (1 byte\nfor TIS 620, and 2 bytes for a canonical UCS-2 Unicode character). The\nfact that multiple coded character elements in TIS 620 (and Unicode)\ncan combine graphically to fill a single display cell does not mean\nthat either of these character sets are \"variable length encodings\".\nThe units of processing for transmission of information are still fixed\nlength regardless of number of display cells used to display character\ndata.\n\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": ">Unless you store bunsetsu boundaries and yomi (phonetic reading) along\n>with the original text, you can pretty much forget about automatic\n>transliteration of Japanese (or Chinese for that matter).\n\nWell, 100% accurate transliteration, or transcription, like 100%\naccurate machine translation, is one of those \"hard\" problems.\n\nI have seen systems that can take a paragraph of kana and convert it\ninto kanji, or take a paragraph of kanji, and convert it into kana.\nThe accuracy is about 98%.\n\nI have also seen machine translation systems working on PC's that can\nachieve readable, if not grammatically correct English and Japanese. \n\nResource usage for the above systems is still fairly high: about on\npar with a typical Microsoft application in fact :-)\n\nNot impossible, not impractical, but not 100% perfect either. Hmm,\nsounds familiar...\n\n\n\n"
        },
        {
            "subject": "Re: Draft 01 of HTTP/1.",
            "content": "[Roy Fielding responds patiently to:]\n> [Paul Burchard's hasty comments on the HTTP draft:]\n> > (1)  In the definition of the Accept request header, there is no  \n\n> > mention of a \"version\" modifier, e.g.,\n> > Accept: text/html; version=3.0\n> \n\n> The BNF for Accept (Section 5.5.8) includes *(\";\" parameter):\n\nAh, so it does!  Still, I think it's worth pointing this out  \nspecially, because gauging client capabilities reliably has been a  \nreal problem in the past.\n\n> > (2)  The semantics of Expires should discourage clients from  \n\n> > _indiscriminately_ trying to refetch objects, just because\n> > they have expired.  Dynamically-created pages can be ephemeral,\n> > without having time-dependent content that needs to be updated.\n> \n\n> I will try to clarify that.\n\nUnfortunately, what needs to be clarified is that my interpretation  \nof Expires was incorrect. :-)\n\nAs Chris Lilley put it, I was taking a server-centric view of  \nexpiration, whereas Expires was always intended to refer to the  \nclient's viewpoint.  So, expiration *does* mean that the client is  \nencouraged to re-fetch the URL.  I've updated my own server scripts  \naccordingly.\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@math.utah.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: Formalizing Expires, LastModified, &quot;data object&quot",
            "content": "Daniel W. Connolly <connolly@hal.com> writes:\n> I suggest that the protocol be defined by the observable\n> behaviour of correct servers (and clients). For\n> example, the critical property of the Last-Modified and\n> Expires information for a URI is that the server\n> stipulates that it will serve the same entity for that URI\n> (given the same Accept: headers) at all times between the\n> Last-Modified time and the Expires time.\n\nThis is also how I first interpreted Expires -- as a server guarantee  \nthat the URI produces constant data up to the stated time.  However,  \nI've been convinced now that the spec's interpretation -- as a client  \ndirective to discard the information after the given time -- is more  \nuseful.\n\nThe practical considerations are shown most clearly for servers that  \nwant to serve ephemeral documents, custom-produced for each client's  \nrequest (note that even if the document itself is POST output, its  \ninline images will still be obtained by ordinary GET requests, and so  \nExpires still applies).\n\nThe server does not want to have to keep the custom document around  \nafter the requesting client has fetched it; on the other hand, the  \nserver does want the client to enjoy the document for the full  \nintended life of the information it contains.  If Expires is  \ninterpreted as a server guarantee there is no way to communicate the  \n(more important) intended life of the document to the client.\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@math.utah.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "Hi,\n\nThe pragmatic proposal below is driven by a need to meet an existing\n(and pressing) need and a desire not to a derail long-term multilingual\nsolutions.\n\nMy assumptions:\n        o There's lot of Japanese Web pages in ISO2022-JP that we need to\n          be able to browse today.\n\n        o Lots more non-Latin1 text files will be (or already are) created\n          on the Web.\n\n        o Changes must not require changes of Web file contents.\n\n        o Unicode in one form or another will be used for future Web pages.\n\n        o New clients should not break existing servers and new servers\n          should not break existing clients (backwards-compatibility).\n\nComments, please!\n\n-bob\n============================================================================\n\"Accept-Charset\" and \"charset\" Support for Web Browsers\n\nIn order to render single language (actually single character set encoding)\ntext files on the Web correctly, a mechanism is needed to identify the\ncharacter set encoding per text file.  For example, files encoded in\nISO2022-JP should be rendered as Japanese and files encoded in ISO8859-1\nshould be rendered as Latin characters.  Currently, there is no\ndeterministic way to know the character set encoding of a text file.\n\nThe MIME content type header provides a mechanism for this by means of the\n\"charset=xxx\" parameter.\n\nFor example:\n        Content-Type: text/plain; charset=ISO_8859-1:1987\nor\n        Content-Type: text/html; charset=ISO-2022-JP\n\nThe problem is that many browsers today do not parse for parameters and\nwill be confused by the above examples.   Some browsers will take the\nentire string \"text/plain; charset=ISO_8859-1:1987\" instead of \"text/plain\"\nas the content type.\n\nTherefore, I suggest that charset-parameter-savvy browsers, send servers a\nnew accept header, \"Accept-Charset\".   This would look like:\n\n        Accept-Charset: ISO_8859-1:1987\n        Accept-Charset: ISO-2022-JP\n\nThe  \"Accept-charset\" header was proposed by Gavin Nichol in a document\nsent to the  several mailing lists (html.wg, http.wg, www.unicode),\n\"Handling Multilingual Documents in the WWW\". See\nhttp://www10.w3.org/hypertext/WWW/Administration/Mailing/Outside_mailing.htm\nl\n\nI propose that servers only send the MIME charset parameter if it has\nreceived an \"Accept-Charset\" from the browser.  This convention will\nprevent compatibility problems with current browsers.\n\nThe charset-parameter-savvy browsers should send \"Accept-Charset\" headers\nfor the charsets they recognize.\n\nThe \"Accept-Charset\" header should NOT restrict servers from sending text\nfiles in other charsets.  It is the browsers' responsibility to handle\nunsupported charsets gracefully.\n\nIf the browser receives text files without charset information, then the\nbehavior will be implementation dependent.  In this case, I suggest that\nthe browser use a per-window default.   This allows knowledgeable users to\nread Japanese newsgroups in one browser window and French newsgroups in\nanother one, even if the charset is not specified in the headers.  Ideally,\nall text files will provide charset headers, but the per-window default\nwould provide users with a means to deal with unidentified text data.\n\nIt is the browsers responsibility  to know how to render the text file\ncorrectly.  This may require converting from the character set encoding of\nthe file to another internal character set encoding.  Whether this internal\nencoding is Unicode or some other encoding is implementation dependent.\n\nIn the future, there may be HTML tags that specifies character set encoding\nat a finer granularity (i.e., per-string vs. per URL).    These HTML tags\nmay be required to implement multilingual HTML documents.  When (or if)\nthese tags exist, they would take precedence over the MIME charset header\ninformation.\n\nHowever, the MIME charset header information will remain useful for new and\n(especially) existing \"single\" language documents.  The MIME charset\ninformation will allow existing documents to be rendered correctly without\nmodifying their contents by adding new HTML tags.\n\n\n\n"
        },
        {
            "subject": "Re: [www-mling,00149] AcceptCharse",
            "content": ">> While I would like to take credit for Accept-Charset, I cannot:\n>> Larry Masinter actually proposed it after he and I discussed this\n>> issue offline.\n>\n>I can't take credit for it, either; I'm not up for doing the\n>historical research to see where it came from.\n\nI'm glad we're all being intellectually honest.  Or are we just\ntrying to escape the blame :-)\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "In the HTML2 spec in  Section 2.4, sub-heading \"Character sets\"\n(http://www.ics.uci.edu/pub/ietf/html/html2/htmlspec281194_9.html#HEADING16),\nthere is the statement that the charset parameter is reserved for future use:\n\n        Character sets\n                The charset parameter is reserved for future use. See Section\n                2.16 for a discussion of character sets and encodings in HTML.\n\nDoes someone know the history behind this paragraph?\nHow does this affect our discussions about using the charset parameter?\n\nthx,\nbob\n\nBob Jung                        +1 415 254-1900 x2788   fax +1 415 254-2601\nNetscape Communications Corp.   501 E. Middlefield      Mtn View, CA 94041\n        NEW PHONE # on Jan 9: +1 415 568-2688\n\n\n\n"
        },
        {
            "subject": "charset= history, proposa",
            "content": "In message <199501062216.OAA00888@neon.mcom.com>, Bob Jung writes:\n>In the HTML2 spec in  Section 2.4, sub-heading \"Character sets\"\n>(http://www.ics.uci.edu/pub/ietf/html/html2/htmlspec281194_9.html#HEADING16),\n>there is the statement that the charset parameter is reserved for future use:\n>\n>        Character sets\n>                The charset parameter is reserved for future use. See Section\n>                2.16 for a discussion of character sets and encodings in HTML.\n>\n>Does someone know the history behind this paragraph?\n\nI suppose I do. If you could be more specific, I might be able to\ngive a better answer.\n\nBut the gist of it is: MIME (rfc1521) defines some semantics for\ntext/* charset=... which make a certain amount of sense for the web,\nbut aren't widely supported.\n\nSo the 2.0 can't say \"do what MIME says\" cuz then it wouldn't be\ndescriptive of current practice. So it says \"don't use charset= at\nall. Just use ISO-8859-1 implicitly all the time\". That was the only\nsemantics we were going to standardize.\n\nThe theory was that we wanted to get the 2.0 document out in short\norder, and that this issue could wait until later.\n\nNow it's later, and this issue clearly needs addressing.\n\n>How does this affect our discussions about using the charset parameter?\n\nIt means that we're now figuring out the \"reserved for future use\"\nsemantics.\n\nRegarding your proposal <199501060510.VAA15861@neon.mcom.com>...\n\nFirst: it looks good in general.\n\nBut I'd like to take a look at it from a few perspectives:\n\n(1) the language lawyer/formal specs perspective\n(2) the information provider perspective\n(3) the information consumer perspective\n\nrom a formal perspective, I think whatever we come up with should be\nconsistent with MIME. But we'll have to see what that costs...\n\nLet's take a close look at rfc1521 and see how it constrains the semantics\nwe want to define:\n\n|7.1.1.     The charset parameter\n|\n|[...]\n|\n|   The default character set, which\n|   must be assumed in the absence of a charset parameter, is US-ASCII.\n\nThis conflicts somewhat with your proposal. However, the RFC goes on\nto say...\n\n|   The specification for any future subtypes of \"text\" must specify\n|   whether or not they will also utilize a \"charset\" parameter, and may\n|   possibly restrict its values as well.\n\nI wonder if changing the default from \"US-ASCII\" to\n\"implementation-dependent\" can be considered \"restricting the values\"\nof the charset parameter.\n\nI suppose the relevant scenario is where an info provider serves up\nan ISO2022-JP document with a plain old:\nContent-Type: text/plain\nheader. I gather that this is current practice.\n\nThe intent of MIME is that a mail user agent, on seeing text/* with no\ncharset= parameter, can reasonably blast that document to any device\ncapable of handling US-ASCII.\n\nThat intent is already mucked up somewhat by the fact that normal html\ndocuments are allowed to have bytes>127, which are normally\ninterpreted as per ISO8559-1. So we already have the situation where a\nconforming HTTP client, say on a DOS box might retreive a text/html\ndocument and pass it over to a conforming MIME user agent, which would\nthen blast it to the screen. The user would lose, cuz the bytes>127\nwould get all fouled up.\n\nThere was some talk about making the default content type for text/*\nin HTTP be ISO8859-1. In that case, the conforming HTTP client should\nslap on \"charset=ISO8559-1\" when it passes the data to the MIME user\nagent, to make up for the impedence mismatch. Then the user wouldn't\nlose, because the MIME user agent would be obliged to fix up the\nbytes>127 as per ISO8559-1.\n\nBut... back to the case of ISO2022-JP encoded data tagged as plain\n\"text/html\". The business of slapping \"charset=ISO8559-1\" on the end\nwould muck things up. So where do we assign fault?\n\n\nMy vote is to assign fault at the information provider for serving up\n-JP encoded data without tagging it as such.\n\nSo all those information providers serving up ISO2022-JP data without\ntagging it as such are violating the protocol. This doesn't prevent\nNetScape and other vendors from hacking in some heuristic way to\nhandle such a protocol violation. But the spec shouldn't condone this\nbehaviour.\n\n\nOk... so now let's suppose all the information providers agree to\nclean up their act. Somehow, they have to get their HTTP servers to\ntag -JP documents as such.\n\nHow do they do this? File extension mappings? It's not relavent to the\nHTML or HTTP specs, but I think the overall proposal is incomplete\nuntil we have a workable proposal for how to enhance the major httpd\nimplementations to correctly label non-ISO8559-1 documents.\n\n\nThen web clients will start seeing:\n\nContent-Type: text/html; charset=\"ISO2022-JP\"\n\nMany of them will balk at this and punt to \"save to file?\" mode.\n\nIs that a bad thing? For standard NCSA, Mosaic 2.4, no, because\nit can't do any reasonable rendering of these documents anyway.\n\nBut what about the multi-localized version of Mosaic? Does it handle\ncharset=... reasonably? What's the cost of enhancing it to do so and\ndeploying the enhanced version?\n\nThe proposal says that the server should not give the charset=\nparameter unless the client advertises support for it. I think that\nwill cause more trouble than its worth (see the above scenario of\nuntagged -JP documents being passed from HTTP clients to MIME user\nagents on a DOS box.)\n\nOne outstanding question is: does text/html include all charset=\nvariations or just latin1? That is, when a client says:\n\nAccept: text/html\n\nis it implying acceptance of all variations of html, or just latin1?\n\nTo be precise, if a client only groks latin1, and it says accept:\ntext/html, and the server sends ISO2022-JP encoded text, and the user\nloses, is the fault in the client for not supporting ISO2022-JP, or at\nthe server for giving something the client didn't ask for?\n\nFirst, \"text/html\" is just shorthand for \"text/html; charset=ISO8859-1\"\nso the client didn't advertise support for -JP data.\n\nBut \"giving somethign the client didn't ask for\" is _not_ an HTTP\nprotocol viloation (at least not if you ask me; the ink still isn't\ndry on the HTTP 1.0 RFC though...). It's something that the client\nshould be prepared for.\n\nAs above, the server is bound to give \"charset=ISO2022-JP\" if it is\nnot returning latin1 data. So the client does know that it's not\ngetting latin1 data. It has the responsibility to interpret the\ncharset correctly, or save the data to a file or report \"sorry, I\ndon't grok this data\" to the user. If it blindly blasts ISO2022-JP\ntagged data to an ASCII/Latin1 context, then it's broken.\n\n\nDoes this mean that charset negociation is completely unnecessary?\nNo. It's not necessary in any of the above scenarios, but it would be\nnecessary in the case where information can be provided in, for\nexample, unicode UCS-2, UTF-8, UTF-7, or ISO2022-JP, but the client\nonly groks UTF-8.\n\nIn that case, something like:\n\nAccept-Charset: ISO8859-1, ISO2022-JP\n\nor perhaps\n\nAccept-Parameter: charset=ISO8859-1, charset=ISO2022-JP\n\nI'm not convinced of the need for the generality of the latter syntax.\nBesides: we ought to allow preferences to be specified ala:\n\nAccept-Charset: ISO8859-1; q=1\nAccept-Charset: Unicode-UCS-2; q=1\nAccept-Charset: Unicode-UTF-8; q=0.5\nAccept-Charset: Unicode-UTF-7; q=0.4\nAccept-Charset: ISO2022-JP; q=0.2\n\nwhich says \"if you've got latin1 or UCS2, I like that just fine. If\nyou have UTF-8, UTF-7, or -JP, I'll take it, but I won't like it as\nmuch.\"\n\nI'm still not sure this is exactly the right syntax: it's does allow\nyou to sayt that you'll take text/plain in several different charsets,\nbut text/html in only one, for example.\n\nOf course the bandwidth necessary to express the common cases should\nbe minimized...\n\n\nDaniel W. Connolly        \"We believe in the interconnectedness of all things\"\nSoftware Engineer, Hal Software Systems, OLIAS project   (512) 834-9962 x5010\n<connolly@hal.com>                             http://www.hal.com/%7Econnolly\n\n\n\n"
        },
        {
            "subject": "Query on 5.5.8 Accep",
            "content": "The augmented BNF for Accept shows that only a single media type may\nbe specified, but the second paragraph implies that multiple types\nare allowed, and two of the examples show a syntax with commas.\nSomething missing in the BNF?\n\nMike Cowlishaw\nIBM UK Labs.\n\n\n\n"
        },
        {
            "subject": "No More Passwords In The Clear in HTTP",
            "content": "I just read a very interesting proposal:\n\nFrom: \"Electronic Commerce Standards for the WWW (Spyglass)\"\nhttp://www.spyglass.com/techreport/stdsec.htm\nTue Dec  6 21:54:52 1994\n\n|Simple Authentication - OPTIONAL\n|\n|This scheme, proposed by Spyglass, uses a random challenge sent from\n|the server to the client. The client encodes the random challenge\n|using the user's password as an encryption key in order to establish\n|authentication. See Note B for a full specification.\n|\n|This method is currently indicated as OPTIONAL, but Spyglass believes\n|that it should become REQUIRED for HTTP compliance.\n\n\nThis was something of an eye-opener. It's so simple. We should have\nbeen doing this all along. There was never any reason to send\npasswords in the clear (well, uuencoded), given HTTP's two-round-trip\nauthentication mechanism.\n\nWhy is this nifty proposal tucked away in a corner? Why didn't I hear\nabout it before now? I thought I was pretty tuned in to this sort of\nthing...\n\n\nFor the longest time, I was under the impression that the web user\nbase would have two choices:\n\n1. Use a free browser, and access only public information, or\nsend your password essentially in the clear to subscribe to\nfor-pay info.\n\n2. Use a commercial browser that supports the security\noptions (SHTTP, SSL, kerberos...) supported by the services\nyou use.\n\nThe reason I believed this was that real security is to expensive to\ndevelop to give away (and it almost always requires a license of some\nkind...).\n\nAs a result, information providers are faced with the unfortunate\nchoice between:\n\n1. Require users to send passwords essentially in the clear.\n\n2. Require users to license a browser.\n\n\nThis message is a call to eliminate passwords-in-the-clear from HTTP.\nThis means the browser developers should implement something like the\nspyglass proposal (it looks like a few hours more work to upgrade to\nthis from the existing basic auth. scheme), and subscription-based\ninformation providers should _strongly_ encourage their user base to\nupgrade. Something like:\n\n\"Please upgrade to a browser that doesn't send passwords in\nthe clear (such as... links to recommended browsers.). In 6\nmonths, we will not be accepting Basic authentication.\"\n\n\nI suggest that Spyglass lead the way by releasing source code patches\nto lynx and Mosaic to implement this enhancement, just to show how\nit's done. It should be a \"simple\" excercise for them.\n\nUsing high-quality services on the web should not be an incentive to\nsend passwords over the net in the clear. Let's fix this!\n\nDan\n\np.s. I hear s-key is another simple technology that eliminates the\nneed to send passwords in the clear. But for the life of me, I can't\nfind a technical description of it. Is there an RFC that I just can't\nfind? Could somebody send me a pointer?\n\n\n\n"
        },
        {
            "subject": "Re: No More Passwords In The Clear in HTTP",
            "content": "On Mon, 9 Jan 1995, Daniel W. Connolly wrote:\n> Why is this nifty proposal tucked away in a corner? Why didn't I hear\n> about it before now? I thought I was pretty tuned in to this sort of\n> thing...\n\nEric from Spyglass posted to www-talk a proposal for using MD5 encryption \nin a system like this a few weeks ago - it looked solid, and I'm waiting \nfor a server and a browser to implement it (WN and Arena maybe?) so I can \nset it up for HotWired.\n\n> The reason I believed this was that real security is to expensive to\n> develop to give away (and it almost always requires a license of some\n> kind...).\n\nOnly until 1997!  :)\n\n> This message is a call to eliminate passwords-in-the-clear from HTTP.\n> This means the browser developers should implement something like the\n> spyglass proposal (it looks like a few hours more work to upgrade to\n> this from the existing basic auth. scheme), and subscription-based\n> information providers should _strongly_ encourage their user base to\n> upgrade. Something like:\n> \n> \"Please upgrade to a browser that doesn't send passwords in\n> the clear (such as... links to recommended browsers.). In 6\n> months, we will not be accepting Basic authentication.\"\n\nrom a quick glance at the list of browsers used on our site, less than \n%2 are more than 4 months behind the current rev of their browser, so I \ndon't see that as a huge issue.  However the above statement implies that a \nserver can negotiate which type of authentication can be used:\n\nS: Here's a challenge.  Encrypt it.\nC: Huh?  \nS: oh, nevermind.  Send me your uuencoded password.\nC: okay, here goes....\n\n...which doesn't seem to be in the specs anywhere.  I'd prefer not to \nhave two separate URL's for different authentication schemes, though I \ncould hack around that by keeping around a list of browsers implementing \nchallenge-response.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: No More Passwords In The Clear in HTTP",
            "content": "At 1:49 PM 1/9/95, Daniel W. Connolly wrote:\n>From: \"Electronic Commerce Standards for the WWW (Spyglass)\"\n>http://www.spyglass.com/techreport/stdsec.htm\n\n>|Simple Authentication - OPTIONAL\n>|\n>|This scheme, proposed by Spyglass, uses a random challenge sent from\n>|the server to the client. The client encodes the random challenge\n>|using the user's password as an encryption key in order to establish\n>|authentication. See Note B for a full specification.\n>|\n>|This method is currently indicated as OPTIONAL, but Spyglass believes\n>|that it should become REQUIRED for HTTP compliance.\n>\n>This was something of an eye-opener. It's so simple. We should have\n>been doing this all along. There was never any reason to send\n>passwords in the clear (well, uuencoded), given HTTP's two-round-trip\n>authentication mechanism.\n\nThis does look like a good idea. My one concern is that we'd want to make\nsure various extension mechanisms could live together before standardizing\nan Extension: header, but they seem to think this is not essential to make\nthe protocol work.\n\n(Is there any other MD5 PW authentication proposals for WWW lurking around\nin draft form? I think I've seen this before, but I'm not sure, and I think\nit's been done lately for other protocols, too.)\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: No More Passwords In The Clear in HTTP",
            "content": "In message <Pine.BSD.3.91.950109121342.19279d-100000@get.wired.com>, Brian Behl\nendorf writes:\n>Brian\n>\n>--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\n>brian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\nYikes! Jinks! I asked for a reference to s-key in my p.s.\nBrian replies to other issues, but includes the address of\nhis home-page.\n\nDan wastes a little time surfing Brian's home-page, and subconsiously\nfollows these links...\n\nhttp://www.hotwired.com/Staff/brian/\nhttp://www.hotwired.com/Staff/brian/links.html\nhttp://www.ccs.neu.edu/home/thigpen/index.html\nhttp://www.ccs.neu.edu/home/thigpen/html/interests.html\nhttp://www.ccs.neu.edu/home/thigpen/html/security.html\n\nWhich has a handy reference to the S/Key paper from bellcore:\nhttp://www.ccs.neu.edu/home/thigpen/docs/security_papers/ISOC.symp.ps\n\n\nAfter reading the S/Key paper, I think we should consider it in place\nof the simple challenge/response system.\n\nAdvantages of S/Key:\n\n* passwords are _not_ stored on the server side in clear\nform.\n* user can securely use the same password at different sites\n* password can be changed without sending it over the net\n\nDrawbacks:\n* server-side passwd database is not read-only: server must\nupdate the user's count of logins each time\n* doesn't support the opaque=\"...\" feature of the spyglass proposal\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: [wwwmling,00154] charset parameter (long",
            "content": "The goal of my proposal is to\n        (1) Provide a means for new servers & browsers to correctly\n            handle existing (unmodified) Web data in various character\n            set encodings.\n        (2) Not to break the current servers and browsers (anymore than\n            they are already) with regards to handling these code sets.\n\nThe proposal does not try to fix things that are broken in existing\nclients/servers.\n\nI agree with Larry Masinter <masinter@parc.xerox.com>\nthat we should replace the\n\n        Accept-charset=xxx\nwith the\n        accept-parameter charset=xxx\n\nrequest header in this proposal.  Larry, thanks for the update.\n\nHere are my replies to the thoughtful comments of:\n        Daniel W. Connolly <connolly@hal.com>\n        Ken Itakura <itakura@jrdv04.enet.dec-j.co.jp>\n\nDaniel>|7.1.1.     The charset parameter\nDaniel>|\nDaniel>|       [...]\nDaniel>|\nDaniel>|   The default character set, which\nDaniel>|   must be assumed in the absence of a charset parameter, is US-ASCII.\nDaniel>\nDaniel>This conflicts somewhat with your proposal. However, the RFC goes on\nDaniel>to say...\nDaniel>\nDaniel>|   The specification for any future subtypes of \"text\" must specify\nDaniel>|   whether or not they will also utilize a \"charset\" parameter, and may\nDaniel>|   possibly restrict its values as well.\nDaniel>\nDaniel>I wonder if changing the default from \"US-ASCII\" to\nDaniel>\"implementation-dependent\" can be considered \"restricting the values\"\nDaniel>of the charset parameter.\n\nI agree that if the charset parameter is not specified, the default\n***should*** be US-ASCII (or ISO8859-1, if it's been changed).\n\nUnfortunately, since charset was reserved for future use, Japanese servers\nhad no choice but to serve non-Latin files without a charset parameter!!\n\nWhy don't we enforce the default for servers using a future version of the\nHTTP protocol? ...and let current versions be \"implementation dependent\"\nin order to preserve backwards compatibility?\n\nDaniel>I suppose the relevant scenario is where an info provider serves up\nDaniel>an ISO2022-JP document with a plain old:\nDaniel>        Content-Type: text/plain\nDaniel>header. I gather that this is current practice.\n\nYes, this is the current practice (for text/html too).  Additionally, some\nfiles are sent in SJIS and EUC code set encodings with the same headers.\n\nDaniel>That intent is already mucked up somewhat by the fact that normal html\nDaniel>documents are allowed to have bytes>127, which are normally\nDaniel>interpreted as per ISO8559-1. So we already have the situation where a\nDaniel>conforming HTTP client, say on a DOS box might retreive a text/html\nDaniel>document and pass it over to a conforming MIME user agent, which would\nDaniel>then blast it to the screen. The user would lose, cuz the bytes>127\nDaniel>would get all fouled up.\n\nYes, this situation is broken for current browsers/servers and I do not\npropose to fix it.  Using my proposal, a new DOS browser would send:\n\n        accept-charset=x-pc850\n\nand a new server would send back:\n\n        Content-Type: text/html; charset=ISO8859-1\n\nand the new DOS browser should convert it to PC 850 for rendering.\n\nDaniel>But... back to the case of ISO2022-JP encoded data tagged as plain\nDaniel>\"text/html\". The business of slapping \"charset=ISO8559-1\" on the end\nDaniel>would muck things up. So where do we assign fault?\nDaniel>My vote is to assign fault at the information provider for serving up\nDaniel>-JP encoded data without tagging it as such.\n\nWe are not trying to fix existing browsers/servers.\n\nIf a new charset-enable server slaps the wrong charset header (or fails\nto slap on a header for non-Latin1) to a new charset-enabled browser, it\nis the server's fault.\n\nDaniel>So all those information providers serving up ISO2022-JP data without\nDaniel>tagging it as such are violating the protocol. This doesn't prevent\nDaniel>NetScape and other vendors from hacking in some heuristic way to\nDaniel>handle such a protocol violation. But the spec shouldn't condone this\nDaniel>behaviour.\n\nUnfortunately, the spec is lagging behind the implementations.  The spec\ndid not provide a means for the existing servers to resolve this problem.\nPragmatically, I cannot introduce a server or client product that breaks\nestablished conventions.\n\nAs mentioned above, can't this be handled with HTTP versioning?\n\n        HTTP V1.0 && no charset paramter == implementation defined\n        HTTP V3.0(?) && no charset parameter == IS08859-1\n\nDaniel>Ok... so now let's suppose all the information providers agree to\nDaniel>clean up their act. Somehow, they have to get their HTTP servers to\nDaniel>tag -JP documents as such.\nDaniel>\nDaniel>How do they do this? File extension mappings? It's not relavent to the\nDaniel>HTML or HTTP specs,\nDaniel>but I think the overall proposal is incomplete\nDaniel>until we have a workable proposal for how to enhance the major httpd\nDaniel>implementations to correctly label non-ISO8559-1 documents.\n\nYes, I explicitly left this out my proposal, but you're right, we need\nto discuss the implications.\n\nKen> - Before encouragement to label correctly for non-ISO8859-1, we must\nKen>   give servers the way to know what they should label it. Otherwise,\nKen>   nobody can blame the server that distribute illegal information.\nKen>\nKen>The third one has the difficult problem. For the situation for the mail\nKen>may simple, since the user knows he knows what encoding he use now, so\nKen>he can specify the correct label before sending it. (The user who doesn't\nKen>know about encoding at all must not use default encoding.) But for the\nKen>situation for the web documents is difficult. I think the file extension\nKen>mapping nor the classification by the directory structure is not suitable.\nKen>My current Idea is 'server default' + 'directory default' + 'mapping file'.\nKen>But I myself don't like my idea. Does anyone have more elegant idea?\n\nInitally, I assume most web data will be configured on a directory or file\nbasis. I imagine most files will be configured by what directory they live\nin. This should be relatively easy extension for existing servers and\nhow they parse their config files.\n\nFiles like Japanese .fj newsgroups (in ISO2022-JP) are already organized by\ndirectories.  So are a lot of Japanese Web pages.\n\nA web site with versions of the same files in different encodings\n(e.g., SJIS, EUC and JIS) or languages (e.g., English and Japanese) could\ncreate separately rooted trees with the equivalent files in each tree.\nThe top page could say click here for SJIS/EUC/JIS or English/Japanese.\n\nFile-by-file basis would be supported too, but I'd expect this to be\nused infrequently.  Besides, this would be a Web server administrator's\nnightmare to maintain the configuration database.\n\nI don't like the idea of new extensions, although the current server\nsoftware probably could support this.  I think the data should really\nidentify itself and not rely upon extensions.  Also, we don't want\nto make people rename their files.  For example, how are you going\nto rename the news archives?\n\n<TANGENT= warning not relevant to current proposal>\n\nUltimately, I'd like the content itself to specify the encoding.\n\nOne idea, is for a HTML <charset> tag that would take precedence over the\nMIME header:\n\n   <html>\n   <charset=xxx>\n   <head> <title> DOCUMENT TITLE GOES HERE </title> </head>\n   <body>\n   <h1> MAJOR HEADING GOES HERE  </h1>\n\n        THE REST OF THE DOCUMENT GOES HERE\n\n   </body>\n   </html>\n\n</TANGENT>\n\nDaniel>Then web clients will start seeing:\nDaniel>\nDaniel>        Content-Type: text/html; charset=\"ISO2022-JP\"\n\nOnly new charset-enabled clients will see this.\n\nDaniel>Many of them will balk at this and punt to \"save to file?\" mode.\nDaniel>\nDaniel>Is that a bad thing? For standard NCSA, Mosaic 2.4, no, because\nDaniel>it can't do any reasonable rendering of these documents anyway.\nDaniel>\nDaniel>But what about the multi-localized version of Mosaic? Does it handle\nDaniel>charset=... reasonably? What's the cost of enhancing it to do so and\nDaniel>deploying the enhanced version?\nDaniel>\nDaniel>The proposal says that the server should not give the charset=\nDaniel>parameter unless the client advertises support for it. I think that\nDaniel>will cause more trouble than its worth (see the above scenario of\nDaniel>untagged -JP documents being passed from HTTP clients to MIME user\nDaniel>agents on a DOS box.)\n\nWhy is this more trouble?  It's broken now and remains broken.  In either\ncase it would ignore the charset information and guess at the\nencoding (for most clients the guess would be 8859-1).\n\nBut the purpose of NOT returning the charset parameter, has to do with\nnot breaking the client parsing of the MIME Content-Type. If the server\nalways slapped charset on, current clients would parse the header:\n\n        Content-Type: text/html; charset=ISO8859-1\n\nand think the content type was the entire 'text/html; charset=ISO8859-1'\nnot just 'text/html' string and would fail to read Latin1 files!!!!!\n\nTo be backwards compatible, the servers should not send the charset\nparameter to old browsers.\n\nDaniel>One outstanding question is: does text/html include all charset=\nDaniel>variations or just latin1? That is, when a client says:\nDaniel>\nDaniel>        Accept: text/html\nDaniel>\nDaniel>is it implying acceptance of all variations of html, or just latin1?\nDaniel>\nDaniel>To be precise, if a client only groks latin1, and it says accept:\nDaniel>text/html, and the server sends ISO2022-JP encoded text, and the user\nDaniel>loses, is the fault in the client for not supporting ISO2022-JP, or at\nDaniel>the server for giving something the client didn't ask for?\nDaniel>\nDaniel>First, \"text/html\" is just shorthand for \"text/html; charset=ISO8859-1\"\nDaniel>so the client didn't advertise support for -JP data.\nDaniel>\nDaniel>But \"giving somethign the client didn't ask for\" is _not_ an HTTP\nDaniel>protocol viloation (at least not if you ask me; the ink still isn't\nDaniel>dry on the HTTP 1.0 RFC though...). It's something that the client\nDaniel>should be prepared for.\n\nAs you put it \"It's something that the client should be prepared for.\"\n\nI'm still assuming that\n\n        accept-parameter: charset=xxx\n\ndictates if the server sends back the charset parameter.  An old browser\nshould continue to get the 2022-JP data untagged.  A new charset-enabled\nbrowser should get tagged 2022-JP data even if it only advertised 8859-1.\n\nDaniel>As above, the server is bound to give \"charset=ISO2022-JP\" if it is\nDaniel>not returning latin1 data. So the client does know that it's not\nDaniel>getting latin1 data. It has the responsibility to interpret the\nDaniel>charset correctly, or save the data to a file or report \"sorry, I\nDaniel>don't grok this data\" to the user. If it blindly blasts ISO2022-JP\nDaniel>tagged data to an ASCII/Latin1 context, then it's broken.\n\nI agree.  I've purposely read EUC and JIS pages on my Mac (SJIS), so that I\ncould save the source and look (grok) at it later. (not a usual user...)\n\nI'm glad you bring up this point, so we can consider the implications.\nBut what the client does in this situation should be implementation\ndependent and not part of this proposal.\n\nDaniel>Does this mean that charset negociation is completely unnecessary?\nDaniel>No. It's not necessary in any of the above scenarios, but it would be\nDaniel>necessary in the case where information can be provided in, for\nDaniel>example, unicode UCS-2, UTF-8, UTF-7, or ISO2022-JP, but the client\nDaniel>only groks UTF-8.\nDaniel>\nDaniel>In that case, something like:\nDaniel>\nDaniel>        Accept-Charset: ISO8859-1, ISO2022-JP\nDaniel>\nDaniel>or perhaps\nDaniel>\nDaniel>        Accept-Parameter: charset=ISO8859-1, charset=ISO2022-JP\nDaniel>\nDaniel>I'm not convinced of the need for the generality of the latter syntax.\nDaniel>Besides: we ought to allow preferences to be specified ala:\nDaniel>\nDaniel>        Accept-Charset: ISO8859-1; q=1\nDaniel>        Accept-Charset: Unicode-UCS-2; q=1\nDaniel>        Accept-Charset: Unicode-UTF-8; q=0.5\nDaniel>        Accept-Charset: Unicode-UTF-7; q=0.4\nDaniel>        Accept-Charset: ISO2022-JP; q=0.2\nDaniel>\nDaniel>which says \"if you've got latin1 or UCS2, I like that just fine. If\nDaniel>you have UTF-8, UTF-7, or -JP, I'll take it, but I won't like it as\nDaniel>much.\"\n\nKen>I want to add one more thing about this issue. We could have the document\nKen>which uses multiple charset in future. We must define the way to label\nKen>such a document.\nKen>It can be like ...\nKen>        Content-Type: text/html; charset=\"ISO2022-JP\", charset=\"ISO8859-6\"\nKen>Is this OK?\n\nI'd rather leave this as a possible future direction.  Multilingual has\nhad a lot of heated discussions.  If we can agree on a means to support\nthe existing mono-lingual mono-encoded Web data, that will allow us\nto create products to fill an immediate need.  Can we phrase something that\nleaves this open and discuss this in another thread?\n\nRegards,\nBob\n\nBob Jung                        +1 415 528-2688         fax +1 415 254-2601\nNetscape Communications Corp.   501 E. Middlefield      Mtn View, CA 94041\n\n\n\n"
        },
        {
            "subject": "Re: [wwwmling,00154] charset parameter (long",
            "content": "I too, agree with Dan: if the data uses anything other than Latin1, it\nshould be tagged as such.. We need to force people to start using\ntags, one way or another, and this seems a reasonable line to draw. \n\nI would also like to note (and I am sure everyone agrees), that this is\nall a temporary solution. Things like:\n\n>A web site with versions of the same files in different encodings\n>(e.g., SJIS, EUC and JIS) or languages (e.g., English and Japanese) could\n>create separately rooted trees with the equivalent files in each tree.\n>The top page could say click here for SJIS/EUC/JIS or English/Japanese.\n\nare obviously a poor solution at best. Larry's \"conversion server\"\nidea is obviously preferrable. \n\n>Ken>I want to add one more thing about this issue. We could have the document\n>Ken>which uses multiple charset in future. We must define the way to label\n>Ken>such a document.\n>Ken>It can be like ...\n>Ken>        Content-Type: text/html; charset=\"ISO2022-JP\", charset=\"ISO8859-6\"\n>Ken>Is this OK?\n\nAs Dan noted, you cannot do it this way. SGML (even if you play with\nthe declaration) *cannot* handle it. The <charset> tag idea is also not a\nwinner...\n\nForcing charset= usage will break some clients (perhaps all), but\ngiven the distribution channels in Japan where everyone gets such\nclients (magazines, Nifty, NTT), updating should not be hard. It will\ncertainly be easier than supporting broken software for years to\ncome.... \n \n\n\n\n"
        },
        {
            "subject": "Proposed new authentication scheme for HTT",
            "content": "Recently, we proposed a simple authentication scheme, based on the MD5\nalgorithm, for possible inclusion in the discussed HTTP/1.1.\n\nIt has been suggested by Phillip Hallam-Baker that the scheme should\nnot use MD5, due to a known weakness.  He is suggesting the use of SHA\ninstead.  We would like to generate discussion on this, so that we can\nmove quickly toward consensus.\n\nThe choice need not be a difficult one.  This simple access\nauthentication scheme is not intended to be a solution to the entire\nweb-security need.  What we seek is a simple, but relatively secure\nreplacement for the Basic Access Authentication scheme in HTTP/1.0.\n\nWe chose MD5 primarily because code is widely available, in the belief\nthat it was strong enough for our target usage.  However, if there is\nconsensus that SHA would be an all-around better choice, we will revise\nour proposal.\n\nIt is our intention to make source code for our scheme available as we\ncan do so.  Some software developers have already expressed enthusiasm\nfor helping with prototype implementations of the scheme.\n\nThe current draft of our proposal is located at:\n\nhttp://www.spyglass.com/techreport/simple_aa.txt\n\nPlease send comments or responses to the http-wg list.\n\n--\nEric W. Sink                    eric@spyglass.com\nJeffery L. Hostetler            jeff@spyglass.com\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "Proposed new authentication scheme for HTT",
            "content": "Recently, we proposed a simple authentication scheme, based on the MD5\nalgorithm, for possible inclusion in the discussed HTTP/1.1.\n\nIt has been suggested by Phillip Hallam-Baker that the scheme should\nnot use MD5, due to a known weakness.  He is suggesting the use of SHA\ninstead.  We would like to generate discussion on this, so that we can\nmove quickly toward consensus.\n\nThe choice need not be a difficult one.  This simple access\nauthentication scheme is not intended to be a solution to the entire\nweb-security need.  What we seek is a simple but relatively secure\nreplacement for the Basic Access Authentication scheme in HTTP/1.0.\n\nWe chose MD5 primarily because code is widely available, in the belief\nthat it was strong enough for our target usage.  However, if there is\nconsensus that SHA would be an all-around better choice, we will revise\nour proposal.\n\nIt is our intention to make source code for our scheme available as we\ncan do so.  Some software developers have already expressed enthusiasm\nfor helping with prototype implementations of the scheme.\n\nThe current draft of our proposal is located at:\n\nhttp://www.spyglass.com/techreport/simple_aa.txt\n\nPlease send comments to the http-wg mailing list.\n\n--\n\nEric W. Sink            eric@spyglass.com\nJeffery L. Hostetler    jeff@spyglass.com\nSpyglass, Inc.\n\n\n\n"
        },
        {
            "subject": "charset Reality Chec",
            "content": "I have several things to say regarding the character set issue\nwhich reflect the reality of Web technology and how it applies to\nthe HTTP standards process.\n\nFirst, regarding standards:\n\n   As Dan said, the \"official\" standard will always lag behind existing\n   practice -- that is by design.  Although some of the drafts we produce\n   will include things that have not-yet-been-implemented, they will not\n   be submitted for RFC consideration until we have at least two independent,\n   working implementations of everything that is contained in the final\n   draft.  Note that this is also why we work on several versions of the\n   protocol at the same time -- good ideas that are not implemented get\n   shoved off to a later version.\n\nSecond, regarding character sets:\n\n   A character set defines the table of codes used to associate small\n   groups of bits within a document to their individual semantics\n   (in most cases, a common symbol to be manipulated and/or displayed).\n   It does not define the format of the overall document, nor does it\n   have any effect on the language(s) used within the document (other than\n   the incidental one that some languages cannot be represented using\n   the symbols defined by some character sets).  Some interesting discussion\n   of languages and character set issues can be found in the Internet-Draft\n   <draft-ietf-mailext-lang-char-00.txt>.\n\n   Character set names for use in Internet protocols are registered with\n   IANA and listed in STD 2 (RFC 1700).  This is what MIME uses, and what\n   HTTP will use.  The current list includes:\n\n      US-ASCII\n      ISO-8859-1  ISO-8859-2  ISO-8859-3\n      ISO-8859-4  ISO-8859-5  ISO-8859-6\n      ISO-8859-7  ISO-8859-8  ISO-8859-9\n\n   Note that if your favorite character set is not listed in the above,\n   someone needs to get off their duff and have it registered by IANA.\n\n   There are many others that have been listed in related RFCs, e.g.\n\n      UNICODE-1-1  UNICODE-1-1-UTF-8  UNICODE-1-1-UTF-7\n      ISO-2022-JP  ISO-2022-JP-2\n      ISO-2022-KR\n\n   There is also a separate list of character set names in STD 2 that is\n   not yet used by MIME.  These are names approved for Internet documentation\n   (but not necessarily Internet protocols).  The registry is at\n   <ftp://ftp.isi.edu/in-notes/iana/assignments/character-sets>.\n\n   Note that none of the above is specific to HTTP, nor do we have any\n   intention of making it specific to HTTP.\n\nThird, regarding media types:\n\n   A media type is an association between a (usually) large number of\n   bits and a document format.  Most document formats have a default\n   character set which defines how the bits are grouped into meaningful\n   symbols.  Some media types allow the character set to be defined\n   within the document itself.  Some media types which do not have such\n   a capability (including those called text/* by MIME) are provided with\n   a charset=\"\" parameter such that the same media type can be used with\n   character sets other than the default.  The IANA registry is at:\n   <ftp://ftp.isi.edu/in-notes/iana/assignments/media-types>.\n\n   The character set is ALWAYS a feature of the media type, regardless\n   of whether or not the charset parameter is present.  MIME defines the\n   default charset of all text/* types as US-ASCII.  HTTP defines the default\n   charset of all text/* types as ISO-8859-1.  In both cases, the default\n   can be overridden by including a charset=\"\" parameter.  Sending a\n   document containing bits intended to encode UTF-7 characters with the\n   header\n\n         Content-type: text/html\n\n   is ridiculous.  If you want to send a UTF-7 encoded document, it must\n   be sent with (case-insensitive)\n\n         Content-type: text/html; charset=\"unicode-1-1-utf-7\"\n\n   Like character sets, this is not an issue for discussion under HTTP.\n   HTTP simply uses what has already been defined for other Internet\n   protocols.  Changing the default for text/* was a touchy issue, but\n   that reflects the reality of Web technology being better than SMTP\n   and has little effect on accessing older protocols through HTTP.\n   Changing the default to \"implementation dependent\" would make the\n   specification as broken as these clients.\n     \nFourth, regarding applications that can't handle parameters on media types:\n\n   Fix them.  Let's face it folks, we can't allow broken implementations\n   to limit the extensibility of the protocol.  Browsers that cannot parse\n   parameters will never be able to handle character sets other than\n   ISO-8859-1.  Nor will they be able to handle future versions of HTML\n   (which will be indicated by a level parameter).  Thus, it makes perfect\n   sense for them to have to treat the response as application/octet-stream\n   (the default behavior) if the content-type is unknown.\n\nFinally, regarding an Accept-charset or Accept-parameter header:\n\n   I believe it is a mistake to continue loading down the request syntax\n   with content negotiation information which is pointless 99.9999999%\n   of the time.  On my system, a complete accept-charset listing would\n   add an additional 361 bytes to every request, and that's with a small\n   set of X fonts.  The likelihood of me ever needing that information\n   is nil.  To be useful, there would need to be a substantial set of\n   documents that are available in multiple character sets.\n\n   In the rare case where parallel sets of documents exist, it makes more\n   sense to provide a means for automated redirection via URCs and allow\n   the client to determine which is the \"best\" of available options.\n   A non-automated equivalent is the \"click here for our Unicode version\".\n   Sure, it's ugly, but nowhere near as ugly as 2 million clients trying\n   to ask ahead-of-time for every possible format and every possible\n   character set and every possible language accepted by the client for\n   the billion documents which are only available in a single\n   format/language/character set.\n   \n   Having said that, I do expect that an Accept-charset header will appear\n   in HTTP/1.1.  However, it will be used as all Accept-* headers should\n   be used -- only when the client wants to specifically restrict the\n   result to a set of options different than the default (as is the case for\n   in-line images today).\n\n     Accept-charset: UNICODE-1-1-UTF-8, iso-8859-*\n\n   would indicate that only UNICODE-1-1-UTF-8 and the iso-8859 set is\n   allowed as a response to this request.\n\n   BTW, Accept-parameter is not useful; charset is the only parameter\n   shared by multiple media types.  We could invent some new parameters,\n   but that only makes the problem worse.  Also, a quality attribute is\n   only useful if we add it to the content-negotiation algorithm --\n   something I would like to avoid (like the plague that it has become).\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: charset Reality Chec",
            "content": ">   BTW, Accept-parameter is not useful; charset is the only parameter\n>   shared by multiple media types.  We could invent some new parameters,\n>   but that only makes the problem worse.  Also, a quality attribute is\n>   only useful if we add it to the content-negotiation algorithm --\n>   something I would like to avoid (like the plague that it has become).\n\nI think the image/* types might share information about color & size,\njust as the text/* types would share information about character sets.\n\n\n\n"
        },
        {
            "subject": "Re: charset Reality Chec",
            "content": ">>   BTW, Accept-parameter is not useful; charset is the only parameter\n>>   shared by multiple media types.  We could invent some new parameters,\n>>   but that only makes the problem worse.  Also, a quality attribute is\n>>   only useful if we add it to the content-negotiation algorithm --\n>>   something I would like to avoid (like the plague that it has become).\n> \n> I think the image/* types might share information about color & size,\n> just as the text/* types would share information about character sets.\n\nIs such a definition available for image/*?  Personally, I'd rather avoid\nthe whole issue, because the next thing someone is going to ask for is:\n\n    Accept-font: YIKES\n\nPre-emptive content negotiation just doesn't work.  Perhaps we should try\nto define a matrix of common client characteristics/behavior, and just\nprovide a header for that.  E.g. one of\n\n    Agent-profile: graphic, color=24, xwin\n    Agent-profile: graphic, grey=8, mwin, unicode\n    Agent-profile: graphic, bw, xwin\n    Agent-profile: text\n    Agent-profile: braille\n\nor maybe not.  I don't see any good way to go about it.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: charset Reality Chec",
            "content": "I've been patiently watching the discussion of Accept-Parameter, et\nal., and waiting for someone to mention my extensions proposal\n(http://www.research.att.com/~dmk/extend.html) as a possible approach.\nSince no one did, I will.\n\nI hate to see HTTP cluttered with a bunch of rarely-used headers.  At\nthe risk of using more characters (Roy Fielding's caution noted),\nAccept-Parameter could be rendered (from the client):\nExtension: HTTP/charset required=oneof; set=UNICODE-1-1-UTF-8\nExtension: HTTP/charset required=oneof; set=iso-8859-*\n\nThe server would either respond with a similar header, like\nExtension: HTTP/charset set=UNICODE-1-1-UTF-8\nto identify what was used, or it would return a failure, because neither\nof the required sets was supported.\n\nAn Extensions:-unaware server would ignore the header and return none,\nand the client would have to fend for itself, as it does now.\n\nRoy Fielding said:\n  > >>                                           Also, a quality attribute is\n  > >>   only useful if we add it to the content-negotiation algorithm --\n  > >>   something I would like to avoid (like the plague that it has become).\n\n  > Pre-emptive content negotiation just doesn't work.  Perhaps we should try\n  > to define a matrix of common client characteristics/behavior, and just\n  > provide a header for that....\n\nThe matrix approach is interesting (rather than negotiate each little\nitem).  My Extensions: proposal includes a negotiation algorithm.  I\nthink it would be better to settle on a single algorithm than have a\ndifferent one for each thing to be negotiated.  Perhaps the one I've\nproposed is unsatisfactory.  I'm open to improvements.  Nevertheless, I\nthink there will be things in HTTP that will demand negotiation.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposed new authentication scheme for HTT",
            "content": "( Note that the original message appears in www-talk.  I tried responding\nthere and the mail system failed, so I am sending my response directly to\nPhillip, and\nto http-wg ).\n\nIn www-talk, Phillip Hallam-Baker wrote:\n\n>No, that is NOT what I was suggesting, I was suggesting that the digest\n>algorithm should be a parameter.\n\n(Yes, I realized later that I misquoted you.  My apologies.)\n\nI humbly disagree.  If the digest algorithm is a parameter, then we might\nhave to rename the scheme to Complicated.  The proposal is aimed to create\none, simple access authentication scheme suitable for inclusion in\nHTTP/1.1.\n\n>This is what I was looking for as well, however I also want the maximum\n>possible integration with Alan and Erics ideas at EIT. S-HTTP is NOT a\n>separate protocol, it is a set of suggestions for a security extension. What\n>we should be focusing on is the integration of these into a single coherent\n>scheme such that the simple, exportable system is a coherent subset.\n\nS-HTTP is one security scheme.  It happens to be a scheme which tries to\naddress far more issues than the Simple authentication scheme we are\nproposing.  I think S-HTTP does its job quite well, and we as a company are\nparticipating in the effort.  However, we believe that a single security\nscheme is unlikely to please everyone.\n\n>|>Please send comments or responses to the http-wg list.\n>\n>No, please direct to the security list because it is a security question and\n>we have\n>gone round the subject on this one already...\n\nRespectfully, I disagree again.  While anyone's input is certainly welcome,\nmoving this discussion to the www-security list is surely the best way to\nmove the proposal farther into the realm of Complicated.  This is why we\ninitiated the discussion on the http-wg list.  I see www-security as the\nappropriate place for discussions of hard questions.  There are no hard\nsecurity questions related to SimpleMD5, because it's, er... Simple. :-)\n\n[ discussion of MOO, Mallet, and Alice deleted ]\n\n[ discussion of Diffie-Helleman key exchange applications deleted ]\n\nAll of the issues raised above in the text I deleted are valid and\ninteresting questions regarding the big picture.  I have nothing against\nprogress and discussion of all security issues relevant to WWW.  I am\nsimply trying to make clear that the proposal for SimpleMD5 was intended to\nbe very focussed at solving the following problem:\n\nThe private key has already been exchanged between client and server in a\nfashion we assume to be secure.  How can the client authenticate itself\nwithout sending the password in the clear?\n\nSolving this problem will benefit the web.  Solving it is easy, if we stay\nfocussed on keeping it Simple.\n\nClearly, solving the rest of the security problems benefits the web even more.\nThere appears to be no reason we can't do both.\n\nAnalogy: Basic Authentication is like a mailbox sitting on a wooden post.\nIt's not really attached, so if a decent wind comes up, it justs falls\ndown, probably resulting in the loss of some mail.  SimpleMD5 seeks a\nworkable solution by simply nailing the box to the post.  It would appear\nthat one nail, maybe two will do it.  S-HTTP replaces the post with a\nlarger capacity mailbox in a lovely brick enclosure with brass trim.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re:  charset parameter (long",
            "content": "Bob Jung <bobj@mcom.com> writes:\n>I agree with Larry Masinter <masinter@parc.xerox.com> \n>that we should replace the\n>\n>        Accept-charset=xxx\n>with the\n>        accept-parameter charset=xxx\n\nI'm not sure this is really better.  Have we lost the preference (q) parameter? \nFurthermore, as Daniel Connolly wrote, the bandwidth necessary to express the \ncommon cases should be minimized and this seems to be a move in the wrong \ndirection.\n\n>I agree that if the charset parameter is not specified, the default \n>***should*** be US-ASCII (or ISO8859-1, if it's been changed).\n\nChanged where?  In MIME or in HTTP?\n\n>Unfortunately, since charset was reserved for future use, Japanese servers \n>had no choice but to serve non-Latin files without a charset parameter!!\n>\n>Why don't we enforce the default for servers using a future version of \n>the HTTP protocol? ...and let current versions be \"implementation \n>dependent\" in order to preserve backwards compatibility?\n\nI agree that it should stay \"implementation dependent\" for current \nservers, but I'm just not sure it is necessary or wise to *enforce* a \ndefault in the future.  What's the point, except for sticking to the \nletter of MIME, which has to live with the limitations of mail?  Does that \nmean that, say, Mosaic-L10N would have to forgo its (limited, but still \nuseful) automatic codeset detection?\n\n>Daniel>...but I think the overall proposal is incomplete\n>Daniel>until we have a workable proposal for how to enhance the major httpd \n>Daniel>implementations to correctly label non-ISO8559-1 documents.\n>\n> [...]\n>\n>A web site with versions of the same files in different encodings\n>(e.g., SJIS, EUC and JIS) or languages (e.g., English and Japanese) \n>could create separately rooted trees with the equivalent files in each \n>tree. The top page could say click here for SJIS/EUC/JIS or \n>English/Japanese.\n\nAnd how is the top page encoded?  In what language?  This just shows that \nAccept-Language and Accept-Charset are needed,\n\n>One idea, is for a HTML <charset> tag that would take precedence over the \n>MIME header:\n>\n>   <html>\n>   <charset=xxx>\n>   <head> <title> DOCUMENT TITLE GOES HERE </title> </head>\n>   <body>\n>   <h1> MAJOR HEADING GOES HERE  </h1>\n>\n>        THE REST OF THE DOCUMENT GOES HERE\n>\n>   </body>\n>   </html>\n\nI like this, but will it fly?  What about multi-charset documents?\n\n>Daniel>But \"giving somethign the client didn't ask for\" is _not_ an HTTP \n>Daniel>protocol viloation (at least not if you ask me; the ink still isn't \n>Daniel>dry on the HTTP 1.0 RFC though...). It's something that the client \n>Daniel>should be prepared for.\n>\n>As you put it \"It's something that the client should be prepared for.\"\n>\n>I'm still assuming that\n>\n>        accept-parameter: charset=xxx\n>\n>dictates if the server sends back the charset parameter.  An old browser \n>should continue to get the 2022-JP data untagged.  A new charset-enabled \n>browser should get tagged 2022-JP data even if it only advertised \n>8859-1.\n\nWhat's the point, if it just advertised that it couldn't deal with \n2022-JP?  It seems to me that a \"new\" server (able to tag) should send a \nmore economical error reply, leaving the browser with the alternative of \n1) abandonning (can't read anyway) or 2) reformulating the request.  It \ncould advise the user and ask for guidance.  Of course the browser must \nstill be prepared to receive *untagged* whatever from an old server.\n\n>I agree.  I've purposely read EUC and JIS pages on my Mac (SJIS), so that I \n>could save the source and look (grok) at it later. (not a usual user...)\n\nCan still be done with 2) above.\n\n>Ken>I want to add one more thing about this issue. We could have the document \n>Ken>which uses multiple charset in future. We must define the way to label \n>Ken>such a document.\n>Ken>It can be like ...\n>Ken>        Content-Type: text/html; charset=\"ISO2022-JP\", charset=\"ISO8859-6\" \n>Ken>Is this OK?\n>\n>I'd rather leave this as a possible future direction.  Multilingual has \n>had a lot of heated discussions.  If we can agree on a means to support \n>the existing mono-lingual mono-encoded Web data, that will allow us\n>to create products to fill an immediate need.  Can we phrase something that \n>leaves this open and discuss this in another thread?\n\nIt is the purpose of this (www-mling) list, after all!\n\nRegards,\n\n-- \nFrancois Yergeau  <yergeau@alis.ca>\nAlis Technologies Inc.\n+1 514 738-9171\n\n\n\n"
        },
        {
            "subject": "Document SimpleMD5 weakness to man-in-themiddle attac",
            "content": "I like the SimpleMD5 proposal. The quicker it gets deployed, the better,\nthe way I see it. I've been reviewing S/Key etc., and they don't have\nthe right characteristics to get quickly and cheaply deployed.\n\nHowever... folks should know what they're getting into if they settle\nfor SimpleMD5: it _is_ subject to the man-in-the-middle attack: I can\ndo this without even forging/sniffing IP packets:\n\nMr. BlackHat sees some juicy financial data offered by Dow Jones on a\nsubscription basis. He posts to some discussion forum where\nsubscribers of this service hang out: he makes it look like his\nmessage/article is from Dow Jones (easy!), and says:\n\nWe're pleased to\nannounce a <a href=\"http://black.hat.org/steal-juicy-data\">replica of\nthe Juicy Financial Database</a>.\n\nSome unsuspecting subscribers follow that link, and send requests with\ntheir md5 hashed passwords to the black.hat.org server.\n\nAt least with SimpleMD5, Mr. BlackHat hasn't got their password. With\nBasic auth, he could re-use the password later. With a subscription\nservice, that's is no big deal (small loss of revenue for the\nprovider). But in a pay-per-access setup, it would cause unauthorized\ncharges for the end user.\n\nSo Mr. BlackHat's CGI-bin script forwards the request on to\nwww.dowjones.com, which serves up the juicy data.  black.hat.org\nrecords the juicy data and passes it back to the subscriber, who never\nnotices the difference (although he's a little dissapointed in the\nperformance of the replica :-). BlackHat's CGI script could even\n_change_ the request, grab the data he really wants, and report an\nerror to the end user.\n\nMr. BlackHat doesn't go completely undetected. The log on\nwww.dowjones.com _will_ show accesses from black.hat.org. But of\ncourse Mr. BlackHat could operate this scam from prep.ai.mit.edu or\nsome such machine with zillions of users.\n\n\nAs I say... SimpleMD5 is a _huge_ improvement over Basic\nauthentication, for very little cost.\n\nBut the specification should explicitly call out the man-in-the-middle\nas a possible attack in the security considerations section or some such.\n\nThe bottom line is that with Basic authentication, the end user's\nconfidential information is compromised. With SimpleMD5, the end\nuser's confidential information _stays_ confidential, but the\ninformation provider is not guaranteed the authenticity of requests.\n\nThe information providers I know will definitely still go for this,\nbut they need to know just how much effort is required to crack the\nsystem so they can figure it into their cost model.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: Query on 5.5.8 Accep",
            "content": "Mike Cowlishaw writes:\n\n> The augmented BNF for Accept shows that only a single media type may\n> be specified, but the second paragraph implies that multiple types\n> are allowed, and two of the examples show a syntax with commas.\n> Something missing in the BNF?\n\nNope.  The \"1#(\" at the beginning is the short-form for a comma-separated list.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "&quot;Expires&quot; again..",
            "content": "Chris Wilson <cwilson@spry.com> writes:\n> Bill Allocca <allocca@OpenMarket.com> writes:\n> >Is there any way in HTTP for a Server to automatically update a page\n> >without requiring the user of the client to click on anything?\n>\n> Because of the other problems that you noted (namely,\n> that clients are not required to automatically reload a\n> document when it passes the Expired time- in fact, it\n> would be detrimental to do so),\n>\n> >there are cases where a page is valid only once --\n> >they expire immediately and the client shouldn't get\n> >them again.\n\nThe current HTTP draft spec *does* specifically\ninterpret \"Expires:\" as a client directive to\nautomatically reload the data at that time -- i.e. expiry\ninterpreted as \"data expiry\".\n\nHowever, along with Dan Connolly, and others that Bill\nAllocca mentions, I also originally interpreted\nExpires as a server guarantee to deliver constant data up\nto that time -- i.e. expiry interpreted as \"URL expiry\".\n\nMy own example of \"URL expiry\" without \"data expiry\" was\nthat of custom inline images inside FORM-generated\ndocuments, which get deleted from the server\nimmediately upon retrieval.  According to the HTTP draft\nspec, I should not tell the client to \"expire\" these\nimages.\n\n> I would propose an \"Expires-Auto-Update:\" field - with a\n> value of \"yes\" or \"no\", default \"no\" in case of the header\n> line not appearing - which would force the client to\n> automatically update the page.\n\nSince, \"data expiry\" and \"URL expiry\" are independent\nissues, as the examples have shown, it seems like we\nreally need two independent time headers, say\n\"Data-Expires:\" and \"URI-Expires:\".\n\nAccording to the current HTTP spec draft, \"Expires:\" means\n\"Data-Expires:\", and this does seem like the more generally\nuseful one.\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@math.utah.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: charset parameter (long",
            "content": "(note  cross-posting)\n\nAt 1:00 PM 1/15/95, Gavin Nicol wrote on html-wg:\n>>>Also, do we really want to get into the business of multi-charsets w/in 1\n>>>document??\n>>\n>>Emphatically yes!\n>\n>Well, even if we wanted to, we cannot. SGML does not have any way of\n>defining that a given bit combination belongs to more than one\n>character class. In other words, documents containing multiple\n>character sets must be \"normalised\" *before* then parser sees the\n>data.\n>\n>In my earlier paper I pointed this out, and it is one reason for using\n>Unicode. As Larry noted, multilingual documents can be written using a\n>coded character set that includes codes for the desired language, and\n>in no other way.\n>\n>We emphatically *do* want multilingual capabilities, so we must not\n>restrict ourselves to US-ASCII or ISO-8859-1, but we most certainly do\n>not want multiple character sets per document: that path is a long\n>road leading to madness.\n>\n>>>I hope not otherwise all the discussion on a header line with the desired\n>>>charset for negotiating on a perfered format is for\n>>>nothing.  (I ask for a document in EUC but it has JIS or SJIS\n>>>intermixed; how could I grok those parts?)\n>>First thing, the different charsets have to be identifiable, and that means\n>>tagging.\n>\n>No. As I said before. SGML has no (working) way of handling this. The\n>data *must* be normalised. Dan has spent a long time making HTML a\n>conforming application of SGML, and this would invalidate all that\n>effort (as well as making it *very* difficult to write generic SGML\n>viewers that could also handle HTML).\n>\n>Say \"yes\" to Accept-Charset:\n>Say \"NO\" to multiple character sets.\n\nI think allowing documents to be in a single character set from:\nISO-8859-X for the same values of \"X\" allowed in MIME is a fairly\nnon-controversial extension to HTML/HTTP. (Not for HTML 2.0, but HTML 2.x)\n\nCan we cite some outside source for additional character sets names that\nwill include Unicode and a reasonable assortment of other national\ncharacter encodings not covered by ISO-8859-X,  like ISO-2022-JP so we\ndon't have to act as the body to pick allowed charater sets and wind up\nwith yet another WWW -specific variation?\n\nIt's more important to pick a well-defined name space  than to have all\nbrowsers support everything.\n\nI'm not totally convinced that transferring a whole document in a single\nencoding, a la Unicode, is the _only_ way to handle multi-lingual\ndocuments, though I'm not an SGML expert and could use some discussion on\nthis. At least the characters used in tagging need to be mapping in a\nsingle character set before parsing. (This would seem easier in codes that\nhave US-ASCII as a proper subset.)\n\nAnother possiblity would be to define a meta-encoding for multiple\ncharacter sets, where the escape codes to shift character sets would not be\nrepresented in _any_ of the character sets. It would then be up to a\nmulti-lingual HTML implementer to provide a pre-processor to get this\ninformation into a form an SGML parser could deal with (maybe by\nnormalizing to a combined  character set, maybe by adding extra markup)\nThis does sound less elegant than Unicode, but I'd like to hear more about\nwhy it won't work before ruling it out.\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Shen Digest Authentication scheme available",
            "content": "Dear all,\n\nAs promised my Digest Authentication proposal is now avaliable, specs avaliable\nfrom:\n\nhttp://info.cern.ch/hypertext/WWW/Protocols/HTTP/digest_specification.html\n\nThe source code is avaliable from:\n\nftp://ftp.w3.org/pub/www/Shen/WWWLibrary_3.0shen1.tar.Z\n\nThis source is ASIS. It is not in fact a Shen release, it contains no encryption \nproduct, these are stripped out. This product performs authentication of clients \nto servers only. It is for demonstration purposes only, we reserve the right to \nchange the spec in subtle ways to screw people up etc... \n\n\n\nNotes:-\n\n1) Compared with the EIT S-HTTP scheme this proposal is as secure with respect \nto authentication provided a secure mechanism for communicating the shared \nsecret is avaliable.\n\n2) The scheme is simpler but rather more secure than the Spyglass proposal, in\nparticular the scheme is secure against a person-in-the-middle attack. \n\n3) The reason why I beleive that we need this level of security is that I would\nlike to be able to introduce S-HTTP features to the Web through a \nsecurity-proxy. This would allow connection by the client via the Digest scheme \nand perform S-HTTP style enhancements. This would decouple the use of security \nenhancements from the client itself which would aid introduction. For this to be \nacceptable however it is essential that the mechanism be as secure \nalgorithmically as the S-HTTP scheme. The complexity in S-HTTP is to make the \nsecurity scheme workable in a distributed system where the degree of trust \nbetween the parties is limited.\n\nThis is one reason why I feel the extra complexity involved is justified. It \nmeans that client authours can decouple themselves from the security side of \nthings until we get closure and in the meantime users can have a realistic \nproduct. I don't think I'm being too heartless though :-)\n\nNote that the requirements for the security proxy look an awfull lot like a URN \nproxy as well. There is synergy there :-)\n\n4) Protection against replay attacks is currently via time stamps. Alternatively \nthe anon session Id scheme could be used.\n\n5) Once a client knows that Digest authentication is required for a site it can \nbe applied without requiring a fresh challenge key for each request.\n\n6) The scheme is completely idempotent, single request/reply style. I would like \nto move to a multiple transation version as part of HTTP/2.0 but feel that this \nshould also incorporate the possiblity of negotiation of security and other \nparameters.\n\n7) The main issue as I see it with respect to implementation of S-HTTP is \nwhether security is the only area for which negotiation applies or whether it is \nmore general. In the first case the S-HTTP scheme could be adopted without \nmodification. In the second it would be better to reorganise the negotiation \nheader tags quite a bit to permit a more general scheme to be developed.\n\n8) This would have been released before the X-Mass break but I didn't want to \nrelease just before being off the net for 3 weeks. Unfortunately Eric opened the \ndiscussion after I had left :-)\n\n9) The spec has a number of issues and options specified in brackets. Please \nrefer to these in replies. I'm trying an experiment in structuring the \ndevelopment discussion and would like to know if people find this approach \nuseful.\n\n10) Missing from the spec is a proposal on modifying HTML forms to cause hashing \nof a field before it is sent :-(\n\n11) I'd like to work on the rest of the S-HTTP proposal ASAP so I would \nappreciate comments as soon as possible.\n\n12) I have not put this out on the www-talk list yet. This is a pre-pre-release \nonly. It has been tested and works on my machines but I haven't tested it on \nyours [which might be broken]. I'd prefer to leave off a general announcement \nbecause a lot of people might take it as a definitive release and then we would \nnot be able to change the protocol.\n\n\nPhill Hallam-Baker \n\n\n\n"
        },
        {
            "subject": "Re: charset parameter (long",
            "content": "Albert Lunde writes:\n\n>It's more important to pick a well-defined name space  than to have all\n>browsers support everything.\n\nYes, and for charsets we think are needed, but are not within this\nnamespace, standardisation must be begun. \n\n>I'm not totally convinced that transferring a whole document in a single\n>encoding, a la Unicode, is the _only_ way to handle multi-lingual\n>documents, though I'm not an SGML expert and could use some discussion on\n\nThere is no need for them to be *transferred* as a single character\nset, but they must be *converted into* a single character set before\nthe parser sees the character stream. Why not therefore save the time\nand simply transfer it as a single character set in the first place? \n\n>Another possiblity would be to define a meta-encoding for multiple\n>character sets, where the escape codes to shift character sets would not be\n>represented in _any_ of the character sets. It would then be up to a\n>multi-lingual HTML implementer to provide a pre-processor to get this\n>information into a form an SGML parser could deal with (maybe by\n>normalizing to a combined  character set, maybe by adding extra markup)\n>This does sound less elegant than Unicode, but I'd like to hear more about\n>why it won't work before ruling it out.\n\nSounds much like ISO-2022... and provided the data is normalised\nbefore the parser ever sees the data, it will work. However, you will\nhave to convert the data into something the parser understands. What?\nWhat character sets support multiple languages? Then ask yourself: \"why\nnot simply convert to one of those\"?\n\nI have never said Unicode must be the *only* character set, only the\n*commonly understood* character set....\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "Sorry to take so long to reply to Gavin Nicol's proposal; first there were\nthe holidays, then things got busy at work.\n\nIn general, I think the document does a good job of discussing the issues,\nand I agree (not surprisingly) with the idea of using Unicode for this\npurpose. I do have some specific comments on aspects of the document.\n\nSection 3.2.1\n\nThere is no character set \"UNICODE-1-1-UCS-2\"; the UCS-2 form is identified\nby \"UNICODE-1-1\" (cf. RFC 1641). There is also no \"UNICODE-1-1-UTF-8\", but\nI am in the process of getting it registered right now. I don't\nparticularly consider UTF-7 a good candidate for WWW work, since HTTP is an\n8-bit-clean protocol, but if someone had another context in mind where a\n7-bit safe form was needed, I'd like to know about it.\n\nSimilarly, later in the section it says that \"[UTF-7] is perfect for WWW\nand MIME uses\". The same comment applies.\n\nIt says at the end of 3.2.1 that the costs of translating documents to\nUnicode might appear prohibitive, but that caches would solve the problem.\nI guess I don't understand why transcoding a typical HTML document using a\ntable would be expensive. I would expect it to be much cheaper than the\ntime needed to push the document out over the wire. Why would this be\nprohibitively expensive?\n\nSection 3.2.2\n\nI'm of two minds about supporting other non-ASCII character sets. On the\none hand, lots of people are doing this already, and you can't very well\nexpect them to stop. On the other hand, formalizing this will impede the\ngoal of getting to internationalization of the WWW, where any client can\ntalk to any server. You would get into the situation where clients and\nservers spoke different character sets, just like now (although you'd know\nit up front, rather than finding out when garbage showed up on your\nscreen).\n\nI suppose that if the number of supported character sets was limited\n(unlikely), clients could support them all, translating through Unicode.\nThis doesn't seem like it's a probably outcome.\n\nThis feature is probably necessary to support existing practice, but I\nworry it will lead to \"Balkanization\" of the WWW, namely clients and\nservers that don't speak to each other. Of course, even with Unicode, if\nyour server is serving up Thai, and I don't have any Thai fonts, I'm out of\nluck. It would be nice, though, if that didn't happen just because you and\nI use different character sets.\n\nSection 3.2.4\n\nThis is probably the section I have the most problem with. Unicode\nspecifically was designed with the idea that attributes such as language,\nfonts, etc. would be encoded out-of-band, via high level tags or even out\nof the character stream entirely (see section 2 of the Unicode Standard,\nvolume 1). In particular, the private use area is specifically reserved for\nuse by corporations and end users, by private agreement, and trying to\nassign a code in the private use area for general public use is contrary to\nthe spirit and probably the letter of the standard. I know that there has\nbeen considerable resistance to other attempts to include formatting codes\nin the standard, so I don't think this is the right approach.\n\nOn the other hand, using embedded escape sequences (like HTML) is perfectly\nall right, and in fact on page 15 of volume 1 there is an example of this\nkind. The only difference in this example is that a character code is not\nassigned as an escape sequence.\n\nAlthough the proposal could be amended to be compatible by using existing\ncharacter codes, I think this would duplicate work being done for HTML, so\nI think this problem should be dealt with at a higher level. Furthermore, I\ndon't think it needs to be dealt with before Unicode can start being used\nfor WWW purposes. The only time such a tag will be used is (for example) if\na client could display both Japanese and Chinese; the tag would then help\nspecify which one to choose. For the vast majority of monolingual clients,\nyou'll use whatever font is available, so the tag would be ignored. Plain\nUnicode is more than capable of basic legibility when displaying text,\nwithout additional language or font tags (again, see the design principles\nin section 2 of the standard). Such display may not be typographically\nideal, but then HTML is nowhere near rich enough for advanced typography\nanyway (right now, anyway).\n\nFor now, then I propose that this issue be deferred. High-level tags are\nthe right way to deal with it, because like other font, size, style, etc.\ninformation, it's not critical to the basic legibility of the information.\nIf HTML 3.0 has a language tag, all the more reason not to invent another\nmechanism. It certainly doesn't belong at the level of character codes.\n\nSection 5.1\n\nI'm pleased to hear that the issue on not requiring stict MIME line break\nrules has been dealt with. It would be nice if there were a reference of\nsome sort here so that people could see the decision in writing (assuming\nit's written down somewhere).\n\nSection 6\n\nIt's worth adding the URL for the Unicode WWW page, http://unicode.org/, in\naddition to the ISBN numbers for the two books.\n\n\nAgain, thanks to Gavin for putting the work into writing up this proposal.\nDoes anyone else have comments?\n\n----------------------------\nDavid Goldsmith\ndavid_goldsmith@taligent.com\nSenior Scientist\nTaligent, Inc.\n10201 N. DeAnza Blvd.\nCupertino, CA  95014-2233\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "David Goldsmith writes:\n\n> Again, thanks to Gavin for putting the work into writing up this proposal.\n> Does anyone else have comments?\n...\n> Section 3.2.2 \n> \n> I'm of two minds about supporting other non-ASCII character sets. On the\n> one hand, lots of people are doing this already, and you can't very well\n> expect them to stop. On the other hand, formalizing this will impede the\n> goal of getting to internationalization of the WWW, where any client can\n> talk to any server. You would get into the situation where clients and\n> servers spoke different character sets, just like now (although you'd know\n> it up front, rather than finding out when garbage showed up on your\n> screen).\n\nUnless non-Latin-based languages are treated as equal to Latin-based \nlanguages, WWW should be renamed. I think the Accept-charset would \naddress this well: also, it suggests an architecture/protocol, and so is \ninherently enabling rather than fettering.\n\n> I suppose that if the number of supported character sets was limited\n> (unlikely), clients could support them all, translating through Unicode.\n> This doesn't seem like it's a probably outcome.\n\nI think Gavin's solution comes down to that every WWW server should be \nable to negotiate and supply:\n\n* some ISO 646 based lowest-common-denominator encoding, e.g. ISO 8859-1;\n* some ISO 10646 encoding for rest-of-world documents; and\n* the local national character set (in English-speaking countries, this is \nnot needed). \n \nThe first allows good support of English readers, and majority-white\ncountries and their ex-colonies well. (I mention \"white\" not to impute\nracism or at least parochialism, but to emphasise that for WWW to be able\nreach a browsership in non-Western countries other than the technical and\neducated classes, the small Latin-based character sets are of no use.) The\nsecond allows support of international access of documents. The third\nallows national support of local languages. \n\n> This feature is probably necessary to support existing practice, but I\n> worry it will lead to \"Balkanization\" of the WWW, namely clients and\n> servers that don't speak to each other. \n\nLocal/regional character sets and encodings won't go away in the forseeable \nfuture. WWW should have a \"lingua franca\" character set, and that \ncharacter set shouldn't be a national or regional character set: ISO \n10646/Unicode is the only choice.\n\nSuch a three character set approach (LCD, internationals,\nnational/regional) is also fairly efficient: it means that clients and\nservers can negotiate the character set for the least amount of character\nmapping. \n\nGavin goes further and specifies particular encodings of ISO 10646 that \nevery server should support: I think that naturally follows.\n\n> Of course, even with Unicode, if\n> your server is serving up Thai, and I don't have any Thai fonts, I'm out of\n> luck. It would be nice, though, if that didn't happen just because you and\n> I use different character sets.\n\nPerhaps a smart server could serve the fonts too: it could have \n\"Accept-charset\" for the character sets and encodings it will handle, and \n\"Display-charset\" for the character sets it can display. The smart server \nwould figure out what is needed. Alternatively, it could be done on an \nexception basis: if a client can't display a character, it sends a \nrequest somewhere to a font-server (back to the WWW server?): this may be \nslow, but this is handling exceptions not normal operation.\n\n\nAs a side comment, I'd mention that the Extended Reference Concrete Syntax\n(ERCS) proposals I am involved in developing have as an aim to allow SGML\ntagging using native-language tag names (GIs), if the name characters also\nappear in Unicode.  This is in part to allow structure-based searching of\ndocuments by non-English speakers using meaningful (to them) tag names. At\nthe moment, it is a reasonable assumption that SGML markup will only use\nISO 646 characters: the ERCS would hopefully change that, as far as SGML\ndocuments go. If some version of HTML allows arbitrary tag names, I hope \nthat there are no ISO 646 dependencies.\n\n-ricko\n\nRick Jelliffe                          email: ricko@allette.com.au\nAllette Systems                        phone: +61 2 262 4777\nSydney, Australia                      fax:   +61 2 262 4774\n\n\n\n"
        },
        {
            "subject": "Grammar for UserAgent heade",
            "content": "The grammar for the User-Agent header currently reads like this:\n\n>       User-Agent      = \"User-Agent\" \":\" 1*( product )\n>\n>       product         = token [\"/\" product-version]\n>       product-version = 1*DIGIT \".\" 1*DIGIT\n\nIn my opinion, this is a bit too restrictive.  For example, all of the\nfollowing version numbers are illegal using this grammar.\n\n1               simple integer, no minor rev\n2.0a1           Apple standard format for an alpha release\n1.0.2.87        Microsoft standard format\n\nOther version numbering schemes may cause other problems.  May I suggest\nthat we not try to impose the format of the version number beyond saying\nwhich characters we allow there?\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n                                           I don't speak for Spyglass.\n\n\n\n"
        },
        {
            "subject": "Re: Grammar for UserAgent heade",
            "content": "> The grammar for the User-Agent header currently reads like this:\n> \n> >       User-Agent      = \"User-Agent\" \":\" 1*( product )\n> >\n> >       product         = token [\"/\" product-version]\n> >       product-version = 1*DIGIT \".\" 1*DIGIT\n> \n> In my opinion, this is a bit too restrictive.  For example, all of the\n> following version numbers are illegal using this grammar.\n> \n> 1               simple integer, no minor rev\n> 2.0a1           Apple standard format for an alpha release\n> 1.0.2.87        Microsoft standard format\n> \n> Other version numbering schemes may cause other problems.  May I suggest\n> that we not try to impose the format of the version number beyond saying\n> which characters we allow there?\n\nSounds fair - what about:\n\nUser-Agent      = \"User-Agent\" \":\" 1*( product )\nproduct         = token [\"/\" token]\n\n\n-- cheers --\n\nHenrik Frystyk\nfrystyk@W3.org\n+ 41 22 767 8265\nWorld-Wide Web Project,\nCERN, CH-1211 Geneva 23,\nSwitzerland\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "At 5:50 PM 1/17/95, Gavin Nicol wrote:\n>Yes, and I am very hesitant to suggest using any special purpose\n>codes. The problem is that there does need to be some standard (low\n>level) way of saying that some text is in Japanese, and some text is\n>in Chinese. Now, the real debate is how to represent this, and I think\n>the recent idea I proposed is not bad.\n\nThis is really the crux of the matter. Why do you think there needs to be a\n*low level* way of differentiating Japanese and Chinese text? The WWW seems\nto operate quite well now without any way to differentiate German, English,\nFrench, or Italian text (all handled by 8859-1). What problems --\nspecifically -- would arise in typical WWW applications if such text is not\ntagged? How would lack of this information impede you when writing\nUnicode-capable servers and clients, and how would it impact end users?\n\nI fully agree that language (and font, and style, and ...) tags are useful\nand highly desirable at a high level, and support for this should be added\nto HTML (or at whatever level is appropriate). I don't think that language\nis any different from these other attributes, nor does it need special\ntreatment. Doing it at a low level adds complexity and complicates clients\nand servers that use Unicode. There has to be a compelling reason to add\nthis complexity. There has to be a problem that it solves.\n\nGiven that work is in progress to add language information at a higher\nlevel (HTML 3) it seems to me that there would have to be an\nextraordinarily strong reason to add this information at a low level as\nwell.\n\nDavid Goldsmith\nSenior Scientist\nTaligent, Inc.\n10201 N. DeAnza Blvd.\nCupertino, CA 95014-2233\ndavid_goldsmith@taligent.com\n\n\n\n"
        },
        {
            "subject": "Re: Grammar for UserAgent heade",
            "content": "On Tue, 17 Jan 1995, Henrik Frystyk Nielsen wrote:\n\n> \n> > The grammar for the User-Agent header currently reads like this:\n> > \n> > >       User-Agent      = \"User-Agent\" \":\" 1*( product )\n> > >\n> > >       product         = token [\"/\" product-version]\n> > >       product-version = 1*DIGIT \".\" 1*DIGIT\n...\n> > \n> Sounds fair - what about:\n> \n> User-Agent      = \"User-Agent\" \":\" 1*( product )\n> product         = token [\"/\" token]\n\nYes, where token is any string of characters which doesn't require URL\nencoding *OR* any string of characters inside of quotes.  User-Agent\nshould be required to be a unique identifier for a particular version\nof User-Agent.  The standard should be absolutely relaxed about the\nformat of the identifer except that it should be allowed to inhibit\nproper parsing of subsequent headers.  Might include words to describe\nhow to insure uniqueness?  Or leave it to follow the course of domain\nnames.  Or register the unique portion of the ID, etc..\n\nIt should be recommended that a human be able to read the string and\nrelate it to external version control parameters.  \n\nWhat else matters?\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Grammar for UserAgent heade",
            "content": ">> > The grammar for the User-Agent header currently reads like this:\n>> > \n>> > >       User-Agent      = \"User-Agent\" \":\" 1*( product )\n>> > >\n>> > >       product         = token [\"/\" product-version]\n>> > >       product-version = 1*DIGIT \".\" 1*DIGIT\n> ...\n>> > \n>> Sounds fair - what about:\n>> \n>> User-Agent      = \"User-Agent\" \":\" 1*( product )\n>> product         = token [\"/\" token]\n\nI can live with \n\n        User-Agent      = \"User-Agent\" \":\" 1*( product )\n        product         = token [\"/\" product-version]\n        product-version = token\n\nNote, however, that token has a restricted set of allowed characters\nas specified in the BNF.  The reason product-version is separate is\nso that I can explicitly state what is not allowed in the version from\na semantic point-of-view.\n\n> Yes, where token is any string of characters which doesn't require URL\n> encoding *OR* any string of characters inside of quotes.  User-Agent\n> should be required to be a unique identifier for a particular version\n> of User-Agent.  The standard should be absolutely relaxed about the\n> format of the identifer except that it should be allowed to inhibit\n> proper parsing of subsequent headers.  Might include words to describe\n> how to insure uniqueness?  Or leave it to follow the course of domain\n> names.  Or register the unique portion of the ID, etc..\n\nNone of the above.  The HTTP protocol will assume that client authors\nwill want to give their products suitable product tokens.  Requiring a\nregistry will not work due to the overhead and market considerations.\nHowever, it would be nice if W3C set up a voluntary registry.\n\n> It should be recommended that a human be able to read the string and\n> relate it to external version control parameters.  \n\nNope.  This string is explicitly not intended for human readability.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": "[ Apologies if this is seen more than once. ]\n\n>This feature is probably necessary to support existing practice, but I\n>worry it will lead to \"Balkanization\" of the WWW, namely clients and\n>servers that don't speak to each other. Of course, even with Unicode, if\n>your server is serving up Thai, and I don't have any Thai fonts, I'm out of\n>luck. It would be nice, though, if that didn't happen just because you and\n>I use different character sets.\n\nThis has already happened to a degree: information from Japan seldom\nleaks out in anything other than English, and that is but the tip of a\ngrowing iceburg. Unicode provides a good common ground.\n\n>Section 3.2.4\n>This is probably the section I have the most problem with. Unicode\n>specifically was designed with the idea that attributes such as language,\n>fonts, etc. would be encoded out-of-band, via high level tags or even out\n\nYes, and I am very hesitant to suggest using any special purpose\ncodes. The problem is that there does need to be some standard (low\nlevel) way of saying that some text is in Japanese, and some text is\nin Chinese. Now, the real debate is how to represent this, and I think\nthe recent idea I proposed is not bad.\n\nWe *could* say that these codes are simply an artifact of the encoding\n(ie. this is UCS-2-HINTED encoding), and say that they *should* be\nremoved from the data stream once it's received... or we can say they\nare STAGO and STAGC for a high-level tag. \n\nDo you have any complaints about including hints in a transfer\nencoding? \n\n\n\n"
        },
        {
            "subject": "Re: A truly multilingual WW",
            "content": ">Perhaps a smart server could serve the fonts too: it could have \n>\"Accept-charset\" for the character sets and encodings it will handle, and \n>\"Display-charset\" for the character sets it can display. The smart server \n>would figure out what is needed. Alternatively, it could be done on an \n>exception basis: if a client can't display a character, it sends a \n>request somewhere to a font-server (back to the WWW server?): this may be \n>slow, but this is handling exceptions not normal operation.\n\nThis is certainly possible, and after a certain amount of time, good\ncaching clients will build up a \"working set\" of glyphs. In the Plan 9\ndocumentation, they mention that this working set is generally not\nlarge. \n\nOf course, once free Unicode fonts become available, this will not be\nneeded (except in cases where we specifically want to use different\nglyph images).\n\n\n\n"
        },
        {
            "subject": "Re: Grammar for UserAgent heade",
            "content": "> >> Sounds fair - what about:\n> >> \n> >> User-Agent      = \"User-Agent\" \":\" 1*( product )\n> >> product         = token [\"/\" token]\n> \n> I can live with \n> \n>         User-Agent      = \"User-Agent\" \":\" 1*( product )\n>         product         = token [\"/\" product-version]\n>         product-version = token\n[...]\n> > User-Agent should be required to be a unique identifier for a \n> > particular version of User-Agent.\n\nSome browser authors (Netscape, Spyglass) seem to be using \n\"product/product-version (platform)\" which has the advantage of \ndistinguishing between, say, NCSA Mosaic/2.0a8 (Windows) and\nNCSA Mosaic/2.0a8 (Macintosh), two distinct User-Agents.  Would it be too \nrestrictive to expand \"product-version = token\" so that it includes some \nreference to OS/platform?\n\nM. Hedlund <march@europa.com>\n\n\n\n"
        },
        {
            "subject": "Experimental implementation of SimpleMD",
            "content": "An Experimental Implementation of SimpleMD5\n-------------------------------------------\n\nSimpmd5 is a server side implementation of the proposed SimpleMD5\nauthentication method for HTTP.  The current specification for the\nproposal can be found at\n\nhttp://www.spyglass.com/techreport/simple_aa.txt\n\nThis specification describes an addition to the HTTP protocol to\nallow authentication in a way that does not send passwords over the\nnetwork unencrypted.  It is intended as a replacement for Basic\nauthentication.  At present no publically available clients support it.\n\nThe SimpleMD5 protocol has several important advantages in addition\nto transmitting only encrypted passwords.\n\n1. It is simple, maybe even easier to implement than Basic authentication.\n\n2. It is very limited in its goals.  It has no connection with any of\n   the competing security protocols (Shen, S-HTTP, SSL).  This means\n   clients can introduce it as a replacement for Basic authentication\n   only.  No commitment to one of the security protocols is implied and\n   it is not necessary to wait for security standards to emerge before\n   implementing it.\n\n3. It is flexible.  It allows for added features in the server (see below\n   for examples).\n\n\nSimpmd5 is an experimental implementation of SimpleMD5 intended for\nuse as an authentication module with the WN http server (see\nhttp://hopf.math.nwu.edu/).  But it can also run as a standalone\nprogram in which case it will do two things: a) generate a server\nSimpleMD5 request for authentication or b) consult the enviroment\nvariable HTTP_AUTHORIZATION, parse a client Authorization line there,\nconsult a password file and return a string indicating acceptance\nor rejection.\n\nThis implementation has two interesting features which are not\npart of the specification but are made possible by the flexibility of\nthe proposed method.\n\n1. Timestamps:  The maintainer can set the time period for which\n   authentication granted the client is valid.  After this time period\n   the client will have to re-authenticate.   The time period can be\n   set to any number of seconds (or be unlimited) and is accurate to\n   within 1% of the specified value.  The timestamp is encoded in the\n  \"opaque\" header field (see the specification).\n\n2. IP address stamps:  The IP address (or the IP address of the\n   client's proxy) is encoded in the \"opaque\" header field.  This means\n   that a replay attack would have to spoof the server with a false\n   IP address.\n\nThe simpmd5 program is written in C and the source code is available\nfreely for any use.  You can obtain it from \n\nhttp://hopf.math.nwu.edu/simpmd5.tar\n\n\n\nJohn Franks   john@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "RE: Experimental implementation of SimpleMD",
            "content": ">This implementation has two interesting features which are not\n>part of the specification but are made possible by the flexibility of\n>the proposed method.\n>\n>1. Timestamps:  The maintainer can set the time period for which\n>   authentication granted the client is valid.  After this time period\n>   the client will have to re-authenticate.   The time period can be\n>   set to any number of seconds (or be unlimited) and is accurate to\n>   within 1% of the specified value.  The timestamp is encoded in the\n>  \"opaque\" header field (see the specification).\n\nThe Shen/S-HTTP system also uses timestamps. These have the difficulty\nof clock synchronisation but remove the problem of keeping track of challenge\n/response pairs which is a problem if the connection cannot be kept open.\nThis is especially a problem through gateways.\n\n>2. IP address stamps:  The IP address (or the IP address of the\n>   client's proxy) is encoded in the \"opaque\" header field.  This means\n>   that a replay attack would have to spoof the server with a false\n>   IP address.\n\nHow is the IP address of the proxy known?\n\nUsing the IP address is very unsatisfactory since it adds little real \nsecurity but a lot of inconvenience.\n\n\nAlso I really don't like having a system that only authenticates the fact\nthat a transaction has been authorised but does not authenticate which\nparticular transaction is authorised.\n\nExtension of the system to perform a digest of the message header itself is very \nsimple. It protects against man-in-the-middle attacks which is very important\nwhen using proxies. This extension can be achieved in a manner that is very\nsimple and compatible with the S-http scheme. Thus implementors only have one \nscheme to worry about.\n\n\nI'm also very against discussing this in the http forum and not the security\none. A lot of work has gone into unifying the security schemes proposed last\nyear. It has always been my beief that they are part of the main HTTP spec.\nIn this respect the name S-HTTP is probably misleading. Considering the \npotential problems with turf wars vis a vis various IETF parties we should be \nvery clear that we ensure that security matters are raised on \nwww-security-request@ns1.rutgers.edu. They might also be sent to other lists\nbut they MUST definitely be seen in the security list. I get over 200 messages\na day. I don't read all of them. The security list is one I do read, as do those\ndoing security work.\n\n\nThe bottom line is that with the minimal Digest scheme proposed a secure \ncommunication can be set up between a browser and a security enhancment proxy. \nI do not bleive that the simpleMD5 scheme offers this level of security unless\nthe actual transaction itself is authenticated. Doing this makes the SimpleMD5\nscheme effectively the same as the digest scheme.\n\nIn other words the SimpleMD5 scheme is not a security `enhancement', it is a \nweakening of a scheme already proposed and implemented. :-( Moreover this is\nprecisely the point Alan and myself raised against the original SSL scheme\nat the W3C security meeting.\n\n\nOne other point, before the unification of Shen/S-HTTP proposals started Shen\nused the actual header itself for the authentication. This was discovered not\nto work with some proxies that reorganised the header lines :-( so I adopted \nthe S-HTTP approach of having an encapsulated header, MIME style. Note however\nthat full S-HTTP mucking arroung with ASN.1 encapsulation is not required! \n\n\n\nThe digest scheme specs are avaliable from:-\n\nhttp://info.cern.ch/hypertext/WWW/Protocols/HTTP/digest_specification.html\n\n\nPhill Hallam-Baker\n\n\n\n"
        },
        {
            "subject": "Re: Experimental implementation of SimpleMD",
            "content": "When I posted the availability of an experimental implementation of\nSimpleMD5 authentication I neglected to mention that I am running a\ntest version on my server.  Those of you implementing SimpleMD5 in\nclients are welcome to test your clients at \n\n    http://hopf.math.nwu.edu/simp/index.html\n\nwith the sample username/password pair that is in the SimpleMD5\nspecification from Spyglass.\n\nThe source for my implementation is freely available for any use at\n\n    http://hopf.math.nwu.edu/simplemd5/index.html\n\n\nI might also take this opportunity to respond to the criticism of \nSimpleMD5 posted here by Phil Hallam-Baker.  As I understand it Phil's\ncriticisms are \n\n     1) SimpleMD5 is not as secure as SHTTP, and\n     2) SimpleMD% is not a subset of SHTTP.\n\nBoth of these are quite true.  It is also true that SimpleMD5 is not\nas secure as SSL and not compatibile with it.  But this really misses\nthe point of SimpleMD5.\n\nIn the fullness of time I hope and expect that security standards will\nemerge for HTTP and that they will subsequently be implemented in most\nbrowsers and servers.  However, in the meantime Basic authentication,\nwhich transmits passwords essentially in the clear, is in widespread\nuse.  The point of the Spyglass proposal for SimpleMD5 is as a\nstop-gap measure to replace Basic authentication.  It is nearly\nidentical to Basic authentication except that passwords are encrypted.\nOf course security can be done better -- and presumably the security\nworking group is doing precisely that.  But in the meantime it is\ndefinitely worth quickly replacing Basic authentication.\n\nThe changes required in the protocol are absolutely minimal.  There\nneed to be extra fields in one of the existing authentication header\nlines from the server and one from the client.  Implementation for clients\nshould actually be easier than Basic authentication.  Implementation for\nthe server is not at all difficult.\n\nOne suggestion of Hallam-Baker I think is a very good one and I hope it\nwill be considered before the SimpleMD5 specification is finalized.\nThat is to authenticate the specific transaction rather than just all\ntransactions within a realm.  In practice this means requiring the client\nto encrypt the URI requested along with the \"nonce\" and user password.\nThe added security is probably marginal but still worthwhile since it\nis trivial to implement.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Experimental implementation of SimpleMD",
            "content": "Phillip wrote (in part) on www-security:\n> Proposals:-\n> \n> 1) We authenticate the message body\n> 2) All things being equal we make it as compatible with S-HTTP as possible\n> 3) The discussion of a security scheme for the Web take place on the mailing\n> list the IETF has been told is the official forum for such schemes.\n> 4) We get something out fast enough that it can be included in HTTP/1.1\n\nIn this connection, the www-security list may be the correct forum\nto talk about the security aspects. It seems to me that it would\nhelp to also spell out to the HTTP list what basic set of headers need to be \nused to negotate the use of this scheme and maintain upward compatibility\nwith S-HTTP.  \n\nDiscussion may have started on the HTTP list because of its readership\nor because it has been more active lately...\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Experimental implementation of SimpleMD",
            "content": ">In this connection, the www-security list may be the correct forum\n>to talk about the security aspects.\n\nOh all right! :-)\n\nReaders of www-security:\n\nA discussion of a new authentication scheme for HTTP has broken out.  We\nhere at Spyglass have proposed a digest scheme based on MD5.  You may\nperuse our ideas at:\n\nhttp://www/techreport/simple_aa.txt\n\nSimpleMD5 has been implemented in our client and in 2 servers, and works\nnicely as specified.\n\nPhilip Hallam-Baker does not share our enthusiasm for our proposal, and has\nsuggested something totally distinct but similar, the details of which are\nat:\n\nhttp://info.cern.ch/hypertext/WWW/Protocols/HTTP/digest_specification.html\n\nThe Digest scheme has apparently also been implemented, as the code for it\nis available linked to the above page.\n\nPhilip apparently recently posted the following list of priorities, all of\nwhich I agree with:\n\n> 1) We authenticate the message body\n> 2) All things being equal we make it as compatible with S-HTTP as possible\n> 3) The discussion of a security scheme for the Web take place on the mailing\n>       list the IETF has been told is the official forum for such schemes.\n> 4) We get something out fast enough that it can be included in HTTP/1.1\n\nand, giving the horse yet another kick, I add\n\n5) Make the scheme as SIMPLE as possible, leaving the meatier issues to\nS-HTTP discussions.\n\nThere is every reason to move toward consensus.  Perhaps, through\ndiscussion with the readers of www-security, we can locate the appopriate\nblend of Philip's ideas and ours?\n\n--\nEric W. Sink\neric@spyglass.com\n\n\n\n"
        },
        {
            "subject": "Distribute copy of a document",
            "content": "Hi all!\nFirst, is there a mailing list/newsgroup devoted to http *questions*, as \nopposed to protocol issues?\nAt the moment, I am trying to understand what does it mean to have \nmultiple language versions of a document, as hinted at in section 7.6\nof the draft HTTP document.\n\n\nThe question (hopefully) directly related to this group is another, however.\nIn a distributed environment like WWW it is conceivable that a document is\npresent in many copies scattered throughout the world; for most of these \ndocuments it is not mandatory that the copies are *always* verbatim the \nsame, but a slight delay in propagation can be allowed.\nWould it be useful to add a Request Header Field to ask for possible \nduplicates of the URL, and a corresponding Response Header? This way, the\nclient could implement some Internet metric and choose the \"nearest\"\ndocument.\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: Experimental implementation of SimpleMD",
            "content": "On Tue, 24 Jan 1995, Eric W. Sink wrote:\n\n> \n> A discussion of a new authentication scheme for HTTP has broken out.  We\n> here at Spyglass have proposed a digest scheme based on MD5.  You may\n> peruse our ideas at:\n> \n> http://www/techreport/simple_aa.txt\n\nThis address is not complete.  I did get to the www.rutgers.edu homepage,\nbut havent been able to find the techreport section.\nthanks,\ndrex\n\n\n---------------\ndatkins@unm.edu\"Into the Backing\"\nCIRT-ACS  University of New Mexico  \n----------------------------------\n\n    \n   \n    \n\n\n\n"
        },
        {
            "subject": "Re: Experimental implementation of SimpleMD",
            "content": "> > \n> > A discussion of a new authentication scheme for HTTP has broken out.  We\n> > here at Spyglass have proposed a digest scheme based on MD5.  You may\n> > peruse our ideas at:\n> > \n> > http://www/techreport/simple_aa.txt\n> \n> This address is not complete.  I did get to the www.rutgers.edu homepage,\n> but havent been able to find the techreport section.\n> thanks,\n\nthis should be http://www.spyglass.com/techreport/simple_aa.txt\n\njeff\n\n\n\n"
        },
        {
            "subject": "Re: Distribute copy of a document",
            "content": "> First, is there a mailing list/newsgroup devoted to http *questions*, as \n> opposed to protocol issues?\n\nYes, it's www-talk@info.cern.ch\n\n> At the moment, I am trying to understand what does it mean to have \n> multiple language versions of a document, as hinted at in section 7.6\n> of the draft HTTP document.\n\nHmmm, that sounds to me like an http-wg issue.  As it happens, I rewrote\nthat section just last week.\n\n7.6 Content-Language\n\n   The Content-Language field describes the natural language(s) of the\n   intended audience for the enclosed entity. Note that this may not be\n   equivalent to all the languages used within the entity.\n\n      Content-Language= \"Content-Language\" \":\" 1#language-tag\n\n   The primary purpose of Content-Language is to allow a selective\n   consumer to identify and differentiate resources according to the\n   consumer's own preferred language. Thus, if the body content is\n   intended only for a Danish audience, the appropriate field is\n\n      Content-Language: dk\n\n   If no Content-Language is specified, the default is that the content\n   is intended for all language audiences. This may mean that the sender\n   does not consider it to be specific to any natural language, or that\n   the sender does not know for which language it is intended.\n\n   Multiple languages may be listed for content that is intended for\n   multiple audiences. For example, a rendition of the \"Treaty of\n   Waitangi,\" presented simultaneously in the original Maori and English\n   versions, would call for\n\n      Content-Language: mi, en\n\n   However, just because multiple languages are present within an entity\n   does not mean that it is intended for multiple linguistic audiences.\n   An example would be a beginner's language primer, such as \"A First\n   Lesson in Latin,\" which is clearly intended to be used by an English\n   audience.  In this case, the Content-Language should only include\n   \"en\".\n\n   Content-Language may be applied to any media type -- it should not be\n   considered limited to textual documents.\n\n> The question (hopefully) directly related to this group is another, however.\n> In a distributed environment like WWW it is conceivable that a document is\n> present in many copies scattered throughout the world; for most of these \n> documents it is not mandatory that the copies are *always* verbatim the \n> same, but a slight delay in propagation can be allowed.\n> Would it be useful to add a Request Header Field to ask for possible \n> duplicates of the URL, and a corresponding Response Header? This way, the\n> client could implement some Internet metric and choose the \"nearest\"\n> document.\n\nThat would be the same as returning a URC (or URM) in response to a request\ninstead of the document itself.  Yes, that has been under consideration for\na while now, but not directly as part of this WG, and independently of HTTP\n(though I am one of those working on the issues).  See the URI WG for\nmore info: <http://www.ics.uci.edu/pub/ietf/uri/>\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Distribute copy of a document",
            "content": "In message <9501250951.AA14925@beatles.cselt.stet.it>, Maurizio Codogno writes:\n\n>In a distributed environment like WWW it is conceivable that a document is\n>present in many copies scattered throughout the world; for most of these \n>documents it is not mandatory that the copies are *always* verbatim the \n>same, but a slight delay in propagation can be allowed.\n>Would it be useful to add a Request Header Field to ask for possible \n>duplicates of the URL, and a corresponding Response Header? This way, the\n>client could implement some Internet metric and choose the \"nearest\"\n>document.\n\nVERY interesting question. Caching, replication, and high availability\nare pretty hot topics, if you ask me.\n\nHTTP's design isn't optimal for this sort of thing, but HTTP is\nquickly becoming the proxy-to-everyting protocol, so this\nfunctionality needs to be visible through HTTP somehow.\n\nAs query routing and hierarchical caching get deployed, I think it\nneeds to go both ways: the server should be able to give back some\nvariation on the \"404 redirected\" response that says \"4?? redirected\nto multiple replicas\" Also, the client needs to be able to request\n\"GET any of these replicas...\"\n\nI got the idea for GETting replicas from the The Propero Data Access\nProtocol[1]. Have a look at the Harvest[2] project if you're\ninterested in cool resource discovery and high availability techniques.\n\n\n[1] PDAP draft paper (Augart, Neuman, Rao, unpublished)\nSteven Seger Augart <swa@ISI.EDU>\nFri, 29 Apr 1994 14:26:25 PDT\n\n[2] http://rd.cs.colorado.edu/harvest/\n\n>Hi all!\n>First, is there a mailing list/newsgroup devoted to http *questions*, as \n>opposed to protocol issues?\n\nTopics that aren't the business of any particular working group\nshould probably be addressed to www-talk@info.cern.ch, or to\none of the comp.infosystems.www.* newsgroups.\n\nBut you're clearly in the domain of this mailing list when\nyou start here:\n\n>At the moment, I am trying to understand what does it mean to have \n>multiple language versions of a document, as hinted at in section 7.6\n>of the draft HTTP document.\n\n\n\n"
        },
        {
            "subject": "Re: Distribute copy of a document",
            "content": "Dan writes:\n\n> As query routing and hierarchical caching get deployed, I think it\n> needs to go both ways: the server should be able to give back some\n> variation on the \"404 redirected\" response that says \"4?? redirected\n> to multiple replicas\" Also, the client needs to be able to request\n> \"GET any of these replicas...\"\n\nFunny that...I just added a \"305 Multiple Choices\" to the 1.1 spec last\nnight for the same purpose, but for handling redirection with URC sets.\nThe response would be a document, either text/html or urc/*, from \nwhich the choice can be selected manually or automatically.  Possible\nmedia subtypes for urc would include\n\n    urc/http       HTTP headers\n    urc/usmarc     US-MARC data set\n    urc/iafa       IAFA template\n    urc/tei        TEI SGML header elements\n    ...\n\nI can think of many possiblities (some good, some bad), but this way\nthe URC can be protocol-independent and we can let the market decide\nwhich is the \"best\" format.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "WG Action: HyperText Transfer Protocol (http",
            "content": "The following announcement was posted to the IETF-Announce list, but not to\nthe WG one.  Basically, you are now a WG.\n\nAfter some discussion, IESG made one change to the charter and management of\nthe WG, about which I'm very pleased.   Just as some of the more information\nservices-oriented WGs have long reported jointly to Applications and User\nServices, HTTP will be reporting jointly to Applications and Transport.\nThis will give us more input from the real experts on transport, scaling\nissues, and general impact on the network of various strategies.\n\nI remain your primary contact (\"responsible AD\") for any IESG administrative\npurposes.\n\n    john\n\n--------------- forwarded message ---------\n\nDate: Fri, 27 Jan 1995 15:08:27 -0500\nFrom: IESG Secretary <iesg-secretary@CNRI.Reston.VA.US>\nSubject: WG Action: HyperText Transfer Protocol (http)\nTo: IETF-Announce: ;\n\nA new working group, jointly chartered by the Applications and\nTransport Services Areas of the IETF, has been formed. For more\ninformation, please contact the working group chairs or the Area\nDirectors.\n\n\nHyperText Transfer Protocol (http)\n----------------------------------\n Chair(s):\n     Dave Raggett <dsr@w3.org>\n     Tim Berners-Lee <timbl@w3.org>\n\n Applications Area Director(s):\n     John Klensin  <Klensin@mail1.reston.mci.net>\n     Erik Huizer  <Erik.Huizer@SURFnet.nl>\n\n Area Advisor\n     John Klensin  <Klensin@mail1.reston.mci.net>\n\n Mailing lists:\n     General Discussion:http-wg@cuckoo@hpl.hp.com\n     To Subscribe:      http-wg-request@cuckoo.hpl.hp.com\n In Body:       subscribe http-wg Your Full Name\n     Archive:           http://www.ics.uci.edu/pub/ietf/http/hypermail\n\nDescription of Working Group:\n\nThe HTTP working group will work on the specification of the Hypertext\nTransfer Protocol (HTTP). HTTP is a data access protocol currently run\nover TCP and is the basis of the World-Wide Web. The initial work will\nbe to document existing practice and short-term extensions. Subsequent\nwork will be to extend and revise the protocol. Directions which have\nalready been mentioned include:\n\n   improved efficiency,\n   extended operations,\n   extended negotiation,\n   richer metainformation, and\n   ties with security protocols.\n\nNote: the HTTP working group will not address HTTP security extensions\nas these are expected to be the topic of another working group.\n\nBackground information\n\nThe initial specification of the HTTP protocol was kept in hypertext\nform and a snapshot circulated as an Internet draft between 11/93 and\n5/94. A revision of the specification by Berners-Lee, Fielding and\nFrystyk Nielsen has been circulated as an Internet draft between 11/94\nand 5/95. An overview of the state of the specifications and a\nrepository of pointers to HTTP resources may be found at\n\n    http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nOnce established, the working group will expand and complete that\ndocument to reflect HTTP/1.0 as it has been implemented by World-Wide\nWeb clients and servers prior to November 1994. The resulting\nspecification of HTTP/1.0 will be published for review as an\nInternet-Draft and, if deemed appropriate, will be submitted to the\nIESG for consideration as a Proposed Standard or Informational RFC.\n\nIn parallel with the above effort, the working group will consider\nenhancements/restrictions to the current practice in order to form a\nspecification of the HTTP protocol suitable for eventual consideration\nas a proposed standard.\n\nAlso in parallel with the above efforts, the working group will engage\nin defining (or selecting from various definitions) a next-generation\nprotocol for hypertext transfer (HTTPng).\n\n\n Goals and Milestones:\n\n     Done Draft working group charter. Establish mailing list and archive.\n\n     Done Review draft charter for discussion at the Chicago WWWF'94\n  conference. Invest an interim Chair for the working group. Determine\n  writing assignments for first draft of HTTP/1.0 document.\n\n     Done Publish an Internet-Draft on HTTP as reflected by current practice\n  (HTTP/1.0)\n\n     Done Meet at the San Jose IETF as a BOF. Review HTTP/1.0 Internet-Draft\n  and decide whether it should be published as Informational, should be\n  a candidate for further working group development, or should be\n  allowed to expire. Determine writing assignments for first drafts of\n  the HTTP/1.1 or HTTPng documents. Establish charter and submit to\n  IESG\n\n   Feb 95 Revise the Internet-Draft on HTTP/1.0 and, if desired, submit to the\n  IESG for consideration under the category determined at San Jose\n  IETF.\n\n   Feb 95 Publish Internet-Drafts on HTTP/1.1 and HTTPng.\n\n   Apr 95 Final review of HTTP/1.1 draft at the Danvers IETF. Revise HTTP/1.1\n  draft and submit to IESG for consideration as Proposed Standard.\n  Review progress on HTTPng.\n\n   Dec 95 Final review of HTTPng draft at the Dallas IETF. Revise HTTPng draft\n  and submit to IESG for consideration as Proposed Standard.\n  Retrospective look at the activities of the HTTP WG.\n\n----------- end forwarded message ---------\n\n\n\n"
        },
        {
            "subject": "SimpleMD5 quibble",
            "content": "I have finally gotten to play with the SimpleMD5 spec. from Spyglass\nand John Franks's toolkit.  I would like to offer these suggestions.\n\n1) The password that gets MD5-ed by the client must be stored on the\nserver as plaintext, so the server can do MD5(nonce password).  I find\nthat to be a problem, since, at least in my environment, many of the\nservers are on Unix machines with shared file systems, and it would be\nrelatively easy for someone to find another's password.  I would prefer\nthat the password be stored encoded by some function f() (possibly MD5?).\nThen the client would compute MD5(nonce f(passwd)), and the server could\nduplicate the computation, except it would have f(passwd) in its password\nfile already.\n\n2) I've been annoyed in Basic authentication by the fact that what the\nclient and server call \"realm\" is also used as a prompt to the user.\nCan we separate the two concepts in SimpleMD5 (and Basic, for that\nmatter) by having the client and server continue to exchange a \"realm\"\nattribute and have the server pass a \"prompt\" attribute for the client\nto use?  A server that didn't want to do so pass the same value for\n\"prompt\" as for \"realm\".  A client that didn't see a \"prompt\" attribute\ncould use the value of \"realm\" as a default.\n\n3) In the SimpleMD5 spec. (and Franks's program) there's an insistence\nthat values (e.g., \"PrideRock\" in realm=\"PrideRock\") be \"-delimited.\nSeems to me this is only necessary when there's an embedded space or\nTAB or comma.  How about if we tolerate a non-\"-delimited span of\ncontiguous characters that has no embedded space, TAB, or comma.\n(Are there other characters of which to beware too?)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SimpleMD5 quibble",
            "content": "According to Dave Kristol:\n> \n> I have finally gotten to play with the SimpleMD5 spec. from Spyglass\n> and John Franks's toolkit.  I would like to offer these suggestions.\n> \n> 1) The password that gets MD5-ed by the client must be stored on the\n> server as plaintext, so the server can do MD5(nonce password).  I find\n> that to be a problem, since, at least in my environment, many of the\n> servers are on Unix machines with shared file systems, and it would be\n> relatively easy for someone to find another's password.  I would prefer\n> that the password be stored encoded by some function f() (possibly MD5?).\n> Then the client would compute MD5(nonce f(passwd)), and the server could\n> duplicate the computation, except it would have f(passwd) in its password\n> file already.\n\nThis is a good idea, but it is important to understand that it doesn't\nreally protect you the way you might think.  It is still necessary to\nprotect the password file from being read by any untrusted user.  If\nan untrusted user gets the encoded password f(passwd) he can create\nMD5(nonce f(passwd)) and access everything the user with passwd is\nentitled to.  The reason it is a good idea is that people foolishly\ntend to use the same password on many systems so the sysadmin on the\nSimpleMD5 system might read the password and guess that the user has\nthat password on a different system.\n\nIt would also be a very good idea to actually encrypt the password in\nthe password file and decrypt it when checking access.  This may\ninvolve export problems though.  Here is a way one might do it using\nonly MD5 (which is exportable).  Have a KEY known only to the server\nand password management utilities. In the password file store\n\nusername:salt:encrypted_pw\nwhere\nencrypted_pw :=password xor MD5( username:salt:KEY)\n\nthen the server can extract password because it is equal to\n\nencrypted_pw xor MD5( username:salt:KEY)\n\nThe salt must change whenever the user changes password.  Someone\nreading the password file cannot decrypt the password without knowing\nKEY.  \n\nI don't know how cryptographically sound this is but it should be good\nenough for SimpleMD5.  I also am not certain that the fact that SimpleMD5\nis exportable would make this exportable.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: SimpleMD5 quibble",
            "content": "I suggested having an encoded (encrypted) password in the server-side\npassword file.\n\nJohn Franks said:\n  > This is a good idea, but it is important to understand that it doesn't\n  > really protect you the way you might think.  It is still necessary to\n  > protect the password file from being read by any untrusted user.  If\n  > an untrusted user gets the encoded password f(passwd) he can create\n  > MD5(nonce f(passwd)) and access everything the user with passwd is\n  > entitled to.  The reason it is a good idea is that people foolishly\n  > tend to use the same password on many systems so the sysadmin on the\n  > SimpleMD5 system might read the password and guess that the user has\n  > that password on a different system.\n\nI certainly agree, and I don't want to imply that I believe this is\nbullet-proof security.  The point, though, is that if I grabbed a\npassword from the server-side file, I could masquerade as a user by\nsimply entering that user's password to my favorite browser.  If the\npassword is encoded, I have to go to some more trouble to spoof the\nuser, because I can't simply supply the encoded value to the browser.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SimpleMD5 quibble",
            "content": "                                    __^__\n                                   /(o o)\\\n================================oOO==(_)==OOo==================================\n\n\nGeeee ! Could you please use the correct address. Not only you bother me, but\nthe eric you're trying to send the email to, will not receive it.\n\nEric Cosatto.\n\n                                      _\n================================oOO==( )==OOo==================================\n                                   \\(o o)/\n                                    --v--\n\n\n\n"
        },
        {
            "subject": "HTTP-1.1 and HTTPNG",
            "content": "I have seen the HTTP-NG page, but it is my understanding that there is some\nprovisional specifics called HTTP-1.1 . Are both the same thing? Otherwise,\nwhere should I find docs for the latter?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: SimpleMD5 quibble",
            "content": "> only MD5 (which is exportable).  Have a KEY known only to the server\n> and password management utilities. In the password file store\n[...]\n> The salt must change whenever the user changes password.  Someone\n> reading the password file cannot decrypt the password without knowing\n> KEY.  \n\nHaving the KEY only helps if it's less easy to access than the\npassword files (for the threat you point out). So, how does the server\nknow the KEY? It's in a .conf file somewhere, or it's input on\nstartup. If it's really a key and not a password, the latter will\nnever fly. And actually, if given an option, most sites would take the\n.conf file anyway. Which would mean it was only in memory. Reading\nmemory on a multi-user machine may be slightly harder to do than\nreading a protected file, but the threats are pretty close.\nMez\n\n\n\n"
        },
        {
            "subject": "Re: HTTP-1.1 and HTTPNG",
            "content": "> I have seen the HTTP-NG page, but it is my understanding that there is some\n> provisional specifics called HTTP-1.1 . Are both the same thing? Otherwise,\n> where should I find docs for the latter?\n\nThey are not the same thing.   HTTP/1.1 is what is being called the set \nof extensions to the HTTP/1.0 protocol which are known to be desirable\nfor HTTP1 applications but cannot be implemented properly without some\nindication that both sides understand the extension.  Some of these\nextensions are already listed in the published 1.0 draft -- they will\nbe split off into a separate specification for the next draft.\nNOTE: these are protocol extensions, not server-feature extensions.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Language Tag",
            "content": "My apologies for cross-posting this, but it does apply to both standards.\n\nAlbert Lunde wrote:\n\n> In any case, I think the language attribute should have the same\n> allowed values as the language/dialect in the HTTP Accept-Language and \n> Content-Language headers.\n\nThis has been changed for HTTP/1.0.  We will now be using the language tag\ndefined in <draft-ietf-mailext-lang-tag-02.txt> which will (hopefully) soon\nbe an RFC.  A full URL is\n<http://ds.internic.net/internet-drafts/draft-ietf-mailext-lang-tag-02.txt>\nSince it uses ISO 639 as primary tags, it should be compatible with current\nusage of language names.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "DNS vs HOME PAGE[S]",
            "content": "This seems to be a common naming problem on the web:\n\n   Newsgroups: comp.infosystems.www.providers,comp.infosystems.www.misc\n   From: root@rco.qc.ca (Francois Vrana)\n   Organization: Inter-Acces Communications\n   Date: Sat, 4 Feb 1995 17:33:06 GMT\n   Lines: 20\n\n\n   I have always wondered which www servers support the following:\n\n   would it be possible to have different 'home pages' brought up based on\n   different 'domain names' when users are accessing this server?\n\n   ex: bigsite.com and tinysite.com both map to the same address: 199.84.201.1\n   and the same physical server, but different home pages would be given based \n   on the DNS name selected.\n\n   does anyone know of servers that support this 'off the shelf' ?\n\n\nOne solution is to change HTTP clients to send full URLs all\nthe time (rather than just on proxy accesses).\n\nBut as another solution, it seems that something like the MX record\nfacility for finding SMTP servers would work well for HTTP servers.\n\nThis \"HX record\" would map a domain name to one or more domain/port\npairs or perhaps to one or more URL prefixes, ala:\n\nHX bigsite.comhttp://provider.net/bigsite\nHX bigsite.comhttp://other.provider.net/bigsite\nHX tinysite.comhttp://provider.net/tinysite\n\n\nIt seems to me that we've had this discussion before, but I don't\nremember the resolution. Any pointers?\n\nThanks.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "|>It seems to me that we've had this discussion before, but I don't\n|>remember the resolution. Any pointers?\n\nYeah, we had that discussion before and we never reached some point.\nI really like the idea of always having the full URL in http requests.\n\nBtw: There is another solution to this problem.\nifconfig supports on most platforms (at least with some minor patches)\n\"alias names\". Thus your host can have more than one IP address\nand you can start different httpds for different aliases.\n\nWe use this with FreeBSD and it works fine.\n\n\\Maex\n-- \n______________________________________________________________________________\n Markus Stumpf                        Markus.Stumpf@Informatik.TU-Muenchen.DE \n                                http://www.informatik.tu-muenchen.de/~stumpf/\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "> \n> |>It seems to me that we've had this discussion before, but I don't\n> |>remember the resolution. Any pointers?\n> \n> Yeah, we had that discussion before and we never reached some point.\n> I really like the idea of always having the full URL in http requests.\n> \n> Btw: There is another solution to this problem.\n> ifconfig supports on most platforms (at least with some minor patches)\n> \"alias names\". Thus your host can have more than one IP address\n> and you can start different httpds for different aliases.\n\nIt is a solution, but not for \"most platforms\" (I've seen enough\npeople hunting for appropriate patches to doubt they are\nuniversally available for all forms of Unix.) Then there are Macs\nand PCs. (It works with ifconfig derived from recent BSD stuff.)\n\nIf we could find a way to do it that would not break existing\nstuff, that would be nice. Putting the full URI in a seperate\nrequest header would be safer than putting it on the GET,\nbut would be redundant.\n\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "There are no HTTP servers that support multiple domain names per \nmachine because they cannot under the current protocols.  (There are \nsome versions of Unix which will support multiple peer IP addresses \nper host, but this is not the same thing.)\n\nOutside of the DNS protocols, there are no domain names.  When a TCP\nconnection is made to a particular host, the only designators\navailable are the numeric IP and port addresses.\n\nThere are many reasons why the full URL needs to be provided when an\nHTTP request is made.  I have suggested a number and this is one. \nUnfortunately, I don't see this happening easily -- due to the\ninstalled base.\n\nAnd if you really want to see some fur fly, suggest that DNS needs\nto be modified.  <chuckle>  I've learned MY lesson on that score, I\ncan tell you.  Doesn't matter if the suggestion has merit.  The DNS\nis considered -- by its authors and maintainers -- to be the most\nimportant and fragile of all Internet technologies.  I'd rather \nsuggest modifications to any established world religion.\n\n</rr>\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "> From http-wg-request@cuckoo.hpl.hp.com Fri Feb 10 10:19:37 1995\n> \n> Putting the full URI in a seperate request header would be safer than putting > it on the GET,  but would be redundant.\n\nIncluding the whole URI in the GET would be cleaner except, of course, if you\nhave an old server.\n\nA new request header would only be redundant if the whole URI were included.\nAll you would need to do to send the complete information describing the\nrequested URI would be to send the target hostname or alias in its own request\nheader field.\n\nDon't muck with the request line, put the hostname in a header.\n\n-marc\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "From:             Markus Stumpf <stumpf@informatik.tu-muenchen.de>\n> Btw: There is another solution to this problem.\n> ifconfig supports on most platforms (at least with some minor patches)\n> \"alias names\". Thus your host can have more than one IP address\n> and you can start different httpds for different aliases.\n\nThis is not an acceptable solution.  \n\nAssigning more than one IP address to hosts (which are not gateways\nor routers between different physical networks) is an irresponsible\nuse of a consumable resource. IMHO.\n\n</rr>\n\n\n\n"
        },
        {
            "subject": "RE: DNS vs HOME PAGE[S]",
            "content": "   Date: Fri, 10 Feb 95 09:07:20 CST\n   From: connolly@hal.com (Dan Connolly)\n\n   [ . . . ]\n\n   But as another solution, it seems that something like the MX record\n   facility for finding SMTP servers would work well for HTTP servers.\n\n   This \"HX record\" would map a domain name to one or more domain/port\n   pairs or [ . . . ]\n\nThe basic problem is that at the moment of accepting a client's\nconnection, the server has in hand the IP address of the client but\nnot a clue about either the IP address or domain to which the client\nbelieves it is connecting.  Hence, an \"HX\" won't do the server any\ngood for making a decision about what to do next.\n\nThe notion of something like an \"HX\" record analogous to an MX\nrecord might make some sense despite this, since some sites will want\nto support off-site or redundant HTTP servers.   But a new \"HX\" type\nis NOT a good idea because it requires formally extending the DNS\nspec.\n\nHowever, you can easily implement the exact equivalent of an \"HX\" RR\ntype by using Hesiod's TXT type RR, which allows for application-specific\nsyntax and semantics in its text portion.  For example, you might have\nthis pair of records for \"bigsite\" and \"smallsite\":\n\n www.bigsite.com.   HS TXT  \"http://provider.net/bigsite  http://other.provider/bigsite\"\n www.smallsite.com. HS TXT  \"http://provider.net/smallsite\"\n\nThe syntax of the text string is simply a list of URLs:  a client\nreceiving this RR would try the first URL, then the second, and so\nforth, when searching for the site's \"Home Page\".  For example, given\n\"http://www.bigsite.com/\", after retrieving the Hesiod record for\n\"www.bigsite.com\" the WWW client would start out by trying to GET\n\"http://provider.net/bigsite\" and then \"http://other.provider/bigsite\"\nif the first attempt failed.\n\nAs a default, you could obviously use the same rule used for MX\nrecords:  absent our \"HX\" record, the HTTP client should attempt to\nconnect directly to the domain mentioned in the URL (i.e.\n\"www.bigsite.com\").\n\nOn the downside, unlike the case an MX record, you can't discover this\nHS class TXT record for \"www.domain\" by simply asking your local\nnameserver for class \"IN\", type \"ANY\" records for the domain.  You\nhave to make a separate query using the \"HS\" class to get the record.\nCaching at the local nameserver should reduce the impact.\n\nThe fact that DNS zones are (in theory) always replicated means that\nthis record will (in theory) be redundantly available to all clients\non the net.\n\n\n- Bede McCall   <bede@mitre.org>\n\n   The MITRE Corporation                      Tel: (617) 271-2839\n   Bedford, Massachusetts                     FAX: (617) 271-2423\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "Rob,\n\nto Markus Stumpf <stumpf@informatik.tu-muenchen.de>\n  > > Btw: There is another solution to this problem.\n  > > ifconfig supports on most platforms (at least with some minor patches)\n  > > \"alias names\". Thus your host can have more than one IP address\n  > > and you can start different httpds for different aliases.\n\nyou answered\n  > This is not an acceptable solution.  \n  > \n  > Assigning more than one IP address to hosts (which are not gateways\n  > or routers between different physical networks) is an irresponsible\n  > use of a consumable resource. IMHO.\n\nI appreciate the concern very much (and while running a local registry\nI worked a lot to help avoid wasting of both address space and routing table\nsize), but there are cases in which assigning multiple addresses are \nperfectly OK;  for example\n\n- if you actually would otherwise pay the price to run an additional interface\n  or box (well, first of all I would think that multi-homed hosts OK even\n  if they don't forward packets between different physical networks)\n\n- if someone is using private address space (RFC 1597 - do I need my asbestos\n  suit?) and adds unique addresses for hosts that are externally visible\n  (in particular if the consumed unique address space is densely populated,\n  which can be achieved e.g. by distributing host routes to all routers\n  on the internal paths to the public network)\n\n- if the address space used for secondary addresses is densely populated\n  and routes aggregate well\n\n- temporarily (e.g. to help while renumbering... - and note: if only a small\n  percentage of all your hosts gets external connectivety, renumbering the\n  externally visible secondary addresses can hurt much less than the traditional\n  renumber all of the primary addresses game)\n\nI apologize for abusing the URI and HTTP lists with off-topic traffic;\nhowever I think the question raised by Rob is important enough to have\na reply distributed to the same lists.\nFor discussing this matter in more detail follow up is suggested to\nthe CIDRD list (whatever it's today).\n\nCheers,\n  Ruediger\n\n---\nRuediger Volk\nUniversitaet Dortmund, Informatik IRB\nD-44221 Dortmund, Germany\n\nE-Mail: rv@Informatik.Uni-Dortmund.DE\nPhone:  +49 231 755 4760                 Fax:  +49 231 755 2386\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "While I can appreciate Ruediger's insight here, I would suggest that \nthe simplest solution is to change the HTTP protocol to accept the \nentire URL.  Using more than one IP address per machine, (routers and \ngateways excepted, of course), only complicates the problem more.  \nBesides, this is only appropros for Unix-en where we can play games \nwith the networking code.  Hardly a broadbased solution, eh?\n\nOne of the real problems here that I have been anxious to address is \nthe ability to mirror HTTP resources across multiple hosts in a \nseamless (at least to the user) manner.  If we sent the entire URL in \nthe request, it wouldn't matter what machine actually received it.\n\nI have a modified version of named that allows a variety of \ninteresting games to be played in response to a domain name request.  \nOne of the uses I put this to is to direct requests to multiple \nIP addresses depending on the network of origin of the requestor.\n\nIt attempts to address part of the \"appropriate retrieval decision\" \nproblem that some of you know I am concerned with.\n\n</rr>\n\n\n\n"
        },
        {
            "subject": "Getting full URI to the serve",
            "content": "Why not pass the full URI in a header line?  No change to the request\nline necessary, and essentially upwards compatible.  Perhaps:\n\n  Full-URI: http://foo.bar.org/index.html\n\nMike Cowlishaw\nIBM UK Laboratories\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "> Why not pass the full URI in a header line?  No change to the request\n> line necessary, and essentially upwards compatible.  Perhaps:\n> \n>   Full-URI: http://foo.bar.org/index.html\n\nThis, or some variation upon it, has been discussed several times\non the mailing list.  The most common response is that it includes\ntoo much extra information which would better appear in the Request-URI.\nRather than discussing that again, let's simplify it a bit:\n\n   Host: foo.bar.org\n\nsince that is the only thing interesting which was left off the request.\n[Before anyone mentions it: no, the port number is not interesting].\n\nWe should then ask ourselves why this is desirable and what it allows\nus to achieve?  The only desirable aspect is that it allows a single\nIP address and port to use the same default root address for multiple\nhostnames.  For example, both\n\n   http://www.ics.uci.edu/    and    http://liege.ics.uci.edu/\n\nget resolved to\n\n   http://128.195.1.5:80/\n\nIf those two hostnames were intended to represent separate organizations,\nas in the case of a single server providing Internet-presence-for-hire\n(i.e., vanity hostnames), then there may be a slight political/marketing\nproblem if the result was a page that required the user to select from\nall of the various home pages.\n\nNote that it has no other advantage, as the only non-distinguishable link\nto that server is the root.  So, the next question is what would adding\na \"host\" header (or the equivalent) achieve?  Well, we already know that\nexisting clients do not support such a header, and it would take quite a\nbit of time to get them to, so the server cannot rely on its existence\nto provide the switching mechanism among the intended home pages.  Thus,\nthe best that it can do is provide a supplement -- if the header is present,\nthe choice is made automatically; otherwise, the user is presented with\na \"choose one of the following\" default page.\n\nThe next question is: are there any better ways of accomplishing what\nis desired?  The answer is Yes and No.  Yes in that just putting the\nfull URI in the Request-Line achieves the same effect (plus many more\nimportant effects) and would be consistent with the existing protocol\nfor communicating with proxies.  No, because doing so will not work with\n1.0 servers and thus cannot be introduced before version 2.0.\n\nThe final question is: Does the additional functionality justify the cost\nand effort of including the Host header in the 1.1 standard, with the\nnecessarily strong recommendation that it be included with all requests?\n\nIn my opinion, the answer to this last question is NO.  Allowing a server\nto automatically choose the root URL from among the roots associated with\nmultiple vanity hostnames is simply not a sufficiently important piece of\nfunctionality to justify its inclusion as part of the 1.x standard.\n\nIn order for such a feature to be added to the protocol, it will have to\njustify itself externally to the standards process.  In other words, if\na sufficient number of WWW browsers and servers implement a header with the\nabove syntax and semantics, that header will be added to the specification.\n\nWhen in doubt, bottom-up standardization is better than top-down.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "Thus wrote: \"Roy T. Fielding\"\n>   Host: foo.bar.org\n>\n>since that is the only thing interesting which was left off the request.\n>[Before anyone mentions it: no, the port number is not interesting].\n\nNo, but the fragment identifier (i.e. the #foo that may go on the end\nof a URL) could be there, and could be interesting in some cases.\nThere had been some discussion of extending fragments to allow\nsearches for particular strings, line references, that sort of thing;\nincluding it in the request line could allow servers to do\npreprocessing or only send part of the document if such is\nappropriate, which could in theory be more efficient.  I don't find\nthis compelling, but it is another potential advantage of sending a\nfull URL.\n\n>Note that it has no other advantage, as the only non-distinguishable link\n>to that server is the root.\n\nI wouldn't quite agree there.  Other site-level conventions\n(e.g. /robots.txt, /site.idx, a file for site-level description\ninformation if one is made, etc.) could also benefit from this header.\nIt also could permit a smoother migration of name; for example, if you\nneed to change the name of your server, this header could permit the\nold and new names to be mapped together briefly but permit logging of\nwhich Referer: URLs are still pointing to the old name.\n\nThat said, I'm inclined to agree that the benefits are not terribly\noverwhelming.\n\n- Marc\n--\nMarc VanHeyningen  <URL:http://www.cs.indiana.edu/hyplan/mvanheyn.html>\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "At 8:04 AM 2/12/95, Marc VanHeyningen wrote:\n>I wouldn't quite agree there.  Other site-level conventions\n>(e.g. /robots.txt, /site.idx, a file for site-level description\n>information if one is made, etc.) could also benefit from this header.\n>It also could permit a smoother migration of name; for example, if you\n>need to change the name of your server, this header could permit the\n>old and new names to be mapped together briefly but permit logging of\n>which Referer: URLs are still pointing to the old name.\n>\n>That said, I'm inclined to agree that the benefits are not terribly\n>overwhelming.\n\nTime for $.02 from the peanut gallery. The benefits are not terribly\noverwhelming, it serves only a few specific purposes, and it involves the\naddition of a single request header. Given that this has been discussed and\nrequested and discussed again for the past 6 months, why not mollify the\npeople who want this feature and simply add it to the spec?\n\nRather than continuing to thrash this around repeatedly in e-mail, just\nwrite the paragraph about it in the standard and let the standards process\nfinish off the discussion. One of the complaints many have about HTML and\nHTTP is that the standards process is creeping at a snail's pace, forcing\ncommercial developers to strike out on their own when the standards\n\"keepers\" ignore or discard their requests for feature incorporation into\nthe DRAFT standards.\n\nIn the grand scheme of things, this feature requires NO modifications to\nexisting servers and about a 2 line modification to clients that want to be\ncompliant. Let's stop talking about it and put a strawman in the standard!\nIf it turns out to be a bad idea, we can delete it. That's been done\nbefore. In the meantime, it'll stop complaints that the standards process\nisn't responsive and accomodate the people who want this feature.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "'FullURI' heade",
            "content": "My recent post was perhaps too terse: I intended to imply that 'if this\nextension is indeed needed, then why isn't the proposal to add the\ninformation as a header rather than modifying the Request line'.\n\nAs to whether the extension is needed, I'd lean towards Chuck Shotton's\nview.\n\nIn particular, I publish a page-tree (http://rexx.hursley.ibm.com/rexx)\nwhose address is actually an alias for www.hursley.ibm.com because I\nexpect to move it to a separate physical box as soon as the details\n(like buying the hardware) can be finalized.  I've discovered that all\nover the net, pages from my data are being quoted as at the 'www...'\naddress because some WWW clients are doing the back-substitution when\nconnecting to anything below the index.html.  Given the Full-URL (by\nwhatever means), I could warn people that they're using a suspect\naddress.  Without it, I can't.\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "According to Roy T. Fielding:\n> \n> > Why not pass the full URI in a header line?  No change to the request\n> > line necessary, and essentially upwards compatible.  Perhaps:\n> > \n> >   Full-URI: http://foo.bar.org/index.html\n> \n> This, or some variation upon it, has been discussed several times\n> on the mailing list.  The most common response is that it includes\n> too much extra information which would better appear in the Request-URI.\n> Rather than discussing that again, let's simplify it a bit:\n> \n>    Host: foo.bar.org\n> \n> \n> The final question is: Does the additional functionality justify the cost\n> and effort of including the Host header in the 1.1 standard, with the\n> necessarily strong recommendation that it be included with all requests?\n> \n> In my opinion, the answer to this last question is NO. \n\nI suspect that the ability to customize the default page based on \nhostname part of the URL is the single most requested feature from\nserver maintainers.  I doubt that a week goes by without a thread on\nthis subject in c.i.w.providers.  The practice of using multiple IP\naddresses on a single host for the sole purpose of working around this\ndeficiency in the protocol is becoming increasingly common. \n\nThose who are critical of adding a new HTTP header just for \"vanity\naddresses\" should keep in mind that the likely alternative is the\nwasteful use of IP addresses just for vanity addresses.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "rom Roy T. Fielding:\n> The final question is: Does the additional functionality justify the cost\n> and effort of including the Host header in the 1.1 standard, with the\n> necessarily strong recommendation that it be included with all requests?\n>\n> In my opinion, the answer to this last question is NO.\n\nrom John Franks:\n>I suspect that the ability to customize the default page based on\n>hostname part of the URL is the single most requested feature from\n>server maintainers.  I doubt that a week goes by without a thread on\n>this subject in c.i.w.providers.  The practice of using multiple IP\n>addresses on a single host for the sole purpose of working around this\n>deficiency in the protocol is becoming increasingly common.\n\nThough I can't find it right now, there is actually a web page/tutorial\ndevoted to using multiple IP addresses on one network interface for this\npurpose.\n\n>Those who are critical of adding a new HTTP header just for \"vanity\n>addresses\" should keep in mind that the likely alternative is the\n>wasteful use of IP addresses just for vanity addresses.\n\nNot only is it likely, it is happening quite a bit.  There are plenty of\nbusinesses buying machines and connections exclusively to put up web\nservers, and they often want to split costs with other businesses.\n\nI agree with Roy that some protocol enhancement should drive the addition\nof a Host: header.  It is currently possible to put more than one server at\none address by assigning one of them a non-standard port number\n(http://www.name.dom/ and http://www.name.dom:8080/).  A Host: header could\nprovide the same functionality in a transparent manner.  As Chuck Shotton\nmentioned, this addition would require no change to servers.  The benefit\n-- allowing providers to seperate offerings into categories without making\nthe user choose from a home page menu -- seems worthwhile.  Not just to\nbusinesses.\n\nM. Hedlund <march@europa.com>\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "It seems like it would be simple enough to simply add in an additional\nheader, so that clients and servers (and proxies) could phase in\nsupport as they wish.\n\nI guess the only real questions I have are:\n\n  a) would anyone do it?\n  b) is it too much of a `hack'?  The main advantage over simply\n     changing the format of what gets sent to the server is that it's\n     a smaller, simpler change and thus may be more likely to get\n     implemented.\n\nOpinions?\n\n\n\n"
        },
        {
            "subject": "original host name in request/heade",
            "content": "While we can imagine changing HTTP's GET to include the full URL or to\nask browsers to include the full URL in the header, fixing it in HTTP\nat this point will not have the desired effect of allowing service\nproviders to avoid allocating two IP addresses to the same host.\n\nIn order for this change to be effective, it would have to make its\nway into almost all web browsers. I think that's a latency of at least\na year or two now.\n\nYes, we know net addresses are currently a scarce resource, but trying\nto fix this with HTTP at this point just isn't going to work.\n\nI think this should probably be a requirement for HTTP 2.0, but there\nis no migration strategy that will make this work for HTTP 1.0.\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "Larry Masinter writes:\n>I think this should probably be a requirement for HTTP 2.0, but there\n>is no migration strategy that will make this work for HTTP 1.0.\n\nI disagree.  A system that wanted to use this technique could look for a\n\"Hostname:\" header line, and if it's missing provide the user with a list of\nthe hosts available at that address.\n\nIf they wanted to make it really robust, they could have all of the\nhostname-specific content a directory down, and only look at the Hostname\nheader if the request is \"GET /\".  If the header were there, the server\ncould generate a redirect to the appropriate subdirectory, and if not\nprovide the list of links to hostname-specific info.\n\nIn this scenario, even if someone with a browser which supported this\nextension mailed a URI to someone using a non-extended browser, it would\nstill work since the directory information would be present.  The\nsubdirectory stuff could be phased out once all the browsers supported the\nextension.\n\n--\nJim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "On Sun, 12 Feb 1995, Roy T. Fielding wrote:\n> Note that it has no other advantage, as the only non-distinguishable link\n> to that server is the root.  So, the next question is what would adding\n> a \"host\" header (or the equivalent) achieve?  Well, we already know that\n> existing clients do not support such a header, and it would take quite a\n> bit of time to get them to, so the server cannot rely on its existence\n> to provide the switching mechanism among the intended home pages.  \n\nFortunately, the changes on each side are really minor.  In fact there \nare no changes required on the server side if the request is funneled to \na CGI script (where one would look for the \"HTTP_HOST\" environment \nvariable).\n\nSo, client folk, it's up to you.... do I hear any takers?\n\n> The final question is: Does the additional functionality justify the cost\n> and effort of including the Host header in the 1.1 standard, with the\n> necessarily strong recommendation that it be included with all requests?\n> \n> In my opinion, the answer to this last question is NO.  Allowing a server\n> to automatically choose the root URL from among the roots associated with\n> multiple vanity hostnames is simply not a sufficiently important piece of\n> functionality to justify its inclusion as part of the 1.x standard.\n\nIt seems to me to be the *definition* of something easy to add - if\nsemantically or systemically there are no objections to it, then browser\nauthors should be encouraged to take 10 minutes to add it.  In fact, I\ndon't even see a terrible upgrade issue - if there's a host with multiple \nvanity hostnames going to it, and it doesn't get a Host: header, it can \ngive a default page with a list of all pages, whereas if it does get a \nHost: header it can give the appropriate vanity home page.  \n\nIn some respects this gets into the need for an HTTP user agent \ndefinition, possibly even a numbered-level classification scheme.\n\n> In order for such a feature to be added to the protocol, it will have to\n> justify itself externally to the standards process.  In other words, if\n> a sufficient number of WWW browsers and servers implement a header with the\n> above syntax and semantics, that header will be added to the specification.\n> When in doubt, bottom-up standardization is better than top-down.\n\nAgreed.  Now if only I could find my way around X Mosaic source code...\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "...\n> I have a modified version of named that allows a variety of \n> interesting games to be played in response to a domain name request.  \n> One of the uses I put this to is to direct requests to multiple \n> IP addresses depending on the network of origin of the requestor.\n> \n> It attempts to address part of the \"appropriate retrieval decision\" \n> problem that some of you know I am concerned with.\n\nI've noticed this with a few sites, and it's on my list of things\nto investigate...   Is there a 'standard modified' version yet?\n\nI've also notice that telnet has built in handling of trying each\nA record returned: I wish cern_http lib would do that.\n\n> </rr>\n> \n\nsdw\n-- \nStephen D. Williams    25Feb1965 VW,OH      sdw@lig.net http://www.lig.net/sdw\nSenior Consultant    513-865-9599 FAX/LIG   513.496.5223 OH Page BA Aug94-Feb95\nOO R&D AI:NN/ES crypto     By Buggy: 2464 Rosina Dr., Miamisburg, OH 45342-6430\nFirewall/WWW srvrs ICBM/GPS: 39 38 34N 84 17 12W home, 37 58 41N 122 01 48W wrk\nPres.: Concinnous Consulting,Inc.;SDW Systems;Local Internet Gateway Co.28Jan95\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "At 2:22 PM 2/12/95, Larry Masinter wrote:\n>While we can imagine changing HTTP's GET to include the full URL or to\n>ask browsers to include the full URL in the header, fixing it in HTTP\n>at this point will not have the desired effect of allowing service\n>providers to avoid allocating two IP addresses to the same host.\n\nI think this could be made to work, by a combination of client and server\nstrategy.\n\nIf we assume an intermediate state where half the clients support a full\nURL in the request, and others only do a partial, then a upgraded server\nreceiving a full URL knows what to return, for a partial URL it puts up a\nsingle page advertising the different top level URLs it knows about, and -\nas with the migration strategy for form support - URLs of up-to-date\nbrowsers.\n\n- Mitra\n\n=======================================================================\nMitra                                                    mitra@path.net\nInternet Consulting                                       (415)488-0944\n<http://www.path.net/mitra>                           fax (415)488-0988\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "Jim Seidman wrote:\n\n> I disagree.  A system that wanted to use this technique could look for a\n> \"Hostname:\" header line, and if it's missing provide the user with a list of\n> the hosts available at that address.\n\nThe problem is that in this case, the client not sending a host[name]\nheader also are the clients which only support one URL in a redirection\nstatus code - or am I missing something?\n\nI also belong to the group that believes that having the full URI in a\nrequest line would be more consistent with proxy requests but that it's\nsomething for version 2.0. If there really is a demand for a quick\nsolution then a host[name] header can be added. However, I am getting\nmore and more convinced that the Web community is getting too large for\nquick solutions - the inertia is simply too big and that time simply\nwill run from this solution.\n\n\n-- cheers --\n\nHenrik Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "Stephen D. Williams wrote\n\n> I've noticed this with a few sites, and it's on my list of things\n> to investigate...   Is there a 'standard modified' version yet?\n> \n> I've also notice that telnet has built in handling of trying each\n> A record returned: I wish cern_http lib would do that.\n\nThe Library of Common Code has for the last couple of releases had this\nfeature. However, it does more than that: it caches all DNS lookups and\nwhen it finds a multihomed host, it times the connection, so that it on\nthe next request can select the fastest IP-address. If one IP-address\nfails, the module tries the other ones until it finds one or all of\nthem fails. This was first written to optimize transatlantic multihomed\nhosts of which info.cern.ch is one. I don't think that there are that\nmany hosts of this type around for the moment, but it works for\n`normal' multihomed hosts as well.\n\nI have written some information but can't say the URL right now as I am\nhaving problems with AFS backup :-( That's why you might not be able to\naccess the www.w3.org server depending on what IP-address you get. The\ngeneral info page for the Library is\n\nhttp://www.w3.org/hypertext/WWW/Library/Status.html\n\nWe hope to get mailing lists back up in a couple of days. We are waiting\nfor some new machiens which will arrive next week.\n\n-- cheers --\n\nHenrik Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "> I wrote:\n>I think this should probably be a requirement for HTTP 2.0, but there\n>is no migration strategy that will make this work for HTTP 1.0.\n\nAnd Jim Seidman replied:\n\n> I disagree.  A system that wanted to use this technique could look for a\n> \"Hostname:\" header line, and if it's missing provide the user with a list of\n> the hosts available at that address.\n\nand went on to elaborate on this scheme.\n\nI stand corrected; there is a migration strategy. Now the main\nquestion is whether you want to change GET to reference the entire\nURL, ask for a \"Hostname:\" or ask for a \"Full-URL:\" in the header with\nthe entire reference (including, for example, original base URL and\nrelative address, or # anchor references).\n\nThe migration strategy for modifying \"GET\" doesn't look good, though.\n \n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "On Fri, 10 Feb 1995, Rob Raisch wrote:\n\n> There are no HTTP servers that support multiple domain names per \n> machine because they cannot under the current protocols.  (There are \n> some versions of Unix which will support multiple peer IP addresses \n> per host, but this is not the same thing.)\n\nAs far as I can tell the concern about this issue stems from a desire to\nbring up a web server on the cheap and appear to be a giant unique\nentity to the world.  The issue can be solved w/o any changes to http\nor more important cluttering up an already verbose protocol costing \nmany in bandwidth and processing cycles to save for the few.  \nSince there are companies (e.g., internex.net) specializing in providing\nweb servers for multiple folks I suspect they have solved the problem\nbut I don't know that for a fact.  Some suggestions:\n1)  Most UNIX boxes can have multiple IP addresses, some per adapter and\n    some by having multiple adapters.  In my limited to the metal\n    TCP and UDP programming I always could get BOTH IP addresses for\n    the 'connection' so if the host were configured for multiple\n    addresses, a simple change to the publicly available server code\n    (CERN & NCSA at least) would support differentiation.\n2)  A credible WWW server can be brought up on LINUX for roughly $2K\n    in hardware costs.  Then you have a truly unique hardware box and\n    no tricks, just a rack full of servers attached to a LAN and thus\n    the Internet.  No funny software, etc.\n3)  It wouldn't be a difficult problem with source access as with\n    LINUX etc. to modify the network support to allow a trully large\n    number of IP addresses per host.  Each could be locally bound to\n    a different HTTPD or the HTTPD code modified to interrogate the\n    destination IP address and select the proper home page from\n    there.\n\nOf course as is often the case, I may have missed a compelling reason why a \ncapability is desireable but if not I have attempted to illustrate \neasy solutions which don't require standards activity.\n\nDave\n\n\n\n"
        },
        {
            "subject": "'FullURI' considered vagu",
            "content": "A couple of people have pointed out offline that 'Full-URI' is a bad\nname for this concept, for sundry reasons (notably, valid translations\nby a proxy, such as on an incoming firewall).  A better name, perhaps,\nmight be \"Original-URI\".  That is, the full URI that the original user\nor agent requested, as distinct from the URI, partial URI, and/or port\nthat the end-server detected.\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "Well, after reading all the comments, I have come to the following\nconclusions:\n\n   1) Too many web developers work on the weekends;\n   2) Marketing has too much influence on web technology; and,\n   3) There's not much I can do about either one. \n\nSo, which one should we include in the 1.1 specification:\n\n   Host: foo.bar.com\n\nor\n\n   Orig-URI: http//foo.bar.com:8001/home/is/where/the/wallet/is.html\n\nAlso, should it be:\n\n   a) recommended for all requests\n   b) recommended only for requests to standard URLs like / and /site.idx\n\nI am assuming that it will not be recommended for requests that already\nuse the full URI, and that will remain the goal for 2.0.  One of the things\nthat we intend to require for 1.1 is that servers know their own set\nof hostnames and do the right thing if they receive a full-URI using one\nof those hostnames.  That would not mean that 1.1 clients would send\nfull-URIs, only that we would have some reasonable hope of doing so for 2.0.\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "Henrik Frystyk Nielsen writes:\n>The problem is that in this case, the client not sending a host[name]\n>header also are the clients which only support one URL in a redirection\n>status code - or am I missing something?\n\nSorry, my message must have been poorly phrased after a long Sunday at work.\nMy intent was that as a migration strategy, redirection would occur if (and\nonly if) a Hostname field was present and the client was retrieving the root\ndocument.  For example:\n\nGET / HTTP/1.0\nHostname: megacorp.com\n\nHTTP/1.0 302 Moved Temporarily\nURI: <http://megacorp.com/megacorp/index.htm>\n\nBut when the client requested any other URI, the server could ignore the\nHostname field:\n\nGET /megacorp/index.htm\nHostname: megacorp.com\n\nHTTP/1.0 200 OK\n...\n\nA client which didn't support the Hostname field would never receive the 302\nresponse, but would instead just get a document listing the different hosts\nfor that address.  (Of course this same strategy could be applied to a\nOriginal-URI or similar scheme.)\n\nAs a transitional scheme, this is nice because all of the URIs for all of\nthe hostnames, with the exception of the root document, would be unique.  If\nsomeone told someone, \"Hey, look at the great content at\nhttp://megacorp.com/megacorp/cool.htm\" it wouldn't matter whether or not\ntheir browser supported the Hostname field, or even if it handled redirects\nproperly.  The URI would just work.\n\n--\nJim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "At 12:45 AM 2/13/95, David - Morris wrote:\n>1)  Most UNIX boxes can have multiple IP addresses, some per adapter and\n>    some by having multiple adapters.  In my limited to the metal\n>    TCP and UDP programming I always could get BOTH IP addresses for\n>    the 'connection' so if the host were configured for multiple\n>    addresses, a simple change to the publicly available server code\n>    (CERN & NCSA at least) would support differentiation.\n\nFaulty assumption #1. Not all Web servers run on Unix. And as time goes on,\nUnix will definitely be a minority platform for Web services, just like it\nis for all other applications. Relying on Unix-specific abilities to\naccomodate a cross-platform need is inappropriate.\n\n>2)  A credible WWW server can be brought up on LINUX for roughly $2K\n>    in hardware costs.  Then you have a truly unique hardware box and\n>    no tricks, just a rack full of servers attached to a LAN and thus\n>    the Internet.  No funny software, etc.\n\nSome people don't have $2k to spare. Why should a slow, balky standards\nprocess refuse a specific feature request from thousands of users, just\nbecause a few individuals writing the standard perceive that $2k isn't too\nmuch to spend for a unique hardware platform?\n\n>3)  It wouldn't be a difficult problem with source access as with\n>    LINUX etc. to modify the network support to allow a trully large\n>    number of IP addresses per host.\n\nHello!?! Why are we wasting IP addresses for vanity WWW names? I wish\npeople would stop trying to engineer some duct tape and bailing wire\nsolution using multiple addresses and just do the right thing. Add the damn\nhost name to the HTTP request as a standard field and stop this\ninterminable e-mail and news discussion.\n\nGiven the number of people who have requested this capability, adding it to\nthe standard would give them a new target for their frustrations - client\nand server authors. Their repeated whinings would insure quick acceptance\nof this addition across all client and server apps, so we needn't worry\nabout it being implemented in a timely fashion. The HTTP standards people\nhave a great resource here if they'd just elect to use it. Give users a\nstandard that does what they want and let the \"marketplace\" take care of\nthe implementation details. It will.\n\n>  Each could be locally bound to\n>    a different HTTPD or the HTTPD code modified to interrogate the\n>    destination IP address and select the proper home page from\n>    there.\n\nSure, and a large service provider could easily eat up an entire class C\naddress on a single server using your scheme. In case you haven't read the\nnews lately, address space is running out and this is a poor solution.\n\n>Of course as is often the case, I may have missed a compelling reason why a\n>capability is desireable but if not I have attempted to illustrate\n>easy solutions which don't require standards activity.\n\nWhat you missed were the compelling reasons for it to be a standards issue.\nHacking around with an address-based solution is not robust, not\ncross-platform, not economical, and not a wise use of address space.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "At 5:55 AM 2/13/95, Roy T. Fielding wrote:\n>Well, after reading all the comments, I have come to the following\n>conclusions:\n>\n>   1) Too many web developers work on the weekends;\n\nAgree. (unfortunately)\n\n>   2) Marketing has too much influence on web technology; and,\n\nAgree!\n\n>   3) There's not much I can do about either one.\n\nDisagree! You're working very hard to put together a good standard. If it\nmeets the needs of developers and users and gets accepted in a timely\nfashion, this will have much more influence on the WWW than rogue\ncommercial development organizations.\n\n>So, which one should we include in the 1.1 specification:\n>\n>   Host: foo.bar.com\n\nAs you mentioned earlier, this is all that is *required*, since everything\nelse should be in the URL as received. It is certainly is easier to\nimplement because no parsing is required to extract this info.\n\n>   Orig-URI: http//foo.bar.com:8001/home/is/where/the/wallet/is.html\n\nOn the other hand, this format is a little more work to parse, but contains\nvaluable information that is often lost or munged when data passes through\nproxies, etc. Looking at just long-term usefulness, this version seems to\nhave a greater chance at being able to support multiple, unforeseen uses.\n(it certainly makes implementing proxy servers easier...)\n\n>Also, should it be:\n>\n>   a) recommended for all requests\n\nYes!\n\n>   b) recommended only for requests to standard URLs like / and /site.idx\n\nNo!\n\n>I am assuming that it will not be recommended for requests that already\n>use the full URI, and that will remain the goal for 2.0.\n\nWhy not be consistent? Proxies can still munge a full URI as easily as they\ncan mangle a current version. Might as well maintain it unscathed in the\nheader field where the real server can grab it.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "On Mon, 13 Feb 1995, Roy T. Fielding wrote:\n> So, which one should we include in the 1.1 specification:\n> \n>    Host: foo.bar.com\n> \n> or\n> \n>    Orig-URI: http//foo.bar.com:8001/home/is/where/the/wallet/is.html\n\nHost:, as it's the only unique part of the request, and we should \ndefinitely try to keep the amount of stuff in the request headers down.\n\n> Also, should it be:\n> \n>    a) recommended for all requests\n>    b) recommended only for requests to standard URLs like / and /site.idx\n\nIt should be recommended for all requests - otherwise collections of HTML \npages will be nowhere nearly as portable.  \n\nHowever, one way to reduce the number of times it has to stick Host: into \nthe headers would be to do a reverse-lookup of the IP number associated \nwith that Host, and if it returned the same hostname, then that must be \nthe canonical hostname for that IP number and thus no Host: header is \nneeded (presuming providers don't do something stupid like have multiple \nreverse DNS tables for the same IP numbers :)  Just a thought.\n \nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: original host name in request/heade",
            "content": "Hi folks,\n\n\nWe seem to have a number of suggestions :-\n\n1) A request line for the original URI\n2) A request line with the intended host name\n\nThe point is that for the security digest function we have to have (1).\nThis is because the keyed digest is produced as a function of the URI\nto prevent spoof of the URI. [the method is also included].\n\nFor the digest to work the original URI has to be reconstructed. This is\nnot necessarily possible if there is a proxy chain that is preforming \nmultiple URI transformations.\n\n\nSo if (1) is going to be there in any case why not use it for this\nas well?\n\nJeff and I are going to be very keen on having the Digest authentication\nscheme in HTTP/1.1. The basic scheme is a dangerous security hole - Thank you\nITAR regulations! The Digest scheme has nothing like the flexibility of\nShen/S-HTTP but does allow the Basic scheme to be squished quickly. \n\n\nPhill.\n\n\n\n"
        },
        {
            "subject": "Re: DNS vs HOME PAGE[S]",
            "content": "Just two quick comments on home pages and DNS; \n\n1) home pages.\n\n(I've been following the discussion under the assumption that the aim of \nthe exercise is to allow marketers to advertise a URL of http://foo.com/\ninstead of  http://foo.com/foo/ for cases where several companies share a \nweb server)\n\nChanging the spec to require full URL qualification  still means that \nnon-1.1 browsers would still send hostless requests. A company \nadvertising its homepage would still need to give a url that could be \nhandled by 1.0 clients, making the whole point a bit moot. \n\n2) DNS\n\nRob Raisch mentioned how hard it is to get any changes made to DNS. \nFortunately, DNS already contains a godawful hack, thanks to those \nawfully nice Athena people. Nearly all DNS servers support TXT records, \nwhich can be used to hold all sorts of nice strings (passwd file entries, \netc. etc.). It'd be easy to set up a convention of sticking a prefix onto \na hostname for an http-server (e.g. http-info.sunsite.unc.edu.), and \nputting an extra question in the dns request the client sends off to do the\nhostname lookup. This technique doesn't help the homepage problem, as it \nhas the same backwards compatibility headaches as mentioned before, but \nit's an interesting way of doing things like versioning and \nback-compatibility.\n\nSimon\n\nContract with America - Explained!|Phone: +44-81-500-3000\nContract: verb|Mail: ses@unc.edu\n1) To shrink or reduce in size - the economy contracted +-----------------------\n2) To become infected -My baby contracted pneumonia when they stopped my welfare\n\n\n\n"
        },
        {
            "subject": "Authorship",
            "content": "In the draft for HTTP/1.0, in section 5.2.3, it is written that POST could\nbe used to extend a document during authorship.\n\nI have two doubts:\n- does any standard (maybe HTML) talk about \"authorship\"?\n- how could this extension cope with the fact that POST creates a \nsubordinate document?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: Authorship",
            "content": "> In the draft for HTTP/1.0, in section 5.2.3, it is written that POST could\n> be used to extend a document during authorship.\n> \n> I have two doubts:\n> - does any standard (maybe HTML) talk about \"authorship\"?\n> - how could this extension cope with the fact that POST creates a \n> subordinate document?\n\nIt is intended for situations where a server allows clients to\nappend information to an existing resource.  A close analogy is the\nnotion of a guestbook, where a GET retrieves the current book and/or\nform and a POST adds what was posted to the book.  The entry posted\nis subordinate to the book, even though it may not have its own URI.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "(hopefully) last questio",
            "content": "I am sorry to peruse this list (and Roy time!), but is seems that \nwww-talk is down - and yes, I am subscribed to it.\n\nAm I correct in inferring that the only difference between a (generic) client\nand a user agent, as far as is concerned in HTTP specifics, is that the \nlatter is *your* client which is making requests (or maybe just the part of\nthe application devoted to this)?\n\nIf not, I'd be grateful if someone could show me the difference. I am \noften really dumb :-)\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: (hopefully) last questio",
            "content": "> I am sorry to peruse this list (and Roy time!), but is seems that \n> www-talk is down - and yes, I am subscribed to it.\n\nwww-talk and all the other www-* mailing lists which is run from CERN\nare down right now. We are about to set up a new mail service at W3C,\nbut it will still take a couple of days before we are there.\n\n> Am I correct in inferring that the only difference between a (generic) client\n> and a user agent, as far as is concerned in HTTP specifics, is that the \n> latter is *your* client which is making requests (or maybe just the part of\n> the application devoted to this)?\n\nThe `client' and `server' definitions are separated from what we\nnormally think of as beeing the client and the server which of course\nis the Line Mode Browser and the CERN server respectively ;-)\n\nThe reason for this is that we would like to decrease the difference\nbetween a client and a server in HTTP. Any application should be able\nto at any time to be a client and/or a server depending on which action\nit wants to perfom. This is to keep the possibility open for extending\nHTTP to have server communications for exchange of cache information\netc.\n\nWe use `user agent' and `origin server' to actually describe the Line\nMode Browser and the CERN server or whatever other application which\nhas chosen to be the origin location for either a request or a\nresponse.\n\n-- cheers --\n\nHenrik Frystyk\n\n\n\n"
        },
        {
            "subject": "Re: Getting full URI to the serve",
            "content": "   Date: Sun, 12 Feb 1995 12:50:45 -0800\n   From: march@europa.com (M. Hedlund)\n\n > Not only is it likely, it is happening quite a bit.  There are plenty of\n > businesses buying machines and connections exclusively to put up web\n > servers, and they often want to split costs with other businesses.\n\nIt's even more complicated than that.  I have a permanent 24 h link\nhome over modem, and five machines home on my net.  I've set up a web\ntree elsewhere, right at a T1 socket, but can't stick it into my named\ndatabase for obvious reasons.  I want to have www.homenet.com point to\nit (by DNS), and have the server at the T1 socket do the right thing.\nModifying httpd to look at the header and pick the root from a config\nfile would be trivial, if the information were just passed.  Playing\ngames with IP addresses is not an option in my case, since I doubt the\nT1 link owner will let me play around with their routers and interface\nconfigurations.\n\nSince any old Sparc-1 with 2GB disk and 32MB memory will make a\nsplendid server, the expensive resource is net bandwidth.  In the\nfuture, more and more small businesses, individuals, and organizations\n(the San Francisco Aids Foundation is trying to do something very\nclose to what I'm trying to do), will buy shares in a dedicated\noff-site web server.  It'll sit right next to an existing T1 socket to\nreduce telco costs\n\nI can't see why there's even any discussion about this.  Just add the\ndamn field to the spec so people can get things working.\n\n-- Jan Brittenson\n   bson@eng.sun.com\n\n\n\n"
        },
        {
            "subject": "New Web server for OS/",
            "content": "I've just made my new Web server, GoServe 2.00, available; for full\ndetails and the code package, see\n\n  http://www2.hursley.ibm.com/goserve\n\nThis has evolved from a Gopher server (indeed, it can still be used as\none), and its Web features have been designed 'from scratch', with\nparticular emphasis on performance and usability.\n\nThe features that may be of interest to this working group are:\n\n  * Conforms to the current HTTP/1.0 draft (I hope!)\n\n  * Connections are handled by threads, not processes, and scripts are\n    run on the same thread, for performance.  The primary script (which\n    currently must be written in Rexx) is cached (run from tokenized\n    copy in memory).  The sample script will run in under 15ms on a\n    (slow) 486/33MHz PC, and simple transactions can complete in one\n    OS/2 timeslice (<32ms).\n\n  * All requests are passed through the script; this means that all\n    requests may have additional HTTP headers, etc., added -- very\n    handy for experimenting with the protocol or testing browsers.\n\n  * There's a graphical user interface for the server owner, with an\n    emphasis on reporting how the server is performing (for example, an\n    active bar chart of response times for the last 100 transactions).\n\n  * Write-through auditing, enforced limits, etc.  See documents.\n\nIf anyone would like a copy, please help yourself.  All comments or\nquestions are very welcome, of course.\n\nMike Cowlishaw\nIBM Fellow\nIBM UK Laboratories\n\n\n\n"
        },
        {
            "subject": "A Proposed Extension to HTTP : SimpleMD5 Access Authenticatio",
            "content": "I skimmed the Internet draft for an MD5 simple access authentication mechanism\nat:\n    http://www.spyglass.com/techreport/simple_aa.txt\n\nI have read about SHTTP and HTTP, and see both using nonces for the\nauthentication step in access control (although SHTTP has some other\nmechanisms as well).  Not meaning to be presumptuous, but\nshouldn't the MD5 response field be  \"<nonce> <password> <resource requested>\"\nas oppossed to \"<nonce> <password>.\"\n\nIt seems to me that the \"<nonce> <password>\" is vulnerable to a man in the\nmiddle attack.  Here's my reasoning:\n\n1. Alice requests resource R1.\n   Mallet simultaneously requests R2 which Alice has access to.\n2. Since these are two separate transactions, the server (or possibly separate\n   servers) returns two nonce values N1 and N2 for R1 and R2 respectively.\n3. Provided that the same password protects both resources, Mallet\n   can swap N2 for N1.\n4. Mallet intercepts Alice's authorization and swaps response field into\n   the authorization for resource R2.\n5. The server returns R2 instead of R1 which is not encrypted\n   (since no encryption mechanism has been employed in HTTP), therefore\n   Mallet picks up R2 as it goes by on the network.\n\nIn this manner, Mallet can gain access to any resource available to Alice so\nlong as the resources are accessed using the same password.\n\nPerhaps this is a petty problem, but it would be so easy to fix.\n\nDavid Harrison\nComputer Science Dept.\nRensselaer Polytechnic Institute\n\n\n\n"
        },
        {
            "subject": "Re: A Proposed Extension to HTTP : SimpleMD5 Access Authenticatio",
            "content": "The Internet-Draft for SimpleMD5 needs another revision to incorporate\nchanges resulting from feedback.  The change you mentioned has already been\naddressed, we just haven't had time to put out the new draft.\n\nBTW, it never was a *real* Internet-Draft, since I never submitted it to\nthe IETF yet.  We will.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "HTTP extensio",
            "content": "Hi,\n\nHas any one heard about the extension of HTTP to a full distributed \nobject-oriented system?\n\nI need information about this subject.\n\nFarah BELAIDI\nESIGETEL computer dept.\nbelaidi@prof.esigetel.fr\n\n\n\n"
        },
        {
            "subject": "Re: HTTP extensio",
            "content": "belaidi@prof.esigetel.fr writes:\n> Hi,\n> \n> Has any one heard about the extension of HTTP to a full distributed \n> object-oriented system?\n> \n> I need information about this subject.\n\nThe extensibility of HTTP is one of the issues we are exploring, but it is not \nclear that an extended HTTP will make a good distributed object invocation \nprotocol. One issue is that objects can have arbitrary sets of methods, and \nmay not have any that match the HTTP basic methods, so adding methods at this \nlevel may be the wrong way to address the problem. The other ways to extend \nHTTP also seem to lead to similar problems.\n\nAnother issue is that the invocation protocol is only a small part of a \ndistributed object-oriented system. There are many other issues such as \nnaming, resource discovery, dependability, security etc. This is only one of \nthe WWW-related groups that have areas of common interest with distributed \nobject-oriented systems.\n\nSeveral groups, including us (ANSA), are working on various kinds of \ninteroperability between HTTP and CORBA. We have an experiment in which CORBA \nclients and servers interact using HTTP as the RPC protocol in such a way \nthat, for simple types, conventional forms and CGI scripts can interoperate \nwith the CORBA parts. One of my colleagues gave a presentation on this to our \ntechnical committee recently, the abstract is at <URL:http://www.ansa.co.uk/pha\nse3-doc-root/approved/APM.1419.01.html>, but the presentation itself, and the \ncode of the prototype, are available only to our sponsors.\n\nGordon Irlam has a page <URL:http://www.base.com/gordoni/web/distribution.html>\n with pointers to relevant resources.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "HTTP should be able to transfer part of a documen",
            "content": "------- Forwarded Message Follows -------\nDate:          Wed, 8 Mar 1995 10:54:10 +0100\nFrom:          MAILER-DAEMON@ms.mff.cuni.cz (Mail Delivery Subsystem)\nSubject:       Returned mail:  Host unknown (Name server: cuckoo.hp1.hp.com: host not found)\nTo:            <DINGLE@ksvi.mff.cuni.cz>\n\nThe original message was received at Wed, 8 Mar 1995 10:54:08 +0100\nfrom ksvi [194.50.17.197]\n\n   ----- The following addresses had delivery problems -----\n<http_wg@cuckoo.hp1.hp.com>  (unrecoverable error)\n\n   ----- Transcript of session follows -----\n501 <http_wg@cuckoo.hp1.hp.com>...  550 Host unknown (Name server: cuckoo.hp1.hp.com: host not found)\n\n   ----- Original message follows -----\nReturn-Path: <DINGLE@ksvi.mff.cuni.cz>\nReceived: from KSVI/MAILQUEUE by ksvi.mff.cuni.cz (Mercury 1.21);\n    8 Mar 95 10:58:35 +0100 (MET)\nReceived: from MAILQUEUE by KSVI (Mercury 1.21); 8 Mar 95 10:58:30 +0100 (MET)\nIn HTTP 1.0 there seems to be no way to retrieve a given part of a document, e.g. bytes\n1500000 through 1600000 of a long binary file.  This seems to be an important \nlimitation, but I haven't seen any discussion of adding the capability for partial \ndocument retrieval to the next version of HTTP, so I thought I'd bring it up \nhere.\n\nPartial document retrieval is important for at least two reasons:\n\n1) If a long transfer is interrupted, it's possible to resume the transfer\nwithout beginning all over again.  (This was a major motivation for the FSP\nprotocol, which allows partial transfers; FTP does not.)\n\n2) The client may be interested in only part of the contents of a document. \nFor example, consider using HTTP to retrieve a .tar or .zip file.  With partial\nretrieval, the client program could retrieve only the index at the beginning of\nthe file, and then only the archive files which the user was interested in. \nToday, there are thousands of useful archive files available on the Web; often\nI just want to read a README.TXT file within such an archive to see if it is of\ninterest, but must download the entire archive.  The limitation of HTTP under\nconsideration prevents me from writing a smart Web browser which can look at\nfiles inside tar and zip archives.\n\nOne possibility would be to address partial HTTP documents using URI \nfragment identifiers, so that, for example, a hypertext page could refer to an individual \nfile within a tar archive, or to a group of several paragraphs within the text of a \nbook.  For example, we might use a syntax such as\n\nhttp://www.cuni.cz/a/b/xxx#(1500000,1600000)\n\nto mean bytes 1500000 through 1600000 of the given document.\n\nOf course, such number-based addressing is somewhat dangerous to use in a\nstatic URL, because if the addressed document changes (e.g. a new file is\ninserted into the tar archive) then the range could reference meaningless or\ngarbage data.\n\nAs an alternative, we might include the byte range not in a HTTP URL, but \nrather as part of a request header or part of a HTTP GET request.  This would \nimply that such addressing is not really appropriate for a URL, but is a \nsort of meta-capability which a smart client might use to implement features \nsuch as (1) and (2), above.\n\nI would be very interested to hear any comments about this.\n\nAdam Dingle\n\n\n\n"
        },
        {
            "subject": "Re: HTTP should be able to transfer part of a documen",
            "content": "In message <1DE9E260DDF@ksvi.mff.cuni.cz> of Wed, 08 Mar 1995 11:17:04\n  +0100, Adam Dingle <DINGLE@ksvi.mff.cuni.cz> wrote:\n\n> One possibility would be to address partial HTTP documents using URI\n> fragment identifiers, so that, for example, a hypertext page could\n> refer to an individual file within a tar archive, or to a group of\n> several paragraphs within the text of a book.  For example, we might\n> use a syntax such as\n> http://www.cuni.cz/a/b/xxx#(1500000,1600000)\n> to mean bytes 1500000 through 1600000 of the given document.\n\nWN already uses a syntax like http://www.cuni.cz/a/b/xxx;bytes=1500000-1600000\nto return ranges.\n\n> Of course, such number-based addressing is somewhat dangerous to use in a\n> static URL,\n\nWell, you could protect it with if-modified-since.  If you want a link\nwhich will get the index to any .zip file on a remote machine even after it\nchanges, then you have to decide where to put the zip-interpreting code.\n\nPutting it on the server is obviously nice because it'll help the users of\nall the non-hacked clients.  In general, however, someone's going to want\nto use a format not supported by someone else's server.\n\nAdding it to the client means a kind of request-wrapping processor has to be\ndefined.  So, here's how I think it might work:\n\n1. Client gets a document via http for display in the normal way.  The\n   server responds with Content-type: application/x-zip.  The client\n   recognises this type from its table of content-type interpreters, and\n   fires up the Zip Interpreting Algorithm.\n2. Under the zip-specific code, the client aborts the http request [*] and\n   starts a new one, retrieving the portion of the zip file which contains\n   the index [&].\n3. When it arrives, it's decoded into a menu of files.  Where links into\n   the zipfile are needed, you can use a URL like:\n\n     internal-zip:name=readme.txt&start=577&end=16320/http://host/big.zip\n\n   (meaning: bytes 577-16320 of http://host/big.zip, to be interpreted as\n    a file named readme.txt).  As you can probably guess, I just made this\n    format up.  But since I'm proud of it, let's take a closer look at\n    its structure:\n\n    <method>:<arguments>/<recursive-url>\n\n    The <arguments> are encoded identically to those used in form responses,\n    and are arguments to the client-side processing identified by <method>.\n    The <recursive-url> identifies the input data to the <method>.  (This\n    allows you to browse tar files inside zip files.)\n\n    For referring to the readme.txt in a static URL, you might use\n\n     internal-zip:name=readme.txt/http://host/big.zip\n\n    (leaving out the byte ranges).  This would cause extra processing in\n    the client to get the index first, but would survive if the zipfile\n    changed [%]\n\n[*] Server implementors may now yelp in pain\n[&] Of course, you have to check that the server can return byte-ranges,\n    perhaps through a server-extensions line in the original reply.\n[%] In fact, if the internal-zip algorithm is stateful or can use a cache,\n    the byte ranges can be left out completely even in the generated menu.\n\nComments?\n\n-- \nAdrian.Colley@sse.ie   <g=Adrian;s=Colley;o=SSE;p=SSE;a=EIRMAIL400;c=ie>\nphones:- work: +353-1-6769089; fax: +353-1-6767984; home: +353-1-6606239\nemployer: Software and Systems Engineering (+=disclaimer)  (Perth)->o~^\\\nY!AWGMTPOAFWY? 4 lines, ok? qebas perl unix-haters kill microsoft  \\@##/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP should be able to transfer part of a documen",
            "content": "According to Adam Dingle:\n> In HTTP 1.0 there seems to be no way to retrieve a given \n> part of a document, e.g. bytes 1500000 through 1600000 of \n> a long binary file.  This seems to be an important \n> limitation, but I haven't seen any discussion of adding \n> the capability for partial  document retrieval to the next\n> version of HTTP, so I thought I'd bring it up  here.\n> \n> I would be very interested to hear any comments about this.\n> \n\nSome of the capability you discusss has been in the WN server since its\nbeginning and in it predecessor GN.  With the WN server, for example, the\nURLs\n\nhttp://host/dir/foo;bytes=15000-25000\nand\n\nhttp://host/dir/foo;lines=256-1024\n\nrequest the corresponding byte and line ranges of file foo.  The lines\nversion is only allowed for files of type text/*.  The content-type is\noften changed.  E.g. a line range from a text/html file is given\ncontent type text/plain.\n\nThis has more uses than you might expect, primarily with text/plain files.\nSee <URL:http://hopf.math.nwu.edu/docs/utility.html#digest>.\nSee <URL:http://hopf.math.nwu.edu/> for more information about WN.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: HTTP should be able to transfer part of a documen",
            "content": "\"Adam Dingle\" <DINGLE@ksvi.mff.cuni.cz> writes:\n> Partial document retrieval is important for at least two reasons:\n> \n> 1) If a long transfer is interrupted, it's possible to resume the transfer\n> without beginning all over again.  (This was a major motivation for the FSP\n> protocol, which allows partial transfers; FTP does not.)\n\nAccording to RFC959, FTP allows restart from a checkpoint in order to cope \nwith long transfers that are interrupted. According to the man page for the \nHP-UX ftp client, Unix systems typically use byte offsets as the checkpoint \nmarkers, and I have successfully used this facility several times, I have even \nused it to retrieve the remainder of a file where the first part was retrieved \nby HTTP, but via a proxy that timed out.\n\nA problem with adding this sort of feature to HTTP is that dynamically created \nresources, and other forms of variable resource, are much more common for HTTP \nthan for FTP, so more care is needed in ensuring that continuing is in the \nsame representation of the same version of the resource, although you could \nuse a different instance (i.e. an identical copy at a different location). \nAlthough the HTTP protocol could be extended to have a \"restart from here\" \nfeature, the implicit assumption that we are always dealing with moderately \nstatic data files needs to be exposed and questioned.\n\nThe retrieval of parts of resources is more a question of the naming of the \nparts of a structured resource, and is not really an HTTP protocol issue. The \nquestion of what constitutes a resource, and how you name resources that are \nparts of collections is the sort of issue that crops up on the URI list, and \nan issue that does not seem to have any completely general solutions.\n\nOn the whole, I would rather encourage the development of smarter servers that \ncan pick components by name out of structured resources like tar or zip \narchives in response to an appropriate URL. This could include delivering \narbitrary ranges out of those resources for which this makes sense. Putting \nthe component description into what the relative URL draft calls the 'param' \npart - i.e. after a ';' the way WN does - is a reasonable approach, but not \nthe only one.\n\nI believe that the current position is that servers can interpret the \n'url-path' in their URLs however they like within the constraints set by the \nvarious Accept...: headers. If the idea is to specify an interpretation for \npart of an http URL, then it would be a good idea to involve the URI group in \nthe discussion since it seems to involve issues important to both groups.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Referer: (sic",
            "content": "Has anyone else noticed that the HTTP header \"Referer:\" is spelled wrong?\n\n\nJohn Franks Dept of Math. Northwestern University\njohn@math.nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Referer: (sic",
            "content": "At 8:46 PM 3/9/95, John Franks wrote:\n>Has anyone else noticed that the HTTP header \"Referer:\" is spelled wrong?\n\nCorrect in Britain, though, no?\n\nM. Hedlund <march@europa.com>\n\n\n\n"
        },
        {
            "subject": "Re: Referer: (sic",
            "content": "> Has anyone else noticed that the HTTP header \"Referer:\" is spelled wrong?\n\nThat's okay, neither one (referer or referrer) is understood by \"spell\"\nanyway.  I say we should just blame it on France.  ;-)\n\n........Roy\n\n\n\n"
        },
        {
            "subject": "Re: Referer (sic",
            "content": "Nope, we Brits spell it with four R's in total (and that's also the\nonly spelling shown in the OED, with quotes from the 1600's and\n1800's).\n\nMike Cowlishaw\nIBM UK Labs.\n\n\n\n"
        },
        {
            "subject": "Re: Referer: (sic",
            "content": ">> Has anyone else noticed that the HTTP header \"Referer:\" is spelled wrong?\n>\n>That's okay, neither one (referer or referrer) is understood by \"spell\"\n>anyway.  I say we should just blame it on France.  ;-)\n\nI also managed to mispell alt.games.mornington.cresent when I created it. I\nthink this one is down to Tim. I sent him a note with the suggestion but I don't \nthink I gave a name for the field.\n\nOh well. We all make mistakes, as the Dalek said climbing off the dustbin.\n\n\nPhill.\n\n\n\n"
        },
        {
            "subject": "New IETF Draft 00 of HTTP/1.",
            "content": "A revised draft of the HTTP/1.0 specification is now available from\n\n    http://www.ics.uci.edu/pub/ietf/http/\n     ftp://www.ics.uci.edu/pub/ietf/http/\nand \n    http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nand will eventually be available at an IETF shadow directory near you called\n    draft-ietf-http-v10-spec-00.{txt,ps}\nYou can tell if you have the right draft by looking in the upper-left\ncorner of the first page for the filename.\n\nIt includes all of the requested changes from prior drafts and is\nalmost complete (only missing a description of URI and a couple\nappendices.\n\nNo diffs are providided, since almost all lines have changed.\nThis would be a good time to read it very carefully and send comments\nto <http-wg@cuckoo.hpl.hp.com>.\n\nHappy reading,\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: HTTP-1.1 and HTTPNG",
            "content": "HTTP-NG and HTTP 1.1 are different;\n\nHTTP 1.1 is a collection of minor modifications to HTTP 1.0 that are \nmostly backwards compatible, but which are too major to make into into the\nHTTP 1.0 spec. \n\nHTTP-NG is rather more distant from HTTP 1.0 in syntax; it is a binary \nprotocol designed for higher performance and better integration with \nsecurity and payment schemes. A more detailed draft of HTTP-NG is currently\nunder preparation for Danvers (Dave Raggett will be translating from \nSimonese to English). \n\nSimon\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v10-spec00.txt, .p",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.0                 \n       Author(s) : T. Berners-Lee, R. Fielding, H. Nielsen\n       Filename  : draft-ietf-http-v10-spec-00.txt, .ps\n       Pages     : 52\n       Date      : 03/09/1995\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol \nwith the lightness and speed necessary for distributed, collaborative, \nhypermedia information systems. It is a generic, stateless, object-oriented\nprotocol which can be used for many tasks, such as name servers and \ndistributed object management systems, through extension of its request \nmethods (commands). A feature of HTTP is the typing and negotiation of data\nrepresentation, allowing systems to be built independently of the data \nbeing transferred.                               \n\nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification reflects preferred usage of the protocol \nreferred to as \"HTTP/1.0\", and is compatible with the most commonly \nused HTTP server and client programs implemented prior to November 1994.                        \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v10-spec-00.txt\".\n Or \n     \"get draft-ietf-http-v10-spec-00.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v10-spec-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.2)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-00.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-00.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950309140821.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-v10-spec-00.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-v10-spec-00.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950309140821.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Re: HTTP should be able to transfer part of a documen",
            "content": "Partial document retrieval based on the original URL is not consistent\nwith the semantics of HTTP URLs, where the data returned is not at all\nrequired to be idempotent with the original data.\n\n> 1) If a long transfer is interrupted, it's possible to resume the transfer\n> without beginning all over again.  (This was a major motivation for the FSP\n> protocol, which allows partial transfers; FTP does not.)\n\nFTP has the same flaw: the file being FTPed could have changed without\nyour knowing it.\n\nI suggest if you want to do this that (a) HTTP GET should return\n(optionally, of course) a unique ID for the actual content. (MD5\nsignature, or at least name & date). (b) any attempt to GET_PARTIAL\ntake the ID rather than the rest of the URL.\n\n \n\n\n\n"
        },
        {
            "subject": "Re: HTTP should be able to transfer part of a documen",
            "content": "Someone said:\n>> Of course, such number-based addressing is somewhat dangerous to use in a\n>> static URL,\n\nAnd Adrian Colley replied:\n> Well, you could protect it with if-modified-since.  If you want a link\n> which will get the index to any .zip file on a remote machine even after it\n> changes, then you have to decide where to put the zip-interpreting code.\n\nActually, you'd have to protect it with an if-not-modified-since. That\nis, you don't really want the piece you've identified if the whole\nthing has been changed.\n\n\n\n"
        },
        {
            "subject": "Re:  HTTP should be able to transfer part of a documen",
            "content": "I think this should only be a transport issue, not a document issue.\n\nThus I think it is inappropriate to put bytes ranges into the URI.\nIf you do put byte ranges in the URI, then you are trying to identify\npart of a resource; as Owen Rees said:\n> The retrieval of parts of resources is more a question of the naming of the\n> parts of a structured resource, and is not really an HTTP protocol issue. The\n> question of what constitutes a resource, and how you name resources that are\n> parts of collections is the sort of issue that crops up on the URI list, and\n> an issue that does not seem to have any completely general solutions.\n\nWhereas a transport protocol feature that allows partial document retrieval\nis not attempting to name parts of resources. It's up to the client to\ndecide whether the data it was sent makes any sense in the context of earlier\nretrievals. Obviously you would want the server to add object-body headers\nto help the client, such as unique resource ids, last-modified dates etc.\n\n David.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-digest-aa00.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : A Proposed Extension to HTTP :  Digest Access \n                   Authentication                                          \n       Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n                   A. Luotonen, E. Sink, L. Stewart\n       Filename  : draft-ietf-http-digest-aa-00.txt\n       Pages     : 6\n       Date      : 03/13/1995\n\nThe protocol referred to as \"HTTP/1.0\" includes specification for a Basic \nAccess Authentication scheme.  This scheme is not considered to be a secure\nmethod of user authentication, as the user name and password are passed \nover the network in an unencrypted form.  A specification for a new \nauthentication scheme is needed for future versions of the HTTP protocol.  \nThis document provides specification for such a scheme, referred to as \n\"Digest Access Authentication\".  The encryption method used is the RSA Data\nSecurity, Inc. MD5 Message-Digest Algorithm.                               \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-digest-aa-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-digest-aa-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.2)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-digest-aa-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950313105202.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-digest-aa-00.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-digest-aa-00.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950313105202.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Digest Access Authenticatio",
            "content": "After but five minutes of perusal, I have these comments:\n\n1) Section 2.1 says the headers must be on one line.  This contradicts\nthe current HTTP spec., which allows continuation if the following line\nbegins with whitespace.\n\n2) I would like to suggest an optional \"prompt\" attribute whose default\nvalue is the same as the \"realm\" attribute.  To my way of thinking, the\n\"realm\" is a short-hand name the server uses to identify the realm,\nwhereas \"prompt\" would be what the server would like the user to know\nabout the realm when s/he is prompted for name/password.\n\nNit:  last sentence of section 1.1:  \"... be included in the the proposed...\".\n(Double \"the\".)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Question about POST in HTTP/1.",
            "content": "Folks:\n\nThis probably is a dumb question, but here goes...\n\nThe HTTP/1.0 spec says that a POST can be used for:\na) allowing an object to be created on the server\nthat is subordinate to the URI in the request, or\nb) providing data for a data-handling process.\n\nMy question is, how does the client and server distinguish\none case from the other? The client may intend one\nmeaning but the server may take the other action.\n\nThanks for taking the time to respond.\n\n--Suresh\n\n\n\n"
        },
        {
            "subject": "Re: Question about POST in HTTP/1.",
            "content": "This is basically a question of how the client can know what will happen to\nthe data sent in a request. As HTTP is stateless there is no way the client\ncan get apriori knowledge of the result or what the server expects. This is\nuniquely up to the server to decide.\n\nIt's a bit the same situation the other way round - the server doesn't know\nwhat will happen to the data sent in a response. This might be of importance\nin a collaborative work between a group of people with version control.\n\nHowever, the client does know from the result of the request - it will (if\neverything goes well) receive either a 200 (nothing created) or a 201 (created)\n\n-- cheers --\n\nHenrik Frystyk\n\n> The HTTP/1.0 spec says that a POST can be used for:\n> a) allowing an object to be created on the server\n> that is subordinate to the URI in the request, or\n> b) providing data for a data-handling process.\n> \n> My question is, how does the client and server distinguish\n> one case from the other? The client may intend one\n> meaning but the server may take the other action.\n\n\n\n"
        },
        {
            "subject": "Clarification on language tag usage in latest draf",
            "content": "I just read the new RFC 1766 on language tags and found something in\nit that seems to clash with the intended usage in HTTP.  Section 2.1\nof RFC 1766 contains the following:\n\n   Applications should always treat language tags as a single token; the\n   division into main tag and subtags is an administrative mechanism,\n   not a navigation aid.\n\nI take that to mean that the tags are not hierarchical, i.e. that \"en\"\nis not to be understood as a superset of \"en-US\".  This may be fine for\nContent-Language, but will not work, IMHO, for Accept-Language.\n\nMy interpretation is that asking for \"en-US\" will NOT get you an \"en\"\ndocument if \"en-US\" is not available.  That may be OK.\n\nLikewise, asking for \"en\" will NOT get you \"en-US\", \"en-UK\" or any\nother \"en-SOMETHING\".  This seems to me to be unnacceptable.  It means\nthat the naive user would have to be aware of all variants of \"en-*\"\nin existence in order to construct the simple request \"Send anything\nin English\".  Same for any other language that has variants.\n\nHere is a suggestion to fix this.  Simply add the following to section\n8.2 of the HTTP draft, just before the Note:\n\n  In the context of the Accept-Language header (section 5.4.4) a\n  language tag is not to be interpreted as a single token, as per RFC\n  1766, but as a hierarchy.  A server should consider that it has a\n  match when a language tag received in an Accept-Language header\n  matches the initial portion of the language tag of a document.  An\n  exact match should be preferred.  This interpretation allows a\n  browser to send, for example:\n\n    Accept-Language: en-US, en\n\n  when the intent is to access, in order of preference, documents in\n  American-English (\"en-US\"), 'plain' or 'international' English\n  (\"en\"), and any other variant of English (initial \"en-\").\n\nI think the above is preferable to changing RFC 1766 and hacking up a\nscheme such as \"en-*\", and is still flexible enough.  Any comments?\n\n-- \nFran?ois Yergeau <yergeau@alis.ca>\nAlis Technologies Inc., Montr?al\n+1 (514) 738-9171\n+1 (514) 342-0318\n\n\n\n"
        },
        {
            "subject": "Clarification on language tag usage in latest draft: take ",
            "content": "[ The list server made a mess of a previous try at sending this ]\n[ message (it does not seem to like MIME headers, or too much). ]\n[ Hopefully this will be clearer. ]\n\nI just read the new RFC 1766 on language tags and found something in\nit that seems to clash with the intended usage in HTTP.  Section 2.1\nof RFC 1766 contains the following:\n\n   Applications should always treat language tags as a single token; the\n   division into main tag and subtags is an administrative mechanism,\n   not a navigation aid.\n\nI take this to mean that the tags are not hierarchical, i.e. that \"en\"\nis not to be understood as a superset of \"en-US\".  This may be fine for\nContent-Language, but will not work, IMHO, for Accept-Language.\n\nMy interpretation is that asking for \"en-US\" will NOT get you an \"en\"\ndocument if \"en-US\" is not available.  That may be OK.\n\nLikewise, asking for \"en\" will NOT get you \"en-US\", \"en-UK\" or any\nother \"en-SOMETHING\".  This seems to me to be unnacceptable.  It means\nthat the naive user would have to be aware of all variants of \"en-*\"\nin existence in order to construct the simple request \"Send anything\nin English\".  Same for any other language that has variants.\n\nHere is a suggestion to fix this.  Simply add the following to section\n8.2 of the HTTP draft, just before the Note:\n\n  In the context of the Accept-Language header (section 5.4.4) a\n  language tag is not to be interpreted as a single token, as per RFC\n  1766, but as a hierarchy.  A server should consider that it has a\n  match when a language tag received in an Accept-Language header\n  matches the initial portion of the language tag of a document.  An\n  exact match should be preferred.  This interpretation allows a\n  browser to send, for example:\n\n    Accept-Language: en-US, en\n\n  when the intent is to access, in order of preference, documents in\n  American-English (\"en-US\"), 'plain' or 'international' English\n  (\"en\"), and any other variant of English (initial \"en-\").\n\nI think the above is preferable to changing RFC 1766 and hacking up a\nscheme such as \"en-*\", and is still flexible enough.  Any comments?\n\n-- \nFrancois Yergeau <yergeau@alis.ca>\nAlis Technologies Inc., Montreal\n+1 (514) 738-9171\n+1 (514) 342-0318\n\n\n\n-- \nFran?ois Yergeau <yergeau@alis.ca>       | Qui se fait brebis le loup\n                                         | le mange.\n\n\n\n"
        },
        {
            "subject": "Re: Clarification on language tag usage in latest draf",
            "content": "> I just read the new RFC 1766 on language tags and found something in\n> it that seems to clash with the intended usage in HTTP.  Section 2.1\n> of RFC 1766 contains the following:\n> \n>    Applications should always treat language tags as a single token; the\n>    division into main tag and subtags is an administrative mechanism,\n>    not a navigation aid.\n> \n> I take that to mean that the tags are not hierarchical, i.e. that \"en\"\n> is not to be understood as a superset of \"en-US\".  This may be fine for\n> Content-Language, but will not work, IMHO, for Accept-Language.\n> \n> My interpretation is that asking for \"en-US\" will NOT get you an \"en\"\n> document if \"en-US\" is not available.  That may be OK.\n> \n> Likewise, asking for \"en\" will NOT get you \"en-US\", \"en-UK\" or any\n> other \"en-SOMETHING\".  This seems to me to be unnacceptable.  It means\n> that the naive user would have to be aware of all variants of \"en-*\"\n> in existence in order to construct the simple request \"Send anything\n> in English\".  Same for any other language that has variants.\n\nYes, this also struck me as a bit odd.  Fortunately, I had a note from\nthe author that essentially said \"do the right thing -- treat it as\nhierarchical\".  I have no idea what was behind that section of RFC 1766.\nI just ignored it for the HTTP spec, but I guess an explicit statement\nwould be preferable.\n\n> Here is a suggestion to fix this.  Simply add the following to section\n> 8.2 of the HTTP draft, just before the Note:\n> \n>   In the context of the Accept-Language header (section 5.4.4) a\n>   language tag is not to be interpreted as a single token, as per RFC\n>   1766, but as a hierarchy.  A server should consider that it has a\n>   match when a language tag received in an Accept-Language header\n>   matches the initial portion of the language tag of a document.  An\n>   exact match should be preferred.  This interpretation allows a\n>   browser to send, for example:\n> \n>     Accept-Language: en-US, en\n> \n>   when the intent is to access, in order of preference, documents in\n>   American-English (\"en-US\"), 'plain' or 'international' English\n>   (\"en\"), and any other variant of English (initial \"en-\").\n> \n> I think the above is preferable to changing RFC 1766 and hacking up a\n> scheme such as \"en-*\", and is still flexible enough.  Any comments?\n\nYes, that will do quite nicely -- I'll add it to the next revision\nif there are no strong objections.\n\n\n......Roy Fielding   ICS Grad Student, University of California, Irvine  USA\n                                     <fielding@ics.uci.edu>\n                     <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "another Digest Access Authentication questio",
            "content": "[Is anyone paying attention to my other questions?]\n\nThe client's Authorization: header contains\nuri=\"<requested-uri>\"\n\nI read the paragraphs about how the server checks the validity of the\ninformation to mean that the server uses the header's \"uri\" value when\nit calculates A2.  Should the server ever compare the \"uri\" value\nagainst the URI it actually received as part of the request, too?  If\nso, does it matter whether that comparison comes before or after the\nvarious MD5 checks?  (I assume if there's a mismatch, the request is\nrejected.  True?)\n\nAnother, minor nuisance, question:  should the MD5 digest function be\nrequired to produce hex with all lower (or upper) case letters?  It's\neasy to check stuff caselessly, but it's a little less efficient.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Clarification on language tag usage in latest draft: take ",
            "content": "Francois writes:\n\n> [ The list server made a mess of a previous try at sending this ]\n> [ message (it does not seem to like MIME headers, or too much). ]\n> [ Hopefully this will be clearer. ]\n\nYep, the http-wg list server is just chewing-gum and sealing wax wrapped\naround sendmail.  For now I'd advise keeping the input non-MIMEd.\n\n-- ange -- <><\n\nange@hplb.hpl.hp.com\n\n\n\n"
        },
        {
            "subject": "Which usual media types can a server sen",
            "content": "In section 5.4.1 of draft-ietf-http-v10-spec-00, it says that \"*/*\" means all \nmedia types but then goes on to imply that this excludes \"unusual\" media \ntypes. It then goes on to suggest that the definition of unusual should be a \nconfigurable aspect of the user agent.\n\nSince the server can have no way of knowing what the client/user agent has \nbeen configured to consider \"unusual\", it is forced to treat \"*/*\" not as \n\"all\" but as \"don't know\". The server must therefore use its own definition of \n\"unusual\" in determining whether or not a media type is acceptable. This seems \nto be undermining the whole purpose of the Accept header.\n\nHaving \"*/*\" really mean \"all\", and this being the default, does create the \nproblem that a server can send an x-perimental/bizzare entity without a \ncontent-length on the grounds that it is self-describing. If the server keeps \nthe connection open waiting for the next request, and the client does not \nrecognise that it has all of the entity then we have a deadlock.\n\nI do not like having */* potentially mean different things to each individual \nclient and server. Here are some other options to consider:\n\n1) */* does not mean all, it means some specified set of media types\n2) */* is not the default; some specified set of media types is the default\n3) */* is the default and really means all - live with the possible deadlock\n\nI am not particularly happy with any of these, but I think that the \nuncertainty about the meaning of */* is worse.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "FullURI, agai",
            "content": "I am now updating my list of features for known, supported HTTP servers,\nand many are adding or will soon add the ability to select actions based on\nvarious request headers. An obvious kind of action many folks like to see\nwould be to change the virtual root based on part or all of the full URI,\nif such information was available.\n\nThe last iteration of this discussion had Roy saying he didn't feel it was\nneeded unless clients built it in first, and the host name was only needed\nfor \"vanity domains\". A few people responded with arguments that knowing\nthe full URI had other benefits, many of which I believe to be significant.\nRoy didn't respond to those (unless I missed his response in the archive),\nno one  objected to Chuck's statement that it was a simple addition and\nlet's get on with it, but Full-URI didn't appear in the -00 rev of the\nspec.\n\nI'd like to make those arguments again and lobby for the simple inclusion\nof this optional header. The few words it will take in the spec will help\nprevent something that almost all of us would probably want: a reduction in\nbroken URL links.\n\nHere are two scenarios that weren't addressed earlier. Both involve the\npossibility of broken links, something that I think we in the HTTP WG\nshould do our part in preventing.\n\nScenario 1) The departments at www.bigstate.edu differentiate their Web\nhierarchies with the top-level directory names like /cs and /math.\nProfessors have their own directories under the respective top-levels. Many\npeople have URLs that point into professors hierarchies, not just at the\ntop level for the department. The computer science department decides to\nmove their hieararchy to their own machine with a different IP address and\nits own domain name, cs.bigstate.edu. Unless they want all the links to\nbreak, they must put forwarding pointers at every place in the old\nhierarchy where someone might have make a bookmark. With a Full-URI header\nand a smart server, the server could catch all items pointing to the\ntop-level /cs directory and do something smart (redirect to cs.bigstate.edu\nand strip off the /cs) or an adequate thing (give a recorded message\npointing to the root of cs.bigstate.edu).\n\nScenario 2) A company starts a Web site on its own host system,\nwww.tinyco.com. It becomes financially unfeasable to keep its Internet\nconnection, and wants to move its Web pages to a Web service provider,\nwww.websrus.com, who will put it under its current hierarchy. This will\nprevent all the published links to various parts of www.tinyco.com from\nbreaking. However, the Web provider uses the current practice of having a\ntop-level directory with an identifying name for each customer, such as\n/tinyco. If www.tinyco.com simply redirects its domain name to the new\nprovider's IP address, all the links will break because they don't have the\nrequired /tinyco lead-in directory name. On the other hand, if the\nwww.websrus.com server could see the incoming \"www.tinyco.com\", it could\nslap on the proper root directory name before processing the request and\nthe requests would be fulfilled.\n\nNote that some of the HTTP servers that are supporting different actions\nbased on headers are also supporting server-side includes. Thus, the Web\nadministrator could not only redirect a whole hierarchy to its proper new\nhome, he/she could add a header that says something like:\nThe location of the page<br>\nhttp://www.bigstate.edu/cs/stein/cs-108-syllabus<br>\nhas moved to a new home:<br>\nhttp://cs.bigstate.edu/stein/cs-108-syllabus.<br>\n<b>Please make a note of the new location.</b> Here's that page:<hr>\n\nThus, I propose the following be added to the spec:\n\n====================\nSection 5.4: No change needs to be made to the introductory paragraph since\nit already says that the headers \"allow the client to pass additional\ninformation about the request\" and are \"optional\".\n\nAdd \"| Full-URI\" on the line after \"| From\".\n\nNew Section 5.4.6:\n\nThe Full-URI header field can be used to indicate the URI requested by the\nuser. The inclusion of this field can aid HTTP server software receiving\nthe request to respond in a manner that gives the user what they probably\nintended to get, or at least give the server information about the URI for\nlogging and later analysis.\n\nFrom-URI = \"From-URI\" \":\" URI-string\n\nThe URI-string is the URI specified by the user after any encoding was\napplied by the client software. Although it is not required, user agents\nshould always include this field with requests in order to aid server\nsoftware that has been enabled to read the field.\n\nAn example is:\n\nFrom-URI: http://www.bigstate.edu/cs/stein/cs-108-syllabus\n====================\n\nI'm open to additions to this. I couldn't think of any security\nimplications, but I'm open to those as well.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "More KeyedDigest... Re: another Digest Access Authentication questio",
            "content": ">I read the paragraphs about how the server checks the validity of the\n>information to mean that the server uses the header's \"uri\" value when\n>it calculates A2.  Should the server ever compare the \"uri\" value\n>against the URI it actually received as part of the request, too?  If\n>so, does it matter whether that comparison comes before or after the\n>various MD5 checks?  (I assume if there's a mismatch, the request is\n>rejected.  True?)\n\nThe order of performing checks is undefined. This is essential since in \nthe ideal case the various parts of the header would be processed in\nparallel \n\nThe specified URI and the digested URI need not be the same, merely \n`equivalent'. This is a hard concept to define though. \n\nThe URI specified to the server may potentially be very different from the \ninitial URI. The server may have caused multiple redirects en route. The initial \nURI may have been a URN and been resolved through a proxy resolver. Provided the \nserver is satisfied that the names are equivalent it can release the page.\n\n\nThere is a serious definciency in the security of a system using KeyedDigest \nwithout encryption. It involves use of a trojan page.\n\nAlice requests a page of text from Bob, because Bob is a suspicious so and so\nBob requires he to identify herself.\n\nAlice requests a page of text from Mallet, this has a link to Alice's email\nstored on Bobs server hiden under the title \"Secrets\".\n\nAlice follows the `secrets' link, the client automatically reusung the same \npassword information as before and is suprised to discover her own email.\n\nMallet who has been tapping the link sells the information to one of the less \nscrupulous chat shows.\n\n\nThe problem is that anchors are not validated and suposedly confidential \ninformation is passed en-clair. A more significant problem arises if the \nKeyedDigest scheme is seen as an authentication service for payments.\n\nThere might be some milage in specifying the conditions under which a link \nshould be followed automatically and under which it should not. In particular:\n\n\n1) A link from an unsecured domain to a secured domain must always result in a \nrequest for authority.\n\n2) A link from a secured domain to a different secured domain should result in a \nrequest for authority in the first instance and may result in a request for \nsubsequent instances.\n\n3) A link from a secured domain to itself may result in a request for authority.\n\n\nIn case (2) the request for authority should provide an option to grant \nauthority for all future operations of like type.\n\nAlternatively a client may permit these options to be overidden in a \nconfiguration language.\n\nThe word Domain is used instead of server. This is beacuse a single server may \nhave multiple areas with different securities. Mallet and Alice may be on the \nsame machine (In fact they are, Alice, Bob and Mallet all have accounts on\nwit.w3.org :-).\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "proposal for a new html ta",
            "content": "Name of the tag : <LP> and its </LP> counterpart\nMnemonics       : Lower Priority\nWhat does it do : This new tag is an attempt at giving the author of an html \ndocument the possibilty to give a relative priority to different parts \nof his document.\n\nWhy it is needed:\nIt often happens that someone wants to view \nhttp://www.foo.bar/obnoxiouslylongdocument.html,\nor wants to view a document of a shorter size, but has a very slow connection.\nWhat usually happens is that the person accessing the document reads the first \nscreenful of text, and bases his decision to wait for the rest of the \ndocument, or to break the transfer all together on that first screenfull.\nSo the writer of a long html-document, or a document with a lot of images has \nto make the first screenfull attractive enough in order to have his entire \ndocument read.\nThis is in sharp contrast with the idea of structured documents.\n\nMy proposed solution to this problem is the following:\nOrder the different parts of the document according to their relative \nimportance, with the use of the proposed <LP> tag.\nThis tag lowers the priority of the enclosed block, and can be nested, to \nlower the priority of some parts of the document even more.\n\nFor example:\nHere is a document describing a better mousetrap, and I, as the author, think \nthat the reader should at least have read all my headers and the abstract.\nThe text between the headers is considered less important.\nAnd as formal proofs are so impopular, the proof that my mousetrap really \nworks has an even lower priority. \n\n<HTML><HEAD><TITLE>Mousetrap</TITLE></HEAD>\n<BODY><H1>My better mousetrap</H1>\n<BLOCKQUOTE>\n<H2>Abstract</H2>\nThis is a proposal for a better mousetrap\n</BLOCKQUOTE>\n<H2>Introduction</H2>\n<LP>                 <!-- HERE the priority is lowered-->\nThis is an idea that\nblahblahblahblah\n</LP>                <!-- HERE the priority is raised again-->\n<H2>How it works</H2>\n<LP>                 <!-- HERE the priority is lowered-->\nIt works like this....\n<LP>                 <!-- HERE the priority is lowered even more-->\n<H3>The formal proof</H3>\nAnd this is the formal proof that it really works:\n....\n</LP>                <!-- HERE the priority is raised to the level of -1 -->\nLo and behold, it works.\n</LP>                <!-- HERE the priority is raised to the begin level-->\n<ADDDRESS><A HREF=\"mailto:psiegma@cs.vu.nl\">Paul Siegmann</A></ADDRESS>\n</BODY></HTML>\n\nNow the viewer, or the person viewing the document could have several options:\n - Fast network connection:\nIgnore the priorities.\n - Medium network connection/curious person\nFirst load all the document parts with the highest priority, and \nautomatically continue loading the rest of the document in order of \npriority.\n - Slow network/modem connection:\nOnly show the parts with the highest priority, and only show parts \nwith a lower priority if they are clicked on (like with delayed image \nloading)\n\nAll comments and suggestions welcome.\n\nPaul Siegmann(psiegma@cs.vu.nl)\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "On Mon, 20 Mar 1995, Siegmann P wrote:\n\n> Name of the tag : <LP> and its </LP> counterpart\n> Mnemonics       : Lower Priority\n> What does it do : This new tag is an attempt at giving the author of an html \n> document the possibilty to give a relative priority to different parts \n> of his document.\n> \n> Why it is needed:\n> It often happens that someone wants to view \n> http://www.foo.bar/obnoxiouslylongdocument.html,\n> or wants to view a document of a shorter size, but has a very slow connection.\n> What usually happens is that the person accessing the document reads the first \n> screenful of text, and bases his decision to wait for the rest of the \n> document, or to break the transfer all together on that first screenfull.\n> So the writer of a long html-document, or a document with a lot of images has \n> to make the first screenfull attractive enough in order to have his entire \n> document read.\n> This is in sharp contrast with the idea of structured documents.\n> \n> My proposed solution to this problem is the following:\n> Order the different parts of the document according to their relative \n> importance, with the use of the proposed <LP> tag.\n> This tag lowers the priority of the enclosed block, and can be nested, to \n> lower the priority of some parts of the document even more.\n> \n> For example:\n> Here is a document describing a better mousetrap, and I, as the author, think \n> that the reader should at least have read all my headers and the abstract.\n> The text between the headers is considered less important.\n> And as formal proofs are so impopular, the proof that my mousetrap really \n> works has an even lower priority. \n> \n> <HTML><HEAD><TITLE>Mousetrap</TITLE></HEAD>\n> <BODY><H1>My better mousetrap</H1>\n> <BLOCKQUOTE>\n> <H2>Abstract</H2>\n> This is a proposal for a better mousetrap\n> </BLOCKQUOTE>\n> <H2>Introduction</H2>\n> <LP>                 <!-- HERE the priority is lowered-->\n> This is an idea that\n> blahblahblahblah\n> </LP>                <!-- HERE the priority is raised again-->\n> <H2>How it works</H2>\n> <LP>                 <!-- HERE the priority is lowered-->\n> It works like this....\n> <LP>                 <!-- HERE the priority is lowered even more-->\n> <H3>The formal proof</H3>\n> And this is the formal proof that it really works:\n> ....\n> </LP>                <!-- HERE the priority is raised to the level of -1 -->\n> Lo and behold, it works.\n> </LP>                <!-- HERE the priority is raised to the begin level-->\n> <ADDDRESS><A HREF=\"mailto:psiegma@cs.vu.nl\">Paul Siegmann</A></ADDRESS>\n> </BODY></HTML>\n> \n> Now the viewer, or the person viewing the document could have several options:\n>  - Fast network connection:\n> Ignore the priorities.\n>  - Medium network connection/curious person\n> First load all the document parts with the highest priority, and \n> automatically continue loading the rest of the document in order of \n> priority.\n>  - Slow network/modem connection:\n> Only show the parts with the highest priority, and only show parts \n> with a lower priority if they are clicked on (like with delayed image \n> loading)\n> \n> All comments and suggestions welcome.\n\nPaul...\n\nI suspect that this message would be best dealt with on the HTML mailing \nlist - anyone have the address handy ?\n\nAs regards your tag, I would have my doubts about the partition of documents \nbased on the speed of network connections --- I reckon that the encoding \nof a document should be as independent as possible of its storage and its \ntransmission. Maybe if you had some other rationale for the \npartitioning of documents....\n\nBesides, I think authoring is such a problem in W3 already, that \nallowing 3 sub-documents within one document is only going \nto mean messier documents and much increased workloads for document \nauthors and maintainers.\n\nWhat about a special summary tag ?\nDoesn't the latest version of HTML have provision for this sort of thing ?\n\n\nLiam\n--\n Liam Relihan,                                        Voice: +353-61-202713\n CSIS, Schumann Building,        [space]                Fax: +353-61-330876\n University Of Limerick,                             E-mail: relihanl@ul.ie\n Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html\n\n\n\n"
        },
        {
            "subject": "Re: FullURI, agai",
            "content": "> The last iteration of this discussion had Roy saying he didn't feel it was\n> needed unless clients built it in first, and the host name was only needed\n> for \"vanity domains\". A few people responded with arguments that knowing\n> the full URI had other benefits, many of which I believe to be significant.\n> Roy didn't respond to those (unless I missed his response in the archive),\n\nYes, you missed it.\n\n> no one  objected to Chuck's statement that it was a simple addition and\n> let's get on with it, but Full-URI didn't appear in the -00 rev of the\n> spec.\n\nOf course not.  What was under discussion was a feature to add to HTTP/1.1.\n<http://www.ics.uci.edu/pub/ietf/http/hypermail/current/0089.html>\nNote, however, that there was no consensus on the name or the contents\nof the header.\n\nI'd like to strongly encourage everyone to pore through the HTTP/1.0\nspecification FIRST and send in their comments (if any) before getting\ninto a discussion about 1.1 features.  Sometime this week, Henrik and I\nwill produce a draft for HTTP/1.1 which will be a basis for discussion\nat the IETF meeting and (hopefully) will allow us to focus some of the\ndiscussions on this mailing list.  It will include an Orig-URI: request\nheader.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "> Name of the tag : <LP> and its </LP> counterpart\n> Mnemonics       : Lower Priority\n> What does it do : This new tag is an attempt at giving the author of an html \n> document the possibilty to give a relative priority to different parts \n> of his document.\n\nSince many browsers display parts of a document in the order they\narrive over the net, implementing faster display of one part\nof the document might require messing with HTTP transport based\nin the HTML markup, to send subsections in a new order (ick).\n\nIn addition this might require both the client and server to\npre-parse the document for this tag before display and transport\nrespectively.\n\nThe overhead involved in departing from a simple linear processing\norder could produce an actual performance loss.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "Liam Relihan <relihanl@ul.ie> writes:\n> I suspect that this message would be best dealt with on the HTML mailing \n> list - anyone have the address handy ?\n \nDiscussion of this sort of thing belongs on <www-html@www10.w3.org>, rather \nthan the IETF HTML WG list <html-wg@oclc.org>.\n\n> As regards your tag, I would have my doubts about the partition of documents \n> based on the speed of network connections --- I reckon that the encoding \n> of a document should be as independent as possible of its storage and its \n> transmission. Maybe if you had some other rationale for the \n> partitioning of documents....\n\nThe idea looks like Ted Nelson's \"Stretch Text\" which goes back to 1975! See \n<URL:http://fire.clarkson.edu/deuelpm/Stretch/intro.html> for ideas on \nverbosity level based on reader interest.\n\nThere is an interesting question if resources can be delivered at different \nverbosity levels. Is this an issue of naming best handled in the URI, or is is \na matter for the protocol? Note the similarity to the question of delivering \npartial documents. Suppose I have retrieved something at verbosity 50% and I \nwant to increase this to 70%. Can I retrieve the difference and merge it, and \nwhat impact would this have on the names, the retrieval protocol, and the \nrepresentation of the resource?\n\nI think this is an interesting idea but I don't have time to pursue it at the \nmoment.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Why is MessageID header in HTTP",
            "content": "Message-ID seems to me to be out of place as an HTTP header. I think it should \nbe removed, and if there is any existing use problem that needs to be \ndescribed, do this in an appendix treating it as an extension-header.\n\nHere is the reasoning that prompted this; quoted text is from Section 4.3.3 of \ndraft-ietf-http-v10-spec-00.\n\n   The Message-ID field is normally not generated by HTTP applications \n   and is never required.\n\nThis seems to be suggesting that Message-ID does not really belong in HTTP.\n\n                          It should only be generated by a gateway \n   application when the message is being posted to some other protocol \n   that desires a Message-ID.\n\nThe word \"only\" is notorious for introducing ambiguity so I shall start with \nalternative wordings that I hope clarify the possible meanings.\n\n(1) \"It should be generated only by a gateway application, and only when the \nmessage is being posted to some other protocol that desires a Message-ID.\" If \nthis is the intended meaning then Message-ID exists only in the \"other \nprotocol\" and does not occur on the HTTP side of the gateway.\n\n(2) \"A gateway application should generate it only when the message is being \nposted to some other protocol that desires a Message-ID.\" For gateways the \nsituation is as for (1), but this wording leaves more scope for things that \nare not gateways to use Message-ID in HTTP.\n\nI assume that (2) is closer to the intended meaning, and that the idea is that \nnon-gateway applications that are aware both of the other protocol and that \nthey are using HTTP to interact with a gateway, will insert Message-ID headers.\n\n                              HTTP responses should only include a \n   Message-ID header field when the entity being transferred already \n   has one assigned to it (as in the case of resources that were \n   originally posted via Internet Mail or USENET).\n\n(Move \"only\" immediately before \"when\" to avoid the silly but possible \n\"include nothing but\" meaning.)\n\nThis seems to support the idea of an application that is aware both of the \nother protocol use of Message-ID and that the message is being delivered by a \ngateway.\n\nThis suggests that the other protocol message is being transmitted with its \nbody as the HTTP message entity, and at least one of its headers merged with \nthe HTTP headers. Why single out Message-ID for this special attention? In \ngeneral there are several headers that need to be passed on, depending which \nprotocol is involved. Receiving Mail or News in an HTTP response is going to \nbe difficult unless the From header is reclassified as a general header, and \nthe conflicting use of Date is resolved (it cannot be both the Date from the \noriginal message and the date/time at which the status line was generated).\n\nGiven these problems, sending and receiving message/* entities looks like a \nmore stable strategy for gateways to other protocols. It has the great benefit \nthat it does not try to push HTTP into being the union of all other protocols \nthat use RFC822 style headers.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Re: More KeyedDigest... Re: another Digest Access Authentication questio",
            "content": "[Has the \"Digest\" authentication scheme become \"KeyedDigest\"?]\n\nI previously asked (less succinctly):\n1) Should the server use the header's \"uri\" value when it calculates A2?\n2) Should the server compare the header's \"uri\" against the URI in the\nrequest line?\n3) If yes to (2), should the comparison occur before or after the various\nMD5 checks?\n\n\"Phillip M. Hallam-Baker\" <hallam@dxal18.cern.ch> responded:\n\n  > The order of performing checks is undefined. This is essential since in \n  > the ideal case the various parts of the header would be processed in\n  > parallel\nFair enough, but what are the answers to (1) and (2)?\n  > \n  > The specified URI and the digested URI need not be the same, merely \n  > `equivalent'. This is a hard concept to define though. \nBut if you can't define it, I can't implement it!  (\"You\" is meant to be\ngeneric, not an attack on Phill.)\n\n  > \n  > The URI specified to the server may potentially be very different from the \n  > initial URI. The server may have caused multiple redirects en route. The initial \n  > URI may have been a URN and been resolved through a proxy resolver. Provided the \n  > server is satisfied that the names are equivalent it can release the page.\n\nAll of which suggests the need for some kind of canonical agreed-to\nversion of a URL.  Otherwise, if there are N ways to describe the\ndocument, there are N(N-2)/2 comparisons to do, and the server may\nnot know all the mappings that produce all N versions.\n  > \n  > \n  > There is a serious definciency in the security of a system using KeyedDigest \n  > without encryption. It involves use of a trojan page.\n  > \n  > Alice requests a page of text from Bob, because Bob is a suspicious so and so\n  > Bob requires he to identify herself.\n  > \n  > Alice requests a page of text from Mallet, this has a link to Alice's email\n  > stored on Bobs server hiden under the title \"Secrets\".\n  > \n  > Alice follows the `secrets' link, the client automatically reusung the same \n  > password information as before and is suprised to discover her own email.\n  > \n  > Mallet who has been tapping the link sells the information to one of the less \n  > scrupulous chat shows.\n  > \n  > \n  > The problem is that anchors are not validated and suposedly confidential \n  > information is passed en-clair. A more significant problem arises if the \n  > KeyedDigest scheme is seen as an authentication service for payments.\n\nI think I understand the threat you describe, but I'm not sure I reach\nthe same conclusions.  First, Alice would certainly be entitled to read\nthe (mailbox) information.  The real problem is that sensitive\ninformation, as you say, is passed en-clair.  Note that Mallet is\nunable to get the information himself and must set up a Trojan page to\nget it.  Had Bob secured the information itself, Mallet would have\nachieved nothing.  Mallet had to tap the link, and we already know if\nhe can do that there's lots of mischief he can make.  So I don't see the\nproblem with unsecured anchors.  The [Keyed]Digest authentication\nsuccessfully required Alice to identify herself, as it was supposed to.\n  > \n  > There might be some milage in specifying the conditions under which a link \n  > should be followed automatically and under which it should not. In particular:\n  > \n  > \n  > 1) A link from an unsecured domain to a secured domain must always result in a \n  > request for authority.\n  > \n  > 2) A link from a secured domain to a different secured domain should result in a \n  > request for authority in the first instance and may result in a request for \n  > subsequent instances.\n  > \n  > 3) A link from a secured domain to itself may result in a request for authority.\nBut a client doesn't necessarily know a priori which domains are secure\nand which are not (particularly when going to a new domain).  And the\nserver doesn't know the predecessor link (if any!) that resulted in a\nrequest's reaching it.  And even if it did, it wouldn't know whether\nthat the predecessor document was in a secured domain if that document\ncame from a different server.  So who's going to make the determination\nthat a security transition occurred?\n  > \n  > \n  > In case (2) the request for authority should provide an option to grant \n  > authority for all future operations of like type.\n  > \n  > Alternatively a client may permit these options to be overidden in a \n  > configuration language.\n  > \n  > The word Domain is used instead of server. This is beacuse a single server may \n  > have multiple areas with different securities. Mallet and Alice may be on the \n  > same machine (In fact they are, Alice, Bob and Mallet all have accounts on\n  > wit.w3.org :-).\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: More KeyedDigest... Re: another Digest Access Authentication  questio",
            "content": ">1) Should the server use the header's \"uri\" value when it calculates A2?\nNo, the value used must be the one given in the authorisation line. The other\nvalue may get changed by intermediate proxies.\n\n>2) Should the server compare the header's \"uri\" against the URI in the\n>request line?\nYes.\n\n>3) If yes to (2), should the comparison occur before or after the various\n>MD5 checks?\nDosen't matter.\n\n\n>  > The specified URI and the digested URI need not be the same, merely \n>  > `equivalent'. This is a hard concept to define though. \n>But if you can't define it, I can't implement it!  (\"You\" is meant to be\n>generic, not an attack on Phill.)\n\nIt can't be defined for all sites in the RFC. An indvidual site should have no \ndifficulty determining what their policy is however. then it is a matter for \nserver designers to be smart and work out how to be very flexible.\n\n>All of which suggests the need for some kind of canonical agreed-to\n>version of a URL.  Otherwise, if there are N ways to describe the\n>document, there are N(N-2)/2 comparisons to do, and the server may\n>not know all the mappings that produce all N versions.\n\nNo, the server simply needs a set of translation rules for the URL. This should \nbe the complete set of translation rules which it has knowledge of. The URL is \nthen translated according to these rules and compared to the cannonical form of \nthe header URL.\n\nMASSIVE KLUDGE ==>> \nSounds difficult? Not really, very few people have translating proxies \nand those who do can probably perform the authentication at the first proxy and \nattach a secondary authentication. The sort of situations where this would be \nused would be a main server with an auilliary backup such as an intelligent \nsearch engine which is only occasionaly referred to.\n\nIn other words this is pretty much a theoretical issue at this moment. \nBut for a very limited number of people it is not. That is why the specification \nis the way it is.\n\nIts worth pointing out that in the original specification the URI was \nnot even present...\n\n>I think I understand the threat you describe, but I'm not sure I reach\n>the same conclusions.  First, Alice would certainly be entitled to read\n>the (mailbox) information.  The real problem is that sensitive\n>information, as you say, is passed en-clair. \n\nThis is true but unhelpfull...\n\nThe problem is not just that the information is passed en-clair. Mallet is able \nto bypass the authentication system so that he has greater access than that of a \npure passive lisner.\n\nPersonally I would prefer to encrypt the content using the shared secret \n(modified in some manner) and DES or IDEA. I did think of suggesting this at the \ntime but then of course we are into massive ITAR problems :-( But no patent\nproblems :-)\n\n \nWe should probably have a bridge note written since S-HTTP has lots of shared \nsecret mechanisms. Since we now have a shared secret we should employ it...\n\nThe simplest mode would be to take the shared secret key  [MD5 (password, \ndomain, username)] and XOR it with some random 128 bits. Then use the first 64 \nbits for the key and the other 64 bits for the IV of the cipher. (PKCS #5). The \nrandom bitstring is needed because one should attempt to limit the quantities of \nciphertext sent under the same key.\n\n\nPhill.\n\n\n\n"
        },
        {
            "subject": "Re: Why is MessageID header in HTTP",
            "content": ">Message-ID seems to me to be out of place as an HTTP header. I think it should \n>be removed, and if there is any existing use problem that needs to be \n>described, do this in an appendix treating it as an extension-header.\n\nI disagree. There are several instances when it is usefull to be able to specify \nthe message-id as a meta item. For example a HTTP server may be a news/mail \ngateway in its own right. A user with an HTTP client needs to be able to \ndetermine the Message Id used in certain circumstances. \n\nIf we have proxy chains there is a great advantage in being able to index and \nidentify messaged by an identifier.\n\nI have always thought that its a pity this field was not made mandatory. \nEspecialy for replies.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: FullURI, agai",
            "content": ">> Roy didn't respond to those (unless I missed his response in the archive),\n>\n>Yes, you missed it.\n\nAh.\n\n>Of course not.  What was under discussion was a feature to add to HTTP/1.1.\n><http://www.ics.uci.edu/pub/ietf/http/hypermail/current/0089.html>\n\nThanks for the pointer! It seems the threading on the mail archive is a bit\nfunky. The \"next in thread\" link from your message (0089) leads to Chuck's\nmessage (0072), which I got to from somewhere else. Unfortuantely, neither\nof the back-pointing links in 0072 points to 0089, so I never saw your\nmessage.\n\nFolks traversing the archives by thread, beware.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": ">It often happens that someone wants to view\n>http://www.foo.bar/obnoxiouslylongdocument.html,\n>or wants to view a document of a shorter size, but has a very slow\n>connection. \n\n3 comments:\n\n1) Intelligent servers should be able to break up long documents as\nnecessary (though lack of HTML structure makes it more\ndifficult). This is more a question of how to send the data, rather\nthan how to mark it up. DynaWeb does this with large SGML documents.\n\n2) Given <DIV> and the CLASS attribute, it seems quite possible to do\nmuch the same thing by hiding an element and all of it's children\nbased on some condition.\n\n3) Your proposal still requires the data to be sent to the client, or\nthe server to parse the data and send only the higher priority\nitems. You do not specify a way of stating a preference to the server,\nand besides, if you parse the document, you should also be able to\nbreak it into pieces, bringing us back to comment 1.\n\n\n\n"
        },
        {
            "subject": "Re: More KeyedDigest... Re: another Digest Access Authentication  questio",
            "content": "\"Phillip M. Hallam-Baker\" <hallam@dxal18.cern.ch> writes:\n> The problem is not just that the information is passed en-clair. Mallet\n> is able to bypass the authentication system so that he has greater\n> access than that of a pure passive lisner.\n\nMallet may have been able to fool Alice into accessing her email on Bob's \nserver, thus making it visible to anyone watching the traffic between Alice \nand Bob, but it is not clear that Mallet can do more than this unless there is \nsome problem hidden in the words \"uri sans proxy/routing\" which lets Mallet \nfool Alice into using him as a proxy to Bob.\n\nTo put himself in the access path, the problem for Mallet is to construct a \nURI which causes Alice to send the request to Mallet with a digest that \ncontains a requested URI that Bob will honour. It is not clear how Mallet can \ndo this unless Alice is happy to adopt an arbitrary proxy.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Q: HTTP/1.0 being &quot;...stateless, objec",
            "content": "Referring to the Internet Draft for HTTP/1.0 (March 8, 1995), could someone \nexplain to me what makes this protocol is _object-oriented_ as indicated in \nthe Abstract of this document?\nMany thanks for your comments.\n\nWouter\n --\n                   Wouter van Hulten    \n                   KPN - Koninklijke PTT Nederland / Royal PTT Nederland\nMail           KPN Research, Postbus 15000, 9700 CD Groningen, The \nNetherlands\nE-mail       W.M.vanHulten@research.ptt.nl\nPhone      +31-50-821043\nFax            +31-50-122415\n -----------\n\n\n\n"
        },
        {
            "subject": "Comments on HTTP/1.0 draft [March 8, 1995",
            "content": "Herewith comments on the latest draft, as requested.  Nothing too\nserious!\n\n3.1 HTTP Version, para starting \"HTTP servers are..\"\n\n  a. [nit] \"i.e.\" needs a following comma (possibly elsewhere also)\n\n  b. <major> should be <major+1> ?\n\n3.3.1 Full date\n\n  a. I haven't ever seen the asctime() format arrive at any of my\n     servers -- can this be dropped as a requirement?  RFC 850 does\n     still seem to be around, however.\n\n  b. \"recipients of date values should be robust in accepting...\"\n     -- this is really too vague to be implementable.  Make it a\n     non-normative note or (better) move to Appendix C.?\n\n4.3.3 Message-ID\n\n  a. I implemented this as per the previous draft and found it useful\n     for testing, but I agree that it should not normally be generated.\n     I now generate it only in a 'Test mode'.  However, the new text\n     forbids this.  Add this case as a possibility?\n\n5.2.3 POST\n\n  a. [nit] change \"usually a form\" to \"such as the result of submitting\n     a form\" (or something like that)?\n\n5.4.1 Accept\n\n  a. [aside] I applaud the change of emphasis here.\n\n6.1 Status-line\n\n  a. [nit] The last sentence (\"Although...\") should be a note?\n\n6.2.1 202 Accepted\n\n  a. \"Any method\" seems a bit strong .. doesn't seem very useful for GET\n     or HEAD.\n\n6.2.3 407 PAR\n\n  a. [nit] \"will be available in future versions\" puts a constraint on\n     the future (and future standards processes). Weaken?\n\n6.3.1 Public\n\n  a. Don't understand the \"applies only to the current connection\".\n     Since the request has already been received, and the response\n     connection is about to be closed, this implies that the information\n     must immediately be discarded, and is hence useless?\n\n6.3.2 Retry-After\n\n  a. [nit] change /an full/a full/\n\n7.1 Entity Header Fields, Note\n\n  a. \"It has been proposed..\" probably should be moved to HTTP/1.1?  In\n     particular, duplication of keywords in two separate address spaces\n     between two different layers of protocols (HTTP and HTML) is bound\n     to lead to problems in the future (standards will have to be\n     tightly bound and coordinated).\n\n  b. \"Base will be used...\" same comment as 6.2.3 above.\n\n7.1.1 Allow\n\n  a. [nit] A cross-reference back to Public (6.3.1) similar to the Allow\n     reference there, would be helpful.  (Or drop the earlier\n     cross-reference.)\n\n7.1.4 Content-Length\n\n  a. Add sentence to the effect that if Content-Length is not specified\n     on a request, and the server does not recognize or cannot calculate\n     the length from other fields, then 400 Bad Request may be returned.\n\n     [aside] I still would prefer that Content-Length be required on\n     requests with entity data, as it allows a too-large request to be\n     rejected before reading an excess of data first.  Perhaps for 1.1?\n\n7.1.8 Expires\n\n  a. [nit] The note could be moved to Appendix C.\n\n7.1.9 Last-Modified\n\n  a. [nit] Spell out 'last-mod'?\n\n7.1.11 Location\n\n  a. This (\"considered obsolete\") is inconsistent with 7.1.13 URI, which\n     encourages both.  It would perhaps be better to formalize Location\n     as a useful special-case of URI (especially as it is very much\n     common current practice).  Otherwise, servers must wastefully\n     generate both indefinitely (and clients have no incentive to\n     implement URI, perhaps).\n\n  b. What happens if both Location and URI are specified, but differ?\n     It would be better if one or other (only) were permitted?\n\nMike Cowlishaw\nIBM UK Labs.\n\n\n\n"
        },
        {
            "subject": "Re: Q: HTTP/1.0 being &quot;...stateless, objec",
            "content": "HTTP/1.0 is object oriented in that the semantics of the methods \napplicable to that URL are defined primarily by the URL (plus Tim was into \nobjective-c at the time). \n\n\nOn Tue, 21 Mar 1995, Hulten, W.M. van wrote:\n> \n> Referring to the Internet Draft for HTTP/1.0 (March 8, 1995), could someone \n> explain to me what makes this protocol is _object-oriented_ as indicated in \n\n\n\n"
        },
        {
            "subject": "v10-spec00 comment",
            "content": "Comments on draft-ietf-http-v10-spec-00. The comments on Accept, Message-ID \nand Version are more important than the others.\n\n3.2 Universal Resource Identifiers\n\nShould refer to RFC1738 as the standard for URLs including the http scheme, \nescaping rules and allowed characters.\n\n3.3.1 Full Date\n\nRFC 1123 (5.2.14) says:\n        \"There is a strong trend towards the use of numeric timezone\n         indicators, and implementations SHOULD use numeric timezones\n         instead of timezone names.  However, all implementations MUST\n         accept either notation.\"\n\"+0000\" should at least be permitted as an alternative to \"GMT\" in the \n\"updated by RFC 1123\" case.\n\n4.3.2 Forwarded\n\nThe final sentence about hiding internal hosts should say that existing \nForwarded headers (added by proxies inside the firewall) should be removed.\n\n4.3.3 Message-ID\n\n  The first paragraph specifies that this is a unique identifier for the \nmessage, presumably the HTTP request or response message. The final paragraph \nsays that an HTTP response should only include a Message-ID header if the \nentity has one. Since it is possible to retrieve a Mail/News message more than \nonce, the HTTP Message-ID cannot be the Message-ID of the enclosed entity as \nthis would violate the unique identification property for the HTTP response \nmessage.\n\nMy preferred solution is to remove Message-ID from HTTP altogether, but if it \nis retained it cannot be both a unique identifier for an HTTP message and \nimported from Mail/News/etc. by a gateway.\n\n4.3.4 MIME-Version\n\nI would have classified this as an entity-header rather than a general-header.\n\n5.4.1 Accept\n\nSee my recent note about \"*/*\" meaning \"not unusual\" where the server cannot \nknow how to interpret \"unusual\" because it is customisable by the user agent. \nEncouraging user-agent authors to give users control over media types is good; \nundermining the means by which this information is passed to the server is bad.\n\n7.1.1 Allow\n\nI would have classified this as a response-header rather than an \nentity-header; it is required with 405 Method Not Allowed and will not be \nmeta-information about the entity containing an explanation of the error.\n\n7.1.14 Version\n\n\"A user agent can request a particular version of an entity by including its \ntag in a Version header as part of the request.\" How should Version be \ninterpreted in a POST request? It could refer either to the version of the \nentity, or to the version of the resource to which the entity is to be made \nsubordinate. What is the existing practice in this case?\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Middleware Workshop(ACM SIGCOMM",
            "content": "( I apologize if you receive this announcement more than once;  \nthe deadline for position papers is extended to April 30, 1995 )\n\nAnnouncing a  NEW  feature at ACM SIGCOMM's annual Technical Symposium:\n\n      An Invited\nWORKSHOP on MIDDLEWARE\n  28-29  AUGUST 1995\n\n   immediately preceding  SIGCOMM95\n     30 August - September 1 1995\n\n  CAMBRIDGE, MA  USA\n\nIMPORTANT!!!  The deadline for a brief position paper is APRIL 30, 1995.\n\nWith the growth of the Internet and the corresponding agreement on a common\nnetwork and transport infrastructure comes a new opportunity. The consensus\non the use of the TCP/IP protocol suite, including protocols for routing,\naddressing, and forwarding of packets, end-to-end transport mechanisms, and\nstandard protocols for file transfer, remote login, electronic mail and\nnetwork management has resulted in a wide spread interoperable networking\ninfrastructure. This in turn is creating tremendous opportunities for new\nand innovative services to be provided over the network. An example of this\nis the rapid evolution of the World-Wide Web in the last several years.\n\nAt the same time, the explosive growth in availability and use of the\nInternet is creating new challenges. The existing infrastructure must be\nextended if it is to continue to scale in numbers of users, offer bandwidth\nguarantees, and support new classes of network applications. These trends\ndemand that we raise the level of common services and introduce new types\nof higher-level services. Interoperable information management, search, and\nretrieval mechanisms would create opportunities for new third-party\ninformation services. Interoperable payment mechanisms would spur the\nevolution of electronic commerce. Interoperable multimedia representation\nformats and exchange protocols would spur a new generation of group support\ntools. There are opportunities to create many such middleware components,\nincluding caching and replication services, indexing services, remote\nscripting environments, data typing and structuring primitives, and higher\nlevel communication abstractions such as multicast and causal broadcast.\n\nThe purpose of this workshop is to explore this area of middleware\ninfrastructure. A specific objective is to identify and discuss those areas\nof services that would sit \"above\" the traditional network protocols and\nprovide means for extending the the commonly available services on the\nnetwork to enclose higher layers of abstraction. A critical question to be\naddressed is the balance between standardization and the exploration of new\nconcepts for middleware services. Thus, identifying the appropriate level\nfor standardization and interoperability is expected to be a major topic of\ndiscussion at the workshop. We plan to discuss the vision for where\nmiddleware is heading. For example, what kind of middleware support is\nneeded for the integration of databases, information spaces (such as WWW),\nand personal communication tools. Finally, engineering issues in middleware\nimplementation will be of interest. Questions will be addressed such as how\nto engineer the interface between middleware and TCP and how to take\nadvantage of underneath IP multicast support in caching and replication\nservices.\n\nThe workshop will be organized based on invited participants. A workshop of\napproximately 75 people is anticipated. Brief (no more than three pages)\nposition papers are welcome to aid in the selection of workshop invitees.\nThese position papers are due no later than 30 April 1995. The workshop\nitself will last 1.5 days immediately preceding the main SIGCOMM'95\nconference.\n\nPosition papers should be sent by electronic mail in ascii to\n        SC95WS@mercury.lcs.mit.edu. \nQuestions should be addressed to the Workshop Program Chair, \n        Dr. Barry M. Leiner, BLeiner@arpa.mil.\n\nWorkshop Program Committee:\n        Ken Birman - Cornell                 David Gifford - MIT          \n        Barry Leiner - ARPA                  Larry Masinter - Xerox       \n        Michael Schwartz - U. Colorado       Karen Sollins - MIT\n        Jay Weber - EIT                      Lixia Zhang - Xerox\n\n\n\n"
        },
        {
            "subject": "Re: v10-spec00 comment",
            "content": "On Tue, 21 Mar 1995, Owen Rees wrote:\n> 5.4.1 Accept\n> \n> See my recent note about \"*/*\" meaning \"not unusual\" where the server \n> cannot know how to interpret \"unusual\" because it is customisable by \n> the user agent.  Encouraging user-agent authors to give users control \n> over media types is good; undermining the means by which this \n> information is passed to the server is bad.\n\nA better definition of \"unusual\" is needed.  I agreed with Owen's earlier \ncomments[1] and I agree with this one; I've whined and moaned about \nAccept before.  I think the offending text is in the last paragraph of 5.4.1:\n\"If no quality factors have been set by the user, and the context \nof the request is such that the user agent is capable of saving \nthe entity to a file if the received media type is unknown, then\nthe only appropriate value for Accept is '*/*' and the list of \nunusual types.  Whether or not a particular media type is \ndeemed 'unusual' should be a configurable aspect of the user agent.\"\n\nSome examples:\n\n* It is neither configurable nor unusual for Netscape 1.1 to \naccept tables -- every copy of 1.1 does, and if you don't like it, stick \nto 1.0 or another browser.  Do tables constitute an 'unusual' media type \nbecause not every server uses them?  If so, why is 'unusual' determined \non the browser's side?  Should Netscape 1.1 be sending 'Accept: */*; \nq=0.5, text/x-html-with-tables'?  If so, should other browsers be forced to \nsave text/x-html-with-tables to a file?\n\n* It is not currently unusual for Lynx 2.3.8 to accept HTML \nwithout tables or math.  A year from now (or five) when HTML 3.0 has been \nreleased, should Lynx 2.3.8 start considering its requests for HTML, by \nwhich it means HTML 2.0, 'unusual'?  If the user hasn't the sense to \nupgrade, will the user nonetheless be expected to run Lynx in \n'anachronistic mode'?\n\n* I prefer HTML to PostScript, and let's pretend my browser knows that.\nWhen downloading the HTML/1.0 draft to print out, however, I prefer \nPostScript to HTML.  Should I open my preferences, reset my quality \nvalues, make the request, and then return my quality values to their \nstandard settings?\n\nSome suggestions:\n\n* 'Unusual' should be defined more strictly.  One possibility \nwould be in reference to IANA registered media types.\n\n* As new browsers implement wider acceptance of _usual_ media types \n(i.e., HTML 3.0, image/jpeg, whatever), they should be encouraged to \nexplicitly list these media types in their accept headers.  This will \nallow server maintainers to avoid \"saving to file\" whenever possible \n(i.e., send gif unless they _say_ send jpeg).\n\n* The Content Negotiation section could remove its language about \nchoosing from several acceptable media types \"possibly at random,\" and \ninstead suggest that when different versions of an entity exist, the \ndefault should be the version most likely directly viewable by the oldest \nbrowsers (text/html, text/plain, image/gif).\n\n* The browser should be able to indicate that it will accept */*, \nbut if different versions exist, it specifically _wants_ a \"300 Multiple \nChoices\" response (if you have more than one version, let me see what \nyou have).\n\n* Similarly, browser authors should be encouraged to allow \nchanges to the Accept quality values on a per-request basis.\n\n* A browser should be allowed to specify a preference for certain \n\"usual\" media types without the user configuring it to do so!  The \nlanguage I quoted above doesn't seem to allow for that.  For instance, \nthe browser could check for the presence of external viewers to handle a \nvariety of media types, and list those media types as preferred to media \ntypes neither the browser nor any of the user's external viewers can \nhandle.  The browser would then ask for, in order, (1) types it can  \ndisplay directly, (2) types it has external viewers to display, and then \n(3) anything else (*/*).\n\nThe HTTP/1.0 draft approaches \"Accept\" at the literal level of \"What \n_can_ this browser somehow take?\"  Some of my suggestions above might be \nmore appropriate for a header named \"Prefer,\" but then, the only purpose \nof quality values is to express preferences between multiple choices.  \nMaybe a separate preferences header would make this easier.  I agree \nthat content negotiation should be structured to avoid \"406 None \nAcceptable\" whenever possible; but I would strongly prefer that Accept \nnot be saddled with strict limitations on its content (\"...the only \nappropriate value for Accept...\").  Mandating \"Accept: */*\" makes \ncontent negotiation more difficult.\n\nM. Hedlund <march@europa.com>\n\n-----\n[1] <URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/current/0130.html>\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "    Why it is needed:\n    It often happens that someone wants to view \n    http://www.foo.bar/obnoxiouslylongdocument.html,\n    or wants to view a document of a shorter size, but has a very slow\n    connection.  What usually happens is that the person accessing the\n    document reads the first screenful of text, and bases his decision\n    to wait for the rest of the document, or to break the transfer all\n    together on that first screenfull.  So the writer of a long\n    html-document, or a document with a lot of images has to make the\n    first screenfull attractive enough in order to have his entire\n    document read.  This is in sharp contrast with the idea of\n    structured documents.\n    \n    My proposed solution to this problem is the following:  Order the\n    different parts of the document according to their relative\n    importance, with the use of the proposed <LP> tag.  This tag lowers\n    the priority of the enclosed block, and can be nested, to lower the\n    priority of some parts of the document even more.\n\nI think you have identified an important problem, but I'm not so\nhappy with your proposed solution.  Other people have already pointed\nout some of the flaws.\n\nI'd like to propose a different approach.  This also may not be such\na good idea, so if other people don't like it, I'm not going to push\nit too hard.\n\nSuppose, as you did, that the author of the HTML file expects some\nusers will want to view parts of the document before the whole file\nis transmitted.  Instead of marking certain parts with priorities\nthat control the order in which they are transmitted, however, the\nauthor marks the parts with the order in which they should be displayed.\n\nThe HTTP server (and the client's HTTP layer) can therefore remain\nentirely unmodified.  The client's HTML parser, however, needs to\nbe able to display sub-parts out of order, inserting blank space\n(or some other placeholder) where necessary to indicate missing\npieces, and inserting these pieces as they arrive.\n\nAs the retrieval progresses, the user may see parts of the document\n\"expand\" on the screen, the same way that Netscape fills in images.\nOr perhaps the browser could change some visual attribute of the\nplaceholder (such as its color or texture), allowing the user to\nclick on the placeholder to see the newly-arrived sub-part.\n\nThe document could start with a (hidden) \"table of contents\" that shows\nthe relative sizes of the parts.  This would make it somewhat easier\nto pre-allocate screen space, although I imagine that in practice\nthe allocation would be inexact.\n\nOf course, the right \"solution\" might be to encourage authors\nto provide their large documents in two forms: monolithic (single\nHTML file) for people with fast connections and/or for people who\nwant to quickly search the documents for particular character\nstrings, and \"outlined\", for people who want to retrieve just\nthe parts they are looking for.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": ">Suppose, as you did, that the author of the HTML file expects some\n>users will want to view parts of the document before the whole file\n>is transmitted.  Instead of marking certain parts with priorities\n>that control the order in which they are transmitted, however, the\n>author marks the parts with the order in which they should be displayed.\n\nIsn't this, in effect, what an anchor tag does? While most browsers\nperform content replacement, this is not necessarily the only way. \n\n>Of course, the right \"solution\" might be to encourage authors\n>to provide their large documents in two forms: monolithic (single\n>HTML file) for people with fast connections and/or for people who\n>want to quickly search the documents for particular character\n>strings, and \"outlined\", for people who want to retrieve just\n>the parts they are looking for.\n \nRequiring the authors to think of how to break up a document\n(especially very large ones) is not a winning philosophy: it entails\ntoo much work.\n\nServers can, and should be able to do this, but HTML get's in the\nway. HTML is designed for smallish documents. I think servers like\nDynaWeb will become increasingly important. I know of one case where\nit took a week to publish something in HTML (after converting from\nSGML), but the same thing was accomplished in DynaWeb in minutes,\nwith an up one time cost of perhaps an hour or so. DynaWeb generates\nlinks on the fly, so link maintenance (a real headache for large\ndatabases) is an insignificant part of the publishing process.\n\nWhile I'm not keen on expounding the benefits of EBT's products on a\npublic list, I think DynaWeb represents a significant shift in\nemphasis, and technology that is useful for large databases. When I\nwrote DynaWeb, I (and others at EBT who shared in the design), had\nprecisely this problem in mind. I think we solved it fairly well, and\nI'd be very surprised if other servers do not appear that offer the\nsame functionality. It is the way things will go.\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "On Tue, 21 Mar 1995, Jeffrey Mogul wrote:\n\n> Of course, the right \"solution\" might be to encourage authors\n> to provide their large documents in two forms: monolithic (single\n> HTML file) for people with fast connections and/or for people who\n> want to quickly search the documents for particular character\n> strings, and \"outlined\", for people who want to retrieve just\n> the parts they are looking for.\n\nInstead of having a single 'file' with priorities, display orders, etc.,\nnone of which solve the basic problem that the decision basis content\nhasn't been transfered, it seems to me that some form of >include<\ncapability would allow a single logical document to be split for\nstorage, maintenance, transmission, etc.  Perhaps an inline \nattribute on an <A> or a new tag.  Prefered retrieval order could\naccomadate publisher hints.  Browsers would retrieve and render the\nbase file and then start filling in the includes much like images\nare handled today when the size isn't known.  By organizing the\ndocument and includes an overview of content could be quickly \navailable with more to follow.  Browsers could let users delay\nretrieval of includes just as images can be delayed today.  The structure\nprovided would also facilitate outline viewing ... much like many\nfolks skim a printed document by reading the chapter or section\nlead paragraphs.  Or perhaps the structure is achieved via <div>\nor whatever and thinking about the structure facilitates >include<\norganization.\n\nHTML and WWW norms tend to excessive fragmentation of information and\ntoo many hlinks to investigate just to guess which to follow.  There\nare many times when a complex subject (like the HTML specs) needs\nto be studied in a more linear sequence than can be achieved yet\nwith current markup the author is discouraged from providing a well\norganized linear document.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "> Instead of having a single 'file' with priorities, display orders, etc.,\n> none of which solve the basic problem that the decision basis content\n> hasn't been transfered, it seems to me that some form of >include<\n> capability would allow a single logical document to be split for\n> storage, maintenance, transmission, etc. \n\nAn \"include\" capability could provide other benefits. For instance,\nonly the parts of a document that the user is allowed to see can be\nreturned, and reconstituted as a single logical view. This would allow\nsites that currently support both an internal and an external home\npage to have just one home page.\n\nI thought I stumbled across a server once that was supporting an\ninclude capability, but I no longer remember what it was. Anyone else\nhave a pointer?\n\nMez\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "> I thought I stumbled across a server once that was supporting an\n> include capability, but I no longer remember what it was. Anyone else\n> have a pointer?\n> \n> Mez\n\n\nYou're thinking of the <!--#include file=\"foobar.html\" --> server side\ninclude facility provided by NCSA httpd 1.3+ and other servers.  Take a\nlook at the NCSA on-line documentation.\n\nCheers,\nAy.\n\n     Andrew Wilson     URL: http://www.cm.cf.ac.uk/User/Andrew.Wilson/\nElsevier Science, Oxford   Office: +44 0865 843155     Mobile: +44 0589 616144\n\n\n\n"
        },
        {
            "subject": "still more Digest Authentication comment",
            "content": "Earlier I raised a concern about MD5 and the case (upper or lower) used\nfor the hex output.  There's a more fundamental problem.  I believe MD5\ndefines how to get a 128 bit digest of a message.  WE have to define\nhow such a string of bits gets converted to ASCII characters:  byte\n(and nibble) order, prefixes (0x?) or suffixes, and case.  \"It;s\nobvious is an unacceptable answer when you're defining\ninter-operability standards.\n\nThe case problem (is it 789ABCDEF or 789abcdef?) is important because\ndigests of A1 and A2 themselves get digested in <digest>.  Clearly a\ndigest of 789ABCDEF is different from a digest of 789abcdef.\n\nHere are some proposed words:\n\n1.3, MD5 Digest\n\nFor the purposes of this Standard, an MD5 digest of 128 bits is\nrepresented as 32 ASCII printable characters.  The bits in the 128 bit\ndigest are converted from most significant to least significant bit,\nfour bits at a time to their ASCII presentation as follows.  Each four\nbits is represented by its familiar hexadecimal notation from the\ncharacters 0123456789abcdef.  That is binary 0000 gets represented by\nthe character '0', 0001, by '1', and so on up to the representation of\n1111 as 'f'.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Digest Access Authentication proposal  &quot;Authorization&quot",
            "content": "It seems that from the start of WWW security, the terms authentication\nand authorization have been used in ways that obfuscate the difference\nbetween them. The DAA proposal carries on that grand tradition, but\nI'd like to suggest that we get some clarity in the terms, and\npropagate it. Authentication is the process of proving\nidentity. Authorization is the process of deciding if a request will\nbe honored. Authorization often uses authentication information\n(proven identity). Authorization also uses other information, such as\nuser attributes, object attributes, contextual information, and, most\nfamiliarly, a database associating requests and objects with user\nidentities (or user attributes). That's files like .htaccess.\n\nAs far as I can see, the item called the \"Authorization\" header in the\nDAA proposal just contains authentication information, tightly coupled\nto the request it's associated with (authentication information that\nisn't tightly coupled with a request is often not useful). The\nprotocol does authentication, in support of authorization, but does\nnot necessarily determine where authorization happens. For example, a\nserver doing authentication for another can redirect, sending it's\nstamp of approval on the authentication information in the opaque\nfield, and the target server could do the actual authorization, based\non the verified authentication.\n\nSo, if my plea is compelling, I'd like to see every reference to the\nword \"authorization\" in the DAA proposal excised (except for the\n\"Unauthorized\" status; that's a given). Also, the example conflates\nthe two by implying that there's a single username and password \"for\"\na document. It should say that that username/password has authorized\naccess to the document.\n\nMez\n\n\n\n"
        },
        {
            "subject": "Re: Digest Access Authentication proposal  &quot;Authorization&quot",
            "content": ">So, if my plea is compelling, I'd like to see every reference to the\n>word \"authorization\" in the DAA proposal excised (except for the\n>\"Unauthorized\" status; that's a given). Also, the example conflates\n>the two by implying that there's a single username and password \"for\"\n>a document. It should say that that username/password has authorized\n>access to the document.\n\nWell, I guess I just learned something new. :-)  I intend to change the\ndraft accordingly.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Semantics of &quot;Accept:&quot",
            "content": "Let me start off by saying that agree completely with the general sentiment\nthat \"*/*\" meaning \"not unusual\" is unworkable.  This is especially true\nsince 5.4.1 is self-contradictory, as the first paragraph states that \"*/*\"\nindicates all media types.  When writing a server, I want to know that in\nresponse to a \"*/*\" I can return any file.\n\nMy biggest problem is that current practice for most browsers seems to be an\naccept line like:\n\nAccept: text/html, text/plain, image/gif, image/jpeg, */*; q=0.3\n\nmeaning that the browser would prefer those types with q=1.0 since it can\ndisplay them implicitly, but it can take anything else and save it to disk\n(or route it to a helper app, or whatever).  I haven't been convinced that\nit's worth making the spec diverge from current practice in the way that\n5.4.1 specifies.\n\nI'd also like to quibble over the statement \"If at least one Accept header\nis present, a quality factor of 0 is equivalent to not sending an Accept\nheader field containing that media-type or set of media-types.\"  Does this\nmean that these lines are identical?\n\nAccept: application/octet-stream; q=0, application/* \nAccept: application/*\n\nIt seems like it would be more useful if q=0 could be used to explicitly\ndisallow a certain type.  Comments?\n\n--\nJim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": ">>The case problem (is it 789ABCDEF or 789abcdef?) is important because\n>>digests of A1 and A2 themselves get digested in <digest>.  Clearly a\n>>digest of 789ABCDEF is different from a digest of 789abcdef.\n\n>I agree with Dave on this.  It makes a difference, and it's easier to just\n>be explicit.\n\nHang on a second. WHERE is this thought to occur?\n\nIf a Digest is being digested then it is the digest value that is digested. Not\nthe digest converted to base 16, 64 or any other form.\n\nThis is essential since otherwise it introduces unnecessary transformations\nwhen gating HTTP-NG.\n\n\nThe Digest is the 128 bits of information. The hexadecimal is nothing other than \na means of transporting the digest. If this is unclear in the spec it should be \nmade so.\n\nSpecifying use of upper or lowercase should be irrelevant.\n\n\nPhill.\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": ">The case problem (is it 789ABCDEF or 789abcdef?) is important because\n>digests of A1 and A2 themselves get digested in <digest>.  Clearly a\n>digest of 789ABCDEF is different from a digest of 789abcdef.\n\nI agree with Dave on this.  It makes a difference, and it's easier to just\nbe explicit.\n\nBTW, I'm currently adding support for Digest authentication to the NCSA\nhttpd server.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Re: another Digest Access Authentication questio",
            "content": ">[Has the \"Digest\" authentication scheme become \"KeyedDigest\"?]\n\nNo, it's \"Digest\".\n\n>So I don't see the\n>problem with unsecured anchors.  The [Keyed]Digest authentication\n>successfully required Alice to identify herself, as it was supposed to.\n\nAgreed, Digest Authentication, is just that -- Authentication.  It is not\nencryption, and it wasn't intended to be.  If you want to pass sensitive\ninformation, use SHTTP.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Re: Digest Access Authentication proposal  &quot;Authorization&quot",
            "content": "zurko@osf.org (Mary Ellen Zurko) writes:\n> It seems that from the start of WWW security, the terms authentication\n> and authorization have been used in ways that obfuscate the difference\n> between them.\n[etc]\n\nI have just read through both section 10 of draft-ietf-http-v10-spec-00 and \ndraft-ietf-http-digest-aa-00 and have found only one place where I think that \neither term is used incorrectly. This is under <domain> in 2.1 of digest-aa, \nwhere I think \"authorization\" should replace \"authentication.\n\nThe credentials are authorization information; \"This request is from <username>\n who claims the right to access <requested-uri> in <realm>\" the digest is the \nauthentication information for the claim \"Only <username> could and would have \ncreated this <digest> of that claim and the secret we share\". The server could \nhave constructed the credentials, but this scheme is not trying to address the \nnon-repudiation problem.\n\nReading through digest-aa-00 again has revealed two other problems:\n\n1) <digest> and <message-digest> have only the nonce to link them in a \nrequest. Specifically, if the server re-uses nonces, the client cannot ensure \nthat the message body matches the requested uri - this could be a problem \nPOSTing.\n\n2) There is nothing except the nonce to link the response to the request. If \nthe nonce is re-used, an intruder could substitute an old response.\n\nIf there is any possibility that the client has used the same method:URI pair \nbefore, the only way to ensure that the entities in request and response \nmessages are current is for the client to create a nonce of its own. This has \nto be added to both <digest> amd <message-digest> (both ways) and therefore be \nanother parameter to the Authorization header so that the server can create \nthe <message-digest> for the response. Guaranteeing a current response from \nthe server only matters if the resource could change. If the client is happy \nwith a cached version, then it could use method:URI in place of the nonce to \nensure that the reply is to some version of this question.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "still more Digest Authentication comment",
            "content": "HTTP already uses MIME-64 encoding for converting octects to characters;\nI'd suggest that re-using the same encoding scheme would make sense\n(since servers are likely to include the code already, and it's also\nmore compact that 4bits->1octect encoding).\n\nMike Cowlishaw\nIBM UK Labs.\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": ">HTTP already uses MIME-64 encoding for converting octects to characters;\n>I'd suggest that re-using the same encoding scheme would make sense\n>(since servers are likely to include the code already, and it's also\n>more compact that 4bits->1octect encoding).\n\nI agree. (and not just because its only a single line change in my code :-)\n\nMD5s are recognisable as base64 objects. Base16 is still appropriate for the \nlikes of timestamps and such though since they are genuinely numbers rather than \nblocks of random bits.\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: More KeyedDigest... Re: another Digest Access Authentication  questio",
            "content": "Phill writes:\n\n[Note that I finally wised up and noticed that Phillip has TWO l's not one...]\n\n>Personally I would prefer to encrypt the content using the shared secret\n>(modified in some manner) and DES or IDEA. I did think of suggesting this\n>at the\n>time but then of course we are into massive ITAR problems :-( But no patent\n>problems :-)\n>\n>\n>We should probably have a bridge note written since S-HTTP has lots of shared\n>secret mechanisms. Since we now have a shared secret we should employ it...\n>\n>The simplest mode would be to take the shared secret key  [MD5 (password,\n>domain, username)] and XOR it with some random 128 bits. Then use the first 64\n>bits for the key and the other 64 bits for the IV of the cipher. (PKCS\n>#5). The\n>random bitstring is needed because one should attempt to limit the\n>quantities of\n>ciphertext sent under the same key.\n\nIf we extend Digest authentication to support encryption, then it becomes\nSomething Else.  This newly created Something Else may be a really good\nthing to have around, but it will indeed have \"massive ITAR problems\".\nDigest Authentication is being proposed for inclusion in HTTP/1.1.  I don't\nthink we should make ITAR an issue in HTTP/1.1.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Quality factors (http draft 8th March",
            "content": "Somewhat of a detail, this, but anyway:\nIn section 5.4.1 Accept, you say:\nfloat  = <  ANSI-C floating point text representation,\n            where (0.0 < float < 1.0) >\nWhat, precisely, do you mean by this? You don't give a reference, so I could\nimagine the following:\n 1. The representation of a floating-point number (float, double or\n    long double) as a token in a conforming ANSI C program, or\n 2. the representation of a floating-point number produced by the ANSI printf\n    routine - but which format? %g, %e, %f??\n 3. the representation of a floating-point number accepted by an ANSI scanf\n    routine.\n\nI even wonder whether 'floating point text representation' means the text\nrepresentation of a floating-point number, or the floating-point (text)\nrepresentation of (any) number.\n\nIn particular, does it allow:\nq=1e-4\nq=0.4f\nq=1.343e-1l\n?\n\nAnd, perhaps more seriously, it currently forbids q=0. and q=1.\n\n David Robinson. (drtr1@cam.ac.uk)\n\n\n\n"
        },
        {
            "subject": "Re: Digest Access Authentication proposal  &quot;Authorization&quot",
            "content": "Hi Owen,\n\n> The credentials are authorization information; \"This request is from <username>\n>  who claims the right to access <requested-uri> in <realm>\" \n\nI'm not sure I see what sense you consider this authorization\ninformation. As I pointed out, even clear authentication information\nis an input to authorization decisions, so it's not incorrect to call\neven the digest \"information for the authorization decision\". It does,\nhowever, confuse people to call either the digest or the credentials\n(in this case) simply \"authorization information\".\n\nThe credentials do not drive the authorization decision the way\ncapabilities (or, if I dare, ANSA credentials :-) drive authorization\ndecisions. They're not emitted and protected by a trusted authority;\nthey don't (usually) grant authorization by their very existance. What\nthey do is tie the request (at some level) to the authentication\ninformation. Both the request (method and URI) and the authentication\ninformation (username and realm; the realm is strictly for\nauthentication I believe) are necessary for the authorization\ndecision. But, I would argue (I do argue :-), the information that\ndrives the authorization decision is more likely to be a database like\nthe .htaccess file (or a separately passed capability or ANSA-like\ncredential, vouched for by an appropropriate authorization authority).\n\nMez\n\n\n\n"
        },
        {
            "subject": "3rd party Digest Access Authenticatio",
            "content": "I would like to see the Internet Draft modified in a very simple\nway to allow its use with 3rd party authentication. Let me explain\nthe idea:\n\n    a) Client sends request to HTTP server\n\n    b) Server responds with 401 Unauthorized\n\n    c) Client sends authentication request to 3rd party\n       server\n\n    d) 3rd party authentication server replies\n       with authentication ticket\n\n    e) Client resends request to HTTP server, attaching\n       the authentication ticket from 3rd party server\n\n    f) Server authorizes the request based on the ticket\n       and sends the requested document.\n\nOptionally, the server in step (f) can attach a keyed message digest\nso that the client can verify that the message hasn't changed en route.\n\nThe value of this approach is that users don't have to keep track of\npasswords for each merchant they have an account with. Instead, users\nhave a single password shared with the 3rd party authentication server.\nLikewise, merchants don't need separate passwords for each user, instead\nthe merchant needs a single password it shares with the 3rd party server.\n\nIt seems reasonable to use UDP to communicate with the 3rd party\nauthentication server as this will reduce latency and allow the\nserver to handle higher loads. I won't explain the details of how the\nclient communicates with the 3rd party server in this posting, though.\n\nI expect you are wondering by now, how the client can check the message\ndigest sent by the server in step (f). The client doesn't share a key\nwith the server, so it can't verify the message digest itself. Instead,\nit displays the document as it comes off the wire, and when all the data has\narrived involves the help of the trusted 3rd party server, which does know\nthe server's password. To do this the client generates the MD5 hash value\nfor the message body and then sends this to the 3rd party server along\nwith the message digest sent by the HTTP server in step (f).\n\nThe use of 3rd party authentication reduces book keeping to a minimum as\ncompared with using separate keys. It also makes the 3rd party server a\ntarget for hackers!  This can be easily defended by isolating the server\nfrom the net and only responding to UDP packets on the port dedicated to\nthe authentication service. Denial of service attacks are still possible\nbut the Internet as a whole suffers from that problem.\n\nAn obvious question is how does this proposal differ from Kerberos?\nThe answer is that it is simpler, and only uses the public domain\nMD5 algorithm. There are no licensing issues and no export controls!\n\nThe only changes required to the Internet Draft for the Digest\nAccess Authentication scheme are:\n\n    1)  add a new optional field  called \"TRUSTED\" to the\n        401 Unauthorized message from the HTTP server\n\nThis identifies a mutually trusted 3rd party using a URI.\n\n    2)  add a new optional field  called \"TRUSTED\" to the\n        Authorization header sent by the client to the HTTP server\n\nThis identifies the 3rd party used by the client to generate H(A1).\nWhen the Authorization header includes the TRUSTED field, the HTTP\nserver uses the password P which it shares with the trusted 3rd\nparty. This password is looked up by the server on the basis of the\nthe value supplied with the trusted field (T) and the realm (R).\nIf the TRUSTED field is omitted, the server looks up P on the\nbasis of the user name (U) and the realm (R).\n\nI have modified the relevant section of the Internet Draft and\ninclude this below. The Digest Access Authentication Internet Draft\nshouldn't specify details of the message exchanges with the 3rd party\nauthentication server, as these will be covered in a separate Internet\nDraft.\n\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: Digest realm=\"<realm>\",\n                            trusted=\"trusted\",\n                            domain=\"<domain>\",\n                            nonce=\"<nonce>\",\n                            opaque=\"<opaque>\",\n                            stale=\"<TRUE | FALSE>\"\n\nThe meanings of the identifers used above are as follows:\n\n    <realm>\n        A name given to users so they know which username and password\n        to send.\n\n    <trusted> OPTIONAL\n        A string of data, identifying a trusted 3rd party authentication\n        server, using the URI notation. \n\n    <domain>  OPTIONAL\n        A comma separated list of URIs, as specified for HTTP/1.0.  The\n        intent is that the client could use this information to know the\n        set of URIs for which the same authentication information should be\n        sent.  The URIs in this list may exist on different servers.  If\n        this keyword is omitted or empty, the client should assume that\n        the domain consists of all URIs on the responding server.\n\n    <nonce>\n        A server-specified integer value which may be uniquely generated\n        each time a 401 response is made.  Servers may defend themselves\n        against replay attacks by refusing to reuse nonce values. The nonce\n        should be considered opqaue by the client.\n\n    <opaque>  OPTIONAL\n        A string of data, specified by the server, which should returned by\n        the client unchanged.  It is recommended that this string be\n        base64 or hexadecimal data.  Specifically, since the string is\n        passed in the header lines as a quoted string, the double-quote\n        character is not allowed.\n\n    <stale>   OPTIONAL\n        A flag, indicating that the previous request from the client\n        was rejected because the nonce value was stale.  If stale\n        is TRUE, the client may wish to simply retry the request with\n        a new encrypted response, without reprompting the user for a\n        new username and password.\n\nThe client is expected to retry the request, passing an Authorization header\nline as follows:\n\n  Authorization: Digest\n        username=\"<username>\",             -- required\n        realm=\"<realm>\",                   -- required\n        trusted=\"<trusted>\",               -- OPTIONAL\n        nonce=\"<nonce>\",                   -- required\n        uri=\"<requested-uri>\",             -- required\n        response=\"<digest>\",               -- required\n        message=\"<message-digest>\",        -- OPTIONAL\n        opaque=\"<opaque>\"                  -- required if provided by server\n\n    where <digest> := H( H(A1) + \":\" + N + \":\" + H(A2) )\n    and <message-digest> := H( H(A1) + \":\" + N + \":\" + H(<message-body>) )\n\n    where:\n\n        A1 := U + ':' + R + ':' + P\n        A2 := <Method> + ':' + <requested-uri>\n\n    with:\n\n        N -- nonce value\n        U -- username\n        T -- trusted 3rd party server name\n        R -- realm\n        P -- password\n        <Method> -- from header line 0\n        <requested-uri> -- uri sans proxy/routing\n\nNote: if T is given is present, then the HTTP server looks\n      up password P on the basis of T and R, otherwise it\n      is looked up on the basis of U and R\n\nWhen authorization succeeds, the Server may optionally provide the\nfollowing:\n\nHTTP/1.1 200 OK\nDigest-MessageDigest:\n              username=\"<username>\",\n              realm=\"<realm>\",\n              trusted=\"<trusted>\",    -- OPTIONAL\n              nonce=\"<nonce>\",\n              message=\"<message-digest>\"\n\nThe Digest-MessageDigest header indicates that the server wants to\ncommunicate some info regarding the successful authentication (such as\na message digest or a receipt of some kind).\n\n        <message-digest> is computed as given above for\n        the client.  this allows the client to verify that\n        the message body has not been changed en-route.\n\nIf the header includes the name of a trusted 3rd party server, the client\nwill need its help to verify the message-digest.\n\netc.\n\np.s. why does the Digest-MessageDigest header need the username?\nSurely this can be derived by the client based on only the realm\nas took place when the 401 unauthorized message was received.\n\n-- Dave Raggett <dsr@w3.org> url = http://www.hpl.hp.co.uk/people/dsr\n   Hewlett Packard Laboratories, Filton Road, | tel: +44 117 922 8046\n   Bristol BS12 6QZ, United Kingdom           | fax: +44 117 922 8924\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "    HTML and WWW norms tend to excessive fragmentation of information and\n    too many hlinks to investigate just to guess which to follow.  There\n    are many times when a complex subject (like the HTML specs) needs\n    to be studied in a more linear sequence than can be achieved yet\n    with current markup the author is discouraged from providing a well\n    organized linear document.\n    \nAgreed.  This is a somewhat different problem than the one which\nstarted this thread, but I would really like to see a browser that\nsupported a \"flatten\" command (maybe \"flatten 1 level\", \"flatten N levels\",\nand \"flatten entirely\" commands).  I don't know if this would require\nany additional HTML support.  I'm more interested in HTTP, and I think\nthis would not require anything new from HTTP (although it might be\nnice to have an efficient way of asking for the entire set of documents\nin one request).\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Semantics of &quot;Accept:&quot",
            "content": "On Wed, 22 Mar 1995, Jim Seidman wrote:\n> My biggest problem is that current practice for most browsers seems \n> to be an accept line like:\n> \n> Accept: text/html, text/plain, image/gif, image/jpeg, */*; q=0.3\n\nSome notes on current practice (from the latest versions I've seen):\n\n* Sends Accept: with at least one q value --\nSpyglass 2.00a1, OmniWeb/0.7.5.5, ATSON-W3LM/1.0 beta\n\n* Sends a list of accepted types, including */* but without q values --\nLynx 2.3.7, WinMosaic 2.0a8, NCSA Mosaic for X 2.5b4, AIR_Mosaic\n3.07.04.02, GWHIS/110, Netscape 1.0\n\n* Sends just a list of accepted types, without */* or q values --\nMacMosaic 2.0.0a17, Netcruiser 1.10 beta, MidasWWW 2.2, EI*Net/0.1\n\nIn addition, Emacs-W3 sends a list with some subtypes wildcarded, but \ndoes not list */* (yay Bill); and Chimera 1.0 just sends */*.  Lynx and \nEmacs-W3 (at least -- probably others as well) modify the Accept: list \nbased on your preferences.\n\n[Of those that send q values, ATSON-W3LM and Spyglass 1.x appear to use \nthem correctly.  OmniWeb sends '*; q=0.500', omitting the wildcard for \nsubtype.  Spyglass 2.00a1 (Win32) sends '*/*, q=0.300', using a comma \ninstead of a semi-colon, but presumably I have old information on that \ncount.]\n\nM. Hedlund <march@europa.com>\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "On Wed, 22 Mar 1995, Jeffrey Mogul wrote:\n> Agreed.  This is a somewhat different problem than the one which\n> started this thread, but I would really like to see a browser that\n> supported a \"flatten\" command (maybe \"flatten 1 level\", \"flatten N levels\",\n> and \"flatten entirely\" commands).  I don't know if this would require\n\nThis is known as \"folding\" on some editing/hypertext systems. \n\nIf we are only interested in folding as a means to allow readers to skim \nthrough text and selectively reveal interesting bits, then this just \nrequires a new tag in HTML and browsers that will fold/unfold text in \nresponse to this tag. \nAside: Should a recursive fold be allowed ?\n\nHowever, if we are interested in folding as a means to reduce network \ntraffic, then things get somewhat more complex as has already been \nsuggested by others.\n\nLiam\n--\n Liam Relihan,                                        Voice: +353-61-202713\n CSIS, Schumann Building,        [space]                Fax: +353-61-330876\n University Of Limerick,                             E-mail: relihanl@ul.ie\n Ireland.                      http://itdsrv1.ul.ie/PERSONNEL/lrelihan.html\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "On Wed, 22 Mar 1995, Liam Relihan wrote:\n> On Wed, 22 Mar 1995, Jeffrey Mogul wrote:\n> > Agreed.  This is a somewhat different problem than the one which\n> > started this thread, but I would really like to see a browser that\n> > supported a \"flatten\" command (maybe \"flatten 1 level\", \"flatten N levels\",\n> > and \"flatten entirely\" commands).  I don't know if this would require\n> \n> This is known as \"folding\" on some editing/hypertext systems. \n> \n> If we are only interested in folding as a means to allow readers to skim \n> through text and selectively reveal interesting bits, then this just \n> requires a new tag in HTML and browsers that will fold/unfold text in \n> response to this tag. \n\nFolding is good.  It's the only way a Unix dweeb like me can handle the \nMacOS directory listing.  Folding where the act of expansion can involve \na fetch of more HTML text (or other objects) to inline would be \nphantastic.\n\n> Aside: Should a recursive fold be allowed ?\n\nYes, definitely.\n\nThough this seems like it should be a www-html discussion....\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": ">>HTTP already uses MIME-64 encoding for converting octects to characters;\n>>I'd suggest that re-using the same encoding scheme would make sense\n>>(since servers are likely to include the code already, and it's also\n>>more compact that 4bits->1octect encoding).\n>\n>I agree. (and not just because its only a single line change in my code :-)\n>\n>MD5s are recognisable as base64 objects. Base16 is still appropriate for the\n>likes of timestamps and such though since they are genuinely numbers\n>rather than\n>blocks of random bits.\n\nI disagree.  I think the choice of base64 vs. base16 is purely arbitrary,\nsince the space savings is hardly significant.  John Franks has already\nimplemented Digest using base16 in his WN server.  Spyglass has already\nimplemented Digest using base16 in our client, which is shipping.  My\nunderstanding is that Netscape has implemented Digest using base16 for a\nfuture release of their server.\n\nI see no compelling reason to change to base64.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": "According to Eric W. Sink:\n> >>HTTP already uses MIME-64 encoding for converting octects to characters;\n> >>I'd suggest that re-using the same encoding scheme would make sense\n> >>(since servers are likely to include the code already, and it's also\n> >>more compact that 4bits->1octect encoding).\n> >\n> >I agree. (and not just because its only a single line change in my code :-)\n> >\n> >MD5s are recognisable as base64 objects. Base16 is still appropriate for the\n> >likes of timestamps and such though since they are genuinely numbers\n> >rather than\n> >blocks of random bits.\n> \n> I disagree.  I think the choice of base64 vs. base16 is purely arbitrary,\n> since the space savings is hardly significant.  John Franks has already\n> implemented Digest using base16 in his WN server.  Spyglass has already\n> implemented Digest using base16 in our client, which is shipping.  My\n> understanding is that Netscape has implemented Digest using base16 for a\n> future release of their server.\n> \n> I see no compelling reason to change to base64.\n> \n\nI agree with Eric.  It is simpler and easier to implement base16. \nAll else being equal Simple is Better.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": "> \n> >>HTTP already uses MIME-64 encoding for converting octects to characters;\n> >>I'd suggest that re-using the same encoding scheme would make sense\n> >>(since servers are likely to include the code already, and it's also\n> >>more compact that 4bits->1octect encoding).\n> >\n> >I agree. (and not just because its only a single line change in my code :-)\n> >\n> >MD5s are recognisable as base64 objects. Base16 is still appropriate for the\n> >likes of timestamps and such though since they are genuinely numbers\n> >rather than\n> >blocks of random bits.\n> \n> I disagree.  I think the choice of base64 vs. base16 is purely arbitrary,\n> since the space savings is hardly significant.  John Franks has already\n> implemented Digest using base16 in his WN server.  Spyglass has already\n> implemented Digest using base16 in our client, which is shipping.  My\n> understanding is that Netscape has implemented Digest using base16 for a\n> future release of their server.\n> \n> I see no compelling reason to change to base64.\n\nThis is not the first use of something like MD5 authentication\nin Internet protocols (i.e. APOP in POP3) ... are there precidents\nfrom other RFCs?\n\n\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": "Albert Lunde wrote:\n>This is not the first use of something like MD5 authentication\n>in Internet protocols (i.e. APOP in POP3) ... are there precidents\n>from other RFCs?\n\nAnswering my own question:\n\nRFC1544 The Content-MD5 Header\nUses Base 64\n\nRFC1725 Post Office Protocol - Version 3\nuses Base 16\n\nRFC1421.txt PEM\nuses Base 64\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "On Wed, 22 Mar 1995, Liam Relihan wrote:\n\n> If we are only interested in folding as a means to allow readers to skim \n> through text and selectively reveal interesting bits, then this just \n> requires a new tag in HTML and browsers that will fold/unfold text in \n> response to this tag. \n> Aside: Should a recursive fold be allowed ?\n> \n> However, if we are interested in folding as a means to reduce network \n> traffic, then things get somewhat more complex as has already been \n> suggested by others.\n> \nI think folding provides a nice intersection between two objectives ...\nthat of helping the user cope with information AND by helping the user\nimplicitly request less information up front reduce/manage network\ntraffic.  Serving the user and network with the same feature!\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "charset overlabelin",
            "content": "Section 8.1.1 of the current draft says:\n\n   HTTP does not require that the character set of an entity body be \n   labelled as the lowest common denominator of the character codes \n   used within a document.\n\nI suggest you change this into\n\nIt is recommended but not required that ...\n\nbecause otherwise you encourage people to send something like\n\nContent-Type: text/html; charset=KOI8-R\n\nand have me change my font even if the message is plain ASCII.\n\n\n\n"
        },
        {
            "subject": "Why is From: limited",
            "content": "I've got two questions regarding 5.4.6 From\n\n1. Why is From: only a request header?  It would be nice to provide\n   the address of the file owner so you know where to send comments\n   if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n\n2. Why is it an addr-spec only instead of a full RFC822 mailbox?\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": "Jeffrey Mogul writes:\n\n> Of course, the right \"solution\" might be to encourage authors\n> to provide their large documents in two forms: monolithic (single\n> HTML file) for people with fast connections and/or for people who\n> want to quickly search the documents for particular character\n> strings, and \"outlined\", for people who want to retrieve just\n> the parts they are looking for.\n\nAnother way of doing it, obtaning good sides of both approaches \nis to use the \"lines\" query approach found in servers such as GN.\nIn this way you can have a small file with the table of contents\nand then refer to the specific parts of a big .htlm document\nby line references. This caters for the guy with the small bandwidth.\nAs it is still one monolitic document, document-wide searches can\nstill be done conveniently.\n\nKeld\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "At 11:15 AM 3/23/95, Roman Czyborra wrote:\n>I've got two questions regarding 5.4.6 From\n>\n>1. Why is From: only a request header?  It would be nice to provide\n>   the address of the file owner so you know where to send comments\n>   if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n\nSure, it'd be nice. It's probably impossible to implement in an efficient\nfashion on most servers today. It means that server implementors will have\nto add yet another config file that specifies the author of every single\ndocument being served by their server, just so the From: line can be filled\nin in a HTTP response header. This isn't practical and is the reason why\npeople put their address info at the bottom of HTML documents when they\nwant to be contacted.\n\nThere are plenty of times when the author of a document has no desire to be\ncontacted by millions of people on the net. This is the same reason that\nFrom: no longer sends the e-mail address of users when clients make\nrequests. People want their anonymity and building big brother features\ninto HTTP, even though they may be convenient, are probably inappropriate.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "According to Roman Czyborra:\n> \n> 1. Why is From: only a request header?  It would be nice to provide\n>    the address of the file owner so you know where to send comments\n>    if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n> \n\n\nWould this be different from the current header\n\nLink: <mailto:e-mail_address>; REL=\"made\"\n\nMany people find the Link header cryptic, perhaps because of its\ngenerality, but I think it supplies the information you want.  I\ndon't know if this is widely used, but it is used by the WN server\n(by default for all files -- even non-HTML files).\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": "I said:\n  > >>The case problem (is it 789ABCDEF or 789abcdef?) is important because\n  > >>digests of A1 and A2 themselves get digested in <digest>.  Clearly a\n  > >>digest of 789ABCDEF is different from a digest of 789abcdef.\n  > \n\nEric Sink said:\n  > >I agree with Dave on this.  It makes a difference, and it's easier to just\n  > >be explicit.\n  > \n\nPhillip Hallam-Baker responded:\n  > Hang on a second. WHERE is this thought to occur?\n  > \n  > If a Digest is being digested then it is the digest value that is digested. Not\n  > the digest converted to base 16, 64 or any other form.\n  > \n  > This is essential since otherwise it introduces unnecessary transformations\n  > when gating HTTP-NG.\n  > \n  > \n  > The Digest is the 128 bits of information. The hexadecimal is nothing other than \n  > a means of transporting the digest. If this is unclear in the spec it should be \n  > made so.\n  > \n  > Specifying use of upper or lowercase should be irrelevant.\n\nEric Sink said (in response to another follow-up):\n  > I disagree.  I think the choice of base64 vs. base16 is purely arbitrary,\n  > since the space savings is hardly significant.  John Franks has already\n  > implemented Digest using base16 in his WN server.  Spyglass has already\n  > implemented Digest using base16 in our client, which is shipping.  My\n  > understanding is that Netscape has implemented Digest using base16 for a\n  > future release of their server.\n  > \n  > I see no compelling reason to change to base64.\n\nFWIW, I've implemented John Franks's scheme in my server using base16.\nI think that biased my thinking.  Phillip is right that MD5 is 128 bits,\nand nothing need be said about encodings, except for transport.  However,\nexisting (shipping) implementations tend to carry weight.  Gee, I hope\nthey made interoperable design choices!\n\nThat said,\n1) The fact that there has been discussion points out that the spec.\nneeds to be tightened up to be absolutely clear.  The words I proposed\nyesterday(?) would at least impose a particular interpretation that, I\nthink, would result in consistent (interoperable) implementations.\n\n2) If 16 binary bytes should be used for each of H(A1) and H(A2) in\n<digest>, or for H(A1) in <message-digest>, why bother with the ':'s?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Suggestion for HTTP 1.",
            "content": "Dear Team,\nI represent a firm producing HTTP clients and servers for use in vertical\nmobile computing environments, using PDA-style devices.  In the last year, we\nhave produced a HTTP 0.9 compliant browser, and are currently collaborating\nwith other firms to produce a HTTP 1.0 compliant browser and proxy server for\nwired or wireless access to devices which may or may not have an underlying\nTCP/IP stack.  To this end, we have reviewd the draft IETF HTTP spec (0.1,\ndated 19 December 1994, expiring June 19, 1995) quite thoroughly, and it has\nbeen very useful in both the development of our implementation and in\ndeveloping proxy servers for this and other applications.\n\nIn our work, we have seen a fairly strong need to reduce the amount of\ndocument\ndata transferred between proxy and mobile client - traditionally, this link\nis\nachieved via a slow-bandwidth or high-byte-cost connection (or more often,\nboth) such as the ARDIS network or a 2400bd modem connection. \n\nAs we approach incorporating HTTP 1.0 support, we have debated adding a\nsubdocument query extension to HTTP, to support the following scenario:\n\n1. The mobile client requests a document from the proxy, indicating it only\n wants a single \"page\" (of client-specified length).\n2. The proxy fetches the full document (or perhaps only the first page,\n depending on the version of its source)\n3. The proxy returns the desired \"page\" to the client.\n\nThis page is an arbitrary range of bytes, which requires that the client have\nsome mechanism of dealing with fragmented data (split HTML tokens, for\nexample,\nor reuniting pieces of a split graphic as required).  By allowing this\n\"paging\"\nto be performed by the proxy, which presumably has an inexpensive\nhigh-bandwidth connection, we can decrease transfer time and transfer cost. \nOur user studies have indicated that in a large number of situations, only a\nsmall segment of a document (usually, although not always, the beginning) is\nused by a user before pursuing another.  Rather than using a start/stop\nscheme,\nwe thought that this paging mechanism would be more robust.\n\nBased on our experience in mobile data transfer, we think an extension to the\nRequest \nHeader of HTTP 1.0 requests to permit the request of only a certain byte\nrange\nmay be of use to more than just us and our customers; as wireless access to\nthe\nnetwork grows, proxy servers used by wireless providers could offer this\nmode to lower transfer time and save customer costs, for example.  Thus, we\nfelt it would be best to call your attention to our efforts in the hopes that\na\nunified standard for this mechanism could be made.\n\nWe would thus respectfully submit an additional keyword to the Object header\nto\ndenote the range of octets for a particular document being requested.  This\nkeyword \nwould be known as Octet-Range, and perform as follows.\n\nBNF Description of Octet-Range in the Object Header:\nOctet-Range = \"Octet-Range\" \":\" start \"-\" end\n start  =  1 * DIGIT\n end  = 1 * DIGIT\n \nRequests including the Octet-Range Object Header would generate responses\ncontaining only those octets in the Object Body offset by the\nstart value (counting from zero) and through the end, or the end of the\nObject\nBody, whichever resulted in the smallest Object Body.  It is an error to\nreturn\nOctets before the indicated start offset, or after the indicated end offset. \n\nA response incorporating the Octet-Range would return the Octet-Range of the\nrequest, and the associated octets of the Object Body as the Object-Body.  In\nthe event that the end offset was greater than the total number of octets,\nthe\nOctet-Range Header still has its original value, but the Content-Length\nObject\nHeader shall indicate the number of octets actually returned.  (The client\nmay\nbe able to then deduce the true total length of the document as its start\nplus\nthe returned number of octets without the need for a HEAD request).\n\nYour consideration of this matter is appreciated.  Please feel free to\ncontact\nus with any comments, questions, or feedback you may have.\n\n\n\n Best Regards,\n Ray Rischpater\n Software Craftsman\n AllPen Software, Inc.\n (408) 399-8800 voice\n (408) 399-4395 fax\n dove@allpen.com dove@eWorld.com\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion for HTTP 1.",
            "content": "In message <9503230830.tn68105@eworld.com> of Thu, 23 Mar 1995 08:30:44\n  -0800, you wrote:\n\n> 1. The mobile client requests a document from the proxy, indicating it only\n>  wants a single \"page\" (of client-specified length).\n> 2. The proxy fetches the full document (or perhaps only the first page,\n>  depending on the version of its source)\n> 3. The proxy returns the desired \"page\" to the client.\n\nThe mobile client can do this by specifying a small TCP receive window and\nnot opening it again.  If you want the first 1024 bytes, for instance, you\nset the TCP receive window to 1024 + the probable length of the reply\nheader.  When you receive the reply header, make sure the TCP window doesn't\nmove, but will decrease to zero.\n\nIf the user wants more of the document (or it doesn't fill a page, or\nwhatever), a TCP ACK can be sent reopening the window.  If the proxy hasn't\ntimed out yet, then you win (why? because under your proposed scheme the\nclient would have to make a second request to the server for the next page\nin all cases).  If the proxy has timed out you have to reconnect.\n\nTo prevent overloading the proxy, the client should reset the quiet TCP\nconnection after a while.  It's ungracious, but a normal close would\nwait for the proxy to download the rest of the data, which is wrong.\n\n> BNF Description of Octet-Range in the Object Header:\n> Octet-Range = \"Octet-Range\" \":\" start \"-\" end\n>  start  =  1 * DIGIT\n>  end  = 1 * DIGIT\n\nThis was discussed on the http list a short while ago.  Some servers\nimplement a hack where \";bytes=0-1023\" can be appended to a URI to\nrequest the first 1024 bytes, etc.  Larry Masinter mentioned a\nGET_PARTIAL request, which I'd prefer if you're going to do something\nlike Octet-Range.\n\nThe Relative URI draft mentions the \"param\" part of the URI (after the\nsemicolon).  This seems to bless WN's ;bytes=start-finish hack.  Plus\nit has prior implementation (I'm told GN does the same thing).\n\nSo I'd recommend using a WN-style \"bytes\" param in the URI.  There's no\ntechnical problem implementing Octet-Range that I can see, it's just that\nit seems aesthetically important that the URI should correspond to the\ndata returned (rather than to a larger body containing the returned data).\n\n>  Best Regards,\n>  Ray Rischpater\n>  Software Craftsman\n\nInteresting job title.\n\n-- \nAdrian.Colley@sse.ie   <g=Adrian;s=Colley;o=SSE;p=SSE;a=EIRMAIL400;c=ie>\nphones:- work: +353-1-6769089; fax: +353-1-6767984; home: +353-1-6606239\nemployer: Software and Systems Engineering (+=disclaimer)  (Perth)->o~^\\\nY!AWGMTPOAFWY? 4 lines, ok? qebas perl unix-haters kill microsoft  \\@##/\n\n\n\n"
        },
        {
            "subject": "Re: proposal for a new html ta",
            "content": ">Agreed.  This is a somewhat different problem than the one which\n>started this thread, but I would really like to see a browser that\n>supported a \"flatten\" command (maybe \"flatten 1 level\", \"flatten N levels\",\n>and \"flatten entirely\" commands).  I don't know if this would require\n>any additional HTML support.  I'm more interested in HTTP, and I think\n\nThe problem with flattening is that you can drag in all kinds of\nthings you don't want, and what happens if it said \"flatten 1000\"!\n\nDynaWeb has a parameter on the URL called MAXBYTES. Once the data size\nexceeds this, it tries to generate a TOC. By setting MAXBYTES to some\nlarge number, you can retrieve entire documents. No browser changes\nneeded.\n\n\n\n"
        },
        {
            "subject": "Suggestion: HTTP Timezone heade",
            "content": "A suggestion for HTTP: that clients send a Timezone header in the request,\ndescribing the timezone of the user. This would be very useful\nwhen presenting dates and times to the user; I would envisage using this\nin a CGI script that generated dates on the fly.\n\nThe header would be a request header.\nSyntax:\n\nTimezone = \"Timezone\" \":\" zone\n\nzone would have the RFC 822 definition:\nzone = numeric-zone | ascii-zone | military-zone\nnumeric-zone = ( \"+\" | \"-\" ) 4digit\nascii-zone = 2*3alpha\nmilitary-zone = alpha\n\nI would suggest that the numeric form be recommended; it may even be better to\nonly allow this form.\n\nThere is the obvious problem of how you can determine the true time zone for\na user. As a sensible default, the client program could use the timezone of the\ncomputer on which it is running. Alternatively, I could imagine a client\nhaving an option the user can set to choose the timezone.\n\n David Robinson. (drtr1@cam.ac.uk)\n\n\n\n"
        },
        {
            "subject": "Re: still more Digest Authentication comment",
            "content": ">FWIW, I've implemented John Franks's scheme in my server using base16.\n>I think that biased my thinking.  Phillip is right that MD5 is 128 bits,\n>and nothing need be said about encodings, except for transport. \n\nAh well thats OK then. so long as everyone realises I'm right we can \nput whatever people like in the spec so long as it is clear. In my original\nequations I think I simply assumed that the base-16 encoding was orthogonal to \nthe hash function, the last thing to be done.\n\nThat said we should rewrite the spec using the functions D for digest and H for \nhex (uppercase). And make it explicit that we use H(D( x )) each time H appears \npresently.\n\nMy implementation will then be slightly non standard in using the uppercase for \ngeneration etc but accepting a digest passed using lowercase hex characters :-)\n\n\n>2) If 16 binary bytes should be used for each of H(A1) and H(A2) in\n><digest>, or for H(A1) in <message-digest>, why bother with the ':'s?\n\nI have never understood that one. My original scheme didn't use them.\n\nSame with the usernames. I could never understand the argument on :\nvs @ since the only reason its there is to stop the fields running together\nin a possibly ambiguous fashion.\n\n\n\nPhill.\n\n\n\n"
        },
        {
            "subject": "How are/should redirects of POSTS be handle",
            "content": "I recently came upon this question while moving a web server from one machine\nto another. The need was to keep some executables and cgi scripts on the \noriginal machine. \n\nWhat appears to happen when a POST is redirected is the client attempts a\nGET at the new URL so that the user can fill-in the form again (it may have\nchanged when it moved)\n\nIs this \"correct\" handling? (It seems reasonable but the specification doesn't\nsay what the browser is expected to do)\n\nJeffrey Perry\nOpen Market, Inc.\n617-374-3778\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion for HTTP 1.",
            "content": "    > 1. The mobile client requests a document from the proxy, indicating it only\n    >  wants a single \"page\" (of client-specified length).\n    > 2. The proxy fetches the full document (or perhaps only the first page,\n    >  depending on the version of its source)\n    > 3. The proxy returns the desired \"page\" to the client.\n    \n    The mobile client can do this by specifying a small TCP receive\n    window and not opening it again.  If you want the first 1024 bytes,\n    for instance, you set the TCP receive window to 1024 + the probable\n    length of the reply header.  When you receive the reply header,\n    make sure the TCP window doesn't move, but will decrease to zero.\n\nManipulating the TCP window in this way is potentially risky, because\nit can interact with the so-called \"Silly-Window Syndrome Avoidance\nAlgorithm\" to produce long delays when absolutely nothing happens for\nmany seconds.  Presumably, on a low-bandwidth link, this is exactly\nwhat you don't want to do!\n\nThe standard sender-side SWS-avoidance algorithm will not send less\nthan one TCP segment if it has ever seen a window larger than one\nsegment.  E.g., if your segment size (MSS) is 512 bytes, the receiver\nhas at least once advertised a window of 1024 bytes, and has shrunk\nthe window to (say) 256 bytes, the sender-size SWS-avoidance algorithm\nwill delay transmission.  In theory, this delay is supposed to be\nabout 200 msec (i.e., not very long), but in most BSD-derived systems,\na bug pegs this delay at about 5 seconds.\n\nSo the key to making your suggestion work is \"never shrink the window\nsize below the MSS.\"  (In fact, the lower limit might be 2*MSS; I\nremember that this was a critical limit in some cases, but I can't\nremember exactly which ones.  It has to do with the standard delayed\nACK policy.)  I suppose it is likely that on a low-bandwidth link, the\nMSS will be set fairly small ... but it is not always possible for the\napplication to know/control the actual MSS.  Proceed with caution.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion for HTTP 1.",
            "content": "At 7:48 PM 3/23/95, Jeffrey Mogul wrote:\n>    > 1. The mobile client requests a document from the proxy, indicating\n>it only\n>    >  wants a single \"page\" (of client-specified length).\n>    > 2. The proxy fetches the full document (or perhaps only the first page,\n>    >  depending on the version of its source)\n>    > 3. The proxy returns the desired \"page\" to the client.\n>\n>    The mobile client can do this by specifying a small TCP receive\n>    window and not opening it again.  If you want the first 1024 bytes,\n>    for instance, you set the TCP receive window to 1024 + the probable\n>    length of the reply header.  When you receive the reply header,\n>    make sure the TCP window doesn't move, but will decrease to zero.\n>\n>Manipulating the TCP window in this way is potentially risky, because\n>it can interact with the so-called \"Silly-Window Syndrome Avoidance\n>Algorithm\" to produce long delays when absolutely nothing happens for\n>many seconds.  Presumably, on a low-bandwidth link, this is exactly\n>what you don't want to do!\n\nWhat's more, HTTP should be considered transport independent. Relying on\nsomething as cheesey as twaddling the underlying protocol to do something\nthat should be under conscious control between the client and server is a\ngross hack. Either build the mechanism into HTTP or live without it.\nHacking the transport layer as a means to this end is a recipe for wreck\nand ruin the minute someone sticks this protocol on top of something\nbesides TCP/IP. Remember, WWW != Unix and HTTP != TCP/IP.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: How are/should redirects of POSTS be handle",
            "content": "In message <199503231940.OAA13985@networkxxiii.openmarket.com> of Thu,\n  23 Mar 1995 14:40:38 -0500, you wrote:\n\n> What appears to happen when a POST is redirected is the client attempts a\n> GET at the new URL so that the user can fill-in the form again (it may have\n> changed when it moved)\n\nThat sounds broken, IMHO.  The source of the form has nothing to do\nwith the Action method of it.  For example, I frequently use a small\nform which I took from http://src.doc.ic.ac.uk/archieplexform.html,\ntrimmed, and embedded in my home page.  When I submit the form, it\nposts it to src.doc.ic.ac.uk via http.  If the redirect meant I had to\nfill in the form again, I wouldn't be happy at all.\n\nOr maybe this is a psychological tactic, to encourage users to update\npermanently moved links...\n\nFor temporary redirects, it's definitely wrong (for example, the redirect\nmay depend on the form data).\n\n-- \nAdrian.Colley@sse.ie   <g=Adrian;s=Colley;o=SSE;p=SSE;a=EIRMAIL400;c=ie>\nphones:- work: +353-1-6769089; fax: +353-1-6767984; home: +353-1-6606239\nemployer: Software and Systems Engineering (+=disclaimer)  (Perth)->o~^\\\nY!AWGMTPOAFWY? 4 lines, ok? qebas perl unix-haters kill microsoft  \\@##/\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "> >1. Why is From: only a request header?  It would be nice to provide\n> >   the address of the file owner so you know where to send comments\n> >   if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n>\n> Sure, it'd be nice. It's probably impossible to implement in an efficient\n> fashion on most servers today.\n\nNo, not at all.  On Unix, every file already has an owner, and I could \nhack my httpd in no time to construct an email address from it.\n\n> There are plenty of times when the author of a document has no desire to be\n> contacted by millions of people on the net.\n\nThe response From: could and should be optional as the request From:\n\n> Link: <mailto:e-mail_address>; REL=\"made\"\n\nThis would have to be REV=\"made\", not REL.\n\nFrom: would provide better compatibility with the existing tools for\nrfc822 messages than Link. \n\n\n\n"
        },
        {
            "subject": "Re: Suggestion for HTTP 1.",
            "content": "In message <ab973a1d0802100484fc@[129.106.201.2]> of Thu, 23 Mar 1995\n  14:41:34 -0600, cshotton@biap.com (Chuck Shotton) wrote:\n\n> What's more, HTTP should be considered transport independent. Relying on\n> something as cheesey as twaddling the underlying protocol to do something\n> that should be under conscious control between the client and server is a\n> gross hack. Either build the mechanism into HTTP or live without it.\n> Hacking the transport layer as a means to this end is a recipe for wreck\n> and ruin the minute someone sticks this protocol on top of something\n> besides TCP/IP. Remember, WWW != Unix and HTTP != TCP/IP.\n\nWell, of course it's a cheesey hack.  But to continue the distraction for\na moment, I note that all transports of note have a flow-control element\nwhich can be abused to implement \"tentative download\".  Jeff Mogul's\ncomments on SWS are well taken: I'd forgotten about SWS.\n\nI still think it should be done using uri;bytes=...\n\n-- \nAdrian.Colley@sse.ie   <g=Adrian;s=Colley;o=SSE;p=SSE;a=EIRMAIL400;c=ie>\nphones:- work: +353-1-6769089; fax: +353-1-6767984; home: +353-1-6606239\nemployer: Software and Systems Engineering (+=disclaimer)  (Perth)->o~^\\\nY!AWGMTPOAFWY? 4 lines, ok? qebas perl unix-haters kill microsoft  \\@##/\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "At 10:15 PM 3/23/95, Roman Czyborra wrote:\n>> >1. Why is From: only a request header?  It would be nice to provide\n>> >   the address of the file owner so you know where to send comments\n>> >   if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n>>\n>> Sure, it'd be nice. It's probably impossible to implement in an efficient\n>> fashion on most servers today.\n>\n>No, not at all.  On Unix, every file already has an owner, and I could\n>hack my httpd in no time to construct an email address from it.\n\nIn real life the Unix UID that \"owns\" a document may have nothing to do\nwith the author,  for example on a CWIS where everything is posted by a few\npeople.\n\nThere are already HTML solutions in wide use for indicating/contacting the\nauthor of HTML docs, including <LINK REV=\"made\" HREF=\"mailto:...\"> and\nmailto: HREFs in the body of the document. It's not worth messing with the\nHTTP protocol to add another, IMHO.\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: HTTP Timezone heade",
            "content": "At 11:03 AM 3/23/95, David Robinson wrote:\n>A suggestion for HTTP: that clients send a Timezone header in the request,\n>describing the timezone of the user.\n\nOn first glance, this seems useful. However, I'm not sure I see what kind\nof script would need to know the timezone of the user. Most time-dependant\nCGI scripts I can think of will always be talking about the time in the\nzone in which the server resides. Maybe you want to be able to reply with\nsomething like \"At xx:yy (your time) you said...\", but that seems like a\nstretch, particularly if the user doesn't set the timezone correctly. Toss\nin various daylight savings times and so on, and I think you introduce more\nuncertainty than you want.\n\nIt seems safer to base all times on the server's (hopefully correct) time.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "At 9:15 PM 3/23/95, Roman Czyborra wrote:\n>> >1. Why is From: only a request header?  It would be nice to provide\n>> >   the address of the file owner so you know where to send comments\n>> >   if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n>>\n>> Sure, it'd be nice. It's probably impossible to implement in an efficient\n>> fashion on most servers today.\n>\n>No, not at all.  On Unix, every file already has an owner, and I could\n>hack my httpd in no time to construct an email address from it.\n\n#Up on the soapbox..#\n\nFirst, not every server runs on Unix. (only about half do.) Assuming that a\nfeature in the protocol is easy to implement based on the effort to hack it\ninto Unix is not a valid measure.\n\nSecond, not every server serves files out of a file system, and not every\nfile system has file owners. If the server sits on a database, who's the\nowner now?\n\nThird, there is no correlation between an operating system-specific \"owner\"\nof a file and the author of the file, and it certainly is NOT the case that\nthe owner of a file maps to the contents of a \"From:\" header field.\n\nFourth, this field has no place as a required header field for reasons I\nmentioned earlier (not the least of which is a desire for privacy.) If it's\nnot required, then it is of little value. The bottom line is that\n\"authorship\" of content should be specified in the content or in the URN.\nIt has no place in the transport protocol that moves content of\nindeterminent type from point A to point B.\n\n##Off the soapbox##\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": ">In real life the Unix UID that \"owns\" a document may have nothing to do\n>with the author,  for example on a CWIS where everything is posted by a few\n>people.\n\nFurther, for security, many systems have the owners of many or all\ndocuments have \"noshell\" login accounts where mail would go and probably\neither never be read or get forwarded to someone who knows nothing about\nthe content of the file.\n\nThe return email address is much more likely to be useful if it is part of\nthe content of the document, not generated automatically by the server.\n\n\n\n"
        },
        {
            "subject": "Re: How are/should redirects of POSTS be handle",
            "content": "Jeffrey Perry writes:\n>What appears to happen when a POST is redirected is the client attempts a\n>GET at the new URL so that the user can fill-in the form again (it may have\n>changed when it moved)\n\nThere are several pages on the Web (\"The Revolving Door\" comes to mind)\nwhich, on the basis of form input, redirect you to one of a number of\ndifferent pages.  These pages then need to be retrieved using GET.  This\nusage appears to be much more common than a redirect where the client is\nsupposed to POST again on the target system.\n\nAt a very early stage of development, Enhanced Mosaic would actually do\nanother POST after a redirect.  In every case that this happened, it\ngenerated incorrect results (like an \"Action not allowed\" response).  I\nthink that switching to GET after a redirect is current practice for most\nbrowsers.\n\n--\nJim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Which usual media types can a server sen",
            "content": "Owen Rees said in <9503171749.AA01191@plato.ansa.co.uk>:\n\n> In section 5.4.1 of draft-ietf-http-v10-spec-00, it says that \"*/*\" means all \n> media types but then goes on to imply that this excludes \"unusual\" media \n> types. It then goes on to suggest that the definition of unusual should be a \n> configurable aspect of the user agent.\n> \n> Since the server can have no way of knowing what the client/user agent has \n> been configured to consider \"unusual\", it is forced to treat \"*/*\" not as \n> \"all\" but as \"don't know\". The server must therefore use its own definition of\n> \"unusual\" in determining whether or not a media type is acceptable. This seems\n> to be undermining the whole purpose of the Accept header.\n\nYes, it is clear that I cut one too many corners in trying to finesse this.\nThe notion of \"unusual\" will have to go.\n\n> Having \"*/*\" really mean \"all\", and this being the default, does create the \n> problem that a server can send an x-perimental/bizzare entity without a \n> content-length on the grounds that it is self-describing. If the server keeps \n> the connection open waiting for the next request, and the client does not \n> recognise that it has all of the entity then we have a deadlock.\n\nHmmmm, this is a second issue; it means we will also have to give up any\nnotion of having the end-of-entity indicated by media type. *sigh*\n\n> I do not like having */* potentially mean different things to each individual \n> client and server. Here are some other options to consider:\n> \n> 1) */* does not mean all, it means some specified set of media types\n> 2) */* is not the default; some specified set of media types is the default\n> 3) */* is the default and really means all - live with the possible deadlock\n> \n> I am not particularly happy with any of these, but I think that the \n> uncertainty about the meaning of */* is worse.\n\nThe answer is (3), and we will remove the possibility of deadlock.\n\n==========================================================================\nMark Hedlund added in\n<Pine.SUN.3.91.950321124309.15933B-100000@thetics.europa.com>:\n\n> Some examples:\n> \n>     * It is neither configurable nor unusual for Netscape 1.1 to \n> accept tables -- every copy of 1.1 does, and if you don't like it, stick \n> to 1.0 or another browser.  Do tables constitute an 'unusual' media type \n> because not every server uses them?  If so, why is 'unusual' determined \n> on the browser's side?  Should Netscape 1.1 be sending 'Accept: */*; \n> q=0.5, text/x-html-with-tables'?  If so, should other browsers be forced to \n> save text/x-html-with-tables to a file?\n\nThat was the original intention, yes.\n\n>     * It is not currently unusual for Lynx 2.3.8 to accept HTML \n> without tables or math.  A year from now (or five) when HTML 3.0 has been \n> released, should Lynx 2.3.8 start considering its requests for HTML, by \n> which it means HTML 2.0, 'unusual'?  If the user hasn't the sense to \n> upgrade, will the user nonetheless be expected to run Lynx in \n> 'anachronistic mode'?\n\nNo -- unusual would only apply to \"new\" or experimental types\n(which causes a later problem, but hey -- one at a time ;-).\n\n>     * I prefer HTML to PostScript, and let's pretend my browser knows that.\n> When downloading the HTML/1.0 draft to print out, however, I prefer \n> PostScript to HTML.  Should I open my preferences, reset my quality \n> values, make the request, and then return my quality values to their \n> standard settings?\n\nYes, if what you want is to receive that content by default when requesting\na negotiated URL.  However, what most people will do is get the HTML\nfirst (or some higher level document), and select from it a link to\nthe PostScript-specific version.  In other words, content negotiation\ncannot obviate the need for content-specific pointers.\n\n> Some suggestions:\n> \n>     * 'Unusual' should be defined more strictly.  One possibility \n> would be in reference to IANA registered media types.\n\nYikes!\n\n>     * As new browsers implement wider acceptance of _usual_ media types \n> (i.e., HTML 3.0, image/jpeg, whatever), they should be encouraged to \n> explicitly list these media types in their accept headers.  This will \n> allow server maintainers to avoid \"saving to file\" whenever possible \n> (i.e., send gif unless they _say_ send jpeg).\n\nNo.  The only valid reason for preemptive content negotiation is the *hope*\nthat it will save additional network transfers and bandwidth by getting the\nuser what they want the first time.  If, as is often the case, every\nclient spits out 1k worth of Accept types on every request (even though\nless than 1% of the Web is ever negotiable), preemptive content negotiation\nhas failed its purpose.\n\n>     * The Content Negotiation section could remove its language about \n> choosing from several acceptable media types \"possibly at random,\" and \n> instead suggest that when different versions of an entity exist, the \n> default should be the version most likely directly viewable by the oldest \n> browsers (text/html, text/plain, image/gif).\n\nThat is one alternative, but should be decided by the server.  After all,\nHTTP may be used by non-WWW applications.\n\n>     * The browser should be able to indicate that it will accept */*, \n> but if different versions exist, it specifically _wants_ a \"300 Multiple \n> Choices\" response (if you have more than one version, let me see what \n> you have).\n\nHmmmm, that would be nice -- maybe as part of the feature extensions \nthingy in HTTP/1.1 (not yet defined, but analogous to Dave Kristol's\nproposal).\n\n>     * Similarly, browser authors should be encouraged to allow \n> changes to the Accept quality values on a per-request basis.\n> \n>     * A browser should be allowed to specify a preference for certain \n> \"usual\" media types without the user configuring it to do so!  The \n> language I quoted above doesn't seem to allow for that.\n\nWell, it should.  I'll check that and be sure it does next time.\n\n==========================================================================\n\nand Jim Seidman furthered in <9503221520.AA29145@hook.spyglass.com>:\n\n> I'd also like to quibble over the statement \"If at least one Accept header\n> is present, a quality factor of 0 is equivalent to not sending an Accept\n> header field containing that media-type or set of media-types.\"  Does this\n> mean that these lines are identical?\n> \n> Accept: application/octet-stream; q=0, application/* \n> Accept: application/*\n> \n> It seems like it would be more useful if q=0 could be used to explicitly\n> disallow a certain type.  Comments?\n\nI guess that is reasonable -- I'll try to work it into the algorithm.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Comments on HTTP/1.0 draft [March 8, 1995",
            "content": "> Herewith comments on the latest draft, as requested.  Nothing too\n> serious!\n> \n> 3.1 HTTP Version, para starting \"HTTP servers are..\"\n> \n>   a. [nit] \"i.e.\" needs a following comma (possibly elsewhere also)\n\nYep, force of habit -- I'll do a search for the rest (and e.g. as well).\n\n>   b. <major> should be <major+1> ?\n\nNope -- we can't require a server to understand major versions\ngreater than their native version.  Ummm, I guess the words around\nit could be more explicit in not including <major+1>.\n\n> 3.3.1 Full date\n> \n>   a. I haven't ever seen the asctime() format arrive at any of my\n>      servers -- can this be dropped as a requirement?  RFC 850 does\n>      still seem to be around, however.\n\nI've only seen asctime() from some PC-based servers -- no browsers.\nI don't see any value in dropping it beyond the existing requirement\nthat it never be generated.\n\n>   b. \"recipients of date values should be robust in accepting...\"\n>      -- this is really too vague to be implementable.  Make it a\n>      non-normative note or (better) move to Appendix C.?\n\nI'd prefer to have such things close to the topic's text, so I guess\nthat means another note.\n\n> 4.3.3 Message-ID\n> \n>   a. I implemented this as per the previous draft and found it useful\n>      for testing, but I agree that it should not normally be generated.\n>      I now generate it only in a 'Test mode'.  However, the new text\n>      forbids this.  Add this case as a possibility?\n\nYes, I think we got a little overexcited in that section -- it should\nnot be forbidding anything (except malformed values).\n\n> 5.2.3 POST\n> \n>   a. [nit] change \"usually a form\" to \"such as the result of submitting\n>      a form\" (or something like that)?\n\nokay\n\n> 6.1 Status-line\n> \n>   a. [nit] The last sentence (\"Although...\") should be a note?\n\nActually, it clarifies an ambiguity in the BNF, so I'd like to leave\nit in the body.  I will make it a little less wimpy.\n\n> 6.2.1 202 Accepted\n> \n>   a. \"Any method\" seems a bit strong .. doesn't seem very useful for GET\n>      or HEAD.\n\nThat would depend on what was got.  I don't want to forbid something\njust because it may not be useful -- if it's not useful, people won't use it.\n\n> 6.2.3 407 PAR\n> \n>   a. [nit] \"will be available in future versions\" puts a constraint on\n>      the future (and future standards processes). Weaken?\n\nAgreed -- I'll delete evrything after the \"--\".\n\n> 6.3.1 Public\n> \n>   a. Don't understand the \"applies only to the current connection\".\n>      Since the request has already been received, and the response\n>      connection is about to be closed, this implies that the information\n>      must immediately be discarded, and is hence useless?\n\nEr, bad terminology I guess.  That was meant to mean \"applies only for\nthe nearest neighbor in a connection chain\" (i.e., the closest server).\n\n> 6.3.2 Retry-After\n> \n>   a. [nit] change /an full/a full/\n\nNix the \"full\".\n\n> 7.1 Entity Header Fields, Note\n> \n>   a. \"It has been proposed..\" probably should be moved to HTTP/1.1?  In\n>      particular, duplication of keywords in two separate address spaces\n>      between two different layers of protocols (HTTP and HTML) is bound\n>      to lead to problems in the future (standards will have to be\n>      tightly bound and coordinated).\n> \n>   b. \"Base will be used...\" same comment as 6.2.3 above.\n\nOkay.\n\n> 7.1.1 Allow\n> \n>   a. [nit] A cross-reference back to Public (6.3.1) similar to the Allow\n>      reference there, would be helpful.  (Or drop the earlier\n>      cross-reference.)\n\nOkay.\n\n> 7.1.4 Content-Length\n> \n>   a. Add sentence to the effect that if Content-Length is not specified\n>      on a request, and the server does not recognize or cannot calculate\n>      the length from other fields, then 400 Bad Request may be returned.\n\nThat doesn't say much (400 Bad Request can always be returned), but I get\nthe drift.  It is more appropriate than:\n\n>      [aside] I still would prefer that Content-Length be required on\n>      requests with entity data, as it allows a too-large request to be\n>      rejected before reading an excess of data first.  Perhaps for 1.1?\n\nThat would have to be approved by a resounding consensus, as it would\nremove the possiblity for clients to use packetized encodings to submit\nrequest data of indeterminate size in any of the 1.x protocols.  This\nmay not be a big deal, but I'm not willing to make that decision.\n\n> 7.1.8 Expires\n> \n>   a. [nit] The note could be moved to Appendix C.\n\nI'd rather not.  In fact, appendix C will probably move to Notes.\n\n> 7.1.9 Last-Modified\n> \n>   a. [nit] Spell out 'last-mod'?\n\nOkay.\n\n> 7.1.11 Location\n> \n>   a. This (\"considered obsolete\") is inconsistent with 7.1.13 URI, which\n>      encourages both.  It would perhaps be better to formalize Location\n>      as a useful special-case of URI (especially as it is very much\n>      common current practice).  Otherwise, servers must wastefully\n>      generate both indefinitely (and clients have no incentive to\n>      implement URI, perhaps).\n\nYes, I've been thinking of that as well.  Location is necessary for\nauto-redirects (and can serve as the default URI if URI is not given).\nAt the same time, URI should be required for URLs subject to content\nnegotiation variants.\n\n>   b. What happens if both Location and URI are specified, but differ?\n>      It would be better if one or other (only) were permitted?\n\nNo, they serve different needs.  They are quite likely to differ\n(e.g., you won't find a URN in a Location header).\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: v10-spec00 comment",
            "content": "> Comments on draft-ietf-http-v10-spec-00. The comments on Accept, Message-ID \n> and Version are more important than the others.\n> \n> 3.2 Universal Resource Identifiers\n> \n> Should refer to RFC1738 as the standard for URLs including the http scheme, \n> escaping rules and allowed characters.\n\nRFC1738 is not yet a standard, but it will indeed be referenced in that\nsection.  RFC1738 does not fully define http URLs -- it assumes that\nthey will be defined within the HTTP specification.\n\n> 3.3.1 Full Date\n> \n> RFC 1123 (5.2.14) says:\n>         \"There is a strong trend towards the use of numeric timezone\n>          indicators, and implementations SHOULD use numeric timezones\n>          instead of timezone names.  However, all implementations MUST\n>          accept either notation.\"\n> \"+0000\" should at least be permitted as an alternative to \"GMT\" in the \n> \"updated by RFC 1123\" case.\n\nI have received two comments to that effect.  Since HTTP has never allowed\n\"+0000\" and no application uses it, I will not put it in the HTTP/1.0 spec.\nHowever, we can talk about it for 1.1.\n\n> 4.3.2 Forwarded\n> \n> The final sentence about hiding internal hosts should say that existing \n> Forwarded headers (added by proxies inside the firewall) should be removed.\n\nokay\n\n> 4.3.3 Message-ID\n> \n>   The first paragraph specifies that this is a unique identifier for the \n> message, presumably the HTTP request or response message. The final paragraph \n> says that an HTTP response should only include a Message-ID header if the \n> entity has one. Since it is possible to retrieve a Mail/News message more than\n> once, the HTTP Message-ID cannot be the Message-ID of the enclosed entity as \n> this would violate the unique identification property for the HTTP response \n> message.\n\nNo it would not.  There is only one message in that case.  This situation\nis identical to the handling of messages by proxies.\n\n> My preferred solution is to remove Message-ID from HTTP altogether, but if it \n> is retained it cannot be both a unique identifier for an HTTP message and \n> imported from Mail/News/etc. by a gateway.\n\nIt can if the HTTP server is a gateway to Mail/News/etc.\n\n> 4.3.4 MIME-Version\n> \n> I would have classified this as an entity-header rather than a general-header.\n\nIt has nothing to do with the entity -- only the message syntax.\n\n> 7.1.1 Allow\n> \n> I would have classified this as a response-header rather than an \n> entity-header; it is required with 405 Method Not Allowed and will not be \n> meta-information about the entity containing an explanation of the error.\n\nUnless it is included in a PUT or POST of a new resource.\n\n> 7.1.14 Version\n> \n> \"A user agent can request a particular version of an entity by including its \n> tag in a Version header as part of the request.\" How should Version be \n> interpreted in a POST request? It could refer either to the version of the \n> entity, or to the version of the resource to which the entity is to be made \n> subordinate. What is the existing practice in this case?\n\nIt should be interpreted as the suggested version of the entity.\nThe above quoted sentence will be changed to refer only to non-content\nrequests.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Quality factors (http draft 8th March",
            "content": "> Somewhat of a detail, this, but anyway:\n> In section 5.4.1 Accept, you say:\n> float  = <  ANSI-C floating point text representation,\n>             where (0.0 < float < 1.0) >\n> What, precisely, do you mean by this? You don't give a reference, so I could\n> imagine the following:\n\n[imagination deleted]\n\nYeah, I never liked this myself -- it's a leftover from the old spec.\nI would prefer\n\n     float = ( \"0\" [ \".\" 0*3DIGIT ]\n             |     ( \".\" 0*3DIGIT )\n             | \"1\" [ \".\" 0*3(\"0\") ] )\n\nWould that break any existing systems?\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: charset overlabelin",
            "content": "> Section 8.1.1 of the current draft says:\n> \n>    HTTP does not require that the character set of an entity body be \n>    labelled as the lowest common denominator of the character codes \n>    used within a document.\n> \n> I suggest you change this into\n> \n> It is recommended but not required that ...\n\nokay\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "> I've got two questions regarding 5.4.6 From\n> \n> 1. Why is From: only a request header?  It would be nice to provide\n>    the address of the file owner so you know where to send comments\n>    if the author forgot to add a <LINK REV=\"made\" HREF=\"mailto:...\">\n\nSome people \"forget\" on purpose.  A link is more appropriate in this case.\n\n> 2. Why is it an addr-spec only instead of a full RFC822 mailbox?\n\nBecause for some reason I never noticed the mailbox definition (I just\ndid when I looked at RFC 1123).  That will be fixed.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: HTTP Timezone heade",
            "content": "On Thu, 23 Mar 1995 14:26:53 -0700, Paul Hoffman wrote:\n>>A suggestion for HTTP: that clients send a Timezone header in the request,\n>>describing the timezone of the user.\n>\n>On first glance, this seems useful. However, I'm not sure I see what kind\n>of script would need to know the timezone of the user. Most time-dependant\n>CGI scripts I can think of will always be talking about the time in the\n>zone in which the server resides. Maybe you want to be able to reply with\n>something like \"At xx:yy (your time) you said...\", but that seems like a\n>stretch, particularly if the user doesn't set the timezone correctly. Toss\n>in various daylight savings times and so on, and I think you introduce more\n>uncertainty than you want.\n\nThe applications I have in mind are bulletin board or bug-tracking systems.\nIndividual files contain messages from users, stamped with dates/times in\neither server time or GMT, which is no problem. However, I also provide an\n'index' of recent messages sorted by date, so the users can check for any\nupdates since they last read the messages. Having that list presented in the\nuser's local timezone would make it much more intelligable; otherwise they\nwould have try to figure out whether they have already seen a posting\nat '13:25 CET' when they know they last checked the system at 5:15 PST.\n\n>It seems safer to base all times on the server's (hopefully correct) time.\nI think you are assuming some penalty value for error which may not apply\nin my applications. And I don't believe that _all_ time information would\nsuffer from an attempt at presenting it in the users timezone. I certainly\nwasn't suggesting that use of the Timezone information be mandatory.\n\n David Robinson.\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: HTTP Timezone heade",
            "content": "At 10:39 AM 3/24/95, David Robinson wrote:\n>On Thu, 23 Mar 1995 14:26:53 -0700, Paul Hoffman wrote:\n>>>A suggestion for HTTP: that clients send a Timezone header in the request,\n>>>describing the timezone of the user.\n\n>The applications I have in mind are bulletin board or bug-tracking systems.\n>Individual files contain messages from users, stamped with dates/times in\n>either server time or GMT, which is no problem. However, I also provide an\n>'index' of recent messages sorted by date, so the users can check for any\n>updates since they last read the messages. Having that list presented in the\n>user's local timezone would make it much more intelligable; otherwise they\n>would have try to figure out whether they have already seen a posting\n>at '13:25 CET' when they know they last checked the system at 5:15 PST.\n\nThis is protocol \"candy\". If you need to know the user's timezone for your\nBBS application, why don't you ask for it when you collect their name,\naddress, etc.? There are plenty of message oriented systems on the Internet\ntoday that do not have synchronized time bases, yet are able to present\nusers with messages sorted by date. And the users seem to be able to figure\nout which messages have been read or not.\n\nTwo particular systems that work this way spring to mind -- SMTP E-Mail and\nUseNet News.\n\nAnd if you REALLY have to know the user's timezone without asking, how do\nyou expect to accomodate users with machines in more than one location, or\nusers with portable computers? Your best bet is to build a Whois look-up\ninto your BBS application and find the physical location of the server,\nrather than complicating the date information in HTTP by adding local\ntimezone info which would diminish the standard time base we have with GMT.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: v10-spec00 comment",
            "content": "\"Roy T. Fielding\" <fielding@avron.ICS.UCI.EDU> writes:\n> > > 4.3.3 Message-ID\n> > \n> >   The first paragraph specifies that this is a unique identifier\n> > for the message, presumably the HTTP request or response\n> > message. The final paragraph says that an HTTP response should\n> > only include a Message-ID header if the entity has one. Since it\n> > is possible to retrieve a Mail/News message more than once, the\n> > HTTP Message-ID cannot be the Message-ID of the enclosed entity as\n> > this would violate the unique identification property for the HTTP\n> > response message.\n> \n> No it would not.  There is only one message in that case.  This situation\n> is identical to the handling of messages by proxies.\n\nHere is the problem scenario:\n\nCharles sends one piece of e-mail to both Alice and Bob, Charles' MUA assigns \nMessage-ID M to this message.\nAlice retrieves her e-mail via proxy AP1 to gateway AG.\nBob retrieves his e-mail via gateway BG.\n\nHTTP traffic (without authentication):\n\n1  Alice->AP1 request  GET uri-for-the-mail(via-AG)\n2  AP1->AG    request  GET uri-for-the-mail; Forwarded: AP1 for Alice\n3  AG->AP1    response Message-ID: M3 <the mail>\n4  AP1->Alice response Forwarded: AP1 for AG;Message-ID: M4 <the mail>\n5  Bob->BG    request  GET uri-for-the-mail\n6  BG->Bob    response Message-ID: M6 <the mail>\n\nNote that M3=M4=M6=M and \"is a unique identifier which can be used for \nidentifying the message (not its contents)\" if the implied copying of M out of \nthe mail headers is done.\n\nAP1 is supposed to add a Forwarded header so message 4 is not the same \n\"structured sequence of octets\" (i.e. message as defined in 1.3) as message 3.\n\nSince the email arriving at AG and BG will have different Received headers (if \ntransmitted by SMTP), messages 3 and 6 cannot have the same \"structured \nsequence of octets\" unless these headers are stripped out, along with anything \nelse that may have been added (what if Bob's copy was Resent-To him by Diana, \nthose headers had better go too!).\n\nEven if AG and BG are the same host and the mail transport notices this and \nsends one copy so that the Received headers have the same timestamps, the \nsameness requirement prohibits the use of the Digest-MessageDigest header in \nthe response if digest access autentication is being used in the interaction \nwith the gateway.\n\nTracing messages through chains of proxies has been suggested as a reason for \nhaving Message-ID, but consider what happens if Bob also uses AP1, but as a \nproxy to BG. We now have a situation where AP1 handles two response chains, \nAG->AP1->Alice and BG->AP1->Bob, but both are labelled with the same \nMessage-ID.\n\n> > 7.1.1 Allow\n> > \n> > I would have classified this as a response-header rather than an \n> > entity-header; it is required with 405 Method Not Allowed and will not be \n> > meta-information about the entity containing an explanation of the error.\n> \n> Unless it is included in a PUT or POST of a new resource.\n\nPUT perhaps, but not POST. In a POST, the entity body is to be made \nsubordinate to the resource identified by request-URI. It may be convenient to \nsimultaneously attach a subordinate and specify methods that are to be allowed \non the resource to which it is to be subordinate, but if this is the intent it \nneeds to be made clearer. If the Allow is intended to apply to the entity-body \nin a POST, then the URI-header rather than the Request-URI identifies the \nresource.\n\nWith PUT, Allow would be an entity-header specifying the methods to be \nsupported. If this is a current use, then it should be explained.\n\nIn a 405 response, Allow is meta-information about the resource identified by \nthe Request-URI, it is not meta-information about the entity-body in the \nresponse.\n\nThere seem to be two distinct uses of Allow: information from a server about \nan existing resource, and request parameter from a client when sending a \nresource. The union of these usages does not fit the header taxonomy, so I \nassume that using the same header name for both purposes is a consequence of \ncurrent usage.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "[Dave - this is CC'd to the http working group, but not to the newsgroups]\nDave Garaffa writes:\n> <SNIP>\n> >\n> > I would be in your debt if you could take things one step further.  I am\n> > in the process of breaking down the browsers into platform specific pages\n> > (so if you want to find out the latest version of say a Mac or Windows\n> > browser you are not hit with *every* browser for *every* platform. The\n> > problem is we have no way to automate the task of finding out what\n> > platform the browser is running on.\n[...]\n> \n> A usenet no-no but I am following up to my own post...\n> \n> Bill Perry at Spry send me an e-mail (it might be on the usenet as well\n> but my server doesn't show it yet) and passed the following:\n> \n> > >      Microsoft Internet Explorer/1.0alpha (Windows 95)\n> > >      Mozilla/1.1b2 (X11; U.S.; OSF1 V3.0 alpha)\n> > >      NCSA Mosaic/2.0.0b2rc7 (Windows x86)\n> > >      NCSA Mosaic/2.6a1 (Unix/X11)\n> >\n> >   Do you realize that only Mozilla did this right?  You _CANNOT_ have\n> > spaces in a product name.  And your parser at BrowserWatch is a little\n> > hosed.  I just sent it:\n>\n> And yes Bill is right... after RE-READING the spec it does indeed\n> indicate the <product> as a <word> and to quote the spec:\n> \n> The first white space delimited word must be the software product name,\n> with an optional slash and version designator. Other products which form\n> part of the user agent may be put as separate words.\n> \n> so for a browser to be \"up to spec\" the would have to use the following:\n> \n> NCSA_Mosaic/2.0.0b1 Macintosh/680x0 WWWLib/ver Proxy/ver\n> NCSA_Mosaic/2.0.0b1 UNIX/X11 WWWLib/ver Proxy/ver\n> Quarterdeck_Mosaic/0.0a3 Windows/16bit WWWLib/ver Proxy/ver\n> Quarterdeck_Mosaic/0.0a3 Windows/32bit WWWLib/ver Proxy/ver\n> TCP_Connect_II/0.0a1 Macintosh/PPC WWWLib/ver Proxy/ver\n\n  Shouldn't the User-Agent field follow 'normal' specs for headers and do\nsomething like:\n\nUser-Agent: agent=NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0\n            library=WWWLib/3.0.x ; other=Whatever\n\n  This would make it a little easier for people like Dave to parse the\nstring, without having to worry about order of the arguments, etc.  Plus\nthis would allow whitespace (but not ;) in the specifier.\n\n-Bill P.\n\nPS: I just sent my subscription request to http-wg, so please CC me on any\n    responses for a while to make sure I get them.\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "On Fri, 24 Mar 1995 wmperry@spry.com wrote:\n>   Shouldn't the User-Agent field follow 'normal' specs for headers and do\n> something like:\n> \n> User-Agent: agent=NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0\n>             library=WWWLib/3.0.x ; other=Whatever\n> \n>   This would make it a little easier for people like Dave to parse the\n> string, without having to worry about order of the arguments, etc.  Plus\n> this would allow whitespace (but not ;) in the specifier.\n\nYes, I like Bill's proposal and I've suggested platform in the user-agent \nfield before.  This would, however, completely break current systems \n(which tend to look for the text before the first / as user-agent name, \nand can survive miscreant whitespace) that currently work with HTTP/1.0 \nrequests.  Can we defer this to 1.1?\n\nM. Hedlund <march@europa.com>\n\n\n\n"
        },
        {
            "subject": "Release of WRL &quot;persistentconnection&quot; HTTP implementatio",
            "content": "Since several people have asked us for this, we have finally released\nour modified NCSA Mosaic and NCSA httpd sources, implementing an\nexperimental persistent-connection version of HTTP.  \n\nThese modifications are for non-commercial use, and should in any\ncase be considered experimental and unsupported.  We don't promise\nto fix anything.\n\nThe files are in\n        ftp://ftp.digital.com/pub/DEC/net/phttp.tar.Z\n\nPlease carefully note the copyright terms.\n\nSee\n   http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html\nfor more details.\n\n-Jeff Mogul <mogul@wrl.dec.com>\n Venkata Padmanabhan <padmanab@ICSI.Berkeley.EDU> (who did the work!)\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "M. Hedlund writes:\n> \n> On Fri, 24 Mar 1995 wmperry@spry.com wrote:\n> >   Shouldn't the User-Agent field follow 'normal' specs for headers and do\n> > something like:\n> > \n> > User-Agent: agent=NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0\n> >             library=WWWLib/3.0.x ; other=Whatever\n> > \n> >   This would make it a little easier for people like Dave to parse the\n> > string, without having to worry about order of the arguments, etc.  Plus\n> > this would allow whitespace (but not ;) in the specifier.\n> \n> Yes, I like Bill's proposal and I've suggested platform in the user-agent \n> field before.  This would, however, completely break current systems \n> (which tend to look for the text before the first / as user-agent name, \n> and can survive miscreant whitespace) that currently work with HTTP/1.0 \n> requests.  Can we defer this to 1.1?\n\n  Perhaps we could combine the two?\n\nUser-Agent: NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0 ;\n            library=WWWLib/3.0.x ; other=Whatever\n\n  This would still let old services that look for whatever is before the\nfirst / to still get the same results.  And actually is more MIME-y than my\nfirst one, since the platform=xxxx, can now be considered\nparameters/extensions to the base 'NCSA Mosaic' value.\n\n  -Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": ">M. Hedlund writes:\n>>\n>> Yes, I like Bill's proposal and I've suggested platform in the user-agent\n>> field before.  This would, however, completely break current systems\n>> (which tend to look for the text before the first / as user-agent name,\n>> and can survive miscreant whitespace) that currently work with HTTP/1.0\n>> requests.  Can we defer this to 1.1?\n>\n>  Perhaps we could combine the two?\n>\n>User-Agent: NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0 ;\n>            library=WWWLib/3.0.x ; other=Whatever\n>\n>  This would still let old services that look for whatever is before the\n>first / to still get the same results.  And actually is more MIME-y than my\n>first one, since the platform=xxxx, can now be considered\n>parameters/extensions to the base 'NCSA Mosaic' value.\n>\n>  -Bill P.\n\n\nBill,\n\nI too like your proposal (2nd one *is* better) and less breaking... Where\ncan we go to propose this (or something like this)?\n\nDave,\n\n---------------------------------------------------------------------\n Dave Garaffa                   | email      d-garaffa@ski.mskcc.org\n SKI Computer Support Services  | voice      (212) 639-8588\n Sloan-Kettering Cancer Center  | fax        (212) 717-3627\n---------------------------------------------------------------------\n\"You know you've been using the web WAY too long when...\n   you access your hotlist and the lights in your office dim.\"\n\n                 Yuji Shinozaki <yuji@chem.duke.edu>\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "M. Hedlund writes:\n> \n> [this is just to Bill and Dave -- not the http list.]\n> \n> On Fri, 24 Mar 1995 wmperry@spry.com wrote:\n> > M. Hedlund writes:\n> > > \n> > > Yes, I like Bill's proposal and I've suggested platform in the user-agent \n> > > field before.  This would, however, completely break current systems \n> > > (which tend to look for the text before the first / as user-agent name, \n> > > and can survive miscreant whitespace) that currently work with HTTP/1.0 \n> > > requests.  Can we defer this to 1.1?\n> > \n> >   Perhaps we could combine the two?\n> > \n> > User-Agent: NCSA Mosaic/2.0.0b1 ; platform=Macintosh/680x0 ;\n> >             library=WWWLib/3.0.x ; other=Whatever\n> \n> Yes, I like this even better.  My hesitation about asking that it go into\n> HTTP/1.0 stems from:\n\n> (1) my feeling that we can live without it for 1.0, and that even if we\n> did ask for it in 1.0, 1.1 might be finalized before anyone other than\n> Bill implements it this way :); and\n\n  Well, I haven't looked at what Spyglass 2.0 sends, but the only browsers\nI know of that send this even remotely correctly right now are Emacs-w3,\nAIR Mosaic, and a beta version of netscape set to expire in 7 days.\n\n> (2) deference to Roy's workload -- in addition to his effort to\n> finalize HTTP/1.0 before _tomorrow_, he's also just taken on the cleanup\n> of the HTML/2.0 spec.\n\n  Roy, are you insane? :)\n\n> I hate to say I think it's too late because I hate that sort of answer,\n> but I guess I would ask that you consider the urgency of this request.  A\n> conversation on the User-Agent header took place on the http-wg list just\n> recently; see\n\n> <URL:http://www.ics.uci.edu/pub/ietf/http/hypermail/current/0032.html>.\n\n  Sorry for not perusing this beforehand...\n\n  Since 'current practice' is so varied (invariably in the wrong direction\n:), I can go one way or another.\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "Bill Perry writes:\n>  Well, I haven't looked at what Spyglass 2.0 sends, but the only browsers\n>I know of that send this even remotely correctly right now are Emacs-w3,\n>AIR Mosaic, and a beta version of netscape set to expire in 7 days.\n\nWe send something like:\nEnhanced_Mosaic/2.00 Win32 Spyglass/9\n\nThe Enhanced_Mosaic token is sent by all of the versions made by our\nlicensees (all of which have slightly different retail product names).  The\nsecond token varies by platform and processor (so it could be something like\nMac_PowerPC), and the third token specifies the vendor of this specific\nversion and the build number of this vendor's customized version.\n\nOf course, since 2.00 is already shipping, it's a little late for us to\nchange it now, regardless of the outcome of this discussion...\n\n--\nJim Seidman, Senior Software Engineer, Spyglass Inc.\n\n\n\n"
        },
        {
            "subject": "Re: Digest Access Authentication proposal  &quot;Authorization&quot",
            "content": "zurko@osf.org (Mary Ellen Zurko) writes:\n> > The credentials are authorization information; \"This request is from <username>\n> >  who claims the right to access <requested-uri> in <realm>\" \n> \n> I'm not sure I see what sense you consider this authorization\n> information. As I pointed out, even clear authentication information\n> is an input to authorization decisions, so it's not incorrect to call\n> even the digest \"information for the authorization decision\". It does,\n> however, confuse people to call either the digest or the credentials\n> (in this case) simply \"authorization information\".\n\nI think I should have said \"The credentials contain authorization\ninformation\" (rather than \"are\"). draft-ietf-http-v10-spec-00 says\n\"credentials containing the authentication information\" (in 5.4.5 and\n10), and I would argue that in both the Basic and Digest schemes, the\ncredentials contain information used both in authentication and in\nauthorization, but not all of the information used in either\nprocess.\n\nSince the credentials contain information used for both purposes, it\nmight be better to include the syntax rule for the Authorization\nheader in digest-aa so that \"<credentials>\" can be used to replace\n\"Authorization\" in various places. I think I was probably reading the\ncapitalised word as an arbitrary token naming the header containg the\ncredentials, without thinking how it would read if the capitalisation\nwere removed or ignored.\n\nRegards,\n  Owen Rees <rtor@ansa.co.uk>\nInformation about ANSA is at <URL:http://www.ansa.co.uk/>.\n\n\n\n"
        },
        {
            "subject": "Re: Browser Developers (A Plea From BrowserWatch",
            "content": "If anyone wants to see a list of the many things one might get back in the\nuser agent field, feel free to check out\n<URL:http://ici.proper.com/current-count>. It is updated once a day and\nholds the results of the not-at-all-scientific survey at\n<URL:http://ici.proper.com/>.\n\n\n\n"
        },
        {
            "subject": "What the heck am I doing",
            "content": ">> (2) deference to Roy's workload -- in addition to his effort to\n>> finalize HTTP/1.0 before _tomorrow_, he's also just taken on the cleanup\n>> of the HTML/2.0 spec.\n> \n>   Roy, are you insane? :)\n\nYes, but only in short spurts of time.  Cleaning up the HTML spec\n(and then handing it off to Dan) will only take a couple days, and\ndoing it is the only way I can see that it will be acceptable to the\nIESG within our current window-of-opportunity.\n\nI am not, however, planning to finalize HTTP/1.0 before tomorrow!\nAlthough it does seem close to being done, it needs some additional\nwork (on the URI section, for instance) and the appendices regarding\n\"minimum compliance\" will likely be a testy subject.  Fortunately,\nthe majority of the spec seems to be acceptable to the WG.  Hopefully,\nwe will be able to submit it to the IESG within days after Danvers.\n\nI was planning to put together a pre-draft of all the issues discussed\nregarding what should be in HTTP/1.1 before tonight, but that didn't\nhappen due to unrelated stuff at UCI this week.  So, I'll do that Monday \nand advertize it broadly enough so that the attendees at Danvers will\nhave a chance to print it out before the meeting; it will not be a long\ndocument, just a guide for discussion.\n\nSo, I haven't gone completely off my rocker -- just balancing on that\nedge between stability and chaos. ;-)\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-digest-aa01.tx",
            "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : A Proposed Extension to HTTP : Digest Access \n                   Authentication                                          \n       Author(s) : J. Hostetler, J. Franks, P. Hallam-Baker, \n                   A. Luotonen, E. Sink, L. Stewart\n       Filename  : draft-ietf-http-digest-aa-01.txt\n       Pages     : 6\n       Date      : 03/23/1995\n\nThe protocol referred to as \"HTTP/1.0\" includes specification for a Basic \nAccess Authentication scheme.  This scheme is not considered to be a secure\nmethod of user authentication, as the user name and password are passed \nover the network in an unencrypted form.  A specification for a new \nauthentication scheme is needed for future versions of the HTTP protocol.  \nThis document provides specification for such a scheme, referred to as \n\"Digest Access Authentication\".  The encryption method used is the RSA Data\nSecurity, Inc. MD5 Message-Digest Algorithm.                               \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-digest-aa-01.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-digest-aa-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.2)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-digest-aa-01.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950323170740.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-digest-aa-01.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-digest-aa-01.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950323170740.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "3.3.1 Full dat",
            "content": "Just the one followup to Roy Fielding's reply of 23 Mar 1995 21:47:39 ..\n\n> > 3.3.1 Full date\n> >\n> >   a. I haven't ever seen the asctime() format arrive at any of my\n> >      servers -- can this be dropped as a requirement?  RFC 850 does\n> >      still seem to be around, however.\n>\n> I've only seen asctime() from some PC-based servers -- no browsers.\n> I don't see any value in dropping it beyond the existing requirement\n> that it never be generated.\n\nI do see a value in a change here -- as stated, servers have to\nimplement (and test) handling of the asctime() format, even though it's\nknown not to be necessary.  I'd like an addition saying that servers do\nnot need to accept this format.  At present, the draft says that they\nmust.\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "Albert Lunde:\n\n> In real life the Unix UID that \"owns\" a document may have nothing to\n> do with the author, for example on a CWIS where everything is posted\n> by a few people.\n\nBut those few people will have the write permission on the documents\nand know who to forward the criticism to.\n\n> There are already HTML solutions in wide use for\n> indicating/contacting the author of HTML docs.\n\nWhat do you do about non-HTML documents served on HTTP?  Do you put\ncomments on all of your GIFs?\n\n\nChuck Shotton:\n\n> Assuming that a feature in the protocol is easy to implement based\n> on the effort to hack it into Unix is not a valid measure.\n\nAssuming that an optional header cannot be generated on all machines\nis not a valid reason to rule it out for those who could.\n\n> Third, there is no correlation between an operating system-specific\n> \"owner\" of a file and the author of the file\n\nFor the documents at our server, there is a very strong correlation.\n\n> this field has no place as a required header field\n\nAs I said, I was asking for the From: in the response to be as\noptional as the From: in the request.\n\n> If it's not required, then it is of little value.\n\nLINK REV=\"made\" is not required either and still it is very valuable\nin the documents that provide it.\n\n> The bottom line is that \"authorship\" of content has no place in the\n> transport protocol.\n\nI find it very convenient that my RFC822 mail transport automatically\nplaces my name in the header so I don't have to type it into every\nmessage.  I am envisioning the same convenience for HTTP.\n\nFirst thing that I do when I want to debug an URL is to look at the\nHTTP header.  It contains all the MIME typing and date information.\n\n\nPaul Hoffman:\n\n> Further, for security, many systems have the owners of many or all\n> documents have \"noshell\" login accounts where mail would go and\n> probably either never be read or get forwarded to someone who knows\n> nothing about the content of the file.\n\nI would configure my httpd with MailExchange @cs.tu-berlin.de to have\nthe mail routed to the mailhost.\n\n> The return email address is much more likely to be useful if it is\n> part of the content of the document.\n\nI am not arguing that it should vanish from the contents.  I just want\na default place to look at if the document did not provide the address\nlike I can look at Last-Modified if the contents wasn't dated.\n\n\nI suggest the following amendment:\n\n7.1.15 From\n\nThe From field provides an email address to contact for\nchanges to the Entity-Body.\n\nFrom = \"From\" \":\" 1#mailbox\n\nServer administrators may choose to send their own, the file\nowners', or no addresses at all with each document.\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": ">> Assuming that a feature in the protocol is easy to implement based\n<> on the effort to hack it into Unix is not a valid measure.\n>\n>Assuming that an optional header cannot be generated on all machines\n>is not a valid reason to rule it out for those who could.\n\nThe problem is that the number of UNIX machines is now a very small proportion\nof the Web. The majority of clients connecting to the Whitehouse server are \nWindows. There are as many Macs as UNIX boxes. On the server side there is \nprobably a majority of UNIX servers still but we know of a Windows server with \ntens of thousands of installations.\n\nEven if one has a UNIX box the proposed scheme still makes a considerable number \nof assumptions as to the layout and administration of the server. A large number \nof servers ar now run as separate machines and are not simply serving the \nfilespace of the users direct. For such machines every document is owned by the \nsame userid.\n\nUnless a mailto feature was likely to be reliable it should not be implemented. \nBy reliable I mean that the mails should be read. It is a simple enough matter \nto add a `webmaster' line to the bottom of a document with a Perl script. Those \nsites who want such a feature are also likely to want that type of control. A \nfrom line would not be recognised by every browser so most sites would have to \nalso add a webmaster line. This would then generate confusion since the two \nwould have to match.\n\n\nPhill Hallam-Baker\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "At 11:38 AM 3/27/95, Roman Czyborra wrote:\n>Albert Lunde:\n>\n>> In real life the Unix UID that \"owns\" a document may have nothing to\n>> do with the author, for example on a CWIS where everything is posted\n>> by a few people.\n>\n>But those few people will have the write permission on the documents\n>and know who to forward the criticism to.\n\nThis is BOGUS. It makes this feature of NO value. Suppose \"bin\" owns all\nWWW documents? Do you think a system admin wants to spend all day\nforwarding stuff to the appropriate person? Using the group or owner is\nsimply a hack that only works in *some* situations on *some* hosts running\n*some* operating systems. It is an implementation detail of a protocol that\nshould be implementation independent.\n\n>> There are already HTML solutions in wide use for\n>> indicating/contacting the author of HTML docs.\n>\n>What do you do about non-HTML documents served on HTTP?  Do you put\n>comments on all of your GIFs?\n\nNo, that is what copyright notices and artist/photographer attributions are\nfor. There are mechanisms in HTML 3.0 for doing this, and that is why I\npointed this out as a bad thing for HTTP to worry about.\n\n>\n>> Assuming that a feature in the protocol is easy to implement based\n>> on the effort to hack it into Unix is not a valid measure.\n>\n>Assuming that an optional header cannot be generated on all machines\n>is not a valid reason to rule it out for those who could.\n\nI didn't say it might not be a good idea. I said that it couldn't be\nimplemented in an efficient, usable fashion across multiple platforms. The\nproposed \"technique\" was a hack and it didn't even meet the needs of all\nUnix users. HTTP servers ultimately need to sit on databases (object\nbases), not file systems. When this happens, things like this will be easy\nto implement. In the meantime, the value of such protocol candy is minimal\nwhen other, better techniques exist in HTML for doing the same thing.\n\n>> Third, there is no correlation between an operating system-specific\n>> \"owner\" of a file and the author of the file\n>\n>For the documents at our server, there is a very strong correlation.\n\nBut not at mine. Now, whose site is \"right\"? Extrapolating that all sites\nmust be like your site is faulty logic...\n\n>> this field has no place as a required header field\n>\n>As I said, I was asking for the From: in the response to be as\n>optional as the From: in the request.\n\nIf it is optional, then it is useless. And if it is required, then it is an\nunreasonable invasion of privacy for authors that require or desire\nanonymity. You need to understand that there are better mechanisms for\nrepresenting authorship via WWW than a header field in the HTTP protocol.\nPlease read through the URI/URN standards and the HTML 3.0 standard and I\nthink you'll find plenty of stuff to sate your appetite for features.\n\n>> If it's not required, then it is of little value.\n>\n>LINK REV=\"made\" is not required either and still it is very valuable\n>in the documents that provide it.\n\n\"in the documents!!!\" Not \"in the protocol.\"\n\n>> The bottom line is that \"authorship\" of content has no place in the\n>> transport protocol.\n>\n>I find it very convenient that my RFC822 mail transport automatically\n>places my name in the header so I don't have to type it into every\n>message.  I am envisioning the same convenience for HTTP.\n\nE-mail is for person to person communication. Of course you want to know\nwhere the message came from. WWW is for anonymous publisher to anonymous\nreader communication. See the difference? (PLEASE read the proposals for\nthe URN stuff. It is the appropriate mechanism for doing what you desire.)\n\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "According to Chuck Shotton:\n> At 11:38 AM 3/27/95, Roman Czyborra wrote:\n> >Albert Lunde:\n> >\n> >> In real life the Unix UID that \"owns\" a document may have nothing to\n> >> do with the author, for example on a CWIS where everything is posted\n> >> by a few people.\n> >\n> >But those few people will have the write permission on the documents\n> >and know who to forward the criticism to.\n> \n> This is BOGUS. It makes this feature of NO value. \n> \n> >> There are already HTML solutions in wide use for\n> >> indicating/contacting the author of HTML docs.\n> >\n> >What do you do about non-HTML documents served on HTTP?  Do you put\n> >comments on all of your GIFs?\n> \n\nThere is already an HTTP header which has the function you want.\nHere is an example from the HTTP spec:\n\n  Link: <mailto:timbl@w3.org>; rev=\"Made\"; title=\"Tim Berners-Lee\"\n\nThe question of where the information that goes into this header comes\nfrom is a server implementation issue.  Right now I don't think many\nservers implement this.  I know that WN does, but only on a directory\nbasis (i.e. all files in a given directory have the same mailto.)\n\nInstead of trying to persuade the HTTP community to add an additional\nheader, why not try to persuade your server writer to implement this\nheader and to get the information automatically in the way you want.\n\nIt really is a shame that this header and others like Expires and\nKeywords aren't more widely implemented.  It is even more of a shame\nthat browsers for the most part completely ignore most header lines.\nIt wouldn't be that hard to let the user *see* the header lines.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "HTTP/1.0 Issue",
            "content": "Something to think about on the plane to Boston ....\n\nThe HTTP-WG meeting will take place at 1530-1730 on MONDAY, April 3, 1995\nIt will be broadcast on the MBONE.\n\n     5 mins -- Overview of charter, and changes to agenda\n\n    10 mins -- Progress on HTTPng\n\n    10 mins -- Status of HTTP/1.0 spec\n    20 mins -- discussion of outstanding issues regarding HTTP/1.0\n\n    10 mins -- Overview of HTTP/1.1\n    10 mins -- Digest Access Authentication\n    10 mins -- Mediated Digest Authentication\n    45 mins -- Further discussion of what should (not) be in HTTP/1.1\n\nThat doesn't give us a lot of time, but I guess the important things\nwill happen in the hallways anyway.\n\n=========================================================================\nHere's what I have for outstanding issues on the HTTP/1.0 draft:\n\n    Write section about URI\n    Write appendices on minimum compliance\n\n    Status section: Fix mailing list address (www-talk@mail.w3.org)\n\n    3.1 HTTP Version: \"understand requests with a format within one major\n        number\" *less than or equal to their* \"native major version\".\n\n    3.3.1 Full Date: Only proxies need to support the asctime() date format.\n        Make \"robust date handling\" a note, with additional explanation.\n\n    4.2 Message Headers: Needs a clear statement on multiple fields,\n        something like \"duplicate header fields are only allowed where\n        mentioned explicitly, and their use is discouraged\".\n\n    4.3.2: Forwarded: The final sentence about hiding internal hosts should\n        say that existing Forwarded headers (added by proxies inside the\n        firewall) should be removed.\n\n    4.3.3 Message-ID: Allow applications to send Message-ID if they have\n        a good reason for doing so.  Or should we move it into Entity\n        headers? Message-ID causes confusion because of the spec's\n        definition of \"message\" -- perhaps we just need to clarify the\n        difference.\n\n    5.2.3 POST: \"(usually a form)\" should be \", such as the result of\n        submitting an HTML form,\". Must include a valid Content-Length\n        for HTTP/1.0 POST requests.\n\n        What to do when a redirect is received from a POST?\n        Current practice seems to be that the client issues a GET\n        request to the new address.\n\n    5.2.4 PUT: Must include a valid Content-Length for HTTP/1.0 PUT requests.\n\n    5.4.1 Accept:\n\n        Remove the notion of \"unusual types\".\n\n        Should the q value of */* default to 0.5?\n        This would make things easier on backward compatibility issues,\n        and has no affect on proper use of the algorithm.\n\n        If a browser explicitly uses q=0 on a specific type, e.g.,\n\n           Accept: application/octet-stream; q=0, application/*;q=0.5\n\n        then that specific type is explicitly disallowed in spite of the\n        existance of the more general \"application/*;q=0.5\".\n\n    5.4.2 Accept-Charset: remove extra \".\" on end of Note.\n\n    5.4.6 From: \"addr-spec\" should be replaced with \"mailbox in RFC 822\n        (as updated by RFC 1123).\" and same change to BNF.\n\n    6.1 Status-Line: last para, remove \"considered\" and strenghten last\n        sentence.\n\n    6.2.3 407 Proxy Authentication Required: delete \" -- this feature ...\"\n\n    6.3.1 Public: should say \"applies only for the nearest neighbor in a\n        connection chain\" (i.e., the closest server),\n        rather than applies only to the current connection\".\n\n    6.3.2 Retry-After: first para, nix \"full\".\n\n    7.1: Remove note about future use of HTML metainfo names.\n\n    7.1.1 Allow: Not allowed on POST requests.  With PUT, Allow would be an\n        entity-header specifying the methods to be supported.  In a 405\n        response, Allow is metainformation about the resource identified\n        by the Request-URI, it is not metainformation about the entity-body\n        in the response. Add cross-reference to Public header.\n\n    7.1.9 Last-Modified: replace \"last-mod date.\" with \"last-modify time.\"\n\n    7.1.11 Location: Make a legitimate field (and can serve as the default\n        URI if URI-header is not given). \n\n    7.1.13 URI: URI must be required for request URLs subject to\n        content negotiation variants.  Should \"qs\" be given as well?\n\n    7.1.14 Version: \"A user agent can request a particular version of an\n        entity by including its tag in a Version header as part of the\n        request\" should only refer to non-content requests (GET and HEAD).\n\n    7.2.2 Length: HTTP/1.0 POST and PUT must include a valid Content-Length.\n        I've given up any notion of having the end-of-entity indicated by\n        the media type -- it would require the server to indicate acceptance\n        of that type.  Add sentence to the effect that if Content-Length\n        is not specified on a request, and the server does not recognize or\n        cannot calculate the length from other fields, then 400 Bad Request\n        may be returned.\n\n    8.1.1 Canonicalization: last para, \"HTTP does not require that ...\"\n        should be replaced with:\n       \"It is recommended but not required that the character set of an \n        entity body be labelled as the lowest common denominator of the\n        character codes used within a document, with the exception that\n        no label is preferred over the labels US-ASCII or ISO-8859-1.\"\n\n    8.2 Language Tags: description needs to state that they imply\n        hierarchy, e.g.:\n\n           In the context of the Accept-Language header (section 5.4.4) a\n           language tag is not to be interpreted as a single token, as per\n           RFC 1766, but as a hierarchy.  A server should consider that\n           it has a match when a language tag received in an Accept-Language\n           header matches the initial portion of the language tag of a\n           document.  An exact match should be preferred.  This\n           interpretation allows a browser to send, for example:\n     \n             Accept-Language: en-US, en\n     \n           when the intent is to access, in order of preference, documents\n           in US-English (\"en-US\"), 'plain' or 'international' English\n           (\"en\"), and any other variant of English (initial \"en-\").\n\n    8.5 Transfer Encodings: Define what \"safe transport\" means for HTTP\n        and add a note about future use of packetized transfer encodings.\n\n    9. Content Negotiation: \n\n        We need an automatable subset of HTML to act as the response\n        content for 300 and 406 responses.  This would be a mini-URC.\n\n        Do we need guidelines on how to assign qs on the server?\n\n        Should we add a \"ql\" parameter to the Accept-Language header\n        and multiply this in with the other quality factors?\n\n        The browser should be able to indicate that it will accept */*, \n        but if different versions exist, it specifically _wants_ a\n        \"300 Multiple Choices\" response (if you have more than one version,\n        let me see what you have).\n    \n        A browser should be allowed to specify a preference for certain \n        \"usual\" media types without the user configuring it to do so.\n\n        Replace \"ANSI C floating point text representation\" with:\n\n           float = ( \"0\" [ \".\" 0*3DIGIT ]\n                   |     ( \".\" 0*3DIGIT )\n                   | \"1\" [ \".\" 0*3(\"0\") ] )\n\n    10. Access Authentication: Verify that the terms \"authorization\"\n        and \"authentication\" are being used correctly (I think they are).\n\n    Globally replace all \"i.e. \" and \"e.g. \" with \"i.e.,\" and \"e.g.,\"\n       and other minor nits.\n\n    If possible, move appendix C to individual notes.\n\n=========================================================================\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "HTTP/1.1 issue",
            "content": "Here is a brief list of the things that would have been in the\nHTTP/1.1 draft, if we had time to put them in.  I will try to expand\non these issues later today, but wanted to get this out before most\npeople left work.\n\nNew headers:\n   Extensions  (simpler than what Dave Kristol has proposed), or\n      Agent-Profile    (cryptic list of tokens describing UA capability)\n      Server-Profile   (cryptic list of tokens describing server capability)\n   Mandatory   (list of headers/tokens that must be understood by recipient)\n   Connection  (list of headers/tokens that only apply to the current connect)\n   Keep-Alive  (the token that requests a short-term, stateless session)\n               (the response header that indicates a session + parameters)\n   Proxy-Authenticate   (what was in the last 1.0 draft)\n   Proxy-Authorization  (what was in the last 1.0 draft)\n   Orig-URI             (as discussed on the list)\n   Base        (defined in relative URL spec)\nNew CTEs:\n   packet      (Dan Connolly's old proposal)\n   packet64    (same thing, but base64 encoded packets)\nNew methods\n   OPTIONS (similar to the SESSION concept, but not implying anything)\n   MULTI   (a generic version of MGET)\nNew AA\n   Digest (just a pointer to the other draft)\n\nand maybe\n   Pragma      (redefine as a message header?)\n   Date        (allow \"+0000\" to indicate GMT)\n\nHave I forgotten something?\n\nAll of these are, of course, wide open to debate and will likely\nbe the source of heated discussion during/around/after the meeting.\n\n.......Roy\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": "Phill Hallam-Baker writes:\n\n> Unless a mailto feature was likely to be reliable it should not be\n> implemented.  By reliable I mean that the mails should be read.\n\nI agree.\n\n> It is a simple enough matter to add a `webmaster' line to the bottom\n> of a document with a Perl script.\n\nIt is not that simple for me to add such lines to multimedia\ndocuments, let alone to documents that I have no write permission on.\nI don't want to go around amending other people's pages, I want my\ndaemon to automatically insert a header.\n\n> A from line would not be recognised by every browser so most sites\n> would have to also add a webmaster line.\n\nTrue.  But sending the From wouldn't hurt, either.  Chimera already\nlets me see the entire response header.  Give it some time and\nbrowsers will pick it up.  They'll interface even more to the\ntraditional tools to handle mail and news documents so you can easily\nmove around your documents.  HTTP has been moving in the direction of\nRFC822/MIME for quite a while.\n\n> This would then generate confusion since the two would have to\n> match.\n\nNot necessarily.  An authorship defined in the contents should be\nconsidered the first address to complain to.  Only if the real author\nlacks the permissions to change the mirrored copy of his document, the\nrom sender should be contacted.\n\nChuck Shotton writes:\n\n> Suppose \"bin\" owns all WWW documents? Do you think a system admin\n> wants to spend all day forwarding stuff to the appropriate person?\n\nWouldn't any sysop who is so misorganized to install the web pages in\nthe bin account and generate From headers from it deserve this?\n\n> I didn't say it might not be a good idea. I said that it couldn't be\n> implemented in an efficient, usable fashion across multiple\n> platforms.\n\nIf the HTTP servers running on Microsoft can provide rev=made links\nthey can also provide From headers. If not, then not.\n\n> HTTP servers ultimately need to sit on databases (object bases), not\n> file systems. When this happens, things like this will be easy to\n> implement.\n\nAnd then what header will you use?\n\nLink: <mailto:timbl@w3.org>; rev=\"Made\"; title=\"Tim Berners-Lee\"\n\nFrom: Tim Berners-Lee <timbl@w3.org>\n\nPlease don't tell me the From is so much harder to implement than the\nLink or more compatible with existing software on the Internet.\n\n> In the meantime, the value of such protocol candy is minimal when\n> other, better techniques exist in HTML for doing the same thing.\n\nThe HTML tags have to be added manually and that's why so many web\ndocuments lack authorship information.  An automatically inserted\noptional header could help in those cases and may evolve to become the\npreferred tag because it's added automatically like the headers of my\nmail.\n\n> Extrapolating that all sites must be like your site is faulty\n> logic...\n\nI didn't presume that, I have only extrapolated that my site isn't the\nonly one where HTTP exports files and those files have human owners\nnor the only one where sophisticated applications of RFC822 are in\nuse.\n\n> If it is optional, then it is useless.\n\nThere will be plenty of people to benefit from it.  Imagine them\nloading a retrieved document into their mail folder to keep or to\nquote passages from it and comment on them.  If the From is there the\nreturn To will be filled in already.  For documents that lack the From\nthe user may or may not be able to manually fill in the address.\n\n> WWW is for anonymous publisher to anonymous reader communication.\n\nLike news, WWW is a communication from prominent publishers to\nanonymous readers.\n\n> Please read through the URI/URN standards and the HTML 3.0 standard\n\nI did.  From is not in there yet.\n\n> you'll find plenty of stuff to sate your appetite for features.\n\nI am not asking for new features, I am only trying to prevent an old\nand well-established and simple standard from being forgotten.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.1 issue",
            "content": "Does anyone have a proposed set of principles for what might be\navailable in a HTTP header and what might not? Or how header fields\nmight affect or be affected by proxies or caches or robots or ...\n\n\n\n"
        },
        {
            "subject": "Re: Why is From: limited",
            "content": ">Chuck Shotton writes:\n>\n>> Suppose \"bin\" owns all WWW documents? Do you think a system admin\n>> wants to spend all day forwarding stuff to the appropriate person?\n>\n>Wouldn't any sysop who is so misorganized to install the web pages in\n>the bin account and generate From headers from it deserve this?\n\nThe arrogance (or ignorance) contained in this statement is amazing! Why\nshould an applications level communications protocol like HTTP have ANY\nimpact on the way I choose to administer ownership of documents on my WWW\nserver? You Unix guys keep getting lost in implementation details that are\nhacks and are not cross-platform solutions. Cut it out! Using \"user and\ngroup\" info from a specific O/S as a mechanism for identifying authorship\nof WWW documents is about the most fragile, useless way I could think of to\ndo this.\n\nSome sort of database scheme where HTTP entities are tagged with attributes\nlike owner, author, expiration date, etc. is the ideal mechanism for\ngenerating all of these meta headers that don't (properly) come from the\nfile system.\n\n>> I didn't say it might not be a good idea. I said that it couldn't be\n>> implemented in an efficient, usable fashion across multiple\n>> platforms.\n>\n>If the HTTP servers running on Microsoft can provide rev=made links\n>they can also provide From headers. If not, then not.\n\nHuh?\n\n>> HTTP servers ultimately need to sit on databases (object bases), not\n>> file systems. When this happens, things like this will be easy to\n>> implement.\n>\n>And then what header will you use?\n>\n>        Link: <mailto:timbl@w3.org>; rev=\"Made\"; title=\"Tim Berners-Lee\"\n>\n>        From: Tim Berners-Lee <timbl@w3.org>\n\nWhat difference does it make? At that point, you actually have reliable\ninformation that means what it says, and not some ambiguous or inaccurate\ndata pulled out of the file attributes of the document.\n\n>Please don't tell me the From is so much harder to implement than the\n>Link or more compatible with existing software on the Internet.\n\nThat's not the point. The point is that both are equally hard to implement\nin a reliable, accurate fashion. You cannot ever convince me that using the\nfile ownership attributes to determine this info, even if every Web server\non the planet runs on Unix, will ever be 100% correct. Your premise is\nbased on the faulty assumption that Web servers are built to serve files\nout of file systems, and this couldn't be further from the truth. And this\nis only going to become more obvious as WWW software moves from the realm\nof hackers into the realm of real-world programming, where things are\ndesigned and engineered or they sit on the store shelf and rot. These\nservers will serve exclusively from content residing in databases. File\nsystem based servers are definitely an endangered species.\n\n>> In the meantime, the value of such protocol candy is minimal when\n>> other, better techniques exist in HTML for doing the same thing.\n>\n>The HTML tags have to be added manually and that's why so many web\n>documents lack authorship information.  An automatically inserted\n>optional header could help in those cases and may evolve to become the\n>preferred tag because it's added automatically like the headers of my\n>mail.\n\nIn existing server implementations, yes, such tags would have to be added\nmanually. If it is important to you for people to know you are the author\nof a document, I suggest you add them. For others, they may not want to be\npestered by reams of e-mail just because they happened to put a document on\nthe Web. You are completely overlooking the need for anonymity in your\nblanket implementation.\n\n>> WWW is for anonymous publisher to anonymous reader communication.\n>\n>Like news, WWW is a communication from prominent publishers to\n>anonymous readers.\n\nThis is your opinion. How often to you bother to check when following links\naround the Web to see if you've wandered off to a different site without\nrealizing it? The very nature of the Web encourages seamless integration of\ncontent. This, more than anything, makes knowing who the publisher is at\nany given time difficult. And some people actually count on this.\n\n>> Please read through the URI/URN standards and the HTML 3.0 standard\n>\n>I did.  From is not in there yet.\n\nGeez. No, but a way to specify document ownership/authorship is. Did you\nread that part, or did you just grep for \"from\"?\n\n>> you'll find plenty of stuff to sate your appetite for features.\n>\n>I am not asking for new features, I am only trying to prevent an old\n>and well-established and simple standard from being forgotten.\n\nAs long as it is implemented your way...\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "HTTPNG checkpoint ",
            "content": "I've now put a second HTTP-NG checkpoint up on the web; it's still pretty \nrough, but I've added a lot of new text (now that I have my own \nDragonDictate, it's a lot easier to get things done...)\n\nAs before, the URL is http://sunsite.unc.edu/ses/ng-notes.txt\n\nSimon // This Message Composed By Voice\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 Issue",
            "content": ">    5.4.1 Accept:\n ...\n>        Should the q value of */* default to 0.5?\n\n        Ummm...   Is 1.0 a cap?   Or is it  \"unity\"?   If the latter,\nthen 1.0 should the norm,  into which  */*  would fall,  all others\nbeing  +/-  from there.   Thoughts?\n\n--\nRick Troth <troth@ua1vm.ua.edu>, Houston, Texas, USA\nhttp://ua1vm.ua.edu/~troth/\n\n\n\n"
        },
        {
            "subject": "Comments on latest draf",
            "content": "Just a few minor ones:\n\nSection 4.3.4:  Mentioning that HTTP is not a MIME-conformant protocol,\nwithout mentioning why, or at least a pointer to where in the spec this is\nexplained, leaves the reader a little cold.\n\nSection 5.2.1:  The material on If-Modified-Since seems out of place.\nPerhaps move it to the section on that header (5.4.7).\n\nSection 5.2.3:  Mentioning HTML forms here should provide either a\ndefinition of a FORM or a reference to the HTML spec.\n\nSection 5.4.1:  Mentioning in-line images should have a reference to the\nHTML spec.\n\nSection 5.4.2:  The 'Note:' at the bottom has an extra period at the end.\n\nSection 6.1:  The comment about \"HTTP/1.0\" being sufficient to\ndifferentiate Full-Response and Simple-Response seems either superfluous or\nwrong.  Why not make sure there is a 3 digit integer following it too?\n(nit)\n\nSection 6.2.2:  I'm agreement with the momentum in the group meeting that\nspec-ing this should be as bland as possible, not requiring a subset of\nHTML.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Harvest cache now available as an ``httpd accelerator'",
            "content": "Hi.  I'm posting this message to the HTTP Working Group mailing list\nbecause I think you may be interested in our generic approach to improving\nHTTPD performance.\n - Mike Schwartz\n   Principal Investigator, Harvest project\n-------------------------------------------------------------------------------\nThe Harvest \"httpd accelerator\" is a specially configured Harvest object\ncache that intercepts incoming HTTP requests, quickly returning those\nrequests that have already been cached.  The accelerator resolves cache\nmisses and dynamically evaluated queries by contacting your real HTTP\nserver to evaluate the incoming request.\n\nThe httpd accelerator is compatible with both CERN httpd and NCSA httpd\n(it may work with other httpds as well).  It is easily installed, and is\navailable for SunOS 4.1.x, Solaris 2.x, OSF/1 2.0 and 3.0, Linux, HP-UX,\nAIX, and IRIX (note that at present we only support the first 3 of these\nplatforms, however).\n\nThe httpd accelerator services cache hits blazingly fast because it:\n        - runs as a single (threaded) process that never forks,\n        - is implemented with non-blocking I/O,\n        - keeps meta data and especially hot objects cached in RAM,\n        - caches DNS lookups,\n        - supports non-blocking DNS lookups, and\n        - implements negative caching both of objects and of DNS lookups.\n\nOur measurements indicate that the NCSA httpd (running in standalone\nmode) can handle 3 requests per second, while cache hits to the Harvest\nhttpd accelerator are serviced at 200 requests per second.\n\nFor more information about the httpd accelerator, see\nhttp://harvest.cs.colorado.edu/harvest/httpd_accel.html\n\nFor information about Harvest (including demos, papers,\nsoftware, and documentation) see http://harvest.cs.colorado.edu/\n\n\n\n"
        },
        {
            "subject": "Multilingual WW",
            "content": "A very early, and somewhat incomplete version of my paper is available\nat http://www.ebt.com:8080/.\n\nThis is subject to large changes in the future, though the core\nconcepts and explanations are there. Please take the document with a\npound of salt ;-)\n\nI will be updating the document, and would appreciate constructive\nadvice or critisism. \n\nLet's try to avoid \"heated discussions\" this time ;-)\n\n---\nGavin Nicol, EBT\n\n\n\n"
        },
        {
            "subject": "Re: Multilingual WW",
            "content": ">Date: Thu, 6 Apr 1995 14:23:29 -0400\n>From: Gavin Nicol <gtn@ebt.com>\n>\n>A very early, and somewhat incomplete version of my paper is available\n>at http://www.ebt.com:8080/.\n\nI tried to read it, but had some difficulty.  The HTML seems to be\nscrewed up: The table of contents appears like a staircase, the fonts\nare extra-large (like everything's a header) and for some reason the\nbrowser won't let me look at the source.\n\nI tried to print it to read it at leisure on the plane to WWW'95, but\nonly obtained a mess with multiple pages printed on the same sheet.\n\nSomething's wrong I'm afraid, please fix it as I am really really\ninterested in your work.\n\nCordially,\n\n-- \nFran?ois Yergeau <yergeau@alis.ca>       | Qui se fait brebis le loup\n                                         | le mange.\n\n\n\n"
        },
        {
            "subject": "List of HTTP server",
            "content": "The \"WWW Servers Comparison Chart\" has been greatly improved and updated.\nIt includes new servers and many new features. Version 2.0 can be found at:\n<http://sunsite.unc.edu/boutell/faq/chart.html>.\nThe list will be updated approximately every month or two from here on.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Multilingual WW",
            "content": ">I tried to read it, but had some difficulty.  The HTML seems to be\n>screwed up: The table of contents appears like a staircase, the fonts\n>are extra-large (like everything's a header) and for some reason the\n>browser won't let me look at the source.\n \nI just checked again. Mosaic, Netscape, and Lynx all seem to deal with\nit fine. The HTML is fairly plain too. The only slightly strange thing\nabout the source is that the TOC uses a <DL> but the <DT> tags have no\n<DD>. What browser are you using? \n\n>I tried to print it to read it at leisure on the plane to WWW'95, but\n>only obtained a mess with multiple pages printed on the same sheet.\n> \n>Something's wrong I'm afraid, please fix it as I am really really\n>interested in your work.\n\nAnyone else having problems? I have a feeling that your copy is\ncorrupted.\n\n\n\n"
        },
        {
            "subject": "Re: Multilingual WW",
            "content": "Gavin Nicol writes:\n >  \n > I just checked again. Mosaic, Netscape, and Lynx all seem to deal with\n > it fine. The HTML is fairly plain too.\n > > \n > >Something's wrong I'm afraid, please fix it as I am really really\n > >interested in your work.\n > \n > Anyone else having problems?\n\nI was able to view and print the document using Netscape Navigator\n1.0N for X.\n\nThe Multilingual WWW\nhttp://www.ebt.com:8080/docs/multilingual-www.html\nWed Apr  5 19:14:11 1995\n\nconnolly@www19 ../connolly[502] uname -a\nSunOS www19 5.3 Generic sun4m\n\nconnolly@www19 ../connolly[505] xdpyinfo\nname of display:    :0.0\nversion number:    11.0\nvendor string:    Sun Microsystems, Inc.\nvendor release number:    3300\n\nDan\n\n\n\n"
        },
        {
            "subject": "Protecting heirarchie",
            "content": "Hi. One thing I've been thinking about recently is security. I would\nlike to be able to assign different passwords to different levels of\na heirarchy, and to have the clearances be cumultive:\n\n  <root>   Basic foo:bar\n     |\n     +----- <directory1>  Basic grok:baz\n                |\n                +----- <directory2> Basic foo:pax\n\nwhere to get to directory2 via a URL you'd do something like:\n\n   root/directory1/directory2\n\nWhat I want is:\n\n   1) To get to directory2, you need all the name+password pairs\n   2) That when you move back up, you don't need to reauthenticate\n\nThis isn't directly related to HTTP, but is rather a server-side\nissue. I was wondering is anyone has done this?\n\n     \n\n\n\n"
        },
        {
            "subject": "Re: Protecting heirarchie",
            "content": "Two issues:\n\n1) I've seen other situations where \"double-authentication\" for a given \nobject could be desired - consider it analogous to having two dead-bolt \nlocks on your front door.  I'd support some way of doing this that can be \napplied for any new form of authentication we dream up (MD5 digesting, \nSHTTP, etc).\n\n2) If root, directory1, and directory2 were all separate Realms with \nseparate password files, then essentially you want all authentication \ninfo up front (if starting at directory 2 and working back) rather than \nonly getting the pair at each level.  I could see this as a security hole \nfor the Basic scheme at least, given that people in one Realm shouldn't \nbe given the authentication information that another Realm needs.  \n\nBrian\n\np.s. - in the future, name/password combos will be stored in encrypted \nform on your Digicash Smart Card which you always keep with you, so you \nmay never need to type your password ever more than once.  Wheee!\n\n\nOn Mon, 10 Apr 1995, Gavin Nicol wrote:\n> Hi. One thing I've been thinking about recently is security. I would\n> like to be able to assign different passwords to different levels of\n> a heirarchy, and to have the clearances be cumultive:\n> \n>   <root>   Basic foo:bar\n>      |\n>      +----- <directory1>  Basic grok:baz\n>                 |\n>                 +----- <directory2> Basic foo:pax\n> \n> where to get to directory2 via a URL you'd do something like:\n> \n>    root/directory1/directory2\n> \n> What I want is:\n> \n>    1) To get to directory2, you need all the name+password pairs\n>    2) That when you move back up, you don't need to reauthenticate\n> \n> This isn't directly related to HTTP, but is rather a server-side\n> issue. I was wondering is anyone has done this?\n> \n>      \n> \n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@hotwired.com  brian@hyperreal.com  http://www.hotwired.com/Staff/brian/\n\n\n\n"
        },
        {
            "subject": "comments in HTTP header",
            "content": "They're a pain.  They're also ill-specified.\n\nSection 4.2 has a definition of ctext that differs from RFC 822,\nwhich also allows \\ escapes of ( and ).\n\nAlso I'm unclear on which has precedence in HTTP, a comment or a\nblank line in the header.  In other words, how do I parse this:\n\nGET / HTTP/1.0\nAccept: text/basic (this comment\nwill include a blank\n[blank line]\nline)\nAccept: text/html\n[blank line]\n\nwhere \"[blank line]\" is what it says.\n\nDoes the blank line in the comment terminate the request, leaving an\nill-formed comment, or does the blank line outside the comment\nterminate the request?  If the former, the server actually has to parse\ncomments on the fly so as to locate the blank line that actually\nterminates the request.  Do any existing servers do that?  I doubt it.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "SignOf",
            "content": "Sorry to bother the list, but I have desperately been trying to sign-off \nof this list for over one month, now.\n\nAny advice?\n\nPlease assist.  Thank you for any help.\n\n<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>\nKristin Cardinale\nInternational Student Adviser\nUniversity Of South Carolina, Columbia\n<kristinc@studaff.sa.scarolina.edu>\n\nWe cannot change the world all at once, \nWe CAN change it one kind work at a time\n<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-mda00.tx",
            "content": "A New Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Mediated Digest Authentication                          \n       Author(s) : D. Raggett\n       Filename  : draft-ietf-http-mda-00.txt\n       Pages     : 12\n       Date      : 04/10/1995\n\nAs the number of commercial services on the world wide web increases \nrapidly, the need arises for a means for these services to authenticate \nclients, and vice versa. A simple scheme can be based on keyed hash \nfunctions with a shared secret key for each client/server pair. Key \nmanagement becomes impractical for both clients and servers when the number\nof participants is scaled up. This document describes a efficient scheme \nfor using mutually trusted third parties to mediate authentication, as a \ndirect extension of the digest access authentication scheme. The scheme is \nbased upon public domain algorithms, and unlike encryption software, isn't \nsubject to export restrictions. The main benefits to users include: \navoiding having to enter separate user names and passwords for each \nservice, and an ability to authenticate servers. It is proposed that the \nmediated digest authentication scheme be included in the proposed \nHTTP/1.1 specification.                                                             \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-mda-00.txt\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-mda-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.2)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-mda-00.txt\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950410172301.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-mda-00.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-mda-00.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950410172301.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": "> They're a pain.  They're also ill-specified.\n\nPain yes, ill-specified no.\n\n> Section 4.2 has a definition of ctext that differs from RFC 822,\n> which also allows \\ escapes of ( and ).\n\nYep.  When I wrote the initial specification of headers and comments,\nI decided that allowing \\ to mean escape would break too many existing\nparsers.  This is still my opinion, though if a consensus says otherwise\nI will change the draft.\n\n> Also I'm unclear on which has precedence in HTTP, a comment or a\n> blank line in the header.  In other words, how do I parse this:\n> \n> GET / HTTP/1.0\n> Accept: text/basic (this comment\n> will include a blank\n> [blank line]\n> line)\n> Accept: text/html\n> [blank line]\n> \n> where \"[blank line]\" is what it says.\n\nMeaning what?  Only an *empty line* ends the headers of a message,\nand an empty line is not allowed in a comment.  A line containing\nspace or HTAB is not an empty line.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\np.s.: Conference Hell is upon us all -- I am just now getting through the\n      mail amassed during the IETF trip, and am now about to embark on a\n      two-week trip through California, Oregon and Washington\n      (WebWorld and ICSE-17).  Henrik is in Denmark as well, so don't be\n      surprised if mail goes unanswered for a while.\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": "At 6:23 PM 4/17/95, Roy T. Fielding wrote:\n>> They're a pain.  They're also ill-specified.\n>\n>Pain yes, ill-specified no.\n>\n>> Section 4.2 has a definition of ctext that differs from RFC 822,\n>> which also allows \\ escapes of ( and ).\n\nI must've been asleep when this topic first came up. Of what possible use\nare comments in HTTP headers? I'd like to understand why big parser changes\nare going to be needed for something that *seems* to be of no value to the\nclient or the server.\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": "> At 6:23 PM 4/17/95, Roy T. Fielding wrote:\n> >> Section 4.2 has a definition of ctext that differs from RFC 822,\n> >> which also allows \\ escapes of ( and ).\n> \n> I must've been asleep when this topic first came up. Of what possible use\n> are comments in HTTP headers? I'd like to understand why big parser changes\n> are going to be needed for something that *seems* to be of no value to the\n> client or the server.\n\nI suppose some of the value is in making the specification agree more\nwith MIME and USENET so that shared code and cross-protocol gateways\nwork more transparently. I really doubt we _need_ the full glory\nof RFC822 comments most of the time. \n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": "> I must've been asleep when this topic first came up. Of what possible use\n> are comments in HTTP headers? I'd like to understand why big parser changes\n> are going to be needed for something that *seems* to be of no value to the\n> client or the server.\n\nThey are commonly used in User-Agent and From fields.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                                       <fielding@ics.uci.edu>\n                      <URL:http://www.ics.uci.edu/dir/grad/Software/fielding>\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": ">I suppose some of the value is in making the specification agree more\n>with MIME and USENET so that shared code and cross-protocol gateways\n>work more transparently. I really doubt we _need_ the full glory\n>of RFC822 comments most of the time. \n\nUsenet doesn't allow the full glory of 822 either.  In general, Usenet\ndoesn't allow comments except in a particular format for From lines,\nand *sort of* single-word unknown timezones.\n\nI'd drop comment support unless you use a really limited subset of what 822\nallows.  Doing this still allows you to re-use all that wonderful existing\n822-parsing code, but makes it easier on those writing from scratch.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: comments in HTTP header",
            "content": "At 1:19 AM 4/18/95, Rich Salz wrote:\n>>I suppose some of the value is in making the specification agree more\n>>with MIME and USENET so that shared code and cross-protocol gateways\n>>work more transparently. I really doubt we _need_ the full glory\n>>of RFC822 comments most of the time.\n>\n>Usenet doesn't allow the full glory of 822 either.  In general, Usenet\n>doesn't allow comments except in a particular format for From lines,\n>and *sort of* single-word unknown timezones.\n>\n>I'd drop comment support unless you use a really limited subset of what 822\n>allows.  Doing this still allows you to re-use all that wonderful existing\n>822-parsing code, but makes it easier on those writing from scratch.\n\nI'd like to second this. While there do seem to be a few instances where\ncomments in HTTP headers make sense, HTTP is generally a machine-readable\nprotocol only and general purpose human-readable comments serve little\npurpose in the protocol at large.\n\nForcing the widespread implementation of a syntactic feature of limited\nvalue is burdensome at best, especially on custom parser authors as Rich\npoints out. And frankly, there are several of those because of the legal\nimplications of reusing the \"common\" code base in commercial apps. Many\nHTTP authors simply cannot afford to risk reusing existing code and expect\nto retain copyright and ownership.\n\nIf comments are necessary in name-related fields (user, server, agent),\nthen perhaps the syntax should be specified only for those fields. Random\nappearances of comments throughout the header seem to be of little utility\nand present some difficult recoding problems at first glance (at least for\nmy cheesey little HTTP parser!)\n\n-----------------------------------------------------------------------\nChuck Shotton\ncshotton@biap.com                                  http://www.biap.com/\ncshotton@oac.hsc.uth.tmc.edu                           \"I am NOT here.\"\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": "suscribe http-wg  Elliot Tricoche\n\nnow is the time for all good men\n\n\n\n"
        },
        {
            "subject": "Subscrib",
            "content": "Subscribe the http-wg mailing list.\n\n-- \n_________________________________________________________________\n|       /       /    e-mail: jsimao@fct.unl.pt   |\n|      /   _/_/_/    _/_/_/   /     (MIME messages accepted)    |\n|     /      _/   _/     /   Jorge Paulo Ferreira Simao     |\n|    /     _/   _/_/_/    /    MSc Student at        |\n|   /_/  _/        _/   /     Universidade Nova de Lisboa    |\n|  /_/_/    _/_/_/    /      F.C.T. - Dpt Informatica       |\n| /______________________/       2825 Monte Caparica - PORTUGAL |\n|                                                               |\n| URL: http://sasc.di.fct.unl.pt/people/jsimao                  |\n|      (look there for my PGP public key, or use,               |\n|       finger jsimao@stimpy.di.fct.unl.pt)                     |\n|_______________________________________________________________|\n\n\n\n"
        },
        {
            "subject": "Digest Auth..",
            "content": "Anybody got a server doing digest auth ala the March 24 draft?  I just put\nit in emacs-w3, and need to test it.  MDA is next. :)\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": "According to wmperry@spry.com:\n> Sender: http-wg-request@cuckoo.hpl.hp.com\n> \n> Anybody got a server doing digest auth ala the March 24 draft?  I just put\n> it in emacs-w3, and need to test it.  MDA is next. :)\n> \n\nYes, you can try <URL:http://hopf.math.nwu.edu/simp/> which is protected\nby Digest authentication.  Use username \"Mufasa\" and password \"CircleOfLife\"\n(with no quotation marks).  NOTE: You won't be able to access this without\na browser supporting Digest Authentication!\n\nThis is an implementation of the spec at\n<URL:http://www.spyglass.com/techreport/simple_aa.txt>.  It is an\nauthentication module for use with the WN server.  The code (except\nfor MD5 from RSA) is in the public domain.  While designed for use\nwith WN it should not be too difficult to modify for other servers.\nThe best way to get this code is get the WN distribution at \nftp://ftp.acns.nwu.edu/pub/wn/wn.tar.gz and extract the files from the\ndirectory named \"experimental.\"\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": "John Franks writes:\n> According to wmperry@spry.com:\n> > Sender: http-wg-request@cuckoo.hpl.hp.com\n> > \n> > Anybody got a server doing digest auth ala the March 24 draft?  I just put\n> > it in emacs-w3, and need to test it.  MDA is next. :)\n> > \n> \n> Yes, you can try <URL:http://hopf.math.nwu.edu/simp/> which is protected\n> by Digest authentication.  Use username \"Mufasa\" and password \"CircleOfLife\"\n> (with no quotation marks).  NOTE: You won't be able to access this without\n> a browser supporting Digest Authentication!\n\n  And... he shooots.... and SCORES!  Cool, it works. :)\n\n  Thanks for the various pointers everyone.  Ok, now does anyone have an\n  MDA server up and running yet?\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": ">Anybody got a server doing digest auth ala the March 24 draft?  I just put\n>it in emacs-w3, and need to test it.  MDA is next. :)\n\nI saw you got it working with WN -- just for the sake of multiple test\ncases, try:\n\nhttp://www.spyglass.com:4040/~eric/protected/digest.txt\n\nusername Timon\npassword HakunaMatata\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": "Eric W. Sink writes:\n> \n> >Anybody got a server doing digest auth ala the March 24 draft?  I just put\n> >it in emacs-w3, and need to test it.  MDA is next. :)\n> \n> I saw you got it working with WN -- just for the sake of multiple test\n> cases, try:\n> \n> http://www.spyglass.com:4040/~eric/protected/digest.txt\n> \n> username Timon\n> password HakunaMatata\n\n  Doesn't appear to be working... :)\n\n[wmperry]telnet www.spyglass.com 4040\nTrying 192.246.238.10...\nConnected to spyglass.com.\nEscape character is '^]'.\nGET /~eric/protected/digest.txt HTTP/1.0\n\nConnection closed by foreign host.\n\n  D'ohhh!\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": ">> http://www.spyglass.com:4040/~eric/protected/digest.txt\n>>\n>> username Timon\n>> password HakunaMatata\n>\n>  Doesn't appear to be working... :)\n>\n>[wmperry]telnet www.spyglass.com 4040\n>Trying 192.246.238.10...\n>Connected to spyglass.com.\n>Escape character is '^]'.\n>GET /~eric/protected/digest.txt HTTP/1.0\n>\n>Connection closed by foreign host.\n>\n>  D'ohhh!\n\nOn the contrary -- it works just fine, sortof :-).  I did forget to mention\nthat this particular server requires the Extension: Security/Digest header,\nas mentioned in the draft.  If you send that header along, it will function\ncorrectly.  It's behavior without the header is kind of questionable.  This\nis just a copy of NCSA httpd 1.3 which I hacked to support Digest.\n\n--\n\nConnected to spyglass.com.\nEscape character is '^]'.\nGET /~eric/protected/digest.txt HTTP/1.0\nExtension: Security/Digest\n\nHTTP/1.0 401 Unauthorized\nDate: Monday, 24-Apr-95 21:56:51 GMT\nServer: NCSA/1.3\nMIME-version: 1.0\nContent-type: text/html\nWWW-Authenticate: Digest realm=\"DigestAuthenticationInfo\" nonce=\"798760611\" opaq\nue=\"34e1ea8800a2a942d95d31b9a7bd3c81\"\n\n<HEAD><TITLE>Authorization Required</TITLE></HEAD>\n<BODY><H1>Authorization Required</H1>\nBrowser not authentication-capable or\nauthentication failed.\n</BODY>\nConnection closed by foreign host.\n\n\n--\nEric W. Sink, Senior Software Engineer --  eric@spyglass.com\n\n        http://www.spyglass.com/~eric/home.htm\n\n\n\n"
        },
        {
            "subject": "Re: Digest Auth..",
            "content": "Eric W. Sink writes:\n> \n> >> http://www.spyglass.com:4040/~eric/protected/digest.txt\n> >>\n> >> username Timon\n> >> password HakunaMatata\n> >\n> >  Doesn't appear to be working... :)\n> >\n> >[wmperry]telnet www.spyglass.com 4040\n> >Trying 192.246.238.10...\n> >Connected to spyglass.com.\n> >Escape character is '^]'.\n> >GET /~eric/protected/digest.txt HTTP/1.0\n> >\n> >Connection closed by foreign host.\n> >\n> >  D'ohhh!\n> \n> On the contrary -- it works just fine, sortof :-).  I did forget to mention\n> that this particular server requires the Extension: Security/Digest header,\n> as mentioned in the draft.  If you send that header along, it will function\n> correctly.  It's behavior without the header is kind of questionable.  This\n> is just a copy of NCSA httpd 1.3 which I hacked to support Digest.\n\n  Great - thanks for the info.  Seems to work.  Cooool.  :)\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "another Digest tes",
            "content": "Since we're having so much fun, here's another test site for\nDigest Authentication (with another independent implementation)...\n\nhttp://www.research.att.com/digest-test\n\nUser: protected\nPassword:  try-me-out\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Digest authenticatio",
            "content": "Good to see it works.  Question: why is the 'opaque' field part of\nthe WWW-Authenticate: header?  It would seem to be generally useful,\nindependent of authentication, and therefore perhaps merits a header\nof its own:\n\n  Opaque: 34e1....3c81\n\nMike Cowlishaw\nIBM UK Laboratories\n\n\n\n"
        },
        {
            "subject": "Re: Digest authenticatio",
            "content": "Mike Cowlishaw writes:\n> Good to see it works.  Question: why is the 'opaque' field part of\n> the WWW-Authenticate: header?  It would seem to be generally useful,\n> independent of authentication, and therefore perhaps merits a header\n> of its own:\n> \n>   Opaque: 34e1....3c81\n\n  I wondered that myself.  It would dovetail nicely with either the\n'Session-ID' or 'Cookie' proposals.\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Digest authenticatio",
            "content": "Bill Perry said:\n  > Mike Cowlishaw writes:\n  > > Good to see it works.  Question: why is the 'opaque' field part of\n  > > the WWW-Authenticate: header?  It would seem to be generally useful,\n  > > independent of authentication, and therefore perhaps merits a header\n  > > of its own:\n  > > \n  > >   Opaque: 34e1....3c81\n  > \n  >   I wondered that myself.  It would dovetail nicely with either the\n  > 'Session-ID' or 'Cookie' proposals.\n\nBut you might want a separate Opaque: value for each of Session-ID\nand WWW-Authenticate:.  Better to leave them as attributes of a\nparticular header, IMO.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Opaqu",
            "content": "> Dave Kristol wrote:\n> But you might want a separate Opaque: value for each of Session-ID\n> and WWW-Authenticate:.  Better to leave them as attributes of a\n> particular header, IMO.\n\nIt's certainly true that one could have an 'opaque' field defined,\nusefully, for almost any kind of header (different languages, for\nexample, or different dates, or modified by dates, or dependent on\nURI, and so on).\n\nBut it's over-heavy to define these as sub-fields of every kind of\nheader, rather than sub-classes of opaque data (if the latter really\nare needed).\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re:  Opaqu",
            "content": "\"Mike Cowlishaw\" <mfc@VNET.IBM.COM> said:\n  > > Dave Kristol wrote:\n  > > But you might want a separate Opaque: value for each of Session-ID\n  > > and WWW-Authenticate:.  Better to leave them as attributes of a\n  > > particular header, IMO.\n  > \n  > It's certainly true that one could have an 'opaque' field defined,\n  > usefully, for almost any kind of header (different languages, for\n  > example, or different dates, or modified by dates, or dependent on\n  > URI, and so on).\n  > \n  > But it's over-heavy to define these as sub-fields of every kind of\n  > header, rather than sub-classes of opaque data (if the latter really\n  > are needed).\n\nI have to disagree:\n1) Only a few headers need opaque.  We're not burdening all, or anywhere\nclose to all, of them.\n\n2) It's a nuisance for the server to have to collate information from\ntwo different headers.  In particular, if Opaque: has pieces that are\nlabeled for different other headers (if I understand what you're\nproposing), then a server must, for example, parse both the Session-ID\nand Opaque headers to figure out what's going on.  It's much simpler to\nkeep all the information together and parse a single header.\n\nIf you're assuming that there's a relationship between the value of\nOpaque for Session-ID and the value for WWW-Authenticate, I think\nyou're wrong:  I don't think they have any connection.  Certainly\nthere's no obligatory connection.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Opaqu",
            "content": "> Dave Kristol wrote:\n> I have to disagree:\n> 1) Only a few headers need opaque.  We're not burdening all, or anywhere\n> close to all, of them.\nOf course, but once one adds a special opaque field then everyone who\n'owns' a header will add their own, too, citing the precedent.  I'm\nlooking long-term, here, and seeing (from bitter experience) the thin\nedge of a combinatorial wedge, where every header has every possible\ncombination of attribute from every other header.\n\n> 2) It's a nuisance for the server to have to collate information from\n> two different headers.\nNot necessarily.  Most servers have to handle several headers already\n(Last-Modified-Since, Content-Type, Content-Length, etc.)  Assuming\nthere's a reasonable internal lookup-by-name\n\n>                         In particular, if Opaque: has pieces that are\n> labeled for different other headers (if I understand what you're\n> proposing), then a server must, for example, parse both the Session-ID\n> and Opaque headers to figure out what's going on.  It's much simpler to\n> keep all the information together and parse a single header.\n>\n> If you're assuming that there's a relationship between the value of\n> Opaque for Session-ID and the value for WWW-Authenticate, I think\n> you're wrong:  I don't think they have any connection.  Certainly\n> there's no obligatory connection.\n>\n> Dave Kristol\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n\n\n\n"
        },
        {
            "subject": "Opaqu",
            "content": "(Continuing after power-cut .. apologies for duplicated first sentences!)\n\n> Dave Kristol wrote:\n> I have to disagree:\n> 1) Only a few headers need opaque.  We're not burdening all, or anywhere\n> close to all, of them.\nOf course, but once one header adds a special opaque field then\neveryone who 'owns' a header will add their own, too, citing the\nprecedent.  I'm looking long-term, here, and seeing (from bitter\nexperience) the thin edge of a combinatorial wedge, where every header\nhas every possible combination of attribute from every other header.\n\n> 2) It's a nuisance for the server to have to collate information from\n> two different headers.\nNot really.  Most servers have to handle several headers already\n(Last-Modified-Since, Content-Type, Content-Length, etc.)  Assuming\nthere's a reasonable internal lookup-by-name routine, it's no more\nexpensive to look up a separate header than to look up a field within\na header.\n\n>                         In particular, if Opaque: has pieces that are\n> labeled for different other headers (if I understand what you're\n> proposing), then a server must, for example, parse both the Session-ID\n> and Opaque headers to figure out what's going on.  It's much simpler to\n> keep all the information together and parse a single header.\nYou're right for special-case parsing code, but that doesn't work, long\nterm, or help much if you're asking a script-writer to handle new\nheaders.\n\n> If you're assuming that there's a relationship between the value of\n> Opaque for Session-ID and the value for WWW-Authenticate, I think\n> you're wrong:  I don't think they have any connection.  Certainly\n> there's no obligatory connection.\nI think the value of Opaque -- due to its generality -- rather outweighs\nits value for either of those specific uses.  At present, I'm adding\nhidden fields in fake forms in documents (which have no forms otherwise)\nsimply to achieve the Opaque effect.  I'd much rather do that at the HTTP\nlevel, where it belongs.\n\nMike Cowlishaw\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": ">I'm not convinced that this is a bad idea.  It might improve performance\n>(although I would guess that the main advantage will come from reduced\n>cryptographic operations when doing authentication and privacy).  It\n>might not require much extra work on the part of the server (since\n>the server will need at least some \"state\" to hold on to the TCP\n>connection).  And a server implementor who prefers not to implement\n>any state-saving mechanism could simply refuse to allow sessions.\n>\n>However, it might be wise to consider modifying the session model\n>so that the server can reply with either of two codes:\n>Connection: maintain-stateful\n>indicating that the server is remembering the client's request header\n>fields, or\n>Connection: maintain-stateless\n>indicating that the server is not remembering, and the client must\n>retransmit all relevant request headers per method invocation.\n>(Of course, the server has a third option: return neither code,\n>implicitly indicating \"nonpersistent-stateless\".)\nWhile in principle your above suggest seems perfectly sound I would rather\nleave it out since I do think that there can be quite a bit to be gained in\nbeing able to omit sending most of the headers. I guess the most convincing\nreason however to me is just a reduced complexity one. I wanted to add as\nfew options as possible.\n\nI have actually been having some private mail discussions with several\npeople latelly in which they were expressing their frustrations with the #\nof accept headers you would have to transmit in order to properly negotiate\na wide variety of new content-types.\n\nI admit that this doesn't absolve us from having to fix the Accepts: field.\n(In that the # of accepts field headers grows far too large with lots of\nfeatures & content types to negotiate), but I still think it helps is fit\nmost requests into one TCP packet.\n\n\nQuick question since I'm not familiar with most other HTTP server designs.\nIs it really going to be difficult to keep things like accepts header\ninformation along with the connection (in any particular implementation)? It\nseems to me that somewhere you need to keep track of your sockets, whether\nits by thread, process, or just a pool. And so just keeping that info\nassociated with the sockets/connections/whatever-your-platform-calls-it\nshouldn't really be difficult.\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": "It might improve net efficiency (and possibly allow servers to\nprecompute information or ignore these headers if they don't care) to\npackage together those things that are configuration specific (accept,\naccept-encoding, accept-charset, accept-language and user-agent:) and\nsend them by reference, e.g.,\n\nthe client sends:\n\naccept-hash: NNNNNNNNNNNNNNN\n\nwhere NNNNNNNNNNNNNN is the MD5 of the omitted headers; the server\nsends back an error return if it actually needs the fields.\n\nThis would be useful independent of whether the connection remains\nopen: even if the connection closes, the information might affect a\ncache choice; even if the connection remains open, a proxy might want\nto send different header information when proxying for different\nclients. \n\n(Clearly this would be in 1.1; if HTTP were recast as ILU or CORBA, it\nwould be done as a client object that the server could interrogate.)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": "One further comment on HTTP's state(less/full)ness.\n\nIf retrieval is truly a stateless process, then the probability of a an \nobject being requested will be independent of any previous object \nrequests. Any perusal of activity logs will show that this is clearly not \nthe case- if a page with links is requested, any links on that page will \nhave a higher probability of being requested than if the page that links \nto them had not been fetched.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": ">If retrieval is truly a stateless process, then the probability of a an \n>object being requested will be independent of any previous object \n>requests. Any perusal of activity logs will show that this is clearly not \n>the case- if a page with links is requested, any links on that page will \n>have a higher probability of being requested than if the page that links \n>to them had not been fetched.\n\nWhile this may be true, any attempts on the part of the server to attempt to\n*guess* at what a client may do next would require alot of processing and\nwould open a huge can of worms I know I'd rather avoid.  Servers at popular\nsites today have enough trouble keeping up with simple stateless requests.\nI'd hate to see the effect of attempts at making them remember user\ninformation about every one who comes in the door.\n\nIt's clear that we need persistent connections because of inline images.\nBut that doesn't mean we need statefulness.\n\nNow that I 'shared' about statefulness, I may as well speak my mind on\npersistent connections.  I'm not yet convinced persistent connections for\nanything other than a client who can knows for certain they will be\nrequesting multiple documents is a good thing.\n\nCASE 1 -- Have grabbed \"initial.html\".  Doc has 5 inline images.  User's\nindicated inline images are to be grabbed.  My opinion: Making the next\nrequest with a persistent connection is definitely a 'good thing'.\nCASE 2 --  Want to grab \"initial.html\".  Client yet has no clue whether\ndocument contains inline images.  My opinion: I question whether making a\npersistent connection here is valid, especially if the user has requested\ninline images not to be sent.\nCASE 3 -- Want to grab \"initial.html\".  User doesn't load inline images, but\nclient guesses that the user will be so enamored with this document that\nthey might want to click on another document from the same server.  My\nopinion: I think the client is out of line holding a connection open on a\nserver solely for this reason.  Doesn't an extra connection mean an extra\nprocess and an extra copy of the server program in memory for many servers?\nWhat a waste!  Now people who are making legitimate connections with real\nrequests get slower performance while the server swaps memory to disk.\nMaybe next rude clients will hold open connections on documents 3 layers\ndeep in their history, just on the off chance a user might go back to it and\nstart clicking again.\n\nOK, so a server can close down at any time.  Now we place an extra burden on\nthe server to make heuristics and decisions about when to dump, how long is\nlong enough, grep documents on \"<IMG\", etc.\n\nAgain, the above are just my opinion, and my opinions can change given good\narguements.  Let me hear them.\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": "    CASE 2 --  Want to grab \"initial.html\".  Client yet has no clue whether\n    document contains inline images.  My opinion: I question whether making\n    a persistent connection here is valid, especially if the user has\n    requested inline images not to be sent.\n    \n    CASE 3 -- Want to grab \"initial.html\".  User doesn't load inline\n    images, but client guesses that the user will be so enamored with this\n    document that they might want to click on another document from the\n    same server.  My opinion: I think the client is out of line holding a\n    connection open on a server solely for this reason.  Doesn't an extra\n    connection mean an extra process and an extra copy of the server\n    program in memory for many servers?  What a waste!  Now people who are\n    making legitimate connections with real requests get slower performance\n    while the server swaps memory to disk.  Maybe next rude clients will\n    hold open connections on documents 3 layers deep in their history, just\n    on the off chance a user might go back to it and start clicking again.\n\nYou raise legitimate concerns.  However, the results of my simulations\n(based on traces from actual clients and actual servers) indicate that\nyou can probably stop worrying.\n\nSince I looked at the requests coming from actual clients, my traces\nalready account for \"are the inlined images already cached?\".  That\nis, I only looked at client-cache misses.\n\nMy simulations assumed that EVERY connection request should be treated\nas a persistent connection.  That is, I did not try to simulate any\ncleverness on the client's part.  This means that clients were using\npersistent connections to handle all three of your cases.\n\nThe results show that, even with very conservative limits on the\nnumber of open connections at the server (hence, limits on the\nnumber of server processes) and even with this \"greedy client\"\napproach, persistent connections still avoid lots of overhead.\nIn particular, they *reduce* the amount of server memory required\nto store TCP connection tables.\n\nIn other words, it DOES pay to keep the connection open \"on the off\nchance a user might go back to it and start clicking again.\"  Users\ndo this often enough to pay off.\n\nRead the paper for full details.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": ">CASE 1 -- Have grabbed \"initial.html\".  Doc has 5 inline images.  User's\n>indicated inline images are to be grabbed.  My opinion: Making the next\n>request with a persistent connection is definitely a 'good thing'.\nOk, so open the persisitant connection with the initial request and then\nrequest subsequent images with our without that persistant connection. When\nthere are no more images/other things left to fetch from that server, close\nthe connection.\n>CASE 2 --  Want to grab \"initial.html\".  Client yet has no clue whether\n>document contains inline images.  My opinion: I question whether making a\n>persistent connection here is valid, especially if the user has requested\n>inline images not to be sent.\nOk, so open the persisitant connection with the initial request and then\nrequest subsequent images with our without that persistant connection. When\nthere are no more images/other things left to fetch from that server, close\nthe connection. Note: Same thing.\n>CASE 3 -- Want to grab \"initial.html\".  User doesn't load inline images, but\n>client guesses that the user will be so enamored with this document that\n>they might want to click on another document from the same server.  My\n>opinion: I think the client is out of line holding a connection open on a\n>server solely for this reason.  Doesn't an extra connection mean an extra\n>process and an extra copy of the server program in memory for many servers?\n>What a waste!  Now people who are making legitimate connections with real\n>requests get slower performance while the server swaps memory to disk.\n>Maybe next rude clients will hold open connections on documents 3 layers\n>deep in their history, just on the off chance a user might go back to it and\n>start clicking again.\nKeep in mind that either the server or the client can close the connections\nat any time. So your client might request a persistant connection, and then\nrealize that it doesnt want any inlined images and close it right away. Or\nit might give the user 5-10 seconds to request an image.\nAs for the case you bring up of a rude client, the server will probably be\nmonitoring its load. If a server feels that for some reason the connection\nhas lasted too long, go ahead and close it. So a heavily loaded server might\nhave a timeout of 5 seconds (Which seems extremely reasonable given that\n14.4k users and international users will keep links open for minutes just\ndownloading a GIF), while a server with plenty of capacity could let a\nclient keep connections open for a couple minutes on the chance that the\nuser might select another document on the same server.\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Stockholm IET",
            "content": "Unfortunately, I can't make it to Stockholm to chair the IETF HTTP working\ngroup meeting, but Dan Connolly has agreed to chair the meeting in my\nabsence.\n\nCan those of you planning on attending the meeting, please post your\nsuggestions for agenda items to the http-wg mailing list.\n\nI am almost back online following my relocation to MIT and will be\nworking away at my email backlog as fast as possible ...\n--\n Dave Raggett <dsr@w3.org> tel: +1 (617) 258 5741 fax: +1 (617) 258 8682\n   World Wide Web Consortium, 545 Technology Square, Cambridge, MA 02139\n   url = http://www.w3.org/hypertext/WWW/People/Raggett/index.html             <\n\n\n\n"
        },
        {
            "subject": "Re: HTTP Session Extension draf",
            "content": ">Keep in mind that either the server or the client can close the connections\n>at any time. So your client might request a persistant connection, and then\n>realize that it doesnt want any inlined images and close it right away. Or\n>it might give the user 5-10 seconds to request an image.\n\nMany servers are now incorporating internal metrics, and the server can\ndecide how long to hold open a connection based on some (hopefully correct)\ninternal measurement. Even a guess of 5 or 10 seconds seems reasonable for\nmost sensible cases.\n\nI imagine that all servers will allow these timings to be specified in a\nconfiguration file.\n\n\n\n"
        },
        {
            "subject": "Agenda Items for working group meetin",
            "content": "I'd be interested in discussing the Session and Session-id extensions,\nas well as the byte-range proposal.  In general, I think it would\nbe good to discuss version numbering pre http-ng....\nTed Hardie\nNASA NAIC\n\n\n\n"
        },
        {
            "subject": "potential security holes in digest authorizatio",
            "content": "I'm glad to see that you are considering digest authorization \nfor HTTP.  I noticed a few security holes that may be of\nconcern:\n\n- the server's digest database of H(<username> : <realm> : <password>) should\nreceive highest security.  To the knowledgeable user, it is the same as \nstoring passwords in the clear.  This is a weakness of the digest\nmethod.  The passwd file in UNIX that is used for \"basic\" authorization\nmay be released without compromising strong passwords.\n\n- the reuse of stale nonces is a convenience that allows a user to\nact as another user.  The server should at least require that the return\nIP address be the same as before.\n\n- the <message-digest> places the nonce before the <message-body>.  \nCheswick & Bellovin '94 [Firewalls and Internet Security p.222]\nreport a security hole noticed by Tsudik '92 [IEEE Infocom].  They\nrecommend placing the nonce after the message. \n\n                                                    --Brad Barber\n\n\n\nBrad Barber, 116 Fayerweather St., Cambridge MA 02138 \n617-497-8876, barber@tiac.net, bradb@geom.umn.edu\n\n\n\n"
        },
        {
            "subject": "Re:  potential security holes in digest authorizatio",
            "content": "bradb@geom.umn.edu (Brad Barber) said:\n\n  > I'm glad to see that you are considering digest authorization \n  > for HTTP.  I noticed a few security holes that may be of\n  > concern:\n  > \n  > - the server's digest database of H(<username> : <realm> : <password>) should\n  > receive highest security.  To the knowledgeable user, it is the same as \n  > storing passwords in the clear.  This is a weakness of the digest\n  > method.  The passwd file in UNIX that is used for \"basic\" authorization\n  > may be released without compromising strong passwords.\n  [...]\n\nI would like to propose that <password> be replace by H(<password>).\nThe client would pass to the server\nH(<username> : <realm> : H(<password>))\nThe server could store in its user/password file\nuser-name:H(<password>)\nThat way the password would neither be passed in the clear nor stored\nin the clear.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re:  potential security holes in digest authorizatio",
            "content": ">I would like to propose that <password> be replace by H(<password>).\n>The client would pass to the server\n>        H(<username> : <realm> : H(<password>))\n>The server could store in its user/password file\n>        user-name:H(<password>)\n>That way the password would neither be passed in the clear nor stored\n>in the clear.\n>\n>Dave Kristol\n\nRe: the server's digest database is the same as storing passwords\nin the clear.\n\nThere's been some confusion about this.  Using the language of the draft specification,\n\n        If I have <username> and H(A1), it is easy to generate \n\n        H( H(A1) + ':' + ...) and pretend to be <username>. \n\n        This is not true with Unix's passwd file since it stores f(passwd) \n        where f() is a one-way function. \n\nStoring H(<password>) as Dave suggests does not\nsolve the problem.  An attacker can discover <username>,\n<realm> and H(<password>).\n\nI believe the problem is intrinsic to digest authentication.  The\noptions that I see are:\n\n        1) Encrypt the database.\n\n        2) Physically secure the authentication server and use a\n        minimal communications channel.  Encryption is not needed.\n        This should always be done if security is a concern.\n\n        3) Use end-to-end encryption and authentication as in SSL.\n\n                                                --Brad\n\n\n\nBrad Barber, 116 Fayerweather St., Cambridge MA 02138 \n617-497-8876, barber@tiac.net, bradb@geom.umn.edu\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "According to Dave Kristol:\n> \n> bradb@geom.umn.edu (Brad Barber) said:\n> \n>   > I'm glad to see that you are considering digest authorization \n>   > for HTTP.  I noticed a few security holes that may be of\n>   > concern:\n>   > \n>   > - the server's digest database of H(<username> : <realm> : <password>) should\n>   > receive highest security.  To the knowledgeable user, it is the same as \n>   > storing passwords in the clear.  This is a weakness of the digest\n>   > method.  The passwd file in UNIX that is used for \"basic\" authorization\n>   > may be released without compromising strong passwords.\n>   [...]\n> \n> I would like to propose that <password> be replace by H(<password>).\n> The client would pass to the server\n> H(<username> : <realm> : H(<password>))\n> The server could store in its user/password file\n> user-name:H(<password>)\n> That way the password would neither be passed in the clear nor stored\n> in the clear.\n> \n\nUnder the current proposal what is stored in the server user/password\nfile is \nuser:H(<username> : <realm> : <password>)\n\nSo gaining illicit access to the server password file does not\ncompromise the password.  Of course, it *does* grant illicit access to\nthe documents on that server in that realm.  I believe this is what\nBrad Barber was referring to when he said the password file needed to\nreceive highest security.\n\nIt is important that the server store what it does under this\nproposal, because people tend to use the same password for multiple\npurposes and this way the owner of the server password file does not\nhave access to the actual password.\n\nReplacing the password with H(password) would simply make H(password)\nthe new password.\n\nThis is not intended to be the ultimate security system, just an major\nimprovement over basic authentication.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "According to Brad Barber:\n> \n> Re: the server's digest database is the same as storing passwords\n> in the clear.\n> \n> There's been some confusion about this.  Using the language of the draft specification,\n> \n>         If I have <username> and H(A1), it is easy to generate \n> \n>         H( H(A1) + ':' + ...) and pretend to be <username>. \n> \n>         This is not true with Unix's passwd file since it stores f(passwd) \n>         where f() is a one-way function. \n> \n\nThis is correct, but practical experience indicates that Unix's passwd\nsecurity (aka Basic authentication in HTTP) has a serious problem\nbecause the password must be transmitted in the clear over the network.\nAgain digest authentication is not intended to be a strong or complete\nsecurity option.  One of its weaknesses is the server usr/password\nfile.   It is better than the Unix passwd scheme because generally\nfiles on servers are more secure than internet traffic.\n\nDigest authentication was designed with certain constraints.  Among\nthem were the requirements that there be (1) no potential patent restrictions\n(2) no royalties and (3) no U.S. export restrictions.  It meets these \nrequirements at a cost of weakened security.\n\n> \n> I believe the problem is intrinsic to digest authentication.  The\n> options that I see are:\n> \n>         1) Encrypt the database.\n\nProbably in conflict with (1), (2), and (3).\n\n> \n>         2) Physically secure the authentication server and use a\n>         minimal communications channel.  Encryption is not needed.\n>         This should always be done if security is a concern.\n> \n\nA good idea to the extent feasible.\n\n>         3) Use end-to-end encryption and authentication as in SSL.\n> \n\nSurely in conflict with (1), (2), and (3).\n\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "John Franks <john@math.nwu.edu> says:\n  [...]\n  > Under the current proposal what is stored in the server user/password\n  > file is \n  > user:H(<username> : <realm> : <password>)\n  > \n  > So gaining illicit access to the server password file does not\n  > compromise the password.  Of course, it *does* grant illicit access to\n  > the documents on that server in that realm.  I believe this is what\n  > Brad Barber was referring to when he said the password file needed to\n  > receive highest security.\n  [...]\n\nThat helps, but I have a quibble.  I would prefer not to tie the username\nand password so strongly to a particular realm, because:\n    1) I might like to change the name of the realm (if only slightly).\n    2) I might like to use the same password file for more than one realm.\nEach of these is impossible if the information in the password file\nhas the realm embedded in it.\n\nWhile I have the floor (:-), I'll reiterate my standard quibble about\nrealms and prompts.  Currently they are identical.  That is, if I tell\na browser that the protection realm is \"foo\", the browser asks for a\nname and password for \"foo\".  I would prefer to be able to specify the\nprompt separately.  So, the prompt for realm \"foo\" could be \"World War\nII Euphemism\".  I think the name of the realm is a denotational matter\nbetween the client and server, whereas the prompt is something the\nbrowser shows the user.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": ">John Franks <john@math.nwu.edu> says:\n>  [...]\n>  > Under the current proposal what is stored in the server user/password\n>  > file is \n>  > user:H(<username> : <realm> : <password>)\n>  > \n>  > So gaining illicit access to the server password file does not\n>  > compromise the password.  Of course, it *does* grant illicit access to\n>  > the documents on that server in that realm.  I believe this is what\n>  > Brad Barber was referring to when he said the password file needed to\n>  > receive highest security.\n>  [...]\n>\n>That helps, but I have a quibble.  I would prefer not to tie the username\n>and password so strongly to a particular realm, because:\n>    1) I might like to change the name of the realm (if only slightly).\nI have to agree with this first quibble quite a bit. In an actual product\nimplementation of message digest we have had some issues arrise because if\nthe server operator wants to change their realm, their entire user/password\ndatabase suddently becomes inoperative.\n\nAny chance that we could (if there is some move to change the digest draft,\nfor example to move the location of the nonce), change the inclusion of the\nrealm in there with the username and password (The A1 substring)?\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "According to Alex Hopmann:\n> >That helps, but I have a quibble.  I would prefer not to tie the username\n> >and password so strongly to a particular realm, because:\n> >    1) I might like to change the name of the realm (if only slightly).\n> I have to agree with this first quibble quite a bit. In an actual product\n> implementation of message digest we have had some issues arrise because if\n> the server operator wants to change their realm, their entire user/password\n> database suddently becomes inoperative.\n> \n\nThe reason that the realm is encoded with the user and password in the\nserver password file is that people tend to reuse the same password.\nIf only the username and password are encoded and put in the password\nfile then the maintainer of server A, knowing H( username:password)\nfor his server can use this to gain access to those documents on\nserver B to which username has access.  This is assuming that the user\nhas the same password on both servers.\n\nIt would be nice if every user used a different password for every account\nbut this is not realistic.  Nothing in the draft addresses the problem of\nhow the user gets H( user:realm:password) into the server password file.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "John Franks <john@math.nwu.edu> said:\n  > According to Alex Hopmann:\n  > > >That helps, but I have a quibble.  I would prefer not to tie the username\n  > > >and password so strongly to a particular realm, because:\n  > > >    1) I might like to change the name of the realm (if only slightly).\n  > > I have to agree with this first quibble quite a bit. In an actual product\n  > > implementation of message digest we have had some issues arrise because if\n  > > the server operator wants to change their realm, their entire user/password\n  > > database suddently becomes inoperative.\n  > > \n  > \n  > The reason that the realm is encoded with the user and password in the\n  > server password file is that people tend to reuse the same password.\n  > If only the username and password are encoded and put in the password\n  > file then the maintainer of server A, knowing H( username:password)\n  > for his server can use this to gain access to those documents on\n  > server B to which username has access.  This is assuming that the user\n  > has the same password on both servers.\n  [...]\n\nFair enough.  How about using the server-name in place of realm, then?\n(After all, it's possible two webmasters might choose the same realm\nname on different servers, isn't it!) That would render the same\nusername/password combination unique on different machines.  So the\nstored hash would be:\nH(<username> : <server-domain-name> : <password>)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": ">\n>Fair enough.  How about using the server-name in place of realm, then?\n>(After all, it's possible two webmasters might choose the same realm\n>name on different servers, isn't it!) That would render the same\n>username/password combination unique on different machines.  So the\n>stored hash would be:\n>        H(<username> : <server-domain-name> : <password>)\n\nThis isn't any better, given that one user may have multiple occurences of\nthe same name and password for different realms. (It happens!) The best\nwould be a combination of host domain name and realm name.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "According to Chuck Shotton:\n> \n> According to Kristol:\n> >Fair enough.  How about using the server-name in place of realm, then?\n> >(After all, it's possible two webmasters might choose the same realm\n> >name on different servers, isn't it!) That would render the same\n> >username/password combination unique on different machines.  So the\n> >stored hash would be:\n> >        H(<username> : <server-domain-name> : <password>)\n> \n> This isn't any better, given that one user may have multiple occurences of\n> the same name and password for different realms. (It happens!) The best\n> would be a combination of host domain name and realm name.\n> \n\nThis would mean that only one hostname could be used in the URL.  I.e.\neven though host.com and www.host.com are the same host, one of the URLs\n\nhttp://host.com/secret.doc\nand\nhttp://www.host.com/secret.doc\n\nwould have to fail even when the user supplied a valid username/password.\nThis would be a serious flaw.\n\nKeep in mind that the realm can be any (reasonable sized) string supplied by\nthe server maintainer.  Thus choosing a realm like\n\nmyrealm@www.myplace.com\n\nis probably a good idea.  This would prevent another server maintainer\naccidentally choosing the same realm.  If another server maintainer \nmaliciously chooses the same realm, at least that fact is displayed\nto the client each time access is requested.  If you connect to \nwww.myplace.com and see a realm with somewhere.else.com in it you \nshould be very suspicious.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "cshotton@biap.com (Chuck Shotton) said:\n  > >[Dave Kristol said:]\n  > >Fair enough.  How about using the server-name in place of realm, then?\n  > >(After all, it's possible two webmasters might choose the same realm\n  > >name on different servers, isn't it!) That would render the same\n  > >username/password combination unique on different machines.  So the\n  > >stored hash would be:\n  > >        H(<username> : <server-domain-name> : <password>)\n  > \n  > This isn't any better, given that one user may have multiple occurences of\n  > the same name and password for different realms. (It happens!) The best\n  > would be a combination of host domain name and realm name.\n\nTrue enough.  But encoding the realm name (along with host domain name)\nin the stored string would return me to the dilemma I had before:  what\nif I need/want to change domain name?  The entire password file becomes\ninvalid.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "At 10:17 AM 7/17/95, dmk@allegra.att.com wrote:\n>Fair enough.  How about using the server-name in place of realm, then?\n>(After all, it's possible two webmasters might choose the same realm\n>name on different servers, isn't it!) That would render the same\n>username/password combination unique on different machines.  So the\n>stored hash would be:\n>H(<username> : <server-domain-name> : <password>)\n\nIt may not be obvious to a client which of several CNAMEs for a particular\nserver should be used, (this relates to the vanity-names/URL issue).\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "John Franks <john@math.nwu.edu> said:\n  [regarding my proposal to embed hostname in the password file line]\n  > This would mean that only one hostname could be used in the URL.  I.e.\n  > even though host.com and www.host.com are the same host, one of the URLs\n  > \n  > http://host.com/secret.doc\n  > and\n  > http://www.host.com/secret.doc\n  > \n  > would have to fail even when the user supplied a valid username/password.\n  > This would be a serious flaw.\n\nI disagree with the premise.  I wouldn't encode the domain name that\nthe user accessed to reach my server.  I would encode the name that the\nserver uses for itself, for example the name set by NCSA HTTPD\nServerName directive.\n  > \n  > Keep in mind that the realm can be any (reasonable sized) string supplied by\n  > the server maintainer.  Thus choosing a realm like\n  > \n  > myrealm@www.myplace.com\n  > \n  > is probably a good idea.  This would prevent another server maintainer\n  > accidentally choosing the same realm.  If another server maintainer \n  > maliciously chooses the same realm, at least that fact is displayed\n  > to the client each time access is requested.  If you connect to \n  > www.myplace.com and see a realm with somewhere.else.com in it you \n  > should be very suspicious.\n\nThank you for motivating my second quibble, namely:  I want to be able\nto specify a (user/password) prompt independent of the realm.  I don't\nthink much of a realm named \"myrealm@www.myplace.com\", but maybe I'm\nperverse.  I prefer\nEnter username for [prompt that I specify] at www.research.att.com:\nto\nEnter username for myrealm@www.myplace.com at www.myplace.com\n\nEvidently (sigh) I'm the only person in the world who feels this way.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "According to Dave Kristol:\n> John Franks <john@math.nwu.edu> said:\n>   [regarding my proposal to embed hostname in the password file line]\n>   > This would mean that only one hostname could be used in the URL.  I.e.\n>   > even though host.com and www.host.com are the same host, one of the URLs\n>   > \n>   > http://host.com/secret.doc\n>   > and\n>   > http://www.host.com/secret.doc\n>   > \n>   > would have to fail even when the user supplied a valid username/password.\n>   > This would be a serious flaw.\n> \n> I disagree with the premise.  I wouldn't encode the domain name that\n> the user accessed to reach my server.  I would encode the name that the\n> server uses for itself, for example the name set by NCSA HTTPD\n> ServerName directive.\n\nHow is the client supposed to know this?  You'll have to make further\nadditions to the protocol.  Maybe I am confusing who said what but\ndidn't you also complain that encoding the hostname would make it\nimpossible to move the password file to a new host?  This is a good\npoint and suggests a realm containing the enterprise name, but not the\nhost name -- something like \"group@Enterprise_Name\" e.g.\n\"Engineering@ATT_Bell_Labs\".\n\n> \n> Thank you for motivating my second quibble, namely:  I want to be able\n> to specify a (user/password) prompt independent of the realm.  I don't\n> think much of a realm named \"myrealm@www.myplace.com\", but maybe I'm\n> perverse.  I prefer\n> Enter username for [prompt that I specify] at www.research.att.com:\n> to\n> Enter username for myrealm@www.myplace.com at www.myplace.com\n> \n> Evidently (sigh) I'm the only person in the world who feels this way.\n> \n\nThe prompt displayed is entirely up to the browser.  Different\nbrowsers will do it differently with different kinds of dialog boxes.\nThey seem to all display the realm.  If you want to get them all to\nagree to let the server supply the prompt you have a tough task.\n\nThere may also be security issues since I could display a prompt \nconcealing the fact that I am using a realm identical to someone elses.\nDisplaying a prompt and a realm would be too much.\n\nThis discussion has highlighted the importance of displaying the\nrealm and users not entering a password unless the realm agrees with\nenterprise they are accessing!\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "John Franks <john@math.nwu.edu> said:\n  [I said:]\n  > > I disagree with the premise.  I wouldn't encode the domain name that\n  > > the user accessed to reach my server.  I would encode the name that the\n  > > server uses for itself, for example the name set by NCSA HTTPD\n  > > ServerName directive.\n  > \n  > How is the client supposed to know this?  You'll have to make further\n  > additions to the protocol.  Maybe I am confusing who said what but\n  > didn't you also complain that encoding the hostname would make it\n  > impossible to move the password file to a new host?  This is a good\n  > point and suggests a realm containing the enterprise name, but not the\n  > host name -- something like \"group@Enterprise_Name\" e.g.\n  > \"Engineering@ATT_Bell_Labs\".\nYou're right -- the client doesn't know the name.  Stupid idea on my part.\n[...]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Report: Problems with the Expires header",
            "content": "This is to announce the availability of a report on problems with the\nHTTP Expires header.\n\nThis report is available in hypertext form at\nhttp://www.amazon.com/expires-report.html, and in plain text form at\nhttp://www.amazon.com/expires-report.txt.  For discussions about the\nreport, we suggest using the www-talk mailing list.\n\nExcerpts from the text version of the report are included below.\n\nKoen.\n\n------------------------------------------------------------------------\n\n  PROBLEMS WITH THE EXPIRES HEADER\n\n  Dynamic documents vs. history functions\n\n                                        Koen Holtman, koen@win.tue.nl\n                                        Shel Kaphan, sjk@amazon.com\n19 July 1995\n\nSUMMARY\n\nDevelopments in the implementation of browser history functions\nprevent the HTTP Expires header from becoming the standard, generally\nusable mechanism for controlling the behavior of caches.\n\nTo allow all types of dynamic services, which by their nature need to\nrestrict caching, to be implemented in a simple, reliable, and efficient\nway, the HTTP specification must be extended.  We will propose several\nalternative solutions.\n\nSome of the proposed HTTP extensions would not only fix problems of\ncache control, but their acceptance would also add a means of\ncontrolling the contents of browser history lists.  Better control\nover history list contents is needed for some types of dynamic\nservices.\n\n\nAUDIENCE\n\nThis report discusses problems concerning cache control mechanisms\nthat are relevant to browser authors, the authors of dynamic web\nservices, and everyone contributing to the future development of HTTP.\n\nWe will present several alternative proposals for extensions to the\nHTTP specification that solve the discussed cache control problems.\nWe hope that this report will generate a discussion leading to a\nconsensus about which alternative is best.\n\n[...]\n\nTABLE OF CONTENTS\n\n    Introduction\n    What The HTTP Specification Says\n    What The HTTP Specification Does Not Say\n    Why Does This Matter?\n    An Example\n    What Browsers Do\n    Disadvantages Unrelated to Network Load\n    Uncoupling Cache and History\n    What Is Needed\n    List of Proposals\nproposal 1: do nothing\nproposal 2: discourage honoring expires in history functions\nproposal 3A: add a header to control history: History-Control\nproposal 3B: add a header to control history: History-Expires\nproposal 4A: add a second way of disabling caching: pragma no-cache\nproposal 4B: add a second way of disabling caching: last-modified\nproposal 5: conditional posts\n    Summary of Proposal Advantages and Disadvantages\n\n[...]\n\nSUMMARY OF PROPOSAL ADVANTAGES AND DISADVANTAGES\n\n[...]\n\n|---------------------------------|----|----|----|----|----|----|----|\n|Proposal number                  |  1 |  2 | 3A | 3B | 4A | 4B |  5 |\n|---------------------------------|----|----|----|----|----|----|----|\n|Solves Expires/caching problem   | -- | ++ | ++ | ++ | ++ | ++ |  + |\n|Browsers now conform             | ++ | -- |  0 |  0 |  0 |  + |  0 |\n|Quick move to conformance        |  + |  - |  + |  - |  + |  + | -- |\n|---------------------------------|----|----|----|----|----|----|----|\n|Allows history function control  |  0 |  0 | ++ |  + |  0 |  0 |  0 |\n|---------------------------------|----|----|----|----|----|----|----|\n| Work needed on: (more is worse) |    |    |    |    |    |    |    |\n|       - HTTP specification      |  0 |  - | -- | -- | -- | -- | -- |\n|       - New browser code        |  0 |  - | -- | -- |  - | -- | -- |\n|       - New proxy code          |  0 |  0 |  0 |  0 |  - |  0 | -- |\n|       - New server code         |  0 |  0 |  0 |  0 |  0 |  0 | -- |\n|---------------------------------|----|----|----|----|----|----|----|\n|Overall conclusion               | -- |  + | ++ |  + |  0 |  0 |  - |\n|---------------------------------|----|----|----|----|----|----|----|\n\nMeaning of ratings:\n  -- very bad  /  - bad  /  0 neutral  /  + good  /  ++ very good\n\n\n\n"
        },
        {
            "subject": "406 None Acceptabl",
            "content": "***The 406 None Acceptable response\nWe have a couple of problems with the 406 None Acceptable response.\nJim Seidman (also here at Spyglass) posted these some time ago, but\nwe didn't any proper resolution, so I'm asking for info on how these\nissues stand:\n\nFirst, it specifies that Content-*\nheaders are required.  If there's more than one version of the file,\nwhich set of content headers do we send?  That is, if there's a PNG and a\nTIFF version of a file, and the client requests only accepts GIF and JPEG,\nwhich info do we send?  It's ambiguous to just specify\nmultiple content-types, content-languages, etc., since you could have a\nsituation where you have a French PostScript file and a German HTML file.\n\nSecond, it says you can't have any entity body.  What are browsers supposed\nto display in this case, since for every other error the server can return\nan entity body giving explanatory text?\n\n--\nEric W. Sink\nSenior Software Engineer, Spyglass\neric@spyglass.com\n\n\n\n"
        },
        {
            "subject": "Re: 406 None Acceptabl",
            "content": ">***The 406 None Acceptable response\n\nThis was discussed at the Danvers IETF, and will be in the next version\nof the spec.\n\nThe 406 response will be changed to return content, in which the\navailable variants are described, preferably in a format that can\nbe used by a browser to select the appropriate choice.\n\nIn other words, a URC in HTML format.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Report: Problems with the Expires header",
            "content": "On Thu, 20 Jul 1995, Koen Holtman wrote:\n> This is to announce the availability of a report on problems with the\n> HTTP Expires header.\n> This report is available in hypertext form at\n> http://www.amazon.com/expires-report.html, and in plain text form at\n> http://www.amazon.com/expires-report.txt.  For discussions about the\n> report, we suggest using the www-talk mailing list.\n\nOK, I've just read through the report and my vote goes to option 2\n(Discourage honouring expires in history functions).  This gives us\nsomething to hammer browser writers over the head with when they don't\nconform to the spec, doesn't break things and is simple to do.  Also just\nbecause Netscape does it a different way now doesn't mean that we\nshouldn't write the spec in a different way and hope they'll abide by it\n(there's plenty of other HTML and HTTP things that will have to change in\nNetscape in the next couple of years if they want to conform to the specs \nand this is a simple one for them to fix).\n\nI don't like 3A or 3B at all as it gives the CGI author control over _my_\nhistory list.  To me the history list is just that - my personal history\nof where I've been and what I've seen (notice the tenses there people :-)\n).  I'd rather not give Joe Random CGI-Author a remote control for any of\nit.  If this goes in the spec then there'll be at least one \nnon-conforming browser (my hacked X Mosaic! :-) ).\n\nOtherwise I agree with the authors' disadvantages to options 1, 4A and \n4B. \n\nJon\n\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nJon Knight, Researcher, Sysop and General Dogsbody, Department of Computer\nStudies, Loughborough University of Technology, Leics., ENGLAND.  LE11 3TU.\n*** Nothing looks so like a man of sense as a fool who holds his tongue ***\n\n\n\n"
        },
        {
            "subject": "Content-TransferEncoding &quot;packet&quot",
            "content": "Hi all,\n\nTo get the ball rolling again on the features of HTTP/1.1,\nI'd like to get a firm definition of the packetized transfer\nencoding.  The packet CTE will be used to safely delimit the\nsize of dynamically generated response messages and (if the\nserver is known to be 1.1 or above) requests with content.\n\nThe last time I posted my notes regarding the format, I referred\nto it as being basically the same as that proposed by Dan Connolly\nway back in<9409271503.AA27488@austin2.hal.com> on www-talk.\nI'd like to change that now, before we get products out on the\nstreet that use the sucker.\n\nAfter investigating the needs of the security/authentication folks\nregarding the desire to sign the content of a message after it\nis generated, Phill Hallam-Baker has proposed adding a footer to\nthe format that would contain post-generation Entity-Header fields.\n\nAlso, I've been playing around with various formats and have\nfound that the optimum for most transfers uses a simple one-byte\nprefix to encode the length of each packet, with a zero byte\nindicating end-of-packets.\n\nSo, without further ado, here's the format:\n\nContent-Transfer-Encoding: packet\n\n    Entity-Body = *( packet ) NUL\n                  footer\n                  CRLF\n\n    NUL         = <octet 0>\n\n    packet      = packet-size packet-data\n\n    packet-size = <OCTET, excluding octet 0, representing\n                   the unsigned integer (1-255)>\n\n    packet-data = packet-size(OCTET)\n\n    footer      = *( Entity-Header )\n\nNote that the footer is terminated by an empty line (just like the\nheaders) and is optional.  The semantics of a footer are as if the\ngiven Entity-Header fields are part of the message headers.\n\nThe un-packetizer algorithm is fairly simple.\n\n    length := 0\n    packet-size := read-byte\n    while (packet-size > 0) do\n    {\n        read input until amt-read == packet-size\n        write buffer to BODY\n        length := length + packet-size\n        packet-size := read-byte\n    }\n    line := read-line\n    while (line is not empty) do\n    {\n        process Entity-Header\n        append header to HEADERS\n    }\n    replace Content-Length value with length.\n\nIn other words, the result looks like a normal message, with\nthe Content-Length computed by the unpacketizer.  In actuality,\nit is often faster to read packet-size + 1 bytes, slurping in\nthe next packet-size as part of the prior packet read.\n\nSo, is that enough detail for discussion?  I will try to get\nthe updated 1.0 spec done this week, with the 1.1 spec soon\nafter that.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re:  Content-TransferEncoding &quot;packet&quot",
            "content": "Roy Fielding <fielding@beach.w3.org> said:\n\n  [...]\n  > Also, I've been playing around with various formats and have\n  > found that the optimum for most transfers uses a simple one-byte\n  > prefix to encode the length of each packet, with a zero byte\n  > indicating end-of-packets.\nCould you elaborate?  I'd be curious to know the tradeoffs you examined\nbefore choosing this approach.  In particular, was the overhead of an\nASCII packet length (i.e., human readable) so onorous?\n  > \n  > So, without further ado, here's the format:\n  > \n  > Content-Transfer-Encoding: packet\n  > \n  >     Entity-Body = *( packet ) NUL\n  >                   footer\n  >                   CRLF\n  > \n  >     NUL         = <octet 0>\n  > \n  >     packet      = packet-size packet-data\n  > \n  >     packet-size = <OCTET, excluding octet 0, representing\n  >                    the unsigned integer (1-255)>\n  > \n  >     packet-data = packet-size(OCTET)\n  > \n  >     footer      = *( Entity-Header )\n  > \n  > Note that the footer is terminated by an empty line (just like the\n  > headers) and is optional.  The semantics of a footer are as if the\n  > given Entity-Header fields are part of the message headers.\n\n  [most of un-packetizer algorithm omitted]\n  replace Content-Length value with length.\n  > \n  > In other words, the result looks like a normal message, with\n  > the Content-Length computed by the unpacketizer.  In actuality,\n  > it is often faster to read packet-size + 1 bytes, slurping in\n  > the next packet-size as part of the prior packet read.\n\nThe proposal looks reasonable to me.  I have these questions:\n\n1) If I understand correctly, there is always at least a blank line\nfollowing packetized data (for a null footer Entity-Header).  True?\n\n2) Does the footer applies only for C-T-E \"packet\", or would it\napply to other (to be specified) C-T-E's?\n\n3) What happens if (yes, it would be bizarre) one of the footers is\nContent-Length?  (It looks like it gets overwritten with the one that\nthe packetizer computes.)\n\n4) Will support for packetization be a required part of HTTP/1.1?\n\n5) How will acceptable packet sizes be negotiated (or specified)?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "According to Dave Kristol:\n> Roy Fielding <fielding@beach.w3.org> said:\n> \n>   [...]\n>   > Also, I've been playing around with various formats and have\n>   > found that the optimum for most transfers uses a simple one-byte\n>   > prefix to encode the length of each packet, with a zero byte\n>   > indicating end-of-packets.\n> Could you elaborate?  I'd be curious to know the tradeoffs you examined\n> before choosing this approach.  In particular, was the overhead of an\n> ASCII packet length (i.e., human readable) so onorous?\n\nAlso a maiximum packet size of 255 bytes seems quite small.  Could you\nexplain the rationale for that?\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">Could you elaborate?  I'd be curious to know the tradeoffs you examined\n>before choosing this approach.  In particular, was the overhead of an\n>ASCII packet length (i.e., human readable) so onorous?\n\nAn ASCII decimal format is, yes.  Human readable was not an issue\nin this case.\n \nThe advantages that the one-byte packet-size has is that it takes up\na minimal amount of space in the stream of bits to the transfer and\nis trivial for any system to produce or consume.  Allowing larger\npackets means we have to use decimal (with the additional CRLF delimiters)\nor hope that everyone remembers to read the number in network byte order.\n \n*NOTE*  We need to keep in mind that the CTE packet size has little\n        to do with the TCP packet size.  There is absolutely nothing\n        preventing an application from writing up to X CTE packets\n        per network write, where\n \n                      Usable Window\n                  X = -------------\n                          256\n \nIt does have an impact on the receiving application's read buffer\nsize, but that is usually not an issue these days (most systems are\ncapable of reading data from the TCP buffers much faster than it gets\nfilled, so the application's reads are not in the critical path).\n\nThe BIG question is what effect it will have on the number of\ninternal system calls and data copies required by the server or \nthe client.  That I do not know, and Jim Gettis (the W3C person who\nis about to start looking into HTTP/2.x) mentioned earlier today\nthat they may overwhelm any gain from simplicity.  Of course, this\nis also dependent on the average size of dynamic data transfers\n(i.e., if the vast majority of transfers is \"small\", than a small\npacket size is preferable).\n\n===================================================================\nSo, here is a task for the WG:\n\n   If you have a simulator handy (or just some free time on your\n   hands), please attempt to analyze the behavior of a representative\n   sample of dynamic responses, with the idea being a comparison\n   between the simple \"256packet\" implementation of my last message\n   <199507230403.AAA10044@beach.w3.org> and the following\n   \"decimal packet\" CTE:\n\n   Entity-Body = *( packet ) \n                 \"0\" CRLF\n                 footer\n                 CRLF\n \n   packet      = packet-size packet-data\n \n   packet-size = (\"1\"|\"2\"|\"3\"|\"4\"|\"5\"|\"6\"|\"7\"|\"8\"|\"9\") *DIGIT\n \n   packet-data = packet-size(OCTET) CRLF\n \n   footer      = *( Entity-Header )\n\nand report your results back to the mailing list.  Keep in mind\nthat the goal is consensus and submittal of a proposed standard RFC\non HTTP/1.1 by September 21.\n===================================================================\n\n>1) If I understand correctly, there is always at least a blank line\n>following packetized data (for a null footer Entity-Header).  True?\n\nThat depends on where you consider the beginning of the line to be.\nIf you consider it to be after the zero (NUL), then yes, since the\n256packet scheme would end in\n   <last-packet> NUL CRLF\nif there is no footer.\n\nIn the decimal packet scheme there would indeed always be an empty line,\nsince it would end in\n   <last-packet> 0 CRLF CRLF\nif there is no footer.\n\n>2) Does the footer applies only for C-T-E \"packet\", or would it\n>apply to other (to be specified) C-T-E's?\n\nOnly to CTE packet, unless we defined more new CTEs (unlikely,\nexcept for the possibility of a packet64 CTE for 7bit transfer).\n\n>3) What happens if (yes, it would be bizarre) one of the footers is\n>Content-Length?  (It looks like it gets overwritten with the one that\n>the packetizer computes.)\n\nIt gets overwritten (in fact, it *must* be overwritten).\n\n>4) Will support for packetization be a required part of HTTP/1.1?\n\nYes.\n\n>5) How will acceptable packet sizes be negotiated (or specified)?\n\nWell, if we have small packets, they won't be.  If we have decimal\npackets, then that is another matter for discussion.  Does it matter?\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">  > The advantages that the one-byte packet-size has is that it takes up\n>  > a minimal amount of space in the stream of bits to the transfer and\n>  > is trivial for any system to produce or consume.  Allowing larger\n>  > packets means we have to use decimal (with the additional CRLF delimiters)\n>  > or hope that everyone remembers to read the number in network byte order.\n>\n>I acknowledge a small space improvement (see below).  I wouldn't rank\n>it as \"onorous\".  I think \"network byte order\" is a red herring -- you\n>would be converting a decimal number to binary.\n\nThat is an \"or\", as in we could send the number using a binary integer\nif the integer was restricted to network-byte-order interpretation, but\nimplementations are notorious for screwing that up in spite of the specs.\n\n>Here is a comparison of the two for selected message sizes.  (Please\n\nThat is a useful comparison for data transfer overhead (thanks),\nbut what we really need is a comparison of processing overhead,\ntaking into account the vagaries of TCP socket reads/writes.\n\nKeep it coming ....\n\n\n.......Roy\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "        I very much prefer the design Dan originally posted\n\n                Content-Transfer-Encoding: packet\n\n                15\n                fifteen bytes\n\n        where the  1 5 CR LF  indicate  \"read exactly fifteen octets\"\n(the example happening to have a CR and LF at the end of the printable\ncharacters),  at which point the receiver (either client or server)\nlooks for another ASCII byte count,  always terminated by CR/LF.\n\n        I do not see the problem with using ASCII numerals terminated\nby CR/LF.   Though I expect quite a few real transfers using such a\nmethod to be effectively unreadable,  it's likely that the \"packets\"\nwill often be large enough to hold the entire \"file\",  in which case\nthe transfers will again be humanly readable.   I expect this latter\nto happen frequently in a development scenario,  and keeping the\nprotocol \"eyeballable\" to developers only helps.   Simplify life\nfor the developers and you get a more solid product and get it\nto market quicker.   CPU time is less expensive than man hours.\n\n>The last time I posted my notes regarding the format, I referred\n>to it as being basically the same as that proposed by Dan Connolly\n>way back in<9409271503.AA27488@austin2.hal.com> on www-talk.\n>I'd like to change that now, before we get products out on the\n>street that use the sucker.\n\n        I disagree.   I see no justification for not using it.\n\n>Also, I've been playing around with various formats and have\n>found that the optimum for most transfers uses a simple one-byte\n>prefix to encode the length of each packet, with a zero byte\n>indicating end-of-packets.\n\n        I like this idea;  just pick another name.   I hope that\nDan's \"packet\" CTE will see the light of day  (even though he may\nhimself have abandonned it by now),  and maybe reserving that name\nwill help.   Maybe call yours  \"pack8bit\"?\n\n        Another variation might be to call all of them \"packet\",\nand require a parameter for the prefix:  8bit, 16bit, or ASCII.\n\n>In other words, the result looks like a normal message, with\n>the Content-Length computed by the unpacketizer.  In actuality,\n>it is often faster to read packet-size + 1 bytes, slurping in\n>the next packet-size as part of the prior packet read.\n\n        That's an optimization that works on some systems;\nnot on others.   Keep the protocol free from platform specifics.\n\n> ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n>                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n>                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n        Thanks for the contribution!\n\n--\nRick Troth <troth@ua1vm.ua.edu>, Houston, Texas, USA\nhttp://ua1vm.ua.edu/~troth/\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "    >In other words, the result looks like a normal message, with\n    >the Content-Length computed by the unpacketizer.  In actuality,\n    >it is often faster to read packet-size + 1 bytes, slurping in\n    >the next packet-size as part of the prior packet read.\n    \n    That's an optimization that works on some systems;\n    not on others.   Keep the protocol free from platform specifics.\n\nAlthough I strongly favor the use of ASCII encodings of \"packet\"\nlengths (perhaps we could use a term besides \"packet\", which will\ncause untold confusion for years to come), I agree that it might\nbe nice to use an encoding that occupies a fixed number of bytes.\nIt's not exactly \"platform-specific\" to optimize things in this\nway; it's basically a tradeoff between these two algorithms:\n\n[*** Anti-paranoia warning: I'm writing these examples using UNIX\n *** APIs for concreteness, NOT because this is the the only API\n *** I care about.  So don't flame me about that, please.]\n\n   Naive algorithm:\n\nwhile (not done)\nfgets(length, sizeof(length), stream);\nn = decode(length);\nfread(buffer, n, 1, stream);\nend\n\n   Clever algorithm:\n\nfgets(length, sizeof(length), stream);\nn = decode(length)\nwhile (not done)\nfread(buffer, n+sizeof(length), 1, stream);\nn = decode(&buffer[n]);\nend\n\nThe clever algorithm uses M+1 instead of 2*M reads from the\ninput stream.  This is especially important if your code\ndoesn't want to read past the end of the input stream (cf.\nthe NCSA httpd server), in which case you would write this\nusing read() system calls instead of using stdio buffering.\n\nNote also that if we use a simple ASCII encoding, you need\nto parse the input stream to figure out where the end of\nthe \"packet\" length string is.  If we use an encoding that\ntakes a fixed number of columns, then you don't need to parse\nanything, you just grab a bunch of bytes.\n\nSince I also don't see any reason to limit the \"packet\" size\nto just 256 bytes, or to any small value, I recommend that the\nlength field be fairly large.  For example, suppose that the\nstandard says \"use 6 digits + CR + LF\".  Then this leaves\nthe following data nicely aligned (even for 64-bit machines),\nand allows almost a million bytes per \"packet\".  Better yet,\nuse a hexadecimal encoding; this allows >16 million bytes, and\nis much easier to encode and decode.\n\nExample:\n0 0 0 0 0 8 CR LF\neight bytes of data\n1 0 0 0 0 0 CR LF\n1,048,576 bytes of data\n0 0 0 0 0 0 CR LF\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">Although I strongly favor the use of ASCII encodings of \"packet\"\n>lengths (perhaps we could use a term besides \"packet\", which will\n>cause untold confusion for years to come), I agree that it might\n>be nice to use an encoding that occupies a fixed number of bytes.\n\nWe could call it \"chunky\" if you like... I prefer that over \"chunks\",\nthough I'm sure we could have no end of amusement talking about\nblowing chunks across the network.  ;-)\n\n>Note also that if we use a simple ASCII encoding, you need\n>to parse the input stream to figure out where the end of\n>the \"packet\" length string is.  If we use an encoding that\n>takes a fixed number of columns, then you don't need to parse\n>anything, you just grab a bunch of bytes.\n\nGood point -- I prefer that as well (in fact, knowing the size of\nthe size ahead of time was one of the main reasons to do a 256chunk).\n\n>Since I also don't see any reason to limit the \"packet\" size\n>to just 256 bytes, or to any small value, I recommend that the\n>length field be fairly large.  For example, suppose that the\n>standard says \"use 6 digits + CR + LF\".  Then this leaves\n>the following data nicely aligned (even for 64-bit machines),\n>and allows almost a million bytes per \"packet\".  Better yet,\n>use a hexadecimal encoding; this allows >16 million bytes, and\n>is much easier to encode and decode.\n\nHmmm, fixed hex sounds reasonable, but 6 digits is a bit overboard.\nAnd why would we need the CRLF if the size is fixed?  It does not\nimprove readability (aside from the first chunk).\n\nHow about four hex bytes:\n\n0 0 0 8\neight bytes of data\nF F F F\n65,535 bytes of data\n0 0 0 0 CR LF\n<footer>\nCR LF\n\nI would still prefer to see some hard data on the performance\ndifferences.  Unlike the rest of HTTP/1.1, this has to be right\nthe first time its distributed in beta code.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">        I like this idea;  just pick another name.   I hope that\n>Dan's \"packet\" CTE will see the light of day  (even though he may\n>himself have abandonned it by now),  and maybe reserving that name\n>will help.   Maybe call yours  \"pack8bit\"?\n\nNo, there will be only one packetized 8bit-clean CTE that will be\nrequired for HTTP/1.1 compliance.  That is why we are having this\ndiscussion now, instead of after a full draft is produced.\n\n>>In other words, the result looks like a normal message, with\n>>the Content-Length computed by the unpacketizer.  In actuality,\n>>it is often faster to read packet-size + 1 bytes, slurping in\n>>the next packet-size as part of the prior packet read.\n>\n>        That's an optimization that works on some systems;\n>not on others.   Keep the protocol free from platform specifics.\n\nFolks, I am getting a little tired of this line of response\nwithout any thought behind it.  There is absolutely nothing\nplatform-specific about preferring 1 read over two reads.\nIf you have a technical disagreement, fine, but the next person\nwho cries \"platform specific\" better damn well include at least\none concrete example wherein the protocol prefers one platform over\nanother, and not just because of differring TCP implementation bugs.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "I became a convert to the 'boundary' method for delimiting otherwise\nunbounded data, as is done with multipart/* messages. E.g.,\n\ncontent-transfer-encoding: bounded-binary; boundary=\"xxxxxx\"\n\nwould signal that the actual data started with\n\n--xxxxxx\n\nand ended with\n\n--xxxxxx--\n\nThis is simple for both the sender and recipient; the packetized\nencodings seem messy. I'd worry that we'd need an\n'accept-transfer-encoding' to allow negotiation of CTE, though.\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "Roy,\none thing.\nwould you mind not calling it a content-transfer-encoding?\nThe possibility of something with a c-t-e of \"packet\" escaping into\nmail, or the possibility of something with (for some reason) c-t-e base64\nlosing its c-t-e because it's being sent with \"packet\" disturbs my sense\nof orthogonality.\n\n\"Content-encoding\" I wouldn't mind, since it is wholly a Web thing;\n\"Transfer-mode\" I wouldn't mind either, for the same reason, but throwing\na packetized transfer mode into a field used for some other purpose\ndisturbs me deeply.\n\nYes, it should have been \"content-encoding\" in MIME, because that fits\nwhat it really is better, but it's too late to change MIME now.\n\n         Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "I thought 'content-transfer-encoding' was appropriate, but might\nprefer to name it more explicitly as 'binary-packet'.  You wouldn't\nsend c-t-e base64 in a binary-packet encoding just as you can't nest\nother MIME c-t-e.\n\n(On a related thought, I've been thinking of a new MIME top-level type\ncalled 'container', where 'container/zip' or 'container/tar' or\n'container/bento' might be allowable registered types. The\ninterpretation is that a container contains one or more other objects\npacked together in a binary encoding; the goal is to forstall the\nunfortunate current use of multipart/zip.)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "    Hmmm, fixed hex sounds reasonable, but 6 digits is a bit overboard.\n\nWell, I was assuming that 4-byte or 8-byte alignment is a Good Thing\nfor fast processing, and that we would use CRLF.  With 2 bytes taken\nup by CRLF, that leaves either 2 hex digits or 6 hex digits.\n\nTo quote Gordon Bell and Bill Strecker, \"There is only one mistake that\ncan be made in computer design that is difficult to recover from -- not\nhaving enough address bits for memory addressing and memory\nmanagement.\"[1]  That seems to apply to network protocols, too: look\nat the problems with TCP sequence number space and window size, and\nwith NFS request size and file offset size.\n\n4 hex digits *might* be enough, but since (as you've observed several\ntimes) we won't have a chance to fix this once it is widely\nimplemented, I'd vote for 6 or 8 hex digits, just to be safe.\n\n    And why would we need the CRLF if the size is fixed?  It does not\n    improve readability (aside from the first chunk).\n    \nI suppose that's open to interpretation.  Anyway, if you buy the\nargument that 6 hex digits are worth having, and you buy the argument\nthat the fields should 4-byte aligned, then the CRLF doesn't really\ncost anything.\n\n-Jeff\n\n[1] C. G. Bell and W. D. Strecker, \"Computer structures: What have we\nlearned from the PDP-11?\", Proc. Third Annual Symposium on Computer\nArchitecture, Pittsburgh, PA, pp 1-14.  January, 1976.\n\n\n\n"
        },
        {
            "subject": "Proposal:  Checksum header or metho",
            "content": "A \"Last-modified\" header currently exists to provide information on the \nmodification of a file.  However, this information can be inaccurate if \nthe document is produced periodically by an automatic generator, as in \nthe case of user's homepages.  Simple file length may not suffice, so it \nwould be better to determine the checksum.  Although the client could \ncalculate this, it makes more sense to do so on the server side, to \npreserve bandwidth.\n\nThis \"ckecksum\" information could be implemented either as a header to be \nprovided by the GET and HEAD methods.  However, since few clients would \nrequire such information, it makes more sense to restrict it to special \nmethods:  either HEAD, which anyway exists for such purposes, or perhaps \nsome new method.\n\n\n     -Mordechai T. Abzug\nHomepage:  http://www.gl.umbc.edu/~mabzug1   send email to:  mabzug1@umbc.edu\n(GC-3) GCS/S d++ s+:- a-- c++++ UI++>++++ P+++>++++ L E W++ N++ K? w---\n       O(+++) M- V-- PS+ PE Y+ PGP t+ 5++ R tv b+++>$ DI D++ G e h! r--- !y\nDiscoveries are made by not following instructions. \n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "    Simple file length may not suffice, so it would be better to determine\n    the checksum.  Although the client could calculate this, it makes more\n    sense to do so on the server side, to preserve bandwidth.\n\nThe use of a simple checksum might be too prone to false positives.\nI'm not an expert on the topic, but apparently people have developed\n\"fingerprinting\" algorithms that have very low probabilities of\nmapping two texts to the same value.\n\nI found one citation on the topic:\nRabin, M. O.\n\"Fingerprinting by Random Polynomials\"\nCenter for Research in Computing Technology\nHarvard University\nReport TR-15-81, 1981\n\nbut I suspect there are others.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">I became a convert to the 'boundary' method for delimiting otherwise\n>unbounded data, as is done with multipart/* messages. E.g.,\n>\n>content-transfer-encoding: bounded-binary; boundary=\"xxxxxx\"\n>\n>would signal that the actual data started with\n>\n>--xxxxxx\n>\n>and ended with\n>\n>--xxxxxx--\n>\n>This is simple for both the sender and recipient; the packetized\n>encodings seem messy. I'd worry that we'd need an\n>'accept-transfer-encoding' to allow negotiation of CTE, though.\n\nIn fact, the \"--xxxxxx--\" does not necessarily end the multipart\nbody, it is just the end of the interesting stuff.  Without additional\ncontraints on accuracy, multipart is worthless as a size delimiter.\nIn addition, it requires the receiver to scan the entire message\nbody for the delimiter, which has proven to be contrary to the design\ngoals of HTTP.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: 406 None Acceptabl",
            "content": ">is the new text in HTTP/1.0 or in HTTP/1.1?\n\nIt will be in HTTP/1.0.\n\n>Is anyone presently using 406?\n\nI don't know, but it has long been known to be a missing feature\nrequired to implement content negotiation correctly.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: 406 None Acceptabl",
            "content": "On Tue, 25 Jul 1995, Roy Fielding wrote:\n> >is the new text in HTTP/1.0 or in HTTP/1.1?\n> \n> It will be in HTTP/1.0.\n> \n> >Is anyone presently using 406?\n> \n> I don't know, but it has long been known to be a missing feature\n> required to implement content negotiation correctly.\n\nIn fact, it would seem like this would be the preferable way to do \ncontent negotiation without requiring hundreds of bytes of Accept: \nheaders.  A browser sends a small set of types it knows it can handle \n(html, gif, mpeg, etc), and if there are None Acceptible, it can select \nfrom the list of representations returned in the 406 response (by \nresending a request with just the content-type they want in the Accept:)\n\nTake that, lazy browser writers :)\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">would you mind not calling it a content-transfer-encoding?\n\nYes.  The proposed \"packet\" CTE is a transfer encoding.  That is,\nit indicates what (if any) type of transformation has been applied\nto the entity in order to safely transfer it between the sender and\nthe recipient. This differs from the Content-Encoding in that the\nCTE is a property of the message, not of the original resource.\n\n>The possibility of something with a c-t-e of \"packet\" escaping into\n>mail, or the possibility of something with (for some reason) c-t-e base64\n>losing its c-t-e because it's being sent with \"packet\" disturbs my sense\n>of orthogonality.\n\nThat is because they are not orthogonal.  As I mentioned earlier, we may\nwant to define a packet64 encoding for this case, but we do not want\nto allow a packet encoded message to be CTEd by another encoding.\nThe packetizing must be the last encoding applied to the message.\n\n>\"Content-encoding\" I wouldn't mind, since it is wholly a Web thing;\n>\"Transfer-mode\" I wouldn't mind either, for the same reason, but throwing\n>a packetized transfer mode into a field used for some other purpose\n>disturbs me deeply.\n\nIt fits the definition (if not practice) of MIME.  Those fields are\nextensible, so it is reasonable to extend them.  The packet scheme is\nnot a transfer mode -- it applies to the message body, and could be\nindependent of the protocol (if other protocols used it).\n\n>Yes, it should have been \"content-encoding\" in MIME, because that fits\n>what it really is better, but it's too late to change MIME now.\n\nI disagree -- that is not what content-encoding means in HTTP, nor\ndo I think it is too late to change MIME now.  Not allowing layered\nencodings is a fundamental problem with the media type mechanism.\nMIME chose to use a mechanism that replaces the content-type, thus\ndestroying some very important information.  HTTP can't do that,\nso we invented something different.  Internet mail will have to do\nthe same eventually, whether it is in the MIME spec or not.\n\nMy job (in HTTP/1.x) is to describe a protocol in which both can\ncoexist, using the same basic abstractions and theory, such that\nthe implementation of the protocol remains relatively simple.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "Roy,\ntwo points:\n\n- MIME is an installed base now, and we can't change labels on them just\n  because the meaning of the labels in current practice doesn't fit the\n  meaning of the labels in English\n- The MIME spec deliberately made it harder to register a c-t-e than it\n  is to register a content-type, because having more destroys interoperability.\n- There were fierce battles in the MIME group about whether to ban nested\n  c-t-es outright, discourage the practice, or allow it. I don't think that\n  it is a choice one should make without thinking about it carefully.\n\nIf you want to name it transfer-encoding, because that is what it is, fine,\nand if you want to remove content-transfer-encoding entirely from HTTP,\nI wouldn't mind too much, but using the same label with different semantics\nthan in MIME is, IMHO, going to lead to problems.\n\n       Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "In what way does what is being proposed here (the \"packet\" proposal\nhowever you name it) have different semantics from MIME c-t-e?\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "BASIC authentication is a serious security flaw and one which may compromise\nthe security of other systems. Many users use the same password on all their\naccounts. Thus a scheme which sends a password en-clair may result in other\nsystems being compromised. There is a similar problem with MUDS, MOOS etc.\n\nUser education is not an acceptable ?solution\". I simply don't want the\nsecurity of my system dependent on their competence.\n\nWithout public key being avaliable there is a choice, either the \nauthentication system will involve sending messages en-clair which are\neffectively authorisation tokens or there will be a stored database of clear\nauthentication tokens. One can combine the schemes but the system is still\ncompromised if both the transport and authentication database is compromised.\n\n\nWe thus took the decision based on the observation that the network is\neasily compromised, files stored on machines less so. I do not accept that\nBill Moriss's argument concerning keeping password files in the clear\nis or was ever a valid one, it is preferable to keep password files in\na form which prevents the access tokens from being deciphered. It is only\none secuirty concern and a minor one, sites which cannot protect the security\nof their password file probably arn't interested in security. The main\njustification I would give for one way encrypted password files would be to\nprotect the system from corrupt sysops.\n\nMDA is not proposed as a replacement for or competitor for SSL/S-HTTP. It is\nmerely a simple scheme that provides the best protection we could provide at\nlow cost.\n\n\nConcerning the tying of the username to the realm, this was a deliberate choice\non my part. If a user has the same username/password on multiple machines a\nsystem manager at one site could obtain access to the other if there was\nnothing to differentiate them. Its a realm name and not the server name to\npermit multiple servers to share the same authentication data. What is missing\nis the requirement that the realm name should be an INTERNIC reserved one, eg\nwe could use w3.org or blink.w3.org. I think this prevents collisions in the\ndesired manner.\n\nAnother reason for setting up the system in this way is to allow a user to\nprovide a password in such a way that the remote system need never know the\npassword. The authentication token is created in the client (eg forms i/f) and\nsent to the other side. if this transmission is insecure the security is of\ncourse compromised to an extent. It provides the user with better security\nguaranies when dealing with buletin boards, MUDs etc.\n\n\nAnother consideration, the password database could be regularly refreshed via a\nsecure transport. Consider a situation where the realm changes every 24 hours\nand a central server distributes new authentication tokens to remote systems\n(clints and servers) each day. This provides very kerberos like functionality.\n\n\nConcerning the nonce placement, I would prefer that additional schemes be\nprovided for rather than adjusting the default. This is because there is a\nvery substantial installed base of browsers with the draft scheme. I would also\nsggest that we do *not* discuss a new MD5 based scheme. I believe that a very\nmuch better scheme will shortly become avaliable. \n\nMoving the nonce to the end would not greatly impact security, the argument on\nthe nonce placement is mainly concerned with the function H ( k, m) where m is\nthe message. I deliberately specified H(k,H(m)) to ensure sealing of the\nmessage and prevent possible extension attacks while ensuring that the\ncryptographic distance of k, m to the result was comparable. I have since been\nshown that there is a \"question\" as to whether I should have incorporated\npadding or not. I suspect its not a problem since the MDA  authentication\npacket cannot be extended in a simple manner.\n\nI'm rather more concerned that this is not an appropriate use for MD5 than by\nthe structure of the keying mode. A keyed digest is a function of two variables\nnot one. There are simple modifications which could dramatically improve the\ncryptographic power of the scheme, varying Ron's mysterious constants according\nto the key for example. This requires testeing etc so I prefer to leave it to\nthe experts.\n\n\nI do very strongly advise that people consider implementing this scheme and\nconsider the withdrawal of the BASIC scheme. Although backawrds compatibility\nis a consideration it is one which in this case I would prefer to lose at the\nearliest opportunity. We are aware that digest authentication makes a number of\ncompromises, we consider however that they are the appropriate compromises to\nmake.\n\nI suggest that further development of the digest scheme consider adding a\nkeyed digest algorithm tag. It would be sensible to retain MD5 as the password\ndigest function however to maintain database compatibility.\n\nAuthorization: Digest\n           username=\"<username>\",             -- required\n           realm=\"<realm>\",                   -- required\n           nonce=\"<nonce>\",                   -- required\n           uri=\"<requested-uri>\",             -- required\n           response=\"<digest>\",               -- required\n           message=\"<message-digest>\",        -- OPTIONAL\n           opaque=\"<opaque>\"                  -- required if provided by server\n\n   kdalgid=\"<algid>\"-- OPTIONAL, default v1 scheme\n\n-- \nPhillip M. Hallam-Baker            Not speaking for anoyone else\nhallam@w3.org http://www.w3.org/hypertext/WWW/People/hallam.html\nInformation Superhighway -----> Hi-ho! Yow! I'm surfing Arpanet!\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "someone has brought to my attention that there is a debate on a\npacket encoding for content. Here are my comments:-\n\n1) Boundary delimiters (like mime)\nThese are absolutely, fundamentaly and irrevokably unacceptable. They\nrequire the originator to be precient is choice of boundary marker,\nthey require the recipient to scan for them. \n\nAd-hoc arguments resorting to MD5 and probablistic considerations\ncarry no weight with me. In the first place there is a very strong\nprobability that a persistent TCP/IP stream will be self referential,\nthat is make a response based upon the low level encoding of the\nstream. Consider two people in a chat conversation debugging HTTP\nconnections, At some point somone will incorporate the message\nboundary into a message and the system will come to a grinding halt.\nIn the second place there is the processing overhead of scanning a\nlive MPEG feed for a boundary delimeter.\n\nThis is simply not the right thing.\n\nThe only argument for the boundary strings I have heard is to simplify\nMAIL gateways. I don't accept the argument that one compromises a\nsystem for a minor simplification of backwards compatibility. If a\ngateway likes boundary delimeters it can easily convert. The email\nsystem is horribly compromised already by the numerous concessions it\nmakes to broken MTAs. One more should not hurt. Do not ask the clean\nprotocol to start making the same concessions by proxy.\n\n\n2) On binary vs human readable.\n\nI prefer that a protocol be faithful unto itself. Either be human\nreadable or let us lose with ASN.1 BER or something like it. There is\nno point in half measures one way or the other. If you must consider\nperformance at this level use base 16. Base 10 is probably as\nefficient since there is not the upper/lowercase ambiguity.\n\nIf you don't think this can be done well in an ascii based scheme\nplease make that statement. We can then decide how to produce a\ngeneric scheme for employing binary encodings as MIME/RFC822 achieves\nfor ascii, human readable encodings.\n\nPlease do not attempt to create a hybrid that is neither one thing nor\nthe other. If you are going to have an ascii based scheme there will\nbe an overhead. I don't think its a very large one but it will exist.\nThere are a number of binarry encodings with worse overhead, ASN.1 DER\nfor example.\n\n3) Encryption\n\nPlease remeber that people will want to encode streams using block\nciphers. This?requires provision for specifying the number of padding\noctets. Note that PEM style padding cannot be used since is creates a\nsecurity problem when used for repeated numbers of small packets. It\nis essential that this padding be random bytes.\n\n4) On a fixed upper limit on the packet size.\n\nThis is not an acceptable proposal. Whatever size you chose I can\nprovide examples where there is a very good need for a bigger size.\nSince history is littered with standards whose creators did not\nanticipate future needs I don't think we should make an arbitrary\nrestriction which has no rational justification. I have just helped\norder a pair of machines with 512Mb of RAM, I have used file systems\nwith a capacity of 6 Tb. I have built systems with a bandwidth of\n6Tb/sec. 32 bits is simply not enough for these purposes. Experience\ndemonstrates that to build a system for the future we must at least be\nable to satisfy the most exotic needs of the day.\n\nSelf describing binary length encodings are simple enough. ASN.1 has\none, one of the sensible suggestions in the original design that\nsurvived the committee cycle?\n\nASN.1 has two legth encodings, short and long. The first octet defines\nthe length of the length. If bit 7 is clear the length is encoded in a\nsingle octet a bits 0-6 of the first octet. Otherwise bits 0-6 give\nthe number of additional octets the length.\n\nA perhaps cleaner way to do it combines the recycling unused bits in\nthe lead octet idea with the fact that most lengths will be most\nconveniently expressed \nusing 1, 2, 4 or 8 bytes. The decision table is as follows:\n\nIF\n    Bit 7 clear,\nlength is 1 octet, bits 0-6 give value\n    Bit 6 clear,\nlength is 2 octets, bits 0-5 give msb of value, octet 2 gives LSB\n    Bit 5 clear,\nlength is 4 octets, bits 0-4 give msb of value, octets 2-4 gives LSB\n    Bit 4 clear,\nlength is 8 octets, bits 0-3 give msb of value, octets 2-9 gives LSB\n    TRUE\nthese are reserved for future expansion.\n\n\nOf these points the last is the most critical. If the first is not\naccepted it will not be a significant disaster because I don't think\nthe protocol would be used. If a scheme with a fixed length gains\npopularity it will be very hard to remedy it later on. Thus I would\nlike names of any proponents of a fixed length scheme with their\nrationale. I promise faithfully to retain said list and produce it in\na suitable forum (eg IETF conference dinner) for an \"I told you so\"\nspeach.\n\n-- \nPhillip M. Hallam-Baker            Not speaking for anoyone else\nhallam@w3.org http://www.w3.org/hypertext/WWW/People/hallam.html\nInformation Superhighway -----> Hi-ho! Yow! I'm surfing Arpanet!\n\n\n\n"
        },
        {
            "subject": "(no subject",
            "content": ">>>the next packet-size as part of the prior packet read.\n>>\n>>        That's an optimization that works on some systems;\n>>not on others.   Keep the protocol free from platform specifics.\n\n>Folks, I am getting a little tired of this line of response\n>without any thought behind it.  There is absolutely nothing\n>platform-specific about preferring 1 read over two reads.\n\nThe original poster was right. There is no fundamental reason \nwhy system calls should be considerably more expensive than\nproceedure calls. This is usually true in UNIX because the \nkernels are often baddly designed and cause substantial locking\noverheads.\n\n\nIn any case the standard principle when reads are expensive is \nthat the application program should accept as much as is avaliable \non each read. Thus there is no need to specify the number of bytes \npast the previous packet. either the data is there in which case the \nread returns all relevant bytes (and more) or the data has not yet been\nsent in which case one wants to return with a partially filled buffer \nin any case. \n\nI fail to see any efficiency consideration in connection with the \nfixed lookahead limit. In addition this is an optimisation, hence a \nheuristic approach (eg a 15 byte lookahead is usually sufficient) is\nthe most appropriate solution.\n\n\nThere have been some comments made about UNIX's somewhat exccentric\nbehaviour concerning sockets. It is pretty well unique in allowing \nmultiple processes to share a socket to a third party without having\na common communication path. This seems to be the source of the \nrequirement that processes avoid reading past the end of the packet. \nThis is a very system dependent constraint and since it is a pretty\nmarginal optimisation at best I suggest it be ignored. \n\nNote that most programs which exploit the socket sharing ability have\nto also support a general inter-process message passing scheme. \n\nIn return for this questionable optimisation we are asked to accept \na fixed upper bound on the packet size. This is simply not acceptable.\nIt optimises the wrong condition.\n\n-- \nPhillip M. Hallam-Baker            Not speaking for anoyone else\nhallam@w3.org http://www.w3.org/hypertext/WWW/People/hallam.html\nInformation Superhighway -----> Hi-ho! Yow! I'm surfing Arpanet!\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "\"Phillip M. Hallam-Baker\" <hallam@w3.org> wrote:\n\n  > Concerning the tying of the username to the realm, this was a deliberate =\n  > choice\n  > on my part. If a user has the same username/password on multiple machines=\n  >  a\n  > system manager at one site could obtain access to the other if there was\n  > nothing to differentiate them. Its a realm name and not the server name t=\n  > o\n  > permit multiple servers to share the same authentication data. What is mi=\n  > ssing\n  > is the requirement that the realm name should be an INTERNIC reserved one=\n  > , eg\n  > we could use w3.org or blink.w3.org. I think this prevents collisions in =\n  > the\n  > desired manner.\n  [...]\n\nI'm obviously missing something here concerning realms.  I've seen realms\nused:\n1) As a name in Web servers, to distinguish separately protected domains\nof information.\n2) As a component of the WWW-Authenticate response header.\n3) As the prompt from a Web broswer for a user to authenticate him/herself.\n\nI've argued (unsuccessfully, so far) that (1) and (3) should be separated.\n\nNow Phillip seems to suggest that the realm should be something the\nINTERNIC registers.  I don't understand why.  The client knows the\nserver it connected to, so presumably it can distinguish realm R on\nserver S1 from realm R on server S2.  If he's arguing that the server\nname should be incorporated in the Digest method hash, in addition to\nthe realm, that's reasonable, provided both client and server agree on\nwhat that name is.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": ">Now Phillip seems to suggest that the realm should be something the\n>INTERNIC registers.  I don't understand why.  \n\nIf people want realms to be unique then it would be a good thing to\nhave some mechanism for ensuring that they are. INTERNIC provides a\nmechanism for giving people a unique name. I'm not implying that the\nrealm should be internic addresssable, merely that people should only use\nnames that they \"own\".\n\nOr perhaps what I'm really arguing for is that people should only use internic\nstyle names if they own them. I should not set up a realm att.com for example.\nbut could set up \"Edible\" as a realm but with a danger of collision.\n\n\nPhill\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "> I prefer that a protocol be faithful unto itself. Either be human\n> readable or let us lose with ASN.1 BER or something like it. There is\n> no point in half measures one way or the other.\n\nYes, this makes sense.  Inventing YApacketization scheme seems silly unless\nwe really feel all existing ones are clearly unsuitable.\n\n> There are a number of binarry encodings with worse overhead, ASN.1 DER\n> for example.\n\n?  Not sure what you're hinting at here...\n\n> 3) Encryption\n> \n> Please remeber that people will want to encode streams using block\n> ciphers. This?requires provision for specifying the number of padding\n> octets. Note that PEM style padding cannot be used since is creates a\n> security problem when used for repeated numbers of small packets. It\n> is essential that this padding be random bytes.\n\nI see this as a problem unrelated to packetizing data so you can tell where\nthe end is.\n\n> 4) On a fixed upper limit on the packet size.\n> \n> This is not an acceptable proposal.\n\nI'd like to see arbitrary limits removed to the extent that it's possible;\nthat's one of the things I dislike most about SSL's record layer\n(particularly since SSL adds so much per-record overhead) and I'd like to\nsee that mistake not get repeated.\n\n- Marc\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "The content-MD5 document, RFC 1544, describes exactly such a header.\nIs this what you are looking for?\n\n        Harald T. Alvestrand\n\n\n\n"
        },
        {
            "subject": "Re: 406 None Acceptabl",
            "content": "Thanks for the clarification - I'm looking forward to the day when I can\nexpect content negotiation to work properly, and I agree that 406 was\nan obstacle to this!\n\n       Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "On Wed, 26 Jul 1995 Harald.T.Alvestrand@uninett.no wrote:\n\n> The content-MD5 document, RFC 1544, describes exactly such a header.\n> Is this what you are looking for?\n\nYes.  However, it's optional, and in fact, I've never seen it included\nwith any HTTP-generated header.  It would be nice if such a mechanism were\nformally included as part of the reply to a HEAD request, not for purposes\nof error-chjecking, but merely to know if the document has changed. \n\n     -Mordechai T. Abzug\nHomepage:  http://www.gl.umbc.edu/~mabzug1   send email to:  mabzug1@umbc.edu\nIt's hard to RTFM when you can't find the FM..\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "According to Mordechai T. Abzug:\n> On Wed, 26 Jul 1995 Harald.T.Alvestrand@uninett.no wrote:\n> \n> > The content-MD5 document, RFC 1544, describes exactly such a header.\n> > Is this what you are looking for?\n> \n> Yes.  However, it's optional, and in fact, I've never seen it included\n> with any HTTP-generated header.  It would be nice if such a mechanism were\n> formally included as part of the reply to a HEAD request, not for purposes\n> of error-chjecking, but merely to know if the document has changed. \n> \n\nAre there servers that produce an incorrect last modified date for\nfiles?  I don't know of any, but if they exist then they are broken\nand the proper solution is to have them fixed.  Of course the last\nmodified date for a CGI script may be problematical, but I don't see\nhow checksums could help this.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "On Thu, 27 Jul 1995, John Franks wrote:\n\n> Are there servers that produce an incorrect last modified date for\n> files?  I don't know of any, but if they exist then they are broken\n> and the proper solution is to have them fixed.  Of course the last\n> modified date for a CGI script may be problematical, but I don't see\n> how checksums could help this.\n\nBoth for:  (1) automaticaly generated pages, which have new modification \ndates even when nothing new is added; and (2) CGI script outputs that \nrepresent a search.\n\n     -Mordechai T. Abzug\nHomepage:  http://www.gl.umbc.edu/~mabzug1   send email to:  mabzug1@umbc.edu\nSpell chequers dew knot work write.\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Checksum header or metho",
            "content": "On Wed, 26 Jul 1995 Harald.T.Alvestrand@uninett.no wrote:\n\n> The content-MD5 document, RFC 1544, describes exactly such a header.\n> Is this what you are looking for?\n> \n>         Harald T. Alvestrand\n> \n\n\nI am new to this group, and need just a clarification... What is \ncontent-MD5 document? And where can I find more info on this?\n\nThank you for any responses!!\nVasu.\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "Phillip M. Hallam-Baker writes in <9507261823.AA18496@www18.w3.org>:\n>1) Boundary delimiters (like mime)\n>        These are absolutely, fundamentaly and irrevokably unacceptable. \nThey\n>require the originator to be precient is choice of boundary marker,\n>they require the recipient to scan for them.\nAlthough I don't agree, as\n>Ad-hoc arguments resorting to MD5 and probablistic considerations\ndo carry weight with me, there are times when it is preferable to not have \nto scan for a boundary marker.\n\nIn this spirit, I propose a simple (and perhaps simple-minded :<) method for \na self-describing binary length encoding:\n1. The initial octet shall be in the Base64 alphabet (RFC1341).  It \ndescribes the length of the length string; and\n2. The following octets of the length string shall be in the Base64 \nalphabet, such that \"B\"(64) = 1(10), \"BA\"(64) = 64(10), \"BAAA\"(64) = \n262144(10), and so on.  These examples would then be:\n     octet# on wire octet value\n     -------------- -----------\n     0         B\n     1         B\nfor \"B\"(64) = 1(10),\n     octet# on wire octet value\n     -------------- -----------\n     0         C\n     1         B\n     2         A\nfor \"BA\"(64) = 64(10), and\n     octet# on wire octet value\n     -------------- -----------\n     0         E\n     1         B\n     2         A\n     3         A\n     4         A\nfor \"BAAA\"(64) = 262144(10).\n\nThis complies with the spirit of HTTP, as the length encoding will be in \nASCII as Phill noted:\n>I prefer that a protocol be faithful unto itself.\n\nA self-describing binary length encoding string of length 64 (1 octet for \nthe length of the length + 63 length octets) represented in the Base64 \nalphabet can encode a transmission length of up to 64^63 - 1 octets, or \n61565634681866373769186000156474396570437092610102260418669208444133\\\n9402679643915803347910232576806887603562348543 octets if you prefer.  This \nis around 6.16x10^112, whereas there are only around 10^80 particles in the \nknown universe.  By the time we run into this limit, it is likely we will \nwant to switch away from HTTP :)...\n\nThe Base64 alphabet was chosen because it is a compact printable \nrepresentation of base 64 numbers that is portable between ASCII, EBCDIC, \nISO 646, and ISO 10646 that should have multiple correct alphabet \ntranslation tables already available.\n======================================================================\nMark Fisher                            Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "Content-transfer-encodings generally stay with the message in store,\nand are not used to find the end of a message, but to protect the content.\n\nThe \"packet\" encoding's primary purpose is to locate the end of the\nmessage without requiring scanning for special strings or knowing the\ncomplete length before starting transmission.\n\nNote that an almost identical mechanism was proposed for SMTP - I think\nit is still one of the \"draft-ietf-mailext\" drafts.\n\n                 Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">Content-transfer-encodings generally stay with the message in store,\n>and are not used to find the end of a message, but to protect the content.\n>\n>The \"packet\" encoding's primary purpose is to locate the end of the\n>message without requiring scanning for special strings or knowing the\n>complete length before starting transmission.\n\nYep, and the above two paragraphs are equivalent under HTTP.  The\npacketized CTE would stay with the message (until it was removed),\nand its purpose is to protect the content from a premature closure\nof the connection (i.e., it allows the recipient to know if it got\nthe whole thing, and it allows the sender to apply a signature to\nthe whole thing).\n\n>Note that an almost identical mechanism was proposed for SMTP - I think\n>it is still one of the \"draft-ietf-mailext\" drafts.\n\nYes, I am aware of <draft-ietf-mailext-smtp-binary-07.txt>.  That mechanism\nuses a series of separate messages (each message given a 1*DIGIT length)\nto send the binary data.  In other words, it is a stateful message sequence,\nwhich is something HTTP doesn't do.  It does demonstrate that both protocols\nneed such a thing, and conversion between the two methods would be trivial.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": ">The Base64 alphabet was chosen because it is a compact printable \n>representation of base 64 numbers that is portable between ASCII, EBCDIC, \n>ISO 646, and ISO 10646 that should have multiple correct alphabet \n>translation tables already available.\n\nNo, if we are going to go with something simple(-minded), then a\nstraight CRLF delimited number is more appropriate -- dicking with\nbits and base64 is a waste of time if all you'd every save is\none or two bytes.\n\nRight-o then, here's where the perceived consensus lands us.\n\nContent-Transfer-Encoding: chunked\n\nBNF:\n\n   Entity-Body = *( chunk ) \n                 \"0\" CRLF\n                 footer\n                 CRLF\n \n   chunk       = chunk-size CRLF chunk-data CRLF\n \n   chunk-size  = hex-no-zero *hex\n \n   chunk-data  = chunk-size(OCTET)\n \n   footer      = *( Entity-Header )\n\n   hex         = \"0\" | hex-no-zero\n   hex-no-zero = \"1\"|\"2\"|\"3\"|\"4\"|\"5\"|\"6\"|\"7\"|\"8\"|\"9\"\n               | \"A\"|\"B\"|\"C\"|\"D\"|\"E\"|\"F\"\n               | \"a\"|\"b\"|\"c\"|\"d\"|\"e\"|\"f\"\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "Roy's perceived consensus C-T-E proposal looks okay to me.  And he's\nstated that anything purporting to handle HTTP/1.1 must handle\n\"chunked\".\n\nNext questions:\n1)  Should the specification state when \"chunked\" must and must not be\nused?  Although there's no consensus session/keepalive proposal,\nobviously any content sent from the server to the client for a\nheld-open connection must either have a Content-Length or be chunked.\n\n2) Can a client send chunked content in a POST in lieu of a\nContent-Length, or even with a C-L?  (And what does it mean to have\nboth, especially if they disagree?)\n\n3) If (2) is true, does the CGI interface change to require a CGI script\nto interpret C-T-E, or does the interface stay the same, and the server\nprocesses the chunked content and passes the concatenated chunks to the\nCGI?  If the latter, is it valid for the server to forge a Content-Length\nheader for the CGI to use to read the concatenated content where none\nexisted previously?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "The Case for PersistentConnection HTTP, by Jeff Mogul",
            "content": "At the Stockholm IETF, I promised to provide the address for\nJeff's paper.\n\nYou can find it at:\nhttp://www.research.digital.com/wrl/publications/abstracts/95.4.html\n\nThis will also appear in the minutes of the WG meeting.\n- Jim Gettys\n\n\n\n"
        },
        {
            "subject": "Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Please send me any additions/amendments, before I send these\nin tomorrow afternoon.  Henrik is on vacation, and the minutes are due \nAugust 4, so I wrote up Henrik's notes.  At least we can get the minutes\nin early...\n- Jim Gettys\n\n---------------------------------10768836720160\nContent-Transfer-Encoding: 7bit\nContent-Type: text/plain; charset=iso-8859-1\n\nDraft Minutes of HTTP Working Group, 33rd IETF Meeting, in Stockholm\n\nReported by Jim Gettys from notes taken by Henrik Nielsen.\n\nHTTP 1.0\n\nWe need to come up with a final draft before we can finish all discussion.\nDraft will be available August 1, for anticipated last call later in the\nmonth.\n\nAccess Authentication - MD5 digest\n\nThere is real no objections to the current state of this proposal. HTTP does\nnot provide possibility for having MIME headers after the HTTP object.\n\nThere are multiple implementations:\n\n   *  NCSA server and client\n   *  Spyglass\n   *  Dave Kristol's server\n\nHTTP Session Extension\n\nTed Hardie, NASA, led this discussion.\n\n   *  This proposal would avoid TCP latency, overhead, and slow start\n     performance problems.\n   *  Ted described a proposal from Alex Hopmann, who was not present\n   *  Henrik noted that Request-ID header makes the proposal more flexible\n     as the server can send them back out of order\n   *  There was general talk about sessions with a server.\n   *  Jeff Mogul of DECWRL has made an extensive study and simulation of\n     persistent connection HTTP. The results of this work can be found at\n     http://www.research.digital.com/wrl/publications/abstracts/95.4.html\n   *  Is it a good idea to save headers while a connection is kept alive?\n        o  Eric Sink: No big advantage - 10% (from implementation)\n        o  Masinter: Too early to say\n        o  authentication may be the biggest performance win.\n   *  A number of implementations were mentioned; performance is unclear,\n     and most likely to be seen over long haul and dial up lines, rather\n     than in a local network, where most naive tests are performed.\n   *  The general consensus is that persistent connections are a good idea.\n     There are concerns about upward compatibility and interoperability with\n     1.0; this may or may not require 1.1; it was suggested that operation\n     under 1.0 might be written up as an experimental protocol.\n   *  An open question is the timeouts for the tcp connection; there is some\n     data from Jeff Mogul's simulation.\n\nMIME multi-part\n\nMIME multi-part was not discussed.\n\nSession-ID, Request-ID, cookies\n\nNo one wanted to talk about it at this meeting.\n\nProblem with HTTP PUT and POST\n\nHenrik Nielsen described a problem with HTTP PUT and POST that has recently\nbeen uncovered, and solicited feedback.\n\nHTTP/1.1\n\nA HTTP/1.1 draft will be available in mid-August.\n\nHTTP/NG\n\nHTTP/NG: Andy Norman, Ange@hplb.hpl.hp.com. Stefek Zaba has an experimental\nimplementation of what they call HTTP/NG, and has been taking to Simon\nSpero. Simon was not at the meeting, so there was little discussion of NG.\n\nLarry Masinter asserted that HTTP must support transactions. Larry also\ncommented that RPC keeps the connection open; but we may be reinventing the\nwheel, and would rather see a general HTTP maybe using RPC. This begs the\nquestion: Who has implemented a non-blocking (streaming) RPC system that can\nbe used if we are to avoid rolling our own?\n\nFeedback from John Klensin, and Harald Alvestrand, Area Directors for\nApplications\n\nJohn Klensin expressed great displeasure with the current state of the\nworking group. Some issues he raised, but not necessarily an exhaustive list\ninclude:\n\n   * Working group chairs that do not warn the area director before an IETF\n     meeting that they cannot attend are asking to be shot. John promised to\n     convey his displeasure directly to the chairs.\n   * How will we make progress?\n   * We have a collective problem in the working group. We should stick to\n     the milestones.\n   * Without NG as a milestone for this group, 1.1 will likely end up out of\n     control. Without Simon Spero present, and with his RSI problems, John\n     is very concerned about NG. Jim Gettys volunteered to edit HTTP/NG, if\n     Simon is unable to deal with it due to his problems. When will it\n     become a proposed standard?\n\nHarald Alvestrand noted that there is no reason to wait for an IETF meeting\nin to send a document to the IESG for standardization.\n\nProposed New Milestones\n\nAUG 95 send HTTP/1.0 of to IESG as proposed standard.\nOCT 95 Session as experimental extension.\nAPR 96 HTTP/1.1 as proposed standard\nDEC 96 HTTP/NG as proposed standard. Jim Gettys volunteered to help Simon\nwith writing.\n\n---------------------------------10768836720160--\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": ">>>In other words, the result looks like a normal message, with\n>>>the Content-Length computed by the unpacketizer.  In actuality,\n>>>it is often faster to read packet-size + 1 bytes, slurping in\n>>>the next packet-size as part of the prior packet read.\n>>\n>>        That's an optimization that works on some systems;\n>>not on others.   Keep the protocol free from platform specifics.\n>\n>Folks, I am getting a little tired of this line of response\n>without any thought behind it.\n\n        4 things:  1)  I do try to put some thought behind a response\nbefore pressing the SEND key.   It is unfortunate that I *consistently*\nremember/realize/recognize more pieces *after* pressing SEND for which\nI find I have to follow-up.   Thus,  I remembered my real objection\nafter pressing SEND,  *and* realized that I really LIKE your proposal.\n\n        2)  it is immensely convenient that MIME is \"plain text\".\nIt is unfortunate (but perhaps correctable) that this characteristic\nis not documented, supported, and exploited.   CTE packet (as you've\nproposed) would encourage yet more MIME streams that are valid MIME\nand yet NOT \"plain text\".   Although I like your proposed mechanism\nfor CTE packet,  I do NOT like the idea of more non-plaintext\ncluttering up MIME.   Would that the trend would reverse itself.\n\n        If someone could propose a \"packet\" content transfer encoding\nthat fit well with a \"MIME should be plain text\" objective,  I'd very\nmuch like to see it.   Since I cannot think of one  (Dan's was good),\nI'll accept yours if the rest of the world does,  but I will ask for\nsome additional wording.   (see below)\n\n>                                There is absolutely nothing\n>platform-specific about preferring 1 read over two reads.\n>If you have a technical disagreement, fine, but the next person\n>who cries \"platform specific\" better damn well include at least\n>one concrete example wherein the protocol prefers one platform over\n>another, and not just because of differring TCP implementation bugs.\n\n        3)  There's nothing broken in the TCP stack I'm using.\nBut I'm not reading directly from TCP.   The TCP stream is being\nfed to another stage,  another process,  in the pipeline of my HTTPD.\nIn this context,  I simply don't have a read(,,size) construct available.\nTo that extent,  \"reading one byte beyond\"  is a platform-specific\noptimization.   THIS DOES NOT MAKE YOUR PROPOSAL A SHOW STOPPER,\nit just means that I won't be able to do it as efficiently as others.\nThe *optimization* you recommend is what's platform specific,\nnot the CTE itself.\n\n        Really,  this has nothing directly to do with the platform (VM)\nbut with the VHLL (REXX) and its supporting tools (CMS Pipelines).\nI'm getting a lot of efficiency where I can hand-off the byte counting\nto system level routines.   If the whole thing is plain text,  that\nmakes my work a complete cake walk.   Where things are binary,\nI can still get it done,  it's just not as elegant.\n\n        4)  I suggest (for the sake of argument) that we recommend\nCTE packet explicitly for on-the-wire transfer and  INCLUDE WORDING\nthat as it is taken off-the-wire the CTE packet should be immediately\nprocessed and removed.   Other than that,  this is a good idea.\nI regret that I didn't recognize my pleasure in it at first,  but I\nthink I've outlined the reasons why it struck me wrong initially.\n\n        Using just one byte clearly avoids the network byte order\nproblem.   Using a  \"zero means end\"  signal works just fine.\nExcellent proposal,  Roy.\n\n        So in summary:  I'd like to keep off-the-wire MIME in \"plain\ntext\" (ie: with local canonicalizations and all printable characters)\nform whenever possible.   I'm willing to accept on-the-wire MIME in\n\"binary\" forms if we can be clear about the difference,  can explain\nthe canonicalization issue,  and can encourage the use of the plain text\nforms where feasible.\n\n> ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n>                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n>                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n--\nRick Troth <troth@ua1vm.ua.edu>, Houston, Texas, USA\nhttp://ua1vm.ua.edu/~troth/\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "a) I think we should pay attention to Harald's request that we not\ncall it a C-T-E. Even though we can justify that usage ourselves,\nthere's no harm in using a different header in HTTP, is there?\n\nb) I'll give up calling for a string-terminated boundary marker if no\none else thinks it is worthwhile.\n\nc) So far the HTTP WG has avoided specifying what happens with CGI --\nit's out of scope of the WG, platform dependent, etc. However, changes\nto HTTP do affect CGI, so should be considered.\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "On Thu, 27 Jul 1995, Roy Fielding wrote:\n\n> \n> >Note that an almost identical mechanism was proposed for SMTP - I think\n> >it is still one of the \"draft-ietf-mailext\" drafts.\n> \n> Yes, I am aware of <draft-ietf-mailext-smtp-binary-07.txt>.  That mechanism\n> uses a series of separate messages (each message given a 1*DIGIT length)\n> to send the binary data.  In other words, it is a stateful message sequence,\n> which is something HTTP doesn't do.  \n\nCould please clarify what you mean by 'series of separate messages' since \nthis terminology doesn't fit my understanding of SMTP chunking. The BDAT \ncommand doesn't in any way change data content in the pipe. It's also \nstreamable in the sense that you can have more than on outstanding BDAT \nchunk pending on the connection.\n\n... ian\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncodin",
            "content": "On Tue, 1 Aug 1995, Larry Masinter wrote:\n\n> a) I think we should pay attention to Harald's request that we not\n> call it a C-T-E. Even though we can justify that usage ourselves,\n> there's no harm in using a different header in HTTP, is there?\n\nC-T-E is a really gross but essential bit of cruft. Creating something \nsimilar but with elegant purpose should imply a new name. The MIME \nspec. expressly closes the category off for extension anyhow and it's a \ngesture that should be respected.\n\n> b) I'll give up calling for a string-terminated boundary marker if no\n> one else thinks it is worthwhile.\n\nI think it's essential not just worthwhile, but I suppose it can wait 'til \nHTTP-NG if necessary. Being able to start sending a stream before having \nthe full wad in hand has too many applications to ignore support for it, \nparticularly since we've got a functioning model, that's proven to work \nwell, available to clone.\n\n ...   ian\n\n\n\n"
        },
        {
            "subject": "WWW Servers Comparison Chart, version 2.",
            "content": "Greetings. Version 2.4 of the WWW Servers Comparison Chart is now available from\n<http://www.proper.com/www/servers-chart.html>.\n\nNew in version 2.4:\n- Added CL-HTTP (listed as CL), a Web server that runs under CommonLISP.\n- Changed the name and company of the \"NM\" entry from \"NetWalla Web Server\nfrom NetMagic\" to \"Communications Builder from The Internet Factory\".\n- Removed the list of servers for which I have received no information.\n- Added phttpd, a minimal HTTP server written in Perl, to the tools section.\n- Duplicated the headers within the table to make it easier to follow,\nbased on a suggestion from a reader.\n- Fixed a few more typos contributed by readers. (It is worth noting here\nthat Netscape is much more forgiving of typos than other Web clients.)\n\n--Paul Hoffman, President\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "(Secure) HTTP library API ",
            "content": "I would like to ask whether there are already any efforts to define a\nAPI for a HTTP retrieval engine? (I apologize if this was already\ndiscussed earlier, I have just subscribed to these mailing lists.)\n\nThis would allow to separate the development of WWW Browsers and HTML\ninterpreters from the development of secure protocols. Browsers may be\ncreated (and sold) with a standard library without security. They\ncould be linked together (perhaps at run time by using dynamic\nlinking) with libraries implementing secure retrieval methods as a\n\"black box\".\n\nAs a first idea such a library could use the method given in a url to\ndo a lookup in a system table to find an engine implementing this\nprotocol, e.g. for http, ftp, shttp etc.\n\nAny opinions?\n\n\nHadmut\n\n\n\n"
        },
        {
            "subject": "Globalizing URI",
            "content": "It is my current understanding that arbitrary bytes can be encoded in URLs.\nThis provide a means for an HTML UA to formulate a response to a form\nsubmission for arbitrary character encodings. However, I do have one important\nquestion:  how does an HTTP server identify the encoding of such bytes (i.e.,\nthe CHARSET) and communicate that encoding to the consumer of this data (e.g.,\na CGI script)?\n\nRegards,\nGlenn Adams\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "In message <9508021956.AA23126@trubetzkoy.stonehand.com>, Glenn Adams writes:\n>\n>It is my current understanding that arbitrary bytes can be encoded in URLs.\n\nWell... that's stretching it. Arbibrary bytes can be encoded in\nmorse code too. A URL is a sequence of US-ASCII characters. Check\nRFC1738:\n\n2.2. URL Character Encoding Issues\n\n   URLs are sequences of characters, i.e., letters, digits, and special\n   characters. A URLs may be represented in a variety of ways: e.g., ink\n   on paper, or a sequence of octets in a coded character set. The\n   interpretation of a URL depends only on the identity of the\n   characters used.\n\n   In most URL schemes, the sequences of characters in different parts\n   of a URL are used to represent sequences of octets used in Internet\n   protocols. For example, in the ftp scheme, the host name, directory\n   name and file names are such sequences of octets, represented by\n   parts of the URL.  Within those parts, an octet may be represented by\n   the chararacter which has that octet as its code within the US-ASCII\n   [20] coded character set.\n\n>This provide a means for an HTML UA to formulate a response to a form\n>submission for arbitrary character encodings.\n\nEr... well.. I suppose so. But that seems like a pretty roundabout\nway to go about it.\n\nI'd much prefer to see a general purpose replacement for\nthe application/x-www-form-urlencoded media type.\n\nSomethink like text/tab-separated-values might work nicely.\nOr something SQLish, or lispish, or Tcl-ish. text/tab-separated-values\nwould be nice because you could use other charset= values for\nother encodings. Of course you'd have the same nasty interactions\nbetween octet 7 for the TAB character as octet 10 and 13 for CR/LF.\n\n> However, I do have one important\n>question:  how does an HTTP server identify the encoding of such bytes (i.e.,\n>the CHARSET) and communicate that encoding to the consumer of this data (e.g.,\n>a CGI script)?\n\nWell... I gather you're still talking about the\napplication/x-www-url-encoded media type. The only \"specification\" for\nthat is in the HTML spec.  (hang on... I'd better check the CGI\nspec... nope. It just says stuff like \"Examples of the command line\nusage are much better demonstrated than explained.\")\n\nI think the character encoding scheme is US-ASCII, or perhaps\nISO-Latin-1, by convention.\n\nLike I said... I'd much prefer to see x-www-form-urlencoded replaced\nthan having other character sets shoehorned into that hack.\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "> I'd much prefer to see a general purpose replacement for\n> the application/x-www-form-urlencoded media type.\n\n> Somethink like text/tab-separated-values might work nicely.\n> Or something SQLish, or lispish, or Tcl-ish. text/tab-separated-values\n> would be nice because you could use other charset= values for\n> other encodings. Of course you'd have the same nasty interactions\n> between octet 7 for the TAB character as octet 10 and 13 for CR/LF.\n\nDan, the file upload proposal includes a complete specification for\n'multipart/form-data' which is intended as a general purpose\nreplacement for application/x-www-form-urlencoded. We went through\nmultiple iterations both in the HTML working group and also as a\nresult of the review given by the folks reviewing new media type\nregistration. multipart/form-data is completely capable of dealing\nwith multiple character sets, unicode, or whatever coming back as the\nresult of filling out a form.\n\nCheck it out:\n\nftp://ds.internic.net/internet-drafts/draft-ietf-html-fileupload-02.txt\n\n\n\n"
        },
        {
            "subject": "Eventdriven document expiratio",
            "content": "Yet Another Thing The Expires Header Is Not Appropriate For\n\nEven if the Expires header were honored by all browsers (it isn't),\nand even if the history functions of all browsers ignored the Expires\nheader (they don't, but they should), there would still be some types\nof resources that could not be properly cached given the existing\nprotocol.\n\nThese resources are dynamic by nature, but do not have a fixed\nexpiration date.  Instead, they only change in response to specific\nevents, typically originating with the user of the \"user agent\".  They\n*should be* cached if possible until the occurrence of some event\nthat would change them.\n\nLet me give an example.  Suppose you wish to create an interactive\nstore that has a page indicating current user selections.  For various\nreasons, it is appropriate to implement this using server-side\npersistent state, and to allow the client to request the current\nversion of this \"current selections\" resource at any time.\n\nIt may be desirable to display this current selections page in response to a\nnumber of different events, for example:\n\n- user selects a new item\n\n- user deletes an item\n\n- user requests to see current selections\n\n- user requests another action that cannot be performed because \n  there are no items currently selected -- for instance, show current charges.\n  The desired behavior may be to display the current selections page, showing\n  that it is empty.\n\nSuppose that each different action is requested using a different URI.\nWhat will happen?  Ignoring possible interactions with Expires, we can\nsee that using a different URI in each case will cause a different\ntemporal version of the selections page to be cached using each\ndifferent URI as a key.  What is wrong with this?\n\nThere is a serious problem with the above scenario.  If the user uses\none URI (a) which ends up returning the selections page, then uses\nanother URI (b) which also returns this page, and then goes back to\nURI (a) again, the user may end up viewing a stale copy of the document.\nThis is especially true if URI (a) is implemented by a GET method, which\nwill typically be subject to caching.\n\nThere are mechanisms in HTTP and workaround techniques that allow such\ndocuments to be handled correctly, but it is my feeling that these\ntechniques and mechanisms are too awkward for this purpose.\n\nThe HTTP mechanism is redirection using the \"302 Moved Temporarily\"\nheader.  It usually works, but is somewhat inefficient since it causes\nthe client to make two requests.\n\nThe other approach is the workaround technique of overloading the use\nof a single URI with different functions, possibly by using fields in\nforms to specify what is to happen.\n\nRedirection using 302 usually works because when clients re-request a\ndocument after being redirected with 302, they usually cache the\ndocument using the *redirected* URI as a key, not the original.  But\nit is possible to imagine clients that don't do their caching this way.\nIn fact, I have seen some that don't.\n\nOverloading a single URI usually works because most clients use the\nURI as a cache key.  If a client happens to also use form field names\nas part of the cache key, then this technique will not work reliably,\nsince the different functions using a common URI for common caching \nmust encode the different function-codes somewhere, and the only other\nconvenient place to put them is in form fields.\n\nSo, both techniques I have mentioned just happen to work most of the\ntime, but are not guaranteed to work.  This is, in my view, an\ninferior situation.\n\nIt should also be mentioned that using the Expires header would force\nthe issue, at least for those browsers that bother to pay attention to it.\nIf the current selections page were set to expire immediately in all\ncases, then all requests that ended up displaying that page would be\nforced to fetch a fresh copy of it.  This is somewhat wasteful, since\nthis page would be cacheable until some state-changing event occurred.\n\n\nWhat is Required?\n-----------------\n\nIt is possible to imagine a number of solutions to this problem.\nThere is one approach that I favor, but even if it were to be adopted\ninto the standard in some form, I fear it will be necessary to stick\nwith the above workaround techniques for a long time since browser\nauthors are not so quick to pick up on these kinds of issues, and\nsince there is a growing population of existing browsers out there.\n\nIn any case, my prefered solution would be to use some response header\nto indicate the URI of the resource being returned.  This URI would\nnot have to be the same as the request-URI.  Its purpose in life would\nbe to identify the returned resource, expressly for the purpose of\ncache control.  The client would be directed to use this response-URI\nas the cache key for the requested document.  The URI in this header\nshould be usable with the GET method to obtain a \"current\" copy of the\nresource returned.\n\nAny opinions on any of this?\n\n\n--Shel Kaphan\n  sjk@amazon.com\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "I have something written up on this that was going into the \nrfc. It is appended.\n\n---\n<H2>Notes on the URL syntax</H2>\n\n<P>The URL specification allows arbitrary 8 bit data to form part of\nthe  <TT>&lt;scheme-specific-part></TT> of a URL but requires that only\noctets which correspond to character codes for printable ASCII be\nused in the URL definition. Octets that fall outside of this set must\nbe encoded using the <EM>URL-encoding</EM> mechanism, which encodes\nthe octet as a '%', followed by 2 hexadecimal digits. The '%'\nsign must also be encoded.</P>\n\n<P>URL's often point to files on a file system, which increasingly,\nmay <EM>not</EM> have a name that uses printable ASCII characters. For\nexample, on a Japanese systems, a file might have the name\n\"insatsu.html\", in which the \"insatsu\" might be represented in\nromanji, katakana, hiragana, or kanji. In such cases, the octets that\nfall outside the range of printable ASCII would be encoded as per the\nspecification, resulting in something looking like the following on\nEUC-based systems:\n<PRE>\n       http://www.jacme.co.jp/%B0%F5%BA%FE.html\n</PRE>\n<P>In general, this does not present a problem, because URL's are\nseldom decoded on machines where the coded character set and encoding\ndiffer from that found within the URL. However, in such cases (for\nexample robots), it may, or may not, be possible for the decoder to\nsense the coded character set and encoding used. Even if the decoder\ndoes correctly guess, it is not guaranteed that they will be able to\nsuccessfully decode the URL, and then process the resulting text.</P> \n    \n<P>To allow the coded character set and encoding to be explicitly\nstated in the URL, the URL syntax should be expanded as follows: </P>\n<PRE>\n       &lt;scheme>:&lt;character-set-data>&lt;scheme-specific-part>\n</PRE>\n<P>where the the BNF definition of <TT>&lt;character-set-data></TT>\nwould be:<P> \n<PRE>\n       character-set-data = \"[\" [ character-set \":\" ] encoding \"]:\"\n       character-set      = name-string\n       encoding           = name-string\n       name-string        = 1*[ alpha | digit | \"-\" | \",\" ]\n</PRE>\n<P>and the <TT>&lt;character-set-data></TT> part of a URL should be\noptional, thereby resolving any backward compatibility\nconcerns. An example of such URL's would be:</P>\n<PRE>\n       http:[EUC]//www.jacme.co.jp/%B0%F5%BA%FE.html\n</PRE>\n<P>It would also be advisable for the HTTP protocol to provide some\nmechanism for indication the coded character set and encoding used\nwith URL's that are parts of a request.. For example, the <TT>PUT</TT>\nmethod syntax could be extended such that the coded character set and\nencoding of the URL be an optional part of the method parameters:</P>\n<PRE>\n       request-line          = method SP request-uri SP optional-charset-data\n                               SP http-version CRLF\n       optional-charset-data = \"[\" [ character-set \":\" ] encoding \"]:\"\n       character-set         = name-string\n       encoding              = name-string\n       name-string           = 1*[ alpha | digit | \"-\" | \",\" ]\n</PRE>\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "I don't like this model, but prefer another one:\n\nLet me explain this via an 'ftp' example.\n\nThe FTP protocol doesn't care what character set your file system\nuses. You open a 8-bit connection and send US-ASCII characters to the\nserver. If you want to retrieve a file, you send 'RETR xxxx' and when\nyou want to store a file, you send 'STOR xxxx', where 'xxxx' are\ncharacters *NOT* in the native character set of the file system, but\nrather in whatever transcription of that character set is made\navailable by the FTP server.\n\nThe *PROTOCOL* doesn't define the mapping between the bytes sent and\nthe the file system. This is completely up to the implementation of\nthe FTP server.\n\nNow, the FTP URL scheme defines yet another mapping. If you happen to\nwant to send 'RETR /?#frob' to your FTP server, you have to actually\nencode the '#' in the FTP URL. Thus, there's another level of encoding\nin URL -> ftp-protocol that sits on top of the encoding chosen by your\nimplementation of FTP-protocol -> file system.\n\nThe same situation holds with the HTTP protocol. Implementors of HTTP\nservers which deal with files on file systems that allow file names to\nbe written in other character sets will have to chose some mapping\nbetween those files and the HTTP protocol. That mapping is *not* part\nof the HTTP protocol, and I don't think it should be.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">Dan, the file upload proposal includes a complete specification for\n>'multipart/form-data' which is intended as a general purpose\n>replacement for application/x-www-form-urlencoded. We went through\n\nYep. In section 4.4 of \"The Multilingual World Wide Web\" I make\nmention of it as well. It is a good solution, and will certainly be in\nthe RFC.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">I don't like this model, but prefer another one:\n> \n>Let me explain this via an 'ftp' example.\n> \n>The FTP protocol doesn't care what character set your file system\n>uses. You open a 8-bit connection and send US-ASCII characters to the\n>server. If you want to retrieve a file, you send 'RETR xxxx' and when\n>you want to store a file, you send 'STOR xxxx', where 'xxxx' are\n>characters *NOT* in the native character set of the file system, but\n>rather in whatever transcription of that character set is made\n>available by the FTP server.\n\nI don't understand how this works, especially the \"some transcription\"\npart. How is the receiving server to know the name to store it under,\nor is \"%B0%F5%BA%FE.html\" translated to insatsu.html for storage\npurposes? \n\nThere are 3 problems I have with this model:\n\n1) It would probably require the use of some kind of database to map\n   the local filename to the HTTP representation, because there are\n   possible transcription collisions, and because HTTP is stateless.\n2) Without some standard mapping it seems somewhat difficult for a\n   browser to decide what to send to the server. Yes, I know\n   people will say that the server decides, because they make the\n   URL's available in the first place, but what happens if a server\n   sends me an EUC URL, and I send it a SJIS one back?\n3) URL's are *not* used solely in HTTP transactions. \n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "    From: Larry Masinter <masinter@parc.xerox.com>\n    Date: Wed, 2 Aug 1995 19:37:40 PDT\n\n    'xxxx' are characters *NOT* in the native character set of the file\n    system, but rather in whatever transcription of that character set\n    is made available by the FTP server.\n\nOK.  Say the transcription system used by both the FTP client and\nserver says:\n\n(1) if an octet in the local encoding of a file name is in positions\n    0x20 - 0x24 or 0x26 - 0x7E of US-ASCII, then use that octet;\n(2) if an octet in the local encoding of a file name is in position\n    0x25 of US-ASCII (i.e., '%'), then use \"%%\"\n(3) otherwise, use %XX for each octet in the encoded file name (in big\n    endian order), where XX is the hexidecimal value of each such octet.\n\nNow, say I am a Chinese version of Windows/NT using Unicode and I ask you for:\n\n    RETR E-W%0B%00.%00T%00X%00T\n\nThat is, in Unicode I have the following encoding of my file name:\n\n    4E2D 570B 002E 0054 0058 0054\n\nSay you are a Taiwanese server using the BIG5 character set for you file\nnames.  How do you interpret my request? Do you interpret it as a BIG5\nstring? If so, then you think I just asked for the file \"E-W??.?T?X?T\"\n(assuming for a moment that you don't throw up on NUL and you interpret\nNUL and other C0 escapes as '?'.\n\nEven if both systems are using the same transcription system, we are still\nhosed because you have no idea of how to interpret the decoded results since\nyou don't know what character set encoding applies. If, on the other hand,\nyou know that Unicode was the source encoding, then, at least you know you  \nshouldn't interpret it as a BIG5 string. You may respond:  sorry, I\ndon't grok Unicode path names.  Or, if you have a Unicode -> BIG5 translation\ntable on hand, you could correctly translate it to the corresponding BIG5\nencoding: %A4%A4%B0%EA.TXT. On the other hand, if you do misinterpret the\noriginal file name as a BIG5 string, then you are going to say:\n\n  550 E-W??.?T?X?T: No such file OR directory.\n\nAnd I'm going to be clueless about what went wrong.\n\nIf the protocol doesn't communicate this information, then who/what will?\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "Let me try again:\n\n> <P>URL's often point to files on a file system, which increasingly,\n> may <EM>not</EM> have a name that uses printable ASCII characters. For\n> example, on a Japanese systems, a file might have the name\n> \"insatsu.html\", in which the \"insatsu\" might be represented in\n> romanji, katakana, hiragana, or kanji. In such cases, the octets that\n> fall outside the range of printable ASCII would be encoded as per the\n> specification, resulting in something looking like the following on\n> EUC-based systems:\n> <PRE>\n>        http://www.jacme.co.jp/%B0%F5%BA%FE.html\n> </PRE>\n\nHow the http server for www.jacme.co.jp decides to translate strings\ninto files in its local file system is COMPLETELY up to the\nimplementation of the http server. www.jacme.co.jp could be running\nsome object-oriented database operating system which doesn't have\nfiles at all.  It could be running a file system where every file and\ndirectory was 'named' with a bitmap image rather than a string of\ncharacters.\n\nThe URL standard makes no claims about the mapping of URLs to anything\nat all in the local file system of the local operating system. It\ndefines how URLs are written, and how URLs are translated into\nsequences of octets that are sent in the protocol for the particular\nscheme chosen.\n\nIf you want to build a HTTP server that accepts strings of the form\n\n   [character-set:encoding]:name-string\n\nthen feel free; however, it would have to be written\n\n    http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n\nThis convention requires no changes to the HTTP or URL standards.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">How the http server for www.jacme.co.jp decides to translate strings\n>into files in its local file system is COMPLETELY up to the\n\nTrue.\n\n>The URL standard makes no claims about the mapping of URLs to anything\n>at all in the local file system of the local operating system. It\n>defines how URLs are written, and how URLs are translated into\n>sequences of octets that are sent in the protocol for the particular\n>scheme chosen.\n \nTrue.\n\n>    http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n> \n>This convention requires no changes to the HTTP or URL standards.\n\nWell, this is where we disagree. I think there should be a *standard*\nway to send this information, so as such, the *standard* does require\nchanging. I don't mind where the information is put, but one reason\nfor preferring:\n\n     http:[EUC]//www.jacme.co.jp/%B0%F5%BA%FE.html\n\nover\n\n     http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n\nis that the latter could very will be a legal name within the system,\nleading to ambiguity. In other words, the information about coded\ncharacter set and encoding must be separated from the name itself,\nbecause *any* representation of the information could be legal in the\ncontext of a name (as you mention, anything is legal here).\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "At 7:44 AM 8/3/95, Gavin Nicol wrote:\n>I don't mind where the information is put, but one reason\n>for preferring:\n>\n>     http:[EUC]//www.jacme.co.jp/%B0%F5%BA%FE.html\n>\n>over\n>\n>     http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n>\n>is that the latter could very will be a legal name within the system,\n>leading to ambiguity.\n\nI disagree. The person in charge of www.jacme.co.jp is in charge of\ncreating the object-name part of the URLs. They can make up unambiguous\nnames. End users don't make up URLs, or if they do, they shouldn't assume\nanything about how the server will respond to their requests.\n\nIn the case you give, the server administrator can specify that:\n- [EUC]%B0%F5%BA%FE.html is an alias/link to the file called %B0%F5%BA%FE.html\n- [anything] is truncated from all incoming URLs since it is only useful to\nfolks looking at URLs, not the server\n- [EUC]%B0%F5%BA%FE.html returns a different result than [XYZ]%B0%F5%BA%FE.html\nAll of these are decisions made by the server adminstrator and anyone who\nhe or she allows to create URLs.\n\nThat being said, I think it's fine for people who create URLs that might\nhave different character sets to use a consistent naming character set\nscheme, and client makers can choose whether or not they want to do\nsomething smart about those names. However, it should not be a change to\nthe current URL RFC at this very late date. Feel free to create a seperate\ndraft that describes this as an optional naming convention.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "On Thu, 3 Aug 1995, Gavin Nicol wrote:\n> I don't mind where the information is put, but one reason\n> for preferring:\n>      http:[EUC]//www.jacme.co.jp/%B0%F5%BA%FE.html\n> over\n>      http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n> is that the latter could very will be a legal name within the system,\n> leading to ambiguity.\n\nBut surely the character set in use is a server issue and thus should be\nin the opaque data portion of the URL?  The second scheme will be\nunderstood by server for which that is the correct URL and can be handled\nwith current browsers, whereas the first version will probably cause most\ncurrent browsers severe stomach cramps even if the server understood it. \nIf the URL is valid for the server its pointing to there shouldn't be any\nambiguity with legal names in the system as the server will be expecting\nthe [EUC] or whatever.\n\nHowever, I'd rather see something tacked onto the _end_ of the URL rather\nthan at the start of the opaque data section.  Maybe something like: \n\n      http://www.jacme.co.jp/%B0%F5%BA%FE.html;charset=EUC\n\nIt just seems more inkeeping with other things I've seen suggested in the \npast.\n\nJon\n\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nJon Knight, Researcher, Sysop and General Dogsbody, Department of Computer\nStudies, Loughborough University of Technology, Leics., ENGLAND.  LE11 3TU.\n*** Nothing looks so like a man of sense as a fool who holds his tongue ***\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">I disagree. The person in charge of www.jacme.co.jp is in charge of\n>creating the object-name part of the URLs. They can make up unambiguous\n>names. End users don't make up URLs, or if they do, they shouldn't assume\n>anything about how the server will respond to their requests.\n \nThis is not always true. The [EUC] part could specify one of a myriad\ndifferent coded character sets and encodings, with the following\noctect string being different for each as well. I think it\nunreasonable to expect the administrator to maintain a database of all\npossible alias.\n\nIn addition, I really do think there needs to be a standard way of\nspecifying this kind of data. An example of *why* is spiders: they\nwalk all over the net indexing pages, and some of online indexes\ndisplay URL's as part of the textual data. Without a standard way of\nspecifying the coded character set and encoding, the URL's would\nalways have to be displayed in thier raw form.\n\n>However, it should not be a change to the current URL RFC at this\n>very late date. Feel free to create a seperate draft that describes\n>this as an optional naming convention. \n\nWell, I can sympathise with this position. The change I recommend\nis backward compatible, and will be part of the upcoming I18N RFC.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "In message <Pine.SUN.3.91.950803180302.496U-100000@weeble.lut.ac.uk>, Jon Knigh\nt writes:\n>On Thu, 3 Aug 1995, Gavin Nicol wrote:\n>> I don't mind where the information is put, but one reason\n>> for preferring:\n>>      http:[EUC]//www.jacme.co.jp/%B0%F5%BA%FE.html\n>> over\n>>      http://www.jacme.co.jp/[EUC]%B0%F5%BA%FE.html\n>> is that the latter could very will be a legal name within the system,\n>> leading to ambiguity.\n>\n>But surely the character set in use is a server issue and thus should be\n>in the opaque data portion of the URL?\n\nThat makes a certain amount of sense, but: what about the poor slob\nclients? It might be useful to make the clients aware of some\nconvention for encoding other character sets, so that arabic\nnames show up in arabic on capable clients. And once you've\ngot agreement between clients and servers, you're talking about\nstandardization. :-{\n\nDan\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">That makes a certain amount of sense, but: what about the poor slob\n>clients? It might be useful to make the clients aware of some\n>convention for encoding other character sets, so that arabic\n>names show up in arabic on capable clients. And once you've\n>got agreement between clients and servers, you're talking about\n>standardization. :-{\n\nSad but true ;-)\n\n\n\n"
        },
        {
            "subject": "Re: potential security holes in digest authorizatio",
            "content": "For some reason the realm is one of the places where usability and\nsecurity meet, and are perhaps muddled. I agree with David when he\nsays:\n\n  > perverse.  I prefer\n  >Enter username for [prompt that I specify] at www.research.att.com:\n  > to\n  > Enter username for myrealm@www.myplace.com at www.myplace.com\n\nI don't think I've entirely groked the security argument around always\nand only echoing the realm as is. Perhaps it's all the time I spent\nwith DCE, where UUIDs and end user string representations are\nseparate. So, I went to the HTTP spec to learn just what a realm\nis. It tells me:\n\n\n   \"HTTP access authentication is described in Section 10. If a request \n   is authenticated and a realm specified, the same credentials should \n   be valid for all other requests within this realm.\n\n[...]\n\n   The realm attribute is required for all access authentication \n   schemes which issue a challenge. The realm value, in combination \n   with the root URL of the server being accessed, defines the \n   authorization space. These realms allow the protected resources on \n   a server to be partitioned into a set of authorization spaces, each \n   with its own authentication scheme and/or database. The realm value \n   is a string, generally assigned by the origin server, which may \n   have additional semantics specific to the authentication scheme.\n\n[...]\n\n   The basic authentication scheme is based on the model that the \n   client must authenticate itself with a user-ID and a password for \n   each realm. The realm value should be considered an opaque string \n   which can only be compared for equality with other realms. The \n   server will service the request only if it can validate the user-ID \n   and password for the domain of the Request-URI.\"\n\nOK; I don't know what a root URL is (the server address? the directory\nconfigured as my server's document root?), but it must be\nserver-specific. So it looks like all a realm is is a label for the\nrange of authority of a set of authenticated identities; a name for a\npassword database or authentication authority. In fact, if all a\nbrowser can do with a realm is compare it with other realms, what's it\ndoing putting it in the prompt? Clearly I'm missing something.\n\nNow the DAA proposal says:\n\n\"<realm>\n         A name given to users so they know which username and password\n         to send.\"\n\nThat makes it just a string for end user usability, and not an\nidentifier of the scope of authority of a password\ndatabase. Surprisingly, a search on \"realm\" does not indicate that the\nrealm is part of the hashed password as a salt. But Phill claims it\nis, and I always believe Phill :-). So it's clearly doing at least\ndouble duty in the DAA proposal. \n\nIt has also been argued that it is a weak form of server\nauthentication to echo the realm in the prompt, so that the user does\nnot give away her password due to a spoof (I believe that is in part\nwhat Phill's comment about registering realms is about). I must admit\nI find this line of reasoning confusing. It may perhaps be slightly\neasier to do this than to sniff the line, but it seems to me that the\nmalicious server has to either attract it's victims in the first place\nwith sweet URLs, or has to tamper with a request. \n\nSo, when David says\n\n> I'm obviously missing something here concerning realms.  I've seen realms\n> used:\n> 1) As a name in Web servers, to distinguish separately protected domains\n> of information.\n> 2) As a component of the WWW-Authenticate response header.\n> 3) As the prompt from a Web broswer for a user to authenticate him/herself.\n\nHe captures many of the documented parts that realm plays (at least in the\ntwo specs I read). He is missing the salt role and the \"mutual\nauthentication\" part. Is it doing anything else anywhere?\n\nDo we think the realm is doing too much yet? I do.\nMez\n\n\n\n"
        },
        {
            "subject": "HTTP/1.0 draft 01 (quick note",
            "content": "Hi all,\n\nI just made the fourth edition of the HTTP/1.0 draft available at\nthe usual places (below), but I haven't generated the text and HTML\nformats yet.  I'll do that in the morning and distribute a more general\nnotice then.\n\n     http://www.ics.uci.edu/pub/ietf/http/\n      ftp://www.ics.uci.edu/pub/ietf/http/\nand\n     http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "At 11:20 AM 8/3/95, Gavin Nicol wrote:\n>This is not always true. The [EUC] part could specify one of a myriad\n>different coded character sets and encodings, with the following\n>octect string being different for each as well. I think it\n>unreasonable to expect the administrator to maintain a database of all\n>possible alias.\n\nThey don't have to: they are the ones creating the URLs! If they tack a\n[XYZ] at the front or end of the URL (wherever your proposed standard puts\nit), they should certainly be able to serve it.\n\nThis is an issue for people who create the URLs, and for the Web browser\nwriters. It doesn't have anything to do with HTTP or HTML (the two IETF WG\nlists on which this seems to be being discussed). It's a URI issue all the\nway.\n\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "    Date: Fri, 4 Aug 1995 08:34:49 -0700\n    From: ietf-lists@proper.com (Paul Hoffman)\n\n    It's a URI issue all the way.\n\nThen the URI standard is in need of revision for I18N. It is hopelessly\nbroken.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "Paul Hoffman: It's a URI issue all the way.\nGlenn Adams:  Then the URI standard is in need of revision for I18N. It is hopelessly\n              broken.\nLarry Masinter:\nHyperbole is hopelessly broken.\n\nWhen there are Internet protocols that define network protocols that\nproperly address international issues, then we can invent URL schemes\nfor those protocols. Until then, URLs are merely a convenient linear\nform for what exists.\n\n(This isn't a HTML issue, but it is possibly a HTTP issue. Perhaps\nsome future version of what replaces HTTP can properly express\ncharacter encodings in all of the strings, including HEAD attributes,\netc.)\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": ">>This is not always true. The [EUC] part could specify one of a myriad\n>>different coded character sets and encodings, with the following\n>>octect string being different for each as well. I think it\n>>unreasonable to expect the administrator to maintain a database of all\n>>possible alias.\n> \n>They don't have to: they are the ones creating the URLs! If they tack a\n>[XYZ] at the front or end of the URL (wherever your proposed standard puts\n>it), they should certainly be able to serve it.\n \nYes, but there is a large number of ways of encoding the strings of\ncharacters making up the URL. That's my point.\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "On Thu, 3 Aug 1995, Jon Knight wrote:\n\n> On Thu, 3 Aug 1995, Gavin Nicol wrote:\n> However, I'd rather see something tacked onto the _end_ of the URL rather\n> than at the start of the opaque data section.  Maybe something like: \n> \n>       http://www.jacme.co.jp/%B0%F5%BA%FE.html;charset=EUC\n> \n> It just seems more inkeeping with other things I've seen suggested in the \n> past.\n\nI can't refrain from reminding you about what we -- three\nIETF:ers from Sweden -- suggested on this subject already in\nMay 1993 (and which nobody took any notice of...):\n\n------\nDate: Tue, 11 May 93 10:36:58 +0200\nMessage-Id: <9305110836.AA06718@mercutio.admin.kth.se>\nFrom: Rickard Schoultz <schoultz@othello.admin.kth.se>,\n        Olle Jarnefors <ojarnef@admin.kth.se>,\n        Peter Svanberg <psv@nada.kth.se>\nSender: Olle Jarnefors <ojarnef@admin.kth.se>\nTo: uri@bunyip.com\nIn-Reply-To: <9305080145.AA05811@wilma.cs.utk.edu>\n (Fri, 07 May 1993 21:45:02 -0400, Keith Moore <moore@cs.utk.edu>)\nSubject: Re: Wrappers for URLs\n\n:\n:\nAs has already been pointed out in this discussion, non-ASCII\ncharacters are used a lot in many countries outside USA.  In a\nlanguage using the Latin script like Swedish almost 1/3 of all\nwords contain non-ASCII letters.  In languages such as Greek,\nRussian, Hindi and Chinese no ASCII letters at all are used.\nFor ordinary users in these countries it will be unacceptable to\nsee their everyday letters represented by %-headed hexadecimal\ndigit sequences.\n\nWe propose this solution:\n\nC) In the human form of URLs non-ASCII characters may be used\n   provided a character set indicator is added to the URL\n   immediately before the closing \">\".  This indicator shall\n   have the syntax\n      \"%:\" charset\n   where \"charset\" is a value registered by IANA for MIME use.\n   The corresponding coded character set defines the mapping of\n   the non-ASCII character to a sequence of octets that can be\n   represented by the %-mechanism in the program form of the\n   URL.\n\nTake as an example a file called\n\n   l<a\">s mig\n\nin the directory pub at host othello.admin.kth.se.  Here <a\">\nis the character LATIN SMALL LETTER A WITH DIAERESIS, coded by\nthe octet E4 hex according to ISO-8859-1.  (The name of the file\nis Swedish for \"read me\".)\n\nThe human form URL for this file preferred in Sweden would be\n\n   <ftp://othello.admin.kth.se/pub/l<a\">s mig%:iso-8859-1>\n\n<a\"> here would in reality be the non-ASCII character.\n\nThe program form would be\n\n   <ftp://othello.admin.kth.se/pub/l%e4s mig>\n\nWhy is it necessary to include a character set indicator in\nthese extended URLs containing non-ASCII characters?  It's\nbecause that makes the URL resistent to character-preserving\nconversion between different coded character sets.\n\nSay that the URL in the example is included in a text file coded\nwith ISO-8859-1, so the non-ASCII character is represented by\nthe octet E4.  Then this file is transferred to a Mac and\ntherefore converted to the Macintosh character set.  For the Mac\nuser it will look exactly as intended, containing a lowecase a\nwith diaeresis (which is necessary to form the Swedish\nexpression for \"read me\").  In the Mac file, however, this\nletter will be represented by 8A instead of E4.  Thanks to the\ncharacter set indicator %:iso-8859-1 a client program on the Mac\nwill however be able to feed the right octet E4 to the FTP\nprogram to fetch the correct file from the FTP server (where\nISO-8859-1 is used in file names).\n\nIt could be argued that occasional non-ASCII letters is nothing\nto make a fuss about: European users can be taught to read and\ninput %-sequences instead.  But consider a Greek FTP server,\nwhere almost the whole path of a file is written with Greek\nletters using ISO-8859-7.  In that case URLs will be almost\nthree times as long and consist of mostly a soup of \"%\" and\nhexadecimal digits, interspersed with \"/\".  Such URLs will be\nunusable for humans, unless some way of using the real non-ASCII\nletters is provided.\n\nAnother points in connection with internationalized URLs that\nwe would like to raise:\n\nD) The hexadecimal %-headed representation used in the program\n   form is very inefficient.  In countries with languages using\n   other scripts than the Latin URLs may be almost three times\n   as long as in English-speaking countries.  To reduce this\n   unfairness we could, in addition to the %-representation.\n   include a &-representation: After \"&\" would follow a sequence\n   of octets encoded by a BASE64-like method into a 33 %\n   (instead of 300 %) longer sequence of the characters A-Z,\n   a-z, 0-9, + and -.  This sequence would be ended by a\n   second \"&\".\n\n--\nRickard Schoultz                        schoultz@admin.kth.se\nSUNET/KTH                               +46-8-790 90 88   (voice)\nS-100 44 Stockholm (SWEDEN)             +46-8-10 25 10    (fax)\n\nOlle Jarnefors                     Internet: ojarnef@admin.kth.se\nInformation Management Services        UUCP: ...!uunet!mcsun!sunic!kth!ojarnef\nRoyal Institute of Technology (KTH)  BITNET: ojarnef@sekth  Fax:+46 8 10 25 10\nS-100 44  Stockholm, Sweden           Phone: +46 8 790 71 26 (time zone +0200)\n\nPeter Svanberg, NADA, KTH                   Email: psv@nada.kth.se\nDept of Num An & CS,\nRoyal Inst of Tech                          Phone: +46 8 790 71 40\nS-100 44  Stockholm, SWEDEN                 Fax:   +46 8 790 09 30\n\n\n\n"
        },
        {
            "subject": "Re: Globalizing URI",
            "content": "> Somethink like text/tab-separated-values might work nicely.\n\nWhere can I find the definition of text/tab-separated-values ?\nI've found the name in RFC1700, but I couldn't find its refrences.\n\n\n\n"
        },
        {
            "subject": "Re: Content-TransferEncoding &quot;packet&quot",
            "content": "Sorry for the delay in responding -- all three of the machines I was\nusing died on Aug 1 due to separate hardware failures.  Personally, I\nthink it was a curse brought on by the WG minutes ...\n\n>On Thu, 27 Jul 1995, Roy Fielding wrote:\n>> >Note that an almost identical mechanism was proposed for SMTP - I think\n>> >it is still one of the \"draft-ietf-mailext\" drafts.\n>> \n>> Yes, I am aware of <draft-ietf-mailext-smtp-binary-07.txt>.  That mechanism\n>> uses a series of separate messages (each message given a 1*DIGIT length)\n>> to send the binary data.  In other words, it is a stateful message sequence,\n>> which is something HTTP doesn't do.  \n>\n>Could please clarify what you mean by 'series of separate messages' since \n>this terminology doesn't fit my understanding of SMTP chunking. The BDAT \n>command doesn't in any way change data content in the pipe. It's also \n>streamable in the sense that you can have more than on outstanding BDAT \n>chunk pending on the connection.\n\nOoops, confusing terminology.  SMTP involves sending a series of commands\nand getting a series of responses, with many commands per mail message.\n\nHTTP sends one \"command\" in the form of a message, and gets one response.\nSo, while the concept is similar (and easily translatable for an\nHTTP<->SMTP gateway), they are not \"almost identical\".\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "SessionID proposa",
            "content": "Various Session ID schemes that would allow a server to establish some\nkind of state (akin to Netscape cookies) have been discussed here\nrecently.  I've gathered my thoughts and earlier comments together into\nan Internet non-draft (as yet) for your perusal and comment.  If\nthere's enough positive feedback, I will submit the proposal as a real\nInternet Draft (Internet genuine draft? :-).\nhttp://www.research.att.com/~dmk/session.html\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "reality check:  HTTP comment",
            "content": "Just curious:  do any servers handle comments (including nested ones)\nin HTTP headers, per the HTTP 1.0 specification (sect. 2.2)?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Keep Alives/Session Extensio",
            "content": "!!!! This is not about Session-IDs.  It's about persistant connections.\n\nIs the flipping from \"Connection:\" to \"Session:\" in the Alex Hopman session\nextension draft intentional?  Even from a third reading, it doesn't seem so.\nHowever, this could be a potentially beneficial typo.  I would like to see\nBOTH a Connection: header and a Session: header, the first indicating that\nonly the connection is persistent, the second indicating that not only is\nthe connection persistent, but various headers including the Accept: and\nWWW-Authenticate: are persistent/saved as well.\n\nImplementation of the acceptance array storing is annoying at best, because\nit brings information that naturally pertains to 'request objects' up to a\n'connection object' level, and could get really ugly.  The benefits are\nprobably negligible relative to the TCP overhead, certainly for Accept:\nheaders, and maybe even for WWW-Authenticate: computations.  Anyway, having\nboth Connection: and Session: and would greatly speed up wide-spread\nimplementation (of the former at least), which is what we all want, right?\n\nPS: Oh, and if this WAS an intentional change of terms, indicating exactly\nwhat I described above, please help me remove my foot from my mouth.\n\nPSS: This goes without saying, but we don't have to have Connection: and\nSession:.  There could be \"Connection: maintain\" and \"Connection: maintain,\naccept\", \"Connection: maintain, authenticate\", etc...  Or any other of a\nvariety of syntaxes yielding the behavior I described in paragraph 1.\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nre:  Dave Kristol's Session-ID proposal\n\nWHEN DOES  A SESSION END?\n\nYour draft says that one of the \"key points in the session paradigm\" is that\n\"The session has a beginning and an end.\" However, in your draft, it isn't\napparent in all cases that both parties (or all parties if a proxies are\ninvolved) can determine when a session ends.\n\nYou insist that \"when the client terminates execution, it discards all\nSession-ID information.\" This, I assume, \"ends\" the session from the point\nof view of the client. However, the server will never discover that this has\nhappened.\n\nAlthough you don't specify it clearly, the requirement that a client \"must\nreturn [the Session-ID] to the server for the next transaction by any method\n...\" might be read to imply that the client can end a session by making\nanother request without sending the Session-ID of the last response (if non-\nconformance is an implicit \"end\".). This would require the server to\nmaintain a list of Session-ID's and the client hosts to which they were\nassigned and then doing a lookup in this list on *every* request to\ndetermine if the most recent request should have used a Session-ID. But,\nthat won't work for clients that are hidden behind proxies or that come from\nmulti-tasking machines since many clients may appear to be using the same\nhost name.\n\nOn the server side, you say that the server may send \"...a different, or no\nSession-ID response header\" in response to a request which includes a\nSession-ID. Although your draft doesn't explicitly say this, I assume you\nintend that when the client receives a Session-ID response which is\ndifferent from that transmitted in the corresponding request, that the\nclient should assume that the old session has ended and a new session may be\nstarting. However, it is a method whose utility is limited to only those\ntimes when the session's end coincides with an opportunity to send a\nresponse.\n\nThe fear, of course, is that since there are a number of scenarios in which\nthe server can't discover that a session has ended, that the server will be\nforced to build an ever-growing list of active sessions. This is not good.\nOne solution to this problem would be to provide for a session expiration\ndate (either absolute or relative to last response) which would give the\nserver a mechanism for purging it's Session-ID tables. (Note: Netscape's\n\"cookie\" proposal does something similar although expiring a \"cookie\" is\nsomewhat different than expiring a Session-ID since one would assume that a\nSession-ID has some level of uniqueness -- not addressed in your draft ---,\nwhile there is no such requirement for a cookie.)\n\nWHAT IS  A CLIENT?\n\nAs far as I know, there is no formal reference model for the Web, thus, it\nis necessary from time to time to ask what people mean when talking about\nspecific architectural elements. Seeing your requirement that client\n\"discards all Session-ID information\" when it terminates, and that the\n\"client\" must send the Session-ID on the next request I'm worried about\nimprecision in defining the client. For instance, if I have a Web browser\nthat allows me to have two open windows (i.e. Netscape), if I get a Session-\nID as the result of activity in window \"A\", am I required to send that\nSession-ID with requests generated in window \"B\"? If I close window \"A\", do\nI keep the Session-ID or delete it?\n\nWHAT IS A SERVER?\n\nMany of the demands for Session-ids or session-state have been intended to\nallow CGI scripts to distinguish between clients and to maintain client-\nspecific state on the server side. This leads to the question (another\nreference model problem): What is the server? Is the session with the HTTP\nserver or is it with the CGI script? Your draft indicates that you think the\nserver is identified by \"server name (IP address) and port combination.\" It\nseems to me that this means we're going to have to implement some\npotentially complex method for letting CGI scripts know the session ids for\nthe clients they speak to. Choices seem to be: 1) a configuration parameter\nthat tells the server to always generate Session-ID's when a particular CGI\nscript is run. The Session-ID would then be given to the CGI in an\nenvironment variable or some simlilar process. 2) An API by which the CGI\ncan ask the server to provide a Session-ID and tell the CGI script what it\nis. Additionally, given that you suggest that Session-ID's can be changed or\neliminated by the server, we'll need a mechanism for CGI scripts and servers\nto negotiate and/or inform each other of these changes.\n\nWhatever the method of telling the CGI what the Session-ID is, unless the\nspecification states that Session-ID's have some sort of uniqueness to them,\nthey won't be useful for many of the purposes that CGI scripts would want to\nuse them.\n\nMy personal preference would be for the CGI to be able to generate its own\nSession-ID which addresses whatever it thinks its operational requirements\nare. Thus, I would argue that the CGI script is the \"server\", not the HTTP\ndaemon.\n\nARE WE DEFINING PROTOCOL OR USER INTERFACE?\n\nYour requirement that the client discard information when it \"terminates\nexecution\" might be a good recommendation, however, it is inappropriate as a\n*requirement* of the HTTP Protocol. The protocol should place no constraints\non program execution models -- only on what data flows over the wire. \n\nPROBLEMS WITH CACHING\n\nYou state that when a caching proxy gets a Session-ID response header, \"it\nmust not cache that header as part of its cache state.\" However, you don't\nprohibit the caching proxy from caching the body of the response. Thus, it\nwould appear that a second client could make a request and have that request\nsatisfied from cache without ever discovering that Session-ID's were\navailable for the document. This would be a particular problem when a\nresponse came back with both a Session-ID and an Expires: header since the\ncache might decide not to do a HEAD, GET, or conditional GET until after or\nclose to the Expire time. It would seem that the cache should remember that\nthe Session-ID response header was there (whether or not it caches the\nactual Session-ID) and then always do a conditional GET for the document\neven if the Expires: time hasn't passed. (NOTE: Of course, you could insist\nthat the semantics of Expires be changed if found coincident with a Session-\nID in a response.  -- the question is whether the \"Expires\" header is\nglobally meaningful or meaningful only within the context of the session)\n\nWHAT PROBLEMS ARE BEING SOLVED HERE?\n\nThe draft doesn't really give any information about the specific uses that\nare expected for the protocol features defined. This makes it hard to\nevaluate. For instance, I can see that what you have defined would be useful\nin tracking \"clickstreams.\" However, the problems mentioned above and others\nnot mentioned make it hard to see how this proposal will help with \"shopping\ncarts\" and a variety of other applications identified in the recent www-talk\ndiscussions on this subject.\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "koen@win.tue.nl (Koen Holtman) wrote (on www-talk):\n  > Dave Kristol:\n  > [....]\n  > >http://www.research.att.com/~dmk/session.html\n  > \n  > This proposal is not clear enough about caching. Specifically:\n  > \n  >   is the session-id header in the request part of the cache key for the\n  >   entity in the response?\nNo.  In section 2.3 I said:\n    Similarly, a caching proxy must pass back to the requestor any\n    Session-ID response header it receives, but it must not cache that\n    header as part of its cache state.\n  > \n  > If it is, this means that almost no meaningful caching is possible for\n  > services using session-id, even if 99% if the entities in the session\n  > (inline pictures, product description pages) do not depend on the session\n  > state.\nYes, exactly.\n  > \n  > If it is not, session-id may be an unreliable mechanism (in that it cannot\n  > be used to build reliable statfull dialogs), depending on how pessimistic\n  > you are about cache administrators adhering to standards.\nThat's probably true short-term.  As you say, the result is unreliable\nstateful dialogs if caching proxies don't pass Session-IDs in both\ndirections.\n  > \n  > I'm busy on a session-id proposal that tries to resolve these issues.\n  > I expect to be posting it at the end of this week.\nExcellent.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "Bob Wyman <bobwyman@medio.com> wrote:\n  > \n  > re:  Dave Kristol's Session-ID proposal\n  > \n  > WHEN DOES  A SESSION END?\n  > \n  > Your draft says that one of the \"key points in the session paradigm\" is that\n  > \"The session has a beginning and an end.\" However, in your draft, it isn't\n  > apparent in all cases that both parties (or all parties if a proxies are\n  > involved) can determine when a session ends.\nAgreed.  In fact, either the client or the server can end a session.\nThe client does so by failing to return a Session-ID.  Because the\nsession information is opaque, the client can't really determine when\nthe server ends (or changes) a session, except perhaps if the server\nreturns an empty Session-ID or none.  In fact, the client is a\nrelatively passive collaborator in this mechanism.\n  > \n  > You insist that \"when the client terminates execution, it discards all\n  > Session-ID information.\" This, I assume, \"ends\" the session from the point\n  > of view of the client. However, the server will never discover that this has\n  > happened.\nI'm also assuming that it's not important that the server know this.\n(See below.)\n  > \n  > Although you don't specify it clearly, the requirement that a client \"must\n  > return [the Session-ID] to the server for the next transaction by any method\n  > ...\" might be read to imply that the client can end a session by making\n  > another request without sending the Session-ID of the last response (if non-\n  > conformance is an implicit \"end\".). This would require the server to\nIt isn't non-conformance, really.  It grants control to the client to\nchoose how willing it is to identify itself with a session.  There has\nbeen much discussion about the tradeoff of privacy through \"clicktrail\"\nfollowing vs. the usefulness of having stateful sessions.  So, yes, a\nclient can end a session as you say.\n  > maintain a list of Session-ID's and the client hosts to which they were\n  > assigned and then doing a lookup in this list on *every* request to\n  > determine if the most recent request should have used a Session-ID. But,\n  > that won't work for clients that are hidden behind proxies or that come from\n  > multi-tasking machines since many clients may appear to be using the same\n  > host name.\nI disagree.  The point of the proposal is that all interesting state should\nbe embedded in the opaque session information.  The whole point of the\nproposal, in fact, is to avoid the server's having to keep any kind of\ndatabase to keep track of sessions.  The state bounces back and forth in\nSession-ID headers.\n\nFirewalls and proxies pose no particular problem, because if your request\ninitiates a new session, your client gets a new set of state information\nthat is unique to the request.  The server augments it (perhaps) as it\nservices your request, then returns a revised Session-ID header, which\nyour client saves and returns the next time you make a request to the\nsame server.\n  > \n  > On the server side, you say that the server may send \"...a different, or no\n  > Session-ID response header\" in response to a request which includes a\n  > Session-ID. Although your draft doesn't explicitly say this, I assume you\n  > intend that when the client receives a Session-ID response which is\n  > different from that transmitted in the corresponding request, that the\n  > client should assume that the old session has ended and a new session may be\n  > starting. However, it is a method whose utility is limited to only those\n  > times when the session's end coincides with an opportunity to send a\n  > response.\nAnother point of my proposal is that the client does not have to be very\nsmart.  In particular, its notion of session is limited to the requirement\nthat it return any Session-ID it gets to the server from which it got it.\nSo the client doesn't know or care whether a session has ended and a new\none has started.  Such information would be embodied in the content that\nthe server returns to the client.\n  > \n  > The fear, of course, is that since there are a number of scenarios in which\n  > the server can't discover that a session has ended, that the server will be\n  > forced to build an ever-growing list of active sessions. This is not good.\n  > One solution to this problem would be to provide for a session expiration\n  > date (either absolute or relative to last response) which would give the\n  > server a mechanism for purging it's Session-ID tables. (Note: Netscape's\n  > \"cookie\" proposal does something similar although expiring a \"cookie\" is\n  > somewhat different than expiring a Session-ID since one would assume that a\n  > Session-ID has some level of uniqueness -- not addressed in your draft ---,\n  > while there is no such requirement for a cookie.)\nNot necessary.  As I said, the state is in the opaque session\ninformation and passes back and forth between client and server.  If a\ngiven server wants to put an expiration in that information and act on\nsuch an expiration, it is free to do so.  Or not.\n  > \n  > WHAT IS  A CLIENT?\n  > \n  > As far as I know, there is no formal reference model for the Web, thus, it\n  > is necessary from time to time to ask what people mean when talking about\n  > specific architectural elements. Seeing your requirement that client\n  > \"discards all Session-ID information\" when it terminates, and that the\n  > \"client\" must send the Session-ID on the next request I'm worried about\n  > imprecision in defining the client. For instance, if I have a Web browser\n  > that allows me to have two open windows (i.e. Netscape), if I get a Session-\n  > ID as the result of activity in window \"A\", am I required to send that\n  > Session-ID with requests generated in window \"B\"? If I close window \"A\", do\n  > I keep the Session-ID or delete it?\nExcellent question.  I hadn't considered that case.  If the two windows\ncan act wholly independently, I would consider them to behave like two\nseparate client programs and act accordingly with respect to Session-ID.\n  > \n  > WHAT IS A SERVER?\n  > \n  > Many of the demands for Session-ids or session-state have been intended to\n  > allow CGI scripts to distinguish between clients and to maintain client-\n  > specific state on the server side. This leads to the question (another\n  > reference model problem): What is the server? Is the session with the HTTP\n  > server or is it with the CGI script? Your draft indicates that you think the\n  > server is identified by \"server name (IP address) and port combination.\" It\n  > seems to me that this means we're going to have to implement some\n  > potentially complex method for letting CGI scripts know the session ids for\n  > the clients they speak to. Choices seem to be: 1) a configuration parameter\n  > that tells the server to always generate Session-ID's when a particular CGI\n  > script is run. The Session-ID would then be given to the CGI in an\n  > environment variable or some simlilar process. 2) An API by which the CGI\n  > can ask the server to provide a Session-ID and tell the CGI script what it\n  > is. Additionally, given that you suggest that Session-ID's can be changed or\n  > eliminated by the server, we'll need a mechanism for CGI scripts and servers\n  > to negotiate and/or inform each other of these changes.\nGiven my earlier remarks about how state resides in the opaque\ninformation, it should be clear that the current CGI mechanism is\nadequate.  A CGI can examine the incoming Session-ID, if any, and can\ngenerate a new one or repeat an old one.  Clearly the content of the\nopaque information is application-dependent.  I won't dispute that\nthings could get messy.  I'm trying to propose enabling technology,\nnot solve all the problems.\n\nOne possible implementation is for the opaque information to refer to a\nfile or database on the server side.  The server should be able to collect\nold (datebase) information and identify stale (opaque) information.\n  > \n  > Whatever the method of telling the CGI what the Session-ID is, unless the\n  > specification states that Session-ID's have some sort of uniqueness to them,\n  > they won't be useful for many of the purposes that CGI scripts would want to\n  > use them.\n  > \n  > My personal preference would be for the CGI to be able to generate its own\n  > Session-ID which addresses whatever it thinks its operational requirements\n  > are. Thus, I would argue that the CGI script is the \"server\", not the HTTP\n  > daemon.\nI'm not sure how to respond to these two paragraphs.  I'll hope my\nearlier remarks have done so.  From a user's perspective, an HTTP\ndaemon program and a CGI script that it starts are indistinguishable.\n  > \n  > ARE WE DEFINING PROTOCOL OR USER INTERFACE?\n  > \n  > Your requirement that the client discard information when it \"terminates\n  > execution\" might be a good recommendation, however, it is inappropriate as a\n  > *requirement* of the HTTP Protocol. The protocol should place no constraints\n  > on program execution models -- only on what data flows over the wire. \nI think there are other, similar, examples (If-Modified-Since) where\nthe protocol specification has to spill over into execution models to\nbe useful.  And again, the discarding of Session-IDs is partly a\nprivacy matter, as well as a protocol matter.\n  > \n  > PROBLEMS WITH CACHING\n  > \n  > You state that when a caching proxy gets a Session-ID response header, \"it\n  > must not cache that header as part of its cache state.\" However, you don't\n  > prohibit the caching proxy from caching the body of the response. Thus, it\n  > would appear that a second client could make a request and have that request\n  > satisfied from cache without ever discovering that Session-ID's were\n  > available for the document. This would be a particular problem when a\n  > response came back with both a Session-ID and an Expires: header since the\n  > cache might decide not to do a HEAD, GET, or conditional GET until after or\n  > close to the Expire time. It would seem that the cache should remember that\n  > the Session-ID response header was there (whether or not it caches the\n  > actual Session-ID) and then always do a conditional GET for the document\n  > even if the Expires: time hasn't passed. (NOTE: Of course, you could insist\n  > that the semantics of Expires be changed if found coincident with a Session-\n  > ID in a response.  -- the question is whether the \"Expires\" header is\n  > globally meaningful or meaningful only within the context of the session)\nOther proposals similar to this one have run into caching problems\nbecause the state is somehow embedded in the document or URL.  I wanted\nto avoid that.  I'm assuming that the resource content is the same,\nindependent of Session-ID (subject to the same constraints that would\napply if there were no Session-ID).  Hence the Session-ID gets passed\nthrough the proxy in both directions, but the resource content itself\ncan be cached.  Given this treatment of Session-ID, it's expected and\ndesirable for a caching proxy to serve out the same content when\npossible.\n  > \n  > WHAT PROBLEMS ARE BEING SOLVED HERE?\n  > \n  > The draft doesn't really give any information about the specific uses that\n  > are expected for the protocol features defined. This makes it hard to\n  > evaluate. For instance, I can see that what you have defined would be useful\n  > in tracking \"clickstreams.\" However, the problems mentioned above and others\n  > not mentioned make it hard to see how this proposal will help with \"shopping\n  > carts\" and a variety of other applications identified in the recent www-talk\n  > discussions on this subject.\n\nActually, I had \"clickstreams\" in mind less than \"shopping carts\" and\nlibrary system navigation.\n\nThanks for your extensive comments.  I'll try to tweak the proposal to\nimprove it.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v10-spec01.txt, .p",
            "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.0                 \n       Author(s) : T. Berners-Lee, R. Fielding, H. Nielsen\n       Filename  : draft-ietf-http-v10-spec-01.txt, .ps\n       Pages     : 58\n       Date      : 08/07/1995\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol \nwith the lightness and speed necessary for distributed, collaborative, \nhypermedia information systems. It is a generic, stateless, object-oriented\nprotocol which can be used for many tasks, such as name servers and \ndistributed object management systems, through extension of its request \nmethods (commands). A feature of HTTP is the typing and negotiation of data\nrepresentation, allowing systems to be built independently of the data \nbeing transferred.                               \n\nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990.  This specification reflects preferred usage of the protocol \nreferred to as \"HTTP/1.0\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v10-spec-01.txt\".\n Or \n     \"get draft-ietf-http-v10-spec-01.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v10-spec-01.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-01.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-01.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950807134042.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-v10-spec-01.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-v10-spec-01.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950807134042.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nDave Kristol wrote: \n> Actually, I had \"clickstreams\" in mind less than \"shopping carts\" and \n> library system navigation.\n\nIf this is the case, I would suggest that you could do reviewers a service\nby stating this in the introduction to your draft. I would also suggest that\nit may be a disservice to introduce a protocol modification that has such\nlimited utility. This is particularly true since we already have outstanding\nthe Netscape \"cookie\" proposal which appears to be able to support\n\"clickstreams\" as well as a range of other requirements for session state.\nIf your limited proposal is accepted into HTTP, it seems inevitable that\nsomeone else will argue for and probably succeed in getting a second session\nstate mechanism adopted that handles the other requirements. The result will\nbe, of course, more crust and rust introduced into this young, but rapidly\ntiring, protocol.\n\n>The point of the proposal is that all interesting state should be\n> embedded in the opaque session information.\n\nIt is clear from your comments that you view the opaque data carried in the\n\"Session-ID\" to be something other than and more than a simple identifier\nfor a session. I must admit, however, that I'm confused since if you concern\nis really just \"clickstreams\" then you should be satisfied with a simple\nidentifier... Anyway, I would suggest that you call the thing something\nother than an \"ID.\" You should seriously consider following Behlendorf's\nlead and calling it \"State:\" or \"Session-State:\" Perhaps the non-native-\nenglish-speakers will object to my desire to have protocol tags be faithful\nto the English language... Nonetheless, I think that as long as we're not\nusing hex numbers to identify these things, we should view such faithfulness\nas a means of clarifying the intent of the author.\n\n>A CGI can examine the incoming  Session-ID, if any, and can \n> generate a new one or repeat an old one.  Clearly the content\n> of the opaque information is application-dependent.\n\nIf the opaque information is really application-dependent then there is a\nsmall problem here... You state in your proposal that the \"Session-ID\" must\nbe returned in the next request following the response in which it is\nreceived. Thus, I may get a Session-ID from CGI script \"A\" and if the next\nURL I request is CGI script \"B\", then I am required to forward the Session-\nID I received from \"A\" to \"B\". (I won't reflect on privacy concerns at this\npoint...) The problem here is that \"B\" won't have any idea if it understands\nthe opaque data received since it has no way of knowing who authored it.\nEven if I had sent the Session-ID back to \"A\", script \"A\" would have no way\nof knowing how to interpret the opaque data. Inevitably, we'll see people\nembedding \"application ids\" into this otherwise opaque data and then\nbuilding informal consensus on how to inspect the beginning of a session id\nto determine if it is \"yours.\" The opaque data will start to be less opaque.\nThis will get really ugly when people start declaring rules like: \"If you\ndon't understand the session data, push it back in the response inside a\nwrapper defined in RFCXXXX.\" The result will be rapid accretion of odd bits\nof session data and rapidly growing packets...\n\nNote: The Netscape \"cookie\" proposal has much the same problem, however, the\nimpact is somewhat limited by their inclusion of the \"path\" attribute on the\ncookie. The client is able to be somewhat specific about who it sends the\nsession data to. This reduces the problem somewhat, but not completely.\n\n> Other proposals similar to this one have run into caching problems\n> because the  state is somehow embedded in the document or URL.\n>  I wanted to avoid that.  I'm  assuming that the resource content is\n> the same, independent of Session-ID ... Given this treatment of \n> Session-ID,  it's expected and desirable for a caching  proxy to serve\n> out the same content when possible.\n\nYou imply here that a Session-ID offers no additional semantic richness to a\nclient-server exchange. Thus, a session using Session-IDs and one without\nwould result in identical content exchanges other than Session-ID. If this\nis the case, it would appear that there is no apparent benefit to a client\nfrom hassling with Session-ID's. The only benefit from these things is to\nallow server administrators to characterize their load and marketing or law\nenforcement folk to study our behaviour. If Session-ID is to have some\nutility to the client, then there must be a mechanism specified for a\ncaching proxy to obtain correct and appropriate Session-ID's for each\nrequest -- even if the response body is cached.\n\nAnyway... the basic point here is that I think that clickstreams aren't\nimportant enough to address in isolation of the other requirements for\nserver state. It appears that a solution to the larger problems will\nincorporate a solution for clickstreams as a subset. But, it appears that\nthe larger problems cannot be solved as a simple super-set of clickstream\nsupport. Thus, we should probably address the harder problems first...\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: http-v10-spec01.ps nit",
            "content": ">p.1  Expires date has wrong year!\n\nyep, I caught that one before submitting to IETF (I just edited the PS\nversion, so no need to get a new copy).\n\n>p.8  under implied *LWS:  (tspecials) should be Helvetica font\n\nyep. I do hope that the font changes help in reading the text, since they\nsure are a pain to keep consistent.\n\n>p.17, Sect. 3.8:\n>... all tags are not case-sensitive ... ->\n>... all tags are case-insensitive ...\n\nI'm not sure which is \"better\" English for a spec.  MIME uses both.\n\n>p.31 400 Bad Request\n>... due to it having ... ->\n>... due to its having ...\n\nwow, you must be reading it with a magnifying glass. ;-)\nI think I'll just delete \"it having a\".\n\n>p.42 Sect. 8.12\n>Shouldn't the date format be rfc1123-date, not HTTP-date (which\n>would allow the deprecated asctime-date).\n\nNo.  RFC 1123 accepts a much larger set of formats -- the HTTP one is\nsimply a fixed and unambiguous format that is acceptable by RFC 1123.\n\n>p.55 Sect. 11.2\n>... over the net ... ->\n>... over the network ...\n\n... over the Internet ...\n\n>p.55 Sect. 11.4\n>... nor is there any apriori method ...\n>... nor is there any a priori method ...\n>     -------- ital\n\nyep.\n\n>p.61 C.1.2\n>Also need to recalculate Content-Length if line breaks change.\n\nyep.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Negotiation in draft 0",
            "content": "I have some concerns with how negotiation works in draft 01 as compared to\ndraft 00.  Before I list those, when were all of these changes discussed on\nthe mailing list?  I looked through all of the archives and couldn't find\nany discussion of this.  (Nor could I find the minutes from the Danvers\nIETF, but even if it was discussed there, it still should have been on the\nmailing list.)\n\nAnyway, here are my concerns:\n\n1.  The effect of a request not having an Accept-Encoding or Accept-Charset\nhas flipped completely from 00 to 01.  This seems to make the doubtful\nassumption that current implementations of clients which don't produce these\nheaders can accept any encoding or character set.\n\n2.  All of the Accept-* headers were defined in 00 as requiring at least one\nitem.  In 01 they can have 0.  While RFC822 allows this, under 00 every\nHTTP-header had a field-body.  I wouldn't be surprised if some header\nparsers choke on that.  Requiring empty \"Accept-Encoding:\" and\n\"Accept-Charset:\" strings to describe the most common case for a client\ncould break many existing servers.\n\n3.  The spec allows you to specify \"Accept: \" but doesn't say what the\neffect is.  My reading seems to indicate that this means no MIME type is\nacceptable, \nbut it's somewhat ambiguous.\n\n4.  The Accept example in 8.1 shows a \"text/html;version=2.0\" and a\n\"text/html;level=3\".  Section 3.4 does not specify a list of parameters.\nWas the use of both \"version\" and \"level\" a deliberate attempt to show that\nany parameter name is valid?  It's a little scary to me that someone could\ncreate an arbitrary parameter name and expect the server to parse it.\n(Also, in this case we need to specify a list of reserved attributes, like\nq, ql, mxb, etc.)  We also need to specify what \"more specific\" \n\n5.  If qe and qc default to 0.001 instead of 0, do we provide a client with\nany way at all to say that it doesn't want encodings and character sets it\ncan't handle?  If a web-searching robot, for example, says that it can't\nhandle compressed files then it probably really means that, and would\nprobably prefer a \"406 none acceptable\" to something it can't receive.  Yet\nunder 01 there's no way for the robot to avoid having such content sent to it.\n\n6.  The URI description in 8.28 still doesn't address the issue I brought up\nback in June.  Namely, how would a cache practically use the information\npresented in the URI as described?  Since the URI field doesn't have to\nenumerate all of the variants that are available, it doesn't help to know\nwhat varies unless the next request has an identical entity header for that\ndimension.\n\nI bring this up because the 8.28 says \"When the caching proxy gets a request\nfor that URI, it must forward the request toward the origin server if the\nrequest profile includes a variant dimension that has not already been\ncached.\"  In practice, as currently speced, the request must be forwarded\nunless a requset with an identical profile has been made.\n\nAs an example, suppose that someone sends this request to a proxy server:\n\nGET http://www.bar.com/foo HTTP/1.0\nAccept: image/gif;q=0.5, image/jpeg\n\nThe proxy server sends it along to www.bar.com, which returns\n\n200 OK\nContent-type: image/jpeg\nURI: <foo>; vary=\"type\"\n[etc.]\n\nIf someone then comes along and requests of the proxy server:\n\nGET http://www.bar.com/foo HTTP/1.0\nAccept: image/gif;q=0.8, image/jpeg\n\nThe proxy server needs to send this along to www.bar.com as well, since it\ndoesn't know whether there's a gif version with a higher qs than the jpeg\nversion which might be optimal for this request.  Similarly, if someone\nrequested:\n\nGET http://www.bar.com/foo HTTP/1.0\nAccept: image/jpeg, image/xbm\n\nthe proxy won't know if an xbm version is available.\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "Re: http-v10-spec01.ps nit",
            "content": "Roy Fielding <fielding@beach.w3.org> wrote:\n  > >[Dave Kristol <dmk@allegra.att.com> wrote:]\n  > >p.42 Sect. 8.12\n  > >Shouldn't the date format be rfc1123-date, not HTTP-date (which\n  > >would allow the deprecated asctime-date).\n  > \n  > No.  RFC 1123 accepts a much larger set of formats -- the HTTP one is\n  > simply a fixed and unambiguous format that is acceptable by RFC 1123.\n\nI'm confused.  I wasn't talking about RFC 1123, but about the token\nrfc1123-date (p.13), which defines a specific date format.  The token\nHTTP-date allows one of three different date formats, one of which is\nrfc1123-date.  I assumed you wanted to specify just one of those formats.\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "Bob Wyman <bobwyman@medio.com>:\n  > Dave Kristol wrote: \n  > > Actually, I had \"clickstreams\" in mind less than \"shopping carts\" and \n  > > library system navigation.\n  > \n  > If this is the case, I would suggest that you could do reviewers a service\n  > by stating this in the introduction to your draft. I would also suggest that\nFair enough.\n  > it may be a disservice to introduce a protocol modification that has such\n  > limited utility. This is particularly true since we already have outstanding\n  > the Netscape \"cookie\" proposal which appears to be able to support\n  > \"clickstreams\" as well as a range of other requirements for session state.\nWhile documentation for Netscape's cookies is available on their site, I\nhadn't noticed they had put it forward as an Internet Draft.  Have I\nmissed it?\n\nFWIW, I think my proposal supports \"clickstreams\" too.  (Did you, by\nchance, mis-read the first remarks of mine you quoted above?)  But that\nwasn't the reason I proposed it.\n  > If your limited proposal is accepted into HTTP, it seems inevitable that\n  > someone else will argue for and probably succeed in getting a second session\n  > state mechanism adopted that handles the other requirements. The result will\n  > be, of course, more crust and rust introduced into this young, but rapidly\n  > tiring, protocol.\nWhat other requirements, specifically?  Lots of people have put forward\nproposals for stateful sessions, but few have stated the requirements\nthey are trying to satisfy.  (Including me, I guess.)  In any case,\nstuff evolves.  No doubt Netscape's cookies won't satisfy all\nrequirements forever either.\n  > \n  > >The point of the proposal is that all interesting state should be\n  > > embedded in the opaque session information.\n  > \n  > It is clear from your comments that you view the opaque data carried in the\n  > \"Session-ID\" to be something other than and more than a simple identifier\n  > for a session. I must admit, however, that I'm confused since if you concern\n  > is really just \"clickstreams\" then you should be satisfied with a simple\n  > identifier... Anyway, I would suggest that you call the thing something\n  > other than an \"ID.\" You should seriously consider following Behlendorf's\n  > lead and calling it \"State:\" or \"Session-State:\" Perhaps the non-native-\n  > english-speakers will object to my desire to have protocol tags be faithful\n  > to the English language... Nonetheless, I think that as long as we're not\n  > using hex numbers to identify these things, we should view such faithfulness\n  > as a means of clarifying the intent of the author.\nPerhaps \"Session-ID\" is a poor choice.  Let me leave it for now.  We\ncan argue about the right name later if the functionality is right.\n(\"Session-State\" sounds good.)\n\nYes, I had in mind more than a simple identifier, and I'm interested in\nmore than just \"clickstreams\".  But it could be a simple identifier.\nI deliberately left the semantics unstated, rather than over-specify.\n  > \n  > >A CGI can examine the incoming  Session-ID, if any, and can \n  > > generate a new one or repeat an old one.  Clearly the content\n  > > of the opaque information is application-dependent.\n  > \n  > If the opaque information is really application-dependent then there is a\n  > small problem here... You state in your proposal that the \"Session-ID\" must\n  > be returned in the next request following the response in which it is\n  > received. Thus, I may get a Session-ID from CGI script \"A\" and if the next\n  > URL I request is CGI script \"B\", then I am required to forward the Session-\n  > ID I received from \"A\" to \"B\". (I won't reflect on privacy concerns at this\n  > point...) The problem here is that \"B\" won't have any idea if it understands\n  > the opaque data received since it has no way of knowing who authored it.\n  > Even if I had sent the Session-ID back to \"A\", script \"A\" would have no way\n  > of knowing how to interpret the opaque data. Inevitably, we'll see people\n  > embedding \"application ids\" into this otherwise opaque data and then\n  > building informal consensus on how to inspect the beginning of a session id\n  > to determine if it is \"yours.\" The opaque data will start to be less opaque.\n  > This will get really ugly when people start declaring rules like: \"If you\n  > don't understand the session data, push it back in the response inside a\n  > wrapper defined in RFCXXXX.\" The result will be rapid accretion of odd bits\n  > of session data and rapidly growing packets...\nBy \"opaque\" I meant more that only the server should be paying attention\nto what's contained therein than that it couldn't be understood by someone\nwho happened to see the header.  But the server could, for example, encrypt\nthe content with a server-private key that it applied to all of its\nSession-ID information.\n\nAs for CGI's A and B, you are correct in the abstract, but less so in\nthe specific.  I'm assuming the usual case of someone navigating\nthrough a tree of pages by following hypertext links therein.  The\ncontent is structured in a way that A and B are likely to be part of\none application (say, the LL Bean shopping cart).  So the Session-ID\ninformation will be pertinent to both.\n  > \n  > Note: The Netscape \"cookie\" proposal has much the same problem, however, the\n  > impact is somewhat limited by their inclusion of the \"path\" attribute on the\n  > cookie. The client is able to be somewhat specific about who it sends the\n  > session data to. This reduces the problem somewhat, but not completely.\nAgreed.  But since the semantics of the Session-ID are open-ended, a\nserver could encode path information therein.\n  > \n  > > Other proposals similar to this one have run into caching problems\n  > > because the  state is somehow embedded in the document or URL.\n  > >  I wanted to avoid that.  I'm  assuming that the resource content is\n  > > the same, independent of Session-ID ... Given this treatment of \n  > > Session-ID,  it's expected and desirable for a caching  proxy to serve\n  > > out the same content when possible.\n  > \n  > You imply here that a Session-ID offers no additional semantic richness to a\n  > client-server exchange. Thus, a session using Session-IDs and one without\n  > would result in identical content exchanges other than Session-ID. If this\n  > is the case, it would appear that there is no apparent benefit to a client\n  > from hassling with Session-ID's. The only benefit from these things is to\n  > allow server administrators to characterize their load and marketing or law\n  > enforcement folk to study our behaviour. If Session-ID is to have some\n  > utility to the client, then there must be a mechanism specified for a\n  > caching proxy to obtain correct and appropriate Session-ID's for each\n  > request -- even if the response body is cached.\nSuppose I'm browsing a catalog and have accumulated a \"shopping cart\"\nembodied in a Session-ID.  I reach a page that describes a Full-Duplex\nFramas.  Perhaps there's a link/button/icon that would let me examine\nmy current shopping cart and tell me my current balance.  An \"add to\nshopping cart\" link/button/icon would select the Framas for purchase.\nThe Framas description (URL), however, has no session-specific\ninformation and can be cached.  Any user would see the same stuff for\nthat URL.  The action occurs when I select a link that acts on my\nshopping cart.\n\nNow admittedly there's another possible implementation that, for example,\nalways tells me my current shopping cart balance.  That would require a\nCGI (probably) to display the product description AND give me my balance,\nand the resulting page would not be cacheable.\n  > \n  > Anyway... the basic point here is that I think that clickstreams aren't\n  > important enough to address in isolation of the other requirements for\n  > server state. It appears that a solution to the larger problems will\n  > incorporate a solution for clickstreams as a subset. But, it appears that\n  > the larger problems cannot be solved as a simple super-set of clickstream\n  > support. Thus, we should probably address the harder problems first...\nI agree about clickstreams, and reiterate that my proposal was more\nthan just clickstreams.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Excerpts from ext.ietf-http: 31-Jul-95 Draft Minutes of HTTP Worki.. Jim\nGettys@w3.org (4887*)\n\n> Who has implemented a non-blocking (streaming) RPC system that can\n> be used if we are to avoid rolling our own?\n\nI've sent mail to Jim on this separately, but I wanted to point out that\nILU, which is freely available in source form for both commercial and\nnon-commercial use, includes an implementation of a non-blocking RPC\nsystem.  ILU provides support for both synchronous (blocking) or\nasynchronous (non-blocking) calls.  Since ILU is protocol-independent,\nmany different on-the-wire protocols can be used with it.  Two different\non-the-wire protocol implementations are provided with the ILU sources,\nthe ONC RPC wire protocol and the XNS Courier wire protocol.  It would\nbe easy to implement another wire protocol for ILU embodying HTTP, but\nit seems more useful to use a more efficient binary protocol.  All wire\nprotocols used with ILU share the semantics and infrastructure provided\nby ILU, including its careful management of resources such as file\ndescriptors, connections and buffers.  User-level code is never aware of\nthe specific wire protocol being used.\n\nMore ILU info is at ftp://ftp.parc.xerox.com/pub/ilu/ilu.html.\n\nBill\n\n\n\n"
        },
        {
            "subject": "HTTP/1.0 Review Pla",
            "content": "Hi all,\n\nBy now, most of you should have downloaded the new draft 01 of the\nHTTP/1.0 spec.  If not, it is available at\n\n     http://www.ics.uci.edu/pub/ietf/http/\n      ftp://www.ics.uci.edu/pub/ietf/http/\nand\n     http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n \nor from any of the Internet-Draft archives.\n\nOur plan is to absorb as many comments as possible by August 18\nand then produce a \"final\" draft by August 21 (final in the sense\nthat we believe it to be complete and reflects WG consensus on all\nissues).  After another week of general comments, we would like to\nsubmit it to the IESG as a Draft Standard (not Proposed, as said the\nminutes from Stockholm) if there exists a rough consensus to do so.\n\nIn the mean time (between now and the 18th), I will be busy finishing\nthe HTTP/1.1 draft.  I would like to avoid getting too involved in\nthe debate over portions of the 1.0 draft, except where it becomes\nnecessary to describe the thinking behind some of the recent changes.\nIn particular, I would prefer it if people would attempt to understand\nthe draft without any recourse to the WG minutes, mailing list archives,\nor the authors, since that is the intended goal of the specification.\nBy understand, I mean understand how you would implement each and\nevery feature described in the specification.  If you can't understand\na section, please make a comment to that affect and hopefully the WG\ncan propose specific wording that will fix the problem.\n\nHTTP/1.0  Issues\n================\n\nThere are a few outstanding issues that I am already aware of after\nthe weekend:\n\n1) Content-Transfer-Encoding\n\n   After discussion with the various camps, it appears as if the best\n   thing to do is to simply forbid this header ever appearing in HTTP,\n   and instead make it a requirement of gateways to do addition/removal\n   of CTE when required by MIME.\n\n   For HTTP/1.1, we can define a Transfer-Encoding header which has the\n   analogous purpose, but without the MIME lossage.\n\n2) Caching\n\n   As far as I'm concerned, the 1.0 spec will not be complete until\n   HTTP can be unambiguously cached.  In fact, this is now possible\n   given the additions of \"Pragma: max-age=NN\" and a complete syntax\n   for the URI header.  However, it seems clear that I will have to\n   explain the algorithm for doing so as an appendix to the spec.\n   It is also clear that we must require that servers mark content\n   as being negotiated when that is so.\n\n   The only alternative to the above is the complete removal and\n   prohibition of any preemptive content negotiation.  Not supporting\n   caching is not an alternative.\n\n3) WWW-Authenticate\n\n   The new spec now uses semicolon to separate parameters -- keeping\n   it as comma-separated would prevent people from using more than\n   one AA scheme per resource.  This will break existing implementations\n   of Digest and PGP AA.  One alternative is to leave WWW-Authenticate\n   as a fixed field (i.e., only describe it for Basic), and define a\n   new syntax for an Authenticate header.\n\n   The same applies to Authorization.\n\n4) Comments in Headers\n\n   I personally believe it is easier to define and parse a generic\n   syntax for all headers than it is to have a separate syntax for\n   each defined header.  Therefore, I have included the comments and\n   \\escape mechanisms of RFC 822 in the generic syntax.\n\n   However, if the WG desires it, I can add the requirement that\n   they only be allowed in those header fields which define \"comment\"\n   as a possible field value, and then add that to User-Agent, Server,\n   From, and Forwarded. \n\nAny others?\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: http-v10-spec01.ps nit",
            "content": ">I'm confused.  I wasn't talking about RFC 1123, but about the token\n>rfc1123-date (p.13), which defines a specific date format.  The token\n>HTTP-date allows one of three different date formats, one of which is\n>rfc1123-date.  I assumed you wanted to specify just one of those formats.\n\nAh, sorry, I misunderstood.  The answer is still no, since all three\ndates are still valid in HTTP/1.0 messages.  The requirement that one\nnot be generated is separate from the field syntax.\n\nI also forgot to include that all three formats are only required to\nbe understood by those applications which parse the contents of the\ndate field.\n\n.......Roy\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": ">I have some concerns with how negotiation works in draft 01 as compared to\n>draft 00.  Before I list those, when were all of these changes discussed on\n>the mailing list?\n\nThey were not.  I very rarely design things on the mailing list, preferring\ninstead to work with small groups on particular problems and then presenting\nthe result in the form of a draft.  We spent a great deal of time trying\nto find the best way to cache responses given the possible presence of\ncontent variants.  Not all of the conclusions made it into the draft, but\nthe parsing requirements did.\n\n> I looked through all of the archives and couldn't find\n>any discussion of this.  (Nor could I find the minutes from the Danvers\n>IETF, but even if it was discussed there, it still should have been on the\n>mailing list.)\n\nIt is on the mailing list now.  Discussing half-formed ideas and\nincomplete proposals is a waste of time.  Now that it is written down,\nwe can all determine its desirability or lack of it from the same\nperspective.\n\n>Anyway, here are my concerns:\n>\n>1.  The effect of a request not having an Accept-Encoding or Accept-Charset\n>has flipped completely from 00 to 01.  This seems to make the doubtful\n>assumption that current implementations of clients which don't produce these\n>headers can accept any encoding or character set.\n\nThat was decided just before Danvers, was discussed at the meeting, and\nshould be in the minutes (they are now pointed to by my WG page, though\nI never received a copy either and they contain many errors because they\nwere not posted to the list for review prior to being submitted).\n\nAll current browsers do accept all formats, in the form of the Save As..\ndialog.  The WG decided that it was better to assume the best than to\nrequire all clients to be compliant before the feature could be used.\n\n>2.  All of the Accept-* headers were defined in 00 as requiring at least one\n>item.  In 01 they can have 0.  While RFC822 allows this, under 00 every\n>HTTP-header had a field-body.  I wouldn't be surprised if some header\n>parsers choke on that.  Requiring empty \"Accept-Encoding:\" and\n>\"Accept-Charset:\" strings to describe the most common case for a client\n>could break many existing servers.\n\nAny server that breaks on an empty field value is already broken,\nregardless of what the spec requires.  I can restore the 1#( if that\nmakes people feel better, but it won't make any difference to robust\nimplementations.  Besides, how is the client supposed to say that they\ndo know about Accept-Encoding, but don't accept any?\n\n>3.  The spec allows you to specify \"Accept: \" but doesn't say what the\n>effect is.  My reading seems to indicate that this means no MIME type is\n>acceptable, \n>but it's somewhat ambiguous.\n\nYes.  What should the definition be?\n\n>4.  The Accept example in 8.1 shows a \"text/html;version=2.0\" and a\n>\"text/html;level=3\".  Section 3.4 does not specify a list of parameters.\n\nIt does:\n\n      media-type     = type \"/\" subtype *( \";\" parameter )\n\n>Was the use of both \"version\" and \"level\" a deliberate attempt to show that\n>any parameter name is valid?  It's a little scary to me that someone could\n>create an arbitrary parameter name and expect the server to parse it.\n\nAny parameter name *must* be parseable by the server.  Understanding\nits value is a separate matter and is dependent on the handler for\nthat media type.\n\n>(Also, in this case we need to specify a list of reserved attributes, like\n>q, ql, mxb, etc.)  We also need to specify what \"more specific\" \n\nWhere?  I believe Section 9 is an adequate definition for the latter.\n\n>5.  If qe and qc default to 0.001 instead of 0, do we provide a client with\n>any way at all to say that it doesn't want encodings and character sets it\n>can't handle?  If a web-searching robot, for example, says that it can't\n>handle compressed files then it probably really means that, and would\n>probably prefer a \"406 none acceptable\" to something it can't receive.  Yet\n>under 01 there's no way for the robot to avoid having such content sent to it.\n\nYes, that is true.  Unfortunately, current clients do not send an\nAccept-Charset header even if they do support multiple charsets.\nI was overruled once on this matter, and the WG will have to state clearly\nwhat they want if I am to change it back again.\n\n>6.  The URI description in 8.28 still doesn't address the issue I brought up\n>back in June.  Namely, how would a cache practically use the information\n>presented in the URI as described?  Since the URI field doesn't have to\n>enumerate all of the variants that are available, it doesn't help to know\n>what varies unless the next request has an identical entity header for that\n>dimension.\n\nYes, see my other message.\n\n>I bring this up because the 8.28 says \"When the caching proxy gets a request\n>for that URI, it must forward the request toward the origin server if the\n>request profile includes a variant dimension that has not already been\n>cached.\"  In practice, as currently speced, the request must be forwarded\n>unless a requset with an identical profile has been made.\n\nI'll need to fix that.  The request only needs to be forwarded if no\ncharacteristics are provided.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "And, of course, DCE has non-blocking RPC (via use of threads).  It has\nonly one wire protocol, but that supporst heterogenous secure RPC.\nThe core RPC source is freely available.  We've done lots of\nexploration of merging DCE and the We and will be continuing development.\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": ">They were not.  I very rarely design things on the mailing list, preferring\n>instead to work with small groups on particular problems and then presenting\n>the result in the form of a draft.  We spent a great deal of time trying\n>to find the best way to cache responses given the possible presence of\n>content variants.  Not all of the conclusions made it into the draft, but\n>the parsing requirements did.\n\nThis attitude is offensive.  We are not the huddled tribes waiting for\nstone tablets to come down from above.  The members of this mailing list\nare those people who expect to be involved in the definition of the HTTP\nprotocol.  At least now I understand why a 500-line detailed commentary on\nv10-spec that I sent to this list at the end of May got no reaction.\n\nThis is not the way the IETF works.  At the very least, an apology is\ncalled for.  More likely, a major procedure reset probably needs to be done\nby the Area Director (e.g., replace the current editor).\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Non-blocking RPC via the use of threads is not quite what we were\ntalking about, I think -- doesn't every RPC system provide non-blocking\nRPC, if you specify the use of threads?  But I could be wrong.  The\nnotion from the X Window System protocol that ILU preserves is that of\nstreams of messages that can be sent, in blocks if necessary, without\nrequiring replies.  The other side of the connection can respond to\nthese messages with other messages of its own.\n\nA problem with DCE RPC is the complexity of the messages and\nmarshalling.  While these complexities may in fact be useful for\nimproving performance, I'm still skeptical if they do so sufficiently to\npay for the added costs of implementation and testing.  The size of the\nspec makes it more difficult to produce implementations, which would be,\nI think, a debit for use with the Web.  It has certainly kept us from\nsimply adding the DCE RPC wire protocol to the ILU suite.  On the other\nhand, the semantics of DCE RPC are nicely defined, in sharp contrast to\nONC RPC.\n\nAnother problem with using DCE RPC, if my following of comp.soft-sys.dce\nhas been correct, is that while the core of the RPC system is freely\navailable, other parts, like the threads package necessary to implement\nyour version of non-blocking RPC, aren't, so people are having a hard\ntime getting the free DCE RPC code running.  Similarly, the parts\nnecessary for the secure part of your \"...secure RPC...'' are not freely\navailable.  Perhaps this has changed lately, and I missed it?\n\nBill\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Bill's comments on DCE are correct enough that picking nits isn't worthwhile.\nThanks.  (Wish I could pay as close attention to his work. :)\n\nOn the other hand, it will be bundled with the next release of NT, and\nseveral not-as-ubiquitous operating systems. :)\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": "In message <9508090308.AA04559@sulphur.osf.org>, Rich Salz writes:\nin reply to Roy Fielding:\n\n> >They were not.  I very rarely design things on the mailing list, preferring\n> >instead to work with small groups on particular problems and then presenting\n> >the result in the form of a draft.  We spent a great deal of time trying\n> >to find the best way to cache responses given the possible presence of\n> >content variants.  Not all of the conclusions made it into the draft, but\n> >the parsing requirements did.\n> \n> This attitude is offensive.  We are not the huddled tribes waiting for\n> stone tablets to come down from above.  The members of this mailing list\n> are those people who expect to be involved in the definition of the HTTP\n> protocol.\n\nAnd you are. There's a draft, if you want something changed, get it changed\nby discussing it in the group.\n\n> At least now I understand why a 500-line detailed commentary on\n> v10-spec that I sent to this list at the end of May got no reaction.\n\nI don't, and it's a separate issue.\n\n> This is not the way the IETF works.  At the very least, an apology is\n> called for.  More likely, a major procedure reset probably needs to be done\n> by the Area Director (e.g., replace the current editor).\n> /r$\n\nI don't think an apology is called for, neither is your aggresive reaction.\n\nPersonally I'm glad Roy has provided a solid basis to discuss, rather than\na lot of talking happening without result (that's what www-talk is for,\nright? Joke! Joke, sorry... :-)\n\nI too would like to (have) see(n) a faster turnaround time on the draft,\nbut that's always easy when you're not the one doing the work :-)\n\nEnough soapbox as far as I'm concerned, now where did I put that draft...\n\n-- Martijn\n__________\nInternet: m.koster@nexor.co.uk\nX-400: C=GB; A= ; P=Nexor; O=Nexor; S=koster; I=M\nWWW: http://web.nexor.co.uk/mak/mak.html\n\n\n\n"
        },
        {
            "subject": "Server capabilities investigatio",
            "content": "What is the best way for a http client to find out which\nprotocols a certain server is able to understand.\n\nA server may be able to speak several protocols as\nhttp/0.9, http/1.0 , shttp/1.1, crypto-http/33.3, \nsuper-http/0.35, extra-http/3.9 and other phantastic protocols.\n\nShouldn't there be a protocol independent command to retrieve a list of\nthe protocols the server is able to understand?\n\nHadmut\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": ">All current browsers do accept all formats, in the form of the Save As..\n>dialog.  The WG decided that it was better to assume the best than to\n>require all clients to be compliant before the feature could be used.\n\nBecause I have Save As and an old browser I like sanskrit as much as I like\nus-ascii?  I can see the point of a change from 0 encoding and/or charset\nquality (on lack of the header) to a positive encoding and/or charset\nquality, but all the way to 1?  .001 seems infinitely more appropriate.\n\n>Any server that breaks on an empty field value is already broken,\n>regardless of what the spec requires.  I can restore the 1#( if that\n\nI'm sure our server is immune, but I can sympathize with the idea that\nothers might break given this previously unexistant construct (valid headers\nthat have no body) being introduced.\n\n>implementations.  Besides, how is the client supposed to say that they\n>do know about Accept-Encoding, but don't accept any?\n\nI'm beginning to wonder if a construct like */* and q aren't needed for all\nthe Accept-Foo: headers. As in \"Accept-Encoding: *;q=0.0\".  Of course, then\nyou'll have to explicitly outlaw \"Accept-Charset: iso-8859-1;q=0.0\" or\nsimilar special cases.  If we decide we really want to handle every special\ncase, we should also give someone the ability to indicate that they *only*\ntake zipped files or sanskrit files.  More work for me, but I'd rather have\na sped'c way of doing everything for completeness' sake.  Anything less\nwould be uncivilized.\n\n>>5.  If qe and qc default to 0.001 instead of 0, do we provide a client with\n>>[...]\n>>under 01 there's no way for the robot to avoid having such content sent to it.\n\n>Yes, that is true.  Unfortunately, current clients do not send an\n>Accept-Charset header even if they do support multiple charsets.\n>I was overruled once on this matter, and the WG will have to state clearly\n>what they want if I am to change it back again.\n\nNot having any way to indicate \"effective quality zero\" on charsets and\nencodings seems like a soundly bad idea.  Why give the user-agent the abilty\nto refuse text/plain, but not refuse unicode or ARCed files? (remember ARC,\ngood luck finding a deARCer nowadays).\n\nI can't imagine anyone being satisfied with the current content negotiation\nscheme.\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": "At 10:26 PM 8/8/95 -0400, Roy Fielding wrote:\n>I very rarely design things on the mailing list, preferring\n>instead to work with small groups on particular problems and then presenting\n>the result in the form of a draft.\n\nI have to disagree with the current approach if there are going to be 5\nmonth periods between drafts.  Given that many people are already deploying\nHTTP applications, and have only the I-Ds to go by, having major changes\nlike this appear without warning is, IMHO, unacceptible.\n\nI don't have a problem with the idea of having a small group come up with a\nproposal for discussion.  But couldn't those proposals be discussed on the\nlist as they are formed, rather than waiting months for an I-D?\n\n>>1.  The effect of a request not having an Accept-Encoding or Accept-Charset\n>>has flipped completely from 00 to 01.  This seems to make the doubtful\n>>assumption that current implementations of clients which don't produce these\n>>headers can accept any encoding or character set.\n>\n>That was decided just before Danvers, was discussed at the meeting, and\n>should be in the minutes (they are now pointed to by my WG page, though\n>I never received a copy either and they contain many errors because they\n>were not posted to the list for review prior to being submitted).\n\nI just read the Danvers minutes, and I didn't see this discussed.  When you\nsay \"decided just before Danvers\" do you mean it was discussed here?  If so,\ncould you point me to it in the archives so I can get up to speed on the\ndiscussion that occured at the time?\n\n>All current browsers do accept all formats, in the form of the Save As..\n>dialog.  The WG decided that it was better to assume the best than to\n>require all clients to be compliant before the feature could be used.\n\nThis isn't true.  There are several browsers which will break if, for\nexample, they get an encoded file (because they don't parse\nContent-Encoding).  By this logic, we should also have q default to a\nnon-zero value for those browsers which don't include */* in their Accept\nfield (of which there are some).\n\nAlso, as I pointed out in item 5 of my last note, not all HTTP clients are\nbrowsers.  An automated WWW searching tool (for example) won't have a Save\nAs option.  A kiosk-based browser likely wouldn't have a Save As option.\nThey would just break under this change.  Also, under the new draft, there's\nabsolutely no way for these types of applications to say that they won't\naccept certain encodings and charsets, since even including the headers they\ncan only get qe and qc down to 0.001.\n\n>>3.  The spec allows you to specify \"Accept: \" but doesn't say what the\n>>effect is.  My reading seems to indicate that this means no MIME type is\n>>acceptable, \n>>but it's somewhat ambiguous.\n>\n>Yes.  What should the definition be?\n\nHaving it mean no MIME type is acceptible might be useful as a way to force\na 406 response.  That could provide a consistent way for a client to get a\nlist of varients for a URI.  If we decide this is desirable, it should be\nmade explicit.\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 Review Pla",
            "content": ">I would like to avoid getting too involved in\n>the debate over portions of the 1.0 draft, except where it becomes\n>necessary to describe the thinking behind some of the recent changes.\n\nGiven the number of surprise changes and objectionable ones at that, I\nbelieve this is unrealistic.\n\n>3) WWW-Authenticate\n>\n>   The new spec now uses semicolon to separate parameters -- keeping\n>   it as comma-separated would prevent people from using more than\n>   one AA scheme per resource.  This will break existing implementations\n>   of Digest and PGP AA.  One alternative is to leave WWW-Authenticate\n>   as a fixed field (i.e., only describe it for Basic), and define a\n>   new syntax for an Authenticate header.\n>\n>   The same applies to Authorization.\n\nLet's go for the alternative.  Breaking all existing implementations of\nsomething like this seems unnecessary.  If you *must* go for semicolons,\ndefine a new header.\n\n--\nEric W. Sink\nSenior Software Engineer, Spyglass\neric@spyglass.com\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 InternetDraft 01 Availabl",
            "content": "> \n> \n> A Revised Internet-Draft is available from the on-line Internet-Drafts \n> directories. This draft is a work item of the HyperText Transfer Protocol \n> Working Group of the IETF.  \n\nYeah, I noticed in the course I was giving today on CGI, when the URL \nprinted in the exercises (and checked then)  did not work ;-)       \n\nIs there any particular reason the HTML version has gone from a nicely \nchunked set of pages to one massive document? Pulling over several hundred K \nof html just becuase I forgot what a 406 response code was seems less than \ndesirable.\n\nCould I ask for a chunked version (again) please?\n\n-- \nChris Lilley, Technical Author\n+-------------------------------------------------------------------+\n|       Manchester and North HPC Training & Education Centre        |\n+-------------------------------------------------------------------+\n| Computer Graphics Unit,             Email: Chris.Lilley@mcc.ac.uk |\n| Manchester Computing Centre,        Voice: +44 161 275 6045       |\n| Oxford Road, Manchester, UK.          Fax: +44 161 275 6040       |\n| M13 9PL                            BioMOO: ChrisL                 |\n|     URI: http://info.mcc.ac.uk/CGU/staff/lilley/lilley.html       | \n+-------------------------------------------------------------------+\n\n\n\n"
        },
        {
            "subject": "more http-v10-spec0",
            "content": "Maybe I'm incredibly dense (I see those heads nodding :-), but I found\nSect. 8.28 a bit confusing.  What would have helped me is for there to\nbe an explicit statement that particular vary-dimension's correspond\nexactly to specific HTTP headers, e.g.,\ntypeAccept\ncharsetAccept-Charset\nlanguageAccept-Language\nencodingAccept-Encoding\nuser-agentUser-Agent\nversion??\nThe examples imply the correspondence but there's no explicit statement\nof correlation.\n\nWhat, exactly, is \"version\"?  Where is the version specified in the URI?\n\nHow does a caching intermediary handle a vary-dimension of <token>?\nThat is, if you can't specify the dimensions in advance, how can the\nintermediary figure out what the semantics for such an unspecified\ndimension are?  (If <token> is just meant as a place-holder for future\nexpansion, let's say so.)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Complaints regarding Draft Status and Autho",
            "content": ">This attitude is offensive.  We are not the huddled tribes waiting for\n>stone tablets to come down from above.  The members of this mailing list\n>are those people who expect to be involved in the definition of the HTTP\n>protocol.  At least now I understand why a 500-line detailed commentary on\n>v10-spec that I sent to this list at the end of May got no reaction.\n\nI am sorry that you feel that way, but you are incorrect on all counts.\n\nFirst, I have never refused to make a change to any of the drafts I have\nedited once there became rough consensus to do so.  I have the highest\nregard for the members of this mailing list, and the people whom I *do*\nask about specific design changes are all members of this list.  The reason\nI ask them first and not the list-at-large is because THEY HAVE ALREADY\nIMPLEMENTED THE ITEM BEING DISCUSSED or because I can talk to them\nface-to-face at an IETF meeting, at UCI, at MIT, or at any one of a\ndozen other meetings/conferences I attended over the past four months.\n\nSecond, being \"involved in the definition of the HTTP protocol\" means\nexactly what you are doing now.  Read the draft.  Send in comments.\nThe WG consensus will be polled and, if desired, the changes will be\nmade to the draft.  That is why it is called a DRAFT.  If you want to\npropose the exact wording of the item in question, than do so.  If not,\nthen I will write what I believe to be a suitable draft based on those\ncomments, and then invite additional input (just as I am doing now).\n\nThird, not all comments I receive on the spec are sent via the WG mailing\nlist.  For every one public commentary, I receive three or four private\nmessages.  I must take all into consideration, though all final decisions\nare made by the WG.\n\nFourth, the reason your commentary received no response was because\n\n   1) It was 500 lines long, making it difficult to \"discuss\"\n\n   2) Most of the comments were accurate and agreeable; these changes\n      were all made to the draft [BTW, it took me 16 hours of editing\n      to do so, because most of your suggestions were wholesale changes\n      to the structure of the draft and style issues like replace all\n      parentheses with commas, use must instead of required to, etc.].\n      Did you check the draft for your suggested changes?\n\n   3) It was mailed while I was on vacation, and thus I was not available\n      to respond to it when it was fresh.  Sorry, but I now get 4-6 hours\n      worth of mail to read and respond to *every day* -- I cannot respond\n      to everything that comes my way, particularly not when I agree with\n      what someone else has already said.\n\n>This is not the way the IETF works.  At the very least, an apology is\n>called for.  More likely, a major procedure reset probably needs to be done\n>by the Area Director (e.g., replace the current editor).\n\nI am sorry that my statement has been misunderstood as being a pronouncement\nfrom behind some exalted status.  Please keep in mind that this is E-MAIL\nand thus is a lousy medium for communicating intentions.\n\nWe are proceeding exactly as \"the IETF works\" with the singular exception\nthat I was unable to edit the draft for two months because I was\ndisconnected from my authoring environment.  Yes, that's bad, but there\nwasn't a damn thing I could do about it (I *tried*, honestly I did, but\nits difficult to edit a document when you are on a plane, driving a car,\nlooking for an apartment, and trying to deal with a computer that crashes\nevery 30 minutes).\n\nFinally, note that I am a volunteer who has invested 10 months of my\n*free* time to edit the specification.  To do so, I have delayed\nmy Ph.D. progress by one year and have not released a new version of my\nthree publically available software products in a year.  Aside from the\nfact that I'm listed as an author, I did not get paid for this work\nuntil I arrived at W3C, and even now it is only one of my duties.\n\nEven so, I consider both RFC 1808 and the HTTP/1.0 draft to be among\nthe best written specifications produced by the IETF, as is clear\nfrom the comments I have received outside the mailing list.  I invite\nyou to find a way in which it can be improved.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\np.s. I just spent 2 hours composing this response.  On most days, that would\n     be the sum total of my available free time.  Fortunately, I have\n     arranged to be in a situation where I can spend my full day in what\n     I would normally consider \"free time\".  Please don't waste what has\n     caused me a great deal of pain, expense, and time to arrange.\n\n\n\n"
        },
        {
            "subject": "SessionID proposal updat",
            "content": "In response to yesterday's discussion, I have made changes to my\nSession-ID proposal.  (I didn't change the name of the proposed header\nto \"Session-State\" [or just \"State\"] yet.) Updated versions are at (the\nsame place):\n\nhttp://www.research.att.com/~dmk/session.html\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": ">At 10:26 PM 8/8/95 -0400, Roy Fielding wrote:\n>>I very rarely design things on the mailing list, preferring\n>>instead to work with small groups on particular problems and then presenting\n>>the result in the form of a draft.\n>\n>I have to disagree with the current approach if there are going to be 5\n>month periods between drafts.  Given that many people are already deploying\n>HTTP applications, and have only the I-Ds to go by, having major changes\n>like this appear without warning is, IMHO, unacceptible.\n\nI agree, which is why I proposed a 2-week review cycle before the next\ndraft is produced.\n\n>I don't have a problem with the idea of having a small group come up with a\n>proposal for discussion.  But couldn't those proposals be discussed on the\n>list as they are formed, rather than waiting months for an I-D?\n\nYes, provided that they are not made by the author (me).  My proposals\nare made in the form of a new draft -- doing otherwise just increases\nthe delay between drafts.  I do make mini-proposals during commentary,\nbut some proposals require more thought.\n\n>I just read the Danvers minutes, and I didn't see this discussed.  When you\n>say \"decided just before Danvers\" do you mean it was discussed here?  If so,\n>could you point me to it in the archives so I can get up to speed on the\n>discussion that occured at the time?\n\nYes, it was discussed there.  The point was made, by both Henrik and\nDave Kristol, that existing software (XMosaic) did not send an Accept-Encoding\nfield even when they do accept gzip and compress.  Therefore, the default\nshould be accept-all.  There were no objections raised at the meeting,\nso I recorded it as one of the changes for the next draft, and duly made\nthat change as soon as I could.  It should also be in the minutes, but\nI did not get a chance to review the minutes for accuracy and completeness.\n\n>>All current browsers do accept all formats, in the form of the Save As..\n>>dialog.  The WG decided that it was better to assume the best than to\n>>require all clients to be compliant before the feature could be used.\n>\n>This isn't true.  There are several browsers which will break if, for\n>example, they get an encoded file (because they don't parse\n>Content-Encoding).  By this logic, we should also have q default to a\n>non-zero value for those browsers which don't include */* in their Accept\n>field (of which there are some).\n\nDefine \"break\" in a useful manner.  Better, tell me what the draft needs\nto say to fix this situation.  If the WG agrees, I *will* change the draft.\n\n>Also, as I pointed out in item 5 of my last note, not all HTTP clients are\n>browsers.  An automated WWW searching tool (for example) won't have a Save\n>As option.  A kiosk-based browser likely wouldn't have a Save As option.\n>They would just break under this change.  Also, under the new draft, there's\n>absolutely no way for these types of applications to say that they won't\n>accept certain encodings and charsets, since even including the headers they\n>can only get qe and qc down to 0.001.\n\nRight.  So we need to define those semantics.  I suggest that we allow\ninclusion of the default charsets, such that\n\n   Accept-Charset: iso-8859-1\n\nimplies acceptance of only that charset and US-ASCII (which would always\nbe implied).  No accept-charset would be the same as accepting all, and\ngiven an accept-charset, unlisted charsets would be qc=0.\n\n>>>3.  The spec allows you to specify \"Accept: \" but doesn't say what the\n>>>effect is.  My reading seems to indicate that this means no MIME type is\n>>>acceptable, \n>>>but it's somewhat ambiguous.\n>>\n>>Yes.  What should the definition be?\n>\n>Having it mean no MIME type is acceptible might be useful as a way to force\n>a 406 response.  That could provide a consistent way for a client to get a\n>list of varients for a URI.  If we decide this is desirable, it should be\n>made explicit.\n\nYes, I agree.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 Review Pla",
            "content": "Various people wrote:\n>>>   The same applies to Authorization.\n>>\n>>Let's go for the alternative.  Breaking all existing implementations of\n>>something like this seems unnecessary.  If you *must* go for semicolons,\n>>define a new header.\n>\n>Keep in mind that existing clients will not recognize the new header.\n>That may not be a problem if both are provided, but will remain a problem\n>for the Authorization field.\n>\n>Another alternative would be to forbid multiple schemes per resource,\n>or require that applications parse the AA fields such that they can\n>recover gracefully from unexpected folding.\n>\n>Perhaps the latter would be best for 1.0?\n\nThe only thing is that I have been seeing multiple WWW-Authenticate: fields\nall over the net. If a server wants to inform clients that it will accept\nmultiple authorization schemes for a resources, it really seems to be\n\"current pratice\" that the server include multiple WWW-Authenticate fields\nrather than one WWW-Authenticate with several semicolon separated entries.\n\nDoes anyone else have any ideas about the WWW-Authenticate problem? I\nbelieve that this is a key issue if we want to see DIGEST authentication\ndeployed.\n\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\non (note that I created the fragment ids by hand)\nis more stable.  I will try to transfer the fragments to each version.\n\nWhen it becomes an RFC, I'll do a completely-hyperized version for\nposterity.  ;-)\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\nrtment of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 Review Pla",
            "content": ">>I would like to avoid getting too involved in\n>>the debate over portions of the 1.0 draft, except where it becomes\n>>necessary to describe the thinking behind some of the recent changes.\n>\n>Given the number of surprise changes and objectionable ones at that, I\n>believe this is unrealistic.\n\nWho said \"like to\" had to be realistic?\n\n>>3) WWW-Authenticate\n>>\n>>   The new spec now uses semicolon to separate parameters -- keeping\n>>   it as comma-separated would prevent people from using more than\n>>   one AA scheme per resource.  This will break existing implementations\n>>   of Digest and PGP AA.  One alternative is to leave WWW-Authenticate\n>>   as a fixed field (i.e., only describe it for Basic), and define a\n>>   new syntax for an Authenticate header.\n>>\n>>   The same applies to Authorization.\n>\n>Let's go for the alternative.  Breaking all existing implementations of\n>something like this seems unnecessary.  If you *must* go for semicolons,\n>define a new header.\n\nKeep in mind that existing clients will not recognize the new header.\nThat may not be a problem if both are provided, but will remain a problem\nfor the Authorization field.\n\nAnother alternative would be to forbid multiple schemes per resource,\nor require that applications parse the AA fields such that they can\nrecover gracefully from unexpected folding.\n\nPerhaps the latter would be best for 1.0?\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Complaints regarding Draft Status and Autho",
            "content": "As a former IETF working group chair and I-D author, I've been in Roy's\nshoes (although not quite as severely flamed).  Some people apparently\nhave the misguided opinion that the author of an Internet Draft is\nrequired to satisfy all participants in the process.\n\nNonsense.  This is almost always impossible, and so it would be a\nrecipe for failure.\n\nThe duty of an I-D author is to produce a coherent, readable draft\nthat represents a reasonable consensus of the most appropriate\ntechnical (and sometimes political) design.  If you don't like it,\nyou have several options:\n(1) try to convince the author to change his or her draft.\nThis is often best done in private.\n(2) try to convince the group to reject the draft.  This\nis usually done in public, but it has the unfortunate\nside effect of blocking any progress.\n(3) write your own draft, if you think you know better.\nLet the group choose between two options for progress.\nIt's pointless to complain that an I-D author isn't listening\nto you.  Writing an I-D is hard work (especially for one as\ntechnically and politically complex as HTTP 1.0).  If you\nsimply make the author's life unbearable, we'll end up with\nno draft, not one that represents your own personal opinion.\n\nRoy (or anyone else who attempts to write an HTTP 1.0 standard) has\nthe especially difficult position of trying to describe existing\npractice, not a new protocol, when existing practice is so\nvariable and sometimes idiotic.  I think it might be a good idea\nfor the working group to agree on an explicit statement of criteria\nfor including or not including a feature in the 1.0 standard.  As\nfar as I can tell, such an explicit statement does not exist, at\nleast not (1) in a widely known place and (2) with the consensus\nof the group behind it.  It ought to be part of the Charter.\n\nI suggest that the WG chair make this a high priority, so that we\ncan dispense with arguments about what HTTP 1.0 should and should\nnot be doing, and start discussing whether Roy's draft meets the\nconsensus criteria.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": ">In my last note, I was trying to ask if the negotiation changes were\n>discussed on the list.  My understanding of IETF operation (correct me if\n>I'm wrong) is that the working group mailing list is supposed to be the\n>primary forum for discussion and the meetings are to be secondary.  I get\n>the impression that these issues were discussed at the meeting, but not on\n>the list (until now, much later), which I think is wrong.\n\nAlmost right.  The working group mailing list is the only place where\nwe are allowed to poll for rough consensus (i.e., rough consensus at a\nmeeting has no meaning unless it is also affirmed on the list).  It is\nimpossible for all face-to-face discussions to be reflected back to the\nmailing list -- the bandwidth simply isn't there, and I'm too slow a\ntypist to do it myself.  Therefore, the input is reflected in the next\ndraft and the WG is then polled for consensus.  It only seems wrong because\nit took so bloody long for me to finish the draft, for which I apologize\nagain.\n\n>1. Change the behavior back to draft 00 where the absence of Accept-Charset\n>or Accept-Encoding indicates support for only iso-8859-1 or identity\n>encodings, respectively.\n>\n>2. In the absence of Accept-Charset or Accept-Encoding set qc to 0.001 for\n>all possibilities other than iso-8859-1 or identity encodings.\n\nIdentity encoding won't exist if we get rid of CTE.\n\n>>I suggest that we allow\n>>inclusion of the default charsets, such that\n>>\n>>   Accept-Charset: iso-8859-1\n>>\n>>implies acceptance of only that charset and US-ASCII (which would always\n>>be implied).  No accept-charset would be the same as accepting all, and\n>>given an accept-charset, unlisted charsets would be qc=0.\n>\n>Would this have a different effect than just saying \"Accept-Charset:\" (with\n>no charsets listed)?   In any case, do I understand correctly that you're\n>proposing going back to qc or qe = 0 (rather than 0.001) for unlisted types?\n>If that's a correct interpretation of what you're saying, it's fine with me.\n\nYes, that is what I was saying (it is a return to what I had prior to\nDanvers, with some additional explanation).\n\nIf anyone in the WG disagrees, this is the time to voice your opinion.  \nPlease site specific examples of why you disagree, so that the WG can\njudge which solution is the best given our constraints.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: more http-v10-spec0",
            "content": ">Maybe I'm incredibly dense (I see those heads nodding :-), but I found\n>Sect. 8.28 a bit confusing. \n\nNo, not dense, that was the last section added in response to Jim's\nearlier comments that the URI header was insufficient for caching.\nThis is the first time the text has been reviewed by anybody other than me.\n\n>What would have helped me is for there to\n>be an explicit statement that particular vary-dimension's correspond\n>exactly to specific HTTP headers, e.g.,\n>typeAccept\n>charsetAccept-Charset\n>languageAccept-Language\n>encodingAccept-Encoding\n>user-agentUser-Agent\n>version??\n>The examples imply the correspondence but there's no explicit statement\n>of correlation.\n\nYep, will do.\n\n>What, exactly, is \"version\"?  Where is the version specified in the URI?\n\nOoops, that's a leftover -- version is not necessary and will be deleted.\nVersions are specified by different URIs.\n\n>How does a caching intermediary handle a vary-dimension of <token>?\n>That is, if you can't specify the dimensions in advance, how can the\n>intermediary figure out what the semantics for such an unspecified\n>dimension are?  (If <token> is just meant as a place-holder for future\n>expansion, let's say so.)\n\nActually, I was going to remove token altogether, since these things\ncan't be extended without changing the protocol version.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Negotiation in draft 0",
            "content": "In my last note, I was trying to ask if the negotiation changes were\ndiscussed on the list.  My understanding of IETF operation (correct me if\nI'm wrong) is that the working group mailing list is supposed to be the\nprimary forum for discussion and the meetings are to be secondary.  I get\nthe impression that these issues were discussed at the meeting, but not on\nthe list (until now, much later), which I think is wrong.\n\nAs far as assuming that any encoding or character set is equally acceptible\nin the absence of Accept-* headers, image the case where a server returns a\ngzip'ed text/html document.  A browser which doesn't parse Content-Encoding\n(of which there are several) will try to display this to the user, which\nwill result in garbage being displayed.  It's possible that the browser will\neven do CR-LF munging as the gzip'ed file is received, so that saving at\nthis point would be useless.\n\nSimilarly, a robot cataloging web content might receive a text/html document\nusing iso-2022-jp, even though it sent no Accept-Charset header.  If the\nrobot doesn't look at the returned charset, it might then record bogus\ninformation about the page's content since it can't parse the content\ncorrectly.  What's really upsetting is that the same document might have\nbeen available in an iso-8859-1 version, but iso-2022-jp version was sent\ninstead because all charsets have qc=1.\n\nI have two proposals:\n\n1. Change the behavior back to draft 00 where the absence of Accept-Charset\nor Accept-Encoding indicates support for only iso-8859-1 or identity\nencodings, respectively.\n\n2. In the absence of Accept-Charset or Accept-Encoding set qc to 0.001 for\nall possibilities other than iso-8859-1 or identity encodings.\n\n>I suggest that we allow\n>inclusion of the default charsets, such that\n>\n>   Accept-Charset: iso-8859-1\n>\n>implies acceptance of only that charset and US-ASCII (which would always\n>be implied).  No accept-charset would be the same as accepting all, and\n>given an accept-charset, unlisted charsets would be qc=0.\n\nWould this have a different effect than just saying \"Accept-Charset:\" (with\nno charsets listed)?   In any case, do I understand correctly that you're\nproposing going back to qc or qe = 0 (rather than 0.001) for unlisted types?\nIf that's a correct interpretation of what you're saying, it's fine with me.\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "Defn of Location in 200, 301, and 30",
            "content": "[moved from www-talk -- specification wording should be discussed here].\n\nDave Kristol wrote:\n\n>I quibble with your [Marc's] interpretation of the spec.  The actual words are\n>\"For 2xx responses, the location should be the URL needed to\n>retrieve the same resource again...\"\n>Apparently the \"should\" is ambiguous.  You read it to mean that a\n>server *must* send a Location header, and its value \"should be the\n>URL...\".  I read it to mean that *if* the server sends Location, its\n>value \"should be the URL...\".\n\nYes, and that is stated clearly in the section on 200.\n\n   If the entity corresponds to a resource, the response may include a \n   Location header field giving the actual location of that specific \n   resource for later reference.\n\n>AFAIK, servers are not required to send Location except when they send\n>a 30[123] response.  In looking at the spec., though, I find that it is\n>fuzzy about which headers must be sent under what circumstances.  In\n>particular, the descriptions of the 30[123] response codes should\n>probably make explicit reference to the Location and URI headers.\n\n????? You mean, more explicit than [e.g., 302]\n\n   If the new URI is a single location, its URL must be given by the \n   Location field in the response. If more than one URI exists for the \n   resource, the primary URL should be given in the Location field and \n   the other URIs given in one or more URI-header fields. The Entity-\n   Body of the response should contain a short hypertext note with a \n   hyperlink to the new URI(s).\n\nHow is that not sufficient?\n\n>In general it would be nice to be able to identify quickly which\n>headers are required and which, optional.  (That's a weasily way of\n>saying \"Would someone else please propose such content for the spec.\")\n\nI tried several such mechanisms, none of which worked.  The spec cannot\nbe simpler than the protocol and still remain correct.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Defn of Location in 200, 301, and 30",
            "content": "Roy Fielding <fielding@beach.w3.org> wrote:\n  > \n  > [moved from www-talk -- specification wording should be discussed here].\n  > \n  > Dave Kristol wrote:\n  > \n  > >I quibble with your [Marc's] interpretation of the spec.  The actual words are\n  > >\"For 2xx responses, the location should be the URL needed to\n  > >retrieve the same resource again...\"\n  > >Apparently the \"should\" is ambiguous.  You read it to mean that a\n  > >server *must* send a Location header, and its value \"should be the\n  > >URL...\".  I read it to mean that *if* the server sends Location, its\n  > >value \"should be the URL...\".\n  > \n  > Yes, and that is stated clearly in the section on 200.\nWhich I (gulp) missed, in spite of change bars (which I appreciate) :-(.\n  > \n[...]\n  > >AFAIK, servers are not required to send Location except when they send\n  > >a 30[123] response.  In looking at the spec., though, I find that it is\n  > >fuzzy about which headers must be sent under what circumstances.  In\n  > >particular, the descriptions of the 30[123] response codes should\n  > >probably make explicit reference to the Location and URI headers.\n  > \n  > ????? You mean, more explicit than [e.g., 302]\n[...]\nOops again.  I retract my remarks about 30[123].  (Sherlock Holmes would\nhave said to me, \"You look, but you do not observe.\")\n\n  > >In general it would be nice to be able to identify quickly which\n  > >headers are required and which, optional.  (That's a weasily way of\n  > >saying \"Would someone else please propose such content for the spec.\")\n  > \n  > I tried several such mechanisms, none of which worked.  The spec cannot\n  > be simpler than the protocol and still remain correct.\n\nYeah, the reason I wimped out was that I couldn't think of any either.\nThe problem from an implementor's standpoint (mine) is, how do I know\nwhat stuff I must send or accept, and when?  Yes, careful reading of\nthe spec. should do it.  But it would be nice if the spec. helped a bit.\n(Not meant as a criticism of the editor.)\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Just FYI: Actually, DCE RPC has been in Windows NT since 3.1; and the \nWin32 SDK included support for 16 bit Windows, and it is \"in the box\" \nfor Win95 --- all with threads support to effect non-blocking if you need it.\n\nAlso, you can more than halve the size of the spec you need to \nimplement by just doing the the UDP version -- which, given that one of \nthe problems with HTTP that everyone talks about is the overhead \nincurred from opening TCP connections for one-off transactions, seems \nlike a good direction to go.  (If the normal interaction between client \nand server is idempotent, it will only take one round trip...)\n\nLastly, if I were tasked with implementing datagram DCE RPC from the \nspec, I'd start by looking at the 1990 Prentice Hall book \"Network \nComputing Architecture\", by Zahn, et al, which uses a much clearer \nspecificaiton methodology (IMHO) to specify a previous, backwards \ncompatible, version of the DCE RPC protocol.\n\nPaul\n----------\n] From: Rich Salz  <rsalz@osf.org>\n] To:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>;  \n<janssen@parc.xerox.com>;\n] <rsalz@osf.org>\n] Subject: Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, \nStockholm\n] Date: Tuesday, August 08, 1995 11:43PM\n]\n] Bill's comments on DCE are correct enough that picking nits isn't worthwhile.\n] Thanks.  (Wish I could pay as close attention to his work. :)\n]\n] On the other hand, it will be bundled with the next release of NT, and\n] several not-as-ubiquitous operating systems. :)\n] /r$\n] \n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 InternetDraft 01 Availabl",
            "content": "Roy Fielding said:\n\n>[I said]:\n> >Is there any particular reason the HTML version has gone from a nicely \n> >chunked set of pages to one massive document? Pulling over several hundred K \n> >of html just becuase I forgot what a 406 response code was seems less than \n> >desirable.\n \n> Several reasons.  1) We no longer have access to WebMaker;\n>                   2) WebMaker (or at least the old one) did not provide\n>                      stable names for the chunks;\n\nOK yes that is indeed a problem. We use Webmaker here and find that to \nbe a pain, especially in a document that gets updated regularly.\n\n>                   3) What I do have is MifMucker.\n> \n> >Could I ask for a chunked version (again) please?\n \n> Only if you generate it yourself.  You can do so from the current\n> version in HTML, or I can provide the MIF file. \n\nWe are trying out a new product called HTML Publisher, so that might \nbe interesting. However, it is a time limited demo and expires end August.\n\n> However, I wouldn't\n> bother, because a new draft will be out in less than two weeks.\n> The single-file version (note that I created the fragment ids by hand)\n\nOuch... yes, no point if it will be superceded as soon as it is built.\n\n> is more stable.  I will try to transfer the fragments to each version.\n> \n> When it becomes an RFC, I'll do a completely-hyperized version for\n> posterity.  ;-)\n\nOk well if it is changing every couple of weeks I will just live with it\nuntil it becomes stable. Thanks for the prompt response.\n\n-- \nChris Lilley, Technical Author\n+-------------------------------------------------------------------+\n|       Manchester and North HPC Training & Education Centre        |\n+-------------------------------------------------------------------+\n| Computer Graphics Unit,             Email: Chris.Lilley@mcc.ac.uk |\n| Manchester Computing Centre,        Voice: +44 161 275 6045       |\n| Oxford Road, Manchester, UK.          Fax: +44 161 275 6040       |\n| M13 9PL                            BioMOO: ChrisL                 |\n|     URI: http://info.mcc.ac.uk/CGU/staff/lilley/lilley.html       | \n+-------------------------------------------------------------------+\n\n\n\n"
        },
        {
            "subject": "Session-ID -&gt; StateInf",
            "content": "At Roy Fielding's prodding, I have aligned the terminology of my\nSession-ID pre-proposal to match the HTTP draft.  I also changed\nthe name of the header (but not the URL) from Session-ID to\nState-Info.\n\nFurther comments are welcome.  Documents at\nhttp://www.research.att.com/~dmk/session.html\n\nDave Kristol\n\nsing session-ids, Does this have any functional\n  > implications? I don't see it as an important differnce.\nThe state information is stored on the client-side in both cases.\n  > \n  > I cannot see any big differences in complexity.\n  > Expiration-time that you mention as adding complexity\n  > to cookies seems to me to be a separate issue, we\n  > might want it also in the session-id mechanism. or\n  > we may discard it from cookies.\nBecause the server initiates the session, I felt it was appropriate for\nthe server to decide when a Session-ID was no longer appropriate.  In\nNetscape's cookies, the user agent examines the cookie and can decide\nwhether it had expired.\n  > \n  > Probably an important issue is the diffence in\n  > the \"scope\" of the ids/cookies. \n  > Distribution of cookies is determined by the\n  > url (client send cookies along with request if\n  > the url fills certain criteria). \n  > I thing your proposal leaves the issue open.\nThe user agent always returns the ID.  The server can decide whether\nit's relevant to a particular request.\n  > Everything in the \"same window\" will use the session-id.\n  > What about non-windowing clients? \nThey behave like a single-window client.\n  > \n  > Tying the id to server/port combination may\n  > not be enough, I think the cookie way of looking at\n  > the whole url gives more possibilities (are there\n  > reasons why not to do it?).\nI wanted the client's role to be very simple.  With Session-ID (\"State-Info\"\nimminently), the client never examines the content of the ID; it just sends\nit along.  Having the client act on the URL may have advantages.  (I'd\nlike a proponent to give them.)\n  > \n  > Could some kind of a .. \"flow\" (?)\n  > system be thought of? Server would return a \n  > session-id in all transactions belonging to a\n  > session, and client would always echo it in \n  > the subsequent requests to the same server.\nThat's essentially what I've proposed.\n  > \n  > Or maybe to other servers as well? This gets complicated,\n  > but maybe there is  need for somthing like this;\n  > a server starts a session with a client,\n  > client follows an external link to some other\n  > server that gets the session-id, sends a request\n  > to the original server to get information about\n  > the session and acts on that if useful?\nwww-talk (or was it http-wg?) discussed this recently.  People expressed\nconcerns about privacy and the ability to correlate visits to multiple\nservers.\n\n[...]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "UDP or TCP",
            "content": "    Also, you can more than halve the size of the spec you need to \n    implement by just doing the the UDP version -- which, given that one of \n    the problems with HTTP that everyone talks about is the overhead \n    incurred from opening TCP connections for one-off transactions, seems \n    like a good direction to go.  (If the normal interaction between client \n    and server is idempotent, it will only take one round trip...)\n\nI think there's a misconception here.  Yes, there is some packet\noverhead for creating and destroying TCP connections, and the current\none-request-per-connection model is expensive in that respect.\n\nBut simply going to UDP is not the answer, because UDP (without\nsome extra effort) does not offer any congestion-control mechanisms.\nSure, you can layer them on top of UDP, but I suspect that you would\nquickly get back to something very similar to TCP.\n\nSince we're discussing what is already the dominant source of packets\nin the wide-area Internet, and will surely keep growing, we need to\nbe very sensitive about how HTTP's design affects WAN congestion.\nOtherwise, nobody is going to be very happy.\n\nOne major benefit of a persistent-TCP-connection model is that it\nallows the packet sources to learn, over sufficiently long periods,\nwhat the congestion state of the network is, and therefore adapt\nto it.\n\nI should also point out that the average size of an HTTP retrieval\nis well over one packet's worth of data (I don't have the stats handy,\nbut the number \"8 K bytes\" sticks in my head from someplace).  This\nmeans that even for a single request per connection, the congestion\ncontrol mechanisms of TCP have some chance of being helpful.\n\nPaul, what does DCE RPC do for congestion avoidance and control\nin WANs, when it uses UDP?\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "I was about to make a comment about why UDP for basic HTTP was inappropriate,\nbut Jeff Mogul beat me to it, and as usual has said it better than I could\nhave.  The congestion collapse of the Internet happened just when we were\nbuilding X11.  The moral imperative to protect the Internet from\ndisaster is emblazoned on the scars some of have from getting up in the middle\nof the night to get code across the country, when the network didn't work.\n\nDCE RPC has several other problems, the most important of which\nis that it isn't available universally, and very difficult to make it so\nquickly, independent of other attributes that are needed, some of which are\nbelow.\n\nHere are some requirements for technology used to build a new protocol:\n* ubiquitous: anything we depend on has to either be available \nuniversally, or available and easy to port. \n* needs to support non-blocking or streaming (often called \"batching\")\nof requests; if there is no error from a request and the request has\nno return value, the system shouldn't generate network\ntraffic.  Round trips are the death of performance on a world\nwide network, with round trip times often measured in hundreds\nof milliseconds (or more).\n* needs to support streaming and interleaving of return values;\nentities need to be able to be multiplexed on the return connection\nfrom the server so we can do appropriate prioritization and\nprefetching of objects.\n* bit efficient: the web should be more usable over high latency and\nlow bandwidth links. Too much bandwidth is going into protocol\nand metadata transport in current HTTP.  Think dialup modems,\nor use over cellular modems, and you immediately get round trips\nfrom 150 milliseconds to ~1 second before you start transiting\na global network.\n* fast, at least for the server; think of servers connected\nto truly high speed networks, which may be delivering many\nvideo streams, or millions of requests/hour, for example. \nThink of the load generated by a television AD for a neat\nproduct on a web server 5 years from now, \nwhen that advertisment is made during the World Cup (world wide \ninterest, outside of U.S.)  or Super Bowl (U.S. only). \nEveryone may turn around at the next commercial break and hit \nthe same server.\nCopies of data must be avoided.   \nCycles and bandwidth on clients are much less dear, \nbut may be much slower systems, so speed on the client side \ncan't be ignored either.\n* doesn't suffer from error 33: basing work on top of someone\nelse's research work while it is still research is a good way to\ncause a project (research or not) to never be completed.\n* reliable, dependable, and problems can get resolved quickly so\nforward progress can be made.\n* the usual comments about size, memory consumption, etc.\n\nNot all of these requirements have to necessarily be fulfilled by the\ntransport protocol itself, but the transport system certainly affects\nwhat you can do with what is built on top.\n\nMy suspicion is that rolling one from scratch down to TCP is the likely\nbest choice, but I feel we need to understand if any system out there\nmight in fact be useful.  I'd be VERY happy to be proved wrong.\nI've rolled more protocol stubs by hand than I care to think of for \ntwo different systems both in widespread use, and I certainly do not love doing \nso.\n\nSo I intend to spend some time looking around at what is out there that\nmay fulfill the requirements. (ILU from Xerox, and others that \nmay be suggested to me).  Butler Lampson also pointed out to me that the \nexercise of looking around at existing systems will likely pay off in good \nideas worth stealing, even if the RPC system itself isn't appropriate to this \napplication. (If you can't steal good code, at least steal good ideas; a good \nmotto, me thinks).\n- Jim Gettys\n  W3C\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "As I was reading though this extensive wish list for what an http replacement\nwould look like, I nodded a lot; one place where I shook my head vigorously,\nhowever, is the line \"Copies of data must be avoided\".  \n\nI would argue that a better notion would be to make a protocol capable\nof handling data replication in a smart way.   Working with a number\nof international programs has made me see the usefulness of replicated\ndata; the ability to connect to a server that is reasonably local can\nbe a tremendous win, especially in situations where a connection to\nthe Internet is over a very slow link.  While the URN to URL resolution\nproposals provide a useful model for long-term work, some very simple\nsolutions could work for mirrorinng non-interactive pages even in the\nshort term.  A Mirrors: header, for example, could provide a simple list\nof alternate URLS where the same data could be found (much like the lists\nsome ftp servers provide when user limits have been reached); the client\ncould then analyze the list and choose mirrors which are closer.  While\nit may be difficult to determine what is \"close\" and what is \"distant\"\nin network terms, there are some rules which could be built into the\nclients.  (Actually, a clever server could do the same thing with \nredirects--read the client's network and domain from its headers and\nissue a redirect to a closer mirror).  \n\nMoving this sort of negotiation even lower in the protocol (so that it\noccurs before other header negotiation) might be a bigger win yet,\nalthough you might also want to go through the full negotiation to\nbe sure that the mirror contained the correct flavor of content (if\nit were available in various char sets, for example).\n\nAnyway, the basic point I want to stress is that avoiding data\nduplication may be a win in some circumstances, but using data\nduplication wisely can help you get a lot closer to some of your other\ngoals pretty quickly.\nRegards,\nTed Hardie\nNASA NAIC\n\n\n> \n> I was about to make a comment about why UDP for basic HTTP was inappropriate,\n> but Jeff Mogul beat me to it, and as usual has said it better than I could\n> have.  The congestion collapse of the Internet happened just when we were\n> building X11.  The moral imperative to protect the Internet from\n> disaster is emblazoned on the scars some of have from getting up in the middle\n> of the night to get code across the country, when the network didn't work.\n> \n> DCE RPC has several other problems, the most important of which\n> is that it isn't available universally, and very difficult to make it so\n> quickly, independent of other attributes that are needed, some of which are\n> below.\n> \n> Here are some requirements for technology used to build a new protocol:\n> * ubiquitous: anything we depend on has to either be available \n> universally, or available and easy to port. \n> * needs to support non-blocking or streaming (often called \"batching\")\n> of requests; if there is no error from a request and the request has\n> no return value, the system shouldn't generate network\n> traffic.  Round trips are the death of performance on a world\n> wide network, with round trip times often measured in hundreds\n> of milliseconds (or more).\n> * needs to support streaming and interleaving of return values;\n> entities need to be able to be multiplexed on the return connection\n> from the server so we can do appropriate prioritization and\n> prefetching of objects.\n> * bit efficient: the web should be more usable over high latency and\n> low bandwidth links. Too much bandwidth is going into protocol\n> and metadata transport in current HTTP.  Think dialup modems,\n> or use over cellular modems, and you immediately get round trips\n> from 150 milliseconds to ~1 second before you start transiting\n> a global network.\n> * fast, at least for the server; think of servers connected\n> to truly high speed networks, which may be delivering many\n> video streams, or millions of requests/hour, for example. \n> Think of the load generated by a television AD for a neat\n> product on a web server 5 years from now, \n> when that advertisment is made during the World Cup (world wide \n> interest, outside of U.S.)  or Super Bowl (U.S. only). \n> Everyone may turn around at the next commercial break and hit \n> the same server.\n> Copies of data must be avoided.   \n> Cycles and bandwidth on clients are much less dear, \n> but may be much slower systems, so speed on the client side \n> can't be ignored either.\n> * doesn't suffer from error 33: basing work on top of someone\n> else's research work while it is still research is a good way to\n> cause a project (research or not) to never be completed.\n> * reliable, dependable, and problems can get resolved quickly so\n> forward progress can be made.\n> * the usual comments about size, memory consumption, etc.\n> \n> Not all of these requirements have to necessarily be fulfilled by the\n> transport protocol itself, but the transport system certainly affects\n> what you can do with what is built on top.\n> \n> My suspicion is that rolling one from scratch down to TCP is the likely\n> best choice, but I feel we need to understand if any system out there\n> might in fact be useful.  I'd be VERY happy to be proved wrong.\n> I've rolled more protocol stubs by hand than I care to think of for \n> two different systems both in widespread use, and I certainly do not love doing \n> so.\n> \n> So I intend to spend some time looking around at what is out there that\n> may fulfill the requirements. (ILU from Xerox, and others that \n> may be suggested to me).  Butler Lampson also pointed out to me that the \n> exercise of looking around at existing systems will likely pay off in good \n> ideas worth stealing, even if the RPC system itself isn't appropriate to this \n> application. (If you can't steal good code, at least steal good ideas; a good \n> motto, me thinks).\n> - Jim Gettys\n>   W3C\n> \n\n\n\n"
        },
        {
            "subject": "StateInfo is too fragile..",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nre: David Kristol's proposal: \n\nI can't help thinking that the mechanism you propose is simply too fragile.\n\nYour proposal only works when the client's access pattern is limited to a\ncooperating set of URLs. This will probably typically be only those URL's\nwhich are explicitly embedded in the HTML of other members of the set. This\nmight have a better chance of working in a system like gopher where the\nnatural navigation paradigm is one of walking up and down hierarchical links\n, but Web clients have a much looser style of interaction. The Back button,\nHistory lists, and the ability to jump to links which arrive in mail\nmessages, etc. means that users are often in multiple threads during a\nsingle session. (i.e. while considering what books to buy, they may follow a\nlink found in a mail message which leads to a book review which is NOT one\nof the \"cooperating\" URL's on the relevant server. Doing so would break the\nState-Info chain.)\n\nThe fragility of the system will probably result in a great deal of\nconsternation and confusion among naive users. At first, they probably won't\nunderstand the problem. Later, these users will learn that the sequence in\nwhich they access pages can effect the outcome of a session. They are likely\nto generalize this knowledge broadly and then become much more rigid in\ntheir overall usage patterns and so use the Web like a gopher server... \n\nWe may see client writers who attempt to help users by warning them with\ndialog boxes that say: \"You are jumping to a URL not referenced on the page\nyou are viewing. This could terminate your current session.\" Ugly...\n\nIt would appear that HTML writers of pages that carry State-Info would have\nto be careful about keeping users from \"wandering\" during sessions. They\nwould have to stop the common practice of locating links to the site home\npage on each of their pages since unless that home page was wired to always\nreflect back any State-Info, the State-Info would be lost and the user\ncouldn't resume the \"shopping\" session by \"backing\" back into the session..\nThe practice of \"reflecting\" State-Info which is not understood could grow\nwidely...\n\nIt also appears that restarting a session, once it is broken, could be\ndifficult in the presence of caches. Unless the origin server provides tight\n\"expires\" headers or uses the no-cache pragma, it is likely that pages which\nstart a session will get cached. (By \"start a session\" I mean the page which\nis the first to send back a State-Info: header.) Thus, if the user attempts\nto reload such a page in order to \"restart\" a broken session, they will be\ndisappointed. Of course, the same problem occurs for a second user who\naccesses the same \"starting page\" from cache. They won't get a session. You\ncould fix this by requiring that the\"starting\" pages in a session are always\nnon-cacheable...An alternative would be to require that the cache remembered\nthat State-Info: had been present on a response and then generate\nconditional Gets with null State-Info: if a request for the cached page\narrives with no State-Info: header.  (NOTE: I accept that your caching\nalgorithm works properly in many other circumstances .i.e. when \"starting\" a\nsession isn't the issue.)\n\nHow should a client behave if it gets a \"503 Service Unavailable\" response\nwhich includes a \"Retry-After\" header to a request which had a State-Info\nheader? Your proposal would seem to indicate that this would terminate the\n\"session\" since it is a response from the server which carries no State-Info\n. However, this may not be the intent of the server or the author. Similar\nproblems occur with \"504\", \"409\", \"408\", \"401\", \"402\", \"301\", etc...\n\nThe Netscape \"cookie\" stuff doesn't seem to have as much fragility as your\nproposal. The \"path\" attribute of a cookie allows users a great deal of\nfreedom in the order in which they browse pages -- they trade increased\nclient complexity against increased user-perceived system complexity. The\n\"cookie\" proposal appears to have the same problem with \"starting\"  a new\nsession through a cache although \"restarting\" isn't a problem. The \"cookie\"\nproposal does not seem to have problems with HTTP error messages.\n\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "I don't want to turn this list into a DCE battle, and I apologize\nfor distracting people when they should be reading Roy's draft.\n\nEverything Jeff and Jim wrote seems more-or-less right on target to me.\n\nI'll gladly answer DCE questions in email (or comp.soft-sys.dce), or\neven on this list if there are no other options.\n\n> Paul, what does DCE RPC do for congestion avoidance and control\n> in WANs, when it uses UDP?\n\nNothing.  It should probably send an advisory status code up to the\napplication recommending use of the ncacn_ip_tcp protocol sequence:)\n/r$\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "Dave Kristol writes:\n > koen@win.tue.nl (Koen Holtman) wrote (on www-talk):\n >   > Dave Kristol:\n >   > [....]\n >   > >http://www.research.att.com/~dmk/session.html\n >   > \n >   > This proposal is not clear enough about caching. Specifically:\n >   > \n >   >   is the session-id header in the request part of the cache key for the\n >   >   entity in the response?\n > No.  In section 2.3 I said:\n >     Similarly, a caching proxy must pass back to the requestor any\n >     Session-ID response header it receives, but it must not cache that\n >     header as part of its cache state.\n >   > \n >   > If it is, this means that almost no meaningful caching is possible for\n >   > services using session-id, even if 99% if the entities in the session\n >   > (inline pictures, product description pages) do not depend on the session\n >   > state.\n > Yes, exactly.\n\nHowever, please note that the \"side channel\" of state information that\nflows both directions and bypasses proxy and user-agent caches, even\nif the resources themselves are cached, is not cheap.  Setting up and\ntearing down the TCP connections is a nontrivial fraction of the cost\nof retrieving a small resource (but I admit: I don't have numbers) --\nespecially html files, as opposed to large graphics or audio media files.\nAnd those media files are typically cacheable anyhow.  Even on systems\nwhere URLs contain session-IDs, the URLs for the media files usually\nneed not, and so they're cacheable.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "Jeff is right on the money here- however there are a few interesting \npoints that affect the decision as to which transport to use for general \nHTTP transactions.\n\nThe key point to note is that the vast bulk of data currently transferred \nover HTTP has transport needs that don't correspond to the features \noffered by TCP, and using strategies can greatly improve performance and \ncongestion characteristics.\n\nInline images:\n\nInline images have two properties that make a non-tcp transport \ndesirable.\n\n1) Out-of-Order reassembly: Images are in general 2-dimensional \n(pace, Pesce :). If messages are labelled, they can be rendered \nimmediately upon arrival, regardless of whether preceding messages \nhave been successfully delivered. Because TCP provides ordered \ndelivery, if a single packet is lost, no data can be delivered to \nthe application until that packet is successfully retransmitted \nand received.\n\n2) Some data loss is acceptable: Because the human eye and brain \nare designed to work with partial data, images can be \nreconstructed sucesfully even if some data is missing. Thus, \nnot every lost packet needs to be re-transmitted. This effect is \nmost noticeable if the data is ordered so as to minimise the \nnumber of adjacent pixels contained in a single message. \n        TCP is a reliable transport service - if a message is lost, the \nsender will re-transmit it after a timeout. \n\nAudio data:\n\n1) Audio data is linear, and real time: If a message containing \naudio data fails to arrive within a given window, it has missed its\nchance and is no longer wanted. Retransmission is not helpful- \nthe only way to deal with data loss is via regeneration from \nother packets (I think Christian has done some work on this?)\n\n2) Audio data rates are adaptable: If the bandwidth available to \nthe sender decreases, the data rate can be reduced by increasing \ncompression ratios (MPEG), or by switching to a different \nencoding scheme (ADPCM, etc). TCP does not make bandwidth information\navailable to the application, and does not allow data that has been\nsent but not delivered to be recalled. \n\n\nReal Time Transport protocols.\n\nA better approach for these particular media types is to use some sort of \nReal Time Protocol, such as RTP. These protocols provide feedback to the \napplication as to the bandwidth available, latencies and jitters, \ntogether with an indication of the rate of data loss. Although RTP isn't \na transport protocol in itself, transport protocols can be build on top \nof it by using the information provided to do limited re-transmission \nwhere applicable, and to do some sort of rate based congestion control \n(e.g., clock packets out at the same rate the receiver receives them,\n  treat increased jitter as a sign of router queue instability, etc)\n\nSimon\n\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "On the audio/video/streaming tip - it should be noted that RealAudio is \nusing a proprietary protocol that sits on top of UDP to deliver their \n\"real-time\" audio data.  They haven't released any information about \ninternals, nor mentioned any plans to support caching or application \ngateways for proxies.  \n\nI'm not going to argue they should have done it in HTTP of course, but I \ndo wonder if HTTP should be expected to handle streaming, lossy media \ninstead of some other protocol.\n\nBrian\n\nOn Thu, 10 Aug 1995, Simon Spero wrote:\n> Audio data:\n> \n> 1) Audio data is linear, and real time: If a message containing \n> audio data fails to arrive within a given window, it has missed its\n> chance and is no longer wanted. Retransmission is not helpful- \n> the only way to deal with data loss is via regeneration from \n> other packets (I think Christian has done some work on this?)\n> \n> 2) Audio data rates are adaptable: If the bandwidth available to \n> the sender decreases, the data rate can be reduced by increasing \n> compression ratios (MPEG), or by switching to a different \n> encoding scheme (ADPCM, etc). TCP does not make bandwidth information\n> available to the application, and does not allow data that has been\n> sent but not delivered to be recalled. \n> \n> \n> Real Time Transport protocols.\n> \n> A better approach for these particular media types is to use some sort of \n> Real Time Protocol, such as RTP. These protocols provide feedback to the \n> application as to the bandwidth available, latencies and jitters, \n> together with an indication of the rate of data loss. Although RTP isn't \n> a transport protocol in itself, transport protocols can be build on top \n> of it by using the information provided to do limited re-transmission \n> where applicable, and to do some sort of rate based congestion control \n> (e.g., clock packets out at the same rate the receiver receives them,\n>   treat increased jitter as a sign of router queue instability, etc)\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: StateInfo is too fragile..",
            "content": "Bob Wyman <bobwyman@medio.com> wrote:\n  > re: David Kristol's proposal: \n  > \n  > I can't help thinking that the mechanism you propose is simply too fragile.\n  > \n  > Your proposal only works when the client's access pattern is limited to a\n  > cooperating set of URLs. This will probably typically be only those URL's\n  > which are explicitly embedded in the HTML of other members of the set. This\n  > might have a better chance of working in a system like gopher where the\n  > natural navigation paradigm is one of walking up and down hierarchical links\n  > , but Web clients have a much looser style of interaction. The Back button,\n  > History lists, and the ability to jump to links which arrive in mail\n  > messages, etc. means that users are often in multiple threads during a\n  > single session. (i.e. while considering what books to buy, they may follow a\n  > link found in a mail message which leads to a book review which is NOT one\n  > of the \"cooperating\" URL's on the relevant server. Doing so would break the\n  > State-Info chain.)\nI disagree.  But I would agree that a given origin server has to\npresent a consistent picture with respect to State-Info.  For example,\nif it receives a State-Info header for a page that doesn't require it,\nit should probably reflect the header back to the client.  However, if\nthe user bounces around among a bunch of threads on many servers, the\nuser agent should still keep track of State-Info for ongoing sessions\nand pass them to each server when appropriate.\n  > \n  > The fragility of the system will probably result in a great deal of\n  > consternation and confusion among naive users. At first, they probably won't\n  > understand the problem. Later, these users will learn that the sequence in\n  > which they access pages can effect the outcome of a session. They are likely\n  > to generalize this knowledge broadly and then become much more rigid in\n  > their overall usage patterns and so use the Web like a gopher server... \n  > \n  > We may see client writers who attempt to help users by warning them with\n  > dialog boxes that say: \"You are jumping to a URL not referenced on the page\n  > you are viewing. This could terminate your current session.\" Ugly...\nI think you may have misunderstood something in the proposal.  If you\nview a page from server A that has State-Info associated with it, and\nthat has a link to server B on it and you follow that link, the user\nagent hasn't lost the State-Info for server A.  If you enter a URL for\nserver A explicitly, or follow a link back to it, or back up in your\nuser agent and follow a different link that points to server A, the user\nagent should pass the remembered server A State-Info to server A as\npart of the request.  I don't see a source of confusion there.\n  > \n  > It would appear that HTML writers of pages that carry State-Info would have\n  > to be careful about keeping users from \"wandering\" during sessions. They\n  > would have to stop the common practice of locating links to the site home\n  > page on each of their pages since unless that home page was wired to always\n  > reflect back any State-Info, the State-Info would be lost and the user\n  > couldn't resume the \"shopping\" session by \"backing\" back into the session..\n  > The practice of \"reflecting\" State-Info which is not understood could grow\n  > widely...\nSee my remarks about navigation above.\n\nWhat assumptions are you making about how an origin server works?  It\nappears you believe that the State-Info is somehow embedded in the\nHTML.  I have nowhere specified how a server decides when or how it\nreturns Server-Info or what the Server-Info looks like.  I have only\nspecified the interface behavior.\n  > \n  > It also appears that restarting a session, once it is broken, could be\n  > difficult in the presence of caches. Unless the origin server provides tight\n  > \"expires\" headers or uses the no-cache pragma, it is likely that pages which\n  > start a session will get cached. (By \"start a session\" I mean the page which\n  > is the first to send back a State-Info: header.) Thus, if the user attempts\n*Pages* don't send back Start-Info headers.  *Servers* do.  The proposal says\nquite explicitly that State-Info headers are not cached.\n  > to reload such a page in order to \"restart\" a broken session, they will be\n  > disappointed. Of course, the same problem occurs for a second user who\n  > accesses the same \"starting page\" from cache. They won't get a session. You\n  > could fix this by requiring that the\"starting\" pages in a session are always\n  > non-cacheable...An alternative would be to require that the cache remembered\n  > that State-Info: had been present on a response and then generate\n  > conditional Gets with null State-Info: if a request for the cached page\n  > arrives with no State-Info: header.  (NOTE: I accept that your caching\n  > algorithm works properly in many other circumstances .i.e. when \"starting\" a\n  > session isn't the issue.)\nI thought the proposal explained all this pretty clearly, but I guess\nit didn't.  What you worry about here is a non-problem.  Proxies merely\npass State-Info in both directions, even if a page is cached.  So the\norigin server always provides a proxy with the correct State-Info\nresponse to give to its client.  The State-Info header is never\ncached.\n  > \n  > How should a client behave if it gets a \"503 Service Unavailable\" response\n  > which includes a \"Retry-After\" header to a request which had a State-Info\n  > header? Your proposal would seem to indicate that this would terminate the\n  > \"session\" since it is a response from the server which carries no State-Info\n  > . However, this may not be the intent of the server or the author. Similar\n  > problems occur with \"504\", \"409\", \"408\", \"401\", \"402\", \"301\", etc...\nAhh, now you have identified missing pieces of the proposal.  I have not\ndescribed how to handle various error conditions.  Clearly the intent is\nto sustain a session where possible.  However, defining the conditions\nfor that may be tricky.  I'll add to the proposal accordingly.\n  > \n  > The Netscape \"cookie\" stuff doesn't seem to have as much fragility as your\n  > proposal. The \"path\" attribute of a cookie allows users a great deal of\n  > freedom in the order in which they browse pages -- they trade increased\n  > client complexity against increased user-perceived system complexity. The\n  > \"cookie\" proposal appears to have the same problem with \"starting\"  a new\n  > session through a cache although \"restarting\" isn't a problem. The \"cookie\"\n  > proposal does not seem to have problems with HTTP error messages.\n\nI don't know whether cookies are more or less fragile.  People use\nthem, and they appear to work.  They're just not standardized.\n\nI think State-Info can effect \"path\" functionality.  In any case I\ndon't think that has any effect on the \"freedom in the order\" people\nbrowse pages, as I explained earlier.  I don't think there is any\n\"starting\" problem for sessions through a cache.  I believe cookies\ntolerate errors because their state information has a lifetime that is\nindependent of transactions.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "At 2:25 PM 8/10/95, Simon Spero wrote:\n>Jeff is right on the money here- however there are a few interesting\n>points that affect the decision as to which transport to use for general\n>HTTP transactions.\n>\n>The key point to note is that the vast bulk of data currently transferred\n>over HTTP has transport needs that don't correspond to the features\n>offered by TCP, and using strategies can greatly improve performance and\n>congestion characteristics.\n[...]\n>Real Time Transport protocols.\n>\n>A better approach for these particular media types is to use some sort of\n>Real Time Protocol, such as RTP. These protocols provide feedback to the\n>application as to the bandwidth available, latencies and jitters,\n>together with an indication of the rate of data loss. Although RTP isn't\n>a transport protocol in itself, transport protocols can be build on top\n>of it by using the information provided to do limited re-transmission\n>where applicable, and to do some sort of rate based congestion control\n>(e.g., clock packets out at the same rate the receiver receives them,\n>  treat increased jitter as a sign of router queue instability, etc)\n\nI'm skeptical that rebuilding quasi-reliable protocols on top of UDP or\nsome such is a good idea for long-haul transport. It also making writing\nclients a lot more work, as I recall this was a problem that limited the\ndevelopment of \"archie\" clients.\n\nAnother consideration is that people administering firewalls seem to hate\nUDP because its lack of state makes filtering less reliable.\n\nDoes IPNG or any of the protcol designs related to ATM networking address\nany of these issues? (I'm thinking if we can let someone else re-enginer\nthe lower parts of TCP/IP it could save some work.)\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "The major point, which has to be reiterated, is that protocols\nthe Web uses must implement congestion control.  While\nwhat you say is all true (about images not needing in order delivery,\nif you are willing to tolerate \"interesting\" display while arriving),\nwidespread deployment of software such as the WWW that does not\nobserve congestion control algorithms can cause the widespread collapse\nof the Internet, as happened in the 1980's.  I, among others, still\nhave scars.  And the Web has not (yet) killed the Internet precisely\nbecause it has observed congestion control (by being layered on TCP).\n\nIt is one thing if a uncontrolled protocol is .1% of the internet; it is\nquite another for one which now represents such a large fraction of total\ntraffic.\n\nYou do raise an interesting point, true for some datatypes.\nAudio data only needs to be real-time for teleconferencing; most\napplications don't demand low latency operation.  But for some datatypes,\n(such as audio) you might want to be able to change compression/quality on the\nfly, if you find your link is not able to provide sufficient bandwidth\n(which may vary on a relatively short timescale).  Provision for this\nin NG may be a good thing; but the actual transport of the data better\nobserve congestion control.  This implies algorithms very much like those\nthat TCP already provides.\n- Jim\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "Shel Kaphan <sjk@amazon.com> wrote:\n  > Dave Kristol writes:\n  >  > koen@win.tue.nl (Koen Holtman) wrote (on www-talk):\n  >  >   > Dave Kristol:\n  >  >   > [....]\n  >  >   > >http://www.research.att.com/~dmk/session.html\n  >  >   > \n  >  >   > This proposal is not clear enough about caching. Specifically:\n  >  >   > \n  >  >   >   is the session-id header in the request part of the cache key for the\n  >  >   >   entity in the response?\n  >  > No.  In section 2.3 I said:\n  >  >     Similarly, a caching proxy must pass back to the requestor any\n  >  >     Session-ID response header it receives, but it must not cache that\n  >  >     header as part of its cache state.\n  >  >   > \n  >  >   > If it is, this means that almost no meaningful caching is possible for\n  >  >   > services using session-id, even if 99% if the entities in the session\n  >  >   > (inline pictures, product description pages) do not depend on the session\n  >  >   > state.\n  >  > Yes, exactly.\n  > \n  > However, please note that the \"side channel\" of state information that\n  > flows both directions and bypasses proxy and user-agent caches, even\n  > if the resources themselves are cached, is not cheap.  Setting up and\n  > tearing down the TCP connections is a nontrivial fraction of the cost\n  > of retrieving a small resource (but I admit: I don't have numbers) --\n  > especially html files, as opposed to large graphics or audio media files.\n  > And those media files are typically cacheable anyhow.  Even on systems\n  > where URLs contain session-IDs, the URLs for the media files usually\n  > need not, and so they're cacheable.\n\nI have assumed (erroneously?) that a caching proxy must send\nconditional GETs to the origin server.  If so, there's already the cost\nof a connection.  The State-Info (previously \"Session-ID\") can ride the\nrequest almost for free.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "Excerpts from ext.ietf-http: 10-Aug-95 Re: UDP or TCP? Albert\nLunde@nwu.edu (1794*)\n\n> Does IPNG or any of the protcol designs related to ATM networking address\n> any of these issues? (I'm thinking if we can let someone else re-enginer\n> the lower parts of TCP/IP it could save some work.)\n\nYes, some of these concerns are addressed.  In particular, the quality\nof service issues are interesting, allowing you to specify particular\ncharacteristics of a connection.  Both IPng and ATM are big research\ntopics here at PARC, and we're going to be experimenting with them in\nILU over the next year.\n\nBill\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "    The key point to note is that the vast bulk of data currently\n    transferred over HTTP has transport needs that don't correspond to the\n    features offered by TCP, and using strategies can greatly improve\n    performance and congestion characteristics.\n    \nOne must be careful to distinguish between the desires of an\nindividual user and the needs of a network of 100 million users.\nI would be quite cautious about doing anything that improves\nperceived performance for a single user if that also destabilizes\nthe Internet (since all users ultimately suffer if the network\ncollapses).\n\nFor example, while it is true that inline images might be partially\nrendered faster if the transport protocol doesn't try to preserve\nordering, except on very slow clients I can't imagine that latency\nfor full rendering of an image would be significantly improved by\nout-of-order delivery.\n\nAnd while it is possible to code images so that loss of a packet\ndoes not make the image unreadable, this may conflict with the\ngoal of compressing images as much as possible.  (I don't know,\nI'm not an image-coding expert.)\n\nAnyway, most of the inlined images I see are small enough that even\na single-packet loss is likely to make them pretty useless.\n\nBut mostly I'm concerned that we could micro-optimize the protocol\nfor rapid partial rendering of images in a lossy environment, and\nin doing so fail to attack the principal reason for packet loss\nin the (wired) Internet, which is congestion.  I'd much rather\nkeep HTTP simple, and figure out how to make the best possible\nuse of our existing transport protocol.\n\nAs for real-time delivery of continuous media: I agree with Brian\nBehlendorf, if when he writes \"I do wonder if HTTP should be expected\nto handle streaming, lossy media instead of some other protocol,\" he\nmeans that HTTP should not be all things to all applications.\n\nThat is, we should think of HTTP as a way to move non-realtime objects\nfrom place to place, and develop other mechanisms (such as the\nwork being done in the context of the MBone) for providing real-time\nconnectivity.  The Web is great, but that doesn't mean it should be\ncoerced to do everything.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "On Thu, 10 Aug 1995, Jeffrey Mogul wrote:\n\n> For example, while it is true that inline images might be partially\n> rendered faster if the transport protocol doesn't try to preserve\n> ordering, except on very slow clients I can't imagine that latency\n> for full rendering of an image would be significantly improved by\n> out-of-order delivery.\n\nIt does make a visible difference (and not just in the sniffer window :-) \nI see this effect all the time, when the rendering stops halfway down, \nthen suddenly shoots ahead when the lost packet and all it's followers \nare delivered at once\n\n> That is, we should think of HTTP as a way to move non-realtime objects\n> from place to place, and develop other mechanisms (such as the\n> work being done in the context of the MBone) for providing real-time\n> connectivity.  The Web is great, but that doesn't mean it should be\n> coerced to do everything.\n\n(If I were in emacs, I'd convert that entire region to upper case, bold \nitalics with symphonic accompaniment. This is absolutely key.)\n\nMore and more, I'm starting to thing of HTTP* not so much as \na transport protocol but as a hypermedia control protocol that also \ntransfers data. For shipping around bulk data that needs reliable \ndelivery, it's hard to do much better than TCP (T/TCP + Window scales \nexcepted.), as long as the connection is re-used. What I'm arguing here \nis that most web traffic by volume has different, sometimes weaker \nrequirements than those provided by TCP. This can be used to give \nimproved performance *AND* improved congestion characteristics. \n\nFor example, adapting transmission rates to the rate at which packets are \nbeing sucesfully delivered to the recipient, rather than the rate at \nwhich acks are received at the sender can give finer grain congestion \ncontrol by preventing phenomena such as ack compression. Not \nretransmitting every lost packet saves bandwidth, and makes routers with \nrandom drop less painful. Smart use of latency changes can even be used \nto predict congestion before it occurs (jitter is pretty much a function \nof changes in queue length along the path- a steady increase in latency \nimplies a queue that's filling up). \n\nA lot of the work that's been done for the mbone can be reused here; RTP \nand to a lesser extent RTCP can be borrowed wholesale. \n\nIn summary:\n\nUsing something other than TCP for basic HTTP operation on HTML \nfiles, or other similar object types is almost always a lose (unless you \nreimplement TCP from scratch). Using specialised transports for \nspecialised media can be a tremendous win.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": ">is that most web traffic by volume has different, sometimes weaker \n>requirements than those provided by TCP. This can be used to give \n>improved performance *AND* improved congestion characteristics. \n\nMakes sense to me.\n\nI'd like to know why congestion control can only be done on a TCP-connection\nbasis and not a host-basis?  Seems to me you just need a few new ICMP\npdu's.\n\n>Using something other than TCP for basic HTTP operation on HTML \n>files, or other similar object types is almost always a lose (unless you \n>reimplement TCP from scratch).\n\nThat's pretty strong.  What's your basis for this claim?\n/r$\n\n\n\n"
        },
        {
            "subject": "RE: UDP or TCP",
            "content": "----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] To: Paul Leach\n] Cc:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: UDP or TCP?\n] Date: Thursday, August 10, 1995 9:59AM\n]\n]     Also, you can more than halve the size of the spec you need to\n]     implement by just doing the the UDP version -- which, given that one of\n]     the problems with HTTP that everyone talks about is the overhead\n]     incurred from opening TCP connections for one-off transactions, seems\n]     like a good direction to go.  (If the normal interaction between client\n]     and server is idempotent, it will only take one round trip...)\n]\n] I think there's a misconception here.  Yes, there is some packet\n] overhead for creating and destroying TCP connections, and the current\n] one-request-per-connection model is expensive in that respect.\n]\n] But simply going to UDP is not the answer, because UDP (without\n] some extra effort) does not offer any congestion-control mechanisms.\n] Sure, you can layer them on top of UDP, but I suspect that you would\n] quickly get back to something very similar to TCP.\n\nAll RPCs I know of do the extra flow control thing. For RPC, it is very \nsimple in the basic request/response case, as request/response is \nnaturally flow controlled -- this is why you don't quickly get back to \nTCP for simple cases. When the in or out arguments are large, then you \ndo indeed have essentially the same problem as TCP.\n\n]\n] Since we're discussing what is already the dominant source of packets\n] in the wide-area Internet, and will surely keep growing, we need to\n] be very sensitive about how HTTP's design affects WAN congestion.\n] Otherwise, nobody is going to be very happy.\n\nAbsolutely agree.\n]\n] One major benefit of a persistent-TCP-connection model is that it\n] allows the packet sources to learn, over sufficiently long periods,\n] what the congestion state of the network is, and therefore adapt\n] to it.\n]\n] I should also point out that the average size of an HTTP retrieval\n] is well over one packet's worth of data (I don't have the stats handy,\n] but the number \"8 K bytes\" sticks in my head from someplace).  This\n] means that even for a single request per connection, the congestion\n] control mechanisms of TCP have some chance of being helpful.\n]\n] Paul, what does DCE RPC do for congestion avoidance and control\n] in WANs, when it uses UDP?\n\nThe protocol is designed to use slow start, sliding window, ack maps, \nand serial numbered packets.  I assume that people know about the first \nthree techniques. The last was suggested by Sidebottom's Rx protocol he \ndid at CMU for AFS.  Serial numbers allow detection of exactly what is \nbeing acked -- an original packet or a retransmission thereof, and can \navoid duplicate retransmissions on slow links.\n\nThe control of the sliding window is designed to allow the use of round \ntrip time and effective link speed estimates. It is also designed to \nallow really simple implementations, like stop-and-wait. I don't know \nhow much of the protocol capacity is exploited in any given \nimplementation -- MS, OSF, and OSF members' \"enhanced\" versions.\n\nPaul\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "Jim said:\n]\n] So I intend to spend some time looking around at what is out there that\n] may fulfill the requirements. (ILU from Xerox, and others that\n] may be suggested to me).  Butler Lampson also pointed out to me that the\n] exercise of looking around at existing systems will likely pay off in good\n] ideas worth stealing, even if the RPC system itself isn't appropriate \nto this\n] application. (If you can't steal good code, at least steal good \nideas; a good\n] motto, me thinks).\n] - Jim Gettys\n]\n  W3C\nI agree.  There are at least these four  alternatives available to us:\n1. Adopt an existing protocol and its implementation\n2. Adopt an existing protocol and adapt some existing implementation\n3. Adopt an existing protocol and do a new implementation\n4. Do a new protocol and a new implementation\n(And lots of others in between).\n\nI believe that they are listed in order of desirability.\n\nI don't think that the current DCE RPC _implementation_ has all the \nproperties that we want of an implementation, that you listed. That \nmeans that if one picked it as the protocol, option #1 is not \navailable. #2 might not be such a good idea either, but it might be OK \n-- there's at least a freely available source code base to start from. \nBut #3 still seems preferable to #4, which seems to a direction we \ncould easily head in.  In addition to avoiding the hard job of \ninventing a new protocol, it means that one can interoperate with \nexisting clients and servers that use the existing implementation, even \nif that implementation is crummy.  (On the other hand, protocol design \nis fun, so I'll be glad to help if that seems like the best alternative \nwhen all has been investigated.)\n\nOf course, if there is some other existing  protocol such that the both \nthe protocol and an implementation have all the needed properties, then \nthat's what we should do.\n\nAnd the idea of using something like ILU as the existing implementation \nto adapt seems like it has merit too -- from Bill's description, the \nway it exposes the protocol features to the app works better in a low \nspeed environment than the standard RPC programming model, which has \ntrouble with asynchrony and incrementality (although with the benefit \nthat the model is simpler to program, IMO).\n\nI guess what I'm doing is adding to Butler's maxim that in addition to \nstealing good ideas, one can steal good protocol designs, too.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "] * needs to support non-blocking or streaming (often called \"batching\")\n] of requests; if there is no error from a request and the request has\n] no return value, the system shouldn't generate network\n] traffic.  Round trips are the death of performance on a world\n] wide network, with round trip times often measured in hundreds\n] of milliseconds (or more).\n\nI presume that what you really mean is that round trips that you have \nto wait for are death.  If you want delivery to be reliable, then there \nhas to be an ack at some level.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "HTTP protocol requirements (was: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockholm",
            "content": "] DCE RPC has several other problems, the most important of which\n] is that it isn't available universally, and very difficult to make it so\n] quickly...\n\nWhat's the problem here?  I thought the source code base was easily \navailable. I understand that not all platforms already have it, but if \nit were part of the reference implementation of the browser that \nincorporated the next generation HTTP, I presume that that would \nquickly be a non-problem. (This is really an innocent question -- I \nhave not followed the actual status of the \"freely available\" DCE RPC \nsource code, and it would be entirely possible that I've been taken in \nby OSF hype...)\n\nThis should apply to any protocol/implementation we might choose, not \njust DCE. I wouldn't rule out any existing protocol or implementaiton \njust because it weren't widely deployed, if it otherwise met the requirements.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting,  Stockhol",
            "content": "Ted Hardie writes:\n\n| the Internet is over a very slow link.  While the URN to URL resolution\n| proposals provide a useful model for long-term work, some very simple\n| solutions could work for mirrorinng non-interactive pages even in the\n| short term.  A Mirrors: header, for example, could provide a simple list\n| of alternate URLS where the same data could be found (much like the lists\n| some ftp servers provide when user limits have been reached); the client\n\nAnother approach which I've seen suggested would be to have multiple \nHREFs in an HTML anchor.  This would be nice, in the sense that it \nwould drop straight through the HTTP layer...  Would it be kosher \nSGML, though ?\n\nAre there any plans to further develop the existing HTML & HTTP \n\"HEAD\" mechanisms for passing meta-information ?  Methinks it would \nbe good to have something along the lines of the \"Dublin Core\" \nelement set [1] in there.  Who knows, this approach might just make \nURCs redundant! :-)\n\n| could then analyze the list and choose mirrors which are closer.  While\n| it may be difficult to determine what is \"close\" and what is \"distant\"\n| in network terms, there are some rules which could be built into the\n| clients.  (Actually, a clever server could do the same thing with \n| redirects--read the client's network and domain from its headers and\n| issue a redirect to a closer mirror).\n\nThe client could ping the servers and choose the one with the shortest round-trip-time ?\n\nCheerio,\n\nMartin\n\n[1] <URL:http://www.oclc.org:5046/conferences/metadata/>\n\n\n\n"
        },
        {
            "subject": "Nonpersistent Cookie proposa",
            "content": "As promised on www-talk, here is a session state header proposal that,\namong other things, tries to address some of the potential caching\nproblems in the Session-ID (State-Info) proposal by Dave Kristol.\n\nThese caching problems are in the area of graceful degradation when\nstateful dialogs (for which Session-ID is a support mechanism) are\nused with caches that do not conform to the Expires header definition\nin the (draft) http spec.  Some people, particularly people who want\nto use stateful dialogs in applications where real money is involved,\nare very concerned about such non-conforming caches: the fear is that\ncustomers will blame them if the service does something inappropriate\nbecause of a broken cache, while it is really the fault of the cache\nadministrator.\n\nFor the record: I am not in the tele-shopping business myself.  I\ndon't know if the level of paranoia about non-conforming caches found\nthere is justified, but I fear that it just might be.  In my opinion,\nmore information about current practice, and informed speculation\nabout future practice, is needed to decide on this issue.\n\nThe material in the appendix about the Request-ID header at the end of\nthis message not new: I have sent it to www-talk (but not http-wg)\nsome weeks ago.  I have decided to include it here again, as there has\nbeen some discussion on merging facilities for stateful dialogs and\nfor clicktrail statistics recently, just as I thought that this topic\nwas dead on www-talk (the consensus being that this merging would be a\nbad idea because it would generate too much confusion).\n\nKoen.\n\n----snip----\n\n\nNon-persistent Cookie proposal\n==============================\n\n                                        Koen Holtman, koen@win.tue.nl\n\nThis document can be seen as a personal summary of some parts of the\nthe session-id discussion on www-talk in July 1995.  [2] contains a\nsummary of privacy issues discussed.  \n\nI do not want to argue that the proposal below is the only way to go\nwhen putting support for stateful dialogs in http.\n\nSimpler mechanisms are possible, for example [3].  The `solution\nspace' for stateful dialogs has a number of dimensions:\n\n - simplicity of implementation\n - time of general availability when standardized\n - downward compatibility\n - simplicity of use\n - reliability\n - amount of privacy protection\n - maximum complexity of stateful dialogs supported\n - amount of cache control possible\n - risks when used with non-conforming caches\n - amount of confusion on www-talk generated\n\nThe proposal below occupies one point in this solution space.\n\nThe main goal of the text below is to make all dimensions visible, not\nto give a personal opinion on what the best point in the space is.  In\nfact, my personal opinion on this has changed a lot over the last\nmonth, and I have not reached a final conclusion yet.\n\n\n** Background\n\nThis document assumes that you have read [1] and [2], and that you are\nfamiliar with [4].\n\n[1] the NetScape Cookie proposal\n<URL:http://home.mcom.com/newsref/std/cookie_spec.html>.\n\n[2] the proposals for of gathering consumer demographics\n<URL:http://www.w3.org/hypertext/WWW/Protocols/demographics.html> by\nDaniel W. Connolly.\n\n[3] Session-ID (State-Info) proposal by Dave Kristol\n<URL:http://www.research.att.com/~dmk/session.html>\n\n[4] The draft HTTP/1.0 specification\n<URL:http://www.ics.uci.edu/pub/ietf/http/>\n\n\n** Terminology\n\nMost terminology in this document is as in [4].  New terminology:\n\nBrowser: user agent (used for interactive web sessions).\n\nStateful dialog: An information exchange between a web user and a web\nserver that that extends beyond the submission of one form.  In a\nstateful dialog, the server changes its behavior as a result of\nprevious actions by the user.  Conceptually, the state is a property\nof the dialog (or session), not of the browser or server.  On the\nimplementation side, either the browser, the server, or both hold the\nstate.\n\nCookie: a string to be sent by a browser to a server in requests,\nrepresenting the state in a stateful dialog.  The string is kept at\nthe browser side, but supplied and updated by the server using\nset-cookie response headers.  Only the server need to be able do\ndecode the cookie.  See [1] for an example cookie definition.  A\ncookie can represent the entire dialog state directly, or be a key in\na server-side dialog state database.\n\nRequest-id: a unique value sent by a browser to a server in requests\nto allow the server to generate better clicktrail statistics. \n\nSession-id: a unique identifier sent by a browser to a server in\nrequests to give the server a way of keeping it apart from other\nbrowsers for the purpose of engaging in stateful dialogs.  Unlike a\ncookie, the value of a session-id can never change during a stateful \ndialog.\n\nPersistent information: information that is remembered by the browser\nfor future sessions\n\nNon-persistent information: information that is lost when the browser\nexits\n\n\n\nProposal:  Non-persistent cookies\n=================================\n\n** 1. The set-cookie response header\n\nA server may choose to send a set-cookie response header in a response \nto a browser.  The header looks like\n\n   Set-Cookie: <string>\n\nwhere <string> does not contain whitespace. (This allows for future \nextensions).  Examples:\n\n   Set-Cookie: color_screen;no_jpgs;small_graphics\n   Set-Cookie: Joe%20User;joe@foo.com\n   Set-Cookie: id324254\n   Set-Cookie: joe:4234982343\n\nIf the browser receives a Set-Cookie response header, it can either\nhonor it or ignore it.\n\n** 2. Honoring a Set-Cookie header\n\nTo honor a set-cookie header received from a server, a browser will \nstart including a header\n\n   Cookie: <string>\n\nin _direct_ requests to that server, and to that server only.  For the\npurpose of this definition, individual servers are identified by the\nhostname+portnumber pair in the request URL.  Directness of requests\nwill be defined later.\n\nThe cookie <string> is taken from the Set-cookie header, and may be\nchanged by the server by sending a new Set-cookie header with a new\nstring.  A Set-cookie header with an empty string can be taken as a\nrequest to stop sending Cookie headers.\n\n\n** 3. The decision to honor a Set-Cookie header\n\nIf a Set-Cookie response header is honored, this means that a server\ncan get more accurate statistics about the behavior of the browser\nuser if the browser user is behind a firewall or proxy cache.  Thus,\nthe user should have the option of deciding not to honor Set-cookie\nheaders for privacy reasons.\n\nIt is suggested that browsers provide something like the following\npreferences box:\n\n   +-----------------------------------------------------------------+\n    Honor set-cookie requests:\n        ( ) Always honor request\n        ( ) Start honoring requests if one is done in a response to\n            a form submission (POST request).\n        (*) Ask once for every site, use reply in later sessions\n        ( ) Never honor requests\n   +-----------------------------------------------------------------+\n\nwhere the (*) is the default setting.  In real browsers, the\nterminology used in this box should probably be adapted to make more\nsense to the average user.\n\nAs a matter of etiquette, services should only send Set-Cookie headers\nif honoring them would bring some direct advantage to the user, like\nbeing able to use a stateful dialog.  Sending Set-Cookie headers only\nto get better clicktrail statistics should be considered a breach of\netiquette.  To get clicktrail statistics, the Request-id header (see\nthe appendix below or [2]) should be used.\n\nIf a browser decides to honor one Set-Cookie header from a server, the\nservice author can expect that Set-Cookie headers sent in the near\nfuture will also be honored.\n\nA browser can contain a timeout mechanism to stop honoring Set-cookie\nheaders when, say, 30 minutes have past since the last contact with\nthe server.\n\n\n** 4. Direct and Indirect Requests\n\nA request is indirect if it\n 1) fetches the contents of an inline picture or other inlined object\n 2) resolves a 3xx (redirection) response\n\nAll other requests are direct.\n\nIf a browser is to honor a Set-Cookie header, Cookie headers must be\nsent in _direct_ requests to the server.  It is preferred that Cookie\nheaders are never sent in indirect requests to a server.\n\nThe distinction between direct and indirect requests is made for two\nreasons:\n\n a) this allows for better caching of stateful dialogs, as discussed\n    below\n b) this allows for better privacy: it makes impossible some\n    `stealthy' cookie matching strategies that could be adopted by\n    cooperating web service providers to allow matching clicktrails.\n\n\n** 5. Cookie headers and caching: the default\n\nBy default, a cache, no matter whether it is in a browser or in a\nproxy, must never cache responses to requests with Cookie headers in\nthem.\n\nResponses which contain Set-Cookie headers must also never be cached,\nno matter whether the request itself contained a Cookie header.\n\nThere are three reasons for this default\n\n 1) Responses in stateful dialogs are dynamic by nature.  No big\n    payoff can be expected from caching them.  In fact, in a scheme\n    where they can be cached, there is a danger of cache memories\n    dropping useful `static' responses (like inline pictures from\n    normal sites) to store relatively useless dynamic responses.\n\n 2) It is vital that responses in stateful dialogs are never cached,\n    else services using stateful dialogs would become unreliable.\n\n    An Expires: <yesterday> header in a response can be used to\n    disable caching (in both browsers and proxy caches), but some\n    system operators may be tempted to `tune' proxy or browser caches\n    under their control to keep expired responses around for, say, 5\n    minutes, even though this makes the cache non-conformant to the\n    http spec.\n\n    Such `tuning' is relatively harmless for normal, non-interactive\n    services, but disastrous for stateful dialog reliability.  As\n    stateful dialogs are still relatively uncommon, it is a valid\n    assumption that many system operators are not aware of the\n    stateful dialog risks involved with `Expires tuning', so\n    non-conforming caches may remain with us for some time to come.\n\n    By making responses in stateful dialogs using the Cookie headers\n    a special case in the cache algorithm, independent of `Expires\n    tuning', this particular non-conformance risk to stateful dialogs\n    is eliminated.\n\n    (Note: the Session-ID (State-Info) proposal by Dave Kristol [3]\n    assumes that system operators will never do such `tuning': this\n    allows proposal [3] to be very much simpler than this cookie\n    proposal.)\n\n 3) The developer of a stateful dialog service will usually not have a\n    cache between his browser and the CGI scripts under development.\n    If caching were enabled by default on requests with Cookie\n    headers, the author would have to remember to put Expires:\n    <yesterday> headers in the responses generated by the scripts; if\n    a forgetful or badly informed author of a stateful dialog service\n    would not do this, the resulting unreliability would not show up\n    on tests within the local environment.\n\n\nThis default behavior of _not_ caching responses to requests with\nCookie headers is the main reason why indirect requests, which do not\nget Cookie headers, were introduced.  An inline picture request is\nindirect, so inline pictures on stateful dialog pages will get cached\nby default.\n\nOf course, if caching of an inline picture is not desirable, the\nservice author can always put an Expires header in the inline picture\nresponse.\n\nIf a page contains pictures that depend on the dialog state, the\nservice author can implement these state dependent pictures by making\nthe generation of the URLs in the <IMG SRC=...> tags depend on the\nstate.  Similar state-dependent URL generation can be done for\nredirection (3xx) codes.\n\n\n** 6. Cookie headers and caching: overriding the default\n\nIf the response to a request on URL U with a Cookie header contains an\n\n  Expires: <date>\n\nheader, and no Set-Cookie header, a cache can interpret this to mean\ntwo things:\n\n 1) the entity included may be cached, but not beyond the\n    <date> given,\n 2) the entity in the response does *not* depend on the dialog state.\n    Thus, if the entity is cached and some browser does a request on\n    URL U, it is OK to serve the cached entity, no matter what the\n    cookie header value in the request is, even no matter whether a\n    cookie header is present at all in the request.\n\nNote that it makes no sense for servers to send both a Set-Cookie\nheader and an Expires header in the same response.  Servers may\nhowever choose to do so to get backwards compatibility with old\nproxy caches.\n\nAlso note that it makes no sense to put Expires: <yesterday> headers\nin responses to requests which contain Cookie headers.  Servers may\nhowever choose to do so to get backwards compatibility with old proxy\ncaches.\n\nThis way of defining Expires: semantics ensures that caches need never\nconsider the cookie header when accessing their cache memory.  The\nCookie header is never part of the cache key, the header is only\nimportant when making the decision whether to cache or not.\n\n\n\n==========================\n\n\nAPPENDIX: Proposal: The Request-ID: header field.\n---------------------------------------------------\n\nTo write the text below, I took proposal I. from [2]:\n<URL:http://www.w3.org/hypertext/WWW/Protocols/demographics.html>, and\nchanged things according to issues (mainly connected to privacy and\ncaching) discussed in the Session-id threads on www-talk.\n\nThe text below can be seen as a personal summary of the parts of\nthese threads that pertain to Request-IDs and privacy.\n\n\nThe Request-ID: header field.\n\n   Adapted from the proposal in\n   <URL:http://www.w3.org/hypertext/WWW/Protocols/demographics.html>.\n\n   Am HTTP request may include a header field of the form:\n\n        Request-ID: $session $request++\n\n   e.g.\n\n        Request-ID: 342%33a4d443 12\n\n   The HTTP client chooses a random string as a \"session identifier\",\n   and each request in that session is identified by a number that\n   increases monotonically with time.\n\n   It is suggested that clients use a different random $session string\n   for each server they talk to.  This will make it more difficult for\n   cooperating web service providers to match clicktrails in their\n   logfiles, thereby getting user profiling information that is much\n   more accurate than the user would want to give them without some\n   form of compensation.  Note that it is illegal to match logfiles\n   under the privacy laws in some countries.  The suggestion to use\n   different $session strings can be seen as supporting these laws by\n   making the crime of matching logfiles pay off less.\n\n   A \"session\" is not formally defined (other than \"a set of requests\n   with the same $session id\"), though I suggest that browsers begin a\n   session when they are invoked and when they have been idle for 30\n   minutes or more, and allow some user interface to say \"start a new\n   session\" (i.e. \"choose a new random session ID\").\n\n   Each user agent must provide a mechanism to turn the generation of\n   Request-Ids off, especially for site security administrators that\n   prohibit its use.\n\n   If no Request-ID headers are present, this should be interpreted by\n   web service providers as a statement that the user does not wish to\n   reveal his or her exact clicktrail for privacy reasons.  An attempt\n   by service providers to silently obtain the clicktrail by some\n   other means (for example by using a session-id, cookie, or\n   anonymous authentication mechanism that could be part of future\n   versions of HTTP), should be considered to violate the privacy\n   wishes of the user.\n\n   Whether HTTP clients use a global $request counter, or one counter\n   for each server talked to, is up to the clients.  HTTP clients\n   which are not traditional user agents (e.g. multi-threaded robots)\n   may use several sessions in parallel.\n\n   A proxy must pass the Request-ID: header through unmodified. One might\n   consider some sort of Proxy-Request-ID, though I doubt it would be\n   valuable.\n\n   An HTTP cache can assume that the response to an HTTP request does\n   _not_ vary as a function of the Request-ID.  That is, an HTTP proxy\n   need not include the Request-ID in its \"cache key.\"  If the\n   response to a request can vary, an Expires header should be used in\n   the response to reflect this dynamism.\n\n   It is preferred that the request-ID header is _not_ used to\n   implement stateful dialogs, in which the content of pages is\n   different for different sessions.  For stateful dialog support,\n   other mechanisms (for example a session-id, cookie, or anonymous\n   authentication mechanism that could be part of future versions of\n   HTTP) should be used.\n\n\nAlternative proposal:\n\n  Instead of introducing a new Request-ID: header, include the \n\n     $session $request++\n\n  information in the From: header.  Examples:\n\n   From: (#342%33a4d443 12)\n\n   From: \"Roy T. Fielding\" <fielding@beach.w3.org> (#342%33a4d443 12)\n\n\n\n======================\n\n\n\n"
        },
        {
            "subject": "RE: HTTP version 1.0 or 1.1",
            "content": "I like it -- it would be nice to have a good spec for what to do to \ninteroperate with the current deployed browsers/servers (warts and all...).\n\nI'd have second thoughts if you said it would take a long time, but if \nit can really be accomplished in a few days, and you're willing to do \nthe work, I'd concur.\n----------\n] From: Roy Fielding  <fielding@beach.w3.org>\n] To:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: HTTP version 1.0 or 1.1?\n] Date: Friday, August 11, 1995 11:46AM\n]\n] I have had several comments sent to me (most off the list) along the\n] lines of \"how can we keep changing 1.0 when there are deployed apps?\"\n] and \"how can I know whether a server supports the standard or not if\n] we continue to use the same version number?\", etc.\n]\n] As we mentioned back in San Jose, we have a choice as to whether\n] we try to standardize 1.0, or release a current-practice document on\n] 1.0 and move our standard to HTTP/1.1 (which in turn would mean that\n] what we are currently calling 1.1 features would be in HTTP/1.2).\n]\n] Since the IETF recently created the BCP (\"Best Current Practice\")\n] series, we can actually do this now within a reasonable framework.\n]\n] The action required would be for Henrik and I to create a stripped-down\n] HTTP/1.0 document and release it along with the current draft renamed\n] as HTTP/1.1.  This will take three days for us to accomplish, but may\n] be worth the saving in confusion and aggravation.\n]\n] Does the WG want to do this?\n]\n]\n]  ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n]                       Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n]                       (fielding@w3.org)                (fielding@ics.uci.edu)\n] \n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting,  Stockhol",
            "content": "Martin Hamilton <martin@mrrl.lut.ac.uk> Writes:\n \n> Are there any plans to further develop the existing HTML & HTTP \n> \"HEAD\" mechanisms for passing meta-information ?  Methinks it would \n> be good to have something along the lines of the \"Dublin Core\" \n> element set [1] in there.  Who knows, this approach might just make \n> URCs redundant! :-)\n\nYes there are ! But not exactly on those lines; as HEAD is the header\nof the HTML document (and intended to make the RENDERING more usefull)\nI am very strongly in favour (and have implemented) a new META tag.\n\nTo return to an earlier discussion; people wanted to have meta data\nin the head, mainly because it was easy to enter there in some kind\nof simple HTML. but were quite concerned that the meta data would\nbreak/show up in excisting clients. A problem further augmented by\nthe fact that the default for 'new' tags is 'show' for the browser.\n\nTherefore I use a new META mechanism, *but* the actual information\nis simply in the normal <head>..</head> section, as a <meta>..</meta>\nsectoin... but this is stripped out when there is a GET. POST or HEAD\nrequest. This allows easy editing/adding with excisting editors/tools.\n\nThe 'meta' output format depents on the Accept-type negotiation, and currently\nonly support urc0, Aliweb, html and some SGML+DTD format.\n\nThe above are more or less 'carrier' formats; I try to adhere to the\ndublin-core article and the aliweb RFC for the information I encode\nand there conceptual meaning.\n \nDw.\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "    ] But simply going to UDP is not the answer, because UDP (without\n    ] some extra effort) does not offer any congestion-control mechanisms.\n    ] Sure, you can layer them on top of UDP, but I suspect that you would\n    ] quickly get back to something very similar to TCP.\n\n    All RPCs I know of do the extra flow control thing. For RPC, it is\n    very simple in the basic request/response case, as request/response\n    is naturally flow controlled -- this is why you don't quickly get\n    back to TCP for simple cases. When the in or out arguments are\n    large, then you do indeed have essentially the same problem as\n    TCP.\n    \nFlow control is not congestion control.\n\n    ] Paul, what does DCE RPC do for congestion avoidance and control\n    ] in WANs, when it uses UDP?\n\n    The protocol is designed to use slow start, sliding window, ack\n    maps, and serial numbered packets.\n\nSlow start (depending on exactly how it is done) will help with\ncongestion.  It may not be a full solution.\n\n    I don't know how much of the protocol capacity is\n    exploited in any given implementation -- MS, OSF, and OSF members'\n    \"enhanced\" versions.\n    \nIf congestion control is not a mandatory part of the implementation,\npresent in all cases, it would be unwise to deploy this as the basis\nfor the Web.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Draft Minutes of HTTP Working Group, 33rd IETF Meeting, Stockhol",
            "content": "    I agree.  There are at least these four  alternatives available to us:\n    1. Adopt an existing protocol and its implementation\n    2. Adopt an existing protocol and adapt some existing implementation\n    3. Adopt an existing protocol and do a new implementation\n    4. Do a new protocol and a new implementation\n    (And lots of others in between).\n    \n    I believe that they are listed in order of desirability.\n    \nWill someone please try to explain why we need something besides\nTCP?  In particular, what problem is RPC intended to solve?\n\nThe \"excess overhead packets\" problem melts away with persistent\nconnections; the extra 6 or 7 packets and one RTT are quickly\namortized by the high locality of requests.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "    I'd like to know why congestion control can only be done on a\n    TCP-connection basis and not a host-basis?  Seems to me you just\n    need a few new ICMP pdu's.\n\nThe debate about congestion avoidance and control makes the arguments\non this mailing list look tame.  There's quite a lot of literature\non the topic, which I won't attempt to summarize.\n\nBut I will point out that the reason why congestion avoidance and control\nis done *today* on a TCP-connection basis is because it's really the\nonly thing that actually works.  There is, in fact, an ICMP message\ntype (\"Source Quench\").  But the consensus seems to be that this is\nnot a good idea.  To quote from RFC1716:\n\n            Research seems to suggest that Source Quench consumes\n            network bandwidth but is an ineffective (and unfair)\n            antidote to congestion.  See, for example, [INTERNET:9] and\n            [INTERNET:10].  Section [5.3.6] discusses the current\n            thinking on how routers ought to deal with overload and\n            network congestion.\n\nINTERNET:9.\n     A. Mankin, G. Hollingsworth, G. Reichlen, K. Thompson, R.  Wilder,\n     and R. Zahavi, Evaluation of Internet Performance - FY89, Technical\n     Report MTR-89W00216, MITRE Corporation, February, 1990.\n\nINTERNET:10.\n     G. Finn, A Connectionless Congestion Control Algorithm, Computer\n     Communications Review, vol. 19, no. 5, Association for Computing\n     Machinery, October 1989.\n\nAs to doing congestion control on a \"host basis\": this can't work.\nIn general, it's not hosts that get congested (flow control solves\nthat), it's *paths* that get congested.  And congestion is a highly\ndynamic condition; it can occur or clear up on short time scales.\n\nSome implementations apparently cache \"hints\" about the path to\npeer hosts between TCP connections, but this does not relieve\nthe TCP implementation from the responsibility to detect and\navoid congestion at any time.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re:  Nonpersistent Cookie proposa",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  > [... concerned about caching issues with State-Info (Session-ID) ...]\n  > \n  > Non-persistent Cookie proposal\n  > ==============================\n  > \n  >                                         Koen Holtman, koen@win.tue.nl\n  > \n  > This document can be seen as a personal summary of some parts of the\n  > the session-id discussion on www-talk in July 1995.  [2] contains a\n  > summary of privacy issues discussed.  \n  > \n  > I do not want to argue that the proposal below is the only way to go\n  > when putting support for stateful dialogs in http.\n  > \n  > Simpler mechanisms are possible, for example [3].  The `solution\n  > space' for stateful dialogs has a number of dimensions:\n  > \n  >  - simplicity of implementation\n  >  - time of general availability when standardized\n  >  - downward compatibility\n  >  - simplicity of use\n  >  - reliability\n  >  - amount of privacy protection\n  >  - maximum complexity of stateful dialogs supported\n  >  - amount of cache control possible\n  >  - risks when used with non-conforming caches\n  >  - amount of confusion on www-talk generated\nThis is a great list!  Mind if I steal it for my proposal?\n  > \n  > The proposal below occupies one point in this solution space.\n  > \n  > The main goal of the text below is to make all dimensions visible, not\n  > to give a personal opinion on what the best point in the space is.  In\n  > fact, my personal opinion on this has changed a lot over the last\n  > month, and I have not reached a final conclusion yet.\n  > \n  > \n  > ** Background\n  > \n  > This document assumes that you have read [1] and [2], and that you are\n  > familiar with [4].\n  > \n  > [1] the NetScape Cookie proposal\n  > <URL:http://home.mcom.com/newsref/std/cookie_spec.html>.\n  > \n  > [2] the proposals for of gathering consumer demographics\n  > <URL:http://www.w3.org/hypertext/WWW/Protocols/demographics.html> by\n  > Daniel W. Connolly.\n  > \n  > [3] Session-ID (State-Info) proposal by Dave Kristol\n  > <URL:http://www.research.att.com/~dmk/session.html>\n  > \n  > [4] The draft HTTP/1.0 specification\n  > <URL:http://www.ics.uci.edu/pub/ietf/http/>\n  > \n  > \n  > ** Terminology\n  > \n  > Most terminology in this document is as in [4].  New terminology:\n  > \n  > Browser: user agent (used for interactive web sessions).\n  > \n  > Stateful dialog: An information exchange between a web user and a web\n  > server that that extends beyond the submission of one form.  In a\n  > stateful dialog, the server changes its behavior as a result of\n  > previous actions by the user.  Conceptually, the state is a property\n  > of the dialog (or session), not of the browser or server.  On the\n  > implementation side, either the browser, the server, or both hold the\n  > state.\nYou failed to use the [4] terminology above and below.  In particular I\nthink you mean \"origin server\" where you say \"server\" and \"web server\".\n  > \n  > Cookie: a string to be sent by a browser to a server in requests,\n  > representing the state in a stateful dialog.  The string is kept at\n  > the browser side, but supplied and updated by the server using\n  > set-cookie response headers.  Only the server need to be able do\n  > decode the cookie.  See [1] for an example cookie definition.  A\nActually, Netscape's cookies are interpreted by the browser to the extent\nthat it returns them selectively to the origin server, based on request\nURL.  It also makes them expire.  Those two points are different from my\nproposal, where the browser does not peek inside.\n  > cookie can represent the entire dialog state directly, or be a key in\n  > a server-side dialog state database.\n  > \n  > Request-id: a unique value sent by a browser to a server in requests\n  > to allow the server to generate better clicktrail statistics. \n  > \n  > Session-id: a unique identifier sent by a browser to a server in\n  > requests to give the server a way of keeping it apart from other\n  > browsers for the purpose of engaging in stateful dialogs.  Unlike a\n  > cookie, the value of a session-id can never change during a stateful \n  > dialog.\nIf you're referring to my State-Info (formerly Session-ID) proposal, this\nis a misunderstanding.  The server is free to change the State-Info in\nresponses.\n  > \n  > Persistent information: information that is remembered by the browser\n  > for future sessions\n  > \n  > Non-persistent information: information that is lost when the browser\n  > exits\nPerhaps you need to add words about what happens when one window of a\nmulti-window browser exits.\n  > \n  > \n  > \n  > Proposal:  Non-persistent cookies\n  > =================================\n  > \n  > ** 1. The set-cookie response header\n  > \n  > A server may choose to send a set-cookie response header in a response \n  > to a browser.  The header looks like\n  > \n  >    Set-Cookie: <string>\n  > \n  > where <string> does not contain whitespace. (This allows for future \n  > extensions).  Examples:\n  > \n  >    Set-Cookie: color_screen;no_jpgs;small_graphics\n  >    Set-Cookie: Joe%20User;joe@foo.com\n  >    Set-Cookie: id324254\n  >    Set-Cookie: joe:4234982343\n  > \n  > If the browser receives a Set-Cookie response header, it can either\n  > honor it or ignore it.\n  > \n  > ** 2. Honoring a Set-Cookie header\n  > \n  > To honor a set-cookie header received from a server, a browser will \n  > start including a header\n  > \n  >    Cookie: <string>\n  > \n  > in _direct_ requests to that server, and to that server only.  For the\n  > purpose of this definition, individual servers are identified by the\n  > hostname+portnumber pair in the request URL.  Directness of requests\n  > will be defined later.\n  > \n  > The cookie <string> is taken from the Set-cookie header, and may be\n  > changed by the server by sending a new Set-cookie header with a new\n  > string.  A Set-cookie header with an empty string can be taken as a\n  > request to stop sending Cookie headers.\n  > \n  > \n  > ** 3. The decision to honor a Set-Cookie header\n  > \n  > If a Set-Cookie response header is honored, this means that a server\n  > can get more accurate statistics about the behavior of the browser\nBut the purpose of cookies is much more than just allowing an origin\nserver to gather statistics.\n  > user if the browser user is behind a firewall or proxy cache.  Thus,\n  > the user should have the option of deciding not to honor Set-cookie\n  > headers for privacy reasons.\n  > \n  > It is suggested that browsers provide something like the following\n  > preferences box:\n  > \n  >    +-----------------------------------------------------------------+\n  >     Honor set-cookie requests:\n  >         ( ) Always honor request\n  >         ( ) Start honoring requests if one is done in a response to\n  >             a form submission (POST request).\n  >         (*) Ask once for every site, use reply in later sessions\n  >         ( ) Never honor requests\n  >    +-----------------------------------------------------------------+\n  > \n  > where the (*) is the default setting.  In real browsers, the\n  > terminology used in this box should probably be adapted to make more\n  > sense to the average user.\n  > \n  > As a matter of etiquette, services should only send Set-Cookie headers\n  > if honoring them would bring some direct advantage to the user, like\n  > being able to use a stateful dialog.  Sending Set-Cookie headers only\n  > to get better clicktrail statistics should be considered a breach of\n  > etiquette.  To get clicktrail statistics, the Request-id header (see\nSo cookies are not for statistics!?\n  > the appendix below or [2]) should be used.\n  > \n  > If a browser decides to honor one Set-Cookie header from a server, the\n  > service author can expect that Set-Cookie headers sent in the near\n  > future will also be honored.\n  > \n  > A browser can contain a timeout mechanism to stop honoring Set-cookie\n  > headers when, say, 30 minutes have past since the last contact with\n  > the server.\nI think you mean that the browser will flush remembered cookies for an\norigin server at some chosen time period after the last contact with\nit.  I assume the browser would honor new Set-cookie's if it contacted\nthe origin server again after that period.\n  > \n  > \n  > ** 4. Direct and Indirect Requests\n  > \n  > A request is indirect if it\n  >  1) fetches the contents of an inline picture or other inlined object\n  >  2) resolves a 3xx (redirection) response\n  > \n  > All other requests are direct.\n  > \n  > If a browser is to honor a Set-Cookie header, Cookie headers must be\n  > sent in _direct_ requests to the server.  It is preferred that Cookie\n  > headers are never sent in indirect requests to a server.\n  > \n  > The distinction between direct and indirect requests is made for two\n  > reasons:\n  > \n  >  a) this allows for better caching of stateful dialogs, as discussed\n  >     below\n  >  b) this allows for better privacy: it makes impossible some\n  >     `stealthy' cookie matching strategies that could be adopted by\n  >     cooperating web service providers to allow matching clicktrails.\n  > \n  > \n  > ** 5. Cookie headers and caching: the default\n  > \n  > By default, a cache, no matter whether it is in a browser or in a\n  > proxy, must never cache responses to requests with Cookie headers in\n  > them.\n  > \n  > Responses which contain Set-Cookie headers must also never be cached,\n  > no matter whether the request itself contained a Cookie header.\n  > \n  > There are three reasons for this default\n  > \n  >  1) Responses in stateful dialogs are dynamic by nature.  No big\n  >     payoff can be expected from caching them.  In fact, in a scheme\n  >     where they can be cached, there is a danger of cache memories\n  >     dropping useful `static' responses (like inline pictures from\n  >     normal sites) to store relatively useless dynamic responses.\n\nThis seems to be unduly restrictive.  In recent messages on this list I\nhave described how a shopping application might consist of pages that\ndescribe a product, and that have a link to \"Show me my current\nshopping basket.\" The product description could benefit from caching,\neven though the request might have contained a Cookie (State-Info in my\nproposal) header.\n  > \n  >  2) It is vital that responses in stateful dialogs are never cached,\n  >     else services using stateful dialogs would become unreliable.\n  > \n  >     An Expires: <yesterday> header in a response can be used to\n  >     disable caching (in both browsers and proxy caches), but some\n  >     system operators may be tempted to `tune' proxy or browser caches\n  >     under their control to keep expired responses around for, say, 5\n  >     minutes, even though this makes the cache non-conformant to the\n  >     http spec.\n  > \n  >     Such `tuning' is relatively harmless for normal, non-interactive\n  >     services, but disastrous for stateful dialog reliability.  As\n  >     stateful dialogs are still relatively uncommon, it is a valid\n  >     assumption that many system operators are not aware of the\n  >     stateful dialog risks involved with `Expires tuning', so\n  >     non-conforming caches may remain with us for some time to come.\n  > \n  >     By making responses in stateful dialogs using the Cookie headers\n  >     a special case in the cache algorithm, independent of `Expires\n  >     tuning', this particular non-conformance risk to stateful dialogs\n  >     is eliminated.\n\nIt seems to me it will take at least as long to deploy caching proxies\nthat understand (and don't cache) requests/responses that use\nCookie/Set-cookie as it will to fix badly tuned caching proxies.\n  > \n  >     (Note: the Session-ID (State-Info) proposal by Dave Kristol [3]\n  >     assumes that system operators will never do such `tuning': this\n  >     allows proposal [3] to be very much simpler than this cookie\n  >     proposal.)\n  > \n  >  3) The developer of a stateful dialog service will usually not have a\n  >     cache between his browser and the CGI scripts under development.\n  >     If caching were enabled by default on requests with Cookie\n  >     headers, the author would have to remember to put Expires:\n  >     <yesterday> headers in the responses generated by the scripts; if\n  >     a forgetful or badly informed author of a stateful dialog service\n  >     would not do this, the resulting unreliability would not show up\n  >     on tests within the local environment.\n\nOne hopes such a developer would also beta test the application in a more\nrealistic environment, too.\n  > \n  > \n  > This default behavior of _not_ caching responses to requests with\n  > Cookie headers is the main reason why indirect requests, which do not\n  > get Cookie headers, were introduced.  An inline picture request is\n  > indirect, so inline pictures on stateful dialog pages will get cached\n  > by default.\n  > \n  > Of course, if caching of an inline picture is not desirable, the\n  > service author can always put an Expires header in the inline picture\n  > response.\n  > \n  > If a page contains pictures that depend on the dialog state, the\n  > service author can implement these state dependent pictures by making\n  > the generation of the URLs in the <IMG SRC=...> tags depend on the\n  > state.  Similar state-dependent URL generation can be done for\n  > redirection (3xx) codes.\n  > \n  > \n  > ** 6. Cookie headers and caching: overriding the default\n  > \n  > If the response to a request on URL U with a Cookie header contains an\n  > \n  >   Expires: <date>\n  > \n  > header, and no Set-Cookie header, a cache can interpret this to mean\n  > two things:\nI thought a caching proxy was obliged never to cache a response to a\nrequest that carried a Cookie header (sect. 4).\n  > \n  >  1) the entity included may be cached, but not beyond the\n  >     <date> given,\nBefore you didn't trust caching proxy operators to treat Expires\ncorrectly.  Now you do?\n  >  2) the entity in the response does *not* depend on the dialog state.\n  >     Thus, if the entity is cached and some browser does a request on\n  >     URL U, it is OK to serve the cached entity, no matter what the\n  >     cookie header value in the request is, even no matter whether a\n  >     cookie header is present at all in the request.\n  > \n  > Note that it makes no sense for servers to send both a Set-Cookie\n  > header and an Expires header in the same response.  Servers may\n  > however choose to do so to get backwards compatibility with old\n  > proxy caches.\n  > \n  > Also note that it makes no sense to put Expires: <yesterday> headers\n  > in responses to requests which contain Cookie headers.  Servers may\n  > however choose to do so to get backwards compatibility with old proxy\n  > caches.\n  > \n  > This way of defining Expires: semantics ensures that caches need never\n  > consider the cookie header when accessing their cache memory.  The\n  > Cookie header is never part of the cache key, the header is only\n  > important when making the decision whether to cache or not.\n\n  > [ Request-ID proposal omitted ]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "HTTP version 1.0 or 1.1",
            "content": "I have had several comments sent to me (most off the list) along the\nlines of \"how can we keep changing 1.0 when there are deployed apps?\"\nand \"how can I know whether a server supports the standard or not if\nwe continue to use the same version number?\", etc.\n\nAs we mentioned back in San Jose, we have a choice as to whether\nwe try to standardize 1.0, or release a current-practice document on\n1.0 and move our standard to HTTP/1.1 (which in turn would mean that \nwhat we are currently calling 1.1 features would be in HTTP/1.2).\n\nSince the IETF recently created the BCP (\"Best Current Practice\")\nseries, we can actually do this now within a reasonable framework.\n\nThe action required would be for Henrik and I to create a stripped-down\nHTTP/1.0 document and release it along with the current draft renamed\nas HTTP/1.1.  This will take three days for us to accomplish, but may\nbe worth the saving in confusion and aggravation.\n\nDoes the WG want to do this?\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TC",
            "content": "----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] To: Paul Leach\n] Date: Friday, August 11, 1995 11:48AM\n]\n]\n] Will someone please try to explain why we need something besides\n] TCP?  In particular, what problem is RPC intended to solve?\n]\n] The \"excess overhead packets\" problem melts away with persistent\n] connections; the extra 6 or 7 packets and one RTT are quickly\n] amortized by the high locality of requests.\n\nI agree that if there are persistent connection and enough locality \nthen connection overhead will be a  non-problem.\n\nHere are the reasons I still think about alternatives to TCP:\n\nI suspect that locality might not be as high as (say) with distributed \nfile systems -- the hyperlinks can jump you all over the place. I'd \nlove to see data; I could be easily convinced otherwise.\n\nAlso, there's a situation I believe will arise that may reduce \nlocality. If proxy caches are in widespread use, then many/most hits \nwill be to the proxy caches, and lots of the traffic that gets onto the \nnet will be conditional gets for cached pages whose TTL has expired. \nMuch of the locality will be absorbed by the caches if the TTLs of \npages from the same server are uncorrelated.  If I had a good set of \ntraces I could simulate this and see if the hypothesis was true.\n\nAnother question I have is, once we make the TCP connection persistent, \nhow well TCP implementations work if there are servers that now have \n10s of thousands of open connections. Your HotOS paper points out the \nproblems with short lived connections; many (but not all) of them would \nseem to be exacerbated by longer lived connections (buffer space and \nPCB problems, e.g.).\n\nI will agree in advance that there is nothing that would prevent any of \nthese problems with TCP implementations from being fixed -- even the \nconnection overhead (I've been told that there are experimental \nimplementations where the extra connection overhead is piggybacked on \ninitial data packets). However, since TCP is in most people's kernel, \nit's harder to get such a fixed implementation deployed than a \nsemantically identical protocol layered on top of UDP embodied in an \nHTTP server.\n\n(Maybe if we could fix our servers so that clients that were using such \na more net-friendly TCP to the head of the service queue, then that \nmight be incentive enough to cause the problem to be fixed.)\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: UDP or TCP",
            "content": "----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] To: Paul Leach\n] Cc:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>;  <mogul@pa.dec.com>\n] Subject: Re: UDP or TCP?\n] Date: Friday, August 11, 1995 11:44AM\n]\n]     ] But simply going to UDP is not the answer, because UDP (without\n]     ] some extra effort) does not offer any congestion-control mechanisms.\n]     ] Sure, you can layer them on top of UDP, but I suspect that you would\n]     ] quickly get back to something very similar to TCP.\n]\n]     All RPCs I know of do the extra flow control thing. For RPC, it is\n]     very simple in the basic request/response case, as request/response\n]     is naturally flow controlled -- this is why you don't quickly get\n]     back to TCP for simple cases. When the in or out arguments are\n]     large, then you do indeed have essentially the same problem as\n]     TCP.\n]\n] Flow control is not congestion control.\n\nI mis-wrote.  I meant to say congestion control. If a sender sends one \npacket, and waits for a response before sending another, then \ncongestion automatically slows down the rate at which the sender transmits.\n\n]\n]     ] Paul, what does DCE RPC do for congestion avoidance and control\n]     ] in WANs, when it uses UDP?\n]\n]     The protocol is designed to use slow start, sliding window, ack\n]     maps, and serial numbered packets.\n]\n] Slow start (depending on exactly how it is done) will help with\n] congestion.  It may not be a full solution.\n\nLet me restate: the combination of the above items is everything TCP \ndoes, and more. ACk maps reduce retranmissions, and serial numbered \npacket reduce retransmission and make round trip time estimation more accurate.\n\n]\n]     I don't know how much of the protocol capacity is\n]     exploited in any given implementation -- MS, OSF, and OSF members'\n]     \"enhanced\" versions.\n]\n] If congestion control is not a mandatory part of the implementation,\n] present in all cases, it would be unwise to deploy this as the basis\n] for the Web.\n\nI agree. Before using this protocol, the rules for an implementation \nthat would be used for the Web should be tightened a lot.  An almost \nliteral copying of the relevant portions of the TCP spec into its spec \nwould do the trick. And the reference implementation should of course conform.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Nonpersistent Cookie proposa",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nkoen@win.tue.nl wrote:\n> By default, a cache, no matter whether it is in a browser or in a proxy,\nmust\n> never cache responses to requests with Cookie headers in them.\n> Responses which contain Set-Cookie headers must also never be cached, no\nmatter\n> whether the request itself contained a Cookie header.\n\nThis rule is unnecessarily harsh and doesn't seem to provide any real\nbenefit given that alternatives exist.\n\nBecause you state no rules in your proposal for maintaining the continuity\nof a session (i.e. you don't address the \"fragility\" issues I raised\nconcerning Kristol's proposal and you don't mention Netscape-like \"path\"\ndata in your proposal.) I'll assume you mean to accept the rules that\nKristol proposed, including his recent, but as yet undocumented, statement\nthat he expects servers to \"reflect\" State-Info when received in requests\nfor URI's that don't require State-Info and his statement that he expects\nState-Info to be able to carry State for more than one application at a time\n.\n\n    1. Assume that a browser has picked up, because of prior interactions\n       with ServerA, a cookie or State-Info header that it must send in the\n       next request to ServerA.\n    2. Assume that the browser requests URI-1 from ServerA via Proxy-A. \n       URI-1 does not require any State-Info.\n    3. Assume that a Proxy-A has a non-expired copy of URI-1 in cache.\n\nWhen the browser requests URI-1, it will send the State-Info to Proxy-A\nwhich should then forward that State-Info on to Server-A using a conditional\nGet. The response to this request should include the State-Info \"reflected\"\nback. Now, the question is, what should the cache do? Should it purge URI-1\nfrom its cache since it has been \"received\" as part of a response that\ncontained State-Info? Should it update any internal information concerning\nURI-1? NOTE: If you allow a scenario like this to purge items from cache,\nyou'll find that a reader who wanders around a site with some outstanding\nState-Info can blast all sorts of things out of the cache unnecessarily.\n\nNow, using only assumptions 1 and 2 above and with assumption 3 now reading:\n\n    3. Assume that Proxy-A does not have a copy of URI-1 in cache.\n\nWhat should the cache do when it gets a response to the browsers request for\nURI-1? We know (although the cache doesn't) that URI-1 is perfectly safe to\ncache since it doesn't have any need for State-Info, Cookies, etc. How do we\nget the thing in cache? Or, do we simply hope this doesn't happen too often?\n\nI think these problems are all solved by insisting that applications that\nsend data that shouldn't be cached use the \"Pragma: no-cache\" header and\nrelieve the cache writers from trying to intuit what the heck is going on.\n\nCache writers who want to be \"very sure\" can implement slightly more complex\nstrategies. For instance:\n    1. If you have something in cache and you get a request with State-Info,\n        forward the request using conditional get.\n    2. If you get a response which includes State-Info, then, in the absence\nof \n        Pragma's, Expires, etc. cache the object but remember that it had\n        State-Info in the response. (don't cache the State-Info)\n    3. If you get a request for a resource that has the \"had State-Info\"\n        flag set, then do a conditional get with no State-Info to give\n        the origin-server a chance to create a new session.\n    4. If a \"304 Not Modified\" response is received and carries\n        State-Info, remember that you had State-Info only if you didn't\n        have the \"had State-Info\" flag set for the item before.\n    5. If a \"304 Not Modified\" response without State-Info is \n        received for something that has the \"had State-Info\" flag set, \n        clear the flag.\n\nbob wyman\n       \n\n\n\n"
        },
        {
            "subject": "posts to both www-talk and httpw",
            "content": "Request: I suspect there is no need to post anything to both www-talk\nand http-wg.  I suspect that everyone who reads http-wg also reads \nwww-talk.  I am posting this to both lists in case this is not true --\nthen someone can flame me down.\n\nIn any case, I see no reason why anything needs to go to both lists.\nIf nobody objects too strenuously, let's save each other some time by\navoiding this.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "&quot;Hits&quot; pragm",
            "content": "Brian Behlendorf <brian@organic.com> writes:\n> One thing that would encourage the use of Expires:\n> headers of course would be a way for caches to report hits\n> they served directly without a long-distance conditional GET.\n\nHow about using an additional pragma directive\n\nPragma: hits = 35\n\nto denote the accumulated number of unforwarded requests received  \nby a proxy, in addition to the request being forwarded.  Multiple  \n\"hits\" directives make sense, and can be accumulated by intermediate  \nproxies.\n\nUsing this pragma, hit counts will be accurate over time intervals  \non the order of the expiration interval of the resource (which is  \ntunable by the server).\n\nThis proposal requires only minimal changes to caches, servers, and  \nlog analysis tools -- and offers a graceful, incremental upgrade  \npath  in the meantime (since Pragma headers are already passed  \nthrough by any conforming proxy).  Performance of all components is  \npractically unchanged since no additional network connections are  \nused.  And perhaps most important for successful adoption, this  \nscheme avoids imposing any burdensome reporting duties (such as  \n\"accounting batch runs\") on proxy maintainers.\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@cs.princeton.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "Paul Burchard writes:\n> Brian Behlendorf <brian@organic.com> writes:\n> > One thing that would encourage the use of Expires:\n> > headers of course would be a way for caches to report hits\n> > they served directly without a long-distance conditional GET.\n> \n> How about using an additional pragma directive\n> \n> Pragma: hits = 35\n> \n> to denote the accumulated number of unforwarded requests received  \n> by a proxy, in addition to the request being forwarded.  Multiple  \n> \"hits\" directives make sense, and can be accumulated by intermediate  \n> proxies.\nI'd preferred a\nPragma: hits hits# proxy-name\nvariant instead.\n> Using this pragma, hit counts will be accurate over time intervals  \n> on the order of the expiration interval of the resource (which is  \n> tunable by the server).\n> \n> This proposal requires only minimal changes to caches, servers, and  \n> log analysis tools -- and offers a graceful, incremental upgrade  \n> path  in the meantime (since Pragma headers are already passed  \n> through by any conforming proxy).  Performance of all components is  \n> practically unchanged since no additional network connections are  \n> used.  And perhaps most important for successful adoption, this  \n> scheme avoids imposing any burdensome reporting duties (such as  \n> \"accounting batch runs\") on proxy maintainers.\nAgree.\nBefore I read this, prepared a discussion of hit reports, now I escaped\nfinishing it and you escaped reading it.\nWithout the Hits pragma, some web-admins or document owners would use\nthe no-cache pragma to have those hit counts, which has a very expensive\nside effect for both the client and server sides. (bigger system load on server,\nand bigger costly wan traffic for both.)\nBut will the web-admins and document owners be satisfied with raw counts?\nAnd on the opposite end, will admins running caching proxies on firewalls\nhonor specs for more detailed statistics?\n\nWe shall found a good compromise between the stats hungry server-side\nand terse client-side requirements, otherwise too many misconfigured systems \nwould be running in the near future. (I mean possible mis-use of no-chache\npragma, and consequently caches not honoring it.)\n\nNow that we have this idea intoducing the hits pragma we have a lot to do \nelaborating the concise definition and exact usage of it, and we shall have to\nfinish it till 18 Aug.\n\nI presume we shall incorporate the hits pragma in the ongoing version of the\nHTTP draft as a must requirement.\n\nAny ideas, suggestions, comments?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Comment on http 1.0 draft 01: authentication and cachin",
            "content": "In section 10 of <draft-ietf-http-v10-spec-01.txt>, it says:\n\n   Proxies must be completely transparent regarding user agent\n   authentication. That is, they must forward the WWW-Authenticate and\n   Authorization headers untouched. HTTP/1.0 does not provide a means\n   for a client to be authenticated with a proxy.\n\nI read this to imply that caching proxies may never cache responses to\nrequests with Authorization headers.\n\nIs this really the intended meaning?  It sounds like a wasteful\nrequirement to me.\n\nI feel that passing along Authorization headers untouched is fine as a\ndefault, but that there has to be some way to override this default.\n\nA response message could contain a header to explicitly _allow_ a\nproxy cache not to be transparent, e.g.\n\n   URI: <http://shopping.com/food/vegetables/carrots.gif>;\n        unvary=\"authorization\"\n\nThe `unvary' would tell the cache that the response does not vary if\nthe Authorization header varies, implying that no authentication is\ndone on http://shopping.com/food/vegetables/carrots.gif.  This would\nallow the proxy cache to act non-transparently, to serve future\nrequests for that picture from cache memory without ever contacting\nthe origin server.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version 1.0 or 1.1",
            "content": "Roy Fielding:\n[....]\n\n>Since the IETF recently created the BCP (\"Best Current Practice\")\n>series, we can actually do this now within a reasonable framework.\n>\n>The action required would be for Henrik and I to create a stripped-down\n>HTTP/1.0 document and release it along with the current draft renamed\n>as HTTP/1.1.  This will take three days for us to accomplish, but may\n>be worth the saving in confusion and aggravation.\n>\n>Does the WG want to do this?\n\nI see a number of advantages to this approach, and almost no\ndisadvantages, except that it may take rather more than 2 times 3\nperson-days, if the BCP document is really meant to reflect the amount\nof http functionality that can be relied upon to be present today.\n\nIn one area I am familiar with, redirection codes, best current\npractice would be `only 301 and 302 can be expected to work as\nspecified, and only in responses to GET requests' (at least, that is\nmy observation on doing experiments with browsers).\n\nCurrent support for redirection is rather less than what is specified\nin the 1.0-01 draft, so this points to the need for a BCP document.\nOn the other hand, the redirection case shows that, to write a good\nBCP, a lot of information about the behavior of current web software\nwill be needed, and this information may take a long time to get\n(either by doing experiments yourselves or by asking for information\non www-talk).\n\nAnd of course, as soon as the BCP is released for review, you will get\nflamewars (i.e. large amounts of wasted time) on www-talk and http-wg\nalong the lines of `well, I know that browsers X, Y, and Z do not\nsupport protocol feature P, but I call X, Y, and Z broken browsers\nbecause of this, so P needs to be in the BCP document nevertheless'.\n\n> ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP version 1.0 or 1.1",
            "content": "Roy Fileding asked about:\n> The action required would be for Henrik and I to create a stripped-down\n> HTTP/1.0 document and release it along with the current draft renamed\n> as HTTP/1.1.  This will take three days for us to accomplish, but may\n> be worth the saving in confusion and aggravation.\n> \n> Does the WG want to do this?\nYES! 6 man*days effort is worth for benefits.\nI'm sorry, but with my english knowledge I can't help in doing that.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Comment on http 1.0 draft 01: authentication and cachin",
            "content": "Koen Holtman writes:\n> \n> In section 10 of <draft-ietf-http-v10-spec-01.txt>, it says:\n> \n>    Proxies must be completely transparent regarding user agent\n>    authentication. That is, they must forward the WWW-Authenticate and\n>    Authorization headers untouched. HTTP/1.0 does not provide a means\n>    for a client to be authenticated with a proxy.\n> \n> I read this to imply that caching proxies may never cache responses to\n> requests with Authorization headers.\nI think, that once we have the pragma no-cache, this should be applied to\nevery non-cacheable responses. \n> Is this really the intended meaning?  It sounds like a wasteful\n> requirement to me.\n> \n> I feel that passing along Authorization headers untouched is fine as a\n> default, but that there has to be some way to override this default.\n> \n> A response message could contain a header to explicitly _allow_ a\n> proxy cache not to be transparent, e.g.\n> \n>    URI: <http://shopping.com/food/vegetables/carrots.gif>;\n>         unvary=\"authorization\"\n> \n> The `unvary' would tell the cache that the response does not vary if\n> the Authorization header varies, implying that no authentication is\n> done on http://shopping.com/food/vegetables/carrots.gif.  This would\n> allow the proxy cache to act non-transparently, to serve future\n> requests for that picture from cache memory without ever contacting\n> the origin server.\nAnd the fact, that the no-cache pragma isn't applied, shall be a garantee,\nthat the result is cacheable.\n\nOops! If this is true, - is it, honorable collegaues in WG?\nthen in shttp draft this should be stated - as long, as any shttp\nresponse isn't appropriate to cacheing, that a Pragma: no-cache \nMUST BE applied in every response, which contain encapsulated shttp messages.\n\nMy point is: if the presence/absence of the no-cache pragma means non-cacheable/\ncacheable status of the message, then it's very easy to handle in cacheing\nproxies the whole question. Oherwise the cacheing proxy should know about every\nextension, which may concern cacheability status.\nThe requirement for caches, that the whole cacheing proxy should be as simple\nas possible for better performance, I can't imagine other considerations, which\ncan overrule my position.\nAnd the WG's point in this question?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "On Fri, 11 Aug 1995, Paul Burchard wrote:\n> Brian Behlendorf <brian@organic.com> writes:\n> > One thing that would encourage the use of Expires:\n> > headers of course would be a way for caches to report hits\n> > they served directly without a long-distance conditional GET.\n> \n> How about using an additional pragma directive\n> \n> Pragma: hits = 35\n> \n> to denote the accumulated number of unforwarded requests received  \n> by a proxy, in addition to the request being forwarded.  Multiple  \n> \"hits\" directives make sense, and can be accumulated by intermediate  \n> proxies.\n> \n> Using this pragma, hit counts will be accurate over time intervals  \n> on the order of the expiration interval of the resource (which is  \n> tunable by the server).\n\nSo for every piece of data in the proxy, the server will have to maintain \na counter on number of accesses, to be sent when the document expires.  \nThis is about halfway there - *every* client of ours wants stats as to \nthe busiest time of day for their sites (even to particular parts of \ntheir site) and it doesn't let us analyse \"clickstreams\" using either \npathway heuristics or RequestID info.[1]\n\nIt would be more useful to the server if it gave a more detailed report, \neven if it's just a list of RequestID's, timestamps, and Referrers.  \nUserAgent would be marginally useful as well... perhaps a configurable \nformat the server could give an an HTTP header on the original attribute?\nYes, this means more bandwidth at the reload time, but it's still less \nthan all those separate connections.\n\nWhat would the proxy administrator get out of this?  Well, the more info \nthat can be forwarded, the more likely content providers will start \nputting useful Expires in their documents.  Web protocols of course \nshould not be designed around \"who's more selfish\", but hopefully there's \na common ground that can be reached.\n\n> This proposal requires only minimal changes to caches, servers, and  \n> log analysis tools -- and offers a graceful, incremental upgrade  \n> path  in the meantime (since Pragma headers are already passed  \n> through by any conforming proxy).  Performance of all components is  \n> practically unchanged since no additional network connections are  \n> used.  And perhaps most important for successful adoption, this  \n> scheme avoids imposing any burdensome reporting duties (such as  \n> \"accounting batch runs\") on proxy maintainers.\n\nI think this still holds....\n\nBrian\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "Brian Behlendorf <brian@organic.com> writes:\n> What would the proxy administrator get out of this?  Well,\n> the more info  that can be forwarded, the more likely\n> content providers will start  putting useful Expires in\n> their documents.  Web protocols of course  should not be\n> designed around \"who's more selfish\", but hopefully\n> there's  a common ground that can be reached.\n\nFinding this common ground is the crucial point.  Could you perhaps  \nwhittle your \"wish list\" of reporting information down to a  \n\"requirements list\" or even a \"prevention of open rebellion list\"?\n\nMy main point is that there _is_ a way to start a positive feedback  \nloop and get out of this prisoners' dilemma (\"who's more selfish\"):\n\n  (1) merge reporting into the ordinary, profit-making operations\n      of the proxy (by forwarding \"bundled\" requests).\n\n  (2) make sure adoption of such reporting by major proxies will act\n      as a positive incentive for servers to start using Expires\n      correctly (if you use Expires right you get periodic reports\n      automatically, while if you don't you get blocked!).\n\nI put forward the simplest possible backwards-compatible scheme of  \nthis sort, in the hopes of getting _something_ started.  But if the  \n\"open rebellion list\" includes more than hit counts, we will  \nprobably need to include some additional \"bundled reporting\"  \nfeatures.\n\n> *every* client of ours wants stats as to  the busiest time\n> of day for their sites\n\nI don't get it....isn't the point of electronic commerce to break  \nout of the constraints of space and time that limit ordinary  \ncommerce?\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@cs.princeton.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "At 3:50 PM 8/12/95, Paul Burchard wrote:\n>Could you [Brian] perhaps\n>whittle your \"wish list\" of reporting information down to a\n>\"requirements list\" or even a \"prevention of open rebellion list\"?\n\nI think Brian hit it right on -- request-id, time, referer, and user-agent.\nBrian gave reasons for the first two.  Referer seems to be the equivalent\nof Nielsen demographics in some providers' minds -- especially those who\nare paying for referring links.  User-agent not only gives 'browser\ncapability' information, but also often platform information.\n\nIf the proxy services more than one domain (that's possible, yes? -- maybe\none of the proposed SOAPs from a few months ago), the last half of\n'Forwarded' would also be frequently-requested.\n\nAnother way of looking at the list above is that it describes what\ncommercial providers currently get from most usage-analysis reports.  If\nyou reduce the information in batch reports from proxies beyond this, I\nsuspect what you're calling 'open rebellion' will occur.  The question will\nbe, why are we getting less information now than we were before?  Having\nproxies lock them out for non-compliance is a great idea, but I suspect AOL\nor CIS would have to do so before most businesses would pay attention.\n\n>> *every* client of ours wants stats as to  the busiest time\n>> of day for their sites\n>\n>I don't get it....isn't the point of electronic commerce to break\n>out of the constraints of space and time that limit ordinary\n>commerce?\n\nBack up a step.  We tell them, \"Now customers can buy your products\ntwenty-four hours a day.\"  And they say, \"Okay, how many products have we\nsold between midnight and eight A.M?\"  To them, it is a matter of\njustifying their web presence and the expense of providing it.  This is\nactually the same reason more than just hit counts are important to\ncommercial providers: electronic commerce prevents them from _seeing_ their\ncustomers, so they want to know whatever else they can.  I'm sure they\nwould all want From as well, but I'm more than happy to leave that out (he\nsays, deleting three pieces of junkymail from Usenet posts, and two pieces\nof 'webmaster@' junkymail ..... not bad for a week).\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "Paul Burchard<burchard@cs.princeton.edu> writes:\n> \n> Talking to several people privately, I've been convinced that  \n> \"bundled request\" reporting information would most naturally be  \n> placed into a Forwarded header, instead of a Pragma.  Forwarded  \n> headers have similar semantics to Pragmas (they accumulate and must  \n> be passed on down the line of proxies), and already represent a  \n> primitive form of reporting.\n> \n> So, keeping in mind Brian Behlendorf's requirements list, here is a  \n> more refined proposal containing two extensions of the Forwarded  \n> header.  I think both should be considered, since they provide  \n> different balances between the demands of proxies and servers.\n> \n> The first (proxy-biased) extension, which only requires constant  \n> storage per cached item at the proxy, adds a \"count\" clause to the  \n> Forwarded header:\n> \n> Forwarded: by http://proxy/ count 34\n> \n> This is equivalent to the hits pragma, except that hit counts are   \n> assigned to specific \"leaf\" proxies.  To be precise:  The count is  \n> reset to zero after the proxy forwards a request.  Thereafter, every  \n> request for the resource received _without_ any Forwarded headers  \n> increases the count by one; every request _with_ one or more  \n> Forwarded headers simply contributes its Forwarded headers to the  \n> eventual list of headers to be submitted by the proxy.\nGREAT! I think, this will satisty people 'just wondering' about hit counts.\n> The second extension, which supplies a more acceptable level of  \n> information, but at the cost of proxy storage proportional to the  \n> number of hits received, adds an \"mfor\" (multiple-for) clause to the  \n> Forwarded header:\n> \n> Forwarded: by http://proxy/ mfor Pr5CH77RbN7g0HTux90R7GHK\n> \n> Here, the argument of the \"mfor\" clause is a compressed logfile,  \n> representing those requests received by the proxy for this resource  \n> since last forwarding, which did not contain any Forwarded headers  \n> of their own (as before, requests forwarded from proxies upstream  \n> simply contribute their own Forwarded headers).\nI think the detailed reporting mechanism can better be done external\nto the http protocol. In the http we should specify only an option for\ninformation providers (e.g. Web page owners) to specify such requirements\nin a standardised way. This can be done by introducing a\nHit-reports-to: URL [format-spec]\nheader or something like that.\nAdditionally, according to the discussion in this list, we can state, that using\nthis header is legal only in a http response, which contain a valid\nExpires: header.\nI suggest, that at least http (which should be acted trough POST or PUT?)\nand mailto URL shall be applicable here.\nThe second important thing is to specify the optional format-spec, \n(the default may be the 'common log file format' described in\nhttp://www.w3.org/hypertext/WWW/Daemon/User/Config/Logging.html#common-logfile-format ),\nbecause we can't specify now the possible other formats in detail.\nMaybe somebody requires filling a form, for that case we shall enable \nhttp url-s as format specifiers. This adds of course some complexity\nimplementing compliant caches, but this is a relative simple way for customisation, and filling the from requires human intervention, which improves security \n(but depends on exact knowledge of security policies, which in turn is a MUST\nrequirement to any computer user.)\n> What needs to be decided more precisely is (a) the compression  \n> algorithm to be applied to the logfile, and (b) the format and  \n> fields of the logfile.  Brian Behlendorf suggested a good minimum  \n> set of log fields:  host, timestamp, referer (although some sort of  \n> \"host hiding\" should probably be supported for privacy/security  \n> reasons).  Any suggestions for the compression?  It would be nice to  \n> have something that could be used incrementally by the proxy to  \n> save space.\nCompression is a good idea, but its applicability depends on the protocols,\ncarrying the information.\nI suggest, that including log files (using even very sophisticated compression\ntechniques) in http request headers is dangerous:\nresulting many 1000 char continuation lines may break too many implementations.\n\nPrivacy/security issues are important here. We shall state, that converting\nlogin@host info into official e-mail address (like F.Last@domain) is legal\nand recommended.\n\nNow I make a try to implement this (e.g hit count, Hit-reports-to: and login/site hiding) in ichtus cache, and will report the progress.\n\nGenerally, recording hit counts will only improve the caches. Having hit counts,\nthe garbage collector part of the cache can take it into account, when chooses\nfiles to be deleted.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "At 05:48 PM 8/10/95 EDT, Dave Kristol writes:\n>I have assumed (erroneously?) that a caching proxy must send\n>conditional GETs to the origin server.  If so, there's already the cost\n>of a connection.  The State-Info (previously \"Session-ID\") can ride the\n>request almost for free.\n\nThis isn't necessarily true.  If the server sends \"Expires\" headers, then\nthe proxy can pretty safely avoid sending if-modified-since requests, for\nexample.\n\nEven without \"Expires\" headers, some caching proxies just use simple\nalgorithms to avoid doing a GET on each request, e.g. only re-requesting\ntext/* files if they're at least an hour old.  This applies not only to\nproxies, but also to clients with persistent caches such, if I have a\ndocument still cached on my hard-drive from an access yesterday.\n\nGiven these considerations, and the slowly increasing use of \"Expires\"\nheaders, State-Info could be expensive indeed.\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "HTTP/1.0 draft 0",
            "content": "Hi all,\n\nAfter receiving a unanimous \"yes\" response to splitting the draft 01\ndocument into a \"Best Current Practice\" HTTP/1.0 and a standards-track\nHTTP/1.1, I have completed work on the initial BCP draft.\n\nMy guidelines for inclusion in the HTTP/1.0 spec were that each specified\nfeature must be correctly and consistently implemented on approximately\n80% or more of the currently existing clients and servers.  I am hoping\nthat the WG will agree to that restriction so that we can reach consensus\non this document in near-immediate terms.  Once the guidelines have\nbeen approved, they will be included in the Introduction of the next draft.\n\nDraft 02 is available at the usual places (below) and has been submitted\nto the IETF.  It is available in plain text, HTML, and PostScript\n(w/changebars), and a text diff is also available (generated by\nthe \"diff -b -c1 draft01 draft02\" command).\n\n     http://www.ics.uci.edu/pub/ietf/http/\n      ftp://www.ics.uci.edu/pub/ietf/http/\nand\n     http://www.w3.org/hypertext/WWW/Protocols/Overview.html\n\nSince I expect any changes to this draft to be controversial, please\nsend such comments directly to the WG and not to the authors.\n\nDraft 01 remains available for those wishing to see what HTTP/1.1\nwill look like (modulo a few corrections already noted on the list).\nI am hoping to get an initial HTTP/1.1 draft completed before Friday,\nto forestall any confusion generated by the suddenly-missing features\nfrom HTTP/1.0.\n\nI am currently operating upon the assumption that the HTTP/1.1\ndocument will only contain stuff already determined to be\nrequired for the correct implementation of currently deployed\nHTTP features.  As such, the only things I am planning to add are\nthe Proxy Authentication features (part of earlier drafts, but removed\ndue to the need for a new version number) and Digest AA.\nIf the addition of those two features is objectionable, I'd like\nto hear about it now [I believe both have been adequately implemented?].\n\nOn a related note, I would like to ask the authors of the Digest AA\ndraft to begin work on an updated version which will act as a\ntag-along to the HTTP/1.1 standard.  I believe that this would be\nbetter than attempting to include it in the main document, since\nthe Digest AA may be subjected to more thorough scrutiny.\nThis work must be completed as a product of the HTTP-WG, though\nthe authors should also engage the input of the WTS-WG.\n\nHappy reading,\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "\"Balint Nagy Endre\" <bne@bne.ind.eunet.hu> mentions:\n\n>I presume we shall incorporate the hits pragma in the ongoing version of the\n>HTTP draft as a must requirement.\n\nThe protocol does not require any Pragma directives.  If it is a requirement,\nit is made based on the system semantics (e.g., WWW) rather than at\nthe protocol level.\n\nI am generally against making anything a requirement if it hasn't\nalready been implemented and demonstrated to be a defacto requirement.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Comment on http 1.0 draft 01: authentication and cachin",
            "content": ">In section 10 of <draft-ietf-http-v10-spec-01.txt>, it says:\n>\n>   Proxies must be completely transparent regarding user agent\n>   authentication. That is, they must forward the WWW-Authenticate and\n>   Authorization headers untouched. HTTP/1.0 does not provide a means\n>   for a client to be authenticated with a proxy.\n>\n>I read this to imply that caching proxies may never cache responses to\n>requests with Authorization headers.\n\nActually, it doesn't say that, but it should.  I have added it to draft 02.\nAnything that involves authentication in \"current practice\" also\nimplies exclusion of those not authenticated.  Since the proxy cannot\nduplicate the server's authorization capability, it must not deliver\nthe response to anyone but the client requesting it (and only for that\nparticular request), and therefore should never cache such responses.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "In regard to the proxy passing logfile info to servers, I do hope\npeople discussing these issues have looked at the Security section\nof the HTTP spec.  Some of the suggestions would be a direct\nviolation of those recommendations.\n\nServers have a right to know\n\n    # individuals\n    # hits\n    hit times\n    domains serviced\n\nper URL, but not e-mail addresses, machine names, or any other data\nthat could be used to identify an individual.  Referer data must also]\nbe optional.\n\nLike Andrew mentioned, this is best done by passing a URL to the\norigin server that tells it where it may retrieve a sanitized summary\nof the data.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "caching related topics in the draft and the httpwg discussio",
            "content": "Hi all,\nI try to summarise the impact of the results of current draft and the discussion\non the caching proxies. (O, yeah, caching proxies is my favorite topic for now).\nThe draft states:\na Pragma: no-cache included in the server resposes simply forbids cacheing of\nthe response. It's quite straighforward, and makes it possible to mark web\ndocuments, which are in their own nature unappropriate for cacheing.\nOn the other hand, the draft doesn't state, that absence of this pragma means\nthat the document contained in the response is appropriate for cahceing.\nUnfortunately!\nIf we used the Pragma: no-cache consistently, e.g. applying it to every\nnon-cacheable response, the life of cache implementors and administrators would\nbe much easier. Why not act that way?!\nUpgrading a cache software is a costly process - a cache running significant\ntime can accomodate gigabytes of cached information. If we have no handy way to\nconvert the whole cache into format of the new software, we will lose those\ngigabytes, and will have to pump through the net the whole again.\nAny guesswork made on the future internet traffic says, that the dominant part\nof the traffic will be HTTP. The bandwith  of the long-distance links won't keep\nup with that growth, unless caches will be widely employed. Currently we have\nnot too much experience with caches, especially with cascaded ones. The future\nclearly belongs to cascaded caches. Having good standard, which helps\nimplementing good and stable caches will decrease the probability of such\nsituations. The possible ongoing Proxy-Authetication feature will not prevent\nrunning cache software to be useful, and will not force the costly upgrade\nprocess, unless there will be some (wich is not too probable in the foreseeable\nfuture) safe, stable and standard authentication scheme, which will give enough\nbenefits to compensate the costs. When the standard, safe authentication scheme arrives,\n(but recently PGP, the nearly only candidate was removed from shttp proposal)\nthat time can be a good point to re-standardise the caching again, but that\nshouldn't happen too often.\n\nBoth the referred proposals, e.g.\nDave Kristols state-ids (URL http://www.research.att.com/~dmk/session.html )\nand Koen Holtmans Non-Persistent Cookie proposal want additional rules for\nproxies and caches. If this trend continues, cache/proxie implementors\nwill be lost unless the no-cache pragma is used according to my proposal.\n\nHere comes the proposal itself:\nI want to add the following two sentences to section 8.22 of the draft:\nAll compliant implementations MUST include the no-cache pragma in responses\nwhich are unappropriate to cacheing for any reason. If the reason is political\n(non-technical), server administrators should ensure that.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re:  HTTP/1.0 draft 0",
            "content": "Roy Fielding <fielding@beach.w3.org> wrote:\n  > After receiving a unanimous \"yes\" response to splitting the draft 01\n  > document into a \"Best Current Practice\" HTTP/1.0 and a standards-track\n  > HTTP/1.1, I have completed work on the initial BCP draft.\n  > \n  > My guidelines for inclusion in the HTTP/1.0 spec were that each specified\n  > feature must be correctly and consistently implemented on approximately\n  > 80% or more of the currently existing clients and servers.  I am hoping\n  > that the WG will agree to that restriction so that we can reach consensus\n  > on this document in near-immediate terms.  Once the guidelines have\n  > been approved, they will be included in the Introduction of the next draft.\n  > [...]\n  > \n  > Draft 01 remains available for those wishing to see what HTTP/1.1\n  > will look like (modulo a few corrections already noted on the list).\n  > I am hoping to get an initial HTTP/1.1 draft completed before Friday,\n  > to forestall any confusion generated by the suddenly-missing features\n  > from HTTP/1.0.\n  > [...]\n\nSorry, I'm getting confused about terminology.  Please verify or\ncorrect this summary of the current set of official terminology.\n\n- HTTP/1.0 now refers to \"current practice\", sort of.\n- HTTP/1.1 now refers to what current practice should be, without\nsignificant feature additions other than Digest AA and Proxy Auth.\n- HTTP/1.2 refers to the HTTP/1.1 of a week or two ago?  (You don't\nmention HTTP/1.2 here.)\n\nDave\n\n\n\n"
        },
        {
            "subject": "Re: caching related topics in the draft and the httpwg discussio",
            "content": ">If we used the Pragma: no-cache consistently, e.g. applying it to every\n>non-cacheable response, the life of cache implementors and administrators would\n>be much easier. Why not act that way?!\n\nI think this is partly because, up until last week, Pragma: no-cache was\nonly a request header.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.0 draft 0",
            "content": ">Sorry, I'm getting confused about terminology.  Please verify or\n>correct this summary of the current set of official terminology.\n>\n>- HTTP/1.0 now refers to \"current practice\", sort of.\n>- HTTP/1.1 now refers to what current practice should be, without\n>significant feature additions other than Digest AA and Proxy Auth.\n>- HTTP/1.2 refers to the HTTP/1.1 of a week or two ago?  (You don't\n>mention HTTP/1.2 here.)\n\nYep, that's the plan.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "Roy T. Fielding writes:\n> \"Balint Nagy Endre\" <bne@bne.ind.eunet.hu> mentions:\n> \n> >I presume we shall incorporate the hits pragma in the ongoing version of the\n> >HTTP draft as a must requirement.\n> \n> The protocol does not require any Pragma directives.  If it is a requirement,\n> it is made based on the system semantics (e.g., WWW) rather than at\n> the protocol level.\n> \n> I am generally against making anything a requirement if it hasn't\n> already been implemented and demonstrated to be a defacto requirement.\nAgree, generelly, but in particular, I mean simply that making the hits pragma\noptional may lead to misuse of no-cache pragma. It's not too hard to stuff in\nthe pragma by anybody who owns the document. For example using CERN httpd/3.0\none must create the file $MetaDir/<document-name>$MetaSuffix containing\n\"Pragma: no-cache\".\n(Yeah, it works this way, and the pragma also works, using CERN httpd/3.0,\nI checked right now. I presume using other implementations the situation is\nquite the same.)\nI found too many redirections now, which (I guess) are used to fool caches.\n(At least CERN httpd/3.0 doesn't cache redirections, which is a good practice\naccording to rule '200' responses are cacheable only).\nI'm sure, if we miss the appropriate point to enforce feedback of hit counts,\nthen we will lose in transferring the same over the net again and again,\njust to have those hits counted.\nAnd of course, I mean HTTP/1.1 under 'ongoing version', not HTTP/1.0.\nGenerally, if we help information owners gather the same statistics from caches,\nwhat they may colllect about direct request, then it will be easier to treate\ncache content as part of the network infrastructure versus local copies of\ncopyrighted material. And this is a really important question. If the caches\nwill have bad publicity in this question, we all will lose, except WAN link\nproviders. But the original goal of founding the internet wasn't to make\nWAN link providers rich.\n\nI don't want pression you, rather I want hear other opinions too.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Expires head coun",
            "content": "Recently I counted Exprires headers in my http cache.\nThere were 1714 files, and counted 1 lines beginning Expires:\nbut that was in the body, not in the headers. :-(\n\nAndrew.\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "On Mon, 14 Aug 1995, Roy Fielding wrote:\n> Servers have a right to know\n> \n>     # individuals\n>     # hits\n>     hit times\n>     domains serviced\n> \n> per URL, but not e-mail addresses, machine names, or any other data\n> that could be used to identify an individual.  Referer data must also]\n> be optional.\n\nAre you suggesting that servers only have a right to see aggregate \nstatistics, and not per-request lists?  (i.e. \"32 access of this url by \n23 different hosts\", not \"hosta, timestampa; hostb, timestampB;\"... etc?\nAlso, how does the proxy determine \"# individuals\", or did you mean \"# of \ndifferent IP addresses serviced\"?\n\n> Like Andrew mentioned, this is best done by passing a URL to the\n> origin server that tells it where it may retrieve a sanitized summary\n> of the data.\n\nI much prefer Buchard's model of sending it with the refresh request \nrather than forcing a separate transaction for the data.  If a separate \nrequest were made, the proxy not only has to make that data \nHTTP-accessible to the outside world (a problem for proxies behind \nfirewalls) but also has to have some heuristics as to how long that data \nis around.  With compression it should not add significantly to the size \nof the request.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": ">Are you suggesting that servers only have a right to see aggregate \n>statistics, and not per-request lists?  (i.e. \"32 access of this url by \n>23 different hosts\", not \"hosta, timestampa; hostb, timestampB;\"... etc?\n\nYes.  If you are not given the machine name, matching it with\ntimestamps isn't worth the bother.\n\n>Also, how does the proxy determine \"# individuals\", or did you mean \"# of \n>different IP addresses serviced\"?\n\nThe former, using the same methods the server would.  The proxy may\nknow a lot more information about each user than it is willing to pass on.\n\n>> Like Andrew mentioned, this is best done by passing a URL to the\n>> origin server that tells it where it may retrieve a sanitized summary\n>> of the data.\n>\n>I much prefer Buchard's model of sending it with the refresh request \n>rather than forcing a separate transaction for the data.  If a separate \n>request were made, the proxy not only has to make that data \n>HTTP-accessible to the outside world (a problem for proxies behind \n>firewalls) but also has to have some heuristics as to how long that data \n>is around.  With compression it should not add significantly to the size \n>of the request.\n\nExcept for the minor detail that the last action taken by a cache\nis to throw away the cached document, not make a new request on it.\nThere is no reason to require the proxy to make a new request just\nto inform the server (which in most cases doesn't care anyway) of\nthe last batch of saved hit info.\n\nNote also that the URL for where to retrieve the summaries need\nnot be the same as the proxy sending it.  Of course, proxies behind\nsecure firewalls won't be sending you any data in any case.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "\"Balint Nagy Endre\" <bne@bne.ind.eunet.hu> writes:\n> I think the detailed reporting mechanism can better be\n> done external to the http protocol.\n\nI don't think externally routed reports create the proper  \nincentives for wide adoption -- it's a fragile system that requires  \ntoo much advance cooperation.  In contrast, the forwarding of  \n\"bundled requests\" upon expiration requires no additional  \ncooperation between servers and proxies.\n\n> resulting many 1000 char continuation lines may break\n> too many implementations\n\nSince multiple Forwarded headers are allowed, this isn't a problem.  \n We can recommend an upper limit on the size of each Forwarded  \nheader; proxies can then simply collect and compress the logfile in  \nchunks as they process large numbers of requests.\n\n\nRoy Fielding <fielding@beach.w3.org> writes:\n> Like Andrew mentioned, this is best done by passing a URL\n> to the origin server that tells it where it may retrieve a\n> sanitized summary of the data.\n\nActually, I believe he was suggesting a URL in the *other*  \ndirection.  Allowing report retrieval from the proxy by the origin  \nserver would either be less secure, or even more complex and  \nunreliable.\n\n> In regard to the proxy passing logfile info to servers, I\n> do hope people discussing these issues have looked at the\n> Security section of the HTTP spec.\n\nYes, to be more careful, the log info should rather be:\n\n*.domain [request-id] timestamp [referer]\n\nwhere \"*.domain\" is the hostname sanitized with wildcards as  \nneeded; the optional Referer is null when it would conflict with  \nsecurity; and the presence or absence of the Request-ID is  \ncontrolled at the client (is there any reason for further control at  \nthe proxy?).\n\nProxy chains behind firewalls can also be handled systematically,  \neither by reprocessing the forwarded log info, or more crudely by  \nremoving all the log info and retaining only \"count\" clauses.\n\n--------------------------------------------------------------------\nPaul Burchard<burchard@cs.princeton.edu>\n``I'm still learning how to count backwards from infinity...''\n--------------------------------------------------------------------\n\n\n\n"
        },
        {
            "subject": "Improving If-ModifiedSinc",
            "content": "I've been getting lots of bug reports due to corrupted or out dated\ncaches.  I would like to propose an extension to the If-modified-since\nheader to improve the situation.  I'd like to start sending\n\nIf-modified-since: DATE; size=SIZE\n\nThe addition of size=SIZE informs the server of the current size of\nthe document cached by the client.  The size acts as a checksum,\nif the size of the file to be served is different than the\nsize given in the If-modified-since header than a 304 should\nnot be returned.  \n\nThe advantage of size over other checksums is that it is highly efficient.\nClients and servers can obtain the information at little or no cost.\n\nThe disadvantage of size is that it is not completely accurate as a checksum.\nAn MD5 hash or some other content based checksum would be far more accurate\nbut would require lots of additional overhead.\n\nIn the future, if a stronger checksum is deemed necessary it can be\nadded as another part of the If-modified-since header.  Perhaps:\n If-modified-since: DATE; size=SIZE; md5=SIGNATURE \n\nI have tested this change against the Netscape, NCSA, CERN and Apache\nservers and all of them ignore the addition of size=SIZE, so we\ncan add this addition without fear of backwards compatibility concerns.\n\nComments?\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Lou-\n If the problem is simply maintaining cache validity, then size \nmay not be quite enough, yet MD5 may be too much.\n\nSize will catch quite a few problems, such as truncation or extension, \ntogether with most cases of completely incorrect contents. However, one \ncommmon mode of failure involves files that are created with the correct \nsize, but whose contents are in error. This can happen in several ways-\nNFS lossage is one; system failures when operating on memory mapped files \nare another (the usual M.O for operating on a mapped file is to grow the \nfile to the correct size, then read in to the mapped buffer). \n\nTo detect these more subtle corruptions we need to use a checksum \nalgorithm. There are two main types of checksum- those used to protect \nagainst accidental corruption, and those designed to guard against \ndeliberate modification. An example of the former is the IP checksum \nalgorithm; examples of the latter are MD5 and SHA. Guarding against \ndeliberate evil-doers is much harder than keeping tabs on random \nscrewups; MD5 requires multiple passes and is probably overkill for this \napplication. \n\nThe best compromise would probably be a 32bit checksum or CRC, which can \nbe generated quickly and easily. The checksum could even be included as \npart of a corresponding modification to Last-Modified.\n\nSimon\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "I agree with Lou's proposal. Regarding Simon's suggestion that we also\nprovide a simple checksum, I don't think that would really be a great\nbenifit. The overhead of simply opening a file and scanning through its\ncontents strikes me as greater than that of the MD5 ifself. So personally I\nwould either want an extremely fast mechanism such as the file size & date,\nor else go for the real MD5.\n\n>I've been getting lots of bug reports due to corrupted or out dated\n>caches.  I would like to propose an extension to the If-modified-since\n>header to improve the situation.  I'd like to start sending\n>\n>If-modified-since: DATE; size=SIZE\n>\n>The addition of size=SIZE informs the server of the current size of\n>the document cached by the client.  The size acts as a checksum,\n>if the size of the file to be served is different than the\n>size given in the If-modified-since header than a 304 should\n>not be returned.  \n>\n>The advantage of size over other checksums is that it is highly efficient.\n>Clients and servers can obtain the information at little or no cost.\n>\n>The disadvantage of size is that it is not completely accurate as a checksum.\n>An MD5 hash or some other content based checksum would be far more accurate\n>but would require lots of additional overhead.\n>\n>In the future, if a stronger checksum is deemed necessary it can be\n>added as another part of the If-modified-since header.  Perhaps:\n> If-modified-since: DATE; size=SIZE; md5=SIGNATURE \n>\n>I have tested this change against the Netscape, NCSA, CERN and Apache\n>servers and all of them ignore the addition of size=SIZE, so we\n>can add this addition without fear of backwards compatibility concerns.\n>\n>Comments?\n>\n>:lou\n>-- \n>Lou Montulli                 http://www.mcom.com/people/montulli/\n>       Netscape Communications Corp.\n>\n>\nAlex Hopmann\nResNova Software, Inc.\nhopmann@holonet.net\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Why would a server pumping out bogus last-modified headers act appropriately\nto another type of check?  Adding something to the protocol just because\nanother part is not being used properly seems a bit weird.  If I'm \nunderstanding the problem correctly.\n\nWill the \"size\" be determined from the Content-length header or the size on\nthe cache's disk?  If the former, documents with incorrect content-length\nheaders are essentially uncacheable, as are results from CGI scripts which\ngenerally don't have content-length headers.  If the latter, could there \nbe encoding problems?\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">Why would a server pumping out bogus last-modified headers act appropriately\n>to another type of check?  Adding something to the protocol just because\n>another part is not being used properly seems a bit weird.  If I'm\n>understanding the problem correctly.\n\nThe problems currently encountered are mostly caused by the date comparisons\ndone by most HTTP servers when dealing with If-modified-since requests.\nMost servers assume that as long as the If-modified-since date is equal to\nor AFTER the current modification date of the document then it is unchanged.\n\nThis is a problem because people screw up the dates on their files and\nsometimes give them dates far into the future.  When they fix the\ndates of the files to correspond to the current date, caches never\nget updated.\n\nIn addition to supporting size=SIZE I encourage other server authors to\ndo an _equals_ comparison rather than a greater than or equal comparison\nof the two dates.\n\n>\n>Will the \"size\" be determined from the Content-length header or the size on\n>the cache's disk?  If the former, documents with incorrect content-length\n>headers are essentially uncacheable, as are results from CGI scripts which\n>generally don't have content-length headers.  If the latter, could there\n>be encoding problems?\n\nThe size is determined by taking the current length of the document in the\ncache.  The content-length of the transfer is discarded, so encodings\nshould have no effect.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "On Mon, 14 Aug 1995, Lou Montulli wrote:\n> >Why would a server pumping out bogus last-modified headers act appropriately\n> >to another type of check?  Adding something to the protocol just because\n> >another part is not being used properly seems a bit weird.  If I'm\n> >understanding the problem correctly. \n> \n> The problems currently encountered are mostly caused by the date comparisons\n> done by most HTTP servers when dealing with If-modified-since requests.\n> Most servers assume that as long as the If-modified-since date is equal to\n> or AFTER the current modification date of the document then it is unchanged.\n> \n> This is a problem because people screw up the dates on their files and\n> sometimes give them dates far into the future.  When they fix the\n> dates of the files to correspond to the current date, caches never\n> get updated.\n\nIf people \"screw up their dates\", they're hurting themselves and the people\nwho view their pages.  This *isn't* accidental, is it?  How do you\naccidentally set a last-modified to be some future time?  It's like a \nsurgeon accidentally removing a right leg instead of a left leg or a \ndisgruntled employee accidentally shooting his boss.  \n\nOr maybe it's intentional...\n\n> In addition to supporting size=SIZE I encourage other server authors to\n> do an _equals_ comparison rather than a greater than or equal comparison\n> of the two dates.\n\nThey can't just do a strcmp() since there are a couple date formats it \nneeds to deal with.  Also, consider a situation where there are three \nmirrors for a web site, and all three are hidden behind www.host.com and \nselected through shortest-return-trip calculations (like the CERN \nlinemode browser does).  Getting a last-modified on one which was later \nthan the last-modified on the other, even though they are the same \ndocument, certainly makes sense.  \n\n> >Will the \"size\" be determined from the Content-length header or the size on\n> >the cache's disk?  If the former, documents with incorrect content-length\n> >headers are essentially uncacheable, as are results from CGI scripts which\n> >generally don't have content-length headers.  If the latter, could there\n> >be encoding problems?\n> \n> The size is determined by taking the current length of the document in the\n> cache.  The content-length of the transfer is discarded, so encodings\n> should have no effect.\n\nOkay, so what should a server do when it doesn't know the actual\ncontent-length of an object, like a CGI script?  It's totally plausible to\nask a dynamic object \"hey, has any data upon which you would answer this\nquestion changed since *blah*\" in a way that's very quick to answer, but it's\nhard to imagine asking it \"hey, will your final output be any size\ndifferent than *blah*\" without having it do what it normally does.  So, \nif it's in there, it must be optional for the server to use it.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: &quot;Hits&quot; pragm",
            "content": "Paul Burchard <burchard@cs.princeton.edu> writes:\n> \"Balint Nagy Endre\" <bne@bne.ind.eunet.hu> writes:\n> > I think the detailed reporting mechanism can better be\n> > done external to the http protocol.\n> \n> I don't think externally routed reports create the proper  \n> incentives for wide adoption -- it's a fragile system that requires  \n> too much advance cooperation.  In contrast, the forwarding of  \n> \"bundled requests\" upon expiration requires no additional  \n> cooperation between servers and proxies.\nI mean detailed reports, not the bare hit counts. Who really needs the\nstatistics, will be willing to work a bit for having them.\n> > resulting many 1000 char continuation lines may break\n> > too many implementations\n> \n> Since multiple Forwarded headers are allowed, this isn't a problem.  \n>  We can recommend an upper limit on the size of each Forwarded  \n> header; proxies can then simply collect and compress the logfile in  \n> chunks as they process large numbers of requests.\nHow can split into small chunks the statistics gathered over the whole expiry\nperiod a cache serving thousands of users ? Even in compressed format, this will\noccupy a significant space, compared to every-day request headers!\n> \n> Roy Fielding <fielding@beach.w3.org> writes:\n> > Like Andrew mentioned, this is best done by passing a URL\n> > to the origin server that tells it where it may retrieve a\n> > sanitized summary of the data.\n> \n> Actually, I believe he was suggesting a URL in the *other*  \n> direction.  Allowing report retrieval from the proxy by the origin  \n> server would either be less secure, or even more complex and  \n> unreliable.\n> \n> > In regard to the proxy passing logfile info to servers, I\n> > do hope people discussing these issues have looked at the\n> > Security section of the HTTP spec.\n> \n> Yes, to be more careful, the log info should rather be:\n> \n> *.domain [request-id] timestamp [referer]\n> \n> where \"*.domain\" is the hostname sanitized with wildcards as  \n> needed; the optional Referer is null when it would conflict with  \n> security; and the presence or absence of the Request-ID is  \n> controlled at the client (is there any reason for further control at  \n> the proxy?).\nAll users arent enough picky about security, and a proxy administrator should\nhave possibility to make corrections, when users are weak.\n> Proxy chains behind firewalls can also be handled systematically,  \n> either by reprocessing the forwarded log info, or more crudely by  \n> removing all the log info and retaining only \"count\" clauses.\nThis is that further control.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "On Mon, 14 Aug 1995, Lou Montulli wrote:\n>> >Why would a server pumping out bogus last-modified headers act appropriately\n>> >to another type of check?  Adding something to the protocol just because\n>> >another part is not being used properly seems a bit weird.  If I'm\n>> >understanding the problem correctly.\n>>\n>> The problems currently encountered are mostly caused by the date comparisons\n>> done by most HTTP servers when dealing with If-modified-since requests.\n>> Most servers assume that as long as the If-modified-since date is equal to\n>> or AFTER the current modification date of the document then it is unchanged.\n>>\n>> This is a problem because people screw up the dates on their files and\n>> sometimes give them dates far into the future.  When they fix the\n>> dates of the files to correspond to the current date, caches never\n>> get updated.\n>\n>If people \"screw up their dates\", they're hurting themselves and the people\n>who view their pages.  This *isn't* accidental, is it?  How do you\n>accidentally set a last-modified to be some future time?  It's like a\n>surgeon accidentally removing a right leg instead of a left leg or a\n>disgruntled employee accidentally shooting his boss.\n\nIt's alot easier than you think.  Moving files via tar or NFS can\ncarry over dates from other machines that have been set incorrectly.\nEven the normal system date can get set incorrectly causing lots\nof files to get propogated with wrong dates.\n\n>\n>Or maybe it's intentional...\n>>\n>> In addition to supporting size=SIZE I encourage other server authors to\n>> do an _equals_ comparison rather than a greater than or equal comparison\n>> of the two dates.\n>\n>They can't just do a strcmp() since there are a couple date formats it\n>needs to deal with.  Also, consider a situation where there are three\n>mirrors for a web site, and all three are hidden behind www.host.com and\n>selected through shortest-return-trip calculations (like the CERN\n>linemode browser does).  Getting a last-modified on one which was later\n>than the last-modified on the other, even though they are the same\n>document, certainly makes sense.\n\nNo it doesn't make sense.  A document only has one real date that it\nwas last modified and it is specified in absolute time.  There is no\nneed to ever accept wrong dates, the real modification date should\nbe propogated to all the servers.  BUT, if a particular server implementation\nchooses to do what you have suggested that's fine.  I would just\nlike to see the standard behaviour for single servers be exact date\ncomparison rather than older than or equal to.\n\n>\n>> >Will the \"size\" be determined from the Content-length header or the size on\n>> >the cache's disk?  If the former, documents with incorrect content-length\n>> >headers are essentially uncacheable, as are results from CGI scripts which\n>> >generally don't have content-length headers.  If the latter, could there\n>> >be encoding problems?\n>>\n>> The size is determined by taking the current length of the document in the\n>> cache.  The content-length of the transfer is discarded, so encodings\n>> should have no effect.\n>\n>Okay, so what should a server do when it doesn't know the actual\n>content-length of an object, like a CGI script?  It's totally plausible to\n>ask a dynamic object \"hey, has any data upon which you would answer this\n>question changed since *blah*\" in a way that's very quick to answer, but it's\n>hard to imagine asking it \"hey, will your final output be any size\n>different than *blah*\" without having it do what it normally does.  So,\n>if it's in there, it must be optional for the server to use it.\n>\nFine, then that CGI script can ignore the size and return 304.  Having\nextra information available does nothing to hinder the success of\ncacheing, it can only help.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: caching related topics in the draft and the httpwg discussio",
            "content": "Roy Fielding:\n>[Endre Balint Nagy:]\n>>If we used the Pragma: no-cache consistently, e.g. applying it to every\n>>non-cacheable response, the life of cache implementors and administrators would\n>>be much easier. Why not act that way?!\n>\n>I think this is partly because, up until last week, Pragma: no-cache was\n>only a request header.\n\nAnother reason is that Pragma only affects caches in proxies, not caches in\nbrowsers (user agents).  The Expires header affects all caches: \n\n  expires: <yesterday> \n\nis, as far as proxies are converned, equivalent to Pragma: no-cache.\nAuthors of dynamic services that want to prevent _all_ caching should thus\nsend an Expires header, not a Pragma, if they want to minimize number of\nresponse headers sent.\n\nSo decisions not to cache must involve checking at least two headers.\n\nBut checking two headers for deciding not to cache is the easy thing: the\nthing that makes the life of cache administrators hard is to come up with\ngood replacement heuristics for things that _are_ cached, heuristics that\ncan be based on a great number of headers.\n\nOf course, if cache administrators want to be transparent for NetScape\ncookies and other non-standard html extensions any one browser author feels\nlike introducing, they will have lots of trouble keeping up with browser\nreleases.  However, as a service author, I would never use a non-standard\nextension that needs transparent caches *without* also including Pragma:\nno-cache or Expires: <yesterday>.\n\nBoth Dave Kristol's and my stateful dialog support proposals add extra work\nfor cache implementors, but the difference with NetScape cookies is that,\nthis time, the specification of the work to be done can be found in the HTTP\nstandard document.\n\nThe main reason why my stateful dialog support proposal complicates caching\ndecisions is that it wants to introduce a distinction between frivolous\nrequests to disable caching (which a cache admininstator may want to ignore,\neven if this means not confirming to the standard anymore), and serious\nrequests.\n\n> ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 7:03 PM 8/14/95, Lou Montulli wrote:\n>I've been getting lots of bug reports due to corrupted or out dated\n>caches.  I would like to propose an extension to the If-modified-since\n>header to improve the situation.  I'd like to start sending\n>\n>If-modified-since: DATE; size=SIZE\n\nIt seems like the correct solution is to eliminate the problem with\ncorrupted caches rather than tweaking the standard yet another time.\n\n>I have tested this change against the Netscape, NCSA, CERN and Apache\n>servers and all of them ignore the addition of size=SIZE, so we\n>can add this addition without fear of backwards compatibility concerns.\n\nThis will break MacHTTP and WebSTAR. According to the last published\n\"standard\", the if-modified-since header field's argument (the date) is\nterminated with an end of line sequence. This is how it's currently parsed\nin both of those servers. Given the large installed base of these servers,\nit'd be nice to find an alternate solution.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 9:25 PM 8/14/95, Lou Montulli wrote:\n\n>In addition to supporting size=SIZE I encourage other server authors to\n>do an _equals_ comparison rather than a greater than or equal comparison\n>of the two dates.\n\nIsn't this what the standard says anyway?\n\n>>\n>>Will the \"size\" be determined from the Content-length header or the size on\n>>the cache's disk?  If the former, documents with incorrect content-length\n>>headers are essentially uncacheable, as are results from CGI scripts which\n>>generally don't have content-length headers.  If the latter, could there\n>>be encoding problems?\n>\n>The size is determined by taking the current length of the document in the\n>cache.  The content-length of the transfer is discarded, so encodings\n>should have no effect.\n\nI assume you anticipate this being implemented by having software simply\nask the underlying file system for a byte count of the file? This will\nbreak between any two machines with different end of line sequences. As an\nexample, many servers read text files in their native format, converting\nEOL sequences as they are sent to the client (proxy). This means that the\ndata stream as transmitted to the client (and cached) is a different size\nfrom the actual size of the data stored on the disk at the server.\n\nI think this problem alone is enough to make this scheme unworkable. This\nwill also break any checksum scheme as well. Macs, Windows, VMS, and Unix\nservers all have different EOL sequences and the you can bet that the data\nstream sent by a server is substantially different from the image of the\nfile that resides on the server's disk.\n\nAs I said earlier, the correct solution is to fix the problem with\ncorrupted caches. The modification date is a sufficient mechanism for\ndetermining whether or not a document has changed, assuming both ends are\nable to maintain data integrity. If the proxy server needs to maintain a\ntable of checksums for the files it caches in order to ensure data\nintegrity, that's fine. But it needn't involve the server of the original\ndocument in determining whether or not its own cache has been corrupted.\nSimple comparison of the local checksum to the local data store is\nsufficient to indicate corruption.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">>In addition to supporting size=SIZE I encourage other server authors to\n>>do an _equals_ comparison rather than a greater than or equal comparison\n>>of the two dates.\n>\n>Isn't this what the standard says anyway?\n\nThe spec only refers to \"if the resource has been modified since the\nIf-Modified-Since: date\".  As far as I'm concerned, if the request contains\n\"If-Modified-Since: Mon, 14 Aug 1995 23:38:43 GMT\", and the server has a\nlast modified date of \"Mon, 14 Aug 1995 23:38:44 GMT\", then returning\nsomething other than 304 is a bug.  Who knows how the caching client/proxy\nis saving the date.  Maybe it's saving the last time it requested it, maybe\nit's saving the Date: the server previously sent back, maybe it's saving the\nlast time when the response is completely recieved.  It wasn't specified in\nthe spec <insert standard \"this is a communication protocol\" rant> that\ncache's have to store on the Date: header, so it can't be forced on them\nnow.  Changing from >= to == would probably render a few current caching\nschemes completely useless.\n\nI'm against this change. :)\n\n>I assume you anticipate this being implemented by having software simply\n>ask the underlying file system for a byte count of the file? This will\n\nMakes sense.\n\n>break between any two machines with different end of line sequences. As an\n>example, many servers read text files in their native format, converting\n>EOL sequences as they are sent to the client (proxy). This means that the\n\nThey do?  Shouldn't the client be doing those EOL translations?\n\n>I think this problem alone is enough to make this scheme unworkable. This\n\nCaches would have to store the Content-Length header, not the size of their\ncontent to deal with this EOL translation problem.  Content that lies about\nits length is a much bigger issue (keep alives break).\n\nI think if the scheme is unworkable, it's because of dynamic content.  But,\nthen again, the whole If-Modified-since: thing goes out the window with\ndynamic content, so adding a \"size=value\" paramter won't hurt anything.  (I\nassume most servers ignore If-Modified-since: on CGI/POST requests, am I\ncorrect?)\n\n>corrupted caches. The modification date is a sufficient mechanism for\n>determining whether or not a document has changed, assuming both ends are\n>able to maintain data integrity. If the proxy server needs to maintain a\n\nI would tend to agree, but if people plan on starting a HEAD request,\nchecking Content-Length:, then comparing lengths before doing a GET, we\nmight as well give them the ability to do it with an If-Modified-since:.\n\n>Simple comparison of the local checksum to the local data store is\n>sufficient to indicate corruption.\n\nThat's not the kind of corruption Lou's trying to solve.\n\nI'm for this change. (I think) :)\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">\"If-Modified-Since: Mon, 14 Aug 1995 23:38:43 GMT\", and the server has a\n>last modified date of \"Mon, 14 Aug 1995 23:38:44 GMT\", then returning\n\nArgh.  I mean :42 there.  I would send that out before double-checking.\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Lou Montulli writes:\n\n> I've been getting lots of bug reports due to corrupted or out dated\n> caches.  I would like to propose an extension to the If-modified-since\n> header to improve the situation.  I'd like to start sending\n> \n> If-modified-since: DATE; size=SIZE\n> \n> The addition of size=SIZE informs the server of the current size of the\n> document cached by the client.  The size acts as a checksum, if the size\n> of the file to be served is different than the size given in the\n> If-modified-since header than a 304 should not be returned.\n> \n> The advantage of size over other checksums is that it is highly\n> efficient.  Clients and servers can obtain the information at little or\n> no cost.\n> \n> The disadvantage of size is that it is not completely accurate as a\n> checksum.  An MD5 hash or some other content based checksum would be far\n> more accurate but would require lots of additional overhead.\n> \n> In the future, if a stronger checksum is deemed necessary it can be added\n> as another part of the If-modified-since header.  Perhaps:\n>\n>  If-modified-since: DATE; size=SIZE; md5=SIGNATURE \n> \n> I have tested this change against the Netscape, NCSA, CERN and Apache\n> servers and all of them ignore the addition of size=SIZE, so we can add\n> this addition without fear of backwards compatibility concerns.\n\n  I would say this was a good thing.  I like the idea of using an md5\noptionally.  The processing would generally only be on the server,\nespecially if netscape were to modify its cache index file to include the\nmd5 of the document.  Perhaps the server could be smart about when it\nre-computes the md5 of a document its serving as well.  Perhaps store the\nmd5 and last-modified time in memory for n documents?  Just stat() the file\nand compare the last-modified times to see if it needs to be md5'd again.\n\n  The problem with this is that for an efficient client, an index file for\nthe cache would be necessary.  And I don't like index files. :)\n\n-Bill P.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Can't cache algorithms be modified so that if they spot a timestamp\nthat is more than say half a day into the future (all GMT), then it\ncan safely assume the date is garbage and not trust it for caching ?\n\nIt'd be better to \"punish\" the folks who send out garbage. Sooner or\nlater someone will complain to them and ask for it to be fixed.\n\nrob\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Hello,\n\n  It seems like an obvious solution(??) is for the cache to throw out \nall timestamps greater than a given drift period from the current time.\nIf I told you I did something in the future, would you believe me?\n\nI seems to recall a paper on that measured network clock drift, though \nit is most likely not valid now with all the PCs out there.  Anybody\nelse recall this paper?  If so, we could get an empirical basis for drift.\n\nJim.\n\nMore generally,\n\n<Pine.SOL.3.91.950814215844.21140A-100000@eat.organic.com> from \"Brian Behlendorf\" at Aug 14, 95 10:14:22 pm\n>\n> If people \"screw up their dates\", they're hurting themselves and the people\n> who view their pages.  This *isn't* accidental, is it?  How do you\n> accidentally set a last-modified to be some future time?  It's like a \n> surgeon accidentally removing a right leg instead of a left leg or a \n> disgruntled employee accidentally shooting his boss.  \n> \n> Or maybe it's intentional...\n\nMost likely not. Just about all non-xntp systems need some human to \nadminister - which means occasional ignorance.  A lot of the future\nIMS headers I've seen result from misconfigured systems that have their \ninternal clock set wrong.  Nothing intentional, just their whole filesystem \nhas yet to exist in a sense.\n\n> Okay, so what should a server do when it doesn't know the actual\n> content-length of an object, like a CGI script?  It's totally plausible to\n> ask a dynamic object \"hey, has any data upon which you would answer this\n> question changed since *blah*\" in a way that's very quick to answer, but it's\n> hard to imagine asking it \"hey, will your final output be any size\n> different than *blah*\" without having it do what it normally does.  So, \n> if it's in there, it must be optional for the server to use it.\n\nIf a dymanic object's method has changed this ought to be knowable to the server\neither via the object's intrinsic properties or something like an inode (uglier).\nBut this all assumes server-side execution, which for a lot of cases, can\nbe transfered over to the client, so that only the methods/code get transmitted (stablier)\nFor all but really cpu intensive jobs (probably don't want the world with access\nto those objects anyway), the network cost of transfer ought to out-weight execution, \nso the server retruning a simple hash suggestion seems reasonable.  \n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">I've been getting lots of bug reports due to corrupted or out dated\n>caches.  I would like to propose an extension to the If-modified-since\n>header to improve the situation.  I'd like to start sending\n>\n>If-modified-since: DATE; size=SIZE\n>\n>The addition of size=SIZE informs the server of the current size of\n>the document cached by the client.  The size acts as a checksum,\n>if the size of the file to be served is different than the\n>size given in the If-modified-since header than a 304 should\n>not be returned.  \n\nIn general, I think the idea of giving servers more information from\nwhich to make a decision is a good one.  However, in order to be workable,\nit needs to be the information as delivered by the server (i.e., if it \nis based on length, the client must calculate the length as it is being\nreceived, prior to any local-filesystem-specific writes). \n\nFor HTTP/1.1, we can just specify Content-Length MUST be accurate.\nGiven that, the client can determine whether or not it has the full data\nand adjust its behavior accordingly.  The same goes for MD5 and CRC --\nthe server can send those as headers (or footers, in the case of chunked\nmessages) and the cache can use them to check for validity.  This places\nthe higher processing burden on the clients, where it belongs.\n\nAt the same time, we can specify that IMS may contain additional\nparameters for each of the Content-* headers that are applicable,\nand the server may also use those for determining the correct response.\nThat means \"length\" instead of \"size\" -- there's no sense in using a\ndifferent name for the same thing.\n\nNOTE:  The reason you have been getting lots of bug reports due\nto corrupted or out of date caches is because of a bug in Navigator.\nWhen the user requests a \"Reload\" of a URL, the user agent should add\n\n   Proxy: no-cache\n\nand must not include an If-Modified-Since header.  Navigator 1.1N sends\nthe no-cache directive, but also sends IMS.  Fix that and you won't get\nany more bug reports about cache corruption problems, since the user\nwill be in control of their cache.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">The problems currently encountered are mostly caused by the date comparisons\n>done by most HTTP servers when dealing with If-modified-since requests.\n>Most servers assume that as long as the If-modified-since date is equal to\n>or AFTER the current modification date of the document then it is unchanged.\n\nYes, that is how IMS was designed to work.\n\n>This is a problem because people screw up the dates on their files and\n>sometimes give them dates far into the future.  When they fix the\n>dates of the files to correspond to the current date, caches never\n>get updated.\n\nThat's not entirely true.  The users should have control over the cache,\nand the server can always choose not to honor IMS if it knows that bad\ncopies were given out earlier.  In any case, I don't know of any regional\ncaches that keep non-static documents around for more than a couple days.\n\n>In addition to supporting size=SIZE I encourage other server authors to\n>do an _equals_ comparison rather than a greater than or equal comparison\n>of the two dates.\n\nAbsolutely not -- that would be a violation of both the spec and the\nintentions of IMS.  In addition to being a cache mechanism, it allows\na client to request a document \"only if it has been changed in the last\nfifteen minutes\".  In this case, it may not know what the server's\nLast-Modified date is prior to the request.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "    It seems like an obvious solution(??) is for the cache to throw out\n    all timestamps greater than a given drift period from the current\n    time.  If I told you I did something in the future, would you\n    believe me?\n\nI would suggest that any timestamp more than 1 second in the future\nis suspect, and should be tossed.  Remember that when choosing\nbetween \"cache this\" and \"do not cache this\", the latter is always\nthe safe choice.\n\n    I seems to recall a paper on that measured network clock drift,\n    though it is most likely not valid now with all the PCs out there.\n    Anybody else recall this paper?  If so, we could get an empirical\n    basis for drift.\n    \nDave Mills has written numerous papers, most recently in Proc.\nSIGCOMM '94 (which cites a number of his earlier publications).\nIf the hosts involved are\n    orunning NTP\n    osynchronized to proper time standards\n    ousing reasonably decent clock crystals\nthen it is not at all hard to synchronize clocks to within a few\nmilliseconds, even over the Internet.\n\nIt's quite likely that manufacturers of cheap systems cut corners on\nthe clock-crystal quality, and/or the managers of a server or proxy\nhave failed to configure NTP properly.  So in real life, the clocks\ncould be skewed in either direction.  This, however, is not something\nthat the HTTP spec should be perverted to deal with.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> \n> At 9:25 PM 8/14/95, Lou Montulli wrote:\n> \n> >In addition to supporting size=SIZE I encourage other server authors to\n> >do an _equals_ comparison rather than a greater than or equal comparison\n> >of the two dates.\n> \n> Isn't this what the standard says anyway?\n\nIf it wouldn't be so, an incorrect future date would make a document\nunavailable with a backwards corrected date.\n\n> \n> >>\n> >>Will the \"size\" be determined from the Content-length header or the size on\n> >>the cache's disk?  If the former, documents with incorrect content-length\n> >>headers are essentially uncacheable, as are results from CGI scripts which\n> >>generally don't have content-length headers.  If the latter, could there\n> >>be encoding problems?\n> >\n> >The size is determined by taking the current length of the document in the\n> >cache.  The content-length of the transfer is discarded, so encodings\n> >should have no effect.\n\nIs the content-length discarded by the proxy in every case ? Even\nwithout transfer-encoding header ? \n \n> \n> I assume you anticipate this being implemented by having software simply\n> ask the underlying file system for a byte count of the file? This will\n> break between any two machines with different end of line sequences. As an\n> example, many servers read text files in their native format, converting\n> EOL sequences as they are sent to the client (proxy). This means that the\n> data stream as transmitted to the client (and cached) is a different size\n> from the actual size of the data stored on the disk at the server.\n> \n> I think this problem alone is enough to make this scheme unworkable. This\n> will also break any checksum scheme as well. Macs, Windows, VMS, and Unix\n> servers all have different EOL sequences and the you can bet that the data\n> stream sent by a server is substantially different from the image of the\n> file that resides on the server's disk.\n> \nWhy not keeping the Content-length and transfer encoding headers in the\ncache file, and compare them against the new headers sent by the server\nowning the document ? It is most likely that the transfer encoding, and\nthe transfer length will not change if the document itself hasn't changed.\nIf the transfer encoding scheme is the same, but not the content length,\nit can be assumed that there's a new version of the document. If the\nencoding scheme changed, we still have the Date header to check.\n\n> As I said earlier, the correct solution is to fix the problem with\n> corrupted caches. The modification date is a sufficient mechanism for\n> determining whether or not a document has changed, assuming both ends are\n> able to maintain data integrity. If the proxy server needs to maintain a\n> table of checksums for the files it caches in order to ensure data\n> integrity, that's fine. But it needn't involve the server of the original\n> document in determining whether or not its own cache has been corrupted.\n> Simple comparison of the local checksum to the local data store is\n> sufficient to indicate corruption.\n> \n> --_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n> Chuck Shotton                               StarNine Technologies, Inc.\n> chuck@starnine.com                             http://www.starnine.com/\n> cshotton@biap.com                                  http://www.biap.com/\n>                  \"Shut up and eat your vegetables!\"\n> \n> \n> \nCarlos Horowicz\nMRECIC-ARNET\n\n\n\n"
        },
        {
            "subject": "Re: Expires head coun",
            "content": "Balint Nagy Endre:\n>\n>Recently I counted Exprires headers in my http cache.\n>There were 1714 files, and counted 1 lines beginning Expires:\n>but that was in the body, not in the headers. :-(\n\nThis may not mean that Expires is never used: a request with\n Expires: <yesterday>\nwould never show up in the cache memory.\n\nBut I suspect that even Expires: <yesterday> is not used very often.\n\n>Andrew.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-ietf-http-v10-spec02.txt, .p",
            "content": "A Revised Internet-Draft is available from the on-line Internet-Drafts \ndirectories. This draft is a work item of the HyperText Transfer Protocol \nWorking Group of the IETF.                                                 \n\n       Title     : Hypertext Transfer Protocol -- HTTP/1.0                 \n       Author(s) : T. Berners-Lee, R. Fielding, H. Nielsen\n       Filename  : draft-ietf-http-v10-spec-02.txt, .ps\n       Pages     : 39\n       Date      : 08/14/1995\n\nThe Hypertext Transfer Protocol (HTTP) is an application-level protocol \nwith the lightness and speed necessary for distributed, collaborative, \nhypermedia information systems. It is a generic, stateless, object-oriented\nprotocol which can be used for many tasks, such as name servers and \ndistributed object management systems, through extension of its request \nmethods (commands). A feature of HTTP is the typing and negotiation of data\nrepresentation, allowing systems to be built independently of the data \nbeing transferred.                               \n\nHTTP has been in use by the World-Wide Web global information initiative \nsince 1990. This specification reflects preferred usage of the protocol \nreferred to as \"HTTP/1.0\".                                                                \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-ietf-http-v10-spec-02.txt\".\n Or \n     \"get draft-ietf-http-v10-spec-02.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-ietf-http-v10-spec-02.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-02.txt\".\n Or \n     \"FILE /internet-drafts/draft-ietf-http-v10-spec-02.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950814164110.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-ietf-http-v10-spec-02.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-ietf-http-v10-spec-02.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950814164110.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> At the same time, we can specify that IMS may contain additional\n> parameters for each of the Content-* headers that are applicable,\n> and the server may also use those for determining the correct response.\n> That means \"length\" instead of \"size\" -- there's no sense in using a\n> different name for the same thing.\n\nlength sounds good.  It's two more bytes but it is more consistant.\n\n> \n> NOTE:  The reason you have been getting lots of bug reports due\n> to corrupted or out of date caches is because of a bug in Navigator.\n> When the user requests a \"Reload\" of a URL, the user agent should add\n> \n>    Proxy: no-cache\n> \n> and must not include an If-Modified-Since header.  Navigator 1.1N sends\n> the no-cache directive, but also sends IMS.  Fix that and you won't get\n> any more bug reports about cache corruption problems, since the user\n> will be in control of their cache.\n\nActually it's called \"Pragma: no-cache\", and Netscape has always sent this\nfor reloads.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> >In addition to supporting size=SIZE I encourage other server authors to\n> >do an _equals_ comparison rather than a greater than or equal comparison\n> >of the two dates.\n> \n> Absolutely not -- that would be a violation of both the spec and the\n> intentions of IMS.  In addition to being a cache mechanism, it allows\n> a client to request a document \"only if it has been changed in the last\n> fifteen minutes\".  In this case, it may not know what the server's\n> Last-Modified date is prior to the request.\n\nIf it is a violation of the spec then you really ought to make the spec\nclearer on this point.  All the major servers that I know of do a\ngreater than or equal comparison.  The spec needs to be changed to\nspecifically say that 304 should only be returned when the If-modified-since\ndate matches the current modification date of the file.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": " \n> > and must not include an If-Modified-Since header.  Navigator 1.1N sends\n> > the no-cache directive, but also sends IMS.  Fix that and you won't get\n> > any more bug reports about cache corruption problems, since the user\n> > will be in control of their cache.\n> \n> Actually it's called \"Pragma: no-cache\", and Netscape has always sent this\n> for reloads.\n\nRoy's point is that Netscape also sends IMS along with *all* cached requests.\nThat's what's causing so many Netscape caches to get stuck with junk\nwhich the user cannot refresh, and the only solution is to clear your\nentire Netscape cache.\n\n\"Pragma: no-cache\" is not aimed at end servers (only proxies in between),\nso the end servers are always asked to respond \"304 Not Modified\".\n\nAs Roy says, fix that and most of the problems will go away.\n\n\nrob\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> >I have tested this change against the Netscape, NCSA, CERN and Apache\n> >servers and all of them ignore the addition of size=SIZE, so we\n> >can add this addition without fear of backwards compatibility concerns.\n> \n> This will break MacHTTP and WebSTAR. According to the last published\n> \"standard\", the if-modified-since header field's argument (the date) is\n> terminated with an end of line sequence. This is how it's currently parsed\n> in both of those servers. Given the large installed base of these servers,\n> it'd be nice to find an alternate solution.\n\nAre you sure it will break MacHTTP and WebSTAR, or do you just assume it\nwill?  If it does, these servers are not following the spec.  HTTP\nheaders are MIME headers, and MIME headers can always be followed by\na semi-colon and additional attributes.  If they do break with the\nnew header then we will need to figure out how many are out there and\nweather there are enough to make a significant difference in the\ndecision.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": " \nLou M writes,\n\n> If it is a violation of the spec then you really ought to make the spec\n> clearer on this point.  All the major servers that I know of do a\n> greater than or equal comparison.  The spec needs to be changed to\n> specifically say that 304 should only be returned when the If-modified-since\n> date matches the current modification date of the file.\n\nThere's nothing to say that the If-modified-since is the date given to\nthe client by the server it is subsequently sent to.\n\nCondsider a resource mirrored at A and B\nServer A can give you a Last-modified date for a URL which you can\nthen use on server B.\n\nWhat you appear to be asking for (which might be useful as an alternative)\nis a If-something-different type header,\n\ne.g. \na server sends\n\nFoo: 1234\n\n\nclient asks,\n\nIf-Foo-Different: 1234\n\n\n\nThat might give everyone enough flexibility to increase the number of\n\"304 Not Modified\" messages that servers return. It's difficult for\nscript writers to define Last-modified times for some output, but if\nthey could negotiate on something simpler, with a straightforward comparison,\nthen it'd encourage more script writers to write cache-friendly applications.\n\nI abuse the IMS system with lots of scripts by using it in the way Lou\ndescribes; returning a 304 only if the strings match (give or take a few\nformating differences). Writing a full IMS handler is beyond the \ncapabilities of most CGI writers, and I'm just too lazy.\n\n\nrob\n--\nhttp://nqcd.lanl.gov/~hartill/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">> >In addition to supporting size=SIZE I encourage other server authors to\n>> >do an _equals_ comparison rather than a greater than or equal comparison\n>> >of the two dates.\n>> \n>> Absolutely not -- that would be a violation of both the spec and the\n>> intentions of IMS.  In addition to being a cache mechanism, it allows\n>> a client to request a document \"only if it has been changed in the last\n>> fifteen minutes\".  In this case, it may not know what the server's\n>> Last-Modified date is prior to the request.\n>\n>If it is a violation of the spec then you really ought to make the spec\n>clearer on this point.  All the major servers that I know of do a\n>greater than or equal comparison.  The spec needs to be changed to\n>specifically say that 304 should only be returned when the If-modified-since\n>date matches the current modification date of the file.\n\nNo, the spec is correct as it currently stands.  Your interpretation\n(that it should do date matching and not >= comparison) is incorrect\nas defined by the original IMS design and testing.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Hi Rob,\n\n>What you appear to be asking for (which might be useful as an alternative)\n>is a If-something-different type header,\n\nWhat you are talking about is just having a URN.\nBecause a URN identifies data by content,\nnot by place or date.\n\nBy\n  Danny\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">Are you sure it will break MacHTTP and WebSTAR, or do you just assume it\n>will?  If it does, these servers are not following the spec.  HTTP\n>headers are MIME headers, and MIME headers can always be followed by\n>a semi-colon and additional attributes.\n\nEr, well, no, that is only true of headers which contain\n\n   *(\";\" parameter)\n\nas part of the definition.  Each known header's field value is defined\naccording to the specific BNF for that header -- the generic header parsing\nBNF only applies when the header name is unknown or when the parser\nis not looking inside the field value.\n\nHowever, it is a good rule to follow for robust implementations,\nand we are not talking about a change to HTTP/1.0, but rather HTTP/1.1,\nwhere we are able to change specific header-parsing within reason.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> Roy's point is that Netscape also sends IMS along with *all* cached requests.\n> That's what's causing so many Netscape caches to get stuck with junk\n> which the user cannot refresh, and the only solution is to clear your\n> entire Netscape cache.\n> \n> \"Pragma: no-cache\" is not aimed at end servers (only proxies in between),\n> so the end servers are always asked to respond \"304 Not Modified\".\n> \n> As Roy says, fix that and most of the problems will go away.\n\n\"fix\" is an interresting choice of words.  \"change\" is more appropriate.\nNot sending an \"if-modified-since\" header with reloads would be extremely\ncostly in terms of bandwidth.  Adding cache checksums is a much better \nsolution.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": " \n> \"fix\" is an interresting choice of words.  \"change\" is more appropriate.\n> Not sending an \"if-modified-since\" header with reloads would be extremely\n> costly in terms of bandwidth.  Adding cache checksums is a much better \n> solution.\n\nno no no no no. Let the user *choose* to override the sending of\n\"if-modified-since\".\n\nWhich is the bigger bandwidth waste ... discarding 5Mb of disk cache\nor not sending if i-m-s for a request the user has realised is cached\nincorrectly ?\n\nWe're not alking about having it as a default action, just an option\nfor users to refresh bad cache entries. \n\nPresumably the people sending you \"bug\" reports are doing so because\nthey see junk in their caches and can't get rid of it. Your users will\nstill have the same problem with URLs which don't have checksums, and \nthat's going to be the norm for a long time to come.\n\n\nrob\n--\nhttp://nqcd.lanl.gov/~hartill/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "On Tue, 15 Aug 1995, Lou Montulli wrote:\n> > >In addition to supporting size=SIZE I encourage other server authors to\n> > >do an _equals_ comparison rather than a greater than or equal comparison\n> > >of the two dates.\n> > \n> > Absolutely not -- that would be a violation of both the spec and the\n> > intentions of IMS.  In addition to being a cache mechanism, it allows\n> > a client to request a document \"only if it has been changed in the last\n> > fifteen minutes\".  In this case, it may not know what the server's\n> > Last-Modified date is prior to the request.\n> \n> If it is a violation of the spec then you really ought to make the spec\n> clearer on this point.  All the major servers that I know of do a\n> greater than or equal comparison.  \n\n...which is what Roy's saying the spec says they should do.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">> As Roy says, fix that and most of the problems will go away.\n>\n>\"fix\" is an interresting choice of words.  \"change\" is more appropriate.\n>Not sending an \"if-modified-since\" header with reloads would be extremely\n>costly in terms of bandwidth.  Adding cache checksums is a much better \n>solution.\n\nBrowser semantics -- not defined by any spec, but well known enough\nthat I can say with absolute certainty that this is a bug in Navigator.\n\nIf the user requests a \"Reload\" operation, they are asking for a\nnew copy of the resource from its origin.\n\nSending IMS prevents a new copy being generated by the origin server.\n\nSo, don't send IMS when the user hits the Reload button! \n\nBelieve me, nobody is more concerned about network bandwidth and\ncaching than I am.  However, caching only works if the user is\nin control of the cache, not the other way around.  This has\nless significant impact on bandwidth than do the thousands of hopeless\nattempts to override this behavior by current Navigator users.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "HTTP servers and caches are expected to keep correct time. Clients are\ngenerally not. A well-written server encountering a file with a\nmodification date in the future (according to the server's current\ntime) should not send the future 'last-modified' date; if that were to\nhappen anyway, a proxy or cache shouldn't save a document with a\nfuture last-modified.\n\nA client and any proxies along the way should remember the exact\nlast-modified (to the second) with which the document was delivered,\nno approximations, and independent of any local time settings.\n\nWith this scheme, any file whose date was set early (written in 1970)\ncan be updated by setting the modify-date to the current time. Any\nfile whose date was set far in the future will not have been cached.\nIn the infrequently occuring case where a document is actually\nmodified at 12:04 and sent out at 12:05, but the cache's clock is\nslightly misadjusted to think it is 12:03, a document that could be\ncached will not be; the next time the document is requested, though,\nthe modification date will still be 12:04 and caching will happen.\n\nWith these simple conventions (which are just good implementation\nconventions for the current protocol) we don't need to change the\nprotocol or add any heuristics about future or past dates.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">> I think this problem alone is enough to make this scheme unworkable. This\n>> will also break any checksum scheme as well. Macs, Windows, VMS, and Unix\n>> servers all have different EOL sequences and the you can bet that the data\n>> stream sent by a server is substantially different from the image of the\n>> file that resides on the server's disk.\n>>\n>Why not keeping the Content-length and transfer encoding headers in the\n>cache file, and compare them against the new headers sent by the server\n>owning the document ?\n\nThe problem is that the disk storage used on the server is not the same as\nthe disk storage used on the client/proxy. Therefore, the ONLY way that a\nserver can confirm a SIZE request sent by a client/proxy is to parse the\nfile, doing whatever conversions it would normally do and then compute the\nsize of the hypothetical output stream. At this point, it might as well\njust send the entire file, because the server just read it all.\n\nThe point I was trying to make is that size is meaningless between any two\nmachines when text files are being compared. CR/LF on a Windows box may be\nstored as LF only on a Unix host, or CR on a Mac may be converted into\nCR/LF for transmission. This means that the original file on the server is\na physically different size than the file transmitted to the client/proxy,\nand that the cached file size on the proxy may be an entirely different\nsize than the content-length.\n\nFile size is essentially useless as a mechanism for determining whether or\nnot a cached file is the correct version.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 1:11 PM 8/15/95, Lou Montulli wrote:\n>> >I have tested this change against the Netscape, NCSA, CERN and Apache\n>> >servers and all of them ignore the addition of size=SIZE, so we\n>> >can add this addition without fear of backwards compatibility concerns.\n>>\n>> This will break MacHTTP and WebSTAR. According to the last published\n>> \"standard\", the if-modified-since header field's argument (the date) is\n>> terminated with an end of line sequence. This is how it's currently parsed\n>> in both of those servers. Given the large installed base of these servers,\n>> it'd be nice to find an alternate solution.\n>\n>Are you sure it will break MacHTTP and WebSTAR, or do you just assume it\n>will?\n\nGiven that I'm the author, I'm pretty sure. :)\n\n>  If it does, these servers are not following the spec.  HTTP\n>headers are MIME headers, and MIME headers can always be followed by\n>a semi-colon and additional attributes.  If they do break with the\n>new header then we will need to figure out how many are out there and\n>weather there are enough to make a significant difference in the\n>decision.\n\nWell, according to the last 2 Web surveys, it's either the #2 or #3 server\nin terms of installed base (depending on survey), along with NCSA and CERN\nhttpd. I think we need to worry about the installed base.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Expires, Last-Modified, Pragma: nocache etc",
            "content": "It seems to me that much (but not all) of the discussion over the last\nfew months on the Expires, Last-Modified headers is really due to the\ndesire of server administrators to control whether a document is cached.\n\nOne of the big improvements in the current HTTP spec is the \"Pragma:\nno-cache\" header.  This provides a way to say, \"Don't cache this,\"\nwithout the need for fictitious expiration or last-modified dates.\nThis is important.  There are often good reasons for saying that a\ndocument should not be cached, but having a future expiration date\n(for example, if the validity of the document doesn't, in fact *expire*\nuntil that date).\n\nThe only problem is that server admins often want to prevent local disk\ncaching as well as proxy caching and the Pragma: no-cache applies only\nto the latter.  Perhaps we also need a \"Pragma: no-local-cache\".\n\nOne thing of which I am thoroughly convinced is that if a clean mechanism\nfor giving administrators this capablility is not made available then\nevery conceivable ugly hack which has the desired effect will gain\nwidespread use.  We will see 1970 expiration dates (always a lie) and\nfuture last-modified dates (also a lie).  These hacks will likely work\nwith some clients/proxies and fail with others. There will, no doubt,\neven be those who use different hacks based on the User-Agent header\nof the request!  Why not make life easier for everyone and allow a server\nto cleanly request that a document not be cached to local disk.\nThis should be a separate request from the Pragma: no-cache request to\nprevent proxy caching.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> If the user requests a \"Reload\" operation, they are asking for a\n> new copy of the resource from its origin.\n\nWe obviously have different opinions on what a \"reload\" should do.\n\nMy interpretation of \"reload\" is to check everything on the page\nand retransfer any objects that have changed.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 7:53 PM 8/15/95, Lou Montulli wrote:\n>> If the user requests a \"Reload\" operation, they are asking for a\n>> new copy of the resource from its origin.\n>\n>We obviously have different opinions on what a \"reload\" should do.\n>\n>My interpretation of \"reload\" is to check everything on the page\n>and retransfer any objects that have changed.\n\nHmmm. I always thought it meant reload everything on the page. Seemed obvious.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> \n> > If the user requests a \"Reload\" operation, they are asking for a\n> > new copy of the resource from its origin.\n> \n> We obviously have different opinions on what a \"reload\" should do.\n> \n> My interpretation of \"reload\" is to check everything on the page\n> and retransfer any objects that have changed.\n\nThis is a case where it doesn't pay to be too clever.\n\nIf everything works perfectly, there is seldom a reason for\npeople to use \"reload\". The two cases I use it are when I have\na document that looks like it was corrupted by a transmission\nerror or when I have outside information that a document *has*\nchanged (usually because I'm editing it.)\n\nIn neither case do I want a local or remote cache be in the picture.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> >> I think this problem alone is enough to make this scheme unworkable. This\n> >> will also break any checksum scheme as well. Macs, Windows, VMS, and Unix\n> >> servers all have different EOL sequences and the you can bet that the data\n> >> stream sent by a server is substantially different from the image of the\n> >> file that resides on the server's disk.\n> >>\n> >Why not keeping the Content-length and transfer encoding headers in the\n> >cache file, and compare them against the new headers sent by the server\n> >owning the document ?\n> \n> The problem is that the disk storage used on the server is not the same as\n> the disk storage used on the client/proxy. Therefore, the ONLY way that a\n> server can confirm a SIZE request sent by a client/proxy is to parse the\n> file, doing whatever conversions it would normally do and then compute the\n> size of the hypothetical output stream. At this point, it might as well\n> just send the entire file, because the server just read it all.\n\nIf a server chooses to ignore the SIZE that's fine, but your arguments\nare not true for most web server platforms.  You are also assuming that\nyour bandwidth is unlimited, it is certainly not just as efficient to\nsend the entire file as it is to parse the whole file to determine the\nsize.\n\n> \n> The point I was trying to make is that size is meaningless between any two\n> machines when text files are being compared. CR/LF on a Windows box may be\n> stored as LF only on a Unix host, or CR on a Mac may be converted into\n> CR/LF for transmission. This means that the original file on the server is\n> a physically different size than the file transmitted to the client/proxy,\n> and that the cached file size on the proxy may be an entirely different\n> size than the content-length.\n> \n> File size is essentially useless as a mechanism for determining whether or\n> not a cached file is the correct version.\n\nYour facts don't support your conclusion.  If linefeed conversion is\nperformed by the server then it should be done consistantly, you can \ntherefore always compute the eventual size of the object by parsing the \nfile.  If a proxy modifies the file in any way then it needs to remember\nit's original size. \n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "> It seems to me that much (but not all) of the discussion over the last\n> few months on the Expires, Last-Modified headers is really due to the\n> desire of server administrators to control whether a document is cached.\n> \n> One of the big improvements in the current HTTP spec is the \"Pragma:\n> no-cache\" header.  This provides a way to say, \"Don't cache this,\"\n> without the need for fictitious expiration or last-modified dates.\n> This is important.  There are often good reasons for saying that a\n> document should not be cached, but having a future expiration date\n> (for example, if the validity of the document doesn't, in fact *expire*\n> until that date).\n> \n> The only problem is that server admins often want to prevent local disk\n> caching as well as proxy caching and the Pragma: no-cache applies only\n> to the latter.  Perhaps we also need a \"Pragma: no-local-cache\".\n> \n> One thing of which I am thoroughly convinced is that if a clean mechanism\n> for giving administrators this capablility is not made available then\n> every conceivable ugly hack which has the desired effect will gain\n> widespread use.  We will see 1970 expiration dates (always a lie) and\n> future last-modified dates (also a lie).  These hacks will likely work\n> with some clients/proxies and fail with others. There will, no doubt,\n> even be those who use different hacks based on the User-Agent header\n> of the request!  Why not make life easier for everyone and allow a server\n> to cleanly request that a document not be cached to local disk.\n> This should be a separate request from the Pragma: no-cache request to\n> prevent proxy caching.\n\nI recently changed netscape to interpret \"Pragma: no-cache\" and not\ncache the object.  This is slightly different than a \"Expires\" header\nbecause the object will not even be cached for history navigation.\n(Documents that are expired are still shown when traversing the session\nhistory).  Haveing the client interpret \"Pragma: no-cache\" lets servers\ntell the client that this information is highly sensitive or volitile\nand should not be cached in any way.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "According to Lou Montulli:\n> \n> I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> cache the object.  This is slightly different than a \"Expires\" header\n> because the object will not even be cached for history navigation.\n> (Documents that are expired are still shown when traversing the session\n> history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n> tell the client that this information is highly sensitive or volitile\n> and should not be cached in any way.\n> \n\nWell, I find this reasonable.  But, I believe that the latest version of\nthe spec says the client should ignore Pragma: no-cache.  What I think\nis important is that there be some way to stop the client from caching\n-- as apparently there now is with Netscape. :)\n\nI don't care if it is Pragma: no-cache or if there is a separate way\n(Pragma: no-local-cache ?) but apparently someone does since the spec\nwants the client to ignore Pragma: no-cache.  Could someone  explain\nthe rationale for this?\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Lou Montulli writes:\n> John Franks writes:\n> > It seems to me that much (but not all) of the discussion over the last\n> > few months on the Expires, Last-Modified headers is really due to the\n> > desire of server administrators to control whether a document is cached.\n> > \n> > One of the big improvements in the current HTTP spec is the \"Pragma:\n> > no-cache\" header.  This provides a way to say, \"Don't cache this,\"\n> > without the need for fictitious expiration or last-modified dates.\n> > This is important.  There are often good reasons for saying that a\n> > document should not be cached, but having a future expiration date\n> > (for example, if the validity of the document doesn't, in fact *expire*\n> > until that date).\n> > \n> > The only problem is that server admins often want to prevent local disk\n> > caching as well as proxy caching and the Pragma: no-cache applies only\n> > to the latter.  Perhaps we also need a \"Pragma: no-local-cache\".\n> > \n> > One thing of which I am thoroughly convinced is that if a clean mechanism\n> > for giving administrators this capablility is not made available then\n> > every conceivable ugly hack which has the desired effect will gain\n> > widespread use.  We will see 1970 expiration dates (always a lie) and\n> > future last-modified dates (also a lie).  These hacks will likely work\n> > with some clients/proxies and fail with others. There will, no doubt,\n> > even be those who use different hacks based on the User-Agent header\n> > of the request!  Why not make life easier for everyone and allow a server\n> > to cleanly request that a document not be cached to local disk.\n> > This should be a separate request from the Pragma: no-cache request to\n> > prevent proxy caching.\nIt is an interesting question: there are any good reasons to forbid\nclients to cache documents in its non-shared cache? Doubtful, I think.\nBut if anybody can give good examples explaining the need for this,\nthen we shall consider adding the function proposed by \"Pragma: no-local-cache\",\nbut (see later)\n> I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> cache the object.  This is slightly different than a \"Expires\" header\n> because the object will not even be cached for history navigation.\n> (Documents that are expired are still shown when traversing the session\n> history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n> tell the client that this information is highly sensitive or volitile\n> and should not be cached in any way.\nOops! The draft-02 states: (in section 8.13)\n   Pragma directives do not apply to the end-points of a \n   request/response chain. For example, a user agent's internal\n   (non-shared) cache and/or history mechanism should ignore all \n   pragma directives in received messages. Similarly, pragma \n   directives are not applicable to the origin of a resource, though \n   they may be applicable to a server's internal response cache.\nYour change is a non-conformance!\n(continuing on \"Pragma: no-local-cache\") but we should do this using some\nother machanism, not a pragma!\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "In article <199508160450.XAA04365@hopf.math.nwu.edu> John Franks\n<john@math.nwu.edu> wrote:\n> \n> According to Lou Montulli:\n> \n> > I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> > cache the object.  This is slightly different than a \"Expires\" header\n> > because the object will not even be cached for history navigation.\n> > (Documents that are expired are still shown when traversing the session\n> > history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n> > tell the client that this information is highly sensitive or volitile\n> > and should not be cached in any way.\n> \n> \n> Well, I find this reasonable.  But, I believe that the latest version of\n> the spec says the client should ignore Pragma: no-cache.  What I think\n> is important is that there be some way to stop the client from caching\n> -- as apparently there now is with Netscape. :)\n> \n> I don't care if it is Pragma: no-cache or if there is a separate way\n> (Pragma: no-local-cache ?) but apparently someone does since the spec\n> wants the client to ignore Pragma: no-cache.  Could someone  explain\n> the rationale for this?\n> \n\nI would prefer a more standardized way of doing it if one exists or\nis created.  I haven't released any code that parses Pragma: no-cache\nyet, so lets come up with something different and use that.\n\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">> If the user requests a \"Reload\" operation, they are asking for a\n>> new copy of the resource from its origin.\n>\n>We obviously have different opinions on what a \"reload\" should do.\n>\n>My interpretation of \"reload\" is to check everything on the page\n>and retransfer any objects that have changed.\n\nThe behavior you describe sounds more appropriate for a 'refresh' button\nthan a 'reload' button.\n\nThe use of 'Refresh' in Netscape, on the other hand, seems to me to really\nmean 'reload'.  Doesn't a 'refresh' entity header reload the document\nwhether it has changed or not?\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "reload discussio",
            "content": "Lou Montulli and Albert Lundle discussing the reload operation:\n[Albert]\n> > > If the user requests a \"Reload\" operation, they are asking for a\n> > > new copy of the resource from its origin.\n[Lou]\n> > We obviously have different opinions on what a \"reload\" should do.\n> > \n> > My interpretation of \"reload\" is to check everything on the page\n> > and retransfer any objects that have changed.\n> \n> This is a case where it doesn't pay to be too clever.\n> \n[Albert]\n> If everything works perfectly, there is seldom a reason for\n> people to use \"reload\". The two cases I use it are when I have\n> a document that looks like it was corrupted by a transmission\n> error or when I have outside information that a document *has*\n> changed (usually because I'm editing it.)\n> \n> In neither case do I want a local or remote cache be in the picture.\nI use Lynx, Netscape and sometimes Mosaic.\nAll of them has a refresh function - e.g. screen refresh -\nI quesst, normally this shouldn't involve any http action, using local cached\ncopy of the document.\nI just now realised, that the \"Pragma: no-local-cache\" proposed by John Franks\ncan prevent this. <em> I dont want http actions to refresh my lynx screen when\nit's garbled by talkd or write! </em>\nThe reload funtion, on contrary does start a http action, but which action it\nreally does, I not investigated till now.\nMy expectations to the \"reload\" function are:\nre-check the status of the document, and reflect changes in case it's changed.\nThis should be done using HEAD method (HTTP/0.9 clients) or\nGET/If-Modified-Since (HTTP/1.x clients).\nIf I am not satisfyed with the result - (I expected a new version, \nI'm sure that the document changed), I will request a reload again.\nIn this case, a good client may issue a plain GET, if it knows, that is\ntalking to the origin server directly, or a GET/no-cache if knows, that\nthere is some intermediate proxy. While pragmas have effect only on\nintermediates (and I don't want changes in this), it's safe to add the no-cache\npragma, when no-intermediate proxies are involved in the action.\nCan we agree on this approach?\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <v02120d01ac5733010a9c@[204.156.156.16]> hedlund@best.com (Marc\nHedlund) wrote:\n> \n> >> If the user requests a \"Reload\" operation, they are asking for a\n> >> new copy of the resource from its origin.\n> \n> >We obviously have different opinions on what a \"reload\" should do.\n> \n> >My interpretation of \"reload\" is to check everything on the page\n> >and retransfer any objects that have changed.\n> \n> The behavior you describe sounds more appropriate for a 'refresh' button\n> than a 'reload' button.\n\nI agree, 'refresh' does sound like a more appropriate word.  I'll\npass that along to our UI persons.\n\n> \n> The use of 'Refresh' in Netscape, on the other hand, seems to me to really\n> mean 'reload'.  Doesn't a 'refresh' entity header reload the document\n> whether it has changed or not?\n> \n\nI'm currently attempting to change the netscape UI to allow both \nrefresh and what I call \"Super Reload\" (Retransfer everything).\n\"Refresh\", as you call it, will continue to be the default.\nAs others have pointed out, if the cacheing scheme is perfect,\n\"Super Reload\" would never be necessary.  Adding size=SIZE or\nlength=SIZE gets us a little closer and if necessary we could\nmove to MD5.  \n\nHoping for a perfect caching world,\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "If-ModifiedSince and forged date",
            "content": "Hi all,\nI am reponding to the longly discussion, and found no individual message\nto reply to.\nI think, that improving the If-Modified-Since header is a good idea.\nSupplying size and checksums (16bit CRC, 32bit CRC, md5 or md3 if anybody thinks\nthat md5 is too constly) will add some redundancy to protocol.\nGenerally, redundancy is a good stuff to overcome data integrity problems and\nare welcome here, I think.\nThe question to be answered by WG is: which kind of redundancy shall be built-in\ninto the protocoll now or later?\nThe debate on proxies, modifying the Entity-Body is a misunderstandig, I\npresume. Proxies, which are application gateways in TCP/IP sense, aren't\ngateways in http sense (according to draft), and even if they act as gateways\nbetween 0.9 and 1.x parties, they shouldn't modify (applying conversions, etc)\nEntity-Bodies, they will mangle headers only, right?\n\nDealing with forged dates is an other question.\nA picky server implementor may check the timestamp of a file (last modified\ntimestamp, or mtime in unix terms), and when it looks unreal, can use the\nctime instead if the filesystem supports that. (the ctime can't be forged by users. But users can touch his/her files every day or even more often...)\nThis can prevent users in creating documents, having bad last-modified dates,\nbut will not prevent changing system clock, but that can be done only by system\nadministrators.  I hope, the number of system administrators, willing to do\nsuch misconfiguration is negligible.\nI suggest, that this type of workaround should be enabled/diabled in a\nconfiguration option.\nGenerally, this trouble can be overcome in better education of users, and\nelaborating appropriate hit count reporting schemes.\nThe later is our job, and we shall work on that seriously!\nAnd educating users is the job of sysadmins and webmasters.\nWe can publich informational RFC-s on this subject, too.\nBut first, whe shall use Expires: headers as the draft requires whereever\nwe create web documents! (including the WG's hypertext archives)\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "If we have a currently working and then someone says \"I have a\nproblem, users complain about the following bug: bzzzt\" and then \"I\npropose the following solution: sssss\" then it's up to us to decide:\n\n1) Is bzzzt really a problem?\n2) Does sssss actually solve the problem?\n3) Is sssss the best solution? In particular, can the\n   problem be solved without changing the protocol?\n================================================================\nI think in the case of\nbzzzt = 'file has bad date in the future' \nsssss = 'add ; length=nnnn to if-modified request'\n\nThe answers are\n(1) yes: it's a problem. We've all seen it, know that it has multiple\n    causes which are out of our control.\n(2) usually: except when the length is wrong, or hard to compute, or\n    the document is dynamic, or has a different EOL convention and is\n    translated on the fly; we've identified some exceptions.\n\nHowever, I believe that \n(3) is false: the problem can be solved without changing the protocol,\n    as I outlined in a previous message:\n\n* A server encountering a file with a modification date in the future\n  (according to the server's current time) should not send the future\n  'last-modified' date.\n* A proxy or cache shouldn't save a document with a future\n  last-modified.\n* A client and any proxies along the way should remember the exact\n  last-modified (to the second) with which the document was delivered,\n  no approximations, and independent of any local time settings.\n\nLet us also assert that HTTP server and caches on the Internet should\ntry to keep correct time.  There are widely available implementations\nof (S)NTP for most machines. I urge everyone who has a web site about\n'available HTTP servers' to also point to NTP/SNTP implementations.\n\nIf these rules are followed, the only case where a document will be\nmis-cached because of date lossage would be where the file date was in\nthe future but the file content has has been changed and the date set\nback, the server date was in the future but has been reset to the\npresent, and the cacher date was in the future but has now been reset.\n\nThe simplest workaround to that would be for the cache to\n(aggressively) treat as 'expired' any item for which the cache date is\nafter the current date; if the cacher's date gets reset toward the\npast, for example, it would be good practice to immediately clear any\ndata that was cached with a future date.\n================================================================\nIf the problem 'bzzzt' is something else (file system or data\ntransmission errors in the server or cache) then 'length' certainly\nwon't solve the problem, and anything other than checksums at proxy\nand server sites also seems like overkill.  Let's understand the\nnature of the kinds of problems we're trying to solve before we\nengineer a fix.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Tue, 15 Aug 1995, Lou Montulli wrote:\n\n> I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> cache the object.  This is slightly different than a \"Expires\" header\n> because the object will not even be cached for history navigation.\n\nI believe this is the wrong design.  The user *MUST* be able to always\nflip the page backward and see the same material just viewed.  THis\nis a critical user interface usability issue. Within the same session/\ninstance of UAgent execution. I believe it would be/is wrong to\nsilently refresh a document during history navigation when the history\ncache has overflowed. I (and many users I know) expect the history\nto be a record of what I've seen. I would have not objection to a \nbrowser which checked currency and via a non-modal message advised that\nthe history copy wasn't current (\"Current copy not current, RELOAD for\nthe latest copy\" for example).\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "On Tue, 15 Aug 1995, Lou Montulli wrote:\n\n> > If the user requests a \"Reload\" operation, they are asking for a\n> > new copy of the resource from its origin.\n> \n> We obviously have different opinions on what a \"reload\" should do.\n> \n> My interpretation of \"reload\" is to check everything on the page\n> and retransfer any objects that have changed.\n\nThere needs to be a way to recover from errors at every stage of the\npath from the original data to the user's screen.  There could be\ntranmission corruption, a proxies data storeage may have gotten\nstepped on, etc.  If not \"Reload\", what?\n\n> :lou\n> -- \n> Lou Montulli                 http://www.mcom.com/people/montulli/\n>        Netscape Communications Corp.\n\nDonald\n=====================================================================\nDonald E. Eastlake 3rd     +1 508-287-4877(tel)     dee@cybercash.com\n   318 Acton Street        +1 508-371-7148(fax)     dee@world.std.com\nCarlisle, MA 01741 USA     +1 703-620-4200(main office, Reston, VA)\n\n\n\n"
        },
        {
            "subject": "Glossar: reload, refresh, nocach",
            "content": "Hi folks,\n\nI'm new in the http-wg list, but I'm listening\nfor a long time to some of the www-*-lists.\n\nI just listened to a part of the discussions\nof reload/refresh and no-cache.\n\nI'm wondering if someone of you could draft\na HTML/HTTP-glossar. This glossar\nshould contain not only a technical clear\nexplanation of an expression, but should also\ncontain a recommended user-interface-dialog\nwhen the user has to deal with the expression.\n\nI'm not able to do it, is someone of you ?\n\n----\nHere's my opinion to reload, refresh and no-cache:\n\n-reload\n Reload means reload and nothing else. In other words:\n LOAD IT AGAIN !!!\n\n-refresh\n I'm not sure, but if I say Refresh I expect\n  - do a screen refresh\n  and/or\n  - do whatever is necessary to give me something fresh\n    of what I just had\n\n-no-cache\n If I say no-cache, I mean Don't cache, and if the current\n http-spec says the chain-ends of a http-request have to ignore\n any Pragma-directive, they shuold do it. But everything between\n the chain must follow the directive so these shouldn't cache.\n\n\nBy\n  Danny\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 8:07 PM 8/15/95, Lou Montulli wrote:\n\n>> The point I was trying to make is that size is meaningless between any two\n>> machines when text files are being compared. CR/LF on a Windows box may be\n>> stored as LF only on a Unix host, or CR on a Mac may be converted into\n>> CR/LF for transmission. This means that the original file on the server is\n>> a physically different size than the file transmitted to the client/proxy,\n>> and that the cached file size on the proxy may be an entirely different\n>> size than the content-length.\n>>\n>> File size is essentially useless as a mechanism for determining whether or\n>> not a cached file is the correct version.\n>\n>Your facts don't support your conclusion.  If linefeed conversion is\n>performed by the server then it should be done consistantly, you can\n>therefore always compute the eventual size of the object by parsing the\n>file.  If a proxy modifies the file in any way then it needs to remember\n>it's original size.\n\nLou, why are you forcing this computation on the server? The whole problem\nof corrupted or stale caches is a CLIENT problem and the computation should\nhappen there. Why should a server be forced to read and translate every\nbyte of a file, just so it can calculate the content-length for a IMS\nrequest from a client that is trying to use file size to determine file\n\"sameness\"? This is extremely burdensome on the server and shouldn't be the\nserver's job.\n\nHere is an alternative to this whole \"size\" thing. Let's start with an\nassumption. You may not agree with it, but my experience shows otherwise.\nLet's assume that for any client/server pair, the two machines store the\nsame text file with different sizes because of differences in EOL\nconventions. That means that the ONLY common size they can have is the\ncontent-length that the server reports. Since this requires action on the\nserver to compute, it isn't an acceptable mechanism. SO, why not do the\nfollowing?\n\nWhen a server sends a file, the client should make sure that it received at\nleast content-length bytes of data. If it doesn't receive the complete\nfile, it can still display the file, but shouldn't cache it. If the client\nDOES receive the required number of bytes, it should compute a checksum for\nthe file and cache the contents. Future requests for the file can simply\nuse the existing IMS syntax to retrieve new versions based on modification\ndates. If dates match, the client can verify that the file in the local\ncache is uncorrupted simply by recalculating and comparing the checksums.\nAll the work for this is done by the client, no server modifications are\nrequired, and no wasted CPU cycles are spent on the server calculating\ncontent-length values for a file that won't be transmitted.\n\nWhat in this proposal doesn't meet the initial requirements for the\nproposed \"SIZE\" addition it the IMS header? You are still able to confirm\nthat you have the current version, and you are also able to detect\ntruncated and/or corrupted files on the client side. And, it doesn't break\nthe installed server base.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "According to Larry Masinter:\n\n> * A proxy or cache shouldn't save a document with a future\n>   last-modified.\n\n\nIn the absence of another (cleaner) way to prevent caching this will\nbecome the de facto way for a server to tell a client (or proxy) not to cache.\n\nThis, in effect, overloads the Last-Modified header so that \n\nLast-Modified: <date in 2010>\n\nmeans \"please don't cache this document.\"  This is obscure and error prone.\nIt would be much better to have a clean, explicit way for a server to request\nthat a document not be cached by a proxy or client.  The current draft spec\ngives a way to make the request of proxies but not of clients.  I would be\nhappy with one request applying to both or separate requests.  I haven't\nheard any argument for separate requests.\n\nSomeone asked why you would want to request that a document not be\ncached by the client in an unshared cache.  There are lots of reasons.  Think\nabout the famous coffee pot.\n\nThere are also reasons that a request not to cache should be orthogonal to\nthe Expires: header.  The most important in my opinion is that not making\nthem orthogonal encourages munging the header in order to prevent caching.\nFictitious expire dates overloading this header are a bad idea.  Another\nreason is that a document can be *valid* for some period of time even\nthough a new request will give a different document.  Think of a price\nquote made yesterday which will be honored until its expiration date,\nbut a new quote will have a different price.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "John Franks writes:\n > According to Lou Montulli:\n > > \n > > I recently changed netscape to interpret \"Pragma: no-cache\" and not\n > > cache the object.  This is slightly different than a \"Expires\" header\n > > because the object will not even be cached for history navigation.\n > > (Documents that are expired are still shown when traversing the session\n > > history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n > > tell the client that this information is highly sensitive or volitile\n > > and should not be cached in any way.\n > > \n > \n > Well, I find this reasonable.  But, I believe that the latest version of\n > the spec says the client should ignore Pragma: no-cache.  What I think\n > is important is that there be some way to stop the client from caching\n > -- as apparently there now is with Netscape. :)\n > \n > I don't care if it is Pragma: no-cache or if there is a separate way\n > (Pragma: no-local-cache ?) but apparently someone does since the spec\n > wants the client to ignore Pragma: no-cache.  Could someone  explain\n > the rationale for this?\n > \n > John Franks\n\nThe reason Pragma: no-cache is not a redundant way of saying Expires:\n<= now is that it allows servers to send essentially \"private\"\ndocuments to particular clients where these documents need not expire\nimmediately.  If intermediate proxies cannot cache the document (due\nto Pragma: no-cache) but clients can, this makes for more flexibility\nat the client end.  The client would not be required to issue a new\nHTTP request when revisiting the non-expired document.  Since this\nappears to be the only added functionality of Pragma: no-cache as a\nreturn header, though, I think it probably should have been called\nsomething else, such as Pragma: private (or something equally, or more,\nmnemonic).  The problem with calling it Pragma: no-cache is that\nuser-agent designers will feel that it ought to have some effect on\nthe client, which it shouldn't, because Expires is sufficient for\ncontrolling caching in the client.\n\nSo I think the above mentioned changes to Netscape are probably ill\nadvised and in fact defeat the purpose of this header.  If it stays in\nthe Netscape browser as mentioned, the new header will have been\nrendered useless.  (It will not be possible to send \"private\"\ndocuments that are client-cacheable, which is the only added benefit\nof it in the first place).\n\n\n--Shel Kaphan\n  sjk@amazon.com\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "> In the absence of another (cleaner) way to prevent caching this will\n> become the de facto way for a server to tell a client (or proxy) not\n> to cache.\n> \n> This, in effect, overloads the Last-Modified header so that \n> \n> Last-Modified: <date in 2010>\n> \n> means \"please don't cache this document.\"  This is obscure and error prone.\n\nUh, well, just leaving it out will tell the same thing.  That would be\na much better \"de facto\", doesn't involve forging.  Both NS and CERN\nproxies support it now.\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "John Franks writes:\n > According to Larry Masinter:\n > \n > > * A proxy or cache shouldn't save a document with a future\n > >   last-modified.\n > \n > \n > In the absence of another (cleaner) way to prevent caching this will\n > become the de facto way for a server to tell a client (or proxy) not to cache.\n > \n > This, in effect, overloads the Last-Modified header so that \n > \n > Last-Modified: <date in 2010>\n > \n > means \"please don't cache this document.\"  This is obscure and error prone.\n\nI agree that it would be bad if people started deliberately using it\nin this way.  The question is, what is the desired behavior?  The rule\nI would favor is that you needn't compare the last-modified date to\nthe present date at all.  If on one request for a document you get\nlast-modified date X, and on the next you get X+1, then you know the\nserver is saying the document has been modified since the last\nrequest.  That's the only relevant semantic.  GET if-modified since\nshould use the last claimed modification date when making a request. \n\n > It would be much better to have a clean, explicit way for a server to request\n > that a document not be cached by a proxy or client.  The current draft spec\n > gives a way to make the request of proxies but not of clients.\n\nWhat about Expires: <= now?\n\n  I would be\n > happy with one request applying to both or separate requests.  I haven't\n > heard any argument for separate requests.\n > \nSee my previous post regarding Pragma: no-cache, which, by your\nreckoning, is, I guess, a third way to request no caching.\n\n\n > Someone asked why you would want to request that a document not be\n > cached by the client in an unshared cache.  There are lots of reasons.  Think\n > about the famous coffee pot.\n > \n > There are also reasons that a request not to cache should be orthogonal to\n > the Expires: header.  The most important in my opinion is that not making\n > them orthogonal encourages munging the header in order to prevent caching.\n > Fictitious expire dates overloading this header are a bad idea.\n\nThe only possible interpretation (ok, to me, anyway) of an expiration\ndate is that after that date, if you try to look at the document\nagain, you have to refetch it.  Thus the primary purpose of expires is\nto control caching in exactly this way. Why is this overloading?\n\n  Another\n > reason is that a document can be *valid* for some period of time even\n > though a new request will give a different document.  Think of a price\n > quote made yesterday which will be honored until its expiration date,\n > but a new quote will have a different price.\n > \nNow trying to implement *that* at the protocol level really seems like\noverloading to me.\n\n > John Franks\n > \n > \n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Uh, never mind what I just said about future last-modified dates.\nIt might be OK for GET if-modified-since, but obviously fails for\nplain GETs.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "> Last-Modified: <date in 2010>\n\n> means \"please don't cache this document.\"  This is obscure and error prone.\n\nNo, Last-Modified: <date in 2010> means \"I am broken, please notify my\nwebmaster to fix my clock\". (Unless it's already 2010, I suppose.) \n\nI don't think we can possibly design a protocol that can resist the\ndetermined attempts to abuse it, and I don't think we should.\n\n> Someone asked why you would want to request that a document not be\n> cached by the client in an unshared cache.  There are lots of reasons.  Think\n> about the famous coffee pot.\n\nAs you point out, 'Expires' is the right way to handle this.\n\n> There are also reasons that a request not to cache should be orthogonal to\n> the Expires: header.  The most important in my opinion is that not making\n> them orthogonal encourages munging the header in order to prevent caching.\n> Fictitious expire dates overloading this header are a bad idea. \n\nWhy are such 'expires' dates fictitious? Why do you think people would\nnot overload 'Expires' but would overload 'Last-Modified'.\n\n> Another reason is that a document can be *valid* for some period of\n> time even though a new request will give a different document.  Think\n> of a price quote made yesterday which will be honored until its\n> expiration date, but a new quote will have a different price.\n\nThis is a nice thought experiment, but I'm baffled about how you\nexpect caches to behave in such a situation other than they would with\na straightforward application of 'Last-Modified' (date quote was made)\nand 'Expires' (date when quote is no longer valid).\n\nBesides, none of this has to do with adding a 'size' or 'length'\nattribute, which was the original proposal that started this thread.\n \n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "John Franks:\n>\n>According to Larry Masinter:\n>\n>> * A proxy or cache shouldn't save a document with a future\n>>   last-modified.\n>\n>\n>In the absence of another (cleaner) way to prevent caching this will\n>become the de facto way for a server to tell a client (or proxy) not to cache.\n\nSuch another way is not absent.  Expires: <yesterday> is another way, and it\nisn't a de facto way either.  It is in the draft standard, and has been in\nthe draft standard for some time.  From draft-ietf-http-v10-spec-02.html:\n\n|8.7 Expires\n|\n|The Expires field gives the date/time after which the entity should be\n|considered stale. This allows information providers to suggest the\n|volatility of the resource. Caching clients, including proxies, must not\n|cache this copy of the resource beyond the date given, unless its status has\n|been updated by a later check of the origin server\n\nAlso, I think that Expires: <date in 1990> is much cleaner than\nLast-modified: <date in 2010>.\n\n>John Franks\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Shel Kaphan writes:\n > \n > Uh, never mind what I just said about future last-modified dates.\n > It might be OK for GET if-modified-since, but obviously fails for\n > plain GETs.\n > \n > --Shel\n\nI retract this retraction.\nA proxy cache that cares at all about last-modified\nshould always use GET if-modified-since once it has the document\ncached at all.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Ari Luotonen writes:\n > \n > > In the absence of another (cleaner) way to prevent caching this will\n > > become the de facto way for a server to tell a client (or proxy) not\n > > to cache.\n > > \n > > This, in effect, overloads the Last-Modified header so that \n > > \n > > Last-Modified: <date in 2010>\n > > \n > > means \"please don't cache this document.\"  This is obscure and error prone.\n > \n > Uh, well, just leaving it out will tell the same thing.  That would be\n > a much better \"de facto\", doesn't involve forging.  Both NS and CERN\n > proxies support it now.\n > Koen and I suggested that this behavior be at least mentioned\nsomewhere in the spec as \"current practice\", since some current\nservices depend on it, especially in the current world where you can't\nreally use Expires to suppress caching (because of history list\nproblems).  If this behavior (not caching documents that omit\nlast-modified) were to suddenly change, I suspect there would be a lot\nof unhappy service operators out there...\n\n > Cheers,\n > --\n > Ari Luotonenari@netscape.com\n > Netscape Communications Corp.http://home.netscape.com/people/ari/\n > 501 East Middlefield Road\n > Mountain View, CA 94043, USANetscape Server Development Team\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "multiple ContentEncodin",
            "content": "Section 8.7 of draft-ietf-http-v10-spec-01.ps says:\n\nIf multiple encodings have been applied to a resource, the\nencoding-mechanisms must be listed in the order in which they\nwere applied.\n====\n\nI think this is backward.  I think they should be listed in the order\nin which they must be applied by the client in the order they're listed.\n      ==== ==\n\nConsider foo.gz.Z.  Gzip was applied first, then compress.  NCSA HTTP\n1.3 produces this header:\nContent-Encoding: x-compress, x-gzip\n(It looks like the old CERN code I have doesn't support multiple\nencodings.)\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: multiple ContentEncodin",
            "content": ">Section 8.7 of draft-ietf-http-v10-spec-01.ps says:\n>\n>If multiple encodings have been applied to a resource, the\n>encoding-mechanisms must be listed in the order in which they\n>were applied.\n>====\n>\n>I think this is backward.  I think they should be listed in the order\n>in which they must be applied by the client in the order they're listed.\n>      ==== ==\n>\n>Consider foo.gz.Z.  Gzip was applied first, then compress.  NCSA HTTP\n>1.3 produces this header:\n>Content-Encoding: x-compress, x-gzip\n>(It looks like the old CERN code I have doesn't support multiple\n>encodings.)\n\nI didn't think anyone did, but that interpretation is ugly.\nMessage header field values should be \"added\" via an append\noperation.  Thus, each encoding becomes layered, and\n\n     Content-Encoding: first\n     Content-Encoding: second\n\nhas the same semantics as\n\n     Content-Encoding: first, second\n\nIf we reverse the order, then added headers must be prepended.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": ">I recently changed netscape to interpret \"Pragma: no-cache\" and not\n>cache the object.  This is slightly different than a \"Expires\" header\n>because the object will not even be cached for history navigation.\n>(Documents that are expired are still shown when traversing the session\n>history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n>tell the client that this information is highly sensitive or volitile\n>and should not be cached in any way.\n\nHmmm, that's a different interpretation than what I perceived from\nthe third-hand descriptions of what this feature might do.  It really\nis better to hear about protocol experiments from the source.\n\nSomething that the WG needs to keep in mind is that what I write in\nthe *draft* specification is what I believe to be the abstract and\nspecific semantics of each feature in the protocol.  I am counting\non people to correct me when what I write does not match their own\ninterpretation of the protocol.  Since I've developed my own client\nlibrary, client, and small portions of two servers, most of the time\nI do get it right, but certainly not all of the time.\n\nWhat I wrote in Draft 01 for the Pragma field's abstract semantics\nmay be wrong.  It is based on my interpretation of the meaning of\nthe current \"no-cache\" request header, and I merely extended that\nsemantics to the other directives.\n\n   Pragma directives must be passed through by a proxy, regardless of \n   their significance to that proxy, since the directives may be \n   applicable to all intermediaries along the request/response chain. \n   It is not possible to specify a pragma for a specific proxy; \n   however, any pragma directive not relevant to a proxy should be \n   ignored.\n\nInstead of \"intermediaries\", we could say \"recipients\", and the following\nparagraph:\n \n   Pragma directives do not apply to the end-points of a \n   request/response chain. For example, a user agent's internal (non-\n   shared) cache and/or history mechanism should ignore all pragma \n   directives in received messages. Similarly, pragma directives are \n   not applicable to the origin of a resource, though they may be \n   applicable to a server's internal response cache.\n\ncould be replaced with\n\n   Pragma directives only apply to recipients that implement features\n   corresponding to the directive's semantics.  For example, a no-cache\n   directive tells the recipient not to make use of its caching mechanism\n   in satisfying the request when it occurs in a request header, or in\n   storing the response when it occurs in a response.  Pragma directives\n   are also unidirectional in that the presence of a directive in a\n   request does not imply that the same directive be given in the response.\n\nWe can then add a new directive to cover the semantics of a response\nthat must not be shared by multiple users.  We could call it \"private\",\nbut I am afraid that this would also imply privacy, which it shouldn't.\nUnfortunately, there does not seem to be an antonym for \"shared\" or\n\"communal\", so how about\n\n   Pragma: non-shared\n           no-sharing\n           do-not-share\n\nEr, on second thought, maybe we should just use \"private\"...\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: multiple ContentEncodin",
            "content": "Roy Fielding <fielding@beach.w3.org> wrote:\n  > dmk@allegra.att.com (Dave Kristol) wrote:\n  > >Section 8.7 of draft-ietf-http-v10-spec-01.ps says:\n  > >\n  > >If multiple encodings have been applied to a resource, the\n  > >encoding-mechanisms must be listed in the order in which they\n  > >were applied.\n  > >====\n  > >\n  > >I think this is backward.  I think they should be listed in the order\n  > >in which they must be applied by the client in the order they're listed.\n  > >      ==== ==\n  > >\n  > >Consider foo.gz.Z.  Gzip was applied first, then compress.  NCSA HTTP\n  > >1.3 produces this header:\n  > >Content-Encoding: x-compress, x-gzip\n  > >(It looks like the old CERN code I have doesn't support multiple\n  > >encodings.)\n  > \n  > I didn't think anyone did, but that interpretation is ugly.\nUgly or not, it exists.  Whether anyone actually uses it is another\nmatter.\n  > Message header field values should be \"added\" via an append\n  > operation.  Thus, each encoding becomes layered, and\n  > \n  >      Content-Encoding: first\n  >      Content-Encoding: second\n  > \n  > has the same semantics as\n  > \n  >      Content-Encoding: first, second\n  > \n  > If we reverse the order, then added headers must be prepended.\n\nThat makes sense (aesthetically) if you assume that these encodings are\napplied dynamically.  Often they have already been applied to static\nfiles (e.g., my example), in which case you have to reconstruct the\nencodings outside-in.  From an implementation standpoint, it's easier\nto do it the way NCSA has done it.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <v02120d19ac57a7ce5be9@[198.64.246.22]> cshotton@biap.com (Chuck\nShotton) wrote:\n> \n> At 8:07 PM 8/15/95, Lou Montulli wrote:\n> \n> >> The point I was trying to make is that size is meaningless between any two\n> >> machines when text files are being compared. CR/LF on a Windows box may be\n> >> stored as LF only on a Unix host, or CR on a Mac may be converted into\n> >> CR/LF for transmission. This means that the original file on the server is\n> >> a physically different size than the file transmitted to the client/proxy,\n> >> and that the cached file size on the proxy may be an entirely different\n> >> size than the content-length.\n> >>\n> >> File size is essentially useless as a mechanism for determining whether or\n> >> not a cached file is the correct version.\n> \n> >Your facts don't support your conclusion.  If linefeed conversion is\n> >performed by the server then it should be done consistantly, you can\n> >therefore always compute the eventual size of the object by parsing the\n> >file.  If a proxy modifies the file in any way then it needs to remember\n> >it's original size.\n> \n> Lou, why are you forcing this computation on the server? The whole problem\n> of corrupted or stale caches is a CLIENT problem and the computation should\n> happen there. Why should a server be forced to read and translate every\n> byte of a file, just so it can calculate the content-length for a IMS\n> request from a client that is trying to use file size to determine file\n> \"sameness\"? This is extremely burdensome on the server and shouldn't be the\n> server's job.\n\nThis is where you are completely wrong.  In every case of cache corruption\nthat I have seen it has always been caused by server errors.  Dates\nare simply not a strong enough versioning system to prevent lossage.\n\n> \n> Here is an alternative to this whole \"size\" thing. Let's start with an\n> assumption. You may not agree with it, but my experience shows otherwise.\n> Let's assume that for any client/server pair, the two machines store the\n> same text file with different sizes because of differences in EOL\n> conventions. That means that the ONLY common size they can have is the\n> content-length that the server reports. Since this requires action on the\n> server to compute, it isn't an acceptable mechanism. SO, why not do the\n> following?\n> \n> When a server sends a file, the client should make sure that it received at\n> least content-length bytes of data. If it doesn't receive the complete\n> file, it can still display the file, but shouldn't cache it. If the client\n> DOES receive the required number of bytes, it should compute a checksum for\n> the file and cache the contents. Future requests for the file can simply\n> use the existing IMS syntax to retrieve new versions based on modification\n> dates. If dates match, the client can verify that the file in the local\n> cache is uncorrupted simply by recalculating and comparing the checksums.\n> All the work for this is done by the client, no server modifications are\n> required, and no wasted CPU cycles are spent on the server calculating\n> content-length values for a file that won't be transmitted.\n\nThis does nothing to solve the problem.  The problem we are trying\nto solve is not local cache corruption, it is version skew. \n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Shel Kaphan writes:\n> Shel Kaphan writes:\n>  > \n>  > Uh, never mind what I just said about future last-modified dates.\n>  > It might be OK for GET if-modified-since, but obviously fails for\n>  > plain GETs.\n>  > \n>  > --Shel\n\n> I retract this retraction.\n> A proxy cache that cares at all about last-modified\n> should always use GET if-modified-since once it has the document\n> cached at all.\nExcept in the presence of the Pragma: no-cache in the request.\nUnfortunately CERN HTTPD/3.0 uses GET if-modified-since and passes the pragma -\nwhich in turn has no effect, and I can't force a cache refresh!\n\nAndrew.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "> On Tue, 15 Aug 1995, Lou Montulli wrote:\n> \n> > I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> > cache the object.  This is slightly different than a \"Expires\" header\n> > because the object will not even be cached for history navigation.\n> \nDave Morris writes:\n> I believe this is the wrong design.  The user *MUST* be able to always\n> flip the page backward and see the same material just viewed.  THis\n> is a critical user interface usability issue. Within the same session/\n> instance of UAgent execution. I believe it would be/is wrong to\n> silently refresh a document during history navigation when the history\n> cache has overflowed. I (and many users I know) expect the history\n> to be a record of what I've seen. I would have not objection to a \n> browser which checked currency and via a non-modal message advised that\n> the history copy wasn't current (\"Current copy not current, RELOAD for\n> the latest copy\" for example).\nI agree, this is the right approach. According to this, I change my statement\non 'Pragma: no-local-cache' from doubtful to useless.\nI has experience with client cache only using netscape 1.1N.\nNetscape has 3 options on contolling local cache freshness:\na) re-check on every access.\nb) re-check once per execution.\nc) never.\nI preferred the b) variant normally, and c) every time when I worked off-line\nand never choosed a).\nWhen I follow a link in a not fully readed document, I want to go back into\nthe same document, preferably into same position where I left - when I'm done\nwith the followed link. Now I'm reading web documents this way, and netscape\nsupperts me in doing this.\nI'll be worried, if I can't do the same in the future.\nOr somebody in the WG prefers to see a changed document when returns back from\na link to the original, not yet completely read document? I will be suprised if\nsomebody says yes!\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Roy Fielding writes:\n > \n > Instead of \"intermediaries\", we could say \"recipients\", and the following\n > paragraph:\n >  \n >    Pragma directives do not apply to the end-points of a \n >    request/response chain. For example, a user agent's internal (non-\n >    shared) cache and/or history mechanism should ignore all pragma \n >    directives in received messages. Similarly, pragma directives are \n >    not applicable to the origin of a resource, though they may be \n >    applicable to a server's internal response cache.\n > \n > could be replaced with\n > \n >    Pragma directives only apply to recipients that implement features\n >    corresponding to the directive's semantics.  For example, a no-cache\n >    directive tells the recipient not to make use of its caching mechanism\n >    in satisfying the request when it occurs in a request header, or in\n >    storing the response when it occurs in a response.  Pragma directives\n >    are also unidirectional in that the presence of a directive in a\n >    request does not imply that the same directive be given in the response.\n > \n\nI agree; it seems more robust (and useful) to include end-points in\nthe set that can be affected by pragma directives.  If you don't, half\nthe implementations are going to get it wrong anyway.\n\n > We can then add a new directive to cover the semantics of a response\n > that must not be shared by multiple users.  We could call it \"private\",\n > but I am afraid that this would also imply privacy, which it shouldn't.\n > Unfortunately, there does not seem to be an antonym for \"shared\" or\n > \"communal\", so how about\n > \n >    Pragma: non-shared\n >            no-sharing\n >            do-not-share\n > \n > Er, on second thought, maybe we should just use \"private\"...\n > \n\nHow about Pragma: for-your-eyes-only?   \n\nAnyhow, what you suggest seems good, because it not only allows for\nprivate documents (without ugly URL mangling schemes), but it also\nallows for something else that Koen and I were trying to accomplish\nwith our report on the Expires header\n(http://www.amazon.com/expires-report.{html,txt}) which is to provide\nfor a way that history functions *could* cause automatic reloading of\na document.  With your new proposed pragma semantics, and the way it\nhas been interpreted already by Lou Montulli in Netscape, this would\nthen be enabled, and the Netscape changes mentioned could be\n\"rehabilitated\" (to borrow a political term).  My earlier objections\nto that were based on a value judgment that being able to direct a\ndocument to one client only was considerably more valuable than being\nable to have history commands invoke reloading.  But if we can have\nboth, lets!  I suppose it is still open to interpretation whether\npragma: no-cache is to be interpreted to preclude history mechanisms\n(as opposed to caches) from storing documents, but I see no other use\nfor it at the client end (given the existence of Expires), so why not?\n\n > \n >  ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n >                       Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n >                       (fielding@w3.org)                (fielding@ics.uci.edu)\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Is HTTP a better replacement for FTP ",
            "content": "I just was reading in RFC 959 which defines the FTP protocol. FTP is\nquite old now and has a lot of rudiments and old-fashioned features\nwhich nobody uses anymore. \n\nSince HTTP can do everything what FTP could do, but IMHO in a much\nbetter and modern way, I wonder whether it should be one of the goals\nof http development to have a complete replacement for FTP and make\nFTP die silently.\n\nI have both a Web- and a FTP-server, but the FTP-server exists just\nfor historical reasons. Meanwhile the number of Web-servers in the\nworld increases much faster than the number of FTP-servers. I don't\nsee any advantage in having both protocols. \n\nAny opinions?\n\nHadmut\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "In article <95Aug16.004640pdt.2763@golden.parc.xerox.com> Larry Masinter\n<masinter@parc.xerox.com> wrote:\n> \n> If we have a currently working and then someone says \"I have a\n> problem, users complain about the following bug: bzzzt\" and then \"I\n> propose the following solution: sssss\" then it's up to us to decide:\n> \n> 1) Is bzzzt really a problem?\n> 2) Does sssss actually solve the problem?\n> 3) Is sssss the best solution? In particular, can the\n>    problem be solved without changing the protocol?\n> ================================================================\n> I think in the case of\n>         bzzzt = 'file has bad date in the future'\n>         sssss = 'add ; length=nnnn to if-modified request'\n> \n> The answers are\n> (1) yes: it's a problem. We've all seen it, know that it has multiple\n>     causes which are out of our control.\n> (2) usually: except when the length is wrong, or hard to compute, or\n>     the document is dynamic, or has a different EOL convention and is\n>     translated on the fly; we've identified some exceptions.\n> \n> However, I believe that\n> (3) is false: the problem can be solved without changing the protocol,\n>     as I outlined in a previous message:\n> \n> * A server encountering a file with a modification date in the future\n>   (according to the server's current time) should not send the future\n>   'last-modified' date.\n\nYou are forgetting that this only solves a third of the problem.\n\nIt doesn't solve the problem of two dates being exactly the\nsame but the file has been modified, or the problem of a file getting\nmodified and having it's date set into the past.  Date's alone\nare not a strong enough versioning system.  And don't try and tell me \nthat these don't happen.  They are as likely to happen as a file with\na date far forward, and these kinds of problems are happening all\nthe time.\n\nIn addition to these other two problems, by restricting dates that\nhave been set far forward, you are in effect disabling caching for\nthese files.  This is a far worse solution to adding a checksum.\n\nlou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Balint Nagy Endre writes:\n > Shel Kaphan writes:\n > > Shel Kaphan writes:\n > >  > \n > >  > Uh, never mind what I just said about future last-modified dates.\n > >  > It might be OK for GET if-modified-since, but obviously fails for\n > >  > plain GETs.\n > >  > \n > >  > --Shel\n > \n > > I retract this retraction.\n > > A proxy cache that cares at all about last-modified\n > > should always use GET if-modified-since once it has the document\n > > cached at all.\n > Except in the presence of the Pragma: no-cache in the request.\n > Unfortunately CERN HTTPD/3.0 uses GET if-modified-since and passes the pragma -\n > which in turn has no effect, and I can't force a cache refresh!\n > \n > Andrew.\n\nYes, sure, I agree.\n\nLet me expand on my retracted retraction.\n\nProxies should just use the claimed mod date on resources they fetch\nand use that date when doing GET if-modified-since afterwards.  They\nmay also, of course, want to occasionally just get a new copy of\nsomething.  For example, one way to manage this is to keep track of\nthe LOCAL time when a document was last fetched.  The point is that\nthe origin-server's claimed modification time should never be relevant\nto any proxy cache along the line, except as a stored \"cookie\"-like\nthing that can be passed back in GET if-modified-since.  So if some\norigin server claims a document was modified on Stardate 7239.64, a\nproxy ought to just hold on to that, ignore what it means, and pass it\nback to the origin server when it does a GET if-modified-since.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "forged dates and other anticache practice",
            "content": "Hi all,\n\nI'm here again, and want to summarise discussion on cache disabling\ntechniques.\n\nWhy people are against caching documents?\n1. I can only guess intentions of people, never seen personally, but I can\nimagine only one cause: they want precise access statistics.\n2. they aren't paying for non-cached and otherwise cacheable requests.\n\nWhy I want cache documents:\n1. to reduce my phone bills. (I have dialup slip connection to the net)\n2. to reduce the time spent watching the hourglass cursor. (I have better\nentertainment choices to spend that time)\n\nWhat I expect from the ongoing http version:\n1. to satisfy people wanting good access statistics.\n2. to give me chance reducing phone bills and time spent watching hourglass\ncursor by some percents.\n\nWhat we, as http-WG can do:\n1. adding extensions to protocol to make possible those hits counted.\n2. documenting good practices to make effective caching possible\n3. giving people standard tools to prevent cacheing of sensitive documents\n - already done: Pragma: no-cache and Expires: <now or even sooner>\n\nI don't want to list all techniques seen to fool caches, these are numerous.\nBetter I repeat proposal, already done:\n1. Pragma: hits = #\n2. modified Forwarded: header containing hit counts.\n3. Hit-reports-to: header for requesting more detailed stats than bare counts.\n\nI repeat again: if document owners will have nearly the same stats trough\ncaches, which they can collect from direct requests from its servers, they\nnever want to fool caches.\nAnd that will spare our time and money too.\nBecause cache fooling techniques are now in practice, we shall state,\nthat fooling caches is spending other peoples money and time.\n\nAndrew.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": ">In article <v02120d19ac57a7ce5be9@[198.64.246.22]> cshotton@biap.com (Chuck\n>Shotton) wrote:\n>> Lou, why are you forcing this computation on the server? The whole problem\n>> of corrupted or stale caches is a CLIENT problem and the computation should\n>> happen there. Why should a server be forced to read and translate every\n>> byte of a file, just so it can calculate the content-length for a IMS\n>> request from a client that is trying to use file size to determine file\n>> \"sameness\"? This is extremely burdensome on the server and shouldn't be the\n>> server's job.\n>\n>This is where you are completely wrong.  In every case of cache corruption\n>that I have seen it has always been caused by server errors.  Dates\n>are simply not a strong enough versioning system to prevent lossage.\n\nAs if file size is any better? You avoided answering the question, which\nwas why should the server be responsible for essentially maintaining the\nclient/proxy cache? This should be done by the client software, through\nwhatever means the client has at its disposal. I don't care what the\nmechanism is. I just don't want to see thousands of caching clients beating\non servers because they are too lame to keep track of their own cache. If a\ncached file is suspicious because of a date, a file size, or a bad\nchecksum, the client should discard it. Period. Forcing the server to jump\nthrough hoops on every IMS request is contrary to the entire goal of\n\"server serve, clients do the work.\"\n\n>This does nothing to solve the problem.  The problem we are trying\n>to solve is not local cache corruption, it is version skew.\n\nWell, that wasn't your original problem. If I recall correctly, all of this\nstarted as a discussion about how to fix corrupted caches and detect when a\nfile was bad. If we're onto version skew, fine, but let's make sure we ALL\nknow when the subject changes.\n\nI think that Larry summed up this entire discussion very well earlier\ntoday. To beat this horse further is counterproductive.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache, and PRIVAC",
            "content": "In article <199508162029.NAA13943@bert.amazon.com> Shel Kaphan <sjk@amazon.com>\nwrote:\n> \n> I agree; it seems more robust (and useful) to include end-points in\n> the set that can be affected by pragma directives.  If you don't, half\n> the implementations are going to get it wrong anyway.\n> \n\nGreat, pragma's can now be interpreted I'm happy.  Should\nI still interpret \"no-cache\" or should there be something else?\n\"no-cache\" does seem cleanest. \n\nWhile we are on the subject it would be nice to have a header\nthat specified the level of privacy of a document.  Simply\nusing the level of encryption of the document is not enough\nbecause I believe that eventually every document will end up\nbeing transfered securly.  If there was a header that transmitted\nthe privacy level, we could selectively encrypt the disk \nand memory cache to prevent unwanted access.  It is not currently\npractical to do it to every file.  \n\nIs there a header that does this already?  Or should there be\na new Pragma directive?\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <v02120d04ac580956f14c@[198.64.246.22]> cshotton@biap.com (Chuck\nShotton) wrote:\n> \n> >In article <v02120d19ac57a7ce5be9@[198.64.246.22]> cshotton@biap.com (Chuck\n> >Shotton) wrote:\n> >> Lou, why are you forcing this computation on the server? The whole problem\n> >> of corrupted or stale caches is a CLIENT problem and the computation should\n> >> happen there. Why should a server be forced to read and translate every\n> >> byte of a file, just so it can calculate the content-length for a IMS\n> >> request from a client that is trying to use file size to determine file\n> >> \"sameness\"? This is extremely burdensome on the server and shouldn't be the\n> >> server's job.\n> \n> >This is where you are completely wrong.  In every case of cache corruption\n> >that I have seen it has always been caused by server errors.  Dates\n> >are simply not a strong enough versioning system to prevent lossage.\n> \n> As if file size is any better? You avoided answering the question, which\n> was why should the server be responsible for essentially maintaining the\n> client/proxy cache? \n\nThis isn't forcing the server to do anything.  Adding a size simply\nhelps the server to make the correct decision about sending a 304\nor not.  Currently servers are making wrong decisions because it\ncant tell if the file has changed becaused dates are not strong\nenough by themselves to do an adequate job of versioning.\n\n> This should be done by the client software, through\n> whatever means the client has at its disposal. I don't care what the\n> mechanism is. I just don't want to see thousands of caching clients beating\n> on servers because they are too lame to keep track of their own cache. If a\n> cached file is suspicious because of a date, a file size, or a bad\n> checksum, the client should discard it. Period. Forcing the server to jump\n> through hoops on every IMS request is contrary to the entire goal of\n> \"server serve, clients do the work.\"\n\nYou seem to be forgetting that \"jumping through hoops\", as you put it,\nis going to save the server time in the long run.  Remember, bandwidth\nis not free. \n\n> \n> >This does nothing to solve the problem.  The problem we are trying\n> >to solve is not local cache corruption, it is version skew.\n> \n> Well, that wasn't your original problem. If I recall correctly, all of this\n> started as a discussion about how to fix corrupted caches and detect when a\n> file was bad. If we're onto version skew, fine, but let's make sure we ALL\n> know when the subject changes.\n\nThat isn't a change from the subject.  The cache is corrupted because\nof version skew, they are both tightly related.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <199508152149.AA062163389@ooo.lanl.gov> Rob Hartill\n<hartill@ooo.lanl.gov> wrote:\n> \n> \n> > \"fix\" is an interresting choice of words.  \"change\" is more appropriate.\n> > Not sending an \"if-modified-since\" header with reloads would be extremely\n> > costly in terms of bandwidth.  Adding cache checksums is a much better\n> > solution.\n> \n> no no no no no. Let the user *choose* to override the sending of\n> \"if-modified-since\".\n> \n> Which is the bigger bandwidth waste ... discarding 5Mb of disk cache\n> or not sending if i-m-s for a request the user has realised is cached\n> incorrectly ?\n> \n> We're not alking about having it as a default action, just an option\n> for users to refresh bad cache entries.\n> \n> Presumably the people sending you \"bug\" reports are doing so because\n> they see junk in their caches and can't get rid of it. Your users will\n> still have the same problem with URLs which don't have checksums, and\n> that's going to be the norm for a long time to come.\n\nActually they are complaining about version skew because servers are\nmaking bad decisions about IMS dates.\n\nBut, I do agree that having a non-IMS reload is a useful feature, so\nI'm currently adding one.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "411 response code: clarification, pleas",
            "content": "401 and 411 are the response codes for failed authentication.  I'm\nuncertain when an origin server should respond with 411, rather than\n401.  Here's what I think I understand.  Assume, in each case, that the\norigin server requires authentication for the request.\n\n1) Client sends no Authorization request header.  Server always\nresponds with 401.\n\n2) Client sends Authorization request header.  Server doesn't like the\ninformation therein.\n\n    2a) Basic scheme.  At present, servers send 401 if the name:passwd\n    is unacceptable.  Should they send 411 for HTTP/1.1?  Apparently\n    not:  the challenge is likely to be the same as the previous one,\n    so sending the same response is futile.\n\n    2b) Digest scheme.\n    2b1) The \"stale\" attribute says whether the problem is with the\n    nonce value, so the client can tell whether the server thinks\n    that's what the problem is.  Should the server send 401 or 411 on a\n    stale nonce?  (I'm guessing 411, although it doesn't appear to\n    matter.)\n\n    2b2) If the nonce is fresh, but the server rejects the authorization\n    information for other reasons, I'm guessing the server should\n    return a 401. A 411 would imply that the client could recalculate\n    the Authorization header from the challenge (realm and nonce), but\n    they may well be the same the second time, and the server would\n    reject it again.\n\nSo, in all cases but 2b1, the correct answer appears to be 401.  For\ncase 2b1, 401 should work as well as 411, because the \"stale\" attribute\nprovides the equivalent information to 411.  So, what is 411 for??\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> Actually they are complaining about version skew because servers are\n> making bad decisions about IMS dates.\n\nA part of this is due to a bug in Solaris 2.3 that causes GMT to be wrong by \nan hour.  \n \n> But, I do agree that having a non-IMS reload is a useful feature, so\n> I'm currently adding one.\n\nSo am I, and it will get out in the next release of the W3C Reference Library.\n\n\n-- \n\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld-Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache, and PRIVAC",
            "content": ">Great, pragma's can now be interpreted I'm happy.  Should\n>I still interpret \"no-cache\" or should there be something else?\n\nYes, and I'll update the specs [I wish we had this discussion before\nI split them in two].\n\n>While we are on the subject it would be nice to have a header\n>that specified the level of privacy of a document.\n\nYikes!  Then we have to define \"level of privacy\" first, and I\ndon't think this belongs in a Pragma header anyway.\n\nThe W3C security people have been working on a generic extension\nmechanism for this purpose (and a few others) which should be\na proposal for HTTP/1.2 once the documentation is complete.\n\nI'd like to get 1.0 set in stone first, before we open the mondo\ncan-o-worms that generic extensions will entail.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "Lou Montulli <montulli@mozilla.com> wrote:\n  > This isn't forcing the server to do anything.  Adding a size simply\n  > helps the server to make the correct decision about sending a 304\n  > or not.  Currently servers are making wrong decisions because it\n  > cant tell if the file has changed becaused dates are not strong\n  > enough by themselves to do an adequate job of versioning.\n\nPardon my density.  I've been following this discussion, but I've never\nunderstood the argument about bad dates.  I would appreciate an\nexample.\n\nI can imagine deliberately setting a specific file date, although I\ndon't understand the purpose.  Are there also systems where there's no\nuseful modification date, so a server cannot detect changes by checking\nthe file date?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Is HTTP a better replacement for FTP ",
            "content": ">Since HTTP can do everything what FTP could do, but IMHO in a much\n>better and modern way, I wonder whether it should be one of the goals\n>of http development to have a complete replacement for FTP and make\n>FTP die silently.\n\nNo, and this must not be a topic for discussion in this working group.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: forged dates and other anticache practice",
            "content": "On Wed, 16 Aug 1995, Balint Nagy Endre wrote:\n\n> Hi all,\n> \n> I'm here again, and want to summarise discussion on cache disabling\n> techniques.\n> \n> Why people are against caching documents?\n> 1. I can only guess intentions of people, never seen personally, but I can\n> imagine only one cause: they want precise access statistics.\n\nShopping basket applications and dynamic documents. *FAR* more important \nthan precise stats.\n\nIt never even occured to me that it might provide more precise access \nstats when I designed a site to explicitly defeat caching \n(<URL:http://www.psiloveyou.com/>, for those who care). The problem \nwith caching is it makes dynamic documents (as in some shopping basket \napplications) hell. Between Mosiac's \"Expire:? What's that?\" behavior and \ncache corruption in Netscape, I couldn't run a reliable site without \nexplicitly munging URLs to prevent caching in addition to expiring \nlarge sections of the site instantly. Believe me - I tried.\n\nIt wasn't until later discussion by people trying to claim AOL has \nmillions of accesses hiding behind single hits to their proxies that it \noccured to me that it was also a way to defeat the stats unfriendly \nbehavior of caching browsers/proxies. Incidentally, based on the stats \nfrom the cache defeating site - AOL undercounting is no more than a \nfactor of two or three.  I would speculate that the massively broken \nnature of their browser, combined with a huge speed gap between 'Native \nAOL' graphics and inlined Web graphics turns AOLers off the WWW. Paying \nhigh per hour charges to download graphics from the web seems a \nno-brainer (to me anyway).\n\n> 2. they aren't paying for non-cached and otherwise cacheable requests.\n\nI am not at all sure how to parse this. At first I thought you meant that \nthe provider charged by the hit, then I thought the opposite.\n\nCould you clarify what you mean?\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> It doesn't solve the problem of two dates being exactly the\n> same but the file has been modified, or the problem of a file getting\n> modified and having it's date set into the past.  Date's alone\n> are not a strong enough versioning system.  And don't try and tell me \n> that these don't happen.  They are as likely to happen as a file with\n> a date far forward, and these kinds of problems are happening all\n> the time.\n\nThese problems happen, but they often happen in ways that preserve\nlength. If you want something stronger than date, then use a checksum;\nif you're going to use a checksum, you might as well use MD5 or SHA.\n\nChecksums can be pre-computed by the server and periodically\nrecomputed by them as well, sent along as content-md5 headers and just\nsent back in HTTP/1.1 get-if-not-content-MD5 request.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Lou Montulli writes:\n > \n > You are forgetting that this only solves a third of the problem.\n > \n > It doesn't solve the problem of two dates being exactly the\n > same but the file has been modified, or the problem of a file getting\n > modified and having it's date set into the past.  Date's alone\n > are not a strong enough versioning system.  And don't try and tell me \n > that these don't happen.\n\n*I* won't try to tell you that these don't happen, though they seem to\nbe evidence of bugs or human activity that it is not clear HTTP\nshould try to correct for after the fact.  I can't think of a reason\nfor this happening other than someone or something changing a system\nclock or deliberately falsifying a modification date. In any case, it\ndoesn't seem like the responsibility of a communication protocol to\ntry to figure out what's \"really\" going on if a server is too lame to\nget it right for itself.\n\nThe percent of the time that there is going a problem like this seems\nlike it must be small, certainly not worth imposing additional\noverhead on the protocol to correct for, especially if that involves\nscanning whole files for checksums.\n\n  They are as likely to happen as a file with\n > a date far forward, and these kinds of problems are happening all\n > the time.\n > \n > In addition to these other two problems, by restricting dates that\n > have been set far forward, you are in effect disabling caching for\n > these files.  This is a far worse solution to adding a checksum.\n > \nBut you don't really need either if you just treat this future date as an\nopaque thingamabob you only use for GET if-modified-since later.\n\n > lou\n > -- \n > Lou Montulli                 http://www.mcom.com/people/montulli/\n >        Netscape Communications Corp.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "dwm@shell.portal.com said:\n> I believe this is the wrong design.  The user *MUST* be able to \n> always flip the page backward and see the same material just viewed.  \n> THis is a critical user interface usability issue. Within the same \n> session/ instance of UAgent execution. I believe it would be/is wrong \n> to silently refresh a document during history navigation when the \n> history cache has overflowed. I (and many users I know) expect the \n> history to be a record of what I've seen. I would have not objection \n> to a  browser which checked currency and via a non-modal message \n> advised that the history copy wasn't current (\"Current copy not \n> current, RELOAD for the latest copy\" for example).\n\nI think this is a limited view of the world. The way that I have implemented \nit (yesterday, that is) is to let it be up to the user to choose what action \nto be taken if an expired history object is encountered:\n\n1) Do nothing at all and show the document\n\n2) Notify the user that the document is stale but do not refresh it\n\n3) Do an automatic reload when the document becomes active (visible) to    \nthe user.\n\nThis looks like a nice \"option menu\" to me!\n \n-- \n\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld-Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "In article <199508162233.PAA14427@bert.amazon.com> Shel Kaphan <sjk@amazon.com>\nwrote:\n> \n> Lou Montulli writes:\n>  >\n>  > You are forgetting that this only solves a third of the problem.\n>  >\n>  > It doesn't solve the problem of two dates being exactly the\n>  > same but the file has been modified, or the problem of a file getting\n>  > modified and having it's date set into the past.  Date's alone\n>  > are not a strong enough versioning system.  And don't try and tell me\n>  > that these don't happen.\n> \n> *I* won't try to tell you that these don't happen, though they seem to\n> be evidence of bugs or human activity that it is not clear HTTP\n> should try to correct for after the fact.  I can't think of a reason\n> for this happening other than someone or something changing a system\n> clock or deliberately falsifying a modification date. In any case, it\n> doesn't seem like the responsibility of a communication protocol to\n> try to figure out what's \"really\" going on if a server is too lame to\n> get it right for itself.\n\nWith most current implementations of HTTP servers it is impossible to\n\"get it right for itself\".  HTTP servers reference a file system\nthat can be changed at random.  The HTTP server can only rely\non the last modified date of a file and that date can be inaccurate.\nAdding an additional size checksum allows the server to \"get\nit right for itself\" much more often.\n\n> \n> The percent of the time that there is going a problem like this seems\n> like it must be small, certainly not worth imposing additional\n> overhead on the protocol to correct for, especially if that involves\n> scanning whole files for checksums.\n\nProtocols like HTTP are not about getting is right 99% of the time,\nthey need to get it right 99.99% of the time.  Adding a size checksum\nsignificatly reduces the error rate.  Adding an MD5 checksum\nreduces that error rate even furture.\n\nI don't want to require all servers to interpret size= or MD5=, but\nclient should be allowed to send them optionally so that\nthe server can use them if it really wants to make an informed\ndecision about sending a 304.  \n\nPerhaps reliability is not as important to your systems as it is\nto ours?  If that's the case, make your servers ignore size= and\nonly use the date. \n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Lou Montulli writes:\n\n > With most current implementations of HTTP servers it is impossible to\n > \"get it right for itself\".  HTTP servers reference a file system\n > that can be changed at random.  The HTTP server can only rely\n > on the last modified date of a file and that date can be inaccurate.\n > Adding an additional size checksum allows the server to \"get\n > it right for itself\" much more often.\n > \n\nCan't disagree.  I just have to wonder what would be going on that\nwould be introducing such strange modification date anomalies.  Are you\nsaying perhaps that NFS uses the remote system's date for the mod date\nwhen a file is saved on a server(???), and that people might have\nunsynchronized local networks, and that therefore there might be\nlegitimate reasons for a file to change but have a non-monotonically\nincreasing mod date?  \n\nMaybe I've been looking in the wrong places, but I've just never seen\nmuch evidence of mod-date tampering that would require much worry\nabout it.  That said, I can hardly disagree that if it were happening,\nchecksums would help detect it.  And that said, I also do not object\nto this existing as an optional protocol feature; it isn't much of a\nhot button for me.  I was just pointing out the (perceived)\ncost-benefit .\n\n...\n > Perhaps reliability is not as important to your systems as it is\n > to ours?  If that's the case, make your servers ignore size= and\n > only use the date. \n > \n\nAu contraire.  But since the systems I have been working on generate\nmost if not all pages dynamically, and they really are dynamic, I\ndon't even generate last-modified headers, due to (a) the practical\nfact that Expires cannot be used yet, and (b) the fact that most\nexisting proxy caches interpret lack of last-modified to mean \"do not\ncache this\".  But even were that not the case, I don't allow arbitrary\nchanges to our online website from around a local network such that\nthe problem could occur except by malicious intent.\n\n > :lou\n > -- \n > Lou Montulli                 http://www.mcom.com/people/montulli/\n >        Netscape Communications Corp.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "At 2:05 PM 8/16/95, Lou Montulli wrote:\n>In article <v02120d04ac580956f14c@[198.64.246.22]> cshotton@biap.com (Chuck\n>Shotton) wrote:\n>> >This is where you are completely wrong.  In every case of cache corruption\n>> >that I have seen it has always been caused by server errors.  Dates\n>> >are simply not a strong enough versioning system to prevent lossage.\n>>\n>> As if file size is any better? You avoided answering the question, which\n>> was why should the server be responsible for essentially maintaining the\n>> client/proxy cache?\n>\n>This isn't forcing the server to do anything.  Adding a size simply\n>helps the server to make the correct decision about sending a 304\n>or not.  Currently servers are making wrong decisions because it\n>cant tell if the file has changed becaused dates are not strong\n>enough by themselves to do an adequate job of versioning.\n\nLet's try this one last time. Lou, do you agree or disagree that for any\ntwo CPUs, they may have different size representation for the same data\nstream due to EOL differences and or differences between the transmitted\ncontent-length and the number of bytes stored on the disk? If this is the\ncase (and it IS in many implementations), then using size will NEVER work.\nThe client will cache the file with one size, the server will compare it to\nanother \"size\" and the cached file will always appear to be modified or\ndifferent.\n\nThe only way to solve this problem is to normalize the \"size\" to be the\ncontent-length instead of the number of bytes stored on disk on either end\nof the connection. This means that the server will have to read and\ntranslate the file to recompute the content-length size whenever it is\nrequested. Since this is a CPU and disk I/O intensive process, it places a\nburden on the server that we should try to avoid. You seem to be ignoring\nthis flaw in the IMS \"size\" discussion and it is a fatal flaw.\n\n>> This should be done by the client software, through\n>> whatever means the client has at its disposal. I don't care what the\n>> mechanism is. I just don't want to see thousands of caching clients beating\n>> on servers because they are too lame to keep track of their own cache. If a\n>> cached file is suspicious because of a date, a file size, or a bad\n>> checksum, the client should discard it. Period. Forcing the server to jump\n>> through hoops on every IMS request is contrary to the entire goal of\n>> \"server serve, clients do the work.\"\n>\n>You seem to be forgetting that \"jumping through hoops\", as you put it,\n>is going to save the server time in the long run.  Remember, bandwidth\n>is not free.\n\nAnd neither is CPU time or disk I/O. These are much more limited on a\nserver handling lots of parallel requests than the net bandwidth on many\nsystems.\n\n--_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\nChuck Shotton                               StarNine Technologies, Inc.\nchuck@starnine.com                             http://www.starnine.com/\ncshotton@biap.com                                  http://www.biap.com/\n                 \"Shut up and eat your vegetables!\"\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Hello,\n\n<303281A4.3386@mozilla.com> from \"Lou Montulli\" at Aug 16, 95 04:39:16 pm\n> \n> I don't want to require all servers to interpret size= or MD5=, but\n> client should be allowed to send them optionally so that\n> the server can use them if it really wants to make an informed\n> decision about sending a 304.  \n\nI second this motion - it only increases reliability for those that desire it.\nThis along with the below modified guidelines for behavior (originally from\n<95Aug16.102012pdt.2763@golden.parc.xerox.com> from \"Larry Masinter\"):\n\n* A server encountering a file with a modification date in the future\n  (according to the server's current time) should not send the future\n  'last-modified' date.\n\n* Servers ought to be able to either precompute checksums periodically or \n  generate them on the fly, independent of the desires of proxies or caches.\n\n* A proxy or cache shouldn't save a document with a future last-modified.\n\n* A client and any proxies along the way should remember the exact\n  last-modified (to the second) with which the document was delivered,\n  no approximations, and independent of any local time settings. The same \n  applies to size and checksum information if so desired.\n\nJim.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <v02120d09ac5838de1e3c@[198.64.246.22]> cshotton@biap.com (Chuck\nShotton) wrote:\n> \n> The only way to solve this problem is to normalize the \"size\" to be the\n> content-length instead of the number of bytes stored on disk on either end\n> of the connection. This means that the server will have to read and\n> translate the file to recompute the content-length size whenever it is\n> requested. Since this is a CPU and disk I/O intensive process, it places a\n> burden on the server that we should try to avoid. You seem to be ignoring\n> this flaw in the IMS \"size\" discussion and it is a fatal flaw.\n\nIt is not at all a fatal flaw.  The size returned by the client needs\nto be specified to be the same as the \"content-length\" returned by\nthe server during the request.  If line-feed conversion is being\ndone consistantly the size can be compared accurately.\n\n> \n> >> This should be done by the client software, through\n> >> whatever means the client has at its disposal. I don't care what the\n> >> mechanism is. I just don't want to see thousands of caching clients beating\n> >> on servers because they are too lame to keep track of their own cache. If a\n> >> cached file is suspicious because of a date, a file size, or a bad\n> >> checksum, the client should discard it. Period. Forcing the server to jump\n> >> through hoops on every IMS request is contrary to the entire goal of\n> >> \"server serve, clients do the work.\"\n> \n> >You seem to be forgetting that \"jumping through hoops\", as you put it,\n> >is going to save the server time in the long run.  Remember, bandwidth\n> >is not free.\n> \n> And neither is CPU time or disk I/O. These are much more limited on a\n> server handling lots of parallel requests than the net bandwidth on many\n> systems.\n\nThe number of parallel requests are reduced by disconnecting \nuser agents quickly.  This can be occomplished best via 304.\nIf 304 cannot be relied upon to be accurate then it can't be used\nand data retransmission will have occur during every request.\nThat is far more costly than computing a checksum.  And as\nI said before, if you are not as concerned about reliability,\nignore the size...\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "> Can't disagree.  I just have to wonder what would be going on that\n> would be introducing such strange modification date anomalies.\n\nPeople running servers without using NTP, especially Macs and PCs.\n\nPeople moving files around between systems using tar or other archives\nthat can reset the last-modified dates.\n\nI keep my own Mac's clock set OK, but I routinely see files with\nodd dates copied from other systems.\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "> I think, that adding some redundancy to if-modified-since feature will\n> help us overcome any unspecified data-integrity troubles.\n\nI do agree that adding redundancy to 'if-modified-since' would help\novercome data-integrity troubles, but those data-integrity troubles\nare often of a nature that a 'length' hint isn't useful. I don't think\nwe should close our eyes and say 'data-integrity troubles' are\nunspecified; we have ample experience with a wide variety of them, and\nwe should pick a solution that actually solves problems that don't\nhave other solutions.\n\n> CERN HTTPD/3.0 has a configuration parameter OutputTimeout, its default value \n> 20 mins. While sending the response to the client, this timeout is in effect,\n> and after the timeout elapsed, prematurely disconnects the connection.\n> If the client isn't enough picky, (and lynx 2.4.2, Netscape 1.1N and the\n> CERN HTTPD/3.0 itself acting as proxy aren't) we will have truncated documents\n> as the result, no complains, error messages at all.\n\n> If we had the proposed redundancy, we will able to detect that something went\n> wrong, and had a try to recover the problem some way.\n> Not having the redundancy, the recovery process will start only after some\n> human intervention, when the problem detected.\n\n> If this unnice situation happens with my proxy cache, I can run my perl\n> script, which checks the whole cache for cached documents having\n> Content-Length header, and removes truncated files. It's a workaround,\n> but it's really an overkill, scanning a big cache for a few or no files at all.\n\nThis particular problem can be addressed by modifying the behavior of\ncaching programs not to cache data where the reliability of the\ntransmission is suspect. Clearly anything with less data than was\nspecified in the 'content-length' header is suspect, and shouldn't\nhave been put in the cache in the first place.\n\nIf you're going to mandate protocol changes in order to fix problems\nand those protocol changes require caching programs to change their\nbehavior based on those protocol changes, why not just mandate that\nthe cache programs just fix buggy behavior?\n\nProbably we should also ask that that in any transaction that doesn't\nhave a way of confirming the integrity of the transaction (e.g., HTTP\n0.9 implementation or no content-length, content-md5,\ncontent-transfer-encoding: packet) the sender should have a default\ntimeout of N seconds (at which time it should close the connection);\nthe recipient should have a default timeout of K seconds (at which\ntime it should assume that the transmission was prematurely terminated\nand the document not cached or saved) and K << N. If N is 20 minutes,\nmaybe K should be 5 minutes.\n\nI'm not opposed to having a content-MD5 and even a 'if-different-MD5'\nrequest, if you want to deal with end-to-end data integrity issues.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Sorry to be sending so much mail today -- but somehow the discussion\nseems to have come close to certain areas of interest, so...\n\nJames Pitkow writes:\n...\n\n > This along with the below modified guidelines for behavior (originally from\n > <95Aug16.102012pdt.2763@golden.parc.xerox.com> from \"Larry Masinter\"):\n > \n > * A server encountering a file with a modification date in the future\n >   (according to the server's current time) should not send the future\n >   'last-modified' date.\n > \n\nI agree that origin-servers should behave this way, and should log\nsuch errors so the operators can correct the situation.  However I\nthink that intermediate proxies should simply pass on what they get\nfrom the origin server, since *it need not matter to them*.  They also\nshould also save whatever date the upstream server says.  Furthermore,\nwhen they issue a GET-IMS, they'd better use the date the upstream\nserver last claimed, even if it was in \"the future\" according to the\ndownstream server, or else they might not get the appropriate 304 they\ncould otherwise reasonably expect to get. Since downstream clients may\nalso wish to use last-modified dates to generate GET-IMS requests of\ntheir own, that is why it should be passed on.  Expecting all servers\nto be within .0000X seconds of UTC seems just a bit Draconian to me.\n(Are there even NTP implementations for Mac and Windows? If so, how\nmany Mac or PC based web sites run them?) Building in magic time\nthresholds that are \"close enough\" seems like a kluge.  And if a\nserver is just way off and can't interpret GET-IMS requests correctly\nbecause of its own error generating last-modified, that's a local\nproblem.  Server software should, however, log GET-IMS requests where\nthe date is in *its* future, as it probably indicates a problem.\n\nDate errors are the server's to fix, but I don't agree that clock\ndrift should be punished.\n\n > * Servers ought to be able to either precompute checksums periodically or \n >   generate them on the fly, independent of the desires of proxies or caches.\n > \nAgreed.\n\n > * A proxy or cache shouldn't save a document with a future last-modified.\n > \nDisagree. Same reasons as above.\n\n > * A client and any proxies along the way should remember the exact\n >   last-modified (to the second) with which the document was delivered,\n >   no approximations, and independent of any local time settings. The same \n >   applies to size and checksum information if so desired.\n > \nAgreed.\n\n > Jim.\n > \n > \n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "I think there's a misunderstanding somewhere.\n\nI suggested:\n>> * A server encountering a file with a modification date in the future\n>>   (according to the server's current time) should not send the future\n>>   'last-modified' date.\n\nand Shel Kaphan rejoindered lengthily, but I will extract:\n\n> Expecting all servers to be within .0000X seconds of UTC seems just a\n> bit Draconian to me.\n\nClock drift is not an issue here. Suppose the server or file system\nclock is 20 minutes off in such a way that file dates might be 20\nminutes in the future. Then there's a 20 minute window where newly\nwritten files might be sent without a 'last-modified' date. After that\npoint, even though the clock is off a bit, they'll still be cached.\nNothing in the suggestions I made requires 'all servers to be within\n.0000X of UTC'.\n\n> (Are there even NTP implementations for Mac and Windows? If so, how\n> many Mac or PC based web sites run them?)\n\nI confess I don't know about Windows NTP, though I see it in the list\nof things supplied with a couple of 'commercial web services'.\n'Network Time' is easy to install and use for Macs.\n\n> Date errors are the server's to fix, but I don't agree that clock\n> drift should be punished.\n\nClock drift isn't punished. Being a year off might be. Even being an\nhour off just means that files less than an hour old won't get cached,\nbut... time flies! An hour later, they will be.\n\n>> * A proxy or cache shouldn't save a document with a future last-modified.\n> Disagree. Same reasons as above.\n\nYour reasoning apparently has the same flaw here.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "What about servers that are constantly creating dynamic documents?\nIf they're 1 minute ahead, none of their documents will be cacheable.\n\nShel\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Roy Fielding writes:\n> Something that the WG needs to keep in mind is that what I write in\n> the *draft* specification is what I believe to be the abstract and\n> specific semantics of each feature in the protocol.  I am counting\n> on people to correct me when what I write does not match their own\n> interpretation of the protocol.  Since I've developed my own client\n> library, client, and small portions of two servers, most of the time\n> I do get it right, but certainly not all of the time.\nI found the Draft 01 concerning Pragma: no-cache in particular\nand pragmas in general acceptable. I vote for no change. Otherwise\nwhy to introduce pragmas at all. (there wolud be no differenc between \npragmas and request/response headers.)\n... snip ... snip ... snip ...\n> We can then add a new directive to cover the semantics of a response\n> that must not be shared by multiple users.  We could call it \"private\",\n> but I am afraid that this would also imply privacy, which it shouldn't.\n> Unfortunately, there does not seem to be an antonym for \"shared\" or\n> \"communal\", so how about\n> \n>    Pragma: non-shared\n>            no-sharing\n>            do-not-share\n> \n> Er, on second thought, maybe we should just use \"private\"...\nBut what is the difference between the Pragma: no-cache and Pragma: private?\nI see only formal difference, but no semanthical.\n\nAndrew.\n\n\n\n"
        },
        {
            "subject": "De Re If-ModifiedSinc",
            "content": "Let me get this straight:\n\nThe problem: \n1) Documents are sometimes created on a server with incorrect dates. \n2) If such a document is request by a caching agent, the server \n   will send, and the client record, the incorrect date in the  \n   last-modified field.\n3) Such incorrect dates may be corrected on the server.\n4) If the incorrect date is in advance of the corrected date, \n   no modified versions of the documet will be sent in response \n   to requests containing If-Modified-Since dates from the original \n   message until the modifed dates are later than the incorrect dates.\n\nThe proposed cure:\n1) Modify clients and servers to send an additional field with \nconditional requests with a checksum of some form to detect \nactual changes in the content. Such checksum may be in the form \nof a file size, an integrity checksum, or a cryptographic checksum.\n\n---------\n\nComments:\n1) The proposed cure requires changes to the protocol. This \nshould not be done unless it can be shown that no other \n        _reasonable_  fix for the problem exists.\n\n2) The problem as described above is caused by problems at the\nserver side;  the cure can also be localised to the server-side\nwithout protocol modifications- for example, the server can keep \na record of the incorrect dates that have been changed, and always \nretransmit the document if it recieves a request containing one of\nthe bad dates. \n\n3) A cure of this kind fixes problems caused by errors on the \nserver, but does not address the problem of corruption caused by \nactivity on the client (assult with a deadly filemgr). \nHowever, the client can determine whether such corruption has \ntaken place by performing its own checksum calculations, and \nstoring the information in a safe place (e.g. the cache map). \n\n4) Think globally, act locally.\n\nSimon\n-------\n\"I am become Fluffy, destroyer of Curtains\" - Robert Oppenheimer's cat\n\"Teller! Teller!\"  - gripping finale to \"An H-Bomb called desire\"\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "In article <Pine.SOL.3.91.950816205717.5252C-100000@chivalry> Simon Spero\n<ses@tipper.oit.unc.edu> wrote:\n> \n> \n>         2) The problem as described above is caused by problems at the\n>         server side;  the cure can also be localised to the server-side\n>         without protocol modifications- for example, the server can keep\n>         a record of the incorrect dates that have been changed, and always\n>         retransmit the document if it recieves a request containing one of\n>         the bad dates.\n> \n\nThis solution is too costly to implement in existing servers,\non the other hand, size checksums are easy to add to most servers.\nI seriously doubt any server will keep a record of screwups\nand fix them automatically, so this is a completely impractical solution.\n\nYour \"solution\" also fails when the file has been modified but\nhas the same date.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "On Wed, 16 Aug 1995, Shel Kaphan wrote:\n> What about servers that are constantly creating dynamic documents?\n> If they're 1 minute ahead, none of their documents will be cacheable.\n\nRealTimeServerTimeAction\n\n12:0012:01C: GET /this-dynamic-object\nS: 200 Ok\n   Last-Modified: some-old-time\n\n   (some-old-time calculated by determining age\n   of the data it assembles to produce the page)\n\n12:0512:06C: GET /this-dynamic-object\n   If-modified-since: some-old-time\nS: 304 Not Modified\n\n   (the calculation to check last_modified of\n   the data is much faster than assembling the\n   whole page)\n\n12:0712:08(some data changes on the server which \naffects this-dynamic-object)\n\n12:0812:09C: GET /this-dynamic-object\n   IMS: some-old-time\nS: 200 Ok\n   Last-Modified: 12:08\n\nOnly objects which truly change with every minute (i.e., the data used to\ndetermine what the page looks like) need be uncacheable.\n\nBrian\n\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> It is not at all a fatal flaw.  The size returned by the client needs\n> to be specified to be the same as the \"content-length\" returned by\n> the server during the request.  If line-feed conversion is being\n> done consistantly the size can be compared accurately.\n\nThis makes the \"content-length\" an arbitrary value that must be stored\nout of band in the cache, and returned to the server as part of a\n\"version check\" request.\n\nChanging the date comparision from equal to greater than or equal\nmakes the date comparison into another such item, module conversion\nbetween date formats (though most file systems have a place reserved\nfor that). You could implement this by saving the last-modified header\nand sending it back as the value of the if-modified-since header.\n\nIf we're going to change the spec for this, and are requiring that\nclients keep out of band data to comply, why not provide versioning of\nthe kind the server (which has to do the work) wants to support?\n\nObservation: the version string has no meaning as far as the client is\nconcerned.\n\nProposal:\n\nA new header from servers: Version: Version-string\nA new header from clients: If-Not-Version: Version-string\n\nVersion-string is a string of characters other than \";\" and whitespace.\n\nSince version-string only has meaning to the server, it can be\nwhatever you want. The file size; a VMS file version number; an MD5\ndigest; a CRC, etc.\n\n<mike\n\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "On Wed, 16 Aug 1995, Lou Montulli wrote:\n> In article <Pine.SOL.3.91.950816205717.5252C-100000@chivalry> Simon Spero\n> <ses@tipper.oit.unc.edu> wrote:\n> > \n> > \n> >         2) The problem as described above is caused by problems at the\n> >         server side;  the cure can also be localised to the server-side\n> >         without protocol modifications- for example, the server can keep\n> >         a record of the incorrect dates that have been changed, and always\n> >         retransmit the document if it recieves a request containing one of\n> >         the bad dates.\n> > \n> \n> This solution is too costly to implement in existing servers,\n\nBS - the server can easily go \"oh, that IMS date is past my current time. I \nwill presume it's erroneous and send the full copy\".  It doesn't have to\nremember the dates that were sent out, or the files those dates were attached\nto.\n\nHere's the algorithm of IMS requests:\n\nVariables: IMS = If-Modified-Since date\n   LM = Server's last-modified\n   CURRTIME = The current time on the *server*\n\n1) if IMS < LM send document\n2) if IMS > CURRTIME send document\n   that leaves LM < IMS < CURRTIME for the 304 response.  \n\nThe problem situation:\n\nRealTimeServerTimeAction\n\n12:0012:10C: GET /file\nS: 200 Ok\n   Last-modified: 12:08\n12:0212:12 -> 12:02(clock fixed)\n12:0612:06(/file modified)\n12:1012:10C: GET /file\n   IMS: 12:08\nS: 304 Not Modified\n\nso, here the file was modified but reported not modified. By the spec, \nthe server can't go \"hey, 12:08 is past the LM date of /file\".  So, \nwhen would this happen?  Is this common enough to be a problem?  \n\nLet's rewrite this algebraically:\n\nRealTimeServerTimeAction\nAA+LagC: GET /file\nS: 200 Ok\n   Last-modified: LM   (LM < A+Lag)\nA+BA+Lag+B-> A+B(clock fixed)\nA+B+CA+B+C(/file modified)\nA+B+C+DA+B+C+DC: GET /file\n   IMS: LM\n\nif ( LM < A+B+C+D && LM > A+B+C) then a 304 is issued, and LM < A+Lag\n\nso we get \n\nB+C < LM-A < B+C+D    with LM-A < Lag\n\nSo B+C needs to be less than Lag for this race condition to occur.\n\n***\nIn other words, the time between the first, incorrect access and the \nmost recent file modification needs to be less than the Lag for this to \noccur.  \n***\n\nLet's say Lag is 30 years.  The only situation where this race condition \nwould occur is if a file is in a cache for 30 years without reload in the \ncache and without modification on the server, and then after that 30-year \nspan a 304 request was made.  This seems vanishingly unlikely.\n\nLet's say Lag is 5 minutes.  The \"window\" of error is only 5 minutes \nlarge - the file would have to be changed on the server within 5 minutes \nof the erroneous access for an incorrect 304 to be issued.\n\nFInally, let's say Lag is 3 days.  If the very next 304 request isn't \nmade for more than 3 days, and the file was modified within 3 days of the \nlast time it was modified and not after that, then it's a problem.  To \nme, this is perfectly acceptible - most caches out there don't keep \ndocuments around without doing 304 requests often enough for this to be a \nconcern of mine.  What do the statisticians in the audience think?  And \nwas my 11th-hour algebra on the mark?  :)\n\n> Your \"solution\" also fails when the file has been modified but\n> has the same date.\n\nCan we just agree (feel like rodney king here) that this is a broken case \nand not worth wasting time on?  Modified files can also have the same \nsize, even the same checksum if we try hard enough.  \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "In article <Pine.SOL.3.91.950816222536.604K-100000@eat.organic.com> Brian\nBehlendorf <brian@organic.com> wrote:\n> \n> On Wed, 16 Aug 1995, Lou Montulli wrote:\n> > In article <Pine.SOL.3.91.950816205717.5252C-100000@chivalry> Simon Spero\n> > <ses@tipper.oit.unc.edu> wrote:\n> > >\n> > >\n> > >         2) The problem as described above is caused by problems at the\n> > >         server side;  the cure can also be localised to the server-side\n> > >         without protocol modifications- for example, the server can keep\n> > >         a record of the incorrect dates that have been changed, and always\n> > >         retransmit the document if it recieves a request containing one of\n> > >         the bad dates.\n> > >\n> \n> > This solution is too costly to implement in existing servers,\n> \n> BS - the server can easily go \"oh, that IMS date is past my current time. I\n> will presume it's erroneous and send the full copy\".  It doesn't have to\n> remember the dates that were sent out, or the files those dates were attached\n> to.\n> \n> Here's the algorithm of IMS requests:\n> \n> Variables: IMS = If-Modified-Since date\n>            LM = Server's last-modified\n>            CURRTIME = The current time on the *server*\n> \n> 1) if IMS < LM send document\n> 2) if IMS > CURRTIME send document\n>    that leaves LM < IMS < CURRTIME for the 304 response.\n\nBrian you should go read the spec or past messages before you post \nstuff like this.  As pointed out earlier in this thread the\nIMS time can be after the current last modified date of the file\nand still be valid.  Changing this is not an option.\n\n> \n> > Your \"solution\" also fails when the file has been modified but\n> > has the same date.\n> \n> Can we just agree (feel like rodney king here) that this is a broken case\n> and not worth wasting time on?  Modified files can also have the same\n> size, even the same checksum if we try hard enough.\n> \nNo, I'm afraid we can't.  Not when it is as easy to solve as\nsending a length along with the IMS request.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <19950816.75DE920.13DA5@contessa.phone.net> mwm@contessa.phone.net\n(Mike Meyer) wrote:\n> \n> > It is not at all a fatal flaw.  The size returned by the client needs\n> > to be specified to be the same as the \"content-length\" returned by\n> > the server during the request.  If line-feed conversion is being\n> > done consistantly the size can be compared accurately.\n> \n> This makes the \"content-length\" an arbitrary value that must be stored\n> out of band in the cache, and returned to the server as part of a\n> \"version check\" request.\n\nthe IMS last modified date is already a value that must be stored\nout of band in the cache.  The client local time can not be\nrelied upon so it can not be used to compute a last modified date.\nOnly in cases where you can guarentee reliable dates (i.e not\nPC's and not Macs) can you use the local date as a value, I\nsuspect some UNIX proxies do this.\n\n> \n> If we're going to change the spec for this, and are requiring that\n> clients keep out of band data to comply, why not provide versioning of\n> the kind the server (which has to do the work) wants to support?\n> \n> Observation: the version string has no meaning as far as the client is\n>         concerned.\n> \n> Proposal:\n> \n> A new header from servers: Version: Version-string\n> A new header from clients: If-Not-Version: Version-string\n> \n> Version-string is a string of characters other than \";\" and whitespace.\n> \n> Since version-string only has meaning to the server, it can be\n> whatever you want. The file size; a VMS file version number; an MD5\n> digest; a CRC, etc.\n\nThis would work pretty well but has the disadvantage of not being\nreproducable on the client end.  A standardized checksum method\nwould remove the need for out of band data to be stored, or if\nthe out of band data was lost, it could be regenerated.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "On Wed, 16 Aug 1995, Lou Montulli wrote:\n> In article <Pine.SOL.3.91.950816222536.604K-100000@eat.organic.com> Brian\n> Behlendorf <brian@organic.com> wrote:\n> > Variables: IMS = If-Modified-Since date\n> >            LM = Server's last-modified\n> >            CURRTIME = The current time on the *server*\n> > \n> > 1) if IMS < LM send document\n> > 2) if IMS > CURRTIME send document\n> >    that leaves LM < IMS < CURRTIME for the 304 response.\n> \n> Brian you should go read the spec or past messages before you post \n> stuff like this.  As pointed out earlier in this thread the\n> IMS time can be after the current last modified date of the file\n> and still be valid.  Changing this is not an option.\n\nDid you read the spec?  Check out: \nhttp://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v10-spec-02.html#If-Modified-Since\n\n| A conditional GET method requests that the identified resource be\n| transferred only if it has been modified since the date given by the\n| If-Modified-Since header. The algorithm for determining this includes \n| the following cases: \n|\n| a) If the request would normally result in anything other than a 200 \n|    (ok) status, or if the passed If-Modified-Since date is invalid, the \n|    response is exactly the same as for a normal GET.\n\nCheck out \"if the passed If-Modified-Since date is invalid\".  I don't \nknow any sane server author who would consider a date of Last-Modified \nafter the current server date as \"valid\", but I suppose that is a matter \nof webmaster opinion (and thus server config?)  I would certainly \nconsider it invalid, just as I would consider If-Modified-Since: Uranus \ninvalid.\n\n> > > Your \"solution\" also fails when the file has been modified but\n> > > has the same date.\n> > \n> > Can we just agree (feel like rodney king here) that this is a broken case\n> > and not worth wasting time on?  Modified files can also have the same\n> > size, even the same checksum if we try hard enough.\n>\n> No, I'm afraid we can't.  Not when it is as easy to solve as\n> sending a length along with the IMS request.\n\nIn the end, you're right, this is a trivial thing to add to the protocol. \nAnd in the end, as a server-side guy, it doesn't affect what I do unless I\nwant to use that info.  But that's not the only criteria involved when \nanalysing proposals, is it?\n\nAs an attempt to reach common ground, I will say that I think the \nproposal floated about a generic If-Modified-<any response header> header \nmight be a good idea.  Thus, you can have If-Modified-Content-Length if \nyou want.  \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "In article <Pine.SOL.3.91.950816235241.604L-100000@eat.organic.com> Brian\nBehlendorf <brian@organic.com> wrote:\n http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v10-spec-02.html#If-Modified-Since\n> \n> | A conditional GET method requests that the identified resource be\n> | transferred only if it has been modified since the date given by the\n> | If-Modified-Since header. The algorithm for determining this includes\n> | the following cases:\n> \n> | a) If the request would normally result in anything other than a 200\n> |    (ok) status, or if the passed If-Modified-Since date is invalid, the\n> |    response is exactly the same as for a normal GET.\n> \n> Check out \"if the passed If-Modified-Since date is invalid\".  I don't\n> know any sane server author who would consider a date of Last-Modified\n> after the current server date as \"valid\", but I suppose that is a matter\n> of webmaster opinion (and thus server config?)  I would certainly\n> consider it invalid, just as I would consider If-Modified-Since: Uranus\n> invalid.\n\nWell that's how I interpreted it as well, but several people\nresponded over the last couple of days that it was the intention\nof the spec to accept any date equal to or past the current\nmodification date of the file.\n\n> \n> > > > Your \"solution\" also fails when the file has been modified but\n> > > > has the same date.\n> > >\n> > > Can we just agree (feel like rodney king here) that this is a broken case\n> > > and not worth wasting time on?  Modified files can also have the same\n> > > size, even the same checksum if we try hard enough.\n> \n> > No, I'm afraid we can't.  Not when it is as easy to solve as\n> > sending a length along with the IMS request.\n> \n> In the end, you're right, this is a trivial thing to add to the protocol.\n> And in the end, as a server-side guy, it doesn't affect what I do unless I\n> want to use that info.  But that's not the only criteria involved when\n> analysing proposals, is it?\n> \n> As an attempt to reach common ground, I will say that I think the\n> proposal floated about a generic If-Modified-<any response header> header\n> might be a good idea.  Thus, you can have If-Modified-Content-Length if\n> you want.\n\nMaking it a separate header adds 20 characters to the request that\nwould not be needed if \"; length=\" were sent instead.  We should\nuse the standard MIME method of extending headers by using semi-colon\ndelimited name value pairs.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Of course your example is right.\n\nThe questionable area is whether it is reasonable to expect\nservers to set last-modified = the date of the last modification\nto the data that went into producing the dynamic document in question,\nor whether it is legit for them to say last-modified = date of production\nof the actual document that is transmitted.\n\nMy entire argument rests on the assumption that the latter will be done.\nIf that isn't going to work, I recommend it be made explicit somewhere,\nas the failure mode will be subtle -- just a lot of mysteriously \nuncacheable documents.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "On Thu, 17 Aug 1995, Lou Montulli wrote:\n> In article <Pine.SOL.3.91.950816235241.604L-100000@eat.organic.com> Brian\n> Behlendorf <brian@organic.com> wrote:\n>  http://www.ics.uci.edu/pub/ietf/http/draft-ietf-http-v10-spec-02.html#If-Modified-Since\n> > \n> > | A conditional GET method requests that the identified resource be\n> > | transferred only if it has been modified since the date given by the\n> > | If-Modified-Since header. The algorithm for determining this includes\n> > | the following cases:\n> > \n> > | a) If the request would normally result in anything other than a 200\n> > |    (ok) status, or if the passed If-Modified-Since date is invalid, the\n> > |    response is exactly the same as for a normal GET.\n> > \n> > Check out \"if the passed If-Modified-Since date is invalid\".  I don't\n> > know any sane server author who would consider a date of Last-Modified\n> > after the current server date as \"valid\", but I suppose that is a matter\n> > of webmaster opinion (and thus server config?)  I would certainly\n> > consider it invalid, just as I would consider If-Modified-Since: Uranus\n> > invalid.\n> \n> Well that's how I interpreted it as well, but several people\n> responded over the last couple of days that it was the intention\n> of the spec to accept any date equal to or past the current\n> modification date of the file.\n\nI remember you (and/or Rob McCool) proposed just doing a straight string\ncompare of the LM and IMS (LM = IMS), which is *different* than LM < IMS <\nCURRTIME.  If you implemented the latter in the Netsite, the Apache group\nwill put it in Apache, I bet Chuck would put it in Webstar and Henrik would\nput it in CERN and Brandon would put it in NCSA.  Deal?  I honestly think we\nwould address the most serious aspects of the incorrect-time problem without\nforcing a change to the HTTP protocol by doing this. \n\n> > As an attempt to reach common ground, I will say that I think the\n> > proposal floated about a generic If-Modified-<any response header> header\n> > might be a good idea.  Thus, you can have If-Modified-Content-Length if\n> > you want.\n> \n> Making it a separate header adds 20 characters to the request that\n> would not be needed if \"; length=\" were sent instead.  We should\n> use the standard MIME method of extending headers by using semi-colon\n> delimited name value pairs.\n\nOkay, howbout\n\nIf-Different: Date > DATE; Content-length != SIZE; Content-type != text/html;\n\nwhere the conditions are OR'd, and quantities are compared (so we can \nstill have the functionality of IMS  where LM < IMS < CURRTIME).  \nHopefully the syntax could allow queries like \"give it to me if it's a \nlarger file\" or \"give it to me if it's a higher level of HTML\" or \nsomething.  Thoughts?  This could even be really powerful on arbitrary \nHTTP headers.... and open to abuse of course.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "On Thu, 17 Aug 1995, Shel Kaphan wrote:\n> Of course your example is right.\n> \n> The questionable area is whether it is reasonable to expect\n> servers to set last-modified = the date of the last modification\n> to the data that went into producing the dynamic document in question,\n> or whether it is legit for them to say last-modified = date of production\n> of the actual document that is transmitted.\n\nWell, it's also \"legit\" for them to use Pragma: no-cache, so it's really \nup to how cache-friendly the server application developer wants to be.  \nLike everything we've discussed here w/r/t caching....\n\n> My entire argument rests on the assumption that the latter will be done.\n\nWell, if the latter is being done, does the server really want the data \ncached anyways?  It's not getting cached now....\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "> the IMS last modified date is already a value that must be stored\n> out of band in the cache.\n\nI think I pointed that out; it's important to note, though.\n\n> > A new header from servers: Version: Version-string\n> > A new header from clients: If-Not-Version: Version-string\n\n> This would work pretty well but has the disadvantage of not being\n> reproducable on the client end.\n\nThe IMS LM date has the same problem - the client can't reproduce it.\n\nI like the more generic \"If-different\" header. We can even tweak it to\nsave 15 of the 20 bytes you complained about:\n\nIf-Different: 1#( field-name value ; )\n\nThe server then verifies that for each field-name/value pair, it is\ngoing to send a header with that name whose value (after trimming\nleading and trailing whitespace) is the same as the client sent on the\nif-different field. If one of them is different it sends the document.\nOtherwise, it sends a 3xx reply \"Not different\".\n\nYou can therefore get the behavior you want from \n\nif-modified-since: last-modified ; size=length\n\nwith\n\nif-different: last-modified ; content-length length\n\nand leave if-modified-since with the current definition for those who\nhave a use for that.\n\nOptional bandwidth reducer: allow the server to not send any headers\nthat are named on the if-different header that haven't changed.\n\n<mike\n\n\n\n"
        },
        {
            "subject": "Re: Improving If-ModifiedSinc",
            "content": "In article <19950817.77E12D0.23E6@contessa.phone.net> mwm@contessa.phone.net\n(Mike Meyer) wrote:\n> \n> > This would work pretty well but has the disadvantage of not being\n> > reproducable on the client end.\n> \n> The IMS LM date has the same problem - the client can't reproduce it.\n> \n> I like the more generic \"If-different\" header. We can even tweak it to\n> save 15 of the 20 bytes you complained about:\n> \n>         If-Different: 1#( field-name value ; )\n> \n> The server then verifies that for each field-name/value pair, it is\n> going to send a header with that name whose value (after trimming\n> leading and trailing whitespace) is the same as the client sent on the\n> if-different field. If one of them is different it sends the document.\n> Otherwise, it sends a 3xx reply \"Not different\".\n> \n> You can therefore get the behavior you want from\n> \n>         if-modified-since: last-modified ; size=length\n> \n> with\n> \n>         if-different: last-modified ; content-length length\n> \n> and leave if-modified-since with the current definition for those who\n> have a use for that.\n> \n\nThe if-different header would occomplish this, but still think it's\nwasteful.  We should try and use the existing IMS header if\npossible.  If it turns out not to be possible then we should add\nto the future specification that all http headers can be followed\nby one or more arguments delimited by semi-colons.  This will\nkeep this lossage from happening in the future.\n\nOther than that I'm happy with this proposal.  Let's keep the\nnames short and get this into some form of a spec so that we\ncan get on with life.  (not that it hasn't been fun arguing)\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "more If-ModifiedSinc",
            "content": "I feel like the little kid whose questions are missed among the\nshouting of a huddle of bigger kids.  So I'll try again, now that the\nexchanges among people on the U.S. Pacific Coast have died down.  The\nissue is cache corruption and how to avoid it, and I'm obviously not\nfollowing some of the dialog.\n\nYesterday I asked whether Lou Montulli (or anyone else) could describe\nto me the circumstances under which a file's modification date might\nremain unchanged, even though the file content had changed.  I can\nimagine the mechanics (e.g., Unix \"touch\"), but I don't understand why\nsomeone would go to the trouble to do so.  Anyone care to respond?\n(Private is okay.)\n\nLou argues that \"HTTP servers reference a file system that can be\nchanged at random\".  I presume he means by the Webmaster, and not by\nother users.  If the Webmaster doesn't administer a site properly, then\nall bets are off anyway.  Lou also says that \"dates are not strong\nenough by themselves to do an adequate job of versioning\".  I'd like to\nknow why, which gets back to my original question.\n\nI also don't understand the arguments about a Version header.  Mike\nMeyer suggested that the origin server could generate one, and a client\ncould reflect it back to ask whether something changed.  Lou Montulli\nwas unhappy, saying the client could not calculate the Version\ninformation independently.  I don't understand that reasoning.  Why is\nit so hard to store it?  Why does the client need to calculate it?\n\nI infer that Lou wants to use size as a way to tell whether a client\ngot an entire resource in the face of prematurely cut off connections.\nWouldn't a requirement that there be either a Content-Length or\npacketized content (HTTP/1.2) be a better solution?  The client would\nthen know whether it got everything then.\n\nI support the idea of the client's knowing whether its cache is\ncorrect, but I oppose burdening the origin server with significant\nextra work to help it do so.  So I oppose the need for an origin server\nalways to calculate checksums or file sizes, since that might require\nrunning a script.  Even asking the origin server to compare a checksum\nor file size supplied by the client against the server's idea of those\nvalues is a burden.  For scripts, the server would have to do all the\nprocessing of a GET anyway, if only to decide the supplied value\nmatches.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "At 12:49 AM 8/17/95 -0700, Brian Behlendorf wrote:\n>I remember you (and/or Rob McCool) proposed just doing a straight string\n>compare of the LM and IMS (LM = IMS), which is *different* than LM < IMS <\n>CURRTIME.  If you implemented the latter in the Netsite, the Apache group\n>will put it in Apache, I bet Chuck would put it in Webstar and Henrik would\n>put it in CERN and Brandon would put it in NCSA.  Deal?  I honestly think we\n>would address the most serious aspects of the incorrect-time problem without\n>forcing a change to the HTTP protocol by doing this. \n\nI quote myself:\n>The spec only refers to \"if the resource has been modified since the\n>If-Modified-Since: date\".  As far as I'm concerned, if the request contains\n>\"If-Modified-Since: Mon, 14 Aug 1995 23:38:43 GMT\", and the server has a\n>last modified date of \"Mon, 14 Aug 1995 23:38:42 GMT\", then returning\n>something other than 304 is a BUG.  Who knows how the caching client/proxy\n>is saving the date.  Maybe it's saving the last time it requested it, maybe\n>it's saving the Date: the server previously sent back, maybe it's saving the\n>last time when the response is completely received...\n\nmaybe it's saving the Last-Modifed-Date: like everyone here says it\n\"should.\"  Maybe not.  It's not the server's place to force the client to\nstore that information.  The construct as it is simple.  Has this URL\nchanged since X.  And as Roy pointed out, this construct also had in mind\nthe idea of functionality to say: \"give me something if it less than 15\nminutes old\".  If-Modifed-SINCE really does mean If-Modified-SINCE.\n\n>If-Different: Date > DATE; Content-length != SIZE; Content-type != text/html;\n>\n>where the conditions are OR'd, and quantities are compared (so we can \n>still have the functionality of IMS  where LM < IMS < CURRTIME).  \n>Hopefully the syntax could allow queries like \"give it to me if it's a \n>larger file\" or \"give it to me if it's a higher level of HTML\" or \n\nWhere's that darn sledgehammer?  I need to swat this fly.\n-----\nDan DuBois, Software Animal                          ddubois@spyglass.com\n(708) 505-1010 x532                     http://www.spyglass.com/~ddubois/\n\n\n\n"
        },
        {
            "subject": "Re: De Re If-ModifiedSinc",
            "content": "According to Brian Behlendorf:\n> I remember you (and/or Rob McCool) proposed just doing a straight string\n> compare of the LM and IMS (LM = IMS), which is *different* than LM < IMS <\n> CURRTIME.  If you implemented the latter in the Netsite, the Apache group\n> will put it in Apache, I bet Chuck would put it in Webstar and Henrik would\n> put it in CERN and Brandon would put it in NCSA.  Deal?  I honestly think we\n> would address the most serious aspects of the incorrect-time problem without\n> forcing a change to the HTTP protocol by doing this. \n> \n\nI think this *is* forcing a change in the HTTP protocol.  On the other\nhand I agree it is a good idea and it will go a long way toward solving\nthe problems discussed here.\n\nIt might be even better if the \"date\" of the IMS is an opaque string\nso it could be \"<date>;length=<number>\" or \"<date>;md5=<md5digest>\" or\nwhatever.  The only limitation is that it can't be recreated by the\nclient, but I think that is unrealistic anyway.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: If-ModifiedSince and forged date",
            "content": "Brian Behlendorf writes:\n > On Thu, 17 Aug 1995, Shel Kaphan wrote:\n > > Of course your example is right.\n > > \n > > The questionable area is whether it is reasonable to expect\n > > servers to set last-modified = the date of the last modification\n > > to the data that went into producing the dynamic document in question,\n > > or whether it is legit for them to say last-modified = date of production\n > > of the actual document that is transmitted.\n > \n > Well, it's also \"legit\" for them to use Pragma: no-cache, so it's really \n > up to how cache-friendly the server application developer wants to be.  \n > Like everything we've discussed here w/r/t caching....\n > \n > > My entire argument rests on the assumption that the latter will be done.\n > \n > Well, if the latter is being done, does the server really want the data \n > cached anyways?  It's not getting cached now....\n > \n > Brian\n > \n > --=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\n > brian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n > \n\nHere's the chain of logic as I see it: Right now it is impractical to\ndepend on Expires, since the installed base of browsers doesn't\nsupport it adequately (this is another, different topic).  But\nsometimes you have to control caching.  Pragma: no-cache is even newer\nand less supported than Expires, so we can't depend on that yet.  What\nmeans exist to control caching?  One of the main ones is simply to\nomit last-modified, which then triggers \"don't cache\" heuristics in\nproxy caches.  So, in answer to your question, we don't see the\nphenomenon of \"last-modified: right now\" much now because we don't\nsend last-modified headers on dynamic documents because we have to\nomit it entirely to get any control of caching, which right now is in\npractice an all/none choice.  After Expires is generally supported,\nand history mechanisms generally ignore it as per the new spec, then\nwe'll want to start supporting GET-IMS to get finer control and enable\nconditional cache-reloads.  At that time, the cacheability of dynamic\ndocuments will be start to become an issue, so we have to design how\nits going to work now.\n\nIf, as I hope will not become necessary, we have to generate\nlast-modified = now - (maximum clock drift between my server & any\ncache) in order to ensure cacheability of documents we generate, let's\nget that into the spec now.  My earlier argument is that this kluge\nwouldn't be necessary if clients didn't interpret the last-modified\ndate, and so far nobody has argued cogently against that.\n\n  --Shel\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": ">But what is the difference between the Pragma: no-cache and Pragma: private?\n\nprivate would mean the response is intended for a single user agent\nand thus must not be stored in a shared response cache.\n\nno-cache would mean the response must not be stored in any response cache.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Wed, 16 Aug 1995, Balint Nagy Endre wrote:\n\n> It is an interesting question: there are any good reasons to forbid\n> clients to cache documents in its non-shared cache? Doubtful, I think.\n> But if anybody can give good examples explaining the need for this,\n> then we shall consider adding the function proposed by \"Pragma: no-local-cache\",\n> but (see later)\n\nYES! Shopping basket applications and highly dynamic documents *require* \ntotal defeating of caches at every stage. The results otherwise are \nextremely ugly as the user's idea of what is going on becomes radically \nunsynchronized from what the server thinks is going on. Believe me - \nExpires: does not cut it at all. Too many browsers have either truely \nbroken implementations or no implementation at all. Mosaic is a particular \noffender on that front.\n\nThe cleanest solution to the whole problem is no-cache.\n\n-- \nBenjamin Franz\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "> Too many browsers have either truely broken implementations or no\n> implementation at all. Mosaic is a particular offender on that front.\n\n> The cleanest solution to the whole problem is no-cache.\n\nLet me repeat this:\n\nIf current system elements don't properly implement the current\nprotocol, adding more protocol they also don't implement doesn't solve\nthe problem.\n\nIf changing broken system elements to implement the current protocol\ncorrectly fixes the problem, we don't need protocol changes.\n\n\n\n"
        },
        {
            "subject": "Re: 411 response code: clarification, pleas",
            "content": ">401 and 411 are the response codes for failed authentication.  I'm\n>uncertain when an origin server should respond with 411, rather than\n>401.  Here's what I think I understand.  Assume, in each case, that the\n>origin server requires authentication for the request.\n>\n>1) Client sends no Authorization request header.  Server always\n>responds with 401.\n\nYep.\n\n>2) Client sends Authorization request header.  Server doesn't like the\n>information therein.\n>\n>    2a) Basic scheme.  At present, servers send 401 if the name:passwd\n>    is unacceptable.\n\nAre you sure?  I thought they sent 403 in this case.\n\n>    Should they send 411 for HTTP/1.1?\n\nYes, but only if they want to include information about how the\nuser can correct the situation.\n\n>    Apparently\n>    not:  the challenge is likely to be the same as the previous one,\n>    so sending the same response is futile.\n\n\"likely to be the same\" is not a strong statement.\n\n>    2b) Digest scheme.\n>    2b1) The \"stale\" attribute says whether the problem is with the\n>    nonce value, so the client can tell whether the server thinks\n>    that's what the problem is.  Should the server send 401 or 411 on a\n>    stale nonce?  (I'm guessing 411, although it doesn't appear to\n>    matter.)\n\n411 (I was assuming that 401 is only sent when the request had no\n     Authorization field at all).\n\n>    2b2) If the nonce is fresh, but the server rejects the authorization\n>    information for other reasons, I'm guessing the server should\n>    return a 401. A 411 would imply that the client could recalculate\n>    the Authorization header from the challenge (realm and nonce), but\n>    they may well be the same the second time, and the server would\n>    reject it again.\n\n411 if the server wants to give them another chance, 403 otherwise.\nThe description of 411 states that:\n\n   The response must include a WWW-Authenticate header field \n   (Section 8.30) containing a challenge applicable to the requested \n   resource. If the challenge is different from that assumed by the \n   last request, the client may repeat the request with a suitable \n   Authorization header field after obtaining the user's approval.\n\nShould that be \"If and only if\"?\n\nOn the other hand, we could just use 401 for both, but I was told\nearlier (on the list) that the 411 semantics were needed.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: 411 response code: clarification, pleas",
            "content": "Roy Fielding <fielding@beach.w3.org> wrote:\n  > >[Dave Kristol wrote:]\n  > >2) Client sends Authorization request header.  Server doesn't like the\n  > >information therein.\n  > >\n  > >    2a) Basic scheme.  At present, servers send 401 if the name:passwd\n  > >    is unacceptable.\n  > \n  > Are you sure?  I thought they sent 403 in this case.\nNCSA sends 401.  That's also what my server does.  403, according to the\ndraft, means \"Authorization will not help\", so that's wrong.\n  > \n  > >    Should they send 411 for HTTP/1.1?\n  > \n  > Yes, but only if they want to include information about how the\n  > user can correct the situation.\nDo you mean \"include\" as part of WWW-Authenticate, or as part of an\nentity.  I would be leery of the latter, for the same reason that the\nUnix login program doesn't tell you what went wrong:  you don't want to\nsay whether it was a bad user-id, a bad password, or other.\n  > \n  > >    Apparently\n  > >    not:  the challenge is likely to be the same as the previous one,\n  > >    so sending the same response is futile.\n  > \n  > \"likely to be the same\" is not a strong statement.\nWell, for Basic there's really no choice to the challenge, is there?\nSo the challenge will certainly be the same.\n  > \n  > >    2b) Digest scheme.\n  > >    2b1) The \"stale\" attribute says whether the problem is with the\n  > >    nonce value, so the client can tell whether the server thinks\n  > >    that's what the problem is.  Should the server send 401 or 411 on a\n  > >    stale nonce?  (I'm guessing 411, although it doesn't appear to\n  > >    matter.)\n  > \n  > 411 (I was assuming that 401 is only sent when the request had no\n  >      Authorization field at all).\n  > \n  > >    2b2) If the nonce is fresh, but the server rejects the authorization\n  > >    information for other reasons, I'm guessing the server should\n  > >    return a 401. A 411 would imply that the client could recalculate\n  > >    the Authorization header from the challenge (realm and nonce), but\n  > >    they may well be the same the second time, and the server would\n  > >    reject it again.\n  > \n  > 411 if the server wants to give them another chance, 403 otherwise.\nAs I said above, 403 is the wrong response, according to the words in\nthe draft (v10-spec-01).\n  > The description of 411 states that:\n  > \n  >    The response must include a WWW-Authenticate header field \n  >    (Section 8.30) containing a challenge applicable to the requested \n  >    resource. If the challenge is different from that assumed by the \n  >    last request, the client may repeat the request with a suitable \n  >    Authorization header field after obtaining the user's approval.\n  > \n  > Should that be \"If and only if\"?\n\nIf you said that, you would preclude the server's returning the same\nchallenge and letting the client have another guess at the correct\nvalue.  Then again, if that happened, the user could try the same link\nagain, the client would send no Authorization header, the server would\nreturn a challenge, and the user could try again.\n  > \n  > On the other hand, we could just use 401 for both, but I was told\n  > earlier (on the list) that the 411 semantics were needed.\n\nThat's why I asked for clarification.  My gut tells me there's a use\nfor 411, but I don't see it yet.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "A modest proposa",
            "content": "There has been a lot of somewhat repetitive debate on this mailing\nlist over the last few days.  I suspect that some of the confusion\nstems from a lack of a shared set of goals and principles.  Maybe\nwe should settle on those first, before trying to design or tweak\nmechanisms to implement caching?\n\nHence this message.  I'm going to pretend that we are designing\na new protocol, not changing an existing one, to avoid confusing\nthe issue with debates about broken implementations or minimal\nchanges.  Until we can agree on what we want the protocol to do, it's\npointless to argue over how it does it.\n\nLet's assume that the HTTP 1.1 spec, and all subsequent ones,\nexplicitly state the tautology that \"an implementation not conforming\nto this specification does not conform to this specification.\"  Any\n\"broken\" browsers, servers, or clients will simply not be able to claim\nconformance to HTTP 1.1, so it is not necessary to write that spec to\nallow non-conforming implementations.  At the same time, the robustness\nprinciple applies:  we should not write a spec that leads to a\n\"fragile\" system.\n\nTime for a Principle:\n    Principle #1 of caching:\nCaches should never introduce undetectable incorrectness.\nCorrectness always takes precedence over performance.\n\nIf we can't agree on this one, then we will never reach consensus\nabout a caching design.\n\nThe main lesson I draw from this principle is \"when in doubt,\ndon't cache\".  This is also a version of the robustness principle.\nIn practical terms: a client or proxy, when trying to decide\nwhether or not to cache something, should do all feasible sanity checks\nand not cache the object if a check fails.\n\nThe other lesson is that a server needs a foolproof mechanism to\ntell the client (or proxy) not to cache an object.  This cannot\nbe an \"optional\" feature of the protocol, and it should not be\nbased on any client/proxy algorithm that could get a wrong answer.\n\nSo how would I design HTTP 1.1, if I were in charge?\n\n(1) Servers may send a header field explictly controlling\ncaching.  The spec could either be simple, something like\nDont-Cache-This-ever:\nor a little more complicated:\nCaching-allowed: [never | always | byClient | byProxy]\nto allow for unanticipated future developments.\n\n(2) Clients and proxies need a foolproof way to validate cache\nentries.  \"If-modified-since\" seems to be of questionable\nreliability.  I suggest we use an \"opaque version ID\" approach:\n    o   For any cachable object, the server must generate a\n    unique version ID.  This could be constructed from\na timestamp, but only if the server is able to\nguarantee uniqueness of the timestamp.  A server\nwithout a synchronized clock may use any other scheme\nit wants, including (for example) generation of a\nrandom number so large that the chances of collision\nare effectively zero.\n    oClients and proxies caching an object must store this\nversion string as part of the caching data.\n    o   To validate a cached object, a client or proxy presents\n    its version string to the server, along with the URL.\nThe server checks to see if the version string is\nstill valid for the URL.\n    o   Clients and proxies must not do anything with a version\nstring except store it and present it to the server for\nvalidity checking.\n\nA server must have a way of deciding if the version string is valid for\na given object, but the protocol specification need not concern itself\nwith how this is done.  There are several ways to do this, including\nusing file modification dates + \"epoch numbers\" (every time you do\nsomething major to the system clock, you increment the epoch number)\nor adjunct databases.  If a server is unable to generate useful version\nstrings, it can still conform to the spec but it must specify that\nthe objects it returns are uncachable.\n\nI know that some people will object that this makes life hard for\nservers implemented on underpowered machines.  Tough.  It really does\nnot take a lot of CPU cycles or storage to get this right.   If you\ncan't handle the load, buy a faster server.\n\nThis approach decouples cache validity checking and expiration, and so\nremoves any dependency on synchronized clocks from the cache validity\nprotocol.  What about expiration?\n\nThe issue with expiration is how to deal with clock skew.  We can\nprobably learn something from the work done on clock synchronization\nprotocols.  Here's a first cut at a proposal:\n\n(3) Servers may send Expiration information for cachable\nobjects.  (Expiration is not meaningful for noncachable\nobjects.)  Prior to the expiration time, a client or proxy\nneed not validate the cached object with the server, but\nMUST provide a means for the user to request validation.\n\nThe Expiration header looks like this:\n      Expiration-info: Expires-time server-current-time\nThe values are:\n    o   Expires-time\n    Time at which the object expires.\n    oserver-current-time\n    Server's current idea of what time it is\nClients and proxies would set the expiration time for a cached\nobject to the Expires-time value, after correcting for the\nworst-case error.  There are a number of ways to compute this,\nbut a simple technique would be something like this:\n    Tc = client's clock\n    Ts = server-current-time\n    Tr = round-trip-time measured for this request\n    Te = Expires-time\nthen\n    actual-expiration-time = Tc + (Te - Ts) - Tr/2\n\nClients and proxies MUST NOT cache any object whose computed\nactual-expiration-time is highly suspect (for example, Te < Ts\nor Te < Tc, or |Tc - Ts| >> Tr.)\n\nA server may send an \"Expires-time\" of \"infinity\", indicating\nthat the object will never change (although the user must\nstill be able to request that client/proxy validate the object,\njust in case).  The \"infinity\" value is not subject to bogosity\nchecking.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "realms, prompts, WWWAuthenticat",
            "content": "Last week there was some discussion about how to support multiple\nWWW-Authenticate (or equivalent) headers.  Here are some related\nquestions.\n\nLet's assume the server sends multiple WWW-Authenticate headers for a\nsingle resource.  (Or it could be some new header; you get the idea.)\n\n1) Can there be more than one such header that uses the same scheme\n    (e.g., Basic)?\n1a) If so, what does it mean for a resource to be protected in more\n    than one realm of the same authentication scheme?\n\n2) If the headers use more than one scheme, can (must?) the name of a realm\n    for one scheme be the same as the name for another?\n2a) If so, then either:\n    2a1) The user can't tell which one is being prompted for,\nbecause of the insistence (that I have been unable to dislodge)\nthat the prompt for a realm includes the name of the realm.  Or\n    2a2) It doesn't matter, because the name/password (at least for Basic\nand Digest) must be identical in both realms.\n\n3) Does the presence of multiple headers imply that a successful\n    authentication by any one of them is equally acceptable to the\n    server?\n\n4) Given multiple headers, how does the client choose a scheme and/or\n    realm for which to prompt the user?\n\nI'll toss in my standard bleat:  I think \"realm\" is overloaded, and the\nserver ought to be able to specify the realm-name, which the client can\nuse to distinguish protection domains, and a prompt, which the client\ncan use to prompt the user for whatever information is required.  In the\ncase of 2a1, it would be possible to have the same realm name used for\ndifferent authentication schemes; different prompts would enable the\nserver to get the client to ask for the right stuff.\n\nIf there's any intent to introduce another header for authentication,\nthat would be an ideal time to add the prompt I'm asking for.  The\ndefault prompt could be what it is now, the realm-name.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "A modest proposa",
            "content": "Well done!  You addressed my issues and then some.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "Jeffrey made an excellent proposal.  I think it's worth pursuing.\n\nFor the transition phase I still think that we should add Lou's SIZE\nparameter to I-M-S.  Most of the cache corruption is truncation due to\nthe fact that HTTP uses closing of connection as EOF and far too many\nimplementations leave truncation unnoticed (and it's not even possible\nto notice it if there is no C-L header).\n\nCheers,\n--\nAri Luotonenari@netscape.com\nNetscape Communications Corp.http://home.netscape.com/people/ari/\n501 East Middlefield Road\nMountain View, CA 94043, USANetscape Server Development Team\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": ">Hence this message.  I'm going to pretend that we are designing\n>a new protocol, not changing an existing one, to avoid confusing\n>the issue with debates about broken implementations or minimal\n>changes.  Until we can agree on what we want the protocol to do, it's\n>pointless to argue over how it does it.\n\nThat is not within our guidelines for 1.x.  I do not believe in\ncreating large entry barriers between minor protocol revisions,\nand HTTP versioning was designed to prevent it.\n\n>Let's assume that the HTTP 1.1 spec, and all subsequent ones,\n>explicitly state the tautology that \"an implementation not conforming\n>to this specification does not conform to this specification.\"  Any\n>\"broken\" browsers, servers, or clients will simply not be able to claim\n>conformance to HTTP 1.1, so it is not necessary to write that spec to\n>allow non-conforming implementations.  At the same time, the robustness\n>principle applies:  we should not write a spec that leads to a\n>\"fragile\" system.\n\nYep.  I claim that both are already true given the mechanisms we\nhave already discussed and which are within the scope of 1.1.\nThat includes:\n\n    Date:                (no change necessary)\n    Expires:             (no change necessary)\n    Last-Modified:       specify that LM > server time must not be given\n    If-Modified-Since:   specify that IMS > server time is invalid\n    Pragma:              add \"no-cache\", \"private\", and \"max-age\"\n    Content-MD5:         no problemo\n    Content-CRC:         no problemo\n    Transfer-Encoding:   chunked\n\n>So how would I design HTTP 1.1, if I were in charge?\n>\n>(1) Servers may send a header field explictly controlling\n>caching.  The spec could either be simple, something like\n>Dont-Cache-This-ever:\n>or a little more complicated:\n>Caching-allowed: [never | always | byClient | byProxy]\n>to allow for unanticipated future developments.\n\nPragma is already set up to do this, and more (the above does not\ninclude the functionality of max-age).\n\n>(2) Clients and proxies need a foolproof way to validate cache\n>entries.  \"If-modified-since\" seems to be of questionable\n>reliability.\n\nSolved.  IMS is only of questionable reliability when users are prevented\nfrom controlling their own cache, or when servers send out invalid data.\n\n>(3) Servers may send Expiration information for cachable\n>objects.  (Expiration is not meaningful for noncachable\n>objects.)  Prior to the expiration time, a client or proxy\n>need not validate the cached object with the server, but\n>MUST provide a means for the user to request validation.\n>\n>The Expiration header looks like this:\n>      Expiration-info: Expires-time server-current-time\n\nUnnecessary. That is why Date is given in responses.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "> \n> >But what is the difference between the Pragma: no-cache and Pragma: private?\n> \n> private would mean the response is intended for a single user agent\n> and thus must not be stored in a shared response cache.\n> \n> no-cache would mean the response must not be stored in any response cache.\nThis would introduce an incompatibility between HTTP/1.0 and HTTP/1.1!\nI suggest not touching the 1.0 semantics for no-cache pragma because it's\nimplemented in current applications, but we can add a new 'Pragma: dynamic' or\nsomething else with the same semantics.\nPros:\nThere is a demand for the semantics (or better shall I name it feature?),\ngenerated by - important in present and even more important in near future -\nshopping-basket applications.\nIt is simpler, than the alternate solution.\nCons: (regarding Pragma: dynamic)\nThis will change the general semantics of the pragmas. In 1.0 pragmas have\neffect only on intermediates. This isn't a big issue, because we have only\nthe pragma no-cache in 1.0. Leaving it untouched, we will not introduce\nincompatibility really, the incompatibility will be formal only.\nAlternate solution:\nThere is an unanswered (on this list at least) proposal by Shel Kaphan\nto introduce event-driven document expiration.\nImplementing evend-driven document expiration can be done in two ways:\na) outside the http protocol\nThis case shall be discussed not in this list, I presume, but see\nURL http://morse.colorado.edu/~wessels/Proxy/Thesis/newproxy.html by \nDuane Wessels on rcached.\nb) in http protocol\nWhile servers doesn't initiate connections to clients (we can safely assume,\nthat they never will - or firewall implementors will kill us), this can be done\nby enabling expiry notifications stuffed into response headers.\n(eg, in responses to POST-ing shopping actions for shopping basket apps),\nbut general-purpose expiry notifications stuffed into response header of\nunrelated URL-s will make caching proxies a lot more complicate.\n(proxies shall interpret the expiry notifications, forward to current client,\nand then store them independently of the currently requested URLs, and later\npropagate it in other response headers to clients and other proxies, possibly\nhaving stale copies.)\nI don't know, it is possible within http/1.x or not.\n\nAndrew.\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "> >Jeffrey Mogul writes:\n> >Hence this message.  I'm going to pretend that we are designing\n> >a new protocol, not changing an existing one, to avoid confusing\n> >the issue with debates about broken implementations or minimal\n> >changes.  Until we can agree on what we want the protocol to do, it's\n> >pointless to argue over how it does it.\n> \n> That is not within our guidelines for 1.x.  I do not believe in\n> creating large entry barriers between minor protocol revisions,\n> and HTTP versioning was designed to prevent it.\n> \n> >Let's assume that the HTTP 1.1 spec, and all subsequent ones,\n> >explicitly state the tautology that \"an implementation not conforming\n> >to this specification does not conform to this specification.\"  Any\n> >\"broken\" browsers, servers, or clients will simply not be able to claim\n> >conformance to HTTP 1.1, so it is not necessary to write that spec to\n> >allow non-conforming implementations.  At the same time, the robustness\n> >principle applies:  we should not write a spec that leads to a\n> >\"fragile\" system.\n> Roy T. Fielding's answer:\n> Yep.  I claim that both are already true given the mechanisms we\n> have already discussed and which are within the scope of 1.1.\n> That includes:\n> \n>     Date:                (no change necessary)\n>     Expires:             (no change necessary)\n>     Last-Modified:       specify that LM > server time must not be given\n>     If-Modified-Since:   specify that IMS > server time is invalid\n>     Pragma:              add \"no-cache\", \"private\", and \"max-age\"\n>     Content-MD5:         no problemo\n>     Content-CRC:         no problemo\n>     Transfer-Encoding:   chunked\n> ...\nIs the discussion on changing pragma no-cache semantics and introducing\npragma private for old no-cache semantics?\n\nI see Content-MD5, Content-CRC Transfer-Encoding: chunked first time. Maybe I\nshould subscribe to www-talk to be enough informed?\n\nIf the 1.1 spec defines response headers like\nContent-MD5, (Content-MD3 ? - see RFC1810 on MD5 performance)\nContent-CRC, (maybe Content-CRC16 and Content-CRC32 to have more options?\n      about this I'm less sure, than MD3)\nthen when somebody or some application is in doubt, can issue a HEAD request\nto get those headers, and re-check the cache content against them.\nI like this kind of extension!\n\nOnly MD5 can be stated as CPU-intensive, CRC and MD3 can be computed rapidly\nin case, when the origin server is in doubt about cached values, and can be\nsupplied to the server as meta-data, if they are stored separately from the\ndocument. (I mean the technical impossibility of in-lineing \nlike <META HTTP-EQUIV=\"Pragma\" CONTENT=\"no-cache\"> in the HEAD element.\nNot fully true for CRCs.)\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "> For the transition phase I still think that we should add Lou's SIZE\n> parameter to I-M-S.  Most of the cache corruption is truncation due to\n> the fact that HTTP uses closing of connection as EOF and far too many\n> implementations leave truncation unnoticed (and it's not even possible\n> to notice it if there is no C-L header).\n\nWhy is getting someone to pay attention to 'size' easier than getting\nthem not to cache truncated data or data without C-L?\n\nIf the data didn't have a C-L in the first place, how would the server\nbe able to check the length against a I-M-S parameter?\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "I'm sorry, I just realised that I'm completely wrong in assumption\nabout no-cache pragma in responses is in current practice.\nI erroneusly assumed, that no-cache pragma work while really the absence of\nLast-Modified header prevented caching.\nAndrew (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\nP.S.: now I go offline till 28-th Aug for small vacation.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Aug 17, 13:10, Roy Fielding wrote:\n>\n> >But what is the difference between the Pragma: no-cache and Pragma:\n> >private?\n>\n> private would mean the response is intended for a single user agent\n> and thus must not be stored in a shared response cache.\n>\n> no-cache would mean the response must not be stored in any response\n> cache.\n\nRoy,\n\nPlease correct me if I miss your point... What do we gain by having both a\n'no-cache' and a 'private' Pragma in terms of functionality ? A 'shared\nresponse cache' is basically a proxy/cache, and Pragma is meaningful to\nproxies only, not to user agents. So both headers really mean \"don't cache\nthis response in a proxy/cache\", and both let a user-agent local cache free\nto cache the response or not.\n\nWhat's the point of adding 'private' then ? I would understand it if\n'non-shared response proxy/caches' were a reality, but AFAIK, they are not !\n\nI think both cases you described above should use 'Pragma: no-cache'.\n\nJean-Philippe\n\n\n\n"
        },
        {
            "subject": "Proposal: Pragma minage (Was:Re: A modest proposal",
            "content": "Jeffrey Mogul:\n>Time for a Principle:\n>    Principle #1 of caching:\n>Caches should never introduce undetectable incorrectness.\n>Correctness always takes precedence over performance.\n>\n\nAs discussed earlier on www-talk, if a sufficient number of service authors\nkeeps putting Expires: <yesterday> or Pragma: no-cache headers in responses\nfor frivolous reasons, a web cache administrator may want to (selectivelty)\n`tune' the cache to ignore these headers (even if this means that the cache\ndoes not conform to http 1.x anymore).\n\nSuch a `tuned' cache would introduce incorrectness, so a corollary of the\nabove principle is:\n\n If a cache does not honor requests _not_ to cache as in http 1.x, there\n should be a way for _origin servers_ to detect it, so that they can refuse\n to serve dynamic documents through these caches (or embed big warning\n messages in these documents).\n\nSo we could introduce yet another pragma:\n\nPragma: min-age=<delta-seconds>\n\n`Tuned' caches, either in user agents or in proxies, should add\nthis header to the _request_ headers.  Semantics of the header:\n\n- if it is absent: the cache promises to always honor the http 1.x response\n  headers related to cache control (Expires and Pragma in 1.0 and 1.1).\n\n- if it is present:  the cache does only guarantee to honor the http 1.x\n  response headers related to cache control (Expires and Pragma in 1.0 and\n  1.1) _after_ <delta-seconds> seconds after the end of the http\n  connection.  In the time before that, the cache may choose to serve\n  the cached entiry without prior validation with the origin server.\n\nMaybe a special case, \n\nPragma: min-age=session\n\nis also needed to allow user agents to express that an entity is cached\n(without ever sending IMS or head requests to the origin server) until the\nend of the session (where the exact meaning of `session' is left undefined).\n\nAdding Pragma: min-age to a http 1.x spec would make `tuning' legal under\nhttp 1.x.  I think any http spec should strongly discourage tuning.\n\nTuning is with us now: if it has disappeared in, say, 2 years, the http x.y\nspec written in 1997 could make Pragma: min-age an obsolete header.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Jean-Philippe Martin-Flatin:\n> What do we gain by having both a\n>'no-cache' and a 'private' Pragma in terms of functionality ? A 'shared\n>response cache' is basically a proxy/cache, and Pragma is meaningful to\n>proxies only, not to user agents. So both headers really mean \"don't cache\n>this response in a proxy/cache\", and both let a user-agent local cache free\n>to cache the response or not.\n\nPragma: private would instruct a user agent not to cache the response if its\ncache memory (say part of a harddisk in an MS-DOS pc in a university PC lab)\nis publicly accessible.  This is particularly important for user agents that\ndo not clear their caches at the end of the session.\n\nOf course, the user agent needs to be configured to know that its cache is\npublicly accessible, one could have a configuration option like `cache\nprivate responses on local disk?'.\n\n>Jean-Philippe\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Aug 18, 12:21, Koen Holtman wrote:\n>\n> Jean-Philippe Martin-Flatin:\n> >What do we gain by having both a 'no-cache' and a 'private' Pragma\n> >in terms of functionality ? A 'shared response cache' is basically\n> >a proxy/cache, and Pragma is meaningful to proxies only, not to user\n> >agents. So both headers really mean \"don't cache this response in a\n> >proxy/cache\", and both let a user-agent local cache free to cache\n> >the response or not.\n>\n> Pragma: private would instruct a user agent not to cache the response\n> if its cache memory (say part of a harddisk in an MS-DOS pc in a\n> university PC lab) is publicly accessible.  This is particularly\n> important for user agents that do not clear their caches at the end\n> of the session.\n\nPragma is transparent to a user agent. From\n<URL:http://www.w3.org/hypertext/WWW/Protocols/HTTP1.0/draft-ietf-http-spec.html#Pragma>:\n\n    Pragma directives do not apply to the end-points of a request/response\n    chain. For example, a user agent's internal (non-shared) cache and/or\n    history mechanism should ignore all pragma directives in received\n    messages. Similarly, pragma directives are not applicable to the origin\n    of a resource, though they may be applicable to a server's internal\n    response cache.\n\nYou have a point though, and maybe there ought to be another header telling\nthe user agent that this information is private, and that it should make all\npossible efforts (very platform specific) to keep it private. But that\nwouldn't be achieved with a Pragma header field.\n\nJean-Philippe\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Aug 16, 15:07, Roy Fielding wrote:\n>\n> >[Lou Montulli wrote:]\n> >\n> >I recently changed netscape to interpret \"Pragma: no-cache\" and not\n> >cache the object.  This is slightly different than a \"Expires\" header\n> >because the object will not even be cached for history navigation.\n> >(Documents that are expired are still shown when traversing the session\n> >history).  Haveing the client interpret \"Pragma: no-cache\" lets servers\n> >tell the client that this information is highly sensitive or volitile\n> >and should not be cached in any way.\n>\n> [deleted]\n>\n> What I wrote in Draft 01 for the Pragma field's abstract semantics\n> may be wrong.  It is based on my interpretation of the meaning of\n> the current \"no-cache\" request header, and I merely extended that\n> semantics to the other directives.\n>\n>    Pragma directives must be passed through by a proxy, regardless of\n>    their significance to that proxy, since the directives may be\n>    applicable to all intermediaries along the request/response chain.\n>    It is not possible to specify a pragma for a specific proxy;\n>    however, any pragma directive not relevant to a proxy should be\n>    ignored.\n>\n> Instead of \"intermediaries\", we could say \"recipients\"\n\nInstead of \"recipients\", I would prefer \"proxies\". \"Recipients\", which isn't\ndefined in section 1.3 of draft-ietf-http-spec.html, encompasses the proxies\nand the user agent in my view, whereas the Pragma header field is not\ndestined for the user agent.\n\n> and the following paragraph:\n>\n>    Pragma directives do not apply to the end-points of a\n>    request/response chain. For example, a user agent's internal (non-\n>    shared) cache and/or history mechanism should ignore all pragma\n>    directives in received messages. Similarly, pragma directives are\n>    not applicable to the origin of a resource, though they may be\n>    applicable to a server's internal response cache.\n>\n> could be replaced with\n>\n>    Pragma directives only apply to recipients that implement features\n>    corresponding to the directive's semantics.  For example, a no-cache\n>    directive tells the recipient not to make use of its caching mechanism\n>    in satisfying the request when it occurs in a request header, or in\n>    storing the response when it occurs in a response.  Pragma directives\n>    are also unidirectional in that the presence of a directive in a\n>    request does not imply that the same directive be given in the response.\n\nThis somewhat changes the scope of Pragma, which is no longer restricted to\nproxies in the new paragraph.\n\nI think the name \"Pragma\" confuses a number of people (cf Lou's initial\ninterpretation), and it's not clear to all whether a Pragma header field is\ndestined for proxies only, or for proxies and user agents. Based on the fact\nthat not so many WWW clients and servers already implement Pragma, I propose\nthat we replace the name \"Pragma\" with the name \"Proxy\". Maybe in HTTP/1.1\nspecs ? It would be crystal clear that 'Proxy: no-cache' is not destined for\nuser agents.\n\nJean-Philippe\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "> This somewhat changes the scope of Pragma, which is no longer restricted to\n> proxies in the new paragraph.\n> \n> I think the name \"Pragma\" confuses a number of people (cf Lou's initial\n> interpretation), and it's not clear to all whether a Pragma header field is\n> destined for proxies only, or for proxies and user agents. Based on the fact\n> that not so many WWW clients and servers already implement Pragma, I propose\n> that we replace the name \"Pragma\" with the name \"Proxy\". Maybe in HTTP/1.1\n> specs ? It would be crystal clear that 'Proxy: no-cache' is not destined for\n> user agents.\n\nWhen I say the \"Pragma\" header orginially, my first throught was #pragma\nin ANSI C: an escape hatch to provide implementation-specific information.\n\nNow, this was added originally for the benifit of the CERN proxy server,\nand the language of the specs reflect this, but I'm not sure we need\nto stick with this if we can be clear what the scope of particular\nPragmas is... the nature of this sort of thing is that implementations\nwill (and usually should) ignore/discard unknown pragmas.\n\nI _would_ like us to use some kind of current-practice or best-current-practice\ninterpretation of Pragma: no-cache and If-Modified-Since: rather\nthan redefining them in some more complicated/tricky way: thus I\ntend to favor the suggestions that add new headers/pragmas to offer\nnew semantics.\n\n(Should any of this discussion go to www-proxy or is that a dead list?)\n\n\n\n-- \n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "TransferEncoding:  questions not answere",
            "content": "Scanning some back email, I found these questions that I had posed about\nTransfer-Encoding, but that I don't think got answered.\n\n1)  Should the [HTTP/1.2] specification state when \"chunked\" must and\nmust not be used?  Although there's no consensus session/keepalive\nproposal, obviously any content sent from the server to the client for\na held-open connection must either have a Content-Length or be\nchunked for the connection to remain open.\n\n2) Can a client send chunked content in a POST in lieu of a\nContent-Length, or even with a C-L?  (And what does it mean to have\nboth, especially if they disagree?)\n\n3) If (2) is true, does the CGI interface change to require a CGI script\nto interpret T-E, or does the interface stay the same, and the server\nprocesses the chunked content and passes the concatenated chunks to the\nCGI?  If the latter, is it valid for the server to forge a Content-Length\nheader for the CGI to use to read the concatenated content where none\nexisted previously?\n[As Larry Masinter pointed out, the CGI interface is outside the scope of\nthis list, but changes to HTTP can affect it.]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "planned obsolescenc",
            "content": "Anyone who has maintained large old software systems knows how hard it\nis to support lots of old versions compatibly.  (I've supported C\ncompilers that still allowed the =op form of assignment operator....) I\nwould like to propose that the HTTP spec. include a time limit, as well\nas a versioning limit.\n\nSection 3.1 of v10-spec-01 describes the rules for dealing with major\nreleases, and which old ones must be supported.  I would like to add a\nstatement that also sets a time limit for supporting antique versions.\nFor example, the HTTP/1.1 specification might say that after January 1,\n1997, conforming WWW software need not support HTTP/0.9 transactions.\nVendors would be free to continue to support legacy systems, but they\nwould not be required to do so to remain conforming.\n\nSoftware accretions imperil the health of any software system.\nOccasionally scraping them away will reduce size and complexity and\nincrease speed.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": ">Pragma is transparent to a user agent. From\n><URL:http://www.w3.org/hypertext/WWW/Protocols/HTTP1.0/draft-ietf-http-spec.html#Pragma>:\n\nThat is a DRAFT\n\nAt the start of this thread, I said that I would change that paragraph.\nI am still going to change that paragraph.  The fact that it contradicts\nour use of Pragma: on response messages is *why* I am changing that paragraph.\n\nRegarding the name \"Pragma\"\n\n   Yes, it is a bad choice for a protocol element name.\n\nWe have two choices:\n\n   1) Change the name to something relevant, e.g., \"Caching\"\n\n   2) Continue using the same name and simply define the semantics\n      such that it means what we say it means.\n\nThe first choice may look cleaner, but it neglects the fact that Pragma\nis already in use, already recognized (and forwarded) by proxies,\nand already has the de-facto semantics that we need.\n\nSo, what do we do for HTTP/1.1?  I am planning on issuing the first 1.1\ndraft on Monday, so I'd like to hear your opinions now rather than later.\n\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: TransferEncoding: questions not answere",
            "content": ">Scanning some back email, I found these questions that I had posed about\n>Transfer-Encoding, but that I don't think got answered.\n\nI don't like answering questions -- questioning answers is much easier.  ;-)\n\n>1)  Should the [HTTP/1.2] specification state when \"chunked\" must and\n>must not be used?  Although there's no consensus session/keepalive\n>proposal, obviously any content sent from the server to the client for\n>a held-open connection must either have a Content-Length or be\n>chunked for the connection to remain open.\n\nIt will state when a length (chunked or CL) must be given, yes.\n\n>2) Can a client send chunked content in a POST in lieu of a\n>Content-Length, or even with a C-L?  (And what does it mean to have\n>both, especially if they disagree?)\n\nOnly when talking to a server that it knows will support chunked messages.\nIf both are given, CL is overridden.\n\n>3) If (2) is true, does the CGI interface change to require a CGI script\n>to interpret T-E, or does the interface stay the same, and the server\n>processes the chunked content and passes the concatenated chunks to the\n>CGI?  If the latter, is it valid for the server to forge a Content-Length\n>header for the CGI to use to read the concatenated content where none\n>existed previously?\n>[As Larry Masinter pointed out, the CGI interface is outside the scope of\n>this list, but changes to HTTP can affect it.]\n\nCGI is outside the scope of this list, period.  CGI will have to be updated\nno matter what changes are made to HTTP, but that update is an issue for\nserver developers and the WWW.  CGI is not an IETF protocol; it is a\nserver-side API.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "On Aug 18, 13:04, Roy Fielding wrote:\n>\n> Regarding the name \"Pragma\"\n>\n>    Yes, it is a bad choice for a protocol element name.\n>\n> We have two choices:\n>\n>    1) Change the name to something relevant, e.g., \"Caching\"\n>\n>    2) Continue using the same name and simply define the semantics\n>       such that it means what we say it means.\n\nI vote for 1), 2) has proved too confusing already. The name will depend on\nwhat you want to achieve:\n\n1) If you want only proxies not to cache, I vote for:\n         Proxy: no-cache\n  This leaves scope for other proxy-specific directives to be defined\n  at a later stage.\n\n2) If you want both proxies and user agents not to cache, you can either do:\n         Caching: all=no\n         Caching: proxy=no\n         Caching: user-agent=no\n  or\n         Proxy: no-cache\n         Expires: now\n\nTo choose between these 2, let me quote Shel Kaphan who summarized very well\nthe 'Pragma: no-cache' vs 'Expires: now' debate on the 16th:\n\n>>The reason Pragma: no-cache is not a redundant way of saying Expires:\n>><= now is that it allows servers to send essentially \"private\"\n>>documents to particular clients where these documents need not expire\n>>immediately.  If intermediate proxies cannot cache the document (due\n>>to Pragma: no-cache) but clients can, this makes for more flexibility\n>>at the client end.  The client would not be required to issue a new\n>>HTTP request when revisiting the non-expired document.  Since this\n>>appears to be the only added functionality of Pragma: no-cache as a\n>>return header...\n\nThus I think there's no need for the 'Caching' solution, and vote for the\n'Proxy: no-cache' solution as a replacement for 'Pragma: no-cache'.\n\n> The first choice may look cleaner, but it neglects the fact that Pragma\n> is already in use, already recognized (and forwarded) by proxies,\n> and already has the de-facto semantics that we need.\n\nFrom my reading of the different WWW newsgroups this year, I got the opposite\nimpression: almost nobody has a WWW client or server which uses Pragma. This\nis not based on solid stats, though.\n\n> So, what do we do for HTTP/1.1?  I am planning on issuing the first 1.1\n> draft on Monday, so I'd like to hear your opinions now rather than later.\n\nLeaves little time for this side of the Atlantic to react ;-)\n\nJean-Philippe\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Pragma minage (Was:Re: A modest proposal",
            "content": ">As discussed earlier on www-talk, if a sufficient number of service authors\n>keeps putting Expires: <yesterday> or Pragma: no-cache headers in responses\n>for frivolous reasons, a web cache administrator may want to (selectivelty)\n>`tune' the cache to ignore these headers (even if this means that the cache\n>does not conform to http 1.x anymore).\n>\n>Such a `tuned' cache would introduce incorrectness\n\nNo, it *might* introduce incorrectness.  I claim that over 99% of\ntoday's non-cacheable pages are still \"correct\" after being cached,\nwhere correctness is defined as containing the same substantive and\nqualitative information content as would be obtained directly from\nthe origin.  [Note: my claim is based on personal observation, not\ncontrolled experimentation]\n\nThat is why a cache administrator is willing to \"tune\" the cache,\nand it is not something that can be fixed within the protocol.\nCaches will do what providers want them to do when providers stop\nmarking cacheable pages as non-cacheable.\n\n>Pragma: min-age=<delta-seconds>\n\nI am opposed to this change.  First, any sensible cache administrator\nwould never send such a header, since it effectively changes the request\nprofile and therby lowers the quality of the response.\n\nSecond, the cache is tuned based on each individual response, not based\non every request.  If the response looks like it shouldn't be cached,\na tuned cache will not cache it.  If it says \"no-cache\" and is coming\nfrom a reputable provider, a tuned cache will not cache it.  If it\ncomes from a notoriously wasteful provider, the cache will cache it\nand there is no incentive whatsoever for the cache administrator to\nwarn the wasteful provider ahead of time.\n\nBTW folks, lack of a Last-Modified date does not imply no-cache.\nThat is only an optimization used by Ari's two proxies and Navigator;\nit does not hold true in general.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: realms, prompts, WWWAuthenticat",
            "content": ">Last week there was some discussion about how to support multiple\n>WWW-Authenticate (or equivalent) headers.  Here are some related\n>questions.\n>\n>Let's assume the server sends multiple WWW-Authenticate headers for a\n>single resource.  (Or it could be some new header; you get the idea.)\n>\n>1) Can there be more than one such header that uses the same scheme\n>    (e.g., Basic)?\n\nYes.\n\n>1a) If so, what does it mean for a resource to be protected in more\n>    than one realm of the same authentication scheme?\n\nIt means the user may be authenticated by one of the authorization\ndatabases corresponding to those realms.\n\n>2) If the headers use more than one scheme, can (must?) the name of a realm\n>    for one scheme be the same as the name for another?\n\nNope.\n\n>3) Does the presence of multiple headers imply that a successful\n>    authentication by any one of them is equally acceptable to the\n>    server?\n\nYes, assuming that the user is capable of being authorized by\nat least one.  However, Basic will be deprecated (and thus \"less good\")\nif there are any other alternatives.\n\n>4) Given multiple headers, how does the client choose a scheme and/or\n>    realm for which to prompt the user?\n\nThat would be up to the browser.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: planned obsolescenc",
            "content": "Dave Kristol wrote:\n[...]\n> Section 3.1 of v10-spec-01 describes the rules for dealing with major\n> releases, and which old ones must be supported.  I would like to add a\n> statement that also sets a time limit for supporting antique versions.\n> For example, the HTTP/1.1 specification might say that after January 1,\n> 1997, conforming WWW software need not support HTTP/0.9 transactions.\n> \n[...]\nNot supporting HTTP/0.9 would be a mistake.  It's a very useful\nprotocol but not every servers needs.  Server can advertise the\nversion they support.  0.9 may be \"antique\" but about half the\ntransactions hitting my server are for 0.9.  For our server\nI don't expect HTTP/1.2 to be as useful as HTTP/1.0 based on\nthe recommendations being made.\n\nbob jernigan\n\nMaybe HTTP/1.2 = HSBP/1.0 (HyperShoppers Basket Protocol)\n\n\n\n"
        },
        {
            "subject": "Re: realms, prompts, WWWAuthenticat",
            "content": "Another question about multiple WWW-Authenticate response headers:\n\nWouldn't it make sense for the client to tell the server for which\nrealm the client is sending authentication information?  The current\nAuthorization header has no provision for that information.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: 411 response code: clarification, pleas",
            "content": "At 01:50 PM 8/17/95 -0400, Roy Fielding writes:\n>On the other hand, we could just use 401 for both, but I was told\n>earlier (on the list) that the 411 semantics were needed.\n\nCould you summarize why 411 is needed?  If I'm reading draft 01 right,\nanytime someone mistypes their username or password for Basic authentication\nthe server should send back 411.  I know this change will break at least\nsome of the existing web browsers, since they look for 401 to trigger their\nauthentication code.  There should be a very compelling reason to make a\nmajor change from current practice, especially since current practice is so\nwidely implemented.\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "Re: 411 response code: clarification, pleas",
            "content": ">Could you summarize why 411 is needed?  If I'm reading draft 01 right,\n>anytime someone mistypes their username or password for Basic authentication\n>the server should send back 411.  I know this change will break at least\n>some of the existing web browsers, since they look for 401 to trigger their\n>authentication code.  There should be a very compelling reason to make a\n>major change from current practice, especially since current practice is so\n>widely implemented.\n\nWell, here's the problem:\n\n   User requests a protected URI\n                                    Server returns 401 Unauthorized\n   User Agent presents pop-up dialog\n   User enters authentication info\n   (or just OKs a stored credential)\n   UA repeats request using credentials\n                                    Server interprets credentials, and\n                                    finds them lacking\n                                    Server returns 401 Unauthorized\n   User Agent presents pop-up dialog\n   User enters authentication info\n   (or just OKs a stored credential)\n   UA repeats request using credentials\n                                    Server interprets credentials, and\n                                    finds them lacking\n                                    Server returns 401 Unauthorized\n   ...\n\nThe problem is that the User Agent is not showing the user why\nthe authentication failed.  This is not a problem for Basic AA\nbecause it is a simple user:passwd scheme, and it is probably\nbest that the user not know why it failed.\n\nHowever, consider the case of something like Mediated Digest AA,\nthe failure may be do to any number of reasons -- some of which\nmay be correctable by the user outside the scope of this user agent.\nThe user agent needs to know that it must show the user the entity\nenclosed with the response.\n\nSo, the question is, do we want to require that a 401 response\nbe shown to the user if Authorization for the indicated scheme+realm\nhas already been tried and failed?  If yes, then we don't need 411.\nIf no, then we will need 411.\n\nBTW, it was added because of a prior thread on this list regarding\nsomeone's desire to say \"authorization refused\" AND explain what\nthe user must to do to get authorization.  Existing user agents made\nthat impossible because they never show the contents of 401.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: realms, prompts, WWWAuthenticat",
            "content": ">Wouldn't it make sense for the client to tell the server for which\n>realm the client is sending authentication information?  The current\n>Authorization header has no provision for that information.\n\nThat is only true for Basic AA.  All others can use parameters, and\nany scheme could easily require that realm be sent in one form or\nanother.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Koen Holtman writes:\n > Jean-Philippe Martin-Flatin:\n > > What do we gain by having both a\n > >'no-cache' and a 'private' Pragma in terms of functionality ? A 'shared\n > >response cache' is basically a proxy/cache, and Pragma is meaningful to\n > >proxies only, not to user agents. So both headers really mean \"don't cache\n > >this response in a proxy/cache\", and both let a user-agent local cache free\n > >to cache the response or not.\n > \n > Pragma: private would instruct a user agent not to cache the response if its\n > cache memory (say part of a harddisk in an MS-DOS pc in a university PC lab)\n > is publicly accessible.  This is particularly important for user agents that\n > do not clear their caches at the end of the session.\n > \n > Of course, the user agent needs to be configured to know that its cache is\n > publicly accessible, one could have a configuration option like `cache\n > private responses on local disk?'.\n > \n > >Jean-Philippe\n > \n > Koen.\n\nThis explanation is a little different from others floating around.\nThe other interpretation is that Private means that no other\nuser-agent should be allowed to access a cached copy of this document,\nwhich would presumably mean that it should not be cached in any\nintermediate proxy (except possibly for use by the originally\nrequesting user-agent, but that's too hard and not very useful).\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "HTTP/1.2 stuff:  try it out",
            "content": "I've put up a server that (I think) understands HTTP/1.2 features\nkeepalive and Transfer-Encoding.  (Because I haven't seen an official\ncopy of the forthcoming draft, I can't be sure.)  It's at\nhttp://www.research.att.com:8000/\nand serves content identical to the normal www.research.att.com server.\n\nHere's what you can try:\n\n1) If there's a Connection: keepalive request header, the server will\nhold the connection open for 10 seconds.\n\n2) If\n- there's a Connection: keepalive request header,\n- the protocol version in the Full Request line is HTTP/1.2, and\n- the request is a GET on a CGI\nthe server will use Transfer-Encoding: chunked to send the output.  Try\nhttp://www.research.att.com:8000/cgi-bin/test-report\n\n3) The URL http://www.research.att.com:8000/digest-test refers to a\nresource that is protected by Digest authentication.  The user is\n\"protected\".  Password is \"try-me-out\".  The nonce expires after five\nminutes.\n\nTry it out.  Bug reports are welcome.\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "Balint Nagy Endre writes:\n...\n > There is an unanswered (on this list at least) proposal by Shel Kaphan\n > to introduce event-driven document expiration.\n\nFor the record, the message I gave that title to was (if memory serves)\nreally about the fact that I wanted there to be a response header that\nidentified the resource, so that caches would have a unique cache key\nfor a resource.  I got what I wanted: Location can now be used for\n2xx responses.  What I didn't get (unless this has changed since last\nreading) was language in the spec describing the impact on caches of\nthis header for 2xx responses.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: 411 response code: clarification, pleas",
            "content": "On Fri, 18 Aug 1995, Jim Seidman wrote:\n\n> Could you summarize why 411 is needed?  If I'm reading draft 01 right,\n> anytime someone mistypes their username or password for Basic authentication\n> the server should send back 411.  I know this change will break at least\n> some of the existing web browsers, since they look for 401 to trigger their\n> authentication code.  There should be a very compelling reason to make a\n> major change from current practice, especially since current practice is so\n> widely implemented.\n\nWell, it seems to me that, thanks to the change in protocal version to \n1.1, this will in fact not break existing browsers. Simply: If the \nrequest message indicates HTTP/1.0, send 401, if it says HTTP/1.1 or \nlater, send 411.\n\nI believe this is consitent with the spec, and seems to work. In fact, \nmost of the new features outlined in HTTP 1.1 and 1.2 can work this way.\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: Expires, Last-Modified, Pragma: nocache etc",
            "content": "At 1:04 PM 8/18/95, Roy Fielding wrote:\n>Regarding the name \"Pragma\"\n>\n>   Yes, it is a bad choice for a protocol element name.\n>\n>We have two choices:\n>\n>   1) Change the name to something relevant, e.g., \"Caching\"\n>\n>   2) Continue using the same name and simply define the semantics\n>      such that it means what we say it means.\n>\n>The first choice may look cleaner, but it neglects the fact that Pragma\n>is already in use, already recognized (and forwarded) by proxies,\n>and already has the de-facto semantics that we need.\n\nI like something option (2)\n\nKeep the behavior and name of \"Pragma: no-cache the same\", and document new\nsemantics for other pragmas or pragmas in general (and/or add another\nheader as needed.)\n\nAre there any other widely implemented Pragma: headers?\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "...\n >     Date:                (no change necessary)\n >     Expires:             (no change necessary)\n >     Last-Modified:       specify that LM > server time must not be given\n ^^^^^^^^^^^^^^^^\n >     If-Modified-Since:   specify that IMS > server time is invalid\n ^^^^^^^^^^^^^^^^^\n >     Pragma:              add \"no-cache\", \"private\", and \"max-age\"\n >     Content-MD5:         no problemo\n >     Content-CRC:         no problemo\n >     Transfer-Encoding:   chunked\n...\n > >\n > >The Expiration header looks like this:\n > >      Expiration-info: Expires-time server-current-time\n > \n > Unnecessary. That is why Date is given in responses.\n^^^^^^^^^^^^^^^^^^^^^^^^^    \n > \n\nThese comments, and some in Jeff Mogul's thought-experiment, made me\nrealize that the \"argument\" about discarding \"future\" Last-Modified's and\nnot caching the accompanying documents is somewhat of a red herring if\nthe implementation is \"good\" (where I get to define \"good\" :)).\n\nPRINCIPLE:\nIt is not possible to reliably compare times from two different,\nunsynchronized, clocks. So, don't do it.  (Not naively, anyway).\n\nRegarding Last-Modifieds \"in the future\", Last-Modified should be\nignored if the timestamp is later than the accompanying Date response\nheader.  Then it is certainly invalid.  But it can't be reliably\ncompared to the local clock without appropriate adjustment.  So,\nsubject to the following being adopted I'll agree with Larry\nMasinter's rules.\n\nThe following is a proposal, so when I say \"should\" here, I mean in the\ncontext of this proposal, ok?\n\nIf a server sends an expires header that is 30 seconds in the future,\nit means the future as it knows it.  So, when a downstream server\nreceives Expires, it should interpret that in local time using (some\nfunction of) the delta between the accompanying Date header and\nits own clock.\n\nA cache should keep track of the delta between a server's clock (as\nreturned by Date) and its own clock, and include (some function of)\nthat delta in any communication with that server involving timestamps.\n\nThere is some complication with GET-IMS requests being passed along\nfrom downstream servers.  An intermediate proxy cannot tell whether\nthe date in an IMS request is given in the upstream server's\ncoordinates (e.g. an upstream LM header being returned in an IMS\nrequest) or in the downstream server's coordinates (e.g. \"get me\nsomething if it has changed in the past 15 minutes\").\n\nWhen proxies pass through requests or responses including timestamps\nsuch as GET if-modified-since, last-modified, and expires, I propose\nthat they should normalize the times to their own clocks, using an\nunspecified algorithm they determine, which is a function of the delta\nbetween local time and the Date header passed by the server, and may\ninclude information on round-trip delays accumulated over time.  This\nway, for instance, a downstream server's GET-IMS request or an\nupstream server's Expires header can be translated to be in local\ntime system.\n\nSo, here is a PROPOSAL:\n\nFor this reason, I propose that whenever requests or responses\ncontaining these timestamps are generated, that a Date header should be\nREQUIRED, and should indicate the moment the request or response \nis transmitted.\n\nExample: \n\nAssume 0 delays everywhere for simplicity.\nIn this example the RECEIVERs do all time adjustments in both\ndirections.  That is not the only way this could work.\n\nClient CProxy PServer S\n12:0012:0111:59\n---------------\nGET -->--> GET -->Process GET,\nissue headers\nDate: 11:59\nlast-modified: 10:00\n<--\nReceive Date 11:59\nDelta for server S = \n12:01 - 11:59 = +2 minutes\nrewrite headers:\nDate:12:01\nLast-modified: 10:02\n<--\nReceive Date 12:01\nDelta for Proxy P = \n12:00 - 12:01 = -1 minute\ninterpret headers:\ndate: 12:00\nlast-modified:10:01\n\n12:0512:0612:04\n---------------\nGET IMS 10:01 -->\nReceive Date:12:05\nTranslate IMS header\nUsing +1 minute delta,\ntranslate IMS header to\n10:02, \nGET IMS 10:02-->Receive Date: 12:06.\nUse -2 minute delta to\ninterpret IMS header\nas GET IMS 10:00.\n\n\nOne problem with this is that due to inevitable inaccuracies in the\ndelta computations, an IMS request might differ from the LM that it\ncorresponds to.  So, in the long term, some cookie other than\na timestamp is a better way to go for such return-trip operations,\nas both Jeff Mogul and I previously suggested. \n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "> PRINCIPLE:\n> It is not possible to reliably compare times from two different,\n> unsynchronized, clocks. So, don't do it.  (Not naively, anyway).\n\nI just don't think this applies. The 'comparison' I was suggesting was\nnot an equal, but a 'greater' comparison. For the most part, clock\ndrift of seconds or even hours doesn't affect it.\n\nAnd, secondly, I believe it is reasonable at some point to expect HTTP\nclients and servers and the proxies in between to have correct clocks.\nWe're talking about Internet applications now, for machines that are\nwell connected on the net.\n\nMaybe it would simplify things if HTTP servers and proxies also\noffered a time service, and HTTP clients on impoverished platforms had\nan option for setting the time?\n\nI'd handle this using well-known URLs for 'get' rather than some\nspecial protocol, though.\n\n\n\n"
        },
        {
            "subject": "A modest proposal, made more specifi",
            "content": "Larry Masinter writes:\n    And, secondly, I believe it is reasonable at some point to expect HTTP\n    clients and servers and the proxies in between to have correct clocks.\n    We're talking about Internet applications now, for machines that are\n    well connected on the net.\nand suggests that HTTP servers offer time service.    \n\nUgh.  While it is perhaps reasonable to expect HTTP servers and\nclients to have correct clocks, for some definition of \"correct\"\n(talk to Professor Einstein about this), I do not think it is\n*necessary* to rely on this when designing the protocol.  Moreover,\nI would not confuse \"reasonable to expect\" with \"safe to expect\".\n\nShel Kaphan suggests that clients use the \"Date\" information to run\na sort of NTP-like clock synchronization algorithm, keeping\ntrack of each server's relative clock performance.\n\nDouble ugh.  This is a lot of hard work, and it's already done\nby NTP.  And it's not all that useful.\n\nWe're still confused about two uses of timestamps: (1) cache validation\nand (2) expiration.\n\nI think Shel is close to the truth when he writes\n    It is not possible to reliably compare times from two different,\n    unsynchronized, clocks. So, don't do it.\nBut let me try to clarify things a bit more.\n\n(1) For the purpose of cache validation (\"is the cached copy I have\nvalid or not\"), it should not be necessary to do ANYTHING besides\na strict equality comparison.  The cached copy is either the same\nas the server's copy, or it isn't.  If we are using last-modified\ntimes as the validation ID, then either they are equal or they\naren't.  I fail to see any value in allowing a server to return\n304 (not modified) if it's idea of the modification time is prior\n(that is, not exactly equal to) the client's (or proxy's) stored\nmodification time.\n\n[I'll note that all the allowed date formats have one-second\nresolution.  This could be a problem in the future, if things are\nchanging faster than once a second, and browsers can do something\nuseful with this (but this is speculative, I admit).]\n\n(2) For expiration checking, it is obviously necessary to do inequality\ncomparisons (\"is expiration time > now?\").  In this case, though,\nwe don't need to be 100% accurate, since in almost all cases I can\nthink of, when someone assigns an expiration date, it's at best a\nguess, anyway.  Doing expiration checking does not require carefully\nsynchronized clocks; it only requires sufficient sanity checking that\nbadly out-of-whack clocks are not believed.  I think the algorithm\nI suggested in my previous message should work fine, noting Roy's\ncomment that the Date header provides the necessary sanity-checking\ninfo.  (And so HTTP 1.1 should make Date mandatory if Expires is\nsent, I think.)\n\nFor both cache-validation and expiration, I believe that the algorithms\nused by clients and proxies should be identical.  That is, I see no\nreason why a client should cache something that a proxy isn't allowed\nto (except as explicitly instructed by the \"Caching-allowed:\" header\nor whatever we're calling that).  And there can't be any reason to\nallow a proxy to cache something that a client is not allowed to.\n\nSo far, what I've suggested in this message does not change the\nsyntax of the protocol; I'm suggesting changes in what servers,\nclients, and proxies do with the headers we already have.  This\nmeans that (so far) everything I've suggested in this message\nshould interoperate with all reasonable implementations.  (Clearly,\na server sending entirely random values for last-modified, for example,\nisn't entirely reasonable.)\n\nPeople may suspect that I don't entirely like the use of last-modified\ndates as cache validators.  It may be that a server implementor would\nprefer to use a separately managed and opaque unique-ID, such as a\ngeneration number (as done by NFS) or an MD5 checksum.  This relieves\nthe server of having to be cautious about file modification dates, and\nalso solves the problem of insufficient precision in the HTTP timestamp\nformats.\n\nI do not believe that file length is useful as a cache validator.  As\nmany people have pointed out, it's hard to define \"length\" and it's\nnot a safe validator, since the file contents may change without changing\nthe length.  On the other hand, if server implementors are naive enough\nto use file length as their opaque identifiers, I'm not going to stop them\n(but I won't run their servers!).\n\nSo I would like to suggest, for HTTP 1.1, a FULLY COMPATIBLE protocol\nchange that should solve this problem.  Add a new header returned by\na server (perhaps via a proxy):\n\nCache-Validator = \"Cache-Validator\" \":\" opaqueID\nopaqueID       = *( unreserved | reserved )\n\nAnd a new header sent by clients:\n\nIf-Validator-Valid = \"If-Validator-Valid\" \":\" opaqueID\n\nClients and proxies are not allowed to do anything with the opaqueID\nexcept return it to the server that it came from by sending it in an\n\"If-Validator-Valid\" header.\n\nA server that receives both \"If-Validator-Valid\" and \"If-Modified-Since\"\nshould ignore the latter.  Otherwise, the spec for \"If-Validator-Valid\"\nshould look pretty much like the spec for \"If-Modified-Since\", except\nof course for simpler rules about comparisons.\n\nThis allows full interoperability with older implementations.  Old\nservers won't send \"Cache-Validator\" headers; old clients won't\nsend \"If-Validator-Valid\".  New clients will send only \"If-Modified-Since\"\nto old servers, since they won't have a validator in this case.\nOld clients may receive \"Cache-Validator\" headers, but they are\nalready required to ignore unknown headers.  Proxies (old and new)\nwill pass these new headers in either direction; new proxies may even\nuse them.\n\nA server is free to use a timestamp as its opaqueID.  It may even\nuse timestamps for some files, checksums for others, etc.  It might\nmake sense to use one for files, and the other for CGI output.\n\nTo summarize: I think HTTP 1.1 should aim for the simplest possible\ncorrect, interoperable cache-control protocol.  I claim that\nwhat I've proposed fits the bill:\n(1) Servers can use explicit \"Caching-allowed:\" headers\nto force non-caching behavior when necessary.\n(2) Servers and clients/proxies exchange opaqueIDs as cache\nvalidators.  This leaves no confusion about interpretation.\nChoice of how unique opaqueIDs are generated is left entirely\nup to the server implementor.\n(3) Cache expiration is done using a simple \"Expires+Date\"\npair that allows the client/proxy to do a sanity-check,\nwithout requiring synchronized clocks.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposa",
            "content": "I was just about to agree that if servers were synchronized to within\na fraction of a second, and that was a required part of (some future version)\nof http, that would suffice, but then I remembered that clients have\na part in all of this too.  User agents have to do the right thing with\nExpires, and there is no way that anyone's ever going to make all user\nagent clocks correct.  So at minimum, some of this normalization\nhas to occur within user agents.  It affects both handling of Expires\nand generation of GET-IMS, if  you want things to be accurate in time.\nIf accurate timekeeping were part of HTTP for servers & proxies, that would\nremove any necessity for what I suggested for them though.  The question\nyou've gotta ask yourself is \"which is a better engineering solution\"?\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff: try it out",
            "content": "    1) If there's a Connection: keepalive request header, the server will\n    hold the connection open for 10 seconds.\n\nThis might be a bit short.  My trace-based curves show a fairly\nsharp \"knee\" in mean requests/connection at somewhat higher timeouts,\naround 1-2 minutes.\n\nI assume you're closing idle connections as necessary when your\ntotal connection count exceeds a threshold.  If not, then the\n10-second timeout is probably necesary.\n\nI can't remember if we've discussed this on the group-wide mailing\nlist, but I've suggested to some people that the keep-connection\nheader include the IP address of the client or proxy that generates\nit.  This ensures that requests made via an old-style proxy are\nnot help open (which would effectively delay the response).  It\ndoes require that proxies supporting the persistent-connection\nmodel have to rewrite this header, but that seems relatively\nbenign.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Pragma minage (Was:Re: A modest proposal",
            "content": "Koen writes:\n    >Such a `tuned' cache would introduce incorrectness\nRoy writes:\n    No, it *might* introduce incorrectness.  I claim that over 99% of\n    today's non-cacheable pages are still \"correct\" after being cached,\n    where correctness is defined as containing the same substantive and\n    qualitative information content as would be obtained directly from\n    the origin.  [Note: my claim is based on personal observation, not\n    controlled experimentation]\n\nI hate to be pedantic, but let's try to maintain a distinction\nbetween a \"correct protocol\" and \"correct results\".\n\nA correct protocol is one that never yields incorrect results.\n(Usually, this is respect to a given failure model; that is,\nyou have to say what kinds of failures you are willing to defend\nagainst.  But that's splitting hairs.)\n\nA correct result is one that would have resulted from a correct\nprotocol.  An incorrect protocol can generate correct results,\nbut doesn't always do so.\n\nRoy's claim about \"99% of today's non-cacheable pages\" is one\nabout correct results.  Koen's statement is about an incorrect\nprotocol (or rather, an incorrect implementation of the protocol).\n\nI agree with Koen that some proxy implementors (or managers) may want\nto violate the protocol spec, for reasons of their own.  But this\nis outside the realm of the HTTP spec, and I agree with Roy that\nthe spec should not be twisted to support it.\n\nOn the other hand, there is some merit in in allowing a cache\nto return an object of questionable validity.  For example, suppose\nthe original server is down; should the cache return \"sorry\" or\nshould it return a possibly invalid object?  Or (to be more\nspeculative about future versions of HTTP) if the server is really\nslow, perhaps the cache could return the possibly invalid version\nquickly, and then update it with the proper version when it arrives.\n\nOr perhaps the cache is \"tuned\" as Koen has described.  (I might say\n\"kludged\" instead of \"tuned\" :-).)\n\nI suggest that these are potentially useful things for a cache to\ndo, but we ought to think about protecting the ultimate user against\nthe possible inconsistency.  Koen suggested a Pragma sent as part\nof the request, implying that the user agent controls how the cache\nhandles inconsistency, but this doesn't actually help the user\ndecide if the response is questionable or not.\n\nHow about doing things the other way?  Any cache (including\none inside the user agent) that returns a possibly invalid\nobject must mark it with (of course) a new header:\nPossibly-invalid : <reason-why-string>\nThe reason-why-string would be some sort of explanation of what\nis suspect.  For example, \"server is down, this is 1 hour old\"\nor \"we are ignoring Expiration dates from this server, but if\nyou care, the file expired in 1991\", or \"server is slow, try again soon\nbut meanwhile here's my 2-hour-old copy\".  Actually, the latter\nmight better be handled by a different kind of response, to allow\na browser to automatically update the page when it arrives from\nthe server.  Maybe:\nPossibly-invalid-try-again-soon : <reason-why-string>\nIn HTTP 2.0, the cache could probably simply promise to send\nthe response along as soon as it arrives.\n\nA browser could use some sort of status line, or pop-up window,\nor whatever, to display this warning to the user.  Cascaded proxies\nwould have to convey the warning, but of course they would naturally\ndo that with an unexpected header.\n\nThis approach allows the cache to do almost anything, but ensures\nthat the user is never mislead about the validity of a page.\n\nA user who doesn't like the idea of looking at an invalid page\ncan always hit \"Reload\".\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Pragma minage (Was:Re: A modest proposal",
            "content": "I wrote:\n    How about doing things the other way?  Any cache (including\n    one inside the user agent) that returns a possibly invalid\n    object must mark it with (of course) a new header:\n    Possibly-invalid : <reason-why-string>\n    The reason-why-string would be some sort of explanation of what\n    is suspect.  For example, \"server is down, this is 1 hour old\"\n\nOops.  How small-minded of me.  This can't just be a string, since\nthat would only be useful if the proxy spoke the same language as\nthe users.  Which is likely to be true, but it's probably not guaranteed.\n\nSo I guess we would need some sort of scheme of coded responses,\nallowing the browser to turn them into human language.  It would be\nnice to include some parameters, though.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: A modest proposal, made more specifi",
            "content": "I tried to say this before but I think something bad happened when\nI tried to send the mail.\n\nI like Jeff's proposal better than what I suggested.  My mistake\nwas in presuming that nothing can be thrown away, in particular\nif-modified-since.  Jeff's proposal obsoletes if-modified-since.\nSince I don't believe if-modified-since can work without some form\nof synchronization -- either NTP + browser hacks or something like\nwhat I said, which is indeed too complicated, maybe it would be best\nif it just went away in some future version of the protocol, and we\njust not worry about fine-grained synchronization.\n\nIn any case the date+expires rule should go in sooner rather than later.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: Pragma minage (Was:Re: A modest proposal",
            "content": "Roy Fielding:\n>[Koen Holtman:]\n>>Such a `tuned' cache would introduce incorrectness\n>\n>No, it *might* introduce incorrectness.  I claim that over 99% of\n>today's non-cacheable pages are still \"correct\" after being cached,\n>where correctness is defined as containing the same substantive and\n>qualitative information content as would be obtained directly from\n>the origin.\n\nStill \"correct\" after being cached for how long?  30 seconds?  5\nminutes?  For the duration of the user agent session?  1 day?  30\ndays?  1 year?\n\nIf you say 30 seconds, I would agree with your 99% estimate (but only\nif your `pages' do not include responses to POST requests).  For 30\ndays, my guess is about 50%.\n\nBut anyway, saying that 99% of the pages will remain correct even if\ncaches are `tuned' is besides the point: it is the possibility of\ngetting incorrect pages that counts: if the web is an unreliable\ncommunications medium, this rather restricts the kind of content it\ncan carry.\n\n>That is why a cache administrator is willing to \"tune\" the cache,\n>and it is not something that can be fixed within the protocol.\n\nBe fixed, no.  Be taken into consideration, yes.\n\n>Caches will do what providers want them to do when providers stop\n>marking cacheable pages as non-cacheable.\n\nProviders are not one group.  There are at least two groups:\n\n- Static content providers, serving pages that can always be cached.\n\n- Dynamic content providers, serving pages that cannot be cached (or\ncannot be cached for more than N minutes/hours/days).\n\nStatic providers can afford to mark their cacheable pages as\nnon-cacheable for frivolous reasons: if this drives some proxy\nadministrators to `tuning', they only feel a slight dip in hitcounts,\nnothing more.  What have they got to loose?\n\nThe web currently has almost no _dynamic_ providers, because they need\nreliable caches (or at least a way to detect unreliable caches), and\nthey cannot get reliable caches.\n\nA dynamic content provider cannot put a `tornado warning, updated\nevery 10 minutes' page on the web if it cannot be guaranteed that this\npage will never be cached for longer than 10 minutes.  Bad things will\nhappen to unsuspecting users otherwise.\n\nA web-shop cannot put a `shopping cart' application on the web if the\n(dynamic) shopping cart contents may be inappropriately cached.  A\ncustomer pressing `buy' on an outdated shopping basket page may sue\nthe web-shop after receiving the wrong products.\n\nA `war' between proxy administrators and static content providers will\nprevent dynamic content providers from using the web (unless they have\nhuge legal departments).  Thus, such a war prevents the web from\ngrowing beyond the static, read-only medium it is now.\n\n>>Pragma: min-age=<delta-seconds>\n>\n>I am opposed to this change.  First, any sensible cache administrator\n>would never send such a header, since it effectively changes the request\n>profile and therby lowers the quality of the response.\n\nSending the header would allow dynamic content providers to start\nusing the web.  This will greatly increase the quality (or at least\nthe diversity) of web content.\n\nA `tornado warning, updated every 100 minutes' response, put out by a\ntornado warning server after getting a `Pragma: min-age=6000' header\nin the request, has a much higher quality than a 90-minutes old\n`tornado warning, updated every 10 minutes' response from a `tuned'\nproxy cache.\n\nYou call it `sensible' for a cache administrator managing a `tuned'\ncache not to tellthe world it being tuned.  I strongly disagree with\nthis.  Not all actions by cache administrators can be morally\njustified by the existence of selfish server administrators.\n\nUndetectable `tuning' is a very selfish thing to do.  If 10% of cache\nadministrators do undetectable tuning, they will keep dynamic service\nproviders off the web, and spoil the fun for all people behind the 90%\nof working caches as well.\n\nThis is why I propose pragma: min-age.  If 10% of cache administrators\ninsist on fighting a spec non-conformance war with static service\nproviders, at least they should ensure that the other 90%, who put up\nwith more IP traffic in the hope of getting dynamic web services\nsomeday, do not become victims.\n\nA protocol extension cannot make the 10% of tuned caches go away, but\nit can reduce their harmful effects.\n\n>Second, the cache is tuned based on each individual response, not based\n>on every request.  If the response looks like it shouldn't be cached,\n>a tuned cache will not cache it.\n\nI think that a Pragma: min-age request header is an adequate solution\nto tuned cache detection, because I don't expect tuned caches to\nbecome that common.  A much more refined mechanism is not needed in my\nopinion.\n\nA Pragma: pragma-no-cache-coming-from-selfish-provider-ignored\nresponse header, generated by proxies, looks more refined, but has a\nbig disadvantage: it may cause the `warning: this page may be\noutdated' messages by browsers to become so common that users simply\nalways ignore them, even if they are accessing a page that really is\nautdated.  This will leave dynamic service providers with the same\nproblem they have now.\n\n>  If it says \"no-cache\" and is coming\n>from a reputable provider, a tuned cache will not cache it.  If it\n>comes from a notoriously wasteful provider, the cache will cache it\n>and there is no incentive whatsoever for the cache administrator to\n>warn the wasteful provider ahead of time.\n\nTechnology to distinguish between `reputable' and `notoriously\nwasteful' providers is not available now, and I don't have very high\nhopes of it being feasible, available, and used anytime soon.  If\nproxy administrators go through the trouble at all of making any\ndistinction between reputable and wasteful providers, I fear that the\ndefault will be to assume that a provider is wasteful.\n\nIf (new) dynamic service providers cannot count on getting a warning\nfrom tuned caches, they will simply stay off the web.\n\nAlso, if a known wasteful provider is not warned, how is the provider\nto know cache administrators disapprove of him?  If anything, wasteful\nproviders need to get as many warnings (=complaints) as possible.\n\nA wasteful provider responding to these warnings by generating pages\nwith `one-time-urls' or other cache busters would immediately show up\non the `wastefulness detector' you assume present: a proxy\nadministrator could then decide to stop talking to the provider\naltogether.\n\n   _________\n\n\nJeffrey Mogul:\n>I agree with Koen that some proxy implementors (or managers) may want\n>to violate the protocol spec, for reasons of their own.\n\nJust for the record, Roy seems to be much move forgiving of such\nviolations than I am.\n\n>  But this\n>is outside the realm of the HTTP spec, and I agree with Roy that\n>the spec should not be twisted to support it.\n\nThis problem is outside the realm of the current draft HTTP spec (in\nthat a spec can never contain a mechanism to support spec violations),\nbut not outside the realm of HTTP spec _development_.\n\nIf the goal of HTTP spec development is to allow for the growth of the\nweb, the next spec should support the needs of dynamic service\nproviders.  Dynamic service providers need Pragma: min-age, or some\nequivalent warning mechanism, so it should be in the next spec.\n\nPutting Pragma: min-age in the next spec will, if it is done right,\nnot make the spec self-contradictory.  Like the Accept-Encoding\nheader, Pragma min-age would just be a mechanism for a client to\nexpress that it only supports a subset of the defined potential http\nfunctionality.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "I must have missed something ... if I build an application which needs \nsession like control, I have a real hard time believing that I would\nfind any intermediate caching (as in proxy) acceptable. Providing  a\nmechanism where any arbitrary user could retrieve information cached\nfrom a session-id based connection seems like an unnecessary exposure\nof semi-private information.\n\nOn Mon, 14 Aug 1995, Jim Seidman wrote:\n\n> [...] \n> Given these considerations, and the slowly increasing use of \"Expires\"\n> headers, State-Info could be expensive indeed.\n\nHence, I would contend State-Info will have little impact since\ncaching would/should be disabled in most contexts where State-Info\napplies.\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "Re: planned obsolescenc",
            "content": "% From: jern@spaceaix.jhuapl.edu\n% [...]\n% Not supporting HTTP/0.9 would be a mistake.  It's a very useful\n% protocol but not every servers needs.  Server can advertise the\n% version they support.  0.9 may be \"antique\" but about half the\n% transactions hitting my server are for 0.9. \n\nWhat do you mean by saying, \"half the transactions are for 0.9\"? \nI think that most of the transactions are simple, and they could be\nprocessed with 0.9, but nevertheless they carry a HTTP/1.0 label.\nTherefore, a phasing out of 0.9 should not give much problems, at least\nIMNSHO.\n\n.mau.\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff: try it out",
            "content": "Jeffrey Mogul <mogul@pa.dec.com> wrote (Fri Aug 18 21:11:10 1995):\n  >     [dmk wrote:]\n  >     1) If there's a Connection: keepalive request header, the server will\n  >     hold the connection open for 10 seconds.\n  > \n  > This might be a bit short.  My trace-based curves show a fairly\n  > sharp \"knee\" in mean requests/connection at somewhat higher timeouts,\n  > around 1-2 minutes.\nAs you mentioned in your paper\n<http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html>\nthere are two sources of such requests:  requests for inlined images\nand subsequent hits on links by a user.  I'm shooting for the first\nbatch, I admit.  In the future, with authentication and payment\nadditions to HTTP, the short keepalive will address a larger proportion\nof the same-server traffic.  Do you have any measurements for which of\nthe two sources produces more of the same-server hits?\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "David Morris <dwm@shell.portal.com> wrote on Sun, 20 Aug 1995 17:38:17 -0700 (PDT):\n\n  > I must have missed something ... if I build an application which needs \n  > session like control, I have a real hard time believing that I would\n  > find any intermediate caching (as in proxy) acceptable. Providing  a\n  > mechanism where any arbitrary user could retrieve information cached\n  > from a session-id based connection seems like an unnecessary exposure\n  > of semi-private information.\n\nYes, you missed some of the discussion on these mailing lists.  I agree\nthat pages that contain user-specific information, such as the current\ncontents of a shopping basket, are inherently uncachable.  But I\ncontend that it's possible to design a cache-friendly application where\nmost of the pages are cachable, if you accept that State-Info (my\nproposal, http://www.research.att.com/~dmk/session.html) is passed\nthrough intermediaries without becoming part of cache state.\n\nThe example I use is a shopping basket application.  A vendor shows you\na product description page that has, on the bottom, a link to \"My\nCurrent Shopping Basket\".  The product description page is generic, if\nyou assume that the link is really to a CGI that eats the accompanying\nState-Info and spits back a display of your current basket.  So the\nproduct description itself can be cached.  Furthermore, because\nState-Info should not be cached, the semi-private information is no\nmore exposed than it otherwise would be for passing through\nintermediaries.\n  > \n  > On Mon, 14 Aug 1995, Jim Seidman wrote:\n  > \n  > > [...] \n  > > Given these considerations, and the slowly increasing use of \"Expires\"\n  > > headers, State-Info could be expensive indeed.\n\nOne of Jim's objections was to my (erroneous) assumption that caching\nproxies routinely do GET I-M-S to the origin server, so sending\nState-Info was cheap.  If S-I can't piggy-back with I-M-S, I agree S-I\nadds expense.  And because Expires is being sent more often, proxies\nsend I-M-S less often.\n  > \n  > Hence, I would contend State-Info will have little impact since\n  > caching would/should be disabled in most contexts where State-Info\n  > applies.\nPerhaps.  There's still value in not shipping the entire document,\nhowever.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff:  try it out",
            "content": "There's a slight syntactic difference between your implementation and the\none we've done between Spyglass and NCSA.  Just as an FYI: here's a copy of\na mail I sent recently which explains how our implementations work.  I'm\nalso exchanging mails with Alex Hopmann to try and resolve the syntactic\ndifferences there as well.  Both your implementation and Alex's draft\nspecify a multipart response as an alternative to an accurate\nContent-length.  That's fine -- we just have implemented it yet.\n\n--\n\nHere's how current implementations work.  NCSA Mosaic 2.6, NCSA HTTPd 1.5,\nand Enhanced Mosaic 2.1 all support this and interoperate together, and all\nimplementations were done independently.  We didn't share code with NCSA at all.\n\nInformation on the NCSA implementation is at \nhttp://hoohoo.ncsa.uiuc.edu/beta-1.5/\nhttp://hoohoo.ncsa.uiuc.edu/beta-1.5/howto/KeepAlive.html\n\nIf the client wants the connection kept alive, it sends the following header\nwith its request:\n\nConnection: Keep-Alive\n\nIf the server recognizes this and wants to leave the connection open, it\nsends back:\n\nConnection: Keep-Alive\n\nIt may only send this header back if the it also sends back a Content-Length\nheader which the client may assume to be accurate.\n\nIf the connection is left open, then the client may send more requests along\nthe same connection.  Any time the Connection: Keep-Alive header is sent and\nreceived back, then the connection should stay open.\n\nThe NCSA 1.5 server implementation also sends back\n\nKeep-Alive: timeout=n, max=m\n\nbut Enhanced Mosaic 2.1 ignores these.\n\n--\nEric W. Sink\nSenior Software Engineer, Spyglass\neric@spyglass.com\nAll opinions expressed here are my own.\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "David Morris writes:\n > \n > \n > I must have missed something ... if I build an application which needs \n > session like control, I have a real hard time believing that I would\n > find any intermediate caching (as in proxy) acceptable. Providing  a\n > mechanism where any arbitrary user could retrieve information cached\n > from a session-id based connection seems like an unnecessary exposure\n > of semi-private information.\n > \n > On Mon, 14 Aug 1995, Jim Seidman wrote:\n > \n > > [...] \n > > Given these considerations, and the slowly increasing use of \"Expires\"\n > > headers, State-Info could be expensive indeed.\n > \n > Hence, I would contend State-Info will have little impact since\n > caching would/should be disabled in most contexts where State-Info\n > applies.\n > \n > Dave Morris\n\nOnly some of the pages in a session based application need caching.\n(Catalog pages can be cached.  The state of an account cannot be).\nOnly idempotent methods can cache their results.  So, there is no\nreason why state-info needs to be passed through to an origin server\nby a cache when the cache contains a cacheable result already.  This is\nactually a very important optimization.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff:  try it out",
            "content": "  > There's a slight syntactic difference between your implementation and the\n  > one we've done between Spyglass and NCSA.  Just as an FYI: here's a copy of\n  > a mail I sent recently which explains how our implementations work.  I'm\n  > also exchanging mails with Alex Hopmann to try and resolve the syntactic\n  > differences there as well.  Both your implementation and Alex's draft\n  > specify a multipart response as an alternative to an accurate\n  > Content-length.  That's fine -- we just have implemented it yet.\nOkay, I changed my server to behave as below:\nhttp://www.research.att.com:8000/\n  > \n  > --\n  > \n  > Here's how current implementations work.  NCSA Mosaic 2.6, NCSA HTTPd 1.5,\n  > and Enhanced Mosaic 2.1 all support this and interoperate together, and all\n  > implementations were done independently.  We didn't share code with NCSA at all.\n  > \n  > Information on the NCSA implementation is at \n  > http://hoohoo.ncsa.uiuc.edu/beta-1.5/\n  > http://hoohoo.ncsa.uiuc.edu/beta-1.5/howto/KeepAlive.html\n  > \n  > If the client wants the connection kept alive, it sends the following header\n  > with its request:\n  > \n  > Connection: Keep-Alive\nI'll accept this from HTTP/1.0 clients.\n  > \n  > If the server recognizes this and wants to leave the connection open, it\n  > sends back:\n  > \n  > Connection: Keep-Alive\nI'll return this.\n  > \n  > It may only send this header back if the it also sends back a Content-Length\n  > header which the client may assume to be accurate.\nRight.  My variant remains:  if it's CGI output and client says protocol\nis HTTP/1.2, I'll chunk the output (Transfer-Encoding: chunked).\n  > \n  > If the connection is left open, then the client may send more requests along\n  > the same connection.  Any time the Connection: Keep-Alive header is sent and\n  > received back, then the connection should stay open.\nOkay.  But there's a potential problem here.  If the server sends the\nresponse header, then encounters some kind of problem, it may be forced\nto close the connection, thus \"lying\".\n  > \n  > The NCSA 1.5 server implementation also sends back\n  > \n  > Keep-Alive: timeout=n, max=m\nI'll add this later.  Not there yet.\n  > \n  > but Enhanced Mosaic 2.1 ignores these.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff:  try it out",
            "content": ">Right.  My variant remains:  if it's CGI output and client says protocol\n>is HTTP/1.2, I'll chunk the output (Transfer-Encoding: chunked).\n\nFine.\n\n>Okay.  But there's a potential problem here.  If the server sends the\n>response header, then encounters some kind of problem, it may be forced\n>to close the connection, thus \"lying\".\n\nI forgot to mention:\n\nEither client or server can close the connection when necessary, with the\nother side being required to cope.\n\n--\nEric W. Sink\nSenior Software Engineer, Spyglass\neric@spyglass.com\nAll opinions expressed here are my own.\n\n\n\n"
        },
        {
            "subject": "Re: SessionID proposa",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nDavid Morris wrote:\n\n> I must have missed something ... if I build an application which needs \nsession\n> like control, I have a real hard time believing that I would find any\n> intermediate caching (as in proxy) acceptable. \n\nThis is almost certainly to be the case, however, the client which is the\nremote end of your session may have other interactions with the server on\nwhich your app runs. It is possible that the client will interleave non-\ncachable requests to your application with requests for which the responses\nare cacheable. (Example: I'm buying some books from your application but get\nbored and decide to check out the \"What's new page on your server.\" Then,\nafter fooling around a bit, I return to the book-buying process.) Because\nHTTP is connectionless, the only way to make sure that sessions continue is\nto send the State-Info: whenever the client requests the server (or with\ncookies: whenever it requests specific paths on the server.) Sometimes, the\nrequests will go to applications that don't require or can't use the State-\nInfo and those apps may return cachable data. There isn't much anyone can do\nto prevent that.\n\n> Providing  a mechanism where any\n> arbitrary user could retrieve information cached from a session-id based\n> connection seems like an unnecessary exposure of semi-private information.\n\nNone of the proposals that I have seen (Dave Kristol's, Cookies, Koen's, etc\n.) have this property. All provide some mechanism to prevent inappropriate\ncaching of session related information. If anything, the current proposals\nall do too much to prevent or forbid otherwise valid caching.\n\n> Hence, I would contend State-Info will have little impact since caching\n> would/should be disabled in most contexts where State-Info applies.\n\nState-Info: will have an impact if:\n1. Use of State-Info: prevents caching that would otherwise be permitted.\nIn the example above, the \"what's new\" page is probably cachable. However,\nif State-Info: use can prevent that page from being cached, this would not\nbe good. Cache builders also need to think about whether arrival of a\nresponse which contains State-Info: should be allowed to change the caching\nstatus of an earlier response for the same URL that did not contain State-\nInfo. \n2. If State-Info headers become large: i.e. if people starting putting\nkilobytes of shopping cart information into State-Info, then this will tend\nto consume more than a justified amount of net resources. (NOTE: I have\nproposed mechanisms by which servers can ensure that State-Info headers are\nalways very small -- and private.)\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "updated StateInfo proposa",
            "content": "I have incorporated Bob Wyman's suggestion, reworded the User Agent\nsection (now 4.2, formerly 3.2) and added Abstract and Security\nConsiderations sections.  I will submit the current version as an\nInternet Draft shortly.  (The format of the document changed slightly\nto reflect presumed I-D filenames.)\n\nhttp://www.research.att.com/~dmk/session.html\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "any more comments",
            "content": "I didn't get much done over the past four days, so I hope people aren't\nsitting on their comments until the next round.  I'll try to get both\ndrafts updated tonight.\n\nExcept for the definition of Pragma, I have not had any negative\ncomments on the content of the HTTP/1.0 BCP draft.  Does that mean\nwe have reached consensus?\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "RE: any more comments",
            "content": "This was almost definitely meant for the http-wg list proper, not the list\nadmin (i.e. me) ... so I'm forwarding it on...\n\n -- ange -- <><\n\nange@hplb.hpl.hp.com\n\n------- Forwarded Message\n\nDate:    Wed, 23 Aug 1995 13:17:05 -0700\nFrom:    Paul Leach <paulle@microsoft.com>\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: RE: any more comments?\n\nI have some comments on making the spec clearer more precise. They \napply equally to 1.0 and 1.1. I was going to try and make them fairly \ndetailed, but that means it takes a while to type up, and I'm still in \nthe process of doing that. Since silence at this point would be taken \nfor assent, I'm sending this along, even though it is incomplete.\n\nThe overall thrust of what I'd like to see: for each method, a list of \nthe request header fields that are required, optional, and defaults for \nthem, if any; a list of the possible status codes and response header \nfields; and whether of not an entity header is present in either \ndirection and its contents if so.\n\nThis makes the information for a method much like in an API spec: the \nin arguments, out arguments, and error status codes, and can help make \nthings very clear.\n\nFiguring out the above from the current spec is not trivial; one of the \nreasons I don't have a complete set of the above yet is that it takes a \n_very_ close reading to be sure about many of them.\n\nFor example, here is my summary table for request header fields, based \non my reading of the spec.\n\nField:Method:\nGetHeadPostPutDeleteLinkUnlink\nAcceptDefDefIllegalIllegalIllegalIllegalIllegal\nAccept-CharsetDefDefIllegalIllegalIllegalIllegalIllegal\nAccept-EncodingDefDefIllegalIllegalIllegalIllegalIllegal\nAccept-LanguageDefDefIllegalIllegalIllegalIllegalIllegal\nAuthorizationOptOptOptOptOptOptOpt\nFromOptOptOptOptOptOptOpt\nIf-Modified-Since OptIllegalIllegalIllegalIllegalIllegalIllegal\nOrig-URIOptOptOptOptOptOptOpt\nRefererOptOptOptOptOptOptOpt\nUser-AgentShouldShouldShouldShouldShouldShouldShould\n\n\nDef =>field is optional, has default value if not present\nOpt => field is optional\nShould =>field is strongly suggested to be sent\nIllegal=>client shouldn't send field; server must ignore if sent\n\nIf you think this is a good idea, I'll keep working on the other parts \nthat I mentioned above.\n\nPaul\n- ----------\n] From: Roy Fielding  <fielding@beach.w3.org>\n] To:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: any more comments?\n] Date: Wednesday, August 23, 1995 1:19PM\n]\n] I didn't get much done over the past four days, so I hope people aren't\n] sitting on their comments until the next round.  I'll try to get both\n] drafts updated tonight.\n]\n] Except for the definition of Pragma, I have not had any negative\n] comments on the content of the HTTP/1.0 BCP draft.  Does that mean\n] we have reached consensus?\n]\n]\n]  ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n]                       Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n]                       (fielding@w3.org)                (fielding@ics.uci.edu)\n] \n\n\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": "Paul Leach <paulle@microsoft.com> writes:\n\n>The overall thrust of what I'd like to see: for each method, a list of \n>the request header fields that are required, optional, and defaults for \n>them, if any; a list of the possible status codes and response header \n>fields; and whether of not an entity header is present in either \n>direction and its contents if so.\n\nThat would be a 5 dimensional table.  Since I have yet to see a table\nformat that accomplished this without duplicating every line of the\ncurrent spec, I'd be pleasantly surprised if you could do one.\n\nThe reason the current specification seems difficult to read \nis because it is a flattened 5 dimensional table in prose.\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff: try it out",
            "content": "DMK wrote:\n    Jeffrey Mogul <mogul@pa.dec.com> wrote (Fri Aug 18 21:11:10 1995):\n      >   [dmk wrote:]\n      >   1) If there's a Connection: keepalive request header, the server will\n      >   hold the connection open for 10 seconds.\n      > \n      > This might be a bit short.  My trace-based curves show a fairly\n      > sharp \"knee\" in mean requests/connection at somewhat higher timeouts,\n      > around 1-2 minutes.\n    As you mentioned in your paper there are two sources of such\n    requests:  requests for inlined images and subsequent hits on links\n    by a user.  I'm shooting for the first batch, I admit.  In the\n    future, with authentication and payment additions to HTTP, the\n    short keepalive will address a larger proportion of the same-server\n    traffic.  Do you have any measurements for which of the two sources\n    produces more of the same-server hits?\n    \nI did not break down the traces by file name, so I don't have explicit\nresults for inlined images vs. subsequent clicks.\n\nBut in the paper I plot \"requests arriving for already-open connections\"\nvs. idle timeout.  Generally, a large fraction of the gain (almost\nhalf) comes with timeouts greater than 10 seconds.  This implies\neither of two things:\n(1) Many clients were delaying ca. 10 seconds before retrieving\ninlined images.\n(2) there were a lot of subsequent hits in the 10-100 second\nrange.\nHypothesis #1 seems rather unlikely, so I'd bet on #2.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff: try it out",
            "content": "       If the connection is left open, then the client may send more\n       requests along the same connection.  Any time the Connection:\n       Keep-Alive header is sent and received back, then the connection\n       should stay open.\n\n    Okay.  But there's a potential problem here.  If the server sends the\n    response header, then encounters some kind of problem, it may be forced\n    to close the connection, thus \"lying\".\n\nThe basic rule for a persistent-connection HTTP has to be that either\nclient or server (or proxy, for that matter) is allowed to close any\nconnection at any time.  This isn't \"lying\", this is life.\n\nThe word \"should\" (as in \"should stay open\") in IETF-speak usually\nhas the meaning \"do this unless you cannot\".  I think we need to\nword the spec using \"may\"; neither side can compel the other to\nkeep a connection open.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": "On Wed Aug 23, 1995, Roy Fielding wrote:\n\n> Paul Leach <paulle@microsoft.com> writes:\n> \n> >The overall thrust of what I'd like to see: for each method, a list of \n> >the request header fields that are required, optional, and defaults for \n> >them, if any; a list of the possible status codes and response header \n> >fields; and whether of not an entity header is present in either \n> >direction and its contents if so.\n> \n> That would be a 5 dimensional table.  Since I have yet to see a table\n> format that accomplished this without duplicating every line of the\n> current spec, I'd be pleasantly surprised if you could do one.\n\nI don't think Paul necessarily meant a table.\nThink man page per method:\n\n  Method: GET\n  Request Headers:\n  \n      AuthorizationOptional\n      FromOptional\n      If-Modified-Since         Optional\n      Orig-URI                  Optional\n      RefererOptional\n      User-Agent                Should\n  \n  Entity Headers\n  \n  Entity\n  \n  Possible Status Codes\n  \n      200 Success, resource included as entity in response\n      301 The resource has moved permanently, see Location header\n      302 The resource has moved temporarily, See Location header\n      304 ...\n\nI approach the spec from the point of view of implementing a\nserver.  Okay, I need to write the GET code now, what\nheaders can I expect in legal messages and what do I have to\nreturn in the responses.  I would think client writers would\napproach it the same way.  As the spec is currently layed\nout, I find getting this information very difficult.\n\n> The reason the current specification seems difficult to read \n> is because it is a flattened 5 dimensional table in prose.\n\nNo wonder I've been confused :-)  I think 7 man-like pages\n(one per method) would go a long way to helping.\n\nHoward\n\n\n\n"
        },
        {
            "subject": "Re: any more comments? ('204 No Content' clarification",
            "content": "        204 No Content\n\n        The server has fulfilled the request but there is no new\n        information to send back. If the client is a user agent, it\n        should not change its document view from that which\n        caused the request to be generated. This response is primarily\n        intended to allow input for scripts or other actions to take\n        place without causing a change to the user agent's active\n        document view. The response may include new\n        metainformation in the form of entity headers, which\n        should apply to the document currently in the user agent's\n        active view.\n\nWhat is a UA expected to do with form fields in this circumstance?  If a\nscript is to take input and leave the form in view, it would be useful to\nclear the form fields without changing the document.  This would: (1) give\na visual indication that something has happened; and (2) allow repeated\nentries (i.e., data entry) to the same form without reloading or <input\ntype=\"reset\">-ing.\n\nIf there are no objections, I'd like to see this:\n\n        204 No Content\n\n        The server has fulfilled the request but there is no new\n        information to send back. If the client is a user agent, it\n        should __display the document as it was originally received\n        (resetting form fields but not otherwise changing the\n        document view)__. This response is primarily\n        intended to allow input for scripts or other actions to take\n        place without __loading a new document into__ the user\n        agent's active view. The response may include new\n        metainformation in the form of entity headers, which\n        should apply to the document currently in the user agent's\n        active view.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: any more comments? ('204 No Content' clarification",
            "content": "On Wed, 23 Aug 1995, Marc Hedlund wrote:\n\n> What is a UA expected to do with form fields in this circumstance?  If a\n> script is to take input and leave the form in view, it would be useful to\n> clear the form fields without changing the document.  This would: (1) give\n> a visual indication that something has happened; and (2) allow repeated\n> entries (i.e., data entry) to the same form without reloading or <input\n> type=\"reset\">-ing.\n\nWell, this isn't what 204 was originally for. If I remember the mailing\nlist archives correctly (I read them a couple months ago), it was added a\ncouple years ago so that imagemaps could tell the client not to do\nanything when a an 'unused' portion of the map was clicked. In order to\nwork properly, this response code must be interpreted by the client as\ncausing *absolutely* no change to the document window. It should act as if\nyou had clicked on a part of the document where absolutely nothing exists. \nThis is how Netscape and Mosaic both handle it, and it is rather useful -\nI use it in imagemaps all the time. \n\nTo do what you wish to accomplish, within the current spec, simply send a\nresponse that redirects the client to the page they came from (i.e. 301 or\n302). Ideally, the client would then send a request message, get a 304 Not\nModified back from the server, and would redisplay the document \"as it was\noriginally received\". \n\n> If there are no objections, I'd like to see this:\n\nConsider this an objection (of course, I guess it doesn't really count,\nsince I'm nobody important). Besides, the HTTP/1.0 draft is suposed to\nrepresent current practice, and your redefninition does not conform. Now,\nfor HTTP/1.1, I see no reason why a new respose code, along the lines of\n\"205 Reset Document\" couldn't be added, which would have no content, but \nwould instruct the UA to refresh the document internally, clearing any \nuser-modified portions.\n\n--/ Alexei Kosut <akosut@nueva.pvt.k12.ca.us> /--------/ Lefler on IRC\n----------------------------/ <http://www.nueva.pvt.k12.ca.us/~akosut/>\nThe viewpoints expressed above are entirely false, and in no way\nrepresent Alexei Kosut nor any other person or entity. /--------------\n\n\n\n"
        },
        {
            "subject": "Re: any more comments? ('204 No Content' clarification",
            "content": "At 4:35 PM 8/23/95, Alexei Kosut wrote:\n>Consider this an objection (of course, I guess it doesn't really count,\n>since I'm nobody important).\n\nWell, I said \"if there are no objections,\" not \"if there are no objections\nfrom important people.\"  ;)  Your imagemap example makes sense, as do your\ncomments about \"current practice\" -- I had thought there was some variation\nbetween clients, but I can't find an example of the behavior I described,\nso the suggestion is withdrawn for 1.0.\n\n>for HTTP/1.1, I see no reason why a new respose code, along the lines of\n>\"205 Reset Document\" couldn't be added, which would have no content, but\n>would instruct the UA to refresh the document internally, clearing any\n>user-modified portions.\n\nI'd be happy with this.  I could see this response being useful for applets\n(i.e., Java) as well.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: any more comments? ('204 No Content' clarification",
            "content": "In article <v02120d01ac6176031973@[204.156.156.16]> hedlund@best.com (Marc\nHedlund) wrote:\n> \n> >for HTTP/1.1, I see no reason why a new respose code, along the lines of\n> >\"205 Reset Document\" couldn't be added, which would have no content, but\n> >would instruct the UA to refresh the document internally, clearing any\n> >user-modified portions.\n> \n> I'd be happy with this.  I could see this response being useful for applets\n> (i.e., Java) as well.\n> \n\nThis would be really useful for Chat.  Some implementations use\none window that submits the users input and returns 204.\nA second window displays the result.  It would be nice to\nhave a way to clear the form element after submitting.\n\n:lou\n-- \nLou Montulli                 http://www.mcom.com/people/montulli/\n       Netscape Communications Corp.\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": "Howard's interpretation is exactly what I meant. My inclusion of the \ntable seems to have confused things. It was actually just the way I was \npersonally using to help me summarize the information that I would \npresent basically as Howard shows below.\n\nI don't understand Roy's comment about duplicating every line of the \nspec. My intent was that the bulk of the information about each field \nshould be where it is in the current spec, and presence in the list of \nfields for each method is really just an indiciation that that info \napplies to that method.\n\nI recognize Roy's dilemma -- if you're really going to be precise, then \nyou also have to say what response header fields are appropriate\n----------\n] From: Howard Melman  <melman@osf.org>\n] To: Roy Fielding  <fielding@beach.w3.org>\n] Subject: Re: any more comments?\n] Date: Wednesday, August 23, 1995 5:26PM\n]\n]\n] On Wed Aug 23, 1995, Roy Fielding wrote:\n]\n] > Paul Leach <paulle@microsoft.com> writes:\n] >\n] >\n] I don't think Paul necessarily meant a table.\n\n\n\n"
        },
        {
            "subject": "Proposal: a PLAY or STREAM method for http/1.",
            "content": "I am new to this group, coming from a background of storage and\ncommunication subsystems for video servers + familiarity with\nnetworking. I've gone through the HTTP/1.0 and preliminary 1.1\nproposals + the relevant http-wg email exchanges, and also looked in\nSimon Spero's preliminary NG draft of March '95. Finally, I am aware\nof the proposal made by David Levine in May '95 for a transfer-rate\nparameter and the discussion that followed.\n\nIn this note, I propose to add a \"PLAY\" (or \"STREAM\") method to\nhttp/1.1 in support of data-streaming applications.  My proposal is\nstated briefly, followed by FAQ (by me, so far...) which expose my\nrationale. I am looking forward to comments on the proposal,\nalternatives, as well as suggestions for appropriate attributes.\n\n-----------------------------------\n\nPROPOSAL:  add a \"PLAY\" (or \"STREAM\") METHOD to HTTP/1.1\n\nThe semantics of GET are \"provide the requested entity in its entirety\nas soon as possible\". Various qualifiers may influence the content,\nbut the basic semantics remain unchanged.\n\nThe semantics of PLAY are \"provide the requested material at the\nsuitable rate for playback, starting as soon as possible\".  \n\nSimilarly to GET, PLAY could have various qualifiers. For example, one\ncould request the highest data-rate option that the server (and\nperhaps the network) is capable of guaranteeing at that time. Also,\nclient-buffer-size may be provided in order to help the server manage\nits resources in a flexible manner without causing glitches (and\ndetermine whether it can provide the guarantees).\n\nBased on the knowledge of the true request semantics, the server can\nallocate its resources much more intelligently than otherwise, sign an\nappropriate contract with the network (a constant-bit-rate ATM virtual\ncircuit, for example), and the system as a whole can provide the best\npossible service with minimum disturbance to other\nactivity. (Alternatively, the server may refuse service if sufficient\nresources cannot be committed or negotiate down to GET.)  As the\nfraction of network traffic that has this \"stream\" nature increases,\nthe ability to recognize a request's true semantics and to service it\nproperly will become extremely important. In contrast, an inability to\nconvey the \"stream\" semantics would result in an unusual situation\nwhereby lower communication layers provide richer semantics than\nhigher ones can exploit. This would be unfortunate.\n\nOne important case in which PLAY better represents the request\nsemantics is an audio or video file that the user wishes to view\nrather than to \"download\". Explicit support for such requests would\nfacilitate the penetration of WWW in general and http in particular\ninto domains that are still (conveniently?) viewed by some as best\nserved by closed, proprietary systems.\n\n-----------------------\n\nInitial Q&A regarding a \"PLAY\" or \"STREAM\" method in http/1.1\n\nQ0. Is PLAY meaningful on the internet?\n\nUsefulness in the very near future will vary dramatically by\nlocation. However, including PLAY in http now would pose a challenge\nand set a goal for infrastructure implementers, and would help in\nshaping the priorities. Also, it may find use in \"closed\" subnetworks,\nhelping http gain broader acceptance and facilitating integration of\n\"bulk\" and \"streaming\" infrastructures.\n\nQ1. Should PLAY be in http or part of html?  \n\nSince both download (GET) and PLAY requests are sensible for the same\nfile, the only place is http. (The file may contain playback directives.)\n\nQ2. Can PLAY be closely approximated by a sequence of GET(byte range)?\n\nNO! The server would not possess the right criteria for accepting or\nrejecting the request. Even if it did, it would not be able to offer\nguarantees for the entire movie, since each GET would be considered in\nisolation. Finally, even if things did work (aided by sufficient\nclient buffering), the traffic pattern presented to the network would\ndiffer dramatically from that with PLAY (bursty instead of nearly-fixed\nbandwidth), creating unnecessary problems there.\n\nQ3. Is PLAY the same as GET(transfer rate)?\n\nThe two are conceptually different, since PLAY states what is\nrequired, and GET(rate) specifies how it should be done. Confusing the\ntwo is not a good idea, since problems may show up and flexibility is\nreduced.\n\nConsider, for example, the case of a variable-rate stream. PLAY would\nenable a \"smart\" server to supply the entire stream correctly in\nresponse to a single request. A sequence of GET(byte-range, rate)\nrequests (with the client parsing the data and issuing the requests)\ncould perhaps imitate this. However, the server would again be unable\nto reserve resources (its own, network, etc.) for the duration of the\nentire stream based on the semantics of GET.\n\nQ4. Would a session extension do the trick?\n\nThis comes closer if the session setup reserves bandwidth. The server,\nhowever, is still unaware of the commitments (e.g., storage bandwidth)\nthat it has implicitly made, and service interruptions may result.\n\n\nQ5. Should PLAY be a separate method or some new descriptor of GET?\n\nThe semantical difference between PLAY and GET is at least as large\nand fundamental as between GET and HEAD, ==> method.\n\nQ6. Should PLAY also be provided in the reverse direction?\n\nI think this is not necessary, as I don't envision \"playing\" data to a\nserver. (The server can always request if it wishes, using PLAY).\n\nQ7. Would PLAY  be used in practice? \n\nSince the difference between \"download\" and \"play\" is very intuitive\nto any user and application writer, I expect it to be used frequently\n(infrastructure permitting).\n\n\n---------------------\n\nYitzhak Birk\nEE Dept, Technion - Israel Inst. of Technology  birk@ee.technion.ac.il\nPresently at HP Labs, Palo Alto.  (birk@bodega.stanford.edu, birk@hpl.hp.com)\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": ">> That would be a 5 dimensional table.  Since I have yet to see a table\n>> format that accomplished this without duplicating every line of the\n>> current spec, I'd be pleasantly surprised if you could do one.\n>\n>I don't think Paul necessarily meant a table.\n>Think man page per method:\n\nWhich is still a table.  For example, consider the dimensions\n\n(Method x Message-Header x Request-Header x Request-Entity x\n Response-Code x Message-Header x Response-Header x Response-Entity)\n\nnow look at the flattened table of a man page:\n\n>  Method: GET\n>  Request Headers:\n>  \n>      AuthorizationOptional\n>      FromOptional\n>      If-Modified-Since         Optional\n>      Orig-URI                  Optional\n>      RefererOptional\n>      User-Agent                Should\n>  \n>  Entity Headers\n>  \n>  Entity\n>  \n>  Possible Status Codes\n>  \n>      200 Success, resource included as entity in response\n>      301 The resource has moved permanently, see Location header\n>      302 The resource has moved temporarily, See Location header\n>      304 ...\n\nYou should note that this table only gives enough information\nfor (Method x Request-Header) + (Method x Request-Entity) +\n(Method x Status-Code), which is not a sufficient subset for\na complete specification of the GET method.\n\nFor example, (GET x 304) is only possible if (GET x IMS),\nyet that information is not describable by the format above.\n\nNow, the choice is either enumerate all possibilies (which would\nmake the spec several hundred pages long) or include conditional\nstatements within a sparse table (which is exactly what the current\nspec is).\n\nThe reason people must read every line of the spec carefully in\norder to determine what is and what is not defined is because\nthe entire spec is the defintion of that table of defined\nbehavior -- leave a line out, and you do not define HTTP.\n\nCreating an enumerated table of valid HTTP request/response\nconstraints sounds like a great project, perhaps something that\nan author can take up and publish as an O'Reilly reference.\nHowever, I do not think that it is a task for this WG, and it\ncertainly will not happen within the timeframe of the\ninitial HTTP/1.0 and HTTP/1.1 RFCs.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: any more comments? ('204 No Content' clarification",
            "content": "On Wed, 23 Aug 1995, Marc Hedlund wrote:\n\n> What is a UA expected to do with form fields in this circumstance?  If a\n> script is to take input and leave the form in view, it would be useful to\n> clear the form fields without changing the document.  This would: (1) give\n> a visual indication that something has happened; and (2) allow repeated\n> entries (i.e., data entry) to the same form without reloading or <input\n> type=\"reset\">-ing.\n\nI have a real concern about automatic resetting of fields. Such behavior\npresumes that repeated input will be different rather than a small\nchange from the last input. In general, clicking a reset button isn't\nthat big a deal considering all the mouse manipulation required to\nposition oneself for the next input.  And then, in addition to reset,\nshould the form reposition itself to the beginning? Killing the user's\nability for one last confirmation could also be frustrating.\n\nThere are some applications (e.g., Lou's example of CHAT) where immediate\nrecycle is known to be desireable to the application implementor and\nperhaps also to the user.\n\nThis is not a behavior which should be triggered as a side effect of\nother useful bahaviors (like staying on the same document). When and\nif it occurs it should be explicitly requested.\n\nI would suggest a new <input> type for 'SUBMIT&RESET' then the\napplication implementor can offer an appropriate button and even a\nchoice for the user.  This would apply without respect to whether the\napplication also provided a confirmation page.\n\nDave Morris\n\n(PS. For some applications, like chat, and for error feedback from\nmany others, it would be very useful to have a response which said\nthat the output page now arriving should REPLACE the current page.\nCHAT would use this to provide a scrolling dialog window without filling\nthe user's history cache and data base interactive applications could \nprovide a much more effective error feedback as well as applications\nwhich might perform actions such as filling in a customer name and\naddress while leaving the screen about the same.)\n\n\n\n"
        },
        {
            "subject": "Re:  Proposal: a PLAY or STREAM method for http/1.",
            "content": "Yitzhak Birk <birk@bodega.stanford.edu> wrote:\n\n  > [...]\n  > In this note, I propose to add a \"PLAY\" (or \"STREAM\") method to\n  > http/1.1 in support of data-streaming applications.  My proposal is\n  > stated briefly, followed by FAQ (by me, so far...) which expose my\n  > rationale. I am looking forward to comments on the proposal,\n  > alternatives, as well as suggestions for appropriate attributes.\n  > \n  > -----------------------------------\n  > \n  > PROPOSAL:  add a \"PLAY\" (or \"STREAM\") METHOD to HTTP/1.1\n  > \n  > The semantics of GET are \"provide the requested entity in its entirety\n  > as soon as possible\". Various qualifiers may influence the content,\n  > but the basic semantics remain unchanged.\n  > \n  > The semantics of PLAY are \"provide the requested material at the\n  > suitable rate for playback, starting as soon as possible\".  \n  > [remainder deleted]\n\nIMO, this kind of method is outside HTTP.  I think it is best handled by\n(conceptually, at least) an outboard application, as follows:\n1)  user agent -> origin server\nRequest stream entity\n2)  origin server -> user agent\nHere's identifier for entity\n3)  user agent creates (possibly) separate process, handing the\nidentifier to it\n4)  separate process examines identifier, connects to (possibly)\nseparate entity server\n5)  separate process and entity server negotiate bandwidth, etc.\nand commence to handle stream\n\nHTTP is not all things to all people, and I think one of the things it\nis not is a real-time transport protocol.  It seems to me much better\nto segregate such sophisticated real-time processing elsewhere.\n\nNote that, in my description above, there's no requirement for the process\nor entity server to be different from the user agent and origin server.\nBut there's no requirement that they be the same, either.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re:  Proposal: a PLAY or STREAM method for http/1.",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\nDave Kristol wrote: \n> IMO, this kind of method is outside HTTP.  I think it is best handled by\n> (conceptually, at least) an outboard application,\n\nI agree with this position at this time. \n\nI suggest that while Yitzhak Birk may have established a requirement for a\nconnection which provides the semantics he defines as being needed by \"PLAY\"\n, what he hasn't done is establish the requirement that HTTP be the protocol\nwhich provides the required semantics. Yitzhak's only attempt at\nrationalizing that HTTP should be the mechanism for supporting PLAY is his\nFAQ item in which he argues between HTTP and HTML. There are other\nalternatives... TCP/IP supports many ports -- we don't have to pump\n*everything* through port 80...\n\nI suspect that the needs of those developers who require PLAY semantics\nwould probably be much better served if they were to consolidate into a new\neffort to define a protocol which would be a sister protocol to HTTP --\ndesigned to work well with clients which support HTTP. \n\nPlay is not the only protocol type which is not well served by HTTP today.\nFor instance, I think you'll find that the Z39.50 folk and their ISO cousins\nwould argue that HTTP is inferior to what they are building when it comes to\nhandling the complex queries, record/field structured data values, etc. that\nare typical of the applications they support. There is also quite a\ncommunity of game developers, virtual reality folk, system management types,\nusers of Telnet, etc. who would be giving up much if they were to try to\nsqueeze what they have into the confines of HTTP extensions.\n\nThe growing popularity of HTTP is certainly compelling, however, just\nbecause it does what it does doesn't mean it should be made to do everything\nelse. It would be more reasonable to define a family of protocols that\ninterwork well within the context of a single, reasonably easy to construct,\nmulti-protocol client. \n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: '205 Reset Document' (was Any more comments?",
            "content": "[changed the subject line -- not intended as part of the 1.0 review any longer]\n\nAt 12:10 AM 8/24/95, David Morris wrote:\n>This is not a behavior which should be triggered as a side effect of\n>other useful bahaviors (like staying on the same document). When and\n>if it occurs it should be explicitly requested.\n\n'205 Reset Document', suggested in a followup, would be an explicit request\nfor this behavior.\n\n>I would suggest a new <input> type for 'SUBMIT&RESET' then the\n>application implementor can offer an appropriate button and even a\n>choice for the user.  This would apply without respect to whether the\n>application also provided a confirmation page.\n\nThe idea was not to give the user another choice, it was to provide a\nresponse that says, \"that last one was okay, go ahead with the next one,\nand keep using the same form.\"  The user won't see a different result --\nthe same resetting can be accomplished, for instance, by resending the\nblank form.  The difference would be in the speed of the response.\n\nMarc Hedlund <hedlund@best.com>\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": "OK, so let's stipulate that it takes an 8 dimensional table to \n*completely* describe HTTP as a table.\n\nI wasn't suggesting that the complete table needed to be constructed, \nor that it would be an alterative to the current spec.\n\nIf one did construct it, one would notice that a lot of spatially \nnearby entries were the same. One would assuredly want to \ncompress/flatten these: what you call \"conditional statements\". The \nquestion is, what's the clearest way to do it?\n\nI was suggesting some extra info, in each subsection of section 5.2.x, \nthe ones where the methods are described, that would be a little more \nexplicit than the current spec about the allowable header fields and \nentity information in each direction.\n\nLet me try to motivate this. Take GET as an example. Is there ever an \nentity-header or entity-body in a GET request? Normally, one would \nexpect not -- it doesn't seem to make sense. But are there edge cases, \nclever uses, or whatever, where practitioners have come up with a use \nthat implementors are expected to know about? For someone coming fresh \nto the spec, like me, who hasn't implemented HTTP clients or servers, \nthe answer isn't easy, and I believe that it is not explicit anywhere \nin the current spec.\n\nOther examples: are accept* header fields legal in POST, PUT, DELETE, \nLINK, UNLINK request headers? Are entity bodies allowed in responses to \nthese methods? (I think the answer to the first question is \"no\", and \nto the second is definitely \"sometimes, depending on status code\".)\n\nThe form of the \"man page\" that Howard provided could answer many of \nthese questions quickly and definitively. As Roy points out, there are \nstill questions that it can't answer. These could be flagged, to refer \nto the detailed writeup elsewhere in the spec.  To take the GET x 304 \nexample, either a note by the 304 entry saying its only legal if IMS \nwas specified, or simply a \"*\" to indicate that the reader needs to go \nto the section on the 304 status code (6.2.3).\n\nI would estimate that such a description for one method would be about \n30 lines or so, times 6 methods.  I believe that it would be useful.  \nIf there are enough people who agree, I volunteer to produce them so \nthat the rest of the list can see if there's consensus to include them \n-- if not in the main body, then perhaps as an appendix. I will do this \nby next Monday if the feedback comes in positive today or tomorrow.\n\nPaul\n----------\n] From: Roy Fielding  <fielding@beach.w3.org>\n] To: Howard Melman  <melman@osf.org>\n] Cc:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: Re: any more comments?\n] Date: Thursday, August 24, 1995 12:20AM\n]\n[excisions...]\n\n] For example, (GET x 304) is only possible if (GET x IMS),\n] yet that information is not describable by the format above.\n]\n] Now, the choice is either enumerate all possibilies (which would\n] make the spec several hundred pages long) or include conditional\n] statements within a sparse table (which is exactly what the current\n] spec is).\n]\n] The reason people must read every line of the spec carefully in\n] order to determine what is and what is not defined is because\n] the entire spec is the defintion of that table of defined\n] behavior -- leave a line out, and you do not define HTTP.\n]\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": "Paul Leach <paulle@microsoft.com> wrote:\n  > [...]\n  > I would estimate that such a description for one method would be about \n  > 30 lines or so, times 6 methods.  I believe that it would be useful.  \n  > If there are enough people who agree, I volunteer to produce them so \n  > that the rest of the list can see if there's consensus to include them \n  > -- if not in the main body, then perhaps as an appendix. I will do this \n  > by next Monday if the feedback comes in positive today or tomorrow.\n\nI, for one, would welcome such tables.  I've implemented a server from\nthe spec., and while I agree with Roy that reading everything is\nessential, I sure would have liked a set of tables to use as a\nchecklist to make sure I hadn't done something dumb.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "I-D ACTION:draft-kristol-http-state-info00.txt, .p",
            "content": "--NextPart\n\nA New Internet-Draft is available from the on-line Internet-Drafts \ndirectories.                                                               \n\n       Title     : Proposed HTTP State-Info Mechanism                      \n       Author(s) : D. Kristol\n       Filename  : draft-kristol-http-state-info-00.txt, .ps\n       Pages     : 7\n       Date      : 08/23/1995\n\nHTTP, the protocol that underpins the World-Wide Web (WWW), is stateless.  \nThat is, each request stands on its own; origin servers don't need to \nremember what happened with previous requests to service a new one.  \nStatelessness is a mixed blessing, because there are potential WWW \napplications, like ``shopping baskets'' and library browsing, for which the\nhistory of a user's actions is useful or essential.                    \n\nThis proposal outlines a way to introduce state into HTTP.  A new \nrequest/response header, State-Info, carries the state back and forth, thus\nrelieving the origin server from needing to keep an extensive per-user or \nper-connection database.  The changes required to user agents, origin \nservers, and proxy servers to support State-Info are very modest.          \n\nInternet-Drafts are available by anonymous FTP.  Login with the username\n\"anonymous\" and a password of your e-mail address.  After logging in,\ntype \"cd internet-drafts\" and then\n     \"get draft-kristol-http-state-info-00.txt\".\n Or \n     \"get draft-kristol-http-state-info-00.ps\".\nA URL for the Internet-Draft is:\nftp://ds.internic.net/internet-drafts/draft-kristol-http-state-info-00.txt\n \nInternet-Drafts directories are located at:\n                                                \n     o  Africa                                   \n        Address:  ftp.is.co.za (196.4.160.8)\n                                                \n     o  Europe                                   \n        Address:  nic.nordu.net (192.36.148.17)\n        Address:  ftp.nis.garr.it (192.12.192.10)\n                                                \n     o  Pacific Rim                              \n        Address:  munnari.oz.au (128.250.1.21)\n                                                \n     o  US East Coast                            \n        Address:  ds.internic.net (198.49.45.10)\n                                                \n     o  US West Coast                            \n        Address:  ftp.isi.edu (128.9.0.32)  \n                                                \nInternet-Drafts are also available by mail.\n                                                \nSend a message to:  mailserv@ds.internic.net. In the body type: \n     \"FILE /internet-drafts/draft-kristol-http-state-info-00.txt\".\n Or \n     \"FILE /internet-drafts/draft-kristol-http-state-info-00.ps\".\n\nNOTE: The mail server at ds.internic.net can return the document in\n      MIME-encoded form by using the \"mpack\" utility.  To use this\n      feature, insert the command \"ENCODING mime\" before the \"FILE\"\n      command.  To decode the response(s), you will need \"munpack\" or\n      a MIME-compliant mail reader.  Different MIME-compliant mail readers\n      exhibit different behavior, especially when dealing with\n      \"multipart\" MIME messages (i.e., documents which have been split\n      up into multiple messages), so check your local documentation on\n      how to manipulate these messages.\n\nFor questions, please mail to Internet-Drafts@cnri.reston.va.us.\n\n\nBelow is the data which will enable a MIME compliant mail reader \nimplementation to automatically retrieve the ASCII version\nof the Internet-Draft.\n\n--NextPart\nContent-Type: Multipart/Alternative; Boundary=\"OtherAccess\"\n\n--OtherAccess\nContent-Type:  Message/External-body;\n        access-type=\"mail-server\";\n        server=\"mailserv@ds.internic.net\"\n\nContent-Type: text/plain\nContent-ID: <19950823161652.I-D@CNRI.Reston.VA.US>\n\nENCODING mime\nFILE /internet-drafts/draft-kristol-http-state-info-00.txt\n\n--OtherAccess\nContent-Type:   Message/External-body;\n        name=\"draft-kristol-http-state-info-00.txt\";\n        site=\"ds.internic.net\";\n        access-type=\"anon-ftp\";\n        directory=\"internet-drafts\"\n\nContent-Type: text/plain\nContent-ID: <19950823161652.I-D@CNRI.Reston.VA.US>\n\n--OtherAccess--\n\n--NextPart--\n\n\n\n"
        },
        {
            "subject": "Suggestion: Partial file transfer",
            "content": "A suggestion for a new Header field to support partial file transfer. In the normal \nuse of HTTP with HTML, many (most?) transfer activities are aborted. This makes a \ncache much less efficient, as it have to re-start the file transfer from its \nbeginning.\n\nThe suggestion is to add a header field \"Starting-Position:\" to define the requested \nposition in the file.\nThe server reply header will also hold this entry, but only for requests that used \nit. The absence of the Starting-Position implies that the entire file is returned.\n\nThe Content-Length: field is left unchanged, as the entire file size.\nThe client program should validate that the Date and Content-Length fields didn't \nchange from the original file, to make sure that the partial transfer is indeed of \nthe same file.\n\nThe usage of \"Starting-Position\" should be limited to client application (not cache \nservers), and are valid for the same session only.\n\nClient program can safely use this option with any server, as it is simply ignored \nby older (current) servers, and by returning the entire file.\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "Dave and Bob,\n\nThanks for your remarks. Following is my reply, in which I have tried to \nargue strictly from the perspective of HTTP and its current mainstream use.\n\nDave Kristol <dmk@allegra.att.com> wrote:\n\n>IMO, this kind of method is outside HTTP.  I think it is best handled by\n>(conceptually, at least) an outboard application...\n>HTTP is not all things to all people, and I think one of the things it\n>is not is a real-time transport protocol.  It seems to me much better\n>to segregate such sophisticated real-time processing elsewhere.\n-------------\n\nBob Wyman <bobwyman@medio.com> wrote:\n\n>I agree with this (Dave's) position at this time.\n\n>I suggest that while Yitzhak Birk may have established a requirement for a\n>connection which provides the semantics he defines as being needed by \n>\"PLAY\", what he hasn't done is establish the requirement that HTTP be the \n>protocol which provides the required semantics. Yitzhak's only attempt at\n>rationalizing that HTTP should be the mechanism for supporting PLAY is his\n>FAQ item in which he argues between HTTP and HTML. There are other\n>alternatives... TCP/IP supports many ports -- we don't have to pump\n>*everything* through port 80...\n>...\n>Play is not the only protocol type which is not well served by HTTP today.\n>...\n>The growing popularity of HTTP is certainly compelling, however, just\n>because it does what it does doesn't mean it should be made to do everything\n>else. It would be more reasonable to define a family of protocols that\n>interwork well within the context of a single, reasonably easy to construct,\n>multi-protocol client.\n------------------------------------------\n\nClearly, it is always possible to define another protocol and use it\nto get things done. I nonetheless still argue that the semantics of\nPLAY should be \"native\" within HTTP, for reasons that have to do with\nthe way in which HTTP is commonly used and its own performance:\n\n\n- HTTP is commonly used for viewing items, some of which are retained\nby the client.\n\nFor items that only have spatial dimensions, the GET semantics are\nappropriate both for viewing and for retention. Various optimizations\nfor these items are aimed at overcoming the artificial temporal\ndimension introduced by the fact that fetching is not instantaneous.\n\nFor items that do have a built-in temporal dimension, such as video or\naudio clips, GET semantics are still appropriate when the desire is\nstrictly to fetch and retain, but PLAY semantics are the right ones for\nviewing (with or without subsequent retention).\n\nSince viewing very often precedes retention (the decision whether to\nsave may even depend on the result of the viewing) and an increasing\nfraction of material has a temporal dimension, PLAY semantics are as\nnatural as GET for the current mainstream use of HTTP. The fact that\nsuch items tend to be voluminous raises the importance of recognizing\nthe true semantics of requests for them.\n\n\n- In examining the discussions of HTTP and the proposed extensions,\nperformance is clearly of great concern to the HTTP\ncommunity. Concerns include the self-induced problems (e.g., due to an\nexcessive number of handshakes) as well as suboptimal use of shared\nresources (multiple connections for the same document, speculative\nprefetching, etc.).  As is often the case, it is somewhat difficult to\nprovide good solutions without knowing the requirements. Both server\nperformance and network performance can be improved significantly if\nthe semantics of a request are known. (This applies to both types of\nconcerns.)\n\n- I share the reluctance to turn HTTP into a \"real-time\" protocol, and\ndo not suggest to do so. \n\nUnlike interactive applications such as games or video conferencing, however,\nstreaming applications can strike a comfortable balance between delay\nin the beginning of a presentation, buffer size, bandwidth and\nglitches. Therefore, HTTP need only convey the requested semantics and\nperhaps a few simple attributes. The rest is left to the\nimplementations of the client, server and network. A simple server\nmight treat PLAY as GET. A more sophisticated one might blast an\ninitial amount of data to fill the client buffer and then try to\nstream data such that the buffer would not become empty or\noverflow. Network type and condition would also influence the\nservice. All this could be handled through optional extensions with\nsimple defaults once the semantics are there, and does not impose any\nmandatory complexities. Yet, it does enable one to do better, offering\na challenge and an opportunity. All this in the context of current\nmainstream use of HTTP.\n\n\nIn summary, I argue that the PLAY semantics for streaming (as opposed\nto truly interactive conferencing) do fall well within the natural\nscope of HTTP, even for its current mainstream use, and believe that\ntheir inclusion in HTTP would facilitate substantial and meaningful\nperformance improvements. In the absence of PLAY, I suspect that people \nwill simply continue using GET rather than develop a new protocol for \na subset of \"HTTP-like\" requests, and overcoming the resulting \nperformance problems will be more difficult. \n\n---------------------\n\nYitzhak Birk\nEE Dept, Technion - Israel Inst. of Technology  birk@ee.technion.ac.il\nPresently at HP Labs, Palo Alto.  (birk@bodega.stanford.edu, birk@hpl.hp.com)\n\n\n\n"
        },
        {
            "subject": "Re: HTTP/1.2 stuff: try it out",
            "content": "----------\n] From: Jeffrey Mogul  <mogul@pa.dec.com>\n] To: Dave Kristol  <dmk@allegra.att.com>\n] Cc:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>\n] Subject: Re: HTTP/1.2 stuff: try it out!\n] Date: Wednesday, August 23, 1995 1:56PM\n] But in the paper I plot \"requests arriving for already-open connections\"\n] vs. idle timeout.  Generally, a large fraction of the gain (almost\n] half) comes with timeouts greater than 10 seconds.  This implies\n] either of two things:\n] (1) Many clients were delaying ca. 10 seconds before retrieving\n] inlined images.\n] (2) there were a lot of subsequent hits in the 10-100 second\n] range.\n] Hypothesis #1 seems rather unlikely, so I'd bet on #2.\n\nA third hypothesis -- that the previous entity requested took more than \n10 seconds to transmit thru the network, thus delaying the arrival of \nthe following request by that amount. Is this taken into account by the \ntimeout mechanism in your implementation -- i.e., does the timeout \nperiod start when the TCP stack returns to the server from the server's \nsend() call, or when the last byte leaves the server?\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: Suggestion: Partial file transfer",
            "content": "I like the notion of partial entity transfer. However, I would prefer \nthat content-length always refer to the transmitted length of the \ncontent, for all requests.  Siince you're intorducing new header \nfields, why not add one for the total-length of the entity?\n\nThe proposal allows fetch of the tail of a file.  If we go down this \npath, why not generalize it to an arbitrary slice?\n\nIs this makes caches more efficient, why forbid its use by cache servers?\n\nPaul\n----------\n] From: Dror Tirosh  <dror@vocaltec.com>\n] To:  <netmail!http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com>;  \n<dror@vocaltec.com>\n] Subject: Suggestion: Partial file transfer.\n] Date: Saturday, August 24, 1901 10:37PM\n]\n] A suggestion for a new Header field to support partial file transfer. \nIn the normal\n] use of HTTP with HTML, many (most?) transfer activities are aborted. \nThis makes a\n] cache much less efficient, as it have to re-start the file transfer from its\n] beginning.\n\n[ excised stuff]\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "    In summary, I argue that the PLAY semantics for streaming (as\n    opposed to truly interactive conferencing) do fall well within the\n    natural scope of HTTP, even for its current mainstream use, and\n    believe that their inclusion in HTTP would facilitate substantial\n    and meaningful performance improvements. In the absence of PLAY, I\n    suspect that people will simply continue using GET rather than\n    develop a new protocol for a subset of \"HTTP-like\" requests, and\n    overcoming the resulting performance problems will be more\n    difficult.\n\nPeople will either use GET or they will use something different,\nsomething that does not yet exist.  You propose that the new thing\nbe a new method in HTTP.  Others have suggested that the new thing\nbe a new protocol.  Either way, there's a lot of new implementation\nto do.\n\nI'd think about it this way.  \"http:\" is just one of the things\na URL can start with; the world can already deal with that level\nof choice.  Is it better to add a PLAY method to HTTP, or perhaps\nto invent a new \"HTTP-like\" protocol, say \"IMP\" (for \"immediate\nplay protocol\")?\n\nWith IMP, HTML files would include both things like inlined images\n    <IMG SRC=\"http://www.unitedmedia.com/comics/dilbert/todays_dilbert.gif\">\nand inlined \"play now\" things\n    <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_movie.mpeg\">\n    <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_song.au\">\nalthough I imagine that the HTML design to take advantage of this might\nbe more complex.\n\nI suggest that using a new, separate protocol will make things\nsimpler, cleaner, and easier to implement:\n(1) We won't have to figure out how PLAY interacts with the rest\nof HTTP.\n(2) People can start working on prototype implementations of IMP\nin parallel with current and future standardization efforts of HTTP;\nit's a mistake to include an untested feature as part of the revision\nof the standard for a heavily-used system.\n(3) Proxies would have to be written or rewritten in either case,\nso why not just write a separate IMP proxy that knows how to deal\nwith multiple immediate-play streams, and may have to have a completely\ndifferent caching approach.\n(4) Some existing HTTP servers are a real mess inside.  It might be\nhard to fix them to make PLAY work well.\n(5) People who want to use a specific HTTP server could get their\nIMP server from some other source or vendor.\n(6) IMP could avoid some of the baggage that HTTP is stuck with\n(including, perhaps, single-use connections, ASCII headers, etc.)\n\nI'd bet that you could write an IMP server and modify a browser\nto support IMP with far less effort than it would take to convince\nthe HTTP working group to add PLAY to HTTP.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "Yitzhak Birk <birk@bodega.stanford.edu> wrote:\n  > [...]\n  > In summary, I argue that the PLAY semantics for streaming (as opposed\n  > to truly interactive conferencing) do fall well within the natural\n  > scope of HTTP, even for its current mainstream use, and believe that\n  > their inclusion in HTTP would facilitate substantial and meaningful\n  > performance improvements. In the absence of PLAY, I suspect that people \n  > will simply continue using GET rather than develop a new protocol for \n  > a subset of \"HTTP-like\" requests, and overcoming the resulting \n  > performance problems will be more difficult. \n\nI don't believe you can negotiate all the flow parameters in advance,\nbecause the state of the network changes, and the changes affect the\nstreaming.  Thus it's likely there will be two-way communication\nbetween client and server to adjust the data rate.  HTTP is ill-suited\nfor such communication.\n\nPeople here at Bell Labs have done some work on real-time delivery over\nthe Internet.  (See <http://www.research.att.com/~hpk/nemesis.html>.)\nThey use a separate connection in an outboard application, which was\nthe basis for my earlier description.\n\nI see that superficially the PLAY method fits into the HTTP paradigm,\nbut I think the details make the fit a poor one.  Adding a bunch of new\nheaders, and possibly client-directed flow control, means you're a good\nway toward a very different protocol.  So make it a different protocol!\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\n> Clearly, it is always possible to define another protocol and use it to\nget\n> things done. I nonetheless still argue that the semantics of PLAY should\nbe\n> \"native\" within HTTP, for reasons that have to do with the way in which\nHTTP is\n> commonly used and its own performance:\n\nHTTP performance issues can certainly be solved independent of the\nintroduction of PLAY. \n\n> - HTTP is commonly used for viewing items, some of which are retained by\nthe\n> client.\n\nThis business of \"items... which are retained\" is a very important point.\nThat is, HTTP is typically used to transfer contained objects around the\nnetwork. Objects which have size, creation dates, etc. In fact, it isn't\nonly clients that retain these objects but also a variety of proxies, caches\n, HTTP accelerators, etc. PLAY requires a completely different beast to be\nsupported.\n\nIt is important to consider the entire system here -- not just the one\ncomponent which is the HTTP protocol. Unfortunately, we don't have a\ndetailed reference model to show all the components, however, we all know\nthat the \"system\" which is the Web contains proxy-caches and HTTP\nAccelerators...\n\nWhat should a proxy-cache or HTTP accelerator do with a PLAY stream? Do we\nreally want to bog down these servers with the task of shovelling non-\ncacheable streams across the network? Do we really want cache implementors\nto have to address the service level requirements of the streams that are\nlooping through them? Isn't it reasonable to assume that streams would\nperform better if they didn't pass through the same caches and proxies that\nare designed for objects of finite dimensions?\n\nI would suggest that the cache problem alone would be enough to want to put\nstreams on a different port or channel than that which is used for the kinds\nof objects that HTTP transfers today.  There are other good arguments as\nwell...\n\n> In summary, I argue that the PLAY semantics for streaming (as opposed to\ntruly\n> interactive conferencing) do fall well within the natural scope of HTTP,\neven\n> for its current mainstream use, and believe that their inclusion in HTTP\nwould\n> facilitate substantial and meaningful performance improvements. \n\nAs mentioned before, HTTP performance can be improved and is being improved\nwithout introducing PLAY. It would seem that the \"performance improvement\"\nyou are discussing would only really be provided to users of streaming\napplications.\n\n> In the absence of PLAY, I suspect that people  will simply continue \n> using GET rather than develop a new protocol for  a subset of \n> \"HTTP-like\" requests, and overcoming the resulting  performance \n> problems will be more difficult. \n\nIf solving these problems for streaming applications is an important problem\n, people will inevitably and eventually get together to build a solution.\nThey will, in fact, probably end up building a better solution than would\narise from tacking something onto the side of HTTP. Because the semantic\nrequirements of \"documents\" and \"streams\" are so different, whatever comes\nout of a joint effort is likely to make users of either application type\nvery happy. We'll then find ourselves in interminable arguments between the\n\"stream\" people and the \"document\" people. The tremendous amount of\ndiscussion concerning building state into HTTP (for shopping bags, etc.)\nwould probably be nothing compared to what would result once PLAY shows up\nin HTTP.\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "On Thu, 24 Aug 1995, Bob Wyman wrote:\n\n> \n> This business of \"items... which are retained\" is a very important point.\n> That is, HTTP is typically used to transfer contained objects around the\n> network. Objects which have size, creation dates, etc. In fact, it isn't\n> only clients that retain these objects but also a variety of proxies, caches\n> , HTTP accelerators, etc. PLAY requires a completely different beast to be\n> supported.\n> \n\n\n> I would suggest that the cache problem alone would be enough to want to put\n> streams on a different port or channel than that which is used for the kinds\n> of objects that HTTP transfers today.  There are other good arguments as\n> well...\n> \n> \n> As mentioned before, HTTP performance can be improved and is being improved\n> without introducing PLAY. It would seem that the \"performance improvement\"\n> you are discussing would only really be provided to users of streaming\n> applications.\n> \n> \n> The tremendous amount of\n> discussion concerning building state into HTTP (for shopping bags, etc.)\n> would probably be nothing compared to what would result once PLAY shows up\n> in HTTP.\n> \n\n- Whether we like it or not, the different beasts do and will share the \nnetwork. There (e.g., ATM forum), the issue of coexistance of traffic of\ndifferent types is being addressed. Among other features, ATM supports \nbandwidth reservation. \n\n- The beasts (items) themselves aren't different. In the case of PLAY as \nproposed, the requested item is also a wrapped object of known size. \nHowever, something is known about the requested delivery manner which may \nhave important ramifications to the scheduling of server resources and \nuse/reservation of network resources. \n\n\n- The claim that HTTP performance problems can be solved without PLAY is \nonly partly correct: since the optimal (in an informal sense) \nallocation and scheduling of the server and of network resources \ndepend on the performance measures, you should be able to do better \nif you knew what those measures were for each request. (E.g., latency \nuntil last byte vs. until first byte vs. first byte + rate).\n\n- I am aware of the complexities of caches and proxies. However, taking \nthe \"half-full glass\" approach, any use that is made of the knowledge of \nthe true semantics of a request will be better than nothing. Also, caching \nconsiderations are not different than the usual, since these are the same \nobjects that you are GETting today. I am only proposing to supply a \nuseful hint.\n\nMy suggestion is to start with something, mandating only implementation \nstubs that implement GET in response to PLAY, and let things evolve. This \ndoes not preclude the eventual development of a replacement protocol.\n\n-----------------\nYitzhak Birk\nEE Dept, Technion - Israel Inst. of Technology  birk@ee.technion.ac.il\nPresently at HP Labs, Palo Alto.  (birk@bodega.stanford.edu, birk@hpl.hp.com)\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "On Thu, 24 Aug 1995, Jeffrey Mogul wrote:\n\n> People will either use GET or they will use something different,\n> something that does not yet exist.  You propose that the new thing\n> be a new method in HTTP.  Others have suggested that the new thing\n> be a new protocol.  Either way, there's a lot of new implementation\n> to do.\n> \n> I'd think about it this way.  \"http:\" is just one of the things\n> a URL can start with; the world can already deal with that level\n> of choice.  Is it better to add a PLAY method to HTTP, or perhaps\n> to invent a new \"HTTP-like\" protocol, say \"IMP\" (for \"immediate\n> play protocol\")?\n> \n> With IMP, HTML files would include both things like inlined images\n>     <IMG SRC=\"http://www.unitedmedia.com/comics/dilbert/todays_dilbert.gif\">\n> and inlined \"play now\" things\n>     <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_movie.mpeg\">\n>     <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_song.au\">\n> although I imagine that the HTML design to take advantage of this might\n> be more complex.\n> \n> I suggest that using a new, separate protocol will make things\n> simpler, cleaner, and easier to implement:\n>... \n\nI understand the advantages of separating the effort for supporting PLAY \nfrom the HTTP standardization. However, the separation may not be as clean as \nit appears from your example. This is because PLAY/GET is a property of \nthe request, not of the item being requested (though PLAY may not be \nrelevant to certain item types). For example, caches for the \ndifferent protocols may wish to share data. I can even envision cases in \nwhich a proxie responds to a PLAY request with a smooth stream, but \nsends on a GET request to the original server over a very-high-speed \nbackbone. Can be done with separate protocols, of course, but perhaps more \nnatural as a method-change within the same protocol.\n\nI also expect any work on a separate protocol to take much longer than \nthe inclusion of a minimal version of PLAY in HTTP. This could subsequently \nevolve into a more powerful one with high-quality implementations or be \nsucceeded by a separate protocol. I expect the traffic pattern on the \nnetwork as well as server scheduling to be affected substantially even by \nthe simplest form of PLAY whenever viewing of clips accounts for a \nsubstantial fraction of load.\n\n-------------------------------------\n\nYitzhak Birk\nEE Dept, Technion - Israel Inst. of Technology  birk@ee.technion.ac.il\nPresently at HP Labs, Palo Alto.  (birk@bodega.stanford.edu, \nbirk@hpl.hp.com)  \n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "Return-Path: birk@bodega.stanford.edu\nReceived: by acetes.pa.dec.com; id AA03875; Thu, 24 Aug 95 17:04:16 -0700\nReceived: by pobox1.pa.dec.com; id AA22755; Thu, 24 Aug 95 17:04:09 -0700\nReceived: from bodega.Stanford.EDU by mail2.digital.com; (5.65 EXP 4/12/95 for V3.2/1.0/WV)id AA19910; Thu, 24 Aug 1995 16:56:09 -0700\nReceived:  by bodega.stanford.edu (5.65/25-eef) id AA09233; Thu, 24 Aug 1995 16:56:13 -0700\nDate: Thu, 24 Aug 1995 16:56:12 -0700 (PDT)\nFrom: Yitzhak Birk <birk@bodega.stanford.edu>\nTo: Jeffrey Mogul <mogul@pa.dec.com>\nCc: http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com\nSubject: Re: Proposal: a PLAY or STREAM method for http/1.1 \nIn-Reply-To: <9508242041.AA02691@acetes.pa.dec.com>\nMessage-Id: <Pine.ULT.3.91.950824153853.8194C-100000@bodega.stanford.edu>\nMime-Version: 1.0\nContent-Type: TEXT/PLAIN; charset=US-ASCII\n\n\n\nOn Thu, 24 Aug 1995, Jeffrey Mogul wrote:\n\n> People will either use GET or they will use something different,\n> something that does not yet exist.  You propose that the new thing\n> be a new method in HTTP.  Others have suggested that the new thing\n> be a new protocol.  Either way, there's a lot of new implementation\n> to do.\n> \n> I'd think about it this way.  \"http:\" is just one of the things\n> a URL can start with; the world can already deal with that level\n> of choice.  Is it better to add a PLAY method to HTTP, or perhaps\n> to invent a new \"HTTP-like\" protocol, say \"IMP\" (for \"immediate\n> play protocol\")?\n> \n> With IMP, HTML files would include both things like inlined images\n>     <IMG SRC=\"http://www.unitedmedia.com/comics/dilbert/todays_dilbert.gif\">\n> and inlined \"play now\" things\n>     <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_movie.mpeg\">\n>     <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_song.au\">\n> although I imagine that the HTML design to take advantage of this might\n> be more complex.\n> \n> I suggest that using a new, separate protocol will make things\n> simpler, cleaner, and easier to implement:\n>... \n\n    I understand the advantages of separating the effort for supporting\n    PLAY from the HTTP standardization. However, the separation may not\n    be as clean as it appears from your example. This is because\n    PLAY/GET is a property of the request, not of the item being\n    requested (though PLAY may not be relevant to certain item types).\n\nThis can be handled by something like:\n   <PLAY SRC=\"imp://www.unitedmedia.com/comics/dilbert/todays_song.au\"\n         ALT=\"http://www.unitedmedia.com/comics/dilbert/todays_song.au\">\ngiving the browser a choice of mechanisms.\n\n    For example, caches for the different protocols may wish to share\n    data.\n\nIt's clearly possible to use one proxy to handle multiple protocols\n(HTTP, Gopher, FTP, etc.) so adding one more protocol to the set\nshould not be impossible.  In other words, my proposal makes it\n*possible* to share the cache between HTTP and IMP, just not *required*.\n\n    I also expect any work on a separate protocol to take much longer\n    than the inclusion of a minimal version of PLAY in HTTP.\n\nEver looked at the source code for an HTTP server or proxy?  Some\nof them are pretty twisted.  I would bet that you could write a new\nIMP server in less time than it would take to understand an existing\nHTTP server.\n\nAnd it will certainly be easier to write separate specs for IMP and\nHTTP.  Especially because the working group is unlikely to support\nputting an untested design into the HTTP spec.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "On Thu, 24 Aug 1995, Dave Kristol wrote:\n\n> I don't believe you can negotiate all the flow parameters in advance,\n> because the state of the network changes, and the changes affect the\n> streaming.  Thus it's likely there will be two-way communication\n> between client and server to adjust the data rate.  HTTP is ill-suited\n> for such communication.\n> \n\nI believe ATM supports bandwidth reservation and should be able to \nguarantee it.\n\n> People here at Bell Labs have done some work on real-time delivery over\n> the Internet.  (See <http://www.research.att.com/~hpk/nemesis.html>.)\n> They use a separate connection in an outboard application, which was\n> the basis for my earlier description.\n> \n> I see that superficially the PLAY method fits into the HTTP paradigm,\n> but I think the details make the fit a poor one.  Adding a bunch of new\n> headers, and possibly client-directed flow control, means you're a good\n> way toward a very different protocol.  So make it a different protocol!\n> \n\nTo best illustrate why I think it may be useful to \ninclude PLAY in HTTP (not to the exclusion of subsequent better and \nseparate solutions), let me use a simple example.\n\nSuppose a mineral-water distribution company receives one order for \n100K bottles and one hundred 2K-bottle orders. My claim is that \nthe company would really appreciate being told that the 100K bottles are \nneeded at the rate of 2K per day and not ASAP for a big bash, and that the \nclient can store up to 5K. The extent to which the company is actually \nable to take advantage of this knowledge would depend on many factors, but I \nthink it is pretty clear that one should provide the information. Also, \nit is clear that trickling the bottles of the large order at exactly 2K \nper day, or 2K/24 per hour, is not required, and loose scheduling is \npossible.\n\n\nIn summary, I have tried to make several points:\n\n- Within the simple, common use of HTTP, there are sufficiently important \ncases in which large amounts of data are requested, the semantics of the \nrequest differ dramatically from the usual \"everything ASAP\", yet are \nalso very different from \"real time\" with the associated strictness and \ndifficulty of implementation.\n\n- Hiding the semantics of these requests from the \"system\" affects \nperformance, since the scheduling and resource-allocation decisions \nthroughout the system may be based on the wrong performance measures for \nthose requests.\n\n- The issue can be addressed in various ways, levels of quality and \ndifficulty. Doing something is better than nothing, and it is not an \"all \nor none\" situation.\n\nHaving said all this, I hope to have shed light on an issue that is \nrelevant to HTTP since it influences its performance as perceived \nby its users as well as its \"good citizenship\" on the network.  As such, \nthe HTTP community should be interested in having something done about \nit.  I hope to have convinced people of this, regardless of who should do \nthis and whether it should be included in the HTTP protocol itself.\nRelated to the question of whether or not this should be done within \nHTTP, it is perhaps time to better define the scope of HTTP, for example \nby stating explicitly the (assumed) semantics of HTTP requests. \n\n\nYitzhak Birk\nEE Dept, Technion - Israel Inst. of Technology  birk@ee.technion.ac.il\nPresently at HP Labs, Palo Alto.  (birk@bodega.stanford.edu, birk@hpl.hp.com)\n\n\n\n"
        },
        {
            "subject": "Possible optimization to StateInfo proposa",
            "content": "I mentioned this before, obliquely in a response on another topic, but\nif I'm right, what follows can significantly reduce the potential\nperformance problems with DMK's State-Info proposal.\n\nIn the current version of the proposal (Dave, maybe you should\nincrement your file number when you change the contents) DMK states\nthat caching proxies \"must pass along a State-Info request header from\nthe requesting client to the next server, even if it has cached the\nrequested resource locally.\"\n\nAt first this seemed right, and in fact I made some arguments about\nthe potential performance impact.  But somebody please tell me what is\nwrong with the following reasoning: if a resource is in a cache, it\nshould only be there because it is possible to \"legally\" fetch the\nresource from the cache without contacting the origin server.  If this\nwere not the case, the document should not be cached.  Any IDEMPOTENT\nmethod by definition has no side effects at the origin server.  Only\nidempotent requests can be served from caches.  Any non-idempotent\nmethod is not allowed to retrieve a cached copy of a resource -- it\nmust make a request to the origin server.  Key conclusion: An\nidempotent request such as GET, which produces no side effects, cannot\naffect the state of a stateful dialog.  Therefore, it seems\nunnecessary for requests that can be satisfied out of a cache to pass\nthrough any information to the next server.\n\nThis, of course, makes it even more imperative that caches not serve\ncached documents they shouldn't.\n\nWith the recent addition to the proposal that clients stay in the\n\"have state-info\" state when there is no state-info in the server's\n(or cache's) response, this means that caches need not \"reflect\"\nstate-info either.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "-- [ From: Bob Wyman * EMC.Ver #2.5.02 ] --\n\n> With the recent addition to the proposal that clients stay in the \"have\n> state-info\" state when there is no state-info in the server's (or cache's)\n> response, this means that caches need not \"reflect\" state-info either.\n\nActually, \"reflect\" isn't the right word here. It's something more like\n\"forward.\"\n\nI've been awfully tempted on many occaisions to make the same suggestion\nthat you have, with much the same reasoning. However, there are at least two\nproblems with the proposal:\n1.  HTTP V1.0 only says that the idempotent nature of GET and HEAD is \"by\nconvention\" -- it doesn't really state this as a requirement. Should we\nwrite protocol that assumes the \"convention\" is being followed?\n2. If you remove the requirement for forwarding of State-Info, you break\nthe ability of servers to use State-Info for click tracking. This is,\nunfortunately, one of the applications that State-Info is supposed to\nsupport. If it wasn't for click tracking, I would whole-heartedly support\nyour suggestion.\n\nbob wyman\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "  > With the recent addition to the proposal that clients stay in the \"have\n  > state-info\" state when there is no state-info in the server's (or cache's)\n  > response, this means that caches need not \"reflect\" state-info either.\n\n  Actually, \"reflect\" isn't the right word here. It's something more like\n  \"forward.\"\n\nWell, I really meant reflect, as in \"send back to the requestor as is\".\nBut the proxy wouldn't have to *forward* state-info either.\n\n  I've been awfully tempted on many occaisions to make the same suggestion\n  that you have, with much the same reasoning. However, there are at least two\n  problems with the proposal:\n\n  1.  HTTP V1.0 only says that the idempotent nature of GET and HEAD is \"by\n  convention\" -- it doesn't really state this as a requirement. Should we\n  write protocol that assumes the \"convention\" is being followed?\n\nLet's put it this way: since we allowing caching, we have already\nimplicitly written this into the protocol.  Caching results of\nnon-idempotent methods is functionally different from not caching\nthem. The cache interferes in the semantics.  If this happens, you\nhave lost.  So if the spec doesn't make it clear that certain methods\nhave to have no side effects so that their results can be cached, the\nspec needs work.\n\n  2. If you remove the requirement for forwarding of State-Info, you break\n  the ability of servers to use State-Info for click tracking. This is,\n  unfortunately, one of the applications that State-Info is supposed to\n  support. If it wasn't for click tracking, I would whole-heartedly support\n  your suggestion.\n\n  bob wyman\n\nHmmm.  Bug or feature?  Personally I think statefulness is important\nenough to support well even if click tracking isn't supported by the\nsame mechanism.  \n\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "You know, the \"HTTP/1.0 is state of the world\" idea has some good sides.\nNow we can say, with a fair degree of sense, that \"HTTP/1.0 says that\nGET and HEAD are mostly idempotent\", while HTTP/1.1 says that\n\"GET and HEAD are idempotent, and so can be cached, UNLESS the following\nparameters are put on the document by the server....\"\n\nSince 1.0 is descriptive, it has to say that the idempotency of GET\nis \"by convention\", while 1.1 is prescriptive, and can cast the idempotency\ninto stone (rather soft stone, but stone nonetheless).\n\n             Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "> \n> - Whether we like it or not, the different beasts do and will share the \n> network. There (e.g., ATM forum), the issue of coexistance of traffic of\n> different types is being addressed. Among other features, ATM supports \n> bandwidth reservation. \n> \nReality insert:\nThe different beasts will share a *network*, but not necessarily a network\n*connection*.\n\nI believe that if you want to PLAY, say, an MPEG, you would want a connection \nthat:\n\n- has a separate IPv6 flow ID, so that the applications can negotiate with\n  the routers about it (most probably either the workstation or the server is\n  NOT directly on the same ATM netowrk, so an extra ATM channel won't be\n  end-to-end. Don't use ATM as an argument without a clear picture of how ATM\n  and IP will coexist!)\n- is carried over UDP, not TCP, so that data delayed for too long will be\n  dropped, instead of delaying the rest\n- features recipient pushback, so that when the loss rate on your 400 Kbit/s\n  CD-ROM quality playback goes above 90%, you switch to 16 Kbits/sec GSM\n  encoding, giving tinny sound rather than garbled sound\n- allows recipient abort, so that when you discover that \"A nightmare on\n  Elm street\" isn't about tree conservation, you don't have to transfer\n  the rest of the movie.\n\nI don't think any of these things can be optimally achieved using HTTP/1.1;\nI suggest you concentrate on defining a play: URL instead.\n\nOh, and BTW: playback over the network is being addressed by the AVT WG,\nand IP over ATM is being addressed in the IPATM group. Resource reservation\nis done in the RSVP group.\nI will certainly ask all these groups' chairs to comment once you come\nback with a Play: URL spec; it might be a Good Thing to read their stuff\nfirst. (At the moment, URL specifications must be defined in standards-track \ndocuments, which must pass through the Apps area directors)\n\n    harald A\n\n\n\n"
        },
        {
            "subject": "Maurizio Codogno: Re: '205 Reset Document' (was Any more comments?",
            "content": "This was sent (probably erroneously) to the http-wg-REQUEST address.\n\n-- ange -- <><\n\nange@hplb.hpl.hp.com\n\n------- Forwarded Message\n\nDate:    Fri, 25 Aug 1995 08:38:40 +0200\nFrom:    mau@beatles.cselt.stet.it (Maurizio Codogno)\nTo:      http-wg-request@cuckoo.hpl.hp.com\nSubject: Re: '205 Reset Document' (was Any more comments?)\n\n hedlund@best.com (Marc Hedlund) suggests ;\n\n% '205 Reset Document', suggested in a followup, would be an explicit request\n% for this behavior.\n% \n% The idea was not to give the user another choice, it was to provide a\n% response that says, \"that last one was okay, go ahead with the next one,\n% and keep using the same form.\"  The user won't see a different result --\n% the same resetting can be accomplished, for instance, by resending the\n% blank form.  The difference would be in the speed of the response.\n\nI think it is an excellent idea - I was noticing that it's not really\nuser friendly to ask people to reload a page.\n\nI have a related question: What happens if a server does not recognize \na status code?\n\n.mau.\n\n------- End of Forwarded Message\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Shel Kaphan:\n>  [Bob Wyman:]\n>  2. If you remove the requirement for forwarding of State-Info, you break\n>  the ability of servers to use State-Info for click tracking. This is,\n>  unfortunately, one of the applications that State-Info is supposed to\n>  support. If it wasn't for click tracking, I would whole-heartedly support\n>  your suggestion.\n>\n>Hmmm.  Bug or feature?  Personally I think statefulness is important\n>enough to support well even if click tracking isn't supported by the\n>same mechanism.  \n\nGeneral support for click tracking should be *separated* from\nstateful dialog support mechanisms like state-info.\n\nThe reason for this is privacy: a user should have the option of\ndisabling the standard click tracking mechanism for *all* servers,\nwhile still being able to choose to engage in a stateful dialog with\n*some* servers (engaging in a dialog with a server implies allowing\nthat particular server to track clicktrails).\n\nDave proposes an `always do state-info/never do state info'\nconfiguration option in browsers which would apply to all servers at\nonce.  I don't feel that this is a good enough solution to privacy\nproblems.\n\nAs a made-up statistic, under Dave's proposal, 90% of users will set\ntheir browsers on `always do state-info', because they want to use at\nleast one stateful server on the web.  But by doing so, these 90% of\nusers allow 100% of server administrators (not just the administrator\nof the stateful site the user is interested in) to gather clicktrail\nstatistics by using (abusing) the enabled state-info support in their\nbrowsers.  60% of the 90% of users does not care about this, the\nremaining 30% would have rather had a web protocol that would give\nbetter privacy protection.\n\nThis abuse of stateful dialog support by server administrators to get\nbetter statistics without offering a more interesting (stateful) site\nin return is one of the main issues addressed in my `non-persistent\ncookie proposal', posted here a some weeks ago.\n\nI think I'll be posting a new version of this proposal in the near\nfuture, even though Dave's changes to his proposal, and my\n(non-published) changes to my proposal have made the two much more\nsimilar than they were originally.  The only differences remaining are\nprivacy protection mechanisms and the requirement that caches always\nforward state-info.\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Looking for draft version of http 1.",
            "content": "I am new to the http world and looking for the draft version 1.1.\nI have not been able to locate this version, and thought that I \nmight have found it at a site, but it is not available yet.\n\nSpecifically, I am looking at the push server capability aspects of the\nprotocol.\n\nWould appreciate any help on finding http 1.1 or this new feature.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "According to Harald.T.Alvestrand@uninett.no:\n>\n> You know, the \"HTTP/1.0 is state of the world\" idea has some good sides.\n> Now we can say, with a fair degree of sense, that \"HTTP/1.0 says that\n> GET and HEAD are mostly idempotent\", while HTTP/1.1 says that\n> \"GET and HEAD are idempotent, and so can be cached, UNLESS the following\n> parameters are put on the document by the server....\"\n> \n> Since 1.0 is descriptive, it has to say that the idempotency of GET\n> is \"by convention\", while 1.1 is prescriptive, and can cast the idempotency\n> into stone (rather soft stone, but stone nonetheless).\n> \n\nWhat do you propose for the tens of thousands of documents which are\nstatic except for containing a counter?  I think a lot of maintainers\nare going to want to cache them, but they aren't idempotent GETs.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "Yitzhak Birk <birk@bodega.stanford.edu> wrote on Thu, 24 Aug 1995:\n  > On Thu, 24 Aug 1995, Dave Kristol wrote:\n  > \n  > > I don't believe you can negotiate all the flow parameters in advance,\n  > > because the state of the network changes, and the changes affect the\n  > > streaming.  Thus it's likely there will be two-way communication\n  > > between client and server to adjust the data rate.  HTTP is ill-suited\n  > > for such communication.\n  > > \n  > \n  > I believe ATM supports bandwidth reservation and should be able to \n  > guarantee it.\nATM is only one medium that supports the Internet.  Please don't base\nyour proposal on an assumption that it will ride on ATM.  However, as\nHarald Alvestrand pointed out, the RSVP working group is working on\nresource reservation (but only in the context of multicast, I think).\n\nAlso, even if you reserve bandwidth, a receiver may still want to send\nflow control information to the sender.  Imagine a VCR-like control on\nplayback.  The user wants to stop/pause/rewind/play.  You can't do that\nif content keeps flowing down the pipe uncontrolled.  HTTP is ill-suited\nfor sending the control messages.\n  > \n  > [Example about mineral water distribution deleted.  This must be\n    California. :-) ]\n\nI, and others, support the concept of a protocol that allows large\nobjects to be played out in real time.  We just disagree with you that\nsupport for it must be part of HTTP.  You think it should.  We don't.\n  > \n  > In summary, I have tried to make several points:\n  > \n  > - Within the simple, common use of HTTP, there are sufficiently important \n  > cases in which large amounts of data are requested, the semantics of the \n  > request differ dramatically from the usual \"everything ASAP\", yet are \n  > also very different from \"real time\" with the associated strictness and \n  > difficulty of implementation.\nOkay, maybe \"real-time\" is a poor term to use, because the data are already\nrecorded, and their playback rate can be controlled.  So there are two\nclasses of object:\n1) those that have no time-dependency, for which anytime delivery is okay.\n2) those that have a time-dependency in their presentation\nI assert that HTTP is well suited for the former and not for the latter\n(nor should be).  I will agree with you that there is a similarity, in\nthat in each case you want to receive the entire object.\n  > \n  > - Hiding the semantics of these requests from the \"system\" affects \n  > performance, since the scheduling and resource-allocation decisions \n  > throughout the system may be based on the wrong performance measures for \n  > those requests.\nI don't think anyone has said the semantics of the requests should be\nhidden from the system.  They (we) have said this kind of request lies\noutside HTTP.\n  > \n  > - The issue can be addressed in various ways, levels of quality and \n  > difficulty. Doing something is better than nothing, and it is not an \"all \n  > or none\" situation.\n  > \n  > Having said all this, I hope to have shed light on an issue that is \n  > relevant to HTTP since it influences its performance as perceived \n  > by its users as well as its \"good citizenship\" on the network.  As such, \n  > the HTTP community should be interested in having something done about \n  > it.  I hope to have convinced people of this, regardless of who should do \n  > this and whether it should be included in the HTTP protocol itself.\n  > Related to the question of whether or not this should be done within \n  > HTTP, it is perhaps time to better define the scope of HTTP, for example \n  > by stating explicitly the (assumed) semantics of HTTP requests. \n\nAll right thinking people (:-) care about performance, quality of service,\nand other motherhood issues for the Internet.  We differ on whether HTTP\nis the new univeral protocol.\n\nI say it isn't.  I think something like PLAY is a good idea, but outside\nHTTP.  I don't think you can arrange a playout protocol without\nfeedback from the receiver to the sender to control the data rate.\nHTTP does not lend itself to such two-way traffic.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Shel Kaphan <sjk@amazon.com> wrote:\n  > I mentioned this before, obliquely in a response on another topic, but\n  > if I'm right, what follows can significantly reduce the potential\n  > performance problems with DMK's State-Info proposal.\n  >                                             Key conclusion: An\n  > idempotent request such as GET, which produces no side effects, cannot\n  > affect the state of a stateful dialog.  Therefore, it seems\n  > unnecessary for requests that can be satisfied out of a cache to pass\n  > through any information to the next server.\n  > \n  > This, of course, makes it even more imperative that caches not serve\n  > cached documents they shouldn't.\n  > \n  > With the recent addition to the proposal that clients stay in the\n  > \"have state-info\" state when there is no state-info in the server's\n  > (or cache's) response, this means that caches need not \"reflect\"\n  > state-info either.\n\nThis idea sounds promising.  Let me expand a bit from an operational\nstandpoint.  Remember the recent change whereby a user agent retains\nits State-Info if it receives no State-Info from a server.  If caching\nproxies followed the rule that they cached only those documents for\nwhich they received no State-Info from the origin server, and if origin\nservers were careful to send State-Info only when it changed, I believe\ncaching proxies would be able to cache anything cachable (in the\nState-Info context).\n\nUpon getting a one, a caching proxy could satisfy a request for a\nresource from the cache if there were a fresh one available, whether or\nnot the request contained a State-Info request header.  (That's a\nchange, and it would indeed enhance caching.)\n\n\nThe idea of an idempotent method could be expanded to imply that, not\nonly does the resource not change, but its State-Info does not change\neither.  (I guess that's obvious if you bundle State-Info into a\nlarger concept of \"the resource\".)\n\nNice idea!  If the dust settles without major objections, I'll add this\nto a state-info-01 I-D.  That will be a couple of weeks, after some\nvacation.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Bob Wyman <bobwyman@medio.com> wrote:\n  > [...]\n  > 2. If you remove the requirement for forwarding of State-Info, you break\n  > the ability of servers to use State-Info for click tracking. This is,\n  > unfortunately, one of the applications that State-Info is supposed to\n  > support. If it wasn't for click tracking, I would whole-heartedly support\n  > your suggestion.\n\nClicktrails were a fallout from the original State-Info proposal, but\nthey weren't the main focus, and I would be willing to sacrifice them.\nI would support a separate mechanism.\n\nNote that clicktrails raise the same objection that State-Info\nforwarding raised:  each request to a caching proxy would require a\nconnection from the proxy to the server, even if the proxy could\nsatisfy the request.  That's one reason people objected to the\nState-Info forwarding.  Other people have been discussing (on\nwww-talk?) ways for origin servers to request statistics from proxies\nregarding (cache) hits for their server.  That may be a more fruitful\napproach to gathering the statistics.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "John Franks writes:\n...\n > What do you propose for the tens of thousands of documents which are\n > static except for containing a counter?  I think a lot of maintainers\n > are going to want to cache them, but they aren't idempotent GETs.\n > \n > John Franks\n\nThis is a \"problem\" right now, even without State-Info.  If a resource is\ncacheable, then the counter will not be incremented on each GET, due\nto cache hits.\n\nIt is, and will have to remain, in the hands of the server operator\nhow to treat cases like this.  The server of a particular resource can\nuse various headers to control cacheability (Date, Last-modified, Expires,\nPragma: no-cache, Pragma: private...any others I missed???).  If the\nresource is cacheable, the server operator can't expect to maintain a\nstateful dialog of any sort using that document (especially if caches\ndon't pass through State-Info or something like it).\n\nThe idea of idempotence probably needs to be somewhat tempered. It's\nreally only *significant* side effects we have to worry about.  If a\ncounter doesn't get incremented, that may (or may not) be deemed\nsignificant to the state of a dialog.  If a server delivers cacheable\ndocuments that contain counters, the cacheability means the server\ndoesn't care that much if the counter is not incremented each time.\n\nSo, there probably shouldn't be a blanket rule about how all GETs are\nto be considered idempotent -- that can remain a convention -- but\nrather, that GETs (and HEADs) *that return cacheable results* are to\nbe considered idempotent.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Koen Holtman writes in <199508251129.NAA01966@wswiop05.win.tue.nl>:\n>As a made-up statistic, under Dave's proposal, 90% of users will set\n>their browsers on `always do state-info', because they want to use at\n>least one stateful server on the web.  But by doing so, these 90% of\n>users allow 100% of server administrators (not just the administrator\n>of the stateful site the user is interested in) to gather clicktrail\n>statistics by using (abusing) the enabled state-info support in their\n>browsers.  60% of the 90% of users does not care about this, the\n>remaining 30% would have rather had a web protocol that would give\n>better privacy protection.\nMe for one.\n======================================================================\nMark Fisher                            Thomson Consumer Electronics\nfisherm@indy.tce.com                   Indianapolis, IN\n\n\n\n"
        },
        {
            "subject": "Re: Looking for draft version of http 1.",
            "content": ">I am new to the http world and looking for the draft version 1.1.\n>I have not been able to locate this version, and thought that I \n>might have found it at a site, but it is not available yet.\n\nIt is not available yet.  Since I seem to have come down with a cold\nimported from California, it probably won't be available until Monday.\n\n>Specifically, I am looking at the push server capability aspects of the\n>protocol.\n>\n>Would appreciate any help on finding http 1.1 or this new feature.\n\nThat feature is documented somewhere at Netscape HQ.  I have not\nreceived any requests to include that feature in HTTP/1.1.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Looking for draft version of http 1.",
            "content": "On Fri, 25 Aug 1995, Roy Fielding wrote:\n> >Specifically, I am looking at the push server capability aspects of the\n> >protocol.\n> >\n> >Would appreciate any help on finding http 1.1 or this new feature.\n> \n> That feature is documented somewhere at Netscape HQ.  I have not\n> received any requests to include that feature in HTTP/1.1.\n\nIt's also not an HTTP issue.  server-spewHHpush is implemented as \na multipart object with a particular unregistered content type \n(multipart/x-mixed-replace), and since HTTP can transport any MIME \nobject, it's no problem.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Proposal: a PLAY or STREAM method for http/1.",
            "content": "On Thu, 24 Aug 1995, Jeffrey Mogul wrote:\n> I'd bet that you could write an IMP server and modify a browser\n> to support IMP with far less effort than it would take to convince\n> the HTTP working group to add PLAY to HTTP.\n\nParticularly with Java.  New arbitrary protocols are just another applet.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Dave Kristol:\n>\n\n [dropping the requirement that proxy caches must always forward\n state-info headers from clients]\n\n>Nice idea!  If the dust settles without major objections, I'll add this\n>to a state-info-01 I-D.  That will be a couple of weeks, after some\n>vacation.\n\nDropping the state-info forwarding requirement would make your\nstate-info-01 almost identical to the planned second version of my\nnon-persistent cookie proposal.  I guess this means that I don't have\nto write my second version anymore.\n\nLet me try to summarize the state-info/caching requirements we seem to\nhave converged at, in the form of modifications to\ndraft-kristol-http-state-info-00.txt.  Dave, feel free to cut and\npaste from the text below if you can use it.\n\n[Note to Daniel W. Connolly, added later: I just realized that a lot\nof the material in Section 4.3 below could go straight into a \"HTTP\nCaching: rules and Heuristics\" document.  Feel free to cut and paste.]\n\n----------snip----------\n\n1.  ABSTRACT\n\n[Delete the discussion of library browsing, this is no longer directly\nsupported if state-info is not always passed though.  (However, there\nare still ways to implement library browsing, see 5.3 below)]\n\n[...]\n\n4.  PROPOSAL OUTLINE\n\n[...]\n\n4.1  Origin Server Role\n\n[...]\n\nAn origin server may only include State-Info headers in responses to\nnon-idempotent requests.  (Non-idempotent requests are all request\nthat do not use the GET and HEAD methods)\n\n[...]\n\n4.2  User Agent Role\n\n[...]\n\nFor reasons of privacy protection (see Section 6), a user agent should\nignore State-Info headers contained in responses to idempotent (GET\nand HEAD) requests.\n\nA user agent in the state ``have state-info'' should include a\nState-info request header in all requests to the origin server, whether\nthese are idempotent or not.\n\nCaches in user agents should be careful to implement the caching\nsemantics defined in the HTTP protocol, especially when handling\nrequests or responses containing State-Info headers.\n\nIf the user agent allows the user to configure the its cache to\n\n  `check the for validity of document (i.e. issue conditional get if\n   expired) only once per session',\n\nthis configuration option (which makes the user agent violate the HTTP\nspecification anyway) should not override HTTP cache semantics for\ntransactions where requests or responses containing State-Info headers\nare involved, as this will make stateful dialogs impossible or, worse,\ndangerously unreliable.\n\n[...]\n\n4.3  Caching Proxy Role\n\nCaches that conform completely to the (draft) HTTP 1.0 or 1.1\nspecification need not be changed to support State-Info.\n\nAs per the requirements in the HTTP 1.0 and 1.1 drafts, caching\nproxies\n\n  + must never cache responses to non-idempotent requests\n    (note that the drafts probably need to make this requirement\n    more explicit)\n\n  + must not cache a response to an idempotent request if the response\n    contains a Pragma or Expires header with a value that disallows\n    caching\n\nThus, origin servers can expect that proxies forward to them:\n\n - all non-idempotent requests (which may be carrying State-Info\n   headers) issued by user agents\n\n - all idempotent requests (which may be carrying State-Info headers)\n   for every `dynamic' URI D issued by user agents.  An URI D is\n   `dynamic' if the server has consistently put Pragma or Expires\n   headers disallowing caching in every response message to\n   non-idempotent requests for URI D.  Proxies may `downgrade' normal\n   GET requests to conditional GET request when doing the forwarding.\n\nAn example of a `dynamic', idempotent URI is a `shopping basket\ncontents URI' will typically be accessed with the GET method: the link\nto the shopping basket page will be a normal HTML <A HREF=...> link,\nit need not be a form submit button.\n\nOrigin servers can only change the session state (State-Info value\nstored by a user agent) in response to non-idempotent request done by\nuser agents.  However, session-state is not identical to server state\n(the mapping URI->entity is part of the server state). Proxy and user\nagent (cache) authors should be aware that server may *at any time*\nchange the entity bound to any `dynamic' URI.  (An example would be a\n`chat page' under a dynamic URI that changes because another user\nwrites on it, or a `stock quotes' page that is dynamically updated.)\n\nProxy caches that, for whatever reason, are unwilling or unable *not*\nto cache a `dynamic' entity belonging to an URI D should, if\nState-Info headers were present in the request or response for D,\nreturn a HTTP error code 501 (Not Implemented) to the requesting\nclient.\n\n[Note: Below, I distinguish between not conforming to the HTTP spec,\nwhich is bad, and breaching internet etiquette, which is much worse.\nFor example, a user agent that sends out a badly constructed\nUser-Agent: header may not conform to the HTTP spec, but does not\ncommit a grave breach of internet etiquette.]\n\nServing an outdated (incorrectly cached) dynamic response instead of\ngiving an error code is completely unacceptable behavior, because it\nmay break the synchronization between the session state maintained by\nthe server and the user's view of that state.  Willingly breaking this\nsynchronization should be considered as grave a breach of internet\netiquette, as bad as willingly changing the contents of relayed IP\npackets belonging to a telnet session.\n\n\n\n5.  IMPLEMENTATION CONSIDERATIONS\n\n[...]\n\n5.3 Browsing History Tracking\n\nThe state-info facilities only allow origin servers to track user\naccess to non-idempotent and `dynamic' idempotent URI's.  A `library'\nor `magazine browsing' server may want to track all URI's accessed by\nthe user, allowing it to show a list of articles looked at already.\nThis tracking could be accomplished by making all pages `dynamic', or\nby including a small `dynamic' inline picture on every page.  However,\nif neither the user agent nor any proxy close to the user agent has\nconditional GET capability, this technique may cause an unacceptably\nlarge amount of web traffic to be generated.\n\n\n6. PRIVACY\n\n[...]\n\nThe requirement on user agents to ignore all State-Info headers\ncontained in responses to idempotent requests (GET, HEAD) helps to\nprotect the privacy of the user.\n\nA service provider wanting to abuse the State-Info facilities to track\nthe path of each user through the server first has to get the user to\nclick a form submit button or issue another browsing command resulting\nin the sending of a non-idempotent request.\n\nThus, a user that does only `regular' browsing by clicking HTML <A\nHREF> links (or having the user agent resolve inline pictures or\nidempotent request redirections) never has to worry about getting\n`tagged' by a malicious service providers, even if the user has set\nthe user agent to always engage in a stateful session without prior\nnotice.\n\n[Note: the following addition addresses the privacy problems I\ndiscussed in my previous message in this thread.  The non-idempotent\nrequests rule discussed above makes solving these problems relatively\nstraightforward.]\n\n   + It is recommended that a user agent should, as a configuration\n     option, be able to pop up a dialog box when receiving a\n     State-Info response header, like this:\n\n        ---------------------------------------------------------\n          Start a session with server foo.bar? \n           [ Yes ]\n           [ Yes, always ]\n           [ No ]\n          (Help information: Starting a session will allow foo.bar\n          to gather accurate statistics of your actions)\n        ----------------------------------------------------------\n\n    If one of the `yes' buttons is pressed, the user agent should\n    change to the ``have state-info'' state.  The `Yes, always'\n    button will have the additional effect of having the user agent\n    start a session with foo.bar without popping up a dialog box on\n    future invocations.\n\n    [`no' alternative 1:] If the `No' button is pressed, the user\n      agent should stay in the ``no state-info'' state.  When getting\n      a new State-Info response header from foo.bar, the user agent\n      should pop up the dialog box again.  This allows users to\n      reconsider the earlier `No' decision.\n\n    [`no' alternative 2:] If the `No' button is pressed, the user\n      agent should also go to the ``have state-info'' state, but start\n      sending State-info request headers containing empty strings\n      instead of sending headers with the opaque information received\n      in the response header.  When getting a new State-Info response\n      header from foo.bar, the user agent should pop up the dialog box\n      again.  This `No' behavior allows servers to suppress the\n      sending of more State-Info response headers and start a\n      non-stateful dialog with users that do not want to engage in\n      stateful dialogs for privacy reasons.  On the other hand, if a\n      server receives an empty-string state-info header, it can also\n      choose to just send a new state-info response header again,\n      thereby asking the user to reconsider the earlier `No' decision.\n\n[...]\n\n----------snip----------\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  > [...]\n  > Let me try to summarize the state-info/caching requirements we seem to\n  > have converged at, in the form of modifications to\n  > draft-kristol-http-state-info-00.txt.  Dave, feel free to cut and\n  > paste from the text below if you can use it.\n[...]\n  > 4.1  Origin Server Role\n  > [...]\n  > An origin server may only include State-Info headers in responses to\n  > non-idempotent requests.  (Non-idempotent requests are all request\n  > that do not use the GET and HEAD methods)\nSorry, I don't agree with this.  I can imagine GETs that should not be\ncached.  (Are all gets idempotent by definition?)  Consider my favorite\n\"Show me my shopping basket\" link that, when selected, sends a \"GET\n/cgi-bin/shopping-basket HTTP/1.x\" to the origin server, along with\nState-Info.  This is one request that the proxy MUST pass along to the\norigin server.  Perhaps to provoke the proxy not to cache the result,\nthe origin server must send back the same State-Info header.\n  > \n  > [...]\n  > \n  > 4.2  User Agent Role\n  > \n  > [...]\n  > \n  > For reasons of privacy protection (see Section 6), a user agent should\n  > ignore State-Info headers contained in responses to idempotent (GET\n  > and HEAD) requests.\nI don't understand the connection between privacy protection and\nidempotent methods.  The only State-Info headers that the user agent\nshould be getting are its own.  My (new) rule was that caching proxies\ncould only cache responses that have no State-Info response header.\nSo, if you're worried about State-Info coming from a cache, there\nshouldn't be any.\n\n  > \n  > A user agent in the state ``have state-info'' should include a\n  > State-info request header in all requests to the origin server, whether\n  > these are idempotent or not.\nYes.\n  > \n  > Caches in user agents should be careful to implement the caching\n  > semantics defined in the HTTP protocol, especially when handling\n  > requests or responses containing State-Info headers.\nI think it's safe for a user agent to cache responses that contain\nState-Info headers, though I haven't thought it through carefully.\n  > \n  > If the user agent allows the user to configure the its cache to\n  > \n  >   `check the for validity of document (i.e. issue conditional get if\n  >    expired) only once per session',\n  > \n  > this configuration option (which makes the user agent violate the HTTP\n  > specification anyway) should not override HTTP cache semantics for\n  > transactions where requests or responses containing State-Info headers\n  > are involved, as this will make stateful dialogs impossible or, worse,\n  > dangerously unreliable.\n  > \n  > [...]\n  > \n  > 4.3  Caching Proxy Role\n  > \n  > Caches that conform completely to the (draft) HTTP 1.0 or 1.1\n  > specification need not be changed to support State-Info.\n  > \n  > As per the requirements in the HTTP 1.0 and 1.1 drafts, caching\n  > proxies\n  > \n  >   + must never cache responses to non-idempotent requests\n  >     (note that the drafts probably need to make this requirement\n  >     more explicit)\nNote my comments above about State-Info and GET.\n  > \n  >   + must not cache a response to an idempotent request if the response\n  >     contains a Pragma or Expires header with a value that disallows\n  >     caching\n  > \n  > Thus, origin servers can expect that proxies forward to them:\n  > \n  >  - all non-idempotent requests (which may be carrying State-Info\n  >    headers) issued by user agents\n  > \n  >  - all idempotent requests (which may be carrying State-Info headers)\n  >    for every `dynamic' URI D issued by user agents.  An URI D is\n  >    `dynamic' if the server has consistently put Pragma or Expires\n  >    headers disallowing caching in every response message to\n  >    non-idempotent requests for URI D.  Proxies may `downgrade' normal\n  >    GET requests to conditional GET request when doing the forwarding.\n  > \n  > An example of a `dynamic', idempotent URI is a `shopping basket\n  > contents URI' will typically be accessed with the GET method: the link\n  > to the shopping basket page will be a normal HTML <A HREF=...> link,\n  > it need not be a form submit button.\nWell, hello there!  Ummm, this GET is idempotent only when the\nState-Info is the same in a second request.  So let's see, are you\nsaying that State-Info is part of the cache state and therefore won't\nmatch the request unless State-Info matches, too?\n  > \n[... more cache stuff...]\n  > Proxy caches that, for whatever reason, are unwilling or unable *not*\nUmmm, double negatives give me headaches.  Are you saying what you meant\nto say.  It sounds like 501 gets returned only when the proxy understands\nwhat's going on.\n  > to cache a `dynamic' entity belonging to an URI D should, if\n  > State-Info headers were present in the request or response for D,\n  > return a HTTP error code 501 (Not Implemented) to the requesting\n  > client.\n[...]\n  > 5.  IMPLEMENTATION CONSIDERATIONS\n[...]\n  > The state-info facilities only allow origin servers to track user\n  > access to non-idempotent and `dynamic' idempotent URI's.  A `library'\n  > or `magazine browsing' server may want to track all URI's accessed by\n  > the user, allowing it to show a list of articles looked at already.\n  > This tracking could be accomplished by making all pages `dynamic', or\n  > by including a small `dynamic' inline picture on every page.  However,\n  > if neither the user agent nor any proxy close to the user agent has\n  > conditional GET capability, this technique may cause an unacceptably\n  > large amount of web traffic to be generated.\nThis is (to me) an unwelcome consequence of my agreeing that proxies\ndon't have to pass through State-Info for cached items.\n  > \n  > \n  > 6. PRIVACY\n  > \n  > [...]\n  > \n  > The requirement on user agents to ignore all State-Info headers\n  > contained in responses to idempotent requests (GET, HEAD) helps to\n  > protect the privacy of the user.\nI don't understand why.\n  > \n[...]\n  > [Note: the following addition addresses the privacy problems I\n  > discussed in my previous message in this thread.  The non-idempotent\n  > requests rule discussed above makes solving these problems relatively\n  > straightforward.]\n  > \n  >    + It is recommended that a user agent should, as a configuration\n  >      option, be able to pop up a dialog box when receiving a\n  >      State-Info response header, like this:\n  > \n  >         ---------------------------------------------------------\n  >           Start a session with server foo.bar? \n  >            [ Yes ]\n  >            [ Yes, always ]\n  >            [ No ]\n  >           (Help information: Starting a session will allow foo.bar\n  >           to gather accurate statistics of your actions)\nActually, not, since requests using idempotent methods might not get\npassed along.\n  >         ----------------------------------------------------------\n  > \n[...]\n\nSorry, it's been a long day, Koen's message was a long one, and I may\nhave spewed nonsense up there.  And caching is not my strong point (as\nhas probably been obvious before).\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "HEAD and 301/302/303 conflict",
            "content": "Section 5.2.2 on the HEAD method says that HEAD is just like GET, \nexcept that the server must not return any Entity-Body in the response. \nSection 6.2.3 on redirection status codes, 301 and 302 (Moved \nPermanently and Moved Temporarily ) says that the Entity-Body in the \nresponse should contain a short hypertext note. Similarly for 303 (See Other).\n\nPresumably, 301 and 302 are reasonable responses for HEAD requests (303 \nis not so obvious -- it might be only OK as a response to a POST in \n1.1?) and so require Entity-Bodies, which HEAD says can't be sent. In \nthe description for 300, it explictly says that 300 in response to a \nHEAD request doesn't return an Entity-Body; there is no explicit \nexemption for 301, 302, or 303.\n\nIn section 5.2.2, was the intent to say that HEAD is like GET, except \nthat whenever GET would return 200 and an Entity-Body, HEAD will return \n204 and no Entity-Body? That seems plausible, and reconciles all \nbehavioral difference except for when GET would return 300. Equally \nplausible is that 301, 302, and 303 should contain the same exception \nfor HEAD that 300 contains.  I don't know the intent, so I can't make a \nrecommendation.\n\nWhich should it be?\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: HEAD and 301/302/303 conflict",
            "content": "Paul Leech reports:\n\n>Section 5.2.2 on the HEAD method says that HEAD is just like GET, \n>except that the server must not return any Entity-Body in the response. \n>Section 6.2.3 on redirection status codes, 301 and 302 (Moved \n>Permanently and Moved Temporarily ) says that the Entity-Body in the \n>response should contain a short hypertext note. Similarly for 303 (See Other).\n\nOoops, contradiction.\n\n>Presumably, 301 and 302 are reasonable responses for HEAD requests (303 \n>is not so obvious -- it might be only OK as a response to a POST in \n>1.1?) and so require Entity-Bodies, which HEAD says can't be sent. In \n>the description for 300, it explictly says that 300 in response to a \n>HEAD request doesn't return an Entity-Body; there is no explicit \n>exemption for 301, 302, or 303.\n>\n>In section 5.2.2, was the intent to say that HEAD is like GET, except \n>that whenever GET would return 200 and an Entity-Body, HEAD will return \n>204 and no Entity-Body? That seems plausible, and reconciles all \n>behavioral difference except for when GET would return 300. Equally \n>plausible is that 301, 302, and 303 should contain the same exception \n>for HEAD that 300 contains.  I don't know the intent, so I can't make a \n>recommendation.\n>\n>Which should it be?\n\nThe latter -- I'll fix this in both specs.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: Partial file transfer",
            "content": "I'd be more comfortable with this and other proposals if the original\ndata were delivered with a 'content-ID' that was unique to the\nparticular content being delievered, and any follow-up request for\npartial delivery were indexed not by the original URL but by the\ncontent-ID itself. (The lack of a content-ID would be a signal that\nthe object wasn't suitable for partial transmission or that the server\ndidn't support such.)\n\nThe content-ID could also be used as a GET IF-DIFFERENT key, since the\ncontent-ID would change when the content changed. Content-ID is\nrequired to be globally unique. (It might even be a URN, but who\nknows?)\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Dave Kristol:\n>\n>koen@win.tue.nl (Koen Holtman) wrote:\n>  > [...]\n>  > Let me try to summarize the state-info/caching requirements we seem to\n>  > have converged at, in the form of modifications to\n>  > draft-kristol-http-state-info-00.txt.  Dave, feel free to cut and\n>  > paste from the text below if you can use it.\n>[...]\n>  > 4.1  Origin Server Role\n>  > [...]\n>  > An origin server may only include State-Info headers in responses to\n>  > non-idempotent requests.  (Non-idempotent requests are all request\n>  > that do not use the GET and HEAD methods)\n>Sorry, I don't agree with this.  I can imagine GETs that should not be\n>cached.\n\nMe too.  To tell that a GET shouln not be cached, you should use a Pragma:\nno-cache or Expires: <yesterday> response header.\n\n>  (Are all gets idempotent by definition?)\n\nYes.  At least that is my interpretation of the draft http spec.  Note that\nit is easy for a service provider to inplement a GET that is _not_\nidempotent according to the `idempotent' definition that mathematicians use.\nThe draft spec forbids a service provider from doing this, though.\n\n>  Consider my favorite\n>\"Show me my shopping basket\" link that, when selected, sends a \"GET\n>/cgi-bin/shopping-basket HTTP/1.x\" to the origin server, along with\n>State-Info.  This is one request that the proxy MUST pass along to the\n>origin server.  Perhaps to provoke the proxy not to cache the result,\n>the origin server must send back the same State-Info header.\n\nAs I said, this provoking can be done with the usual Pragma and Expires\nresponse headers.\n\n>  > 4.2  User Agent Role\n>  > \n>  > [...]\n>  > \n>  > For reasons of privacy protection (see Section 6), a user agent should\n>  > ignore State-Info headers contained in responses to idempotent (GET\n>  > and HEAD) requests.\n>I don't understand the connection between privacy protection and\n>idempotent methods.\n\nI explained this in section 6, did you read the explanation there?  The idea\nis that you are sending idempotent request most of the time.  If these\ncannot cause you to be `tagged' with a state-info header, you have to worry\nabout getting tagged only some of the time.\n\n>  > Caches in user agents should be careful to implement the caching\n>  > semantics defined in the HTTP protocol, especially when handling\n>  > requests or responses containing State-Info headers.\n>I think it's safe for a user agent to cache responses that contain\n>State-Info headers, though I haven't thought it through carefully.\n\nIt definately is not: the state-info response headers gotten on an URI can\nchange through time.  Imagine a POST-URI working as a toggle button.\n\n[...]\n>  > An example of a `dynamic', idempotent URI is a `shopping basket\n>  > contents URI' will typically be accessed with the GET method: the link\n>  > to the shopping basket page will be a normal HTML <A HREF=...> link,\n>  > it need not be a form submit button.\n>Well, hello there!  Ummm, this GET is idempotent only when the\n>State-Info is the same in a second request.\n\nNo, the GET is idempotent by definition.  You are confusing non-idempotent\n(URI,method) pairs with (URI,method) pairs that give dynamic results.\n\nA (live-snapshot-of-the-coffee-pot-URI,GET) pair is idempotent, but\nalso dynamic.  The same is true for the (shopping basket,GET) pair.\n\n>  So let's see, are you\n>saying that State-Info is part of the cache state and therefore won't\n>match the request unless State-Info matches, too?\n\nNo, I'm saying exactly the opposite.  State-info request headers should not\nbe part of the cache key (like Accept headers are).  If the response\ndepends on the value of the State-Info request header, it should be made\ndynamic by including the usual cache prevention headers.\n\n>[... more cache stuff...]\n>  > Proxy caches that, for whatever reason, are unwilling or unable *not*\n>Ummm, double negatives give me headaches.  Are you saying what you meant\n>to say.  It sounds like 501 gets returned only when the proxy understands\n>what's going on.\n\nLet me try to rephrase that paragraph:\n\n Suppose that a response is dynamic, i.e. suppose it has a Pragma: no-cache\n or Expires: <yesterday> header.  This means that the proxy is forbidden\n from caching the response.  Suppose that the proxy, for whatever reason, is\n unwilling or unable to comply to this restriction.  If this is the case,\n and State-Info headers are involved, them the cache should return a HTTP\n error code 501 (Not Implemented) to the requesting client, and throw away\n the response.\n\n I don't know if you have read the `tuned proxy caches' discussion: the\n above basically says that `tuned' proxies should tell users that\n stateful dialogs through them are not implemented, rather than risk\n breaking the synchronisation between server and user agent by\n inappropriately caching dynamic responses.\n\n>[...]\n>  >           (Help information: Starting a session will allow foo.bar\n>  >           to gather accurate statistics of your actions)\n>Actually, not, since requests using idempotent methods might not get\n>passed along.\n\nThey will always be is the service provider makes all responses dynamic.\n\n[...]\n>Sorry, it's been a long day, Koen's message was a long one, and I may\n>have spewed nonsense up there.  And caching is not my strong point (as\n>has probably been obvious before).\n\nI made my message so long because I felt that Shel Kaphans previous message\ndid not make a clear enough distinction between `dynamic responses' and\n`responses to non-idempotent requests'.  I guess I haven't entirely\nsucceeded in making this distinction much more clear either.\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "The documents that change each time you look at them are my reason for\nthe text for 1.1 that says:\n\n> \"GET and HEAD are idempotent, and so can be cached, UNLESS the following\n> parameters are put on the document by the server....\"\n\nOn second reading, the words \"are idempotent\" should be deleted. Simply\nstate that they can be cached, unless the server takes steps to prevent it.\n\nThat is, if you insist that something wrong happens if you cache it, you\nhave to play by the rules about how to defeat caching.\nOtherwise, what your users get is what you deserve.\n\n    harald A\n\n\n\n"
        },
        {
            "subject": "WWW I18N at Unicode Conference (Sept 14/15",
            "content": "Just a reminder to folks interested in WWW I18N that there will be an\ninformal BOF at the upcoming 7th International Unicode Conference at the\nSan Jose Hyatt on Sept 14 or 15 (exact time and place TBD).\n\nThe full agenda and reg. info can be found on the Unicode home page:\n\n  http://www.stonehand.com/unicode.html\n\nAlso, for any TeX folks out there, Don Knuth will be giving a plenary talk\nentitled: \"Experiences getting ready for Unicode.\"\n\nRegards,\nGlenn\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-kristol-http-state-info00.txt, .p",
            "content": "Dave Kristol writes:\n> A New Internet-Draft is available from the on-line Internet-Drafts \n> directories.                                                               \n> \n>        Title     : Proposed HTTP State-Info Mechanism                      \n>        Author(s) : D. Kristol\n>        Filename  : draft-kristol-http-state-info-00.txt, .ps\n>        Pages     : 7\n>        Date      : 08/23/1995\n> \n> HTTP, the protocol that underpins the World-Wide Web (WWW), is stateless.  \n> That is, each request stands on its own; origin servers don't need to \n> remember what happened with previous requests to service a new one.  \n> Statelessness is a mixed blessing, because there are potential WWW \n> applications, like ``shopping baskets'' and library browsing, for which the\n> history of a user's actions is useful or essential.                    \nFine. The addressed goals are important, and we really need some kinda\n'states' in http.\n> This proposal outlines a way to introduce state into HTTP.  A new \n> request/response header, State-Info, carries the state back and forth, thus\n> relieving the origin server from needing to keep an extensive per-user or \n> per-connection database.  The changes required to user agents, origin \n> servers, and proxy servers to support State-Info are very modest.          \nMy objections:\n1. The proposal (and now the draft) doesn't solve the most serious trouble of\nshopping-basket applications:\nhow can the client application be forced to refresh the 'shopping basket page'\nevery time? We have the no-cache pragma to prevent cacheing, but the semantics\nof the pragma is not clear enough now, and I haven't seen the http 1.1 draft. \n(Roy Fielding published the semantics of the pragma on this list, but we don't\nhave a formal draft describing that semantics yet - perhaps we should wait a\nbit for 1.1 draft?)\n2. we have no mechanism to tell client applications, when state-info should\nbe present in a request. (I see a possible solution, namely the \"METHODS\"\nattribute in anchor html tag - see workinprogess\nURL: ftp://ds.internic.net/internet-drafts/ draft-ietf-html-spec-05.txt - by\nextending METHODS to use GET/State-info and POST/State-Info, e.g. change\nmethods to method/extension in that draft, but this needs consensus from html\nWG. GET/State-Info would mean 'shopping basket page', and POST/State-Info\nwould mean shopping, but shopping is only an example, other types of stateful\ndialogs and other method/extension pairs are possible.)\n3. The current discussion on 'possible optimisation to state-info proposal'\ntends to make the situation worse: \ntrying to solve one problem introduces new requirements to caches.\n(And we have 3 other similar proposals mentioned in Dave Kristols draft + Secure\nmethod in draft URL:\nftp://ds.internic.net/internet-drafts/draft-ietf-wts-shttp-00.txt - this may\nresult in an overkill for cache implementors! Even worse, the S-HTTP proposal \ndoesn't contain the word cache. It would be wise at least to state that\nS-HTTP actions aren't cacheable at all, or even better, to require a private\npragma in every S-HTTP response).\nUsing 'Pragma: private' in responses to 'POST/State-Info' requests may be a\nmore graceful solution, and tagging 'shopping basket' URL-s with\n'Pragma: no-cache' or 'Pragma: Dynamic' and METHODS attribute 'GET/State-Info'\ncan solve both 1. and 2.\n(I don't know if distinguishing dynamic and no-cache pragmas are necessary or\nnot. The distinction may be useful in their impact on browsers history\nmechanism.)\nMy objections are partially motivated by viewpoint of caching proxy\nadministrator/implementor, and address the more general question of http\nextension's effect on caches. If we require explicit statement of\n(non)cacheability in http extensions by requiring the presence of the\nappropriate pragmas/headers, cache implementations may remain useful in the\nlong run, otherwise every new http extension may render current cache\nimplementations non-compliant. If not only I consider that possibility\nunwanted, we shall add a paragaph to the draft on subject 'requirements to\nhttp extensions'.\n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "Re: any more comments",
            "content": ">I, for one, would welcome such tables.  I've implemented a server from\n>the spec., and while I agree with Roy that reading everything is\n>essential, I sure would have liked a set of tables to use as a\n>checklist to make sure I hadn't done something dumb.\n\nOn the other hand, even a 4-dimensional table could take us many months to\ncome to agreement on. I propose that Roy move the current BCP draft\nforwards and, if there is interest in doing such a table, that it come as a\nlater document.\n\nMaybe someone (not me) can start by creating a database of their view of\nthe BCP draft, then  make the database publicly viewable. People could add\nto or disagree with the table (outside the WG, on a mailing list of folks\nconcerned with such details), and any significant differences that are not\ndeliniated in the BCP draft will pop out.\n\nIt seems likely to me that these differences and/or enhancements to the BCP\nwill be more on edge conditions than on the basics of how should HTTP\nservers work. These edge conditions, while important, are not so important\nas to prevent us from wanting a finalized BCP *soon*. We can always do a\nrefinement or addition draft in six months or so. Heck, we could publish\nthe whole database then, if we felt consensus on it.\n\n--Paul Hoffman\n--Proper Publishing\n\n\n\n"
        },
        {
            "subject": "snapsho",
            "content": ">(Roy Fielding published the semantics of the pragma on this list, but we don't\n>have a formal draft describing that semantics yet - perhaps we should wait a\n>bit for 1.1 draft?)\n\nI put a snapshot of my current working copy on my webspace back\nin Irvine.  People can have a look if they like, since I'm going\nto be in a meeting for the next two days and do not expect to get\ndraft out until after it is done.  Since they are not official\ndrafts, be sure to preface any comments with the alpha version number.\n\n    http://www.ics.uci.edu/~fielding/test/draft-v10-03a.ps.gz\n    http://www.ics.uci.edu/~fielding/test/draft-v11-00a.ps.gz\n\nNo other formats are available at this time.\n\n......Roy\n\n\n\n"
        },
        {
            "subject": "Re: snapsho",
            "content": "Er, silly me... I was testing access control over the weekend\nand forgot to remove it.  Those links should work now.\n\n.....Roy\n\n[well, at least now I'm sure access control works...]\n\n\n\n"
        },
        {
            "subject": "Re: I-D ACTION:draft-kristol-http-state-info00.txt, .p",
            "content": "\"Balint Nagy Endre\" <bne@bne.ind.eunet.hu> wrote (on 8/27):\n[in response to my I-D]:\n        \n  > My objections:\n  > 1. The proposal (and now the draft) doesn't solve the most serious trouble of\n  > shopping-basket applications:\n  > how can the client application be forced to refresh the 'shopping basket page'\n  > every time? We have the no-cache pragma to prevent cacheing, but the semantics\n  > of the pragma is not clear enough now, and I haven't seen the http 1.1 draft. \n  > (Roy Fielding published the semantics of the pragma on this list, but we don't\n  > have a formal draft describing that semantics yet - perhaps we should wait a\n  > bit for 1.1 draft?)\n\nThere is no intention to force the client to refresh the \"shopping basket\npage\".  The current state is carried in the State-Info header, and the\nuser can select a link that, when passed the State-Info header, will\nshow the current shopping basket page.\n\n  > 2. we have no mechanism to tell client applications, when state-info should\n  > be present in a request. (I see a possible solution, namely the \"METHODS\"\n\nNo mechanism is required.  The algorithm is that the user agent ALWAYS\nsends State-Info to a server that it has previously received a State-Info\nresponse from.\n\n  > attribute in anchor html tag - see workinprogess\n  > URL: ftp://ds.internic.net/internet-drafts/ draft-ietf-html-spec-05.txt - by\n  > extending METHODS to use GET/State-info and POST/State-Info, e.g. change\n  > methods to method/extension in that draft, but this needs consensus from html\n  > WG. GET/State-Info would mean 'shopping basket page', and POST/State-Info\n  > would mean shopping, but shopping is only an example, other types of stateful\n  > dialogs and other method/extension pairs are possible.)\n  > 3. The current discussion on 'possible optimisation to state-info proposal'\n  > tends to make the situation worse: \n  > trying to solve one problem introduces new requirements to caches.\n\nI don't think the optimization itself adds major complications to\ncaches.  Koen Holtman proposed using the existing \"don't cache\"\nmechanisms, instead of giving special meaning to State-Info, and that\nalternative should satisfy your objection.\n\n  > (And we have 3 other similar proposals mentioned in Dave Kristols draft + Secure\n  > method in draft URL:\n  > ftp://ds.internic.net/internet-drafts/draft-ietf-wts-shttp-00.txt - this may\n  > result in an overkill for cache implementors! Even worse, the S-HTTP proposal \n  > doesn't contain the word cache. It would be wise at least to state that\n  > S-HTTP actions aren't cacheable at all, or even better, to require a private\n  > pragma in every S-HTTP response).\n  > Using 'Pragma: private' in responses to 'POST/State-Info' requests may be a\n  > more graceful solution, and tagging 'shopping basket' URL-s with\n  > 'Pragma: no-cache' or 'Pragma: Dynamic' and METHODS attribute 'GET/State-Info'\n  > can solve both 1. and 2.\n  > (I don't know if distinguishing dynamic and no-cache pragmas are necessary or\n  > not. The distinction may be useful in their impact on browsers history\n  > mechanism.)\n  > My objections are partially motivated by viewpoint of caching proxy\n  > administrator/implementor, and address the more general question of http\n  > extension's effect on caches. If we require explicit statement of\n\nYes, the effect of extensions on caches is poorly defined.  That's why\nthe I-D was explicit about what a cache is supposed to do with State-Info.\n\n  > (non)cacheability in http extensions by requiring the presence of the\n  > appropriate pragmas/headers, cache implementations may remain useful in the\n  > long run, otherwise every new http extension may render current cache\n  > implementations non-compliant. If not only I consider that possibility\n  > unwanted, we shall add a paragaph to the draft on subject 'requirements to\n  > http extensions'.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Dave Kristol writes:\n\n > If you're going to introduce dependencies on State-Info specifically,\n > you could just as well introduce the requirement that responses that\n > contain State-Info not be cached.\n\nI claim that any response that originates from an origin server can\nlegitimately contain a state-info header, and that it's best not to\ncomplicate that.\n\nSuppose that a POST returns the same resource that a later GET\n*could* return. The POST's returned resource gets cached, even though the POST\nitself could not have been served from a cache.  The POST's response can\nlegitimately contain state-info.   So, there is no correlation between\nresponses that can contain state-info and those that can be cached.\nLeave cacheability to the already too numerous headers that control that.\n\n  Why the need for extra headers?\n > Is there ever a reason to cache a response that contains State-Info?\n > In this case, the caching server would respond with 501 when it saw\n > State-Info.\n > \n > Dave Kristol\n\nThere may not be a *need* for it that we can imagine right now, but\nthere's also no need for restrictions against it.  An origin server\nmay legitimately and aggressively try to start sessions whenever it can,\nincluding times when it is delivering cacheable, shared resources.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "> Suppose that a POST returns the same resource that a later GET\n> *could* return. The POST's returned resource gets cached, even though the POST\n\nThis certainly happens on quite a few servers (including ewse.jrc.it) where\nyou have forms which either reveal themselves; or act as a 'post' to \nthemselves. Seen this quite a few times on other servers as well. Either\nfor code management/development reasons; or because of referals from other\nscripts; it allows you to have central data entry pages.\n\n> itself could not have been served from a cache.  The POST's response can\n> legitimately contain state-info.   So, there is no correlation between\n> responses that can contain state-info and those that can be cached.\n> Leave cacheability to the already too numerous headers that control that.\n\nYes exactly. We had to deny access to quite a few chaches on the site \nbecause they just did that.\n \n> There may not be a *need* for it that we can imagine right now, but\n> there's also no need for restrictions against it.  An origin server\n> may legitimately and aggressively try to start sessions whenever it can,\n> including times when it is delivering cacheable, shared resources.\n\nYes; but it would be better, imho, to design things decently from\nscratch and distinish between *what* is to be chached in its context. If\nyou look at our experimental ewse.jrc.it server you might see that the\npages seen also depend on the user who sees them. So I might see a different\nlayout to the shopping mall than you do; even though we use the same url.\n\nThis is something which, although ill defined, is happening at quite\na few places with configurable shopping bags and god knows what. And\nI personnaly, as a server admin, would hate to block chaching completely.\n\nDw.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "> Suppose that a POST returns the same resource that a later GET\n> *could* return. The POST's returned resource gets cached, even though the POST\n\nThis certainly happens on quite a few servers (including ewse.jrc.it) where\nyou have forms which either reveal themselves; or act as a 'post' to \nthemselves. Seen this quite a few times on other servers as well. Either\nfor code management/development reasons; or because of referals from other\nscripts; it allows you to have central data entry pages.\n\n> itself could not have been served from a cache.  The POST's response can\n> legitimately contain state-info.   So, there is no correlation between\n> responses that can contain state-info and those that can be cached.\n> Leave cacheability to the already too numerous headers that control that.\n\nYes exactly. We had to deny access to quite a few chaches on the site \nbecause they just did that.\n \n> There may not be a *need* for it that we can imagine right now, but\n> there's also no need for restrictions against it.  An origin server\n> may legitimately and aggressively try to start sessions whenever it can,\n> including times when it is delivering cacheable, shared resources.\n\nYes; but it would be better, imho, to design things decently from\nscratch and distinish between *what* is to be chached in its context. If\nyou look at our experimental ewse.jrc.it server you might see that the\npages seen also depend on the user who sees them. So I might see a different\nlayout to the shopping mall than you do; even though we use the same url.\n\nThis is something which, although ill defined, is happening at quite\na few places with configurable shopping bags and god knows what. And\nI personnaly, as a server admin, would hate to block chaching completely.\n\nDw.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Some combination of Koen Holtman and Dave Kristol conversed:\n\n > >  > Caches in user agents should be careful to implement the caching\n > >  > semantics defined in the HTTP protocol, especially when handling\n > >  > requests or responses containing State-Info headers.\n > >I think it's safe for a user agent to cache responses that contain\n > >State-Info headers, though I haven't thought it through carefully.\n > \n > It definately is not: the state-info response headers gotten on an URI can\n > change through time.  Imagine a POST-URI working as a toggle button.\n > \n\nOf course you shouldn't cache state-info headers, but you should\ncache a cacheable resource even if it is sent with state-info headers.\nIt's best to make cacheability and transmittal of state-info\ncompletely orthogonal. \n\n[ much discussion of idempotence, caching, etc. ]\n\nI don't think we have to make a big deal about defining idempotence.\nAs I tried to say before, whether a resource can be cached, and hence\nstop participating in a stateful dialog, is up to the origin server\ndesigner/administrator.  If a returned resource is marked as cacheable (no\nPragma: no-cache, no Expires: <yesterday>), the resource is cacheable,\nperiod.  State-info is never cacheable.\n\nState-info can be sent with any response from an origin server,\nbut can never originate from an intermediate proxy.  It can also be\nreturned along with any request from a user agent.\n\nI don't see why it needs to be any more complex than that.\n\n[ tuned caches ... ]\n\nAs for the idea that caches might decide to cache things they\nshouldn't (overruling headers in responses), and then give error\nresponses to requests with state-info requests, I think that's really\na travesty.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Shel Kaphan <sjk@amazon.com> wrote:\n  > Some combination of Koen Holtman and Dave Kristol conversed:\n  > \n  >  > >  > Caches in user agents should be careful to implement the caching\n  >  > >  > semantics defined in the HTTP protocol, especially when handling\n  >  > >  > requests or responses containing State-Info headers.\n  >  > >I think it's safe for a user agent to cache responses that contain\n  >  > >State-Info headers, though I haven't thought it through carefully.\n  >  > \n  >  > It definately is not: the state-info response headers gotten on an URI can\n  >  > change through time.  Imagine a POST-URI working as a toggle button.\n  >  > \n  > \n  > Of course you shouldn't cache state-info headers, but you should\n  > cache a cacheable resource even if it is sent with state-info headers.\n  > It's best to make cacheability and transmittal of state-info\n  > completely orthogonal.\n\nThat particular exchange concerned user-agent-side caches, not proxy\ncaches. \n\nI'm in favor of the KISS (keep it simple, stupid) approach.\n\nI think much of the argument has been over what constitutes a\n\"cacheable resource.\"  And the answer has two dimensions:\nphilosophical and operational.  On the philosophical side, what\nresources must, should, can, should not, can not, and must not be\ncached?  On the operational side, what techniques are employed (e.g.,\nheaders) by the origin server to get the cache to exhibit the desired\nbehavior?\n\nFurther complicating the question is, what do existing caching proxies\ndo, and how would that behavior interact with State-Info?  How would\nthat behavior affect deployment of this new feature?  Ideally the\ninteraction with \"old\" cache would be benign:  things would either work\n(unlikely) or break softly.\n\n  > \n  > [ much discussion of idempotence, caching, etc. ]\n  > \n  > I don't think we have to make a big deal about defining idempotence.\n\nMy only reason for fussing with the definition is to be sure we're talking\nabout the same things and using the same terms to do so.  I feared we were\nusing the same words to mean different things.\n\n  > As I tried to say before, whether a resource can be cached, and hence\n  > stop participating in a stateful dialog, is up to the origin server\n  > designer/administrator.  If a returned resource is marked as cacheable (no\n  > Pragma: no-cache, no Expires: <yesterday>), the resource is cacheable,\n  > period.  State-info is never cacheable.\n  > \n  > State-info can be sent with any response from an origin server,\n  > but can never originate from an intermediate proxy.  It can also be\n  > returned along with any request from a user agent.\n  > \n  > I don't see why it needs to be any more complex than that.\n\nThat much sounds fine, and I agree.  The messiness arises when State-Info\npasses through a caching proxy in one direction or the other.  (I know\nthat isn't news to you.)  The goal is to cache as much stuff as possible\nand still perform correctly.\n  > \n  > [ tuned caches ... ]\n  > \n  > As for the idea that caches might decide to cache things they\n  > shouldn't (overruling headers in responses), and then give error\n  > responses to requests with state-info requests, I think that's really\n  > a travesty.\n\nI agree, but apparently it's something that must be dealt with, just\nlike error conditions in general.  Furthermore, we have to deal with\ndeployment and inter-operation with old caching proxies.\n\nSummary:  I think I largely agree with you (Shel), but I also see that\nthere are complicating factors we can't really ignore.\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Dave Kristol:\n>Further complicating the question is, what do existing caching proxies\n>do, and how would that behavior interact with State-Info?\n\nAs far as I know, you can depend on existing caches knowing about\nExpires: <yesterday>.  The pragma: no-cache *response* header is very\nnew, so existing caches probably won't know about it.\n\nSo it could be said that existing caches are already able to support\n(the latest version of) State-info.\n\nThe biggest problem is existing *user agent* caches.  User agents with\ninternal caches that completely ignore Expires and Pragma are still\nvery common.\n\n>[Shel Kaphan <sjk@amazon.com> wrote: ]\n>  > \n>  >     [ much discussion of idempotence, caching, etc. ]\n>  > \n>  > I don't think we have to make a big deal about defining idempotence.\n> \n>My only reason for fussing with the definition is to be sure we're talking\n>about the same things and using the same terms to do so.  I feared we were\n>using the same words to mean different things.\n\nIn a previous message your definition was:\n\n>I think the prevailing definition of an idempotent {method, URI} pair\n>is that you get back exactly the same content each time you make a\n>request with that pair.\n\nI would call that a `static' pair.\n\nYour meaning of idempotent is not the (prevailing?) one used in the draft\nhttp spec. From draft-ietf-http-v10-spec-02.html:\n\n#10.2 Idempotent Methods\n\n[...]\n\n#In particular, the convention has been established that the GET and\n#HEAD methods should never have the significance of taking an action\n#other than retrieval. These methods should be considered \"safe\" and\n#should not have side effects. This allows the client software to\n#represent other methods, such as POST, in a special way so that the\n#user is made aware of the fact that an non-idempotent action is being\n#requested.\n#\n#Naturally, it is not possible to ensure that the server does not\n#generate side-effects as a result of performing a GET request; in\n#fact, some dynamic resources consider that a feature. The important\n#distinction here is that the user did not request the side-effects, so\n#therefore cannot be held accountable for them.\n\nrom this, I read that:\n - GET and HEAD are defined to be the idempotent methods\n - idempotent means `safe'.\n\nIMO, the spec should be rewritten to avoid the use of the word\n`idempotent' altogether.  Failing that, there should be a proper\ndefinition of it in the terminology section.\n\n[..using existing cache prevention methods for stateful services..]\n\n>  > I don't see why it needs to be any more complex than that.\n> \n>That much sounds fine, and I agree.  The messiness arises when State-Info\n>passes through a caching proxy in one direction or the other.  (I know\n>that isn't news to you.)  The goal is to cache as much stuff as possible\n>and still perform correctly.\n\nThe goal (at least my goal) is to add State-Info to http 1.1.  I would\nlike to discuss the caching implications of State-Info in terms of the\ncurrently drafted http 1.1 caching mechanisms. \n\nState-Info already allows very adequate caching using the existing\nhttp caching definitions.\n\nThe discussion on how to cache as much stuff as possible (by redefining\n`idempotent' in http 1.1 or whatever) should be done in another thread.\n\n>  >     [ tuned caches ... ]\n>  > \n>  > As for the idea that caches might decide to cache things they\n>  > shouldn't (overruling headers in responses), and then give error\n>  > responses to requests with state-info requests, I think that's really\n>  > a travesty.\n> \n>I agree, but apparently it's something that must be dealt with, just\n>like error conditions in general.\n\nThe reason for putting something about tuned caches in the state-info\nspec is that proxy operators need to be reminded of their\nresponsibility to operate a non-broken cache, or, failing that, to\noperate a cache that tells you when it is broken.\n\nAs long as tuning is common (and it seems to be, though I don't have\nany hard figures), proxy operators need to be reminded of this.  Shel,\nI don't like this any more than you do.\n\nNote that the above text about tuned caches does not introduce any\nobligation for the operators of _working_ caches to treat State-Info\nas a special case.\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "koen@win.tue.nl (Koen Holtman) wrote:\n  > Dave Kristol: \n[...]\n  > >My only reason for fussing with the definition is to be sure we're talking\n  > >about the same things and using the same terms to do so.  I feared we were\n  > >using the same words to mean different things.\n  > \n  > In a previous message your definition was:\n  > \n  > >I think the prevailing definition of an idempotent {method, URI} pair\n  > >is that you get back exactly the same content each time you make a\n  > >request with that pair.\n  > \n  > I would call that a `static' pair.\n  > \n  > Your meaning of idempotent is not the (prevailing?) one used in the draft\n  > http spec. From draft-ietf-http-v10-spec-02.html:\n\n  > [definition from spec. omitted]\n\n  > >From this, I read that:\n  >  - GET and HEAD are defined to be the idempotent methods\n  >  - idempotent means `safe'.\n\nSorry to be a pain, but what do you mean by \"safe\"?  This is the\nphilosophical vs. operational divide.  The definition so far has been\noperational:  GET and HEAD are idempotent; they have no side-effects.\nOkay.  But what are we implying when we say that?  What is the\nphilosophical definition, in the context of WWW?\n\n  > \n  > IMO, the spec should be rewritten to avoid the use of the word\n  > `idempotent' altogether.  Failing that, there should be a proper\n  > definition of it in the terminology section.\n[...]\n\nDave Kristol\n\n\n\n"
        },
        {
            "subject": "keepalive/lookahea",
            "content": "I had an interesting conversation with a friend who worked at Gain on\na multimedia/hypermedia authoring system they built several years ago.\n(Gain is part of Sybase now).  He doesn't know much about the web, and\nso is in a good position to make fresh observations.\n\nI was telling him about the keepalive stuff that is happening now, and\nhe mentioned a use for it that is similar to something in the Gain\nsystem but that I have not yet seen discussed in the context of http.\n(It probably has been -- I just don't know any references, sorry).\n\nAnyway, the Gain system had some notion of property lists on objects,\none of which was a property that allowed other objects to be\npre-fetched whenever a given object was fetched.\n\nThe http analog for that would be headers and/or HTML tags of some\nkind that indicate other documents that should be fetched whenever a\ngiven document is fetched because of the high probability that these\nother documents will be viewed whenever the first one is viewed.  This\ndoesn't strictly require keepalive, but it interacts well with it, as\nprefetch would be much cheaper to implement using it.  In fact, for\nsufficiently small documents it would be statistically cheaper to\nprefetch, in one TCP connection, some documents that go unread than to\nset up and tear down TCP connections for every document that is read.\n\nSorry if this is too speculative for this group -- just thought it was\nan interesting idea.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "On Mon, 28 Aug 1995, Dave Kristol wrote:\n\n> koen@win.tue.nl (Koen Holtman) wrote:\n> \n>   > >From this, I read that:\n>   >  - GET and HEAD are defined to be the idempotent methods\n>   >  - idempotent means `safe'.\n> \n> Sorry to be a pain, but what do you mean by \"safe\"?  This is the\n> philosophical vs. operational divide.  The definition so far has been\n> operational:  GET and HEAD are idempotent; they have no side-effects.\n> Okay.  But what are we implying when we say that?  What is the\n> philosophical definition, in the context of WWW?\n\nThe discussion I have read not too many weeks back was that idempotent\nmeant that the transaction made no changes at the server which would\ncause a subsequent re-issue of the same transaction to cause anykind\nof failure.  For this paragraph/context a transaction would be a \nmethod and URL. This definition does not preclude the possiblity\nthat a GET/URL will provide a different response EACH time because\nof other activities at the server.  The obvious example is the\nsearch of a dynamic data base.  \n\nBecause each use of the GET/URL may be different, it is important\n(in my very strong opinion) from a user interface perspective that\nthe User Agents be permitted to maintain mechanisms (like the\nhistory list we often see) which provide a stable view of the\nresult the user expects to reference multiple times as they digest\nthe document, follow links and return.  There are two distinct\npossible facilities a User Agent could provide .. the relatively short\ninstance of the user agent program session history list and a cache.\n\nThe session history is like an XTERM scroll buffer and logically\nbelongs to the user.  The cache is an extension of the server(s) and\nis designed to optimize perormance.  The cache should respond to and\nbe managed by caching control headers.  If the history changes at\nall as a result of server actions, the controls need to be disjoint\nform caching controls ... for example, dynamic documents resulting\nfrom approach such as client-pull/server-push support.\n\nIt is clear from this protracted discussion that overloading the\nmeaning of headers leads to much confusion. Use cache headers to\ncontrol document caching and state-info headers for state info.\n\nI believe I have seen concensus that a cache should not deliver\nstate info.  Therefore it behooves the application which requires\nstate-info to suppress caching itself when the loss of state\ninfo because of an intervening cache would cause a problem.\n\nFinally, if a cache circumvents the protocol for its own tuning\nit is a broken cache. A program bug could have the same effect. Thus,\nI believe the protocol we specify should attempt be robust in the face of\nall manner of bugs and breakage and in particular prevent incorrect\nresult. Attempting to accomadate broken caches should not be our\nconcern (but on the other hand we should try and avoid introducing\nnetwork behaviors with new protocols which increase the incentive\nfor cache administrators to breake the protocols).\n\nDave Morris\n\n\n\n"
        },
        {
            "subject": "questions &ndash;&ndash; clarifications requeste",
            "content": "I have a new respect for Roy's \"8 dimensional table\".  The requests \naren't so bad (which is what I was thinking about when I said I'd do \nit), but the responses are where the dimensions pile up. I'm still \ncranking away at it. But, as I was trying to create the synopses for \neach of the methods, I came across the following questions (in the \ncontext of HTTP 1.1); trying to be sure of the answers slowed me down some:\n\n1. Is the DELETE method allowed to have an Entity-Header and \nEntity-Body in the request? (I would think not.)\n\n2. Are the LINK and UNLINK methods allowed to have an Entity-Body in \nthe request? Or any Entity-Header field other than \"Link:\"? (I would \nthink not.)\n\n(In this message, by \"allowed\" I mean that clients aren't supposed to \nsend, and servers must ignore if sent.)\n\n3. In the PUT, DELETE, LINK, and UNLINK methods, is the intent that \ncontent negotiation is allowed (via the Accept* request header fields), \nbut it is for the purpose of negotiating the content of the entity that \ndescribes the result, not the entity corresponding to the URI? (I would \nthink so.)\n\n4. Is the Title entity-header field required in PUT and POST methods? \nStrongly suggested? Or optional? (I'd guess it's one of these :-)\n\n5. Is the Title entity-header field allowed in the request for DELETE, \nUNLINK, and LINK methods? (I would think not.)\n\n6. Is the URI-header entity-header field allowed in the request for \nPUT, DELETE, UNLINK, and LINK methods? (I would think not.)\n\n7. Is the Expires entity-header field allowed in requests for PUT and \nPOST methods? (I.e., can the client specify when the resource is to \nexpire when they create it?) How about in DELETE, LINK and UNLINK?\n\n8. Is the Last-modified entity-header field allowed in requests for \nPUT, POST, DELETE, LINK and UNLINK methods? I.e., can a client specify \nthe last-modified date of a resource?\n\n9. Is the Expires entity-header field ever returned from POST, PUT, \nDELETE, LINK or UNLINK methods? (These are never cached, and the \ndescription implies of Expires implies it is only used for caching purposes.)\n\n10. Can the PUT, DELETE, LINK, and UNLINK methods ever return 303 (See \nOther) status code?  If so, does it mean that the method succeeded or failed?\n\n11. Is it the intent that PUT, DELETE, LINK, and UNLINK methods ever \nreturn 204 (No Content), which is supposed to mean that there is no new \ncontent to show? If they don't want to return a description of the \nresult of the request, do they return Content-Length of 0, or 204? Same \nquestion for POST? For POST, it seems more plausible that there might \nbe a sequence of POSTs, some of which returned 204 if the result of the \nquery had not changed.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Accept headers and media parameter",
            "content": "The more I think about it, the more uncomfortable I get with the semantics\nof the Accept header.  Section 8.1 of the 01 draft of HTTP/1.0 (sorry, I\ndon't have the 1.1 pre-draft handy) gives the example of:\n\nAccept: text/*, text/html, text/html;version=2.0, */*\n\nand describes the precedence of each item here.  However, it's less clear\nwhat happens in a case like this:\n\nAccept: text/html;version=3.0; q=1.0, text/html;version=1.0; q=0.5,\n        text/plain; q=0.7\n\nNow, support the requested entity is available as a text/html;version=2.0\ndocument or a plaintext document.  What q does a text/html;version=2.0\ndocument have in this case?  If figuring out the answer requires the parser\nto somehow parse the values given to the version arguments, I think we're in\ntrouble.  If the answer is that q=0 since that type isn't specifically\nlisted, then in what cases are these semantics useful?\n\n--\nJim Seidman, Senior Software Engineer\nSpyglass Inc., 1230 E. Diehl Road, Naperville IL 60563\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: Partial file transfer",
            "content": "    I'd be more comfortable with this and other proposals if the original\n    data were delivered with a 'content-ID' that was unique to the\n    particular content being delievered, and any follow-up request for\n    partial delivery were indexed not by the original URL but by the\n    content-ID itself. (The lack of a content-ID would be a signal that\n    the object wasn't suitable for partial transmission or that the server\n    didn't support such.)\n    \n    The content-ID could also be used as a GET IF-DIFFERENT key, since the\n    content-ID would change when the content changed. Content-ID is\n    required to be globally unique. (It might even be a URN, but who\n    knows?)\n\nThis sounds quite like the \"Cache-Validator:\" unique-ID that I proposed\nto replace IF-MODIFIED-SInce (sorry for the weird typing, I'm on a really\nslow link).  Once you have a way for the server to explicitly control\ncaching on an object-by-object basis, lots of problems solve themselves.\n\n-Jeff\n\n\n\n"
        },
        {
            "subject": "questions &ndash;&ndash; clarifications requeste",
            "content": "Paul Leach writes:\n...\n > \n > 9. Is the Expires entity-header field ever returned from POST, PUT, \n > DELETE, LINK or UNLINK methods? (These are never cached, and the \n > description implies of Expires implies it is only used for caching purposes.)\n > \n...\n\nThe *results* of these operations can be cached.  The expires header\nis appropriate.  Requests for these methods must go all the way\nthrough to the origin server, and cannot be served from a cache\nthemselves.  The results can be identified with the Location header in\nsuch a way that a subsequent GET or HEAD can retrieve *its* results\nfrom the cached resource left in the cache by the POST, PUT, etc.\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "----------\n] From: Shel Kaphan  <sjk@amazon.com>\n] To: Paul Leach\n] Subject: questions -- clarifications requested\n] Date: Tuesday, August 29, 1995 10:25AM\n]\n] Paul Leach writes:\n] ...\n]  >\n]  > 9. Is the Expires entity-header field ever returned from POST, PUT,\n]  > DELETE, LINK or UNLINK methods? (These are never cached, and the\n]  > description implies of Expires implies it is only used for caching \npurposes.)\n]  >\n] ...\n]\n] The *results* of these operations can be cached.  The expires header\n] is appropriate.  Requests for these methods must go all the way\n] through to the origin server, and cannot be served from a cache\n] themselves.  The results can be identified with the Location header in\n] such a way that a subsequent GET or HEAD can retrieve *its* results\n] from the cached resource left in the cache by the POST, PUT, etc.\n\nThis wasn't the answer I was expecting, so let me explain why I asked \nthe question.\n\nThis answer seems inconsistent with the description of the nature of \nthe entity-bodies returned from PUT, DELETE, LINK, UNLINK:\n\n(Sec. 6.2.2) \"an entity describing the result of the action\"\n\nFrom this I concluded that it *isn't* the content associated with the \nURI, so that a subsequent GET shouldn't return the _entity_ returned \nfrom these methods. The entity-headers are associated with the resource \nnamed by the URI, so they could be cached (or the entity-headers \nassociated with an already cached copy updated). This won't do much \ngood if the entity itself isn't cached, though, as the next GET will \nstill miss and force a trip to the origin server.\n\nIn the case of DELETE, it would seem logical that any cached info \nshould be purged if the request succeeds :-)\n\nIn the case of PUT, any previous cached copy would be invalid, so an \nExpires: for the new copy isn't very interesting. And since there's no \nguarantee that a subsequent GET will retrieve what the PUT just wrote \n(at least, I don't recall seeing one on the spec), the entity-body that \nwas in the PUT request can't be cached to service subsequent GETs. \n(Adding an indication, in HTTP 1.2, to the response to say that a GET \nof what was just PUT will return the same thing might be a nice \nidea...) So, for now, it seems like a PUT should also cause any cached \ncopy to be deleted.\n\nIn the case of LINK and UNLINK, if the cache already contained an \nunexpired copy of the entity referred to by the URI, then it is \nplausible it could update the Expires (and other meta-data) on that \ncopy if the Last-Modified indicated by the respose hasn't changed.\n\nIMHO, this complexity isn't worth it.  Just say that Expires isn't \nallowed in responses from PUT, DELETE, LINK, and UNLINK.\n\nFor POST, the situation is more complex, since it says that the entity \nreturned is\n\"an entity describing or containing the result of the action\"\nIf there were a way to determine that it contained the result of the \naction, and some guarantee that a subsequent GET (of the URI specified \nby the Location header in the response) would fetch this entity, then \nyour argument would hold. However, since neither of the predicates are \ntrue, it seems that the conclusion doesn't follow.\n\nSo, that's why I posited that Expires shouldn't be returned from POST, \nPUT, DELETE, LINK, and UNLINK.\n\nAnd, having thought about it to respond to Shel, I also conclude that \nPOST, PUT, and DELETE should be specified to delete any cached copies \n(if successful), and that the results from all these methods should \n*not* be cached.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "HTTP &amp; WAI",
            "content": "Hello:\n\nI'm searching information about the best (and free as possible) solution to\nthe HTTP & WAIS relation.\nSomebody can help me ?\n\nThanks in advance,\n\n_____________________________________________________________\n\nCLAUDIA YOLANDA SASTRE C..\nIngeniera de Sistemas y Computacion\nArea de Servicios de Tecnologia Informatica\nCentro de Computo - Universidad de los Andes\nSantafe de Bogota, Colombia\ne-Mail: csastre@uniandes.edu.co\nTel: (+57 1) 2 81 56 80\nFax:  2 84 18 90\n_____________________________________________________________\n\n\n\n"
        },
        {
            "subject": "Re: Suggestion: Partial file transfer",
            "content": "The cache validator for deciding whether to update the cache for an\nentire document might be different than the cache validator you need\nto decide whether you can reuse the first 400 bytes you got the first\ntime before you dropped the connection. I'm willing to forego the\nlooser usage, but unless you tighten it up, you might not be able to\nuse the same identifier for both purposes.\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Paul Leach writes:\n > \n > ----------\n > ] From: Shel Kaphan  <sjk@amazon.com>\n > ] To: Paul Leach\n > ] Subject: questions -- clarifications requested\n > ] Date: Tuesday, August 29, 1995 10:25AM\n > ]\n > ] Paul Leach writes:\n > ] ...\n > ]  >\n > ]  > 9. Is the Expires entity-header field ever returned from POST, PUT,\n > ]  > DELETE, LINK or UNLINK methods? (These are never cached, and the\n > ]  > description implies of Expires implies it is only used for caching \n > purposes.)\n > ]  >\n > ] ...\n > ]\n > ] The *results* of these operations can be cached.  The expires header\n > ] is appropriate.  Requests for these methods must go all the way\n > ] through to the origin server, and cannot be served from a cache\n > ] themselves.  The results can be identified with the Location header in\n > ] such a way that a subsequent GET or HEAD can retrieve *its* results\n > ] from the cached resource left in the cache by the POST, PUT, etc.\n > \n > This wasn't the answer I was expecting, so let me explain why I asked \n > the question.\n > \n > This answer seems inconsistent with the description of the nature of \n > the entity-bodies returned from PUT, DELETE, LINK, UNLINK:\n > \n > (Sec. 6.2.2) \"an entity describing the result of the action\"\n > \n > >From this I concluded that it *isn't* the content associated with the \n       ^^ what is \"it\"?\n > URI, so that a subsequent GET shouldn't return the _entity_ returned \n > from these methods. The entity-headers are associated with the resource \n > named by the URI, so they could be cached (or the entity-headers \n^^^ Which one?\n > associated with an already cached copy updated). This won't do much \n > good if the entity itself isn't cached, though, as the next GET will \n > still miss and force a trip to the origin server.\n > \n\nI'm not sure I understand this paragraph, but I'll interpolate.\n*Which* URI? do you mean?  The request-URI, or the one named in the\nresponse (with Location)?  I claim it makes much more sense for it to\nbe the latter.\n\nI'm really not sure I'm with you.  I think you need to decouple the\nentity returned from the request method/URI.  If a server returns a\nresource in response to any method request, that returned resource, if\nidentified by a Location header, should *clearly* be cached (if it is\ncacheable according to other headers) using the Location response\nheader as its key.  This enables, for instance, \"shopping basket\"\napplications where a POST (with URI `A') can change the contents of the\nbasket and display the results, and then subsequent GETs (with URI `B')\ncan fetch the *cached* results.   Right now, without the Location\nheader in force, this is hard, and what is worse, if the shopping\nbasket contents are returned in response to different POSTs, GETs (and\npossibly other methods), you end up with multiple *different* cached\ncopies of the same resource, some stale.  I have been trying to make\nthis point for a long time, and I am not sure the importance of it has\nsunk in yet.  I can promise you, if you tried to implement a\nusable dynamic web service, you would soon understand why this is an\nimportant issue.  This is one of the most important things the Location\nheader buys.\n\nI'm not going to comment on all the other specific methods you\nmentioned, as they are not much in use now -- I really just want to\nconcentrate on POST, since that and GET are the two most important\nmethods right now.\n\n > For POST, the situation is more complex, since it says that the entity \n > returned is\n > \"an entity describing or containing the result of the action\"\n > If there were a way to determine that it contained the result of the \n > action, and some guarantee that a subsequent GET (of the URI specified \n > by the Location header in the response) would fetch this entity, then \n > your argument would hold. However, since neither of the predicates are \n > true, it seems that the conclusion doesn't follow.\n > \n\nPOST is the clearest case where this functionality is helpful.\n(See the shopping basket example above).\n\n > So, that's why I posited that Expires shouldn't be returned from POST, \n > PUT, DELETE, LINK, and UNLINK.\n > \n > And, having thought about it to respond to Shel, I also conclude that \n > POST, PUT, and DELETE should be specified to delete any cached copies \n > (if successful), and that the results from all these methods should \n > *not* be cached.\n > \n > Paul\n > \n > \n\nSorry, but to the extent I follow your argument, I could not disagree\nwith you more.  My thoughts about this are driven by practical\nexamples where I've had to do things I don't want to describe here to\nsimulate this functionality.  I've been looking forward to the day\nwhen HTTP supports such things naturally, and I certainly hope\nthe HTTP-WG will not throw away this new functionality.  Indeed, I\nsincerely hope it will be more fully written into the spec!\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "More ruminations.\n\nPaul Leach writes:\n > For POST, the situation is more complex, since it says that the entity \n > returned is\n > \"an entity describing or containing the result of the action\"\n > If there were a way to determine that it contained the result of the \n > action,\n\nWhat does that mean?  By definition what is returned as the result of\nan action is the result of the action.\n\n   and some guarantee that a subsequent GET (of the URI specified \n > by the Location header in the response) would fetch this entity,\n\nThat is what the Location header is defined to do.  The alpha 1.1 spec\nsays:  \"For 2xx responses, the location should be the URL needed to\nretrieve that same resource again ...\".  This presumably means, e.g.,\nby using GET.\n\n then \n > your argument would hold. However, since neither of the predicates are \n > true, it seems that the conclusion doesn't follow.\n > \n\nAs far as I can see, both are true.\n--Shel\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Shel writes:\n]\n] More ruminations.\n]\n] Paul Leach writes:\n]  > For POST, the situation is more complex, since it says that the entity\n]  > returned is\n]  > \"an entity describing or containing the result of the action\"\n]  > If there were a way to determine that it contained the result of the\n]  > action,\n]\n] What does that mean?  By definition what is returned as the result of\n] an action is the result of the action.\n\nI didn't write it! Here's what I thought it was saying:  there could be \ntwo flavors of POSTs; an example of each might be:\nFlavor 1:\nThe entity-body in the request says \"order 17 gross of 3 penny nails\"; \nand the the entity-body in the response says: \"your order has been accepted\".\nFlavor 2:\nThe entity-body in the request says: \"fetch me item 33\" and the \nentity-body in the response is item 33.\n\nThe first is an example of the entity-body describing the result of the \naction; the second is one containing the result of the action.\n\nIn flavor 2, one could expect to do a GET on the URL in the Location \nheader field in the response, and get item 33 again. For flavor 1, \nalthough it conflicts with your observation below, it doesn't seem to \nme to make sense that there _must_ exist a URL to put in the Location \nheader such that a GET on it would return an entity containing \"your \norder has been accepted\" again.  Nor does it seem very useful to cache \neither \"order 17 gross...\" or \"your order has been accepted\".  Is that \nreally the intent?\n\n]\n]    and some guarantee that a subsequent GET (of the URI specified\n]  > by the Location header in the response) would fetch this entity,\n]\n] That is what the Location header is defined to do.  The alpha 1.1 spec\n] says:  \"For 2xx responses, the location should be the URL needed to\n] retrieve that same resource again ...\".  This presumably means, e.g.,\n] by using GET.\n\nThat's what it says all right. Inattentive me -- I didn't mentally \nintegrate the it with the semantics of POST (or PUT) properly -- I was \nreading it in the context of GETs that did content negotiation. For \nPUT, that certainly seems to be strong enough to allow the caching of \nthe request entity-body using as a key the URL specified in the \nLocation header in the response. For POST, I still have the \nreservations noted above.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Possible optimization to StateInfo proposa",
            "content": "Dave Kristol:\n>\n>koen@win.tue.nl (Koen Holtman) wrote:\n    [... about the meaning of `idempotent'....]\n>  > >From this, I read that:\n>  >  - GET and HEAD are defined to be the idempotent methods\n>  >  - idempotent means `safe'.\n>\n>Sorry to be a pain, but what do you mean by \"safe\"?  This is the\n>philosophical vs. operational divide.  The definition so far has been\n>operational:  GET and HEAD are idempotent; they have no side-effects.\n\nMy operational definition is: GET and HEAD are idempotent.\n\nThe `no side-effects' in the spec is also a philosophical statement, see the\nlast paragraph of the `idempotent methods' section.  A side effect like\nincrementing a page counter is allowed.\n\n> What is the\n>philosophical definition, in the context of WWW?\n\nThe philosophical definition of `idempotent' is: if I let my browser issue a\nnon-idempotent request on an URI, I can expect to get no side effects that\nhave an unexpected significance to me or others.\n\nWhat this means is that server administrators may not put scripts that send\nmail, post news, cause products to be ordered, cause me to be subscribed to\na mailing list, behind idempotent URI's.  Such scripts should be put behind\nnon-idempotent URI's.\n\nThe main reasoning behind this philosophical definition is:\n\n If www.blah.com puts an `auto-subscribe to our junkmail list` function\n behind a GET in http://www.blah.com/iwantjunkmail, some joker om foo.edu\n will start putting <img src=http://www.blah.com/iwantjunkmail> on\n a totally unrelated page like http://foo.edu/~joker/barney.html.\n\n\n>Dave Kristol\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Comments on draft-v1003a",
            "content": "Below are some comments on\n\n  http://www.ics.uci.edu/~fielding/test/draft-v10-03a.ps.gz .\n\nMost of the comments apply equally to earlier drafts of the HTTP/1.0 spec.\n\nIf a comment also applies to the draft HTTP/1.1 spec, a remark between\nbrackets is made.  I will comment more fully on the draft HTTP/1.1\nspec in a future message.\n\n\n1.3 Terminology\n---------------\n\nAdd a definition of `idempotent'.  Proposed definition:\n\n  Idempotent request\n\n    A request done with either the GET or the HEAD method.\n\nThe word `idempotent' is used in a very confusing way in the spec, its\nuse is very different from its use in mathematics.\n\nDefining it would eliminate some of the confusion, but not all of it.\nA better way to eliminate the confusion would be to use another term\nlike `browse-only request' or `retrieval request' instead of\n`idempotent request'.\n\nOne could argue that it is too late to change `idempotent' to\nsomething else in the 1.0 spec.  (IMO, it should definitely be changed\nin the 1.1 spec.)\n\n\n8.10  Last-Modified\n-------------------\n\nIn current caching practice, this header is very important: caches use\nthe heuristic that documents that contain no Last-modified, no\nExpires, and no Pragma should not be cached.\n\nA vast number of CGI scripts producing dynamic output depend on this\nheuristic to be used.  We are talking about a best-current-practice\ndocument, so I feel that this heuristic should be recorded somewhere,\nfor example in a note at the end of Section 8.10:\n\n  Note: For reasons of downwards compatibility with old CGI scripts,\n  it is suggested that caches do not cache HTTP/0.9 and HTTP/1.0\n  responses that contain no Last-Modified, no Expires, and no Pragma\n  header.\n\n(The note above could also be put into the 1.1 spec.)\n\nAlternatively, an appendix could be devoted to summarizing all caching\nimplications of the protocol and describing current caching practice.\nI think I have enough material to be able to write a first draft of\nsuch an appendix within a few days.  Roy, if you want such a draft,\ntell me.\n\n\n8.11  Location\n--------------\n\nAdd a remark that, except in 301 and 302 responses, the URL given is\nalways supposed to be accessed with the GET method.\n\nThis is particularly relevant for 200 responses to POST requests.  The\nspec is currently not clear enough about this.\n\n(This also applies to the 1.1 spec.)\n\n8.13  Pragma\n------------\n\nI quote:\n\n>All pragma directives specify optional behavior from the viewpoint of\n                               ^^^^^^^^^^^^^^^^^\n>the protocol;\n[....]\n>When the \"no-cache\" directive is present in a request message, a\n>caching intermediary must forward the request toward the origin server\n                      ^^^^\n\nContradiction?  I believe that the consensus is that defined pragma\nheaders should always be honored.  I propose to delete the `optional\nbehavior' sentence.\n\n\n10.2  Idempotent methods\n------------------------\n\nAs discussed above, the word `idempotent' should be either defined in\nSection 1.3, or changed to something else like `browse-only' or\n`retrieval'.\n\nQuoting, about idempotent methods:\n\n>These methods should be considered \"safe\" and should not have side effects.\n\nIf something `should not have side effects', one would expect it to be\ncacheable.  But GET and HEAD transactions are not always cacheable.\n\nThe quoted sentence above should be changed to read\n\n  These methods should be considered \"safe\".\n\nSaying `should not have side effects' in one paragraph, and then\nweakening this to `should not have unexpected/unrequested side\neffects' in the next paragraph is too confusing.\n\n(this also applies to the 1.1 spec)\n\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": ">   Idempotent request\n> \n>     A request done with either the GET or the HEAD method.\n\nIsn't this too circular to be useful?\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": ">Idempotent\n\nI second the request for a clear definition in the context of this spec.\nAnother good reason for this: idempotent is not in any dictionary\n(including my Webster's unabridged) that I could find.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "According to Paul Hoffman:\n> \n> >Idempotent\n> \n> I second the request for a clear definition in the context of this spec.\n> Another good reason for this: idempotent is not in any dictionary\n> (including my Webster's unabridged) that I could find.\n> \n> \n\nAs a professor of mathematics I will be happy to supply a formal\ndefinition on request.  However, I would say that based on the\nmathematical meaning I had always assumed that an idempotent request\nis one which results in identical responses if it is sent multiple\ntimes.  Obviously in this sense \"GET\" is not an idempotent method.\n\nIn any case, we have ample evidence that this term is causing confusion\nand needs to be replaced or carefully defined.\n\nJohn Franks\n\n\n\n"
        },
        {
            "subject": "Idempoten",
            "content": "% >Idempotent\n% \n% I second the request for a clear definition in the context of this spec.\n% Another good reason for this: idempotent is not in any dictionary\n% (including my Webster's unabridged) that I could find.\n\nBeing a mathematician, my idea of idempotence is a function f such as\nf(f(x)) = f(x) . \n\nNow, I admit it is not something I'll put on the http specs, but what about\n\n\"something which does not change if requested twice in rapid succession\"?\n\nciao, .mau.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "    Date: Wed, 30 Aug 1995 09:11:37 -0700\n    From: ietf-lists@proper.com (Paul Hoffman)\n\n    >Idempotent\n\n    I second the request for a clear definition in the context of this spec.\n    Another good reason for this: idempotent is not in any dictionary\n    (including my Webster's unabridged) that I could find.\n\nTry \"The Harper Collins Dictionary of Mathematics,\" 1991:\n\n\"idempotent, adj. 1. (of a matrix, function or ring element) having the\nproperty that it is equal to its own square. For example, the matrix\n\n    -         -\n    |  2   2  |\n    | -1  -1  |\n    -         -\n\nis idempotent.  2. (of an operation) having the property that every\nelement of its domain is idempotent with respect to it; for example,\nset-theoretic intersection and union are idempotent, since S union S =\nS = S intersection S.  A RING of which every member is idempotent is\ncalled a BOOLEAN RING.\"\n\nClear enough?\n\nRegards,\nGlenn Adams\n\nS\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": ">>Idempotent\n>\n>I second the request for a clear definition in the context of this spec.\n>Another good reason for this: idempotent is not in any dictionary\n>(including my Webster's unabridged) that I could find.\n\nIt's a mathematical term.\n\nHere's what our on-line OED says (special symbols are a bit mangled):\n\n>idempotent\n>\n>idempotent <e>ide;mpotent, <e>i:dempou.tent, , a. and sb. f. L. idem same +\n>potent-em\n>powerful, potent\n>\n>A. adj. Of a quantity or element a: having the property that a x a = a,\n>where x represents\n>multiplication or some other (specified) binary operation. Also applied to\n>an operator or set for which\n>this is true for any element a and to statements expressing this fact.\n>\n>      1870 B. Peirce in Amer. Jrnl. Math. (1881) IV. 104 When an\n>expression..raised to a square\n>      or higher power..gives itself as the result, it may be called idempotent.\n>\n>      1937 A. A. Albert Mod. Higher Algebra (1938) iii. 88 A matrix E is\n>called idempotent if E2\n>      = E.\n>\n>      1937 Duke Math. Jrnl. III. 629 We recall that A &211; B if and only\n>if A = (A, B) and B\n>      = [A, B], and that union and crosscut are associative, commutative,\n>and idempotent operations.\n>\n>      1940 W. V. Quine Math. Logic 56 A binary mode of statement\n>composition..is said to\n>      be..idempotent if &431.<phi> &216. ;fkf&432. is true for all\n>statements <phi>;\n>\n>      1941 Birkhoff & MacLane Surv. Mod. Algebra xi. 313 All of these\n>except for the idempotent\n>      laws and the second distributive law correspond to familiar laws of\n>arithmetic.\n>\n>      1941 Mind L. 274 The element is only idempotent with respect to the\n>combining relation\n>      defined as the combining relation of the group.\n>\n>      1950 W. V. Quine Methods of Logic (1952) Sect.1. 3 `pp' reduces to\n>`p'. Conjunction is\n>      idempotent, to persist in the jargon.\n>\n>      1959 E. M. McCormick Digital Computer Primer 181 It is further\n>apparent..that A + A = A\n>      and..that A x A = A. These are sometimes referred to as the\n>idempotent laws.\n>\n>      1967 A. Geddes tr. Dubreil & Dubreil-Jacotin's Lect. Mod; Algebra i.\n>22 If every element of\n>      E is idempotent, the composition law is called idempotent and E is\n>called an idempotent set.\n>\n>B. sb. An idempotent element; also in more restricted use (see quot. 1958).\n>\n>      1941 Birkhoff & MacLane Surv. Mod. Algebra i. 6 Prove that the\n>following rules hold in any\n>      integral domain:..(h) the only `idempotents' (that is, elements x\n>satisfying xx = x) are 0 and 1.\n>\n>      1958 S. Kravetz tr. Zassenhaus's Theory of Groups (ed. 2) 182 The\n>element e is called an\n>      idempotent if ee = e and if e is not a zero element.\n>\n>      1960 C. E. Rickart Gen. Theory Banach Algebras i. 35 Let &326; be a\n>Banach algebra and let\n>      e be a proper idempotent in &326; (that is, e &222; 0, 1 and e2 = e).\n>\n>Hence\n>\n>idempotence\n>\n>idempotence (stress variable),\n>\n>idempotency\n>\n>idem'potency, the property of being idempotent.\n>\n>      1940 Mind XLIX. 461 The truth is that Eddington, in spite of all that\n>he says about getting all\n>      the mathematics he wants out of the idempotency of the J symbols,\n>employs them in accordance\n>      with the laws of ordinary algebra whenever he thinks fit.\n>\n>      1940 W. V. Quine Math. Logic 60 In the case of conjunction and\n>alternation, repetition of\n>      components has..been seen to be immaterial (idempotence).\n>\n>      1957 P. Suppes Introd. Logic ix. 205 Equations (9) and (10) express\n>what is usually called\n>      the idempotency of union and intersection.\n>\n>      1959 K. R. Popper Logic Sci. Discovery 351 p (aa, b) = p (a, b)...\n>This is the law of\n>      idempotence, sometimes also called the `law of tautology'.\n>\n>      1960 P. Suppes Axiomatic Set Theory ii. 27 The next three theorems\n>assert the commutativity,\n>      associativity, and idempotence of union.\n>\n>      1968 New Scientist 16 May 339/1 Idempotency..occurs if an operation\n>produces no change in\n>      the number or set on which it operates.\n>\n\n\nI tend to agree with the line of thought that we may need to define some\nother term of our own to make clear what we really mean.\n\n\n---\n    Albert Lunde                      Albert-Lunde@nwu.edu\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Paul Hoffman writes:\n > >Idempotent\n > \n > I second the request for a clear definition in the context of this spec.\n > Another good reason for this: idempotent is not in any dictionary\n > (including my Webster's unabridged) that I could find.\n > \n > \n\nCan anyone think of a word that means \"without significant\nside-effects\"?  Maybe there's some good word from functional\nprogramming languages. I think that is closer to what is needed than\n\"idempotent\", which always did seem like not quite the right word.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "I think we're better off sticking with computer science terminology,\nrather than reaching into mathematics when describing Internet\nprotocols. I'd suggest we say that GET should be\n\n  \"without additional side-effects if invoked again.\"\n\nThat is, a 'GET' method might cause side effects, but reinvoking it\nwith the same URL shouldn't have any additional side effects.\n\nNote that this definition says nothing at all about what is returned\nby the GET method, which may return a different result every single\ntime, no matter how closely spaced the calls are. The issue is whether\ndoing it again has an effect on the state of the server.\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Paul Leach writes:\n > \n > Shel writes:\n > ]\n > ] More ruminations.\n > ]\n > ] Paul Leach writes:\n > ]  > For POST, the situation is more complex, since it says that the entity\n > ]  > returned is\n > ]  > \"an entity describing or containing the result of the action\"\n > ]  > If there were a way to determine that it contained the result of the\n > ]  > action,\n > ]\n > ] What does that mean?  By definition what is returned as the result of\n > ] an action is the result of the action.\n > \n > I didn't write it!\n\nThe part I questioned is the part you wrote.  The spec is quite\ngeneral, but it doesn't have to be more specific.  The usage is that\nwhen any of these methods are send by a user agent, the user agent is\nthen going to have to display some page -- a page that is returned by\nthe server.  In order to make it possible to storyboard interactive\nservices in a general way, it has to be possible to have the\nreturned resource be whatever the server wants to send in response to\nthe request.\n\n Here's what I thought it was saying:  there could be \n > two flavors of POSTs; an example of each might be:\n > Flavor 1:\n > The entity-body in the request says \"order 17 gross of 3 penny nails\"; \n > and the the entity-body in the response says: \"your order has been accepted\".\n > Flavor 2:\n > The entity-body in the request says: \"fetch me item 33\" and the \n > entity-body in the response is item 33.\n > \n > The first is an example of the entity-body describing the result of the \n > action; the second is one containing the result of the action.\n > \n > In flavor 2, one could expect to do a GET on the URL in the Location \n > header field in the response, and get item 33 again. For flavor 1, \n > although it conflicts with your observation below, it doesn't seem to \n > me to make sense that there _must_ exist a URL to put in the Location \n > header such that a GET on it would return an entity containing \"your \n > order has been accepted\" again.  Nor does it seem very useful to cache \n > either \"order 17 gross...\" or \"your order has been accepted\".  Is that \n > really the intent?\n > \n\nNo, the intent is that if the POST returns something that it would\nmake sense for a GET to get from a cache later, that it should be\npossible.  If this is desired, the server should (a) either give the\nresponse a URI with Location that later GETs will correspond to, or\n(b) arrange for future GETs to use the same request-URI as the POST\ndid, and (c) arrange for the document to be cacheable by proper use of\nExpires, Last-modified, etc.  It need not be the case that all returns\nfrom POST be cached and that links be provided to fetch it from the\ncache, it only needs to be *possible* so that people who need this\nfunctionality can have it.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "RE: Idempoten",
            "content": "----------\n Maurizio Codogno writes:\n]\n] % >Idempotent\n] %\n] % I second the request for a clear definition in the context of this spec.\n] % Another good reason for this: idempotent is not in any dictionary\n] % (including my Webster's unabridged) that I could find.\n]\n] Being a mathematician, my idea of idempotence is a function f such as\n] f(f(x)) = f(x) .\n]\n] Now, I admit it is not something I'll put on the http specs, but what about\n]\n] \"something which does not change if requested twice in rapid succession\"?\n\nHow about:\n\n\"the results do not change in a way that matters (to the client or \nserver) if repeated twice in a row (i.e., with no other intervening methods)\"\n\nThe reason that GET incrementing a counter can still be idempotent is \nthat it doesn't matter to the client or server. (If it did, e.g., \npayment was based on hits, then this arguement wouldn't apply.)\n\nI don't have a position (yet) on whether GET should be required to be \nidempotent. However, if it isn't, then any non-idempotent GET *must* be \nmarked non-cacheable.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Shel Kaphan writes:\n > Paul Hoffman writes:\n >  > >Idempotent\n >  > \n > Can anyone think of a word that means \"without significant\n > side-effects\"?  Maybe there's some good word from functional\n > programming languages. I think that is closer to what is needed than\n > \"idempotent\", which always did seem like not quite the right word.\n\nNo, side-effects are not the issue at all.  The result returned by the\nserver needs to be the \"same\" each time to be considered idempotent,\nbut each time a result is computed, it can have whatever side effects\nit wants to.  It could have no side effects itself and still return\nvery different results each time - obviously, something else is having\na side effect in that case.\n\nActually, \"same\" can be quite different too, as long as it calls it\nthe \"same\".  The server decides what it thinks is equivalent to what.\nIf it decides badly, it is a bad service.\n\nAny particular method may or may not be idempotent, in my opinion,\nwhether GET, POST, SEARCH or whatever.  There is no reason to define\nin the spec that a particular method is always (or never) idempotent.\nThe server simply tells the client whether a particular use of a\nmethod is idempotent with the no-cache header, or whatever it is\ncalled now.\n\nDaniel LaLiberte (liberte@ncsa.uiuc.edu)\nNational Center for Supercomputing Applications\nhttp://union.ncsa.uiuc.edu/~liberte/\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Let me try to guess at some of the operational motivation behind the\nconcept we're trying to define for GET vs POST.\n\nRight now, there are browsers around with a 'reload' button. If you\n'reload' something that you obtained using 'GET', then you might get\ndifferent results, but it is 'safe' to press reload.  However, if you\n'reload' something that you obtained using 'POST', then you might not\nwant to blindly resubmit the same form data. Many browsers either\ncomplain or ask 'Resubmit FORM data?' when asked to 'reload' something\nobtained using 'POST'.\n\nTranslating back from this usage scenario to a set of operational\nrequirements leads me to thinking that we want 'GET' to have no\nadditional side effects if repeated.\n\nThis is only vaguely related to the notion of idempotency. If you\ndefine:\n\n    server-state-after-GET = f<state> ( server-state-before-GET ) \n\nand ask that f<state> be idempotent. Note that the idempotency only\napplies to the 'server state' and not to the value returned.\n\nThe property 'idempotent' can only apply when functions have a range\nthat is a subset of their domain.\n\nIf you define\n\n        value-returned = f<value> (server-state-before-GET )\n\nyou'll note that range of 'value-returned' is a different\ndatatype/class and is disjoint from the domain of\n'server-state-before-GET'.\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Larry Masinter writes:\n > Let me try to guess at some of the operational motivation behind the\n > concept we're trying to define for GET vs POST.\n\n > ...\n\n > Translating back from this usage scenario to a set of operational\n > requirements leads me to thinking that we want 'GET' to have no\n > additional side effects if repeated.\n > \n > This is only vaguely related to the notion of idempotency. If you\n > define:\n >\n >     server-state-after-GET = f<state> ( server-state-before-GET ) \n > \n > and ask that f<state> be idempotent. Note that the idempotency only\n > applies to the 'server state' and not to the value returned.\n\nRight.  This no-side-effects property might be called server-state\nidempotence, if you want to confuse people.  Let's be explicit and\ncall the tag: \"side-effects\".  This is independent of and orthogonal\nto \"no-cache\" which implies the result will be different each time.\n(Actually, \"no-cache\" could simply mean don't keep a copy, for\nwhatever reason.)\n\nBut similar to no-cache, I dont believe we should assume or require\nthat GET has no side-effects, or that POST always does.  I can think\nof contrary cases for each.\n\ndan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Daniel LaLiberte writes:\n > Shel Kaphan writes:\n >  > Paul Hoffman writes:\n >  >  > >Idempotent\n >  >  > \n >  > Can anyone think of a word that means \"without significant\n >  > side-effects\"?  Maybe there's some good word from functional\n >  > programming languages. I think that is closer to what is needed than\n >  > \"idempotent\", which always did seem like not quite the right word.\n > \n > No, side-effects are not the issue at all.  The result returned by the\n > server needs to be the \"same\" each time to be considered idempotent,\n\nWe're talking about whether the concept of idempotence is the right\nconcept, not whether GET and HEAD satisfy a dictionary definition of \"idempotent\".\nAnd side effects are exactly the issue.\n\nLarry Masinter said it like this:\n\nI think we're better off sticking with computer science terminology,\nrather than reaching into mathematics when describing Internet\nprotocols. I'd suggest we say that GET should be\n\n  \"without additional side-effects if invoked again.\"\n\n...\n\nNote that this definition says nothing at all about what is returned\nby the GET method, which may return a different result every single\ntime, no matter how closely spaced the calls are. The issue is whether\ndoing it again has an effect on the state of the server.\n\n > but each time a result is computed, it can have whatever side effects\n > it wants to.\n\nIf that were true, it would be unsafe to ever cache anything.  What we\nneed is the appropriate definition of whatever-property-it-is that\nmakes it reasonable and appropriate to cache something.\n\n  It could have no side effects itself and still return\n > very different results each time - obviously, something else is having\n > a side effect in that case.\n > \n\nSo?  If there are no side effects on the server, then unless a cached\ncopy is outdated, it is legitimate to serve it from the cache.  It is\nthe side effects that matter, not the difference in the returned\nresult each time you re-fetch it from the origin server.\n\n > Actually, \"same\" can be quite different too, as long as it calls it\n > the \"same\".\n\nSay what?\n\n  The server decides what it thinks is equivalent to what.\n > If it decides badly, it is a bad service.\n > \n > Any particular method may or may not be idempotent, in my opinion,\n > whether GET, POST, SEARCH or whatever.  There is no reason to define\n > in the spec that a particular method is always (or never) idempotent.\n > The server simply tells the client whether a particular use of a\n > method is idempotent with the no-cache header, or whatever it is\n > called now.\n > \n\nThere you've got an interesting idea, though I'll have to consider the\nramifications.  Most methods, such as POST, don't really make sense\nunless they go all the way through to the origin server.  What would\nit mean to serve a POST from a cache? It's not clear if it really\nmakes any sense.  There would undoubtedbly be some problems with\ndownwards compatibility too, but I think this is worth thinking\nthrough.\n\n > Daniel LaLiberte (liberte@ncsa.uiuc.edu)\n > National Center for Supercomputing Applications\n > http://union.ncsa.uiuc.edu/~liberte/\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Larry Masinter writes:\n\n > I think we're better off sticking with computer science terminology,\n > rather than reaching into mathematics when describing Internet\n > protocols. I'd suggest we say that GET should be\n > \n >   \"without additional side-effects if invoked again.\"\n > \n > That is, a 'GET' method might cause side effects, but reinvoking it\n > with the same URL shouldn't have any additional side effects.\n > \n\nThough I generally agree with what you're saying, there's a slight\nproblem with this, I think.  If the first GET on a URL has side\neffects necessary to the semantics intended by the server, then it has\nto avoid being served from a cache.  But since caches are potentially\npublic, and also since other methods can leave things in caches under\nthe URL the GET will use (a POST with the same request-URI; anything\nthat returns 2xx and a Location header) it seems a bit dangerous to\nbuild a system where a \"first\" GET (however that could be detected)\nwas supposed to have side effects, but subsequent ones weren't.\nInstead, couldn't we say that if GET on a particular URL has side\neffects and produces a cacheable result, the side effects must be\n*unimportant* to the server, since by making the result cacheable it\nis giving up the right to \"see\" certain future GETs on that URI.\n\n > Note that this definition says nothing at all about what is returned\n > by the GET method, which may return a different result every single\n > time, no matter how closely spaced the calls are. The issue is whether\n > doing it again has an effect on the state of the server.\n > \nI completely agree.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "> Though I generally agree with what you're saying, there's a slight\n> problem with this, I think.  If the first GET on a URL has side\n> effects necessary to the semantics intended by the server, then it has\n> to avoid being served from a cache.\n\nHow'd it get into the cache if it was never GET-ed in the first place?\nImmaculate HTML?\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Larry Masinter writes:\n > > Though I generally agree with what you're saying, there's a slight\n > > problem with this, I think.  If the first GET on a URL has side\n > > effects necessary to the semantics intended by the server, then it has\n > > to avoid being served from a cache.\n > \n > How'd it get into the cache if it was never GET-ed in the first place?\n > Immaculate HTML?\n\nClose.\n\nHere are three ways I can think of:\n\n1. A prior POST on the same URI as the subsequent GET can leave\nsomething in a cache.\n\n2. Any prior response that returned a Location header with the same URI\nas the subsequent GET, in a 2xx response, can leave something in the\ncache.\n\n3. If the cache is a public cache, it might have been\nanother GET.  The side effect might be one that you want to cause the\nfirst time a given *user* GETs a certain resource, not just the first time\nanyone behind a cache requested it.\n\n1. and 2. are extremely useful features, not bugs.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Shel writes:\n] No, the intent is that if the POST returns something that it would\n] make sense for a GET to get from a cache later, that it should be\n] possible.  If this is desired, the server should (a) either give the\n] response a URI with Location that later GETs will correspond to, or\n] (b) arrange for future GETs to use the same request-URI as the POST\n] did, and (c) arrange for the document to be cacheable by proper use of\n] Expires, Last-modified, etc.  It need not be the case that all returns\n] from POST be cached and that links be provided to fetch it from the\n] cache, it only needs to be *possible* so that people who need this\n] functionality can have it.\n\nAs currently written, it seems that returning a Location: header is \nmandatory (even if the same Request-URI would do). This makes  web \npages that are even more dynamic than the shopping basket type, where \nthere is no URL that could be used to refetch the result entity of a \nPOST, impossible.\n\nLet me summarize how I would prpopose to satisfy the needs of both \nstyles and see if you agree:\n\nFor POST, if the response entity-body, in the language of the spec, \n\"contains the result of the action\", and \"corresponds to a resource\", \nand the server wishes the result to be able to be cached, then the \nLocation: header is required, as is proper use of Expires, \nLast-Modified, etc.  If the response entity-body \"describes the result \nof the action\", and does not correspond to a resource, then Location: \nmust not be present, and Expires, Last-Modified, etc., relating to \ncaching are not allowed.\n\nOur disagreement stems from me focussing on the second case, and you on \nthe first.  On the first case, I agree completely, now that I understand it.\n\nI think that the language of section 8.19 on the Location header is at \nbest misleading with respect to your desired POST behavior. (In \naddition to saying that the Location header \"should\" be returned, which \nis at odds with the statement for status code 200 in section 6.2.2 that \nit \"may\" be returned.)\n\nThe first sentence of section 8.19 says  \"the Location response header \nfield defines the exact location of the resource that was identified by \nthe Request-URI\".  For GET, this is true. In the POST case, it is not \nthe resource identified by the Request-URI, but the location of the \nresource created by the POST.\n\nJust for the sake of concreteness, I propose the following replacement \nfor the first sentence of section 8.19:\n\n\"If the entity-body in a response corresponds to a resource (or, in the \ncase of HEAD, would correspond to a resource if it were present), the \nLocation response header field defines the exact location of that \nresource -- even if it is the same as the Request-URI.\"\n\nAnd the wording in section 6.2.2 should change to say that the response \n\"should\" include a Location header field, instead of \"may\", when the \nentity in the response corresponds to a resource.\n\nI also propose the following addition the the end of the description \nfor status code 200 in section 6.2.2:\n\n\"If the response entity-body describes the result of the action, and \ndoes not correspond to a resource, then a Location header field must \nnot be included, and the fields related to caching (Expires, \nLast-Modified, etc.) are not allowed.\"\n\nI believe that this will allow Shel's scenarios, as well as more dynamic ones.\n\nPaul\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Paul Leach writes:\n...\n > As currently written, it seems that returning a Location: header is \n > mandatory (even if the same Request-URI would do).\n\nI don't think so.  Where do you get this?\n\n...\n\n > For POST, if the response entity-body, in the language of the spec, \n > \"contains the result of the action\", and \"corresponds to a resource\", \n > and the server wishes the result to be able to be cached, then the \n > Location: header is required, as is proper use of Expires, \n > Last-Modified, etc.\n\nI don't think they're generally required, unless you need the functionality\nthey provide.\n\n  If the response entity-body \"describes the result \n > of the action\", and does not correspond to a resource, then Location: \n > must not be present, and Expires, Last-Modified, etc., relating to \n > caching are not allowed.\n > \nFine.\n\n > Our disagreement stems from me focussing on the second case, and you on \n > the first.  On the first case, I agree completely, now that I understand it.\n > \n > I think that the language of section 8.19 on the Location header is at \n > best misleading with respect to your desired POST behavior. (In \n > addition to saying that the Location header \"should\" be returned, which \n > is at odds with the statement for status code 200 in section 6.2.2 that \n > it \"may\" be returned.)\n > \n\n8.19 doesn't say Location \"should\" be returned.  But I agree that the spec\nshould spell out how it is to be used by caches, at least a little,\nsince it is so important for cache implementers to get it right, and\nit is so easily misunderstood.\n\n\n > The first sentence of section 8.19 says  \"the Location response header \n > field defines the exact location of the resource that was identified by \n > the Request-URI\".  For GET, this is true. In the POST case, it is not \n > the resource identified by the Request-URI, but the location of the \n > resource created by the POST.\n > \nYes.\n\n > Just for the sake of concreteness, I propose the following replacement \n > for the first sentence of section 8.19:\n > \n > \"If the entity-body in a response corresponds to a resource (or, in the \n > case of HEAD, would correspond to a resource if it were present), the \n > Location response header field defines the exact location of that \n > resource -- even if it is the same as the Request-URI.\"\n > \n > And the wording in section 6.2.2 should change to say that the response \n > \"should\" include a Location header field, instead of \"may\", when the \n > entity in the response corresponds to a resource.\n > \nI don't know -- I think you should look at the uses of \"should\" in\n8.19 a little more closely.\n\n > I also propose the following addition the the end of the description \n > for status code 200 in section 6.2.2:\n > \n > \"If the response entity-body describes the result of the action, and \n > does not correspond to a resource, then a Location header field must \n > not be included, and the fields related to caching (Expires, \n > Last-Modified, etc.) are not allowed.\"\n > \nOn first reading, that seems reasonable.\n\n > I believe that this will allow Shel's scenarios, as well as more dynamic ones.\n > \n > Paul\n > \n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Comments on draft-v1003a",
            "content": "Daniel LaLiberte:\n>No, side-effects are not the issue at all.  \n\nI guess it is time for me to clarify what the `idempotent' issue was\nwhen I started this thread.  I wrote:\n\n|The word `idempotent' is used in a very confusing way in the spec,\n|its use is very different from its use in mathematics.\n\nAn `idempotent method', as described (defined implicitly) in Section\n10.2 of the draft spec should have no bad/unwanted/unintended effects\n*whenever* issued.\n\nThus, `draft-http-spec-idempotent' differs in two ways from\n`math-idempotent':\n\n 1) in the mathematical meaning of idempotence, (bad) side effects are\n    allowed to be present the first time\n 2) in the mathematical meaning, *all* side effects, not just `bad'\n    side effects, must be absent the second time.\n\nIn my original comment on the spec, I basically requested that any\nterminology normally associated with discussions about caching\n(terminology like `idempotent' and `side effect') be eliminated from\nthe security discussion of the spec.\n\nThis thread has developed into a discussion about caching, side\neffects, and requests that are idempotent in the _mathematical_ sense.\nThis is topic drift is fine with me, I like discussing caching, but I\ndo want to point out that this topic drift is exactly the kind of\nthing that makes `idempotent' so confusing.\n\nYou wrote, about idempotent in the mathematical sense:\n\n>Any particular method may or may not be idempotent, in my opinion,\n>whether GET, POST, SEARCH or whatever.  There is no reason to define\n>in the spec that a particular method is always (or never) idempotent.\n>The server simply tells the client whether a particular use of a\n>method is idempotent with the no-cache header, or whatever it is\n>called now.\n\nI agree completely.\n\n>Daniel LaLiberte (liberte@ncsa.uiuc.edu)\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "> For POST, if the response entity-body, in the language of the spec, \n> \"contains the result of the action\", and \"corresponds to a resource\", \n> and the server wishes the result to be able to be cached, then the \n> Location: header is required, as is proper use of Expires, \n> Last-Modified, etc.  If the response entity-body \"describes the result \n> of the action\", and does not correspond to a resource, then Location: \n> must not be present, and Expires, Last-Modified, etc., relating to \n> caching are not allowed.\n\nI wouldn't trust an \"Expires\" that didn't actually come along with the\ndocument being served. There's a security hole otherwise; Joe\n'Microsoft-is-Evil' might put up a form <click here> that returns\n\n================================================================\nLocation: http://www.microsoft.com\nExpires: 01 Jan 2001 12:00:00 pST\n\n<body>I am the evil Borg.</body>\n================================================================\n\n\nWhy don't we leave it as 'Can't cache POST' and not bother gilding\nthis particular lily?\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Larry Masinter writes:\n[ Paul wrote: ]\n > > For POST, if the response entity-body, in the language of the spec, \n > > \"contains the result of the action\", and \"corresponds to a resource\", \n > > and the server wishes the result to be able to be cached, then the \n > > Location: header is required, as is proper use of Expires, \n > > Last-Modified, etc.  If the response entity-body \"describes the result \n > > of the action\", and does not correspond to a resource, then Location: \n > > must not be present, and Expires, Last-Modified, etc., relating to \n > > caching are not allowed.\n > \n > I wouldn't trust an \"Expires\" that didn't actually come along with the\n > document being served. There's a security hole otherwise; Joe\n > 'Microsoft-is-Evil' might put up a form <click here> that returns\n > \n\nrom what are you construing that there would or could be Expires\nheaders sans the document they correspond to?\n\n > ================================================================\n > Location: http://www.microsoft.com\n > Expires: 01 Jan 2001 12:00:00 pST\n > \n > <body>I am the evil Borg.</body>\n > ================================================================\n > \n\nThis really argues against all such use of the Location header,\ndoesn't it?   Not just the Expires header that goes along with it?\nLocation in 2xx responses could be used in just this way\nin any case.  Maybe there just needs to be a restriction that Location\nin 2xx headers must be on the same server as the request URI. \n(This would be similar to the security precautions in the Netscape\ncookie proposal).\n\n > \n > Why don't we leave it as 'Can't cache POST' and not bother gilding\n > this particular lily?\n > \n > \n\nWouldn't we have to *change* it to 'can't cache POST'?  Where is it\nwritten now that POST outputs can't be cached?  And isn't this a\ndifferent issue anyhow?\n\nfearing we're all on different wavelengths as usual,\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: questions &ndash;&ndash; clarifications requeste",
            "content": ">I wouldn't trust an \"Expires\" that didn't actually come along with the\n>document being served. There's a security hole otherwise; Joe\n>'Microsoft-is-Evil' might put up a form <click here> that returns\n>\n>================================================================\n>Location: http://www.microsoft.com\n>Expires: 01 Jan 2001 12:00:00 pST\n>\n><body>I am the evil Borg.</body>\n>================================================================\n>\n>Why don't we leave it as 'Can't cache POST' and not bother gilding\n>this particular lily?\n\nOh, crap!!  Pardon me while I go scream out the window .....\n\nThe same problem is currently present if we allow any 2xx request\nto return a Location field outside the requested server.\n\n\n ....Roy T. Fielding  Department of ICS, University of California, Irvine USA\n                      Visiting Scholar, MIT/LCS + World-Wide Web Consortium\n                      (fielding@w3.org)                (fielding@ics.uci.edu)\n\n\n\n"
        },
        {
            "subject": "Location Proposal",
            "content": "Proposals for additional language in the HTTP 1.1 spec.\n\nIn section 8.19:\n\nTo address the security hole that Larry Masinter recognized:\n\n\"If a Location response header is returned with a 2xx response,\nthe location must be on the same server as the request-URI.\nIf a cache or user agent receives a 2xx response containing a Location\nresponse header with a location on a different server, it should\ndisregard the Location header.\"\n\nTo inform cache and user agent implementors of the significance of the \nLocation header in 2xx responses:\n\n\"If a cache or user agent receives a 2xx response containing a\nLocation header, it should use the location designated by this header\nas the cache key for the returned resource, and should not use the\nrequest-URI for this purpose.\"\n\n\n--Shel Kaphan\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "On Wed, 30 Aug 1995, Shel Kaphan wrote:\n> Proposals for additional language in the HTTP 1.1 spec.\n> \n> In section 8.19:\n> \n> To address the security hole that Larry Masinter recognized:\n> \n> \"If a Location response header is returned with a 2xx response,\n> the location must be on the same server as the request-URI.\n> If a cache or user agent receives a 2xx response containing a Location\n> response header with a location on a different server, it should\n> disregard the Location header.\"\n\nThis assumes \"server\" is a contiguous authority - not true, there are many\nservers out there where one group putting pages might be antagonistic to\nanother group on the same server.  \n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Brian Behlendorf writes:\n...\n > This assumes \"server\" is a contiguous authority - not true, there are many\n > servers out there where one group putting pages might be antagonistic to\n > another group on the same server.  \n\nHow about this:  on the client side, caches and user agents just need\nto check that it's from the same server.\n\nOn the server side, servers should be configurable to allow or\ndisallow Location headers from parts of themselves to other parts of themselves.\nAt least this puts authority and responsibility in the right places.\n\nYes, this is some work for server writers that makes this all somewhat\nless attractive.  But I personally think it's worth it. \n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Brian Behlendorf writes:\n...\n > This assumes \"server\" is a contiguous authority - not true, there are many\n > servers out there where one group putting pages might be antagonistic to\n > another group on the same server.  \n > \n > Brian\n > \n > --=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\n > brian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n > \n\nPoint taken.  Is it possible to come up with a rule based on the URIs\nthemselves?  Otherwise, it sounds like this is just too dangerous to\nkeep. :(.  I liked it so much.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "]  > And the wording in section 6.2.2 should change to say that the response\n]  > \"should\" include a Location header field, instead of \"may\", when the\n]  > entity in the response corresponds to a resource.\n]  >\n] I don't know -- I think you should look at the uses of \"should\" in\n] 8.19 a little more closely.\n\nI'm not sure what your objection is.  But, if a server doesn't include \na Location header field in a response to POST, then you're shopping \nbasket application won't work, just as you seemed to be complaining \nabout.  So it seems to me you should be in favor of a stronger wording.\n\nAs to the meaning of should, lots of RFCs use the following definition \nfor \"should\":\n\n              This word or the adjective \"RECOMMENDED\" means that there\n              may exist valid reasons in particular circumstances to\n              ignore this item, but the full implications should be\n              understood and the case carefully weighed before choosing \nto do so.\n\nThus, \"should\" shouldn't be used if one of the exceptions is (e.g.) \n\"when using POST\" -- i.e., a very common thing.\n\nPaul\n \n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Shel Kaphan writes:\n> Proposals for additional language in the HTTP 1.1 spec.\n> \n> In section 8.19:\n> \n> To address the security hole that Larry Masinter recognized:\n> \n> \"If a Location response header is returned with a 2xx response,\n> the location must be on the same server as the request-URI.\n> If a cache or user agent receives a 2xx response containing a Location\n> response header with a location on a different server, it should\n> disregard the Location header.\"\n        ^^^^^^^^^\nmaybe better to say discard? A client located behind a non-IP-forwarding\nfirewall may not have access to internet DNS to validate the host part,\nwhile the proxy serving it must be able to do the validation! \n> \n> To inform cache and user agent implementors of the significance of the \n> Location header in 2xx responses:\n> \n> \"If a cache or user agent receives a 2xx response containing a\n> Location header, it should use the location designated by this header\n> as the cache key for the returned resource, and should not use the\n> request-URI for this purpose.\"\nHmm. It's a good question: the cache key extracted from the location header is\nan additional or a replacement one?\nThe right answer to this question may depend on accept request headers!\n(And an additional question: how can a client or a cache detect, that the accept\nheaders are in effect or there is only one URL (file) behind the request URL?\nScripts may be hidden behind virtual paths, and may return any entities\navaliable from the origin server and even other (http or other) servers.) \n\nAndrew. (Endre Balint Nagy) <bne@bne.ind.eunet.hu>\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "  ]  > And the wording in section 6.2.2 should change to say that the response\n  ]  > \"should\" include a Location header field, instead of \"may\", when the\n  ]  > entity in the response corresponds to a resource.\n  ]  >\n  ] I don't know -- I think you should look at the uses of \"should\" in\n  ] 8.19 a little more closely.\n\n  I'm not sure what your objection is.\n\nMy objection is that there is no reason to require Location headers\nunless the server has a reason to include them.  The current spec does\nnot require them except in a few specific instances (like 302 responses,\nwhich I think is the only required place), and I don't see any reason\nit should be changed to make them required.\n\n  But, if a server doesn't include \n  a Location header field in a response to POST, then you're shopping \n  basket application won't work, just as you seemed to be complaining \n  about.  \n\nThat's right, but not all applications are the same as my example.\nThe point is that a server wanting to implement an example like mine\ncan do so, and one that is doing any of an infinite variety of other\nthings doesn't have to behave the same way.\n\n  So it seems to me you should be in favor of a stronger wording.\n\nNo, I'm in favor of using things where they make sense and not otherwise.\n\n  As to the meaning of should, lots of RFCs use the following definition \n  for \"should\":\n\nThis word or the adjective \"RECOMMENDED\" means that there\nmay exist valid reasons in particular circumstances to\nignore this item, but the full implications should be\nunderstood and the case carefully weighed before choosing \n  to do so.\n\n  Thus, \"should\" shouldn't be used if one of the exceptions is (e.g.) \n  \"when using POST\" -- i.e., a very common thing.\n\n  Paul\n\nI think, as you suggested, you may be reading some other document than\nthe one I'm reading, as all the uses of \"should\" in the various\ndescriptions of the Location header I have only refer to what the URI\nin the header \"should\" be like, not when the header itself \"should\" be\npresent  (except the discussion on 302 responses, which is uncontroversial)\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "On Wed, 30 Aug 1995, Shel Kaphan wrote:\n> Brian Behlendorf writes:\n> ...\n>  > This assumes \"server\" is a contiguous authority - not true, there are many\n>  > servers out there where one group putting pages might be antagonistic to\n>  > another group on the same server.  \n> \n> How about this:  on the client side, caches and user agents just need\n> to check that it's from the same server.\n> \n> On the server side, servers should be configurable to allow or\n> disallow Location headers from parts of themselves to other parts of themselves.\n> At least this puts authority and responsibility in the right places.\n> \n> Yes, this is some work for server writers that makes this all somewhat\n> less attractive.  But I personally think it's worth it. \n\nThen the server *has* to make sure it parses the headers of every object,\nmaking \"nph\" scripts impossible.  Of course, HTTP-NG (and even HTTP\nkeepalives) make nph scripts difficult to support anyways, so maybe that's a\npiece of server functionality with a limited life span as it is.\n\nBrian\n\n--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--\nbrian@organic.com  brian@hyperreal.com  http://www.[hyperreal,organic].com/\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "> From: Brian Behlendorf <brian@organic.com>\n\n> On Wed, 30 Aug 1995, Shel Kaphan wrote:\n> > Proposals for additional language in the HTTP 1.1 spec.\n> > \n> > \"If a Location response header is returned with a 2xx response,\n> > the location must be on the same server as the request-URI.\n> > If a cache or user agent receives a 2xx response containing a Location\n> > response header with a location on a different server, it should\n> > disregard the Location header.\"\n\n> This assumes \"server\" is a contiguous authority - not true, \n\nI was about to make the same observation, but another area of problems\nis that a server might want to return a URI (is this the new name for\nLocation?) that is a URN for the document.  How is the client supposed\nto recognize that the URN is for the same \"server\"?\n\nThe only generally safe thing I can think of doing is that if a\nURI is returned to the client, it should always be considered a\nredirect, or only allowed in a redirect.  The server ought not return\nthe very same URI as for the request, to avoid an obvious loop.  But if\nit is a different URI, the client ought to be given the chance to find\nit in a local cache anyway, so a redirect is reasonable.  \n\nDaniel LaLiberte (liberte@ncsa.uiuc.edu)\nNational Center for Supercomputing Applications\nhttp://union.ncsa.uiuc.edu/~liberte/\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "  > From: Brian Behlendorf <brian@organic.com>\n...\n  > This assumes \"server\" is a contiguous authority - not true, \n\n  I was about to make the same observation, but another area of problems\n  is that a server might want to return a URI (is this the new name for\n  Location?) that is a URN for the document.  How is the client supposed\n  to recognize that the URN is for the same \"server\"?\n\nI don't really have a good answer...maybe it shouldn't be a URN for\nthis reason.  Ugh. I feel like I'm fighting a rear guard action on\nthis one.  There are so many reasons it would be nice to save this.\n\n  The only generally safe thing I can think of doing is that if a\n  URI is returned to the client, it should always be considered a\n  redirect, or only allowed in a redirect.\n\nJust to be clear, you're advocating removing the newly added\npossibility of returning Location with 2xx responses.\n\n  The server ought not return\n  the very same URI as for the request, to avoid an obvious loop.  But if\n  it is a different URI, the client ought to be given the chance to find\n  it in a local cache anyway, so a redirect is reasonable.  \n\nI'm not denying the existence of the security problem, but again, to\nbe clear, in my model of how this ought to work, the resource enclosed\nin the response should replace anything in the cache under that\nURI. This is what would make this construct so useful.  The use of\nLocation in a 2xx response is to identify a resource being sent, not as\na redirection.\n\n  Daniel LaLiberte (liberte@ncsa.uiuc.edu)\n  National Center for Supercomputing Applications\n  http://union.ncsa.uiuc.edu/~liberte/\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Idempoten",
            "content": "If I understand correctly, there are two different issues with \"idempotence\":\n(a) what it really is\n(b) when it should be applied.\n\nI am not sure that the statement of Koen,\n\n%  2) in the mathematical meaning, *all* side effects, not just `bad'\n%    side effects, must be absent the second time.\n\napplies to a mathematical model of idempotence - but I fail to see any\nmath model with side effects, too :-) anyway, this is not an issue with\nrespect to the draft.\n\nAs for (a), could we say that there is an optional property\n(named idempotence, hysteresis or whatever you want) such that any two \nconsecutive request to the server should return an identical result\n*as far as client is concerned*? I could be wrong, but my idea is that \nwe don't care if the server updates a counter or performs whatever it \nwants, as long as the client cannot notice it.\n\n.mau.\n\n\n\n"
        },
        {
            "subject": "Re: Idempoten",
            "content": "> As for (a), could we say that there is an optional property\n> (named idempotence, hysteresis or whatever you want) such that any two \n> consecutive request to the server should return an identical result\n> *as far as client is concerned*? I could be wrong, but my idea is that \n> we don't care if the server updates a counter or performs whatever it \n> wants, as long as the client cannot notice it.\n\nYou might want to spin that out in more detail; (I know I am stating\nthe obvious, but I have lost track in some of the discussion now, so\nI like to do a few steps back rather than add something: )\n\nI.e: That an idem-potent request; defined by 'method, url, http-version, ...' \nreturns the same result ( regardless of anything you did not define a moment\nago between the two tick-quotes; specifically including indepenence of\nthe time/date, host(s) involved, user-name (from basic-auth or whatever) etc...)\nevery *time/date* it is issued by the *same* client.\n\nBecause what worries me is that we really ought to specify *what* defines\na request. The *whole* header + hosts involved; or just the GET xyz http/get\nline with a few accepts... i.e. where to draw the line. \n\nIf you just look at the request; as you should not look at the server itself\nthen you have\n\n> temporal qualities, i.e. time of RQ and Answer\n> client-host, server-host, port\n> METHOD /ur HTTP/version\n> Misc\n\nI.e. the N-Dimensional approach much talked about :-)\n\nThe temporal qualities are the first issue; they are gouverned by expire or\nwhatever other specific temporal cache information.\n\nThe server-host+port is not the issue; they are gouverned by DNS and are at the\ndiscretion of the owner of the server\n\nThe client-hist *is* an issue. And should have an explicit cache directive,\nie. Reply-Not-Client-Specific * or Not-Client-Specific *.jrc.it or whatever.\n\nThen you need one for the METHOD, i.e. which ones are OK to cache and which\nones are not,\n\nAnd so on... for each of the other aspects. Is this notion correct, is this what\nwe are talking about or have a few aspects already be ruled out ?\n\nDw.\n\n\n\n"
        },
        {
            "subject": "Re: Idempoten",
            "content": "Why not use \"cacheable\" instead of \"idempotent\"?\n\n        Harald A\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Shel Kaphan:\n[... Location: URI in responses to POST requests ...]\n>   in my model of how this ought to work, the resource enclosed\n>in the response should replace anything in the cache under that URI.\n                 ^^^^^^^^^^^^^^\n\nWait a minute!  This requirement on caches opens up a whole new realm\nof failure modes, because it *requires* caches to always do some local\nprocessing on every response relayed.\n\nCurrently, whenever a cache suspects that it *may* misinterpret a\nresponse (e.g. because it contains an unknown header), it is always\nsafe to skip any local processing and pass along the response\ntransparently.  Because of this, caches can be made very robust: there\nis always an easy escape from a header parsing failure.\n\nWith your `should replace' requirement, this is no longer the case.\n\nAlso, currently, one can safely take a proxy cache offline (either\nintentionally or unintentionally) for 10 minutes and then put it up\nagain, without clearing the cache memory.  (I assume that, while the\ncache is offline, local clients will talk directly to the hosts\nnormally served though the cache.)\n\nAgain, with your `should replace' requirement, this is no longer the\ncase.\n\nI have a different model of how Location should work in mind.  Say\nthat the shopping basket URI is http://blah.com/basket.  In my scheme,\nthe server *always* puts an Expires: <yesterday> in every basket\nresponse, forcing the client to do a (conditional) GET whenever the\nbasket is requested by the user.\n\nA `Location: http://blah.com/basket' header in the POST would reduce\nIP traffic _only_ if the client (or any proxy near to the client) has\nconditional GET capability.  Here is a scenario.\n\n**** time_0:\n\n- client (user agent or proxy) cache memory has an entry\n   (Method, URI, last_verified_date, expires_on, entity_body) =\n     (GET, http://blah.com/basket, time_0, yesterday ,\n     \"In your basket: apple, orange\")\n\n**** time_1:\n\n- client user clicks the link to see the shopping basket contents\n\n- client issues a conditional GET on http://blah.com/basket with\n  IMS: time_0.\n\n- Server returns a 304 not modified response with and an\n  Expires: <yesterday> header.\n\n- client displays the basket page containing the text\n    In your basket: apple, orange\n  from the cache.\n\n- client cache memory entry is updated to\n   (Method, URI, last_verified_date, expires_on, entity_body) =\n     (GET, http://blah.com/basket, time_1, yesterday ,\n                                   ^^^^^^   \n     \"In your basket: apple, orange\")\n\n**** time_2:\n\n- client user pushes form submit button on the `buy a carrot' page.\n\n- client issues a POST on http://blah.com/addtobasket/carrot.\n\n- server returns a response\n    200 OK HTTP/1.x\n    Location: http://blah.com/basket\n    Expires: <yesterday>\n    Content-type: text/html\n\n    In your basket: apple, orange, carrot\n  to the client.\n\n- client displays basket page containing the text\n    In your basket: apple, orange, carrot.\n  to the user\n\n- client cache memory entry is updated to\n   (Method, URI, last_verified_date, expires_on, entity_body) =\n     (GET, http://blah.com/basket, time_2, yesterday ,\n                                   ^^^^^^   \n     \"In your basket: apple, orange, carrot\")\n                                   ^^^^^^^^\n\n- note that it makes no sense to add\n   (Method, URI, last_verified_date, expires_on, entity_body) =\n     (POST, http://blah.com/addtobasket/carrot, time_2, yesterday ,\n     \"In your basket: apple, orange, carrot\")\n  to the cache memory because there is no conditional POST.  If the\n  Expires header were `Expires: <tomorrow>', it would make sense.\n\n**** time_3:\n\n- client user clicks the link to see the shopping basket contents\n\n- client issues a conditional GET on http://blah.com/basket with\n  IMS: time_2.\n\n- Server returns a 304 not modified response with and an\n  Expires: <yesterday> header.\n\n- client displays basket page containing the text\n    In your basket: apple, orange, carrot\n  from the cache.\n\nSavings: If no Location were present on the POST response at time_2,\nthe client would have to issue a conditional GET with IMS: time_1 to\nthe server at time_3.  As the contents changed at time_2, the server\nwould then be forced to return a 200 response containing the complete\nshopping basket page in the entity body.\n\nThus, a Location in the POST response saves one retransmission of the\nshopping basket page contents.  It does not save on round-trip times\nfor (conditional) GETs.\n\n\nThis message has gotten a bit longer than I intended to.  Summary:\n\n1) I *never* want to omit Expires: <yesterday> or Pragma: no-cache\n   headers on dynamic responses like shopping basket pages.\n\n2) Implementing dynamic shopping basket pages by requiring that caches\n   update previously cached copies that have not expired yet is too\n   fragile\n\n3) Location: in POST responses can still save IP packets without the\n   requirement in 2) if the client (or a proxy near the client) has\n   conditional GET capability.\n\n\n>--Shel\n\nKoen.\n\n\n\n"
        },
        {
            "subject": "Re: Idempoten",
            "content": "> Why not use \"cacheable\" instead of \"idempotent\"?\n\nNo, that doesn't work - \"cacheable\" refers to the object, \"idempotent\" refers \nto the method. I would rather say that an idempotent method does not change \nthe topology of the Web.\n\nThe reason for using \"topology\" and not \"state\" is that in some cases, \nidempotent requests can change the state of the web - especially if log files \nare considered as a part of the state of the Web, or for example if a document \ncan be accessed 5 times.\n\n-- \n\nHenrik Frystyk Nielsen, <frystyk@w3.org>\nWorld-Wide Web Consortium, MIT/LCS NE43-356\n545 Technology Square, Cambridge MA 02139, USA\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "If there is a real possibility that caches would fail to update, or at\nleast invalidate (which would be almost as good), items in their\ncaches as a result of receiving Location headers, then I am forced to\nagree with you -- certain kinds of pages just have to be completely\nuncacheable.  :(.  \n\nAt this point, using Location would just be introducing a very slight\nbandwidth saving -- so slight as to hardly be worth the trouble, I\nthink.  If certain dynamic pages must always be pre-expired (not\nservable from a cache), then Location is unnecessary to prevent\n\"doppelgangers\" -- out of date duplicates in a cache under different\nURIs.  This was what I imagined as its main use in the context of\ncaching.  Oh well.\n\nThe case of the cache that goes down for a while, and comes up holding\nnow-invalidated copies of things without knowing it, seems to apply\nmore generally than to just this case, however.\n\n--Shel\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Now you've got me worried.  The example you gave requires that your\n\"basket\" page never be cached, essentially because it is accessed\nunder different URIs for different request methods, and caches in the\nworld can't be assumed to be continuously up, robust, and correct.\n\nThis then seems to imply a general unpleasant side effect of using\nLocation URI != request URI.  If any frequently changing responses are\never sent on a request URI where that URI is also used on some other\nresponse as a Location header URI != its request URI, the response\nmust be uncacheable.  Otherwise a non-robust cache that fails to\nnotice the Location header going by may get stuck with an out of date\nversion of the response.\n\n\n\n"
        },
        {
            "subject": "RE: questions &ndash;&ndash; clarifications requeste",
            "content": "Shel writes:\n]\n] I think, as you suggested, you may be reading some other document than\n] the one I'm reading, as all the uses of \"should\" in the various\n] descriptions of the Location header I have only refer to what the URI\n] in the header \"should\" be like, not when the header itself \"should\" be\n] present  (except the discussion on 302 responses, which is uncontroversial)\n\nI think I finally understand; sorry for being so dense.  We're reading \nthe same document, just interpreting it quite differently. One of the \nrelevant sentences in section 8.19 reads in part:\n\" For 2xx responses, the location should be the URL needed to retrieve that\n   same resource again...\"\n\nIt seems you read that as:\n\"If the location header is present, then for 2xx responses, it should \nbe the URL...\"\nwhile I read it as:\n\"For 2xx responses, the location header should be present, and should \nbe the URL...\"\n\nSince we're hopefully both reasonable people, and each read it quite \ndifferently, I'd conclude that it's ambiguous.\n\nWhile I would argue for one of those choices over the other, even more \nimportant is that it be clear which it is.\n\nAnd, if it's the first way, I'd like some definition of the \ncircumstances when it should be present. Which is why I earlier \nproposed the following replacement for the first sentence of section 8.19:\n\n\"If the entity-body in a response corresponds to a resource (or, in the \ncase of HEAD, would correspond to a resource if it were present), the \nLocation response header field defines the exact location of that \nresource -- even if it is the same as the Request-URI.\"\n\nPaul\n\n\n\n"
        },
        {
            "subject": "location of draft-v1003",
            "content": "Hello-\n  Where can I find draft-v10-03a? I cannot find it on ds.internic.net. \n\"draft-ietf-http-v10-spec-02.txt\" is there, but not \n\"draft-ietf-http-v10-spec-03a.txt\".\n\n  Please respond directly to me. (Less waste of bandwidth)\n\nthanx in advance,\n shy aberman\n shy@peachweb.com\n\n\n\n"
        },
        {
            "subject": "Re: Idempoten",
            "content": "Henrik Frystyk Nielsen writes:\n > \n > > Why not use \"cacheable\" instead of \"idempotent\"?\n > \n > No, that doesn't work - \"cacheable\" refers to the object, \"idempotent\" refers \n > to the method. I would rather say that an idempotent method does not change \n > the topology of the Web.\n > \n\nMost server side effects also do not change the topology of the web,\nfor instance, changes to an SQL database attached to the server, that\neventually cause pizzas to be dispatched.  But these server side effects are\nlargely what is at issue.\n\n\n\n"
        },
        {
            "subject": "idempoten",
            "content": "What property is it that GET and HEAD have that no other methods\nhave?   [ don't answer \"idempotence\"! ]\n\nOne answer: they are the only methods (so far) that can be handled by\nan intermediate proxy other than the origin server.\n\nI think that's what should be said about them instead of calling them\n\"idempotent\", \"usually side-effect free\", \"topology-preserving\",\n\"read-only\", \"always-the-same-result-producing-except-when-they-don't\",\nor other properties.\n\nWhy can GET and HEAD be served by caches? Because they conventionally\nhave no side effects at the origin server.  We say \"conventionally\"\nbecause some script writers might choose to violate this rule for\ntheir own nefarious purposes.  They can't be stopped, so don't try.  A\n\"side effect\" is a change that affects the future behavior of the\nserver and its environment as detectable by users and user agents.  So\n\"side effects\" exclude log file entries, last-accessed dates on files,\netc.  It's always going to be a somewhat fuzzy distinction -- after\nall someone could write a script that performed financial transactions\nbased on file access dates or number of characters in a log file.\n\nNo other standard HTTP methods share this property of being able to be\nhandled by intermediate proxies.  (With Pragma: no-cache this is\nuntrue even of GET and HEAD).\n\nIf GET or HEAD normally had side effects on the origin server, that\nwould contradict their ability to be handled by caches.  So there's a\n\"convention\" that they shouldn't.\n\nIsn't that all it is necessary to say about it?\n\n\n\n"
        },
        {
            "subject": "Re: Location Proposal",
            "content": "Shel Kaphan:\n>If there is a real possibility that caches would fail to update, or at\n>least invalidate (which would be almost as good), items in their\n>caches as a result of receiving Location headers, then I am forced to\n>agree with you -- certain kinds of pages just have to be completely\n>uncacheable.  :(.  \n\nYes.  But keep in mind that, with conditional GETs, there is a\ndifference between `this page is not cacheable (has Pragma: no-cache\nor Expires <yesterday>' and `this page cannot be served faster/cheaper\nthe next time using a proxy cache'.\n\nIf a response is not (or no longer) cacheable, this does not mean that\na cache is required to throw it away.  If only means that the cache\nmay not serve the response without first contacting the origin server\n(preferably with a conditional GET).  If the conditional GET returns a\n`not modified' code, the (expired) response in the cache memory may be\npassed on to the client, and bandwidth is saved.\n\n>At this point, using Location would just be introducing a very slight\n>bandwidth saving -- so slight as to hardly be worth the trouble, I\n>think.\n\nAs caches can safely ignore Location headers on POST responses (under\nmy scheme), there is no real disadvantage to putting this type of\nLocation header use in the spec.  Bandwidth saving has to wait until\nconditional GETs become more common.  (Has anyone implemented them\nnow?)\n\n>The case of the cache that goes down for a while, and comes up holding\n>now-invalidated copies of things without knowing it, seems to apply\n>more generally than to just this case, however.\n\nUnder the current spec, going down and coming up without clearing the\ncache database is safe (assuming a reasonable implementation that\nkeeps absolute time stamps in the database, and always checks these\ntime stamps before serving a response from cache).\n\nIf a cache comes up again after 100 days, it will simply find that\nmost of the `expires after' timestamps on its cache slots hold dates\nmore than 90 days before the current time.  No problem.\n\n[..in other message..]\n\n>Now you've got me worried.  The example you gave requires that your\n>\"basket\" page never be cached, essentially because it is accessed\n>under different URIs for different request methods,\n\nNo.  Essentially, it cannot be cached because it is dynamic, because\nit may change 1 second from now.\n\nAnd again, note that `cannot be cached' does not mean that a cache may\nnot store it.\n\n> and caches in the\n>world can't be assumed to be continuously up, robust, and correct.\n\nCaches can be assumed to be robust and correct, even if they go down\nsometimes.  My point was that your `should replace' requirement would\nrequire (correct) caches to mark all unexpired entries as expired if\nthey come up again after having been down.  This is 1) wasteful and 2)\nrequires all current cache implementations to be upgraded.\n\n>This then seems to imply a general unpleasant side effect of using\n>Location URI != request URI.\n\nNo.  Under my scheme, Location URI != request URI does not introduce\nrobustness problems, for non-expired and expired entries alike.\n\n>  If any frequently changing responses are\n>ever sent on a request URI where that URI is also used on some other\n>response as a Location header URI != its request URI, the response\n>must be uncacheable.\n\nNot just those responses, *any* frequently changing response must be\nmarked with Pragma: no-cache or Expires: <yesterday>, even if no\nalternative URI's are given in Location or URI response headers.\n\n>  Otherwise a non-robust cache that fails to\n>notice the Location header going by may get stuck with an out of date\n>version of the response.\n\nYou are still thinking in terms of a mechanism that makes caches\nreplace previously cached, but unexpired, copies.  The caching\nscenario in my previous message assumed that no such mechanism was\npresent.\n\nI guess we need a term for the practice of keeping an expired response\nin cache memory to facilitate future conditional GET gets.  What about\n`conditionally cached'?\n\nHmm.  I may be on to something.  The spec could use language like\n`Pragma: no-cache instructs the cache not to unconditionally cache the\nresponse.  The response may however be cached conditionally.'\n\n>--Shel\n\nKoen.\n\n\n\n"
        }
    ]
}